<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.03243] Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM</title><meta property="og:description" content="This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code. Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the Ev…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.03243">

<!--Generated on Tue Feb 27 20:24:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text" style="font-size:90%;">Safurai Team</span> <span id="p1.1.2" class="ltx_text" style="font-size:90%;">Berlin, Germany
<br class="ltx_break">6th November 2023</span> <span id="p1.1.3" class="ltx_text" style="font-size:90%;">https://www.safurai.com</span></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p ltx_align_center"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p ltx_align_center"><span id="p3.1.1" class="ltx_text"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cifarelli D., Boiardi L., Puppo A., Jovanovic L
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id1.id1" class="ltx_p">This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code. Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded dataset for its fine-tuning process. The results of its performance, a notable score of 56.33% on the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity to streamline developers’ workflows and aid code learning. It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclusive and wide-ranging development in the field of language-specific LLMs.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Artificial intelligence’s influence on software development and coding is immense and rapidly evolving. OpenAI models have led the closed-source models landscape; the open-source community has risen to the challenge primarily with models like Starcoder<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, WizardCoder<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite> and PanguCoder<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>. While these models have significantly closed the performance gap between open and closed-source models in general, the need for language-specific large language models remains apparent. Safurai-Csharp, introduced to fill this gap, excels in programming for the C# language, offering a significant milestone in AI-driven coding tools.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The recent introduction of CodeLlama<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code</span></span></span> by Meta also imposes new potential foundational models for Coding LLMs, marking another significant stride in this burgeoning area. Safurai-Csharp aims to capitalize on these cutting-edge foundational models and novel techniques to offer competitive solutions in the realm of code generation specifically for C#.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper presents an open-source model – Safurai-Csharp, which specializes in facilitating code completion, debugging, and generation for the C# programming language. With a remarkable 56.33% score in the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), Safurai-Csharp exhibits a strong capability to generate code and natural language about code, streamlining developers’ workflows and assisting in code learning.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The model achieves this feat by leveraging a rich and efficiently filtered dataset of 13 billion tokens specific to the C# language. It incorporates the novel Evol Instruct method, introduced by Xu et al. in 2023 with the WizardCoder LLM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>, which enriches and diversifies the initial instruction dataset through iterative evolution and upgrading processes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In summary, our contributions in this paper include:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Introducing Safurai-Csharp, a state-of-the-art model specialized for C# language operations, including code completion, debugging, and generation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Showcasing remarkable performance with a 56.33% score in the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), setting a new standard for open-source C# language model.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Implementing the creation of the synthetic dataset through the data distilliation of CodeLlama-34b combined with the Evol Instruct technique, to iteratively enrich and diversify the initial C# instruction dataset.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We hope that Safurai-Csharp provides a robust opportunity for various sectors and adds a new valuable model to the Open-Source variety of Coding LLMs.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Coding Large Language Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The Codex, a 12 billion parameters model, signifies a remarkable breakthrough in the realm of Python coding, successfully solving roughly 72% of Python-based tasks. This groundbreaking achievement has propelled the advancement of other evolved code producing models such as AlphaCode <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>, PaLM Coder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>, and PanGu-Coder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>. Nevertheless, these cutting-edge models’ lack of open-source availability has been a significant limitation. This gap, however, has been addressed with the introduction of numerous open-source alternatives such as CodeParrot, PolyCoder, PyCodeGPT, SantaCoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>, and StarCoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, rejuvenating the code generation discipline. Additionally, the continual broadening of the application arenas for code generation attests to the field’s escalating practical relevance. For instance, CodeGeeX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite> and ERNIE-Code <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite> were specifically developed to facilitate multilingual modeling. Moreover, JuPyT5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite> was trained on a vast corpus of Jupyter notebooks, targeting the enhancement of interactive programming. In an effort to infuse language models with API functionality, models such as DocCoder and APICoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite> have also been created. Furthermore, models like InCoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>, SantaCoder, and StarCoder provide support for code generation in diverse locations. In recent times, some groups have exploited instructional tuning techniques to access the enormous potential expertise encapsulated within large language models. This procedure involves diligent refinement of these models using high-quality datasets. In the sphere of code generation, models such as WizardCoder (15B), PanguCoder and phi-1 (1.3B) distinguished themselves through exceptional performance, achieved through meticulous fine-tuning using data produced by OpenAI’s GPT-3.5 and GPT-4.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In this context, it is pertinent to mention the CodeLlama <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code</span></span></span> models developed by Meta AI. Code Llama is a subsequent training iteration of Llama 2, specifically designed for code generation and completion. It also offers code debugging and supports a plethora of programming languages including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash, and more. Code Llama variants are available in three sizes, i.e., 7B, 13B and 34B parameters, each trained with 500B tokens of code and code-related data. These models cater to diverse requirements, with the 7B model being deployable on a single GPU, while the 34B model, although requiring more resources, provides superior results. Furthermore, Code Llama has two specialized versions: Code Llama – Python, which is specifically fine-tuned on Python code, and Code Llama – Instruct, which focuses on user instruction comprehension for code generation.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">From Microsoft Research, Phi1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> is a notable development, as an evolutionary step from Phi1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>. The Phi-1.5 model leverages Large Language Models (LLMs) to generate top-quality, textbook-like datasets. This model retains the architectural design of its predecessor, Phi-1, but improves upon it by training on a dataset consisting of 30 billion tokens, of which approximately 20 billion tokens were artificially synthesised from GPT-3.5. The outcomes of Phi-1.5 are comparable with models five times its size on typical language tasks, and it surpasses most LLMs in more complex reasoning assignments. Additional versions of Phi-1.5 were developed using traditional filtered web data along with synthetically created NLP data, named phi-1.5-web-only and phi-1.5-web, to further leverage the potential of this model.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Also the Phind<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.phind.com/blog/code-llama-beats-gpt4</span></span></span> Models reflect an unprecedented evolution in the field of Coding LLMs, demonstrating remarkable capabilities to surpass the proficiency exhibited by GPT-4 on HumanEval. Meta AI developed and fine-tuned two versions of CodeLlama-34B, i.e., CodeLlama-34B and CodeLlama-34B-Python, on an exclusive Phind dataset. Both models demonstrated exemplary performance, achieving 67.6% and 69.5% pass@1 on HumanEval, respectively. It is noteworthy that these figures are superior to GPT-4’s performance, which reached 67% as per the official technical report released in March. These refined models were tested using OpenAI’s decontamination methodology to confirm the validity of the results. Recently, the training of a new model named Phind-CodeLlama-34B-v2 resulted in further augmentation of pass@1 score, reaching 73.8% on HumanEval. These achievements reaffirm the potential of continuous learning and fine-tuning in enhancing the efficacy of Code LLMs, setting new benchmarks in the field.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">In the midst of this exciting ongoing development in the Coding LLMs landscape, a model that struck an effective balance between the essentials of performance and efficiency emerged. Named as Mistral 7B<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>, this model dispels the prevalent notion that augmenting model performance mandates a proportionate increase in the model size. Not only does Mistral 7B present an efficient alternative to this scalibility issue, it demonstrates that strategic design and thoughtful training can yield high performance while ameliorating computational burdens and inference latency. Exceptionally remarkable about this model is that despite belonging to the 7 billion parameter category, it managed to outclass its larger counterparts such as the 13 billion parameter Llama 2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite> across all tested benchmarks, and even outstripped the 34 billion parameter Llama 34B in mathematics and code generation tasks.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Synthetic data generation through distillation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Synthetic data generation through distillation is a machine learning technique that seeks to create artificial datasets that mimic the underlying structures and complexities of real-world data. Essentially, what distillation does is to create a model (teacher model) that can generate data, which feels ’real’ but is actually artificial. This data can then be used to train another model (student model), which ideally, will learn to generalize better from the diversified data produced by the teacher model.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Microsoft Research introduced a model known as Phi-1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>, which is revolutionary in terms of using synthetic data created through distillation for model training. Phi 1 criticizes the usage of standard source of code generation such as The Stack and web-based datasets like StackOverflow and CodeContest, arguing they lack the necessary properties for efficient understanding and planning of algorithms. Phi 1 finds the widely used datasets to be fragmented, containing trivial or boilerplate code, and having an unbalanced distribution of coding concepts and skills. Consequently, Microsoft Research introduced an innovative training technique that utilizes three major datasets namely a filtered code-language dataset, a synthetic textbook dataset and a small synthetic exercises dataset.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">A significant part of their training data comprises less than 1 billion tokens of synthetic data derived from GPT-3.5 generated Python textbooks. This innovative synthetic data generation technique through distillation allowed Microsoft Research to develop superior models like Phi 1 and its evolved version, Phi 1.5<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>. The Phi-1.5 model utilizes Large Language Models (LLMs) to generate top-quality, textbook-like datasets for its training. It was trained on a dataset of 30 billion tokens, where approximately 20 billion tokens were synthetically created from GPT-3.5. The Phi-1.5 model displayed impressive results, matching the efficiency of larger models five times its size on regular language tasks, and outperforming most LLMs on more complex reasoning assignments.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Data Augmentation Techniques</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Data augmentation in the context of LLMs fine-tuning refers to various techniques used to increase the size and improve the quality of training data. This is done through making certain modifications to the existing data which could entail tasks like synonyms replacement, sentence restructuring, translation between multiple languages, adding noise to data, and others. The primary goal is to create a robust model that understands diverse patterns with increased accuracy. This process helps in minimizing over-fitting, improving the model’s performance and its ability to generalize over unseen data.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">An intriguing example of data augmentation techniques specifically tailored for coding tasks in LLMs can be seen in Microsoft’s application of the EvolInstruct method for finetuning the WizardCoder<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite> model. This method revolves around a unique three-stage pipeline that progressively evolves complex instructions, refines the respective responses, and then eliminates deficient instruction data.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">The EvolInstruct approach dives deep into upscaling the complexity of instructions through a dual strategy - ’In-depth Evolving’ and ’In-breadth Evolving’. The former method enhances instruction complexity by integrating constraints, deepening contextual information, concretising abstract ideas, increasing the chain of reasoning steps, and complicating input data. Conversely, the ’In-breadth Evolving’ strategy is designed to create a multitude of new instructions deriving from the existing ones, thereby fostering an expansively diverse set of commands.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Empirical evidence validates the effectiveness of the EvolInstruct method, as it was meticulously deployed for finetuning the LLaMA LLM, leading to the development of the proficient WizardLM model.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Fine-tuning with LoRA and QLoRA</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">LoRA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite> (Low-Rank Adapters) and QLoRA (Quantized LoRA) are revolutionary finetuning techniques for Large Language Models (LLMs). These techniques serve as efficient and practical solutions for managing the large, resource-intensive nature of LLMs. With conventional LLMs, their vast storage requirements and heavy GPU utilisation limits their scalable applications. However, LoRA and QLoRA manage these limitations remarkably, enabling the fine-tuning of the model while reducing significant storage and computational costs.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">LoRA offers a strategic approach by introducing the concept of low-rank approximations of the changes that are made to the weight matrices during training. Instead of updating the complete weight matrix, the LoRA approach approximates these changes and stores them separately. These changes can be applied later when needed for a specific task, in essence, making them adaptable lenses or attachments to the larger model. However, where LoRA really shines is in the immense memory savings enabled through matrix decomposition, where large matrices are decomposed into smaller matrices, reducing the overall data requirements.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">QLoRA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>, on the other hand, takes a different yet complementary approach to the problem. It introduces the concept of quantization, where the model weights are compressed into a smaller data format, significantly reducing the memory footprint. QLoRa is essentially a combination of quantization and LoRA. The model is first quantized into a smaller data format, then LoRA training is performed, reducing further the computational load.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">In a nutshell, LoRA and QLoRA are transformative techniques that not only address the major challenges associated with managing and deploying LLMs but also bring finetuning a step closer to practicality. They ensure high performance of the models, reducing not only storage cost but also computational power requirements. This ability to reduce resources while maintaining model performance establishes LoRA and QLoRA as significant advancements in the machine learning field.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Initial dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This section is dedicated to the detailing of our computational process in creating our initial dataset which we used for the fine-tuning of the LLM.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The foundation model, the CodeLlama34B<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/codellama/CodeLlama-34b-hf</span></span></span>, designed by Meta AI, played an instrumental role in the genesis of the dataset. CodeLlama34B, with its remarkable capacity to generate programming problems, was utilized to produce an initial batch of 100,000 examples of C# programming tasks. The tenacity of the model’s output provided us a large pool from which we could sift and gather top-tier examples that best suited our needs for optimization and refinement.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">However, merely generating a large number of examples was not sufficient; a critical step that followed was the filtration of this dataset. Not all problems generated were required or ideally suited for our particular training set. For instance, we encountered issues with tasks that were too analogous, presenting no significant variance that would offer productive learning for our LLM. Similarly, certain tasks were excessively verbose, which though useful in certain contexts, did not align with the objectives outlined for Safurai-Csharp.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Thus, the steps taken to purify the dataset involved removing these undesirable examples until we were left with a collection of 4,000 problems—one that was both rich in instructional value and diversity, and manageable in terms of size.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Further enrichment of the dataset was achieved by incorporating solutions for each of these C# problems. The same engine, CodeLlama34B, was used to generate relevant solutions for each problem. This not only provided a cohesive dataset of problem-solution pairs, but also ensured comprehension and functionality across a range of C# tasks, thereby enhancing the learning scope of our LLM.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>The distillation prompt used to generate 100,000 examples from Codellama34B</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<code id="S3.SS1.SSS1.p1.1" class="ltx_verbatim"> <svg id="S3.SS1.SSS1.p1.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS1.SSS1.p1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS1.SSS1.p1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S3.SS1.SSS1.p1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Data Augmentation with Evol-Instruct</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We employed the EvolInstruct process, a technique notably applied by Microsoft in their fine-tuning of the WizardCoder model. Our dataset, initially comprising 4,000 pairs of C# problems and solutions, underwent augmentation with the application of EvolInstruct thrice, culminating in a significantly expanded and diversified dataset of 16,000 pairs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The application of EvolInstruct to our dataset adhered to its three-stage pipeline, evolving complex instructions initially, followed by refining associated responses, and, lastly, eliminating unsatisfactory instruction data.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The core of this methodology rests on enhancing the complexity of instructions—the fundamental element in our problem dataset. Here, the ’In-depth Evolving’ strategy was extensively used, interweaving constraints, cementing abstract concepts, deepening contextual information, iterating the reasoning steps, and complicating input data, resulting in a series of intricate and diverse programming tasks.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The augmentation process also included the ’In-breadth Evolving’ strategy, which led to proliferation of our problem set, ensuring an array of diverse instructions stem from the existing ones. The result was a template with manifold complex tasks, embodying a much broader set of commands.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">This approach to data augmentation through the EvolInstruct method proved to be beneficial in optimizing our LLM. Owing to its documented success in fine-tuning the WizardCoder model, its application to our dataset was apt and resulted in a significant improvement in the quality of the training data, bolstering the performance of our Safurai-Csharp model.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2311.03243/assets/distr.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="455" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Token distribution in our dataset</figcaption>
</figure>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Fine-tuning Setup</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Here is a walkthrough of how we finetuned the Safurai-Csharp model.
To begin with, the base model and its respective configuration were implemented using CodeLlama-34b-hf. This base model was derived from the CodeLlamaForCausalLM model class and employed the CodeLlamaTokenizer type, which is specifically designed for programming languages.
The model was set to load in 4-bit mode to save memory and increase the processing speed. The ”strict” mode, used to stringently match the model’s state dictionary with the loaded state dictionary, was set to ’false’ to allow smooth loading even if certain keys are missing.
Safurai-Csharp was trained on the dataset we previously described. The model adopts sample packing and adjusts the input to a sequence length of 4096 to optimize GPU memory usage.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">One of the distinguishing features of this finetuning process is the use of Low Rank Adapters (LoRA). By setting the ’adapter’ option to ‘lora’, we introduced efficient layer-wise low-rank adapter training. We further customized the LoRA setting with a rank (lora_r) of 32 and a scaling factor (lora_alpha) of 16. These settings served to cap the complexity of LoRA while preserving the most significant components from the full-rank space. A dropout rate of 0.05 was set to prevent overfitting during the finetuning phase.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Training parameters were specified to ensure model efficiency. Gradient accumulation steps were set to 4, with a micro-batch size of 2 and three epochs to cover computations for a learner over the entire dataset.
To optimize the model, we went with the adamw_bnb_8bit optimizer. Cosine learning rate scheduling was used, and the initial learning rate was set to 0.0003.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Gradient checkpointing was enabled for better GPU memory usage when dealing with large models and long sequences. Advanced transformer optimizations were achieved through ’flash_attention’ set as true.
The model was designed to perform regular validation checks after 40 steps, also known as warmup steps. The logging of training dynamics was set to occur after every step.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Lastly, with respect to special tokens, the model used <span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_typewriter">&lt;s&gt;</span> for beginning of stream, <span id="S3.SS3.p5.1.2" class="ltx_text ltx_font_typewriter">&lt;/s&gt;</span> for end of stream, and <span id="S3.SS3.p5.1.3" class="ltx_text ltx_font_typewriter">&lt;unk&gt;</span> for unknown tokens.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">In summary, the Safurai-Csharp model has been created with careful selection of parameters and techniques like Low-Rank Adapters (LoRA) and Quantization, ensuring efficient finetuning of the LLM.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.03243/assets/trainloss.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="399" height="250" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Train/Loss Graph</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>The YAML configuration file for Fine-Tuning</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<code id="S3.SS3.SSS1.p1.2" class="ltx_verbatim"> <svg id="S3.SS3.SSS1.p1.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS3.SSS1.p1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS3.SSS1.p1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S3.SS3.SSS1.p1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg> <code id="S3.SS3.SSS1.p1.2.1" class="ltx_verbatim"> <svg id="S3.SS3.SSS1.p1.2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS3.SSS1.p1.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS3.SSS1.p1.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S3.SS3.SSS1.p1.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experimental Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Manual MultiPL-E Evaluation Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">HumanEval<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/openai/human-eval</span></span></span>, established by OpenAI, has been the go-to benchmark in evaluating coding AI models. It focuses on the model’s code-generation capacity based on precise requests and provides standard solutions and tests. MultiPL-E<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://huggingface.co/datasets/nuprl/MultiPL-E/</span></span></span> extends the boundaries of the current benchmarks by translating them to new languages and thereby becoming a massive multi-language benchmarking platform. It is continuously expanding in terms of number of programming languages, providing a useful comparison point for models like Safurai-Csharp.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Our model, Safurai-Csharp, achieved a pass@1 score (Zero-Shot) of 56.33% on the Manual MultiPL-E Benchmark for C# with n=5. Looking beyond the conventional scopes of evaluation, our team set out on the task of MultiPL-E ”custom” evaluation, leaving no stone unturned to extract the most accurate measure of our model, Safurai-Csharp’s performance. Basing ourselves on the MultiPL-E dataset, we conducted tests involving the 158 question-prompts, dedicating 5 unique answer to each prompt.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Traditionally, the MultiPL-E benchmark is designed to take just one function from the Safurai response to complement the function specified in the question-prompt. Safurai very often use helper functions in its responses. So, our team adopted manual evaluations (we completed the code and compiled it to have tests results) to substantiate the true test score and compare it with Codellama 34B<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://huggingface.co/codellama/CodeLlama-34b-hf</span></span></span>, Mistral 7B Instruct<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1</span></span></span>, GPT4<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite> and ChatGPT (we manually tested also these models in this process).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Results of pass@1(%), Zero-Shot, n=5 on manual MultiPL-E C# Benchmark</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:332.7pt;height:1194.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.6pt,149.2pt) scale(0.8,0.8) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.1" class="ltx_p"><span id="S4.T1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</td>
<td id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.1.1" class="ltx_p"><span id="S4.T1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Size</span></span>
</span>
</td>
<td id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.1.1" class="ltx_p"><span id="S4.T1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">C#</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3"><span id="S4.T1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Closed source models</span></td>
</tr>
<tr id="S4.T1.1.1.3.3" class="ltx_tr">
<td id="S4.T1.1.1.3.3.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.3.3.1.1.1" class="ltx_p">GPT-3.5 Turbo [2023]</span>
</span>
</td>
<td id="S4.T1.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.3.3.2.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S4.T1.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.3.3.3.1.1" class="ltx_p">58.22</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.1.4.4" class="ltx_tr">
<td id="S4.T1.1.1.4.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.4.4.1.1.1" class="ltx_p">GPT-4 [2023]</span>
</span>
</td>
<td id="S4.T1.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.4.4.2.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S4.T1.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.4.4.3.1.1" class="ltx_p">65.19</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.1.5.5" class="ltx_tr">
<td id="S4.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3"><span id="S4.T1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">Open source models</span></td>
</tr>
<tr id="S4.T1.1.1.6.6" class="ltx_tr">
<td id="S4.T1.1.1.6.6.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.6.6.1.1.1" class="ltx_p">Mistral-7B-Instruct-v0.1 [2023]</span>
</span>
</td>
<td id="S4.T1.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.6.6.2.1.1" class="ltx_p">7B</span>
</span>
</td>
<td id="S4.T1.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.6.6.3.1.1" class="ltx_p">7.59</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.1.7.7" class="ltx_tr">
<td id="S4.T1.1.1.7.7.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.7.7.1.1.1" class="ltx_p">Codellama 34B [2023]</span>
</span>
</td>
<td id="S4.T1.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.7.7.2.1.1" class="ltx_p">34B</span>
</span>
</td>
<td id="S4.T1.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.7.7.3.1.1" class="ltx_p">30.4</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.1.8.8" class="ltx_tr">
<td id="S4.T1.1.1.8.8.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.8.8.1.1.1" class="ltx_p"><span id="S4.T1.1.1.8.8.1.1.1.1" class="ltx_text ltx_font_bold">Safurai-Csharp [2023]</span></span>
</span>
</td>
<td id="S4.T1.1.1.8.8.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.8.8.2.1.1" class="ltx_p"><span id="S4.T1.1.1.8.8.2.1.1.1" class="ltx_text ltx_font_bold">34B</span></span>
</span>
</td>
<td id="S4.T1.1.1.8.8.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T1.1.1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.8.8.3.1.1" class="ltx_p"><span id="S4.T1.1.1.8.8.3.1.1.1" class="ltx_text ltx_font_bold">56.33</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Improvements from our Foundation Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The evolution from our foundation model, CodeLlama, to Safurai-Csharp stands as a testament to the power of fine-tuning and optimized dataset curations. Safurai-Csharp’s performance as measured by the Manual MultiPL-E Benchmark<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://huggingface.co/datasets/nuprl/MultiPL-E</span></span></span>, scored an impressive 56.33%, almost double that of its predecessor, CodeLlama, which scored 30.4%. This marked improvement is a clear reflection of the considerable strides made during the fine-tuning process.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">A key contribution that led to Safurai-Csharp’s impressive performance gain was the application of the EvolInstruct technique in the augmentation and crafting of the final dataset. Not only did this method help in creating a robust and diverse array of C# problems, but it also contributed greatly to Safurai-Csharp’s ability to generalize and solve unseen tasks effectively. The resulting superior performance of Safurai-Csharp illustrates the virtue of careful and meticulous dataset preparation and the value of elaborate fine-tuning processes.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Comparison of Safurai-Csharp, Codellama 34B, ChatGPT, GPT-4 and Mistral-7B-Instruct-v0.1 on MultiPL-E C# First Problem</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p ltx_align_center"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Problem</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<code id="S4.SS3.p2.7" class="ltx_verbatim"> <svg id="S4.SS3.p2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>
<span id="S4.SS3.p2.7.7" class="ltx_inline-block">
<span id="S4.SS3.p2.7.7.1" class="ltx_p ltx_align_center"><span id="S4.SS3.p2.7.7.1.1" class="ltx_text ltx_font_bold">Safurai-Csharp Output</span></span>
</span>
Sure! Here’s a optimized solution to the ‘StringLength‘ function in C#:
<code id="S4.SS3.p2.7.6" class="ltx_verbatim"> <svg id="S4.SS3.p2.2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg><code id="S4.SS3.p2.7.6.5" class="ltx_verbatim"> <svg id="S4.SS3.p2.3.2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>Explanation:
1. The ‘StringLength‘ function takes a string ‘str‘ as input and returns its length as a ‘long‘ value.
2. To get the length of a string, we can use the ‘Length‘ property of the ‘string‘ class, which provides the number of characters in the string.
3. In the ‘Main‘ method, we test the ‘StringLength‘ function by passing two sample inputs and printing their lengths.
This solution is optimized as it uses the built-in functionality of the ‘string‘ class, which provides an efficient way to get the length of a string. The code is easy to read and understand due to its simplicity and clear comments.”

<span id="S4.SS3.p2.7.6.5.5" class="ltx_inline-block">
<span id="S4.SS3.p2.7.6.5.5.1" class="ltx_p ltx_align_center"><span id="S4.SS3.p2.7.6.5.5.1.1" class="ltx_text ltx_font_bold">ChatGPT Output</span></span>
</span>
It looks like you’ve started defining a C# class called Problem with a Strlen method that is supposed to return the length of a given string. However, the code snippet you provided is incomplete. I’ll help you complete the Strlen method and add a sample Main method to demonstrate how to use it:
<code id="S4.SS3.p2.7.6.5.4" class="ltx_verbatim"> <svg id="S4.SS3.p2.4.3.2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg><code id="S4.SS3.p2.7.6.5.4.3" class="ltx_verbatim"> <svg id="S4.SS3.p2.5.4.3.2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>In this code, I’ve completed the Strlen method by using the Length property of the string to return its length. I’ve also added a Main method where you can see how to use the Strlen method to calculate the lengths of different strings.

<span id="S4.SS3.p2.7.6.5.4.3.3" class="ltx_inline-block">
<span id="S4.SS3.p2.7.6.5.4.3.3.1" class="ltx_p ltx_align_center"><span id="S4.SS3.p2.7.6.5.4.3.3.1.1" class="ltx_text ltx_font_bold">GPT-4 Output</span></span>
</span>
Here is the C# implementation for Strlen function which is used to return the length of the given string.
<code id="S4.SS3.p2.7.6.5.4.3.2" class="ltx_verbatim"> <svg id="S4.SS3.p2.6.5.4.3.2.1.pic1" class="ltx_picture" height="236.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,236.84) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 230.94 C 0 234.2 2.64 236.84 5.91 236.84 L 594.09 236.84 C 597.36 236.84 600 234.2 600 230.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 230.94 C 1.97 233.11 3.73 234.88 5.91 234.88 L 594.09 234.88 C 596.27 234.88 598.03 233.11 598.03 230.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="209.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.6.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.6.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.6.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>In the function above, the Length attribute of ”string” type is used. This attribute returns the number of characters in the current string.

<span id="S4.SS3.p2.7.6.5.4.3.2.2" class="ltx_inline-block">
<span id="S4.SS3.p2.7.6.5.4.3.2.2.1" class="ltx_p ltx_align_center"><span id="S4.SS3.p2.7.6.5.4.3.2.2.1.1" class="ltx_text ltx_font_bold">Mistral 7B Instruct Output</span></span>
</span>
Here’s a possible implementation for the StringLength method in C#:
<code id="S4.SS3.p2.7.6.5.4.3.2.1" class="ltx_verbatim"> <svg id="S4.SS3.p2.7.6.5.4.3.2.1.pic1" class="ltx_picture" height="253.45" overflow="visible" version="1.1" width="600"><g transform="translate(0,253.45) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 247.54 C 0 250.8 2.64 253.45 5.91 253.45 L 594.09 253.45 C 597.36 253.45 600 250.8 600 247.54 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 247.54 C 1.97 249.72 3.73 251.48 5.91 251.48 L 594.09 251.48 C 596.27 251.48 598.03 249.72 598.03 247.54 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="225.89" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S4.SS3.p2.7.6.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S4.SS3.p2.7.6.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1" class="ltx_float ltx_lstlisting">
<span id="S4.SS3.p2.7.6.5.4.3.2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.tab1.1" class="ltx_listing ltx_lstlisting ltx_listing">
</span>
</span></span></foreignObject></g></g></svg>This method simply returns the length of the input string as an integer value, which is then cast to a long. The str.Length property provides access to the number of characters in the string.

<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The rapid expansion of machine learning and AI in the field of programming has necessitated the urgent need for language-specific LLMs. Our contribution through Safurai-Csharp is an experimental testament to the capabilities of such models in better understanding and generating code in a nuanced programming language like C#. Boasting a remarkable score in the Manual MultiPL-E benchmark, Safurai-Csharp underscores the impressive advances that can be achieved with dedicated tools and strategies like the Evol Instruct technique for dataset enhancement and diversification.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Furthermore, through the fine-tuning of the advanced foundational model, CodeLlama, Safurai-Csharp has exemplified how performance strides can be made in large language models, potentially serving as a blueprint for future endeavors focusing on other languages. The open-source nature of Safurai-Csharp holds promises for broader inclusivity, accessibility, and further contributions from the global community, aligning well with the collaborative spirit of AI research and development.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The richness and versatility of Safurai-Csharp’s capabilities, demonstrated in code completion, debugging, and generation for the C# programming language, makes it a compelling tool that could revolutionize developers’ workflows and the journey of code learners. We strongly believe that this open-source initiative is a monumental stride in language-specific LLMs, bringing us closer to real-world applicability and integration of AI in coding.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">We envisage the continued evolution and refinement of such specialized coding LLMs and eagerly anticipate the strides the research community will make in this pursuit. Through Safurai-Csharp, we have showcased a step forward in this arena; however, the road ahead is long and full of fascinating possibilities and challenges.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">If you are interested in trying our model, here are the links to our Hugging Face page:

<br class="ltx_break">Normal Version: <a target="_blank" href="https://huggingface.co/Safurai/Safurai-Csharp-34B" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/Safurai/Safurai-Csharp-34B</a>

<br class="ltx_break">GGUF Quantization: <a target="_blank" href="https://huggingface.co/Safurai/Safurai-Csharp-34B-GGUF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/Safurai/Safurai-Csharp-34B-GGUF</a>

<br class="ltx_break">AWQ Quantization: <a target="_blank" href="https://huggingface.co/Safurai/Safurai-Csharp-34B-AWQ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/Safurai/Safurai-Csharp-34B-AWQ</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Loubna Ben Allal et al.
</span>
<span class="ltx_bibblock">“SantaCoder: don’t reach for the stars!”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">arXiv:2301.03988</em>, 2023
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Yekun Chai et al.
</span>
<span class="ltx_bibblock">“ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">arXiv:2212.06742</em>, 2022
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Shubham Chandel, Colin B. Clement, Guillermo Serrato and Neel Sundaresan
</span>
<span class="ltx_bibblock">“Training and Evaluating a Jupyter Notebook Data Science Assistant”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">arXiv:2201.12901</em>, 2022
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Aakanksha Chowdhery et al.
</span>
<span class="ltx_bibblock">“PaLM: Scaling Language Modeling with Pathways”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">arXiv:2204.02311</em>, 2022
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Tim Dettmers, Artidoro Pagnoni, Ari Holtzman and Luke Zettlemoyer
</span>
<span class="ltx_bibblock">“QLoRA: Efficient Finetuning of Quantized LLMs”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.14314v1</em>, 2023
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Daniel Fried et al.
</span>
<span class="ltx_bibblock">“InCoder: A Generative Model for Code Infilling and Synthesis”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">arXiv:2204.05999</em>, 2022
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Suriya Gunasekar et al.
</span>
<span class="ltx_bibblock">“Textbooks Are All You Need”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.11644</em>, 2023
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Edward J. Hu et al.
</span>
<span class="ltx_bibblock">“LoRA: Low-Rank Adaptation of Large Language Models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">arXiv:2106.09685</em>, 2021
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Albert Q. Jiang et al.
</span>
<span class="ltx_bibblock">“Mistral 7B”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">arXiv:2310.06825</em>, 2023
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Raymond Li et al.
</span>
<span class="ltx_bibblock">“StarCoder: may the source be with you!”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.06161</em>, 2023
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Yuanzhi Li et al.
</span>
<span class="ltx_bibblock">“Textbooks Are All You Need II: phi-1.5 technical report”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">arXiv:2309.05463</em>, 2023
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Yujia Li et al.
</span>
<span class="ltx_bibblock">“Competition-Level Code Generation with AlphaCode”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">arXiv:2203.07814</em>, 2022
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“Gpt-4 technical report”, 2023
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Bo Shen et al.
</span>
<span class="ltx_bibblock">“PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">arXiv:2307.14936</em>, 2023
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Hugo Touvron et al.
</span>
<span class="ltx_bibblock">“Llama 2: Open Foundation and Fine-Tuned Chat Models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">arXiv:2307.09288</em>, 2023
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">BigScience Workshop
</span>
<span class="ltx_bibblock">“BLOOM: A 176B-Parameter Open-Access Multilingual Language Model”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">arXiv:2211.05100</em>, 2022
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Can Xu et al.
</span>
<span class="ltx_bibblock">“WizardLM: Empowering Large Language Models to Follow Complex Instructions”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.12244</em>, 2023
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Daoguang Zan et al.
</span>
<span class="ltx_bibblock">“When Language Model Meets Private Library”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">arXiv:2210.17236</em>, 2022
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Qinkai Zheng et al.
</span>
<span class="ltx_bibblock">“CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">arXiv:2303.17568</em>, 2023
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</section></code></code></code></code></code></code></code>
</div>
</section>
</section></code></code>
</div>
</section>
</section>
</section></code>
</div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.03242" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.03243" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.03243">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.03243" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.03244" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 20:24:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
