<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations</title>
<!--Generated on Thu Oct  3 17:36:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02762v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S1" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S2" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S2.SS1" title="In 2 Related work ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Interpreting Latent Representations in Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S2.SS2" title="In 2 Related work ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Interpreting latent representations in Vision Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S2.SS3" title="In 2 Related work ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Detecting and reducing VLM hallucinations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Extracting Knowledge from VLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.SS1" title="In 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.SS2" title="In 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Applying Logit Lens on VLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Erasing knowledge from VLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS1" title="In 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Erasing objects from image representations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS1.SSS1" title="In 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Removing objects one by one</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS1.SSS2" title="In 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Mass-removing objects</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS2" title="In 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Ablation Study: mass-removing hallucinations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS1" title="In 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Hallucination Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS2" title="In 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Hallucination Removal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS3" title="In 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Zero-shot Segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S6" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and limitations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S6.SS1" title="In 6 Discussion and limitations ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Acknowledgments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1" title="In Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.SS1" title="In Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Mass-removing objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.SS2" title="In Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Ablations for InstructBLIP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.SS3" title="In Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Hallucination Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.SS4" title="In Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Hallucination Reduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.SS5" title="In Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Object Localization</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Interpreting and Editing Vision-Language 
<br class="ltx_break"/>Representations to Mitigate Hallucinations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nick Jiang  , Anish Kachinthaya<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>  , Suzie Petyrk  ,Yossi Gandelsman<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>
<br class="ltx_break"/>University of California, Berkeley
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{nickj,anishk,spetryk,yossi_gandelsman}@berkeley.edu</span>
</span><span class="ltx_author_notes">Equal contribution as first authors.Equal contribution as last authors.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs’ internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model’s latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs’ latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nickjiang2378/vl-interp" title="">https://github.com/nickjiang2378/vl-interp</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Vision-Language Models (VLMs) have recently emerged as powerful tools for understanding images via text <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib14" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib32" title="">2024</a>)</cite>. They have demonstrated remarkable capabilities across multimodal tasks such as image captioning <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib28" title="">2023a</a>)</cite>, visual question answering <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib48" title="">2023</a>)</cite>,
and complex multimodal reasoning <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib2" title="">2023</a>)</cite>. Despite their capabilities, VLMs tend to hallucinate content that does not appear in the images <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib26" title="">2023</a>)</cite>, which poses serious concerns for the reliability of these models in real-world applications <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib23" title="">2023</a>; Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib33" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Widespread belief has been that scaling to larger models and more training data will naturally mitigate hallucinations. However, recent studies have shown that hallucinations persist even in larger and more advanced models <cite class="ltx_cite ltx_citemacro_citep">(Rohrbach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib40" title="">2019</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib30" title="">2023b</a>)</cite>, suggesting that this issue cannot be solved by scale alone. Current methods reduce hallucinations by applying external interventions (e.g. object detectors; <cite class="ltx_cite ltx_citemacro_cite">Yin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib49" title="">2023</a>)</cite>) or additional model fine-tuning (e.g. on hallucination examples; <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib52" title="">2024</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib50" title="">2024a</a>)</cite>).
Nevertheless, these methods often struggle to distinguish between subtle hallucinations and existing details, requiring new models or updated model parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we aim to introduce fine-grained edits directly to the image latent representations of VLMs to reduce hallucinations without hindering their performance, an approach that has had some success in large language models  <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib51" title="">2024b</a>; von Rutte et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib47" title="">2024</a>)</cite>. To edit the latent representations of VLMs, we first explain their role via text. We employ the logit lens technique <cite class="ltx_cite ltx_citemacro_citep">(nostalgebraist, <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib35" title="">2020</a>)</cite> to directly interpret the spatial VLM <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">image</span> representations with VLM <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">text vocabulary</span>. Surprisingly, the characteristics of these image representations are different for real objects that appear in the image and objects that are hallucinated. Moreover, the logit lens enables spatially localizing objects within the input image.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Relying on the ability to detect hallucinated objects, we edit them out by intervening in their internal representations. We introduce a knowledge erasure algorithm, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">ProjectAway</span>, to target and remove objects by linearly orthogonalizing image features with respect to the text features of target objects. We find that <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.2">ProjectAway</span> can erase both real and hallucinated objects with high rates of removal.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S1.F1.g1" src="x1.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Interpreting VLM internal image representations</span>. (a) Given a VLM, (b) we unembed the latent representations from image embeddings to the vocabulary and classify hallucinations. We remove hallucinations by (c) linearly editing them out of the latent representations.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We use our interpretation and editing approach for three tasks. First, we utilize the logit lens on image features to detect hallucinations in the image. We find that it improves mAP by 22.45% and 47.17% in two VLMs. Then, we combine our editing and detection method to erase hallucinations from the VLM’s internal representations, reducing hallucinations up to 25.7% on standard benchmarks, while preserving accuracy in image captioning. Finally, we use the logit lens to localize objects in the image features. We find that our spatial mapping provides comparable performance to state-of-the-art zero-shot segmentation methods. Our results indicate that understanding the internal representations of VLMs can be achieved and used to repair model hallucinations and introduce new capabilities.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Interpreting Latent Representations in Language Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Interpreting the inner workings of large language models enables fine-grained improvement of the language model behavior. Recent work involves utilizing the model’s attention maps <cite class="ltx_cite ltx_citemacro_citep">(Kobayashi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib27" title="">2020</a>; Chefer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib8" title="">2021</a>)</cite>, activation patterns <cite class="ltx_cite ltx_citemacro_citep">(Conmy et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib11" title="">2023</a>; Meng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib34" title="">2023</a>; Bronzini et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib7" title="">2024</a>)</cite>, and latent representations <cite class="ltx_cite ltx_citemacro_citep">(Ghandeharioun et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib19" title="">2024</a>; Cunningham et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib12" title="">2023</a>; Bricken et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib6" title="">2023</a>)</cite> to understand their behavior with applications such as early exiting <cite class="ltx_cite ltx_citemacro_citep">(Halawi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib20" title="">2024</a>)</cite> and editing or erasing the model’s knowledge <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib13" title="">2022</a>; Ravfogel et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib39" title="">2024</a>)</cite>. One class of methods probe the VLMs knowledge with linear classifiers <cite class="ltx_cite ltx_citemacro_citep">(Hewitt &amp; Manning, <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib22" title="">2019</a>; Tucker et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib45" title="">2021</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib29" title="">2024</a>; Belrose et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib5" title="">2023</a>)</cite>. The logit lens method <cite class="ltx_cite ltx_citemacro_citep">(nostalgebraist, <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib35" title="">2020</a>)</cite>, which we will use in our analysis, finds the output distribution over the vocabulary of the language model at intermediate layers with the model’s own unembedding matrix. We apply this approach to <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">VLMs</span> to interpret the model’s understanding of <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">visual information</span> in the model’s textual vocabulary.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Interpreting latent representations in Vision Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Understanding the internal dynamics of vision models is critical for ensuring safety and reliability in multimodal systems. Early works in this area focused on producing saliency maps <cite class="ltx_cite ltx_citemacro_citep">(Petsiuk et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib37" title="">2018</a>)</cite>, analyzing individual neurons <cite class="ltx_cite ltx_citemacro_citep">(Bau et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib4" title="">2020</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib3" title="">2019</a>; Dravid et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib15" title="">2023</a>)</cite>, and training networks to map latent representations to concepts <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib16" title="">2020</a>)</cite>. With the emergence of transformer-based vision models like CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib38" title="">2021</a>)</cite>, recent methods explain latent tokens  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib10" title="">2023</a>)</cite> and the roles of attention heads and neurons with natural language <cite class="ltx_cite ltx_citemacro_citep">(Gandelsman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib18" title="">2024b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib17" title="">a</a>; Shaham et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib42" title="">2024</a>)</cite>. Few works currently interpret the internal computation of VLMs: <cite class="ltx_cite ltx_citemacro_cite">Palit et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib36" title="">2023</a>)</cite> develop a neuron causal tracing tool; <cite class="ltx_cite ltx_citemacro_cite">Schwettmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib41" title="">2023</a>)</cite> identifies multi-modal neurons; and <cite class="ltx_cite ltx_citemacro_cite">Huo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib25" title="">2024</a>)</cite> ablates domain-specific neurons to improve vision question-answering. Whereas past papers have primarily studied the mechanisms (e.g. neuron analysis) that drive VLMs, we focus on interpreting and editing their latent representations for real-world applicability.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Detecting and reducing VLM hallucinations</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">While VLM performances on image caption and visual question answering are continually improving, they continue to hallucinate facts that are not supported by the visual input. Existing methods for detecting hallucinations in language models during inference utilize latent representations <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib21" title="">2024</a>; Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib44" title="">2024</a>)</cite>, activations <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib9" title="">2024</a>)</cite>, and output logit values <cite class="ltx_cite ltx_citemacro_citep">(Varshney et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib46" title="">2023</a>)</cite>. SAPLMA <cite class="ltx_cite ltx_citemacro_citep">(Azaria &amp; Mitchell, <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib1" title="">2023</a>)</cite> trains a hallucination classifier on the internal latent representations. LUNA <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib43" title="">2024</a>)</cite> learns a transition function on latent representations and identifies abnormal transitions. <cite class="ltx_cite ltx_citemacro_cite">Varshney et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib46" title="">2023</a>)</cite> uses the final layer logits to score the model’s confidence in an entity or keyword and intervenes by instructing the model to either repair or remove the hallucinated information. Among VLMs, LURE <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib52" title="">2024</a>)</cite> is a fine-tuned revisor model to detect and reduce hallucinations. OPERA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib24" title="">2024</a>)</cite> uses the model’s internal attention weights to detect and suppress patterns that align with the beginning of hallucinated phrases. In contrast to these methods, we leverage the internal <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">image</span> representations in the VLMs for hallucination reduction and for zero-shot segmentation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Extracting Knowledge from VLMs</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We start by introducing VLMs and the general framework of their architectures in most recent work. We then describe our approach for decoding the features in intermediate image representations in VLMs into text, and apply it to two types of VLMs. Surprisingly, this approach effectively probes the knowledge about objects present in images and can localize objects within the image.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Vision-Language Models.</span> The architecture of recent state-of-the-art VLMs for text generation typically involves three main components: a vision encoder to process image inputs, a mapping network to map image features to image embeddings, and an autoregressive language model to process the image embeddings and prompt embeddings to generate text. We focus on two recent state-of-the-art VLMs: <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">LLaVA</span> 1.5 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib32" title="">2024</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">InstructBLIP</span> <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib14" title="">2023</a>)</cite>. We use 7B versions of both these models. LLaVA utilizes a frozen CLIP vision encoder and an MLP as a mapping network to project the vision encoder outputs into image embeddings for the language model. The MLP is pre-trained on a large vision-language dataset and both the MLP and the language model are fine-tuned on an instruction-focused dataset. In contrast, InstructBLIP freezes both the vision encoder and the language model and only trains the mapping network.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.16"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.16.1">Notations.</span> For the purposes of our work, we define the VLM architecture as follows. The vision encoder processes an input image to produce <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_n</annotation></semantics></math> image features. These image features are projected to embedding space via the mapping network, resulting in <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_n</annotation></semantics></math> <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_d</annotation></semantics></math>-dimensional image embeddings <math alttext="\{k_{i}:k_{i}\in\mathbb{R}^{d},i=1,...,n\}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.5"><semantics id="S3.SS1.p2.4.m4.5a"><mrow id="S3.SS1.p2.4.m4.5.5.2" xref="S3.SS1.p2.4.m4.5.5.3.cmml"><mo id="S3.SS1.p2.4.m4.5.5.2.3" stretchy="false" xref="S3.SS1.p2.4.m4.5.5.3.1.cmml">{</mo><msub id="S3.SS1.p2.4.m4.4.4.1.1" xref="S3.SS1.p2.4.m4.4.4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.4.4.1.1.2" xref="S3.SS1.p2.4.m4.4.4.1.1.2.cmml">k</mi><mi id="S3.SS1.p2.4.m4.4.4.1.1.3" xref="S3.SS1.p2.4.m4.4.4.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.5.5.2.4" lspace="0.278em" rspace="0.278em" xref="S3.SS1.p2.4.m4.5.5.3.1.cmml">:</mo><mrow id="S3.SS1.p2.4.m4.5.5.2.2.2" xref="S3.SS1.p2.4.m4.5.5.2.2.3.cmml"><mrow id="S3.SS1.p2.4.m4.5.5.2.2.1.1" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.cmml"><msub id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.2" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.2.cmml">k</mi><mi id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.3" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.5.5.2.2.1.1.1" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.2" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.3" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.3.cmml">d</mi></msup></mrow><mo id="S3.SS1.p2.4.m4.5.5.2.2.2.3" xref="S3.SS1.p2.4.m4.5.5.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p2.4.m4.5.5.2.2.2.2" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.cmml"><mi id="S3.SS1.p2.4.m4.5.5.2.2.2.2.2" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.2.cmml">i</mi><mo id="S3.SS1.p2.4.m4.5.5.2.2.2.2.1" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.1.cmml">=</mo><mrow id="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.2" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.1.cmml"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">1</mn><mo id="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.2.1" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.2.2" mathvariant="normal" xref="S3.SS1.p2.4.m4.2.2.cmml">…</mi><mo id="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.2.2" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.3.3" xref="S3.SS1.p2.4.m4.3.3.cmml">n</mi></mrow></mrow></mrow><mo id="S3.SS1.p2.4.m4.5.5.2.5" stretchy="false" xref="S3.SS1.p2.4.m4.5.5.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.5b"><apply id="S3.SS1.p2.4.m4.5.5.3.cmml" xref="S3.SS1.p2.4.m4.5.5.2"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.5.5.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.3">conditional-set</csymbol><apply id="S3.SS1.p2.4.m4.4.4.1.1.cmml" xref="S3.SS1.p2.4.m4.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.4.4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.4.4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.4.4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.4.4.1.1.2">𝑘</ci><ci id="S3.SS1.p2.4.m4.4.4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.4.4.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.4.m4.5.5.2.2.3.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.2.2.3a.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.4.m4.5.5.2.2.1.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1"><in id="S3.SS1.p2.4.m4.5.5.2.2.1.1.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.1"></in><apply id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.2">𝑘</ci><ci id="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.2">ℝ</ci><ci id="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.1.1.3.3">𝑑</ci></apply></apply><apply id="S3.SS1.p2.4.m4.5.5.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2"><eq id="S3.SS1.p2.4.m4.5.5.2.2.2.2.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.1"></eq><ci id="S3.SS1.p2.4.m4.5.5.2.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.2">𝑖</ci><list id="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.2.2.2.2.3.2"><cn id="S3.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p2.4.m4.1.1">1</cn><ci id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2">…</ci><ci id="S3.SS1.p2.4.m4.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3">𝑛</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.5c">\{k_{i}:k_{i}\in\mathbb{R}^{d},i=1,...,n\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.5d">{ italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_i = 1 , … , italic_n }</annotation></semantics></math>. For the language model, the entire set of text tokens constitutes the vocabulary <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_V</annotation></semantics></math> with vocabulary size <math alttext="|V|" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.2.2" xref="S3.SS1.p2.6.m6.1.2.1.cmml"><mo id="S3.SS1.p2.6.m6.1.2.2.1" stretchy="false" xref="S3.SS1.p2.6.m6.1.2.1.1.cmml">|</mo><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">V</mi><mo id="S3.SS1.p2.6.m6.1.2.2.2" stretchy="false" xref="S3.SS1.p2.6.m6.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.2.1.cmml" xref="S3.SS1.p2.6.m6.1.2.2"><abs id="S3.SS1.p2.6.m6.1.2.1.1.cmml" xref="S3.SS1.p2.6.m6.1.2.2.1"></abs><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">|V|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">| italic_V |</annotation></semantics></math>. The image embeddings, followed by <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_m</annotation></semantics></math> text embeddings <math alttext="\{t_{i}:t_{i}\in\mathbb{R}^{d},i=1,...,m\}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.5"><semantics id="S3.SS1.p2.8.m8.5a"><mrow id="S3.SS1.p2.8.m8.5.5.2" xref="S3.SS1.p2.8.m8.5.5.3.cmml"><mo id="S3.SS1.p2.8.m8.5.5.2.3" stretchy="false" xref="S3.SS1.p2.8.m8.5.5.3.1.cmml">{</mo><msub id="S3.SS1.p2.8.m8.4.4.1.1" xref="S3.SS1.p2.8.m8.4.4.1.1.cmml"><mi id="S3.SS1.p2.8.m8.4.4.1.1.2" xref="S3.SS1.p2.8.m8.4.4.1.1.2.cmml">t</mi><mi id="S3.SS1.p2.8.m8.4.4.1.1.3" xref="S3.SS1.p2.8.m8.4.4.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.8.m8.5.5.2.4" lspace="0.278em" rspace="0.278em" xref="S3.SS1.p2.8.m8.5.5.3.1.cmml">:</mo><mrow id="S3.SS1.p2.8.m8.5.5.2.2.2" xref="S3.SS1.p2.8.m8.5.5.2.2.3.cmml"><mrow id="S3.SS1.p2.8.m8.5.5.2.2.1.1" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.cmml"><msub id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.cmml"><mi id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.2" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.2.cmml">t</mi><mi id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.3" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p2.8.m8.5.5.2.2.1.1.1" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.2" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.3" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.3.cmml">d</mi></msup></mrow><mo id="S3.SS1.p2.8.m8.5.5.2.2.2.3" xref="S3.SS1.p2.8.m8.5.5.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p2.8.m8.5.5.2.2.2.2" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.cmml"><mi id="S3.SS1.p2.8.m8.5.5.2.2.2.2.2" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.2.cmml">i</mi><mo id="S3.SS1.p2.8.m8.5.5.2.2.2.2.1" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.1.cmml">=</mo><mrow id="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.2" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.1.cmml"><mn id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">1</mn><mo id="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.2.1" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.1.cmml">,</mo><mi id="S3.SS1.p2.8.m8.2.2" mathvariant="normal" xref="S3.SS1.p2.8.m8.2.2.cmml">…</mi><mo id="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.2.2" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.1.cmml">,</mo><mi id="S3.SS1.p2.8.m8.3.3" xref="S3.SS1.p2.8.m8.3.3.cmml">m</mi></mrow></mrow></mrow><mo id="S3.SS1.p2.8.m8.5.5.2.5" stretchy="false" xref="S3.SS1.p2.8.m8.5.5.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.5b"><apply id="S3.SS1.p2.8.m8.5.5.3.cmml" xref="S3.SS1.p2.8.m8.5.5.2"><csymbol cd="latexml" id="S3.SS1.p2.8.m8.5.5.3.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.3">conditional-set</csymbol><apply id="S3.SS1.p2.8.m8.4.4.1.1.cmml" xref="S3.SS1.p2.8.m8.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.4.4.1.1.1.cmml" xref="S3.SS1.p2.8.m8.4.4.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.4.4.1.1.2.cmml" xref="S3.SS1.p2.8.m8.4.4.1.1.2">𝑡</ci><ci id="S3.SS1.p2.8.m8.4.4.1.1.3.cmml" xref="S3.SS1.p2.8.m8.4.4.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.8.m8.5.5.2.2.3.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.5.5.2.2.3a.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.8.m8.5.5.2.2.1.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1"><in id="S3.SS1.p2.8.m8.5.5.2.2.1.1.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.1"></in><apply id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.2.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.2">𝑡</ci><ci id="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.3.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.2">ℝ</ci><ci id="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.1.1.3.3">𝑑</ci></apply></apply><apply id="S3.SS1.p2.8.m8.5.5.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2"><eq id="S3.SS1.p2.8.m8.5.5.2.2.2.2.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.1"></eq><ci id="S3.SS1.p2.8.m8.5.5.2.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.2">𝑖</ci><list id="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.1.cmml" xref="S3.SS1.p2.8.m8.5.5.2.2.2.2.3.2"><cn id="S3.SS1.p2.8.m8.1.1.cmml" type="integer" xref="S3.SS1.p2.8.m8.1.1">1</cn><ci id="S3.SS1.p2.8.m8.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2">…</ci><ci id="S3.SS1.p2.8.m8.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3">𝑚</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.5c">\{t_{i}:t_{i}\in\mathbb{R}^{d},i=1,...,m\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.5d">{ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_i = 1 , … , italic_m }</annotation></semantics></math> of the prompt tokens, are input to the language model through <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.1"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.1d">italic_L</annotation></semantics></math> decoder layers. For an input embedding <math alttext="x\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m10.1"><semantics id="S3.SS1.p2.10.m10.1a"><mrow id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.2" xref="S3.SS1.p2.10.m10.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.10.m10.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml"><mi id="S3.SS1.p2.10.m10.1.1.3.2" xref="S3.SS1.p2.10.m10.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p2.10.m10.1.1.3.3" xref="S3.SS1.p2.10.m10.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><in id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1"></in><ci id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2">𝑥</ci><apply id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.3.1.cmml" xref="S3.SS1.p2.10.m10.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.3.2.cmml" xref="S3.SS1.p2.10.m10.1.1.3.2">ℝ</ci><ci id="S3.SS1.p2.10.m10.1.1.3.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">x\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m10.1d">italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, we define <math alttext="h_{l}(x)\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.p2.11.m11.1"><semantics id="S3.SS1.p2.11.m11.1a"><mrow id="S3.SS1.p2.11.m11.1.2" xref="S3.SS1.p2.11.m11.1.2.cmml"><mrow id="S3.SS1.p2.11.m11.1.2.2" xref="S3.SS1.p2.11.m11.1.2.2.cmml"><msub id="S3.SS1.p2.11.m11.1.2.2.2" xref="S3.SS1.p2.11.m11.1.2.2.2.cmml"><mi id="S3.SS1.p2.11.m11.1.2.2.2.2" xref="S3.SS1.p2.11.m11.1.2.2.2.2.cmml">h</mi><mi id="S3.SS1.p2.11.m11.1.2.2.2.3" xref="S3.SS1.p2.11.m11.1.2.2.2.3.cmml">l</mi></msub><mo id="S3.SS1.p2.11.m11.1.2.2.1" xref="S3.SS1.p2.11.m11.1.2.2.1.cmml">⁢</mo><mrow id="S3.SS1.p2.11.m11.1.2.2.3.2" xref="S3.SS1.p2.11.m11.1.2.2.cmml"><mo id="S3.SS1.p2.11.m11.1.2.2.3.2.1" stretchy="false" xref="S3.SS1.p2.11.m11.1.2.2.cmml">(</mo><mi id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml">x</mi><mo id="S3.SS1.p2.11.m11.1.2.2.3.2.2" stretchy="false" xref="S3.SS1.p2.11.m11.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.11.m11.1.2.1" xref="S3.SS1.p2.11.m11.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.11.m11.1.2.3" xref="S3.SS1.p2.11.m11.1.2.3.cmml"><mi id="S3.SS1.p2.11.m11.1.2.3.2" xref="S3.SS1.p2.11.m11.1.2.3.2.cmml">ℝ</mi><mi id="S3.SS1.p2.11.m11.1.2.3.3" xref="S3.SS1.p2.11.m11.1.2.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.2.cmml" xref="S3.SS1.p2.11.m11.1.2"><in id="S3.SS1.p2.11.m11.1.2.1.cmml" xref="S3.SS1.p2.11.m11.1.2.1"></in><apply id="S3.SS1.p2.11.m11.1.2.2.cmml" xref="S3.SS1.p2.11.m11.1.2.2"><times id="S3.SS1.p2.11.m11.1.2.2.1.cmml" xref="S3.SS1.p2.11.m11.1.2.2.1"></times><apply id="S3.SS1.p2.11.m11.1.2.2.2.cmml" xref="S3.SS1.p2.11.m11.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.2.2.2.1.cmml" xref="S3.SS1.p2.11.m11.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.11.m11.1.2.2.2.2.cmml" xref="S3.SS1.p2.11.m11.1.2.2.2.2">ℎ</ci><ci id="S3.SS1.p2.11.m11.1.2.2.2.3.cmml" xref="S3.SS1.p2.11.m11.1.2.2.2.3">𝑙</ci></apply><ci id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">𝑥</ci></apply><apply id="S3.SS1.p2.11.m11.1.2.3.cmml" xref="S3.SS1.p2.11.m11.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.2.3.1.cmml" xref="S3.SS1.p2.11.m11.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.11.m11.1.2.3.2.cmml" xref="S3.SS1.p2.11.m11.1.2.3.2">ℝ</ci><ci id="S3.SS1.p2.11.m11.1.2.3.3.cmml" xref="S3.SS1.p2.11.m11.1.2.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">h_{l}(x)\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.11.m11.1d">italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_x ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> to be the latent representation for embedding <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p2.12.m12.1"><semantics id="S3.SS1.p2.12.m12.1a"><mi id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><ci id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.12.m12.1d">italic_x</annotation></semantics></math> at layer <math alttext="l\in\{1,...,L\}" class="ltx_Math" display="inline" id="S3.SS1.p2.13.m13.3"><semantics id="S3.SS1.p2.13.m13.3a"><mrow id="S3.SS1.p2.13.m13.3.4" xref="S3.SS1.p2.13.m13.3.4.cmml"><mi id="S3.SS1.p2.13.m13.3.4.2" xref="S3.SS1.p2.13.m13.3.4.2.cmml">l</mi><mo id="S3.SS1.p2.13.m13.3.4.1" xref="S3.SS1.p2.13.m13.3.4.1.cmml">∈</mo><mrow id="S3.SS1.p2.13.m13.3.4.3.2" xref="S3.SS1.p2.13.m13.3.4.3.1.cmml"><mo id="S3.SS1.p2.13.m13.3.4.3.2.1" stretchy="false" xref="S3.SS1.p2.13.m13.3.4.3.1.cmml">{</mo><mn id="S3.SS1.p2.13.m13.1.1" xref="S3.SS1.p2.13.m13.1.1.cmml">1</mn><mo id="S3.SS1.p2.13.m13.3.4.3.2.2" xref="S3.SS1.p2.13.m13.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p2.13.m13.2.2" mathvariant="normal" xref="S3.SS1.p2.13.m13.2.2.cmml">…</mi><mo id="S3.SS1.p2.13.m13.3.4.3.2.3" xref="S3.SS1.p2.13.m13.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p2.13.m13.3.3" xref="S3.SS1.p2.13.m13.3.3.cmml">L</mi><mo id="S3.SS1.p2.13.m13.3.4.3.2.4" stretchy="false" xref="S3.SS1.p2.13.m13.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.3b"><apply id="S3.SS1.p2.13.m13.3.4.cmml" xref="S3.SS1.p2.13.m13.3.4"><in id="S3.SS1.p2.13.m13.3.4.1.cmml" xref="S3.SS1.p2.13.m13.3.4.1"></in><ci id="S3.SS1.p2.13.m13.3.4.2.cmml" xref="S3.SS1.p2.13.m13.3.4.2">𝑙</ci><set id="S3.SS1.p2.13.m13.3.4.3.1.cmml" xref="S3.SS1.p2.13.m13.3.4.3.2"><cn id="S3.SS1.p2.13.m13.1.1.cmml" type="integer" xref="S3.SS1.p2.13.m13.1.1">1</cn><ci id="S3.SS1.p2.13.m13.2.2.cmml" xref="S3.SS1.p2.13.m13.2.2">…</ci><ci id="S3.SS1.p2.13.m13.3.3.cmml" xref="S3.SS1.p2.13.m13.3.3">𝐿</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.3c">l\in\{1,...,L\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.13.m13.3d">italic_l ∈ { 1 , … , italic_L }</annotation></semantics></math>, the output of the decoder layer, which is conditioned on previous tokens of the input sequence. An unembedding matrix <math alttext="W_{U}\in\mathbb{R}^{|V|\times d}" class="ltx_Math" display="inline" id="S3.SS1.p2.14.m14.1"><semantics id="S3.SS1.p2.14.m14.1a"><mrow id="S3.SS1.p2.14.m14.1.2" xref="S3.SS1.p2.14.m14.1.2.cmml"><msub id="S3.SS1.p2.14.m14.1.2.2" xref="S3.SS1.p2.14.m14.1.2.2.cmml"><mi id="S3.SS1.p2.14.m14.1.2.2.2" xref="S3.SS1.p2.14.m14.1.2.2.2.cmml">W</mi><mi id="S3.SS1.p2.14.m14.1.2.2.3" xref="S3.SS1.p2.14.m14.1.2.2.3.cmml">U</mi></msub><mo id="S3.SS1.p2.14.m14.1.2.1" xref="S3.SS1.p2.14.m14.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.14.m14.1.2.3" xref="S3.SS1.p2.14.m14.1.2.3.cmml"><mi id="S3.SS1.p2.14.m14.1.2.3.2" xref="S3.SS1.p2.14.m14.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.14.m14.1.1.1" xref="S3.SS1.p2.14.m14.1.1.1.cmml"><mrow id="S3.SS1.p2.14.m14.1.1.1.3.2" xref="S3.SS1.p2.14.m14.1.1.1.3.1.cmml"><mo id="S3.SS1.p2.14.m14.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p2.14.m14.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p2.14.m14.1.1.1.1" xref="S3.SS1.p2.14.m14.1.1.1.1.cmml">V</mi><mo id="S3.SS1.p2.14.m14.1.1.1.3.2.2" rspace="0.055em" stretchy="false" xref="S3.SS1.p2.14.m14.1.1.1.3.1.1.cmml">|</mo></mrow><mo id="S3.SS1.p2.14.m14.1.1.1.2" rspace="0.222em" xref="S3.SS1.p2.14.m14.1.1.1.2.cmml">×</mo><mi id="S3.SS1.p2.14.m14.1.1.1.4" xref="S3.SS1.p2.14.m14.1.1.1.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.1b"><apply id="S3.SS1.p2.14.m14.1.2.cmml" xref="S3.SS1.p2.14.m14.1.2"><in id="S3.SS1.p2.14.m14.1.2.1.cmml" xref="S3.SS1.p2.14.m14.1.2.1"></in><apply id="S3.SS1.p2.14.m14.1.2.2.cmml" xref="S3.SS1.p2.14.m14.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m14.1.2.2.1.cmml" xref="S3.SS1.p2.14.m14.1.2.2">subscript</csymbol><ci id="S3.SS1.p2.14.m14.1.2.2.2.cmml" xref="S3.SS1.p2.14.m14.1.2.2.2">𝑊</ci><ci id="S3.SS1.p2.14.m14.1.2.2.3.cmml" xref="S3.SS1.p2.14.m14.1.2.2.3">𝑈</ci></apply><apply id="S3.SS1.p2.14.m14.1.2.3.cmml" xref="S3.SS1.p2.14.m14.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m14.1.2.3.1.cmml" xref="S3.SS1.p2.14.m14.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.14.m14.1.2.3.2.cmml" xref="S3.SS1.p2.14.m14.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.14.m14.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1"><times id="S3.SS1.p2.14.m14.1.1.1.2.cmml" xref="S3.SS1.p2.14.m14.1.1.1.2"></times><apply id="S3.SS1.p2.14.m14.1.1.1.3.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1.3.2"><abs id="S3.SS1.p2.14.m14.1.1.1.3.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1.3.2.1"></abs><ci id="S3.SS1.p2.14.m14.1.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1.1">𝑉</ci></apply><ci id="S3.SS1.p2.14.m14.1.1.1.4.cmml" xref="S3.SS1.p2.14.m14.1.1.1.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.1c">W_{U}\in\mathbb{R}^{|V|\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.14.m14.1d">italic_W start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT | italic_V | × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> maps the last latent representation <math alttext="h_{L}(t_{m})" class="ltx_Math" display="inline" id="S3.SS1.p2.15.m15.1"><semantics id="S3.SS1.p2.15.m15.1a"><mrow id="S3.SS1.p2.15.m15.1.1" xref="S3.SS1.p2.15.m15.1.1.cmml"><msub id="S3.SS1.p2.15.m15.1.1.3" xref="S3.SS1.p2.15.m15.1.1.3.cmml"><mi id="S3.SS1.p2.15.m15.1.1.3.2" xref="S3.SS1.p2.15.m15.1.1.3.2.cmml">h</mi><mi id="S3.SS1.p2.15.m15.1.1.3.3" xref="S3.SS1.p2.15.m15.1.1.3.3.cmml">L</mi></msub><mo id="S3.SS1.p2.15.m15.1.1.2" xref="S3.SS1.p2.15.m15.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.15.m15.1.1.1.1" xref="S3.SS1.p2.15.m15.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.15.m15.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.15.m15.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.15.m15.1.1.1.1.1" xref="S3.SS1.p2.15.m15.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.15.m15.1.1.1.1.1.2" xref="S3.SS1.p2.15.m15.1.1.1.1.1.2.cmml">t</mi><mi id="S3.SS1.p2.15.m15.1.1.1.1.1.3" xref="S3.SS1.p2.15.m15.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.SS1.p2.15.m15.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.15.m15.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m15.1b"><apply id="S3.SS1.p2.15.m15.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1"><times id="S3.SS1.p2.15.m15.1.1.2.cmml" xref="S3.SS1.p2.15.m15.1.1.2"></times><apply id="S3.SS1.p2.15.m15.1.1.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m15.1.1.3.1.cmml" xref="S3.SS1.p2.15.m15.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.15.m15.1.1.3.2.cmml" xref="S3.SS1.p2.15.m15.1.1.3.2">ℎ</ci><ci id="S3.SS1.p2.15.m15.1.1.3.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3">𝐿</ci></apply><apply id="S3.SS1.p2.15.m15.1.1.1.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m15.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.15.m15.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.15.m15.1.1.1.1.1.2">𝑡</ci><ci id="S3.SS1.p2.15.m15.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.15.m15.1.1.1.1.1.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m15.1c">h_{L}(t_{m})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.15.m15.1d">italic_h start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math> to a probability distribution over the vocabulary for the next token <math alttext="t_{m+1}" class="ltx_Math" display="inline" id="S3.SS1.p2.16.m16.1"><semantics id="S3.SS1.p2.16.m16.1a"><msub id="S3.SS1.p2.16.m16.1.1" xref="S3.SS1.p2.16.m16.1.1.cmml"><mi id="S3.SS1.p2.16.m16.1.1.2" xref="S3.SS1.p2.16.m16.1.1.2.cmml">t</mi><mrow id="S3.SS1.p2.16.m16.1.1.3" xref="S3.SS1.p2.16.m16.1.1.3.cmml"><mi id="S3.SS1.p2.16.m16.1.1.3.2" xref="S3.SS1.p2.16.m16.1.1.3.2.cmml">m</mi><mo id="S3.SS1.p2.16.m16.1.1.3.1" xref="S3.SS1.p2.16.m16.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p2.16.m16.1.1.3.3" xref="S3.SS1.p2.16.m16.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m16.1b"><apply id="S3.SS1.p2.16.m16.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.16.m16.1.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1">subscript</csymbol><ci id="S3.SS1.p2.16.m16.1.1.2.cmml" xref="S3.SS1.p2.16.m16.1.1.2">𝑡</ci><apply id="S3.SS1.p2.16.m16.1.1.3.cmml" xref="S3.SS1.p2.16.m16.1.1.3"><plus id="S3.SS1.p2.16.m16.1.1.3.1.cmml" xref="S3.SS1.p2.16.m16.1.1.3.1"></plus><ci id="S3.SS1.p2.16.m16.1.1.3.2.cmml" xref="S3.SS1.p2.16.m16.1.1.3.2">𝑚</ci><cn id="S3.SS1.p2.16.m16.1.1.3.3.cmml" type="integer" xref="S3.SS1.p2.16.m16.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m16.1c">t_{m+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.16.m16.1d">italic_t start_POSTSUBSCRIPT italic_m + 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.3.1">Logit Lens.</span> Logit Lens is an interpretability method for intermediate language model representations introduced in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S2.SS1" title="2.1 Interpreting Latent Representations in Language Models ‣ 2 Related work ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>. The logit lens technique applies the unembedding matrix <math alttext="W_{U}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑊</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">W_{U}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT</annotation></semantics></math> to latent representations <math alttext="h_{l}(x)" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><msub id="S3.SS1.p3.2.m2.1.2.2" xref="S3.SS1.p3.2.m2.1.2.2.cmml"><mi id="S3.SS1.p3.2.m2.1.2.2.2" xref="S3.SS1.p3.2.m2.1.2.2.2.cmml">h</mi><mi id="S3.SS1.p3.2.m2.1.2.2.3" xref="S3.SS1.p3.2.m2.1.2.2.3.cmml">l</mi></msub><mo id="S3.SS1.p3.2.m2.1.2.1" xref="S3.SS1.p3.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS1.p3.2.m2.1.2.3.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mo id="S3.SS1.p3.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS1.p3.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">x</mi><mo id="S3.SS1.p3.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS1.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><times id="S3.SS1.p3.2.m2.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2.1"></times><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2">ℎ</ci><ci id="S3.SS1.p3.2.m2.1.2.2.3.cmml" xref="S3.SS1.p3.2.m2.1.2.2.3">𝑙</ci></apply><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">h_{l}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math> in the <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_L</annotation></semantics></math> intermediate layers in the language model to retrieve the logit distributions over the vocabulary.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{l}(t_{m})=W_{U}\cdot h_{l}(t_{m})=[\text{logit}_{1},\text{logit}_{2},\text{%
logit}_{3},\ldots,\text{logit}_{|V|}]" class="ltx_Math" display="block" id="S3.E1.m1.8"><semantics id="S3.E1.m1.8a"><mrow id="S3.E1.m1.8.8" xref="S3.E1.m1.8.8.cmml"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><msub id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml"><mi id="S3.E1.m1.3.3.1.3.2" xref="S3.E1.m1.3.3.1.3.2.cmml">f</mi><mi id="S3.E1.m1.3.3.1.3.3" xref="S3.E1.m1.3.3.1.3.3.cmml">l</mi></msub><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.8.8.8" xref="S3.E1.m1.8.8.8.cmml">=</mo><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><mrow id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml"><msub id="S3.E1.m1.4.4.2.3.2" xref="S3.E1.m1.4.4.2.3.2.cmml"><mi id="S3.E1.m1.4.4.2.3.2.2" xref="S3.E1.m1.4.4.2.3.2.2.cmml">W</mi><mi id="S3.E1.m1.4.4.2.3.2.3" xref="S3.E1.m1.4.4.2.3.2.3.cmml">U</mi></msub><mo id="S3.E1.m1.4.4.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.4.4.2.3.1.cmml">⋅</mo><msub id="S3.E1.m1.4.4.2.3.3" xref="S3.E1.m1.4.4.2.3.3.cmml"><mi id="S3.E1.m1.4.4.2.3.3.2" xref="S3.E1.m1.4.4.2.3.3.2.cmml">h</mi><mi id="S3.E1.m1.4.4.2.3.3.3" xref="S3.E1.m1.4.4.2.3.3.3.cmml">l</mi></msub></mrow><mo id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.2.1.1" xref="S3.E1.m1.4.4.2.1.1.1.cmml"><mo id="S3.E1.m1.4.4.2.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.4.4.2.1.1.1" xref="S3.E1.m1.4.4.2.1.1.1.cmml"><mi id="S3.E1.m1.4.4.2.1.1.1.2" xref="S3.E1.m1.4.4.2.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.4.4.2.1.1.1.3" xref="S3.E1.m1.4.4.2.1.1.1.3.cmml">m</mi></msub><mo id="S3.E1.m1.4.4.2.1.1.3" stretchy="false" xref="S3.E1.m1.4.4.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.8.8.9" xref="S3.E1.m1.8.8.9.cmml">=</mo><mrow id="S3.E1.m1.8.8.6.4" xref="S3.E1.m1.8.8.6.5.cmml"><mo id="S3.E1.m1.8.8.6.4.5" stretchy="false" xref="S3.E1.m1.8.8.6.5.cmml">[</mo><msub id="S3.E1.m1.5.5.3.1.1" xref="S3.E1.m1.5.5.3.1.1.cmml"><mtext id="S3.E1.m1.5.5.3.1.1.2" xref="S3.E1.m1.5.5.3.1.1.2a.cmml">logit</mtext><mn id="S3.E1.m1.5.5.3.1.1.3" xref="S3.E1.m1.5.5.3.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.8.8.6.4.6" xref="S3.E1.m1.8.8.6.5.cmml">,</mo><msub id="S3.E1.m1.6.6.4.2.2" xref="S3.E1.m1.6.6.4.2.2.cmml"><mtext id="S3.E1.m1.6.6.4.2.2.2" xref="S3.E1.m1.6.6.4.2.2.2a.cmml">logit</mtext><mn id="S3.E1.m1.6.6.4.2.2.3" xref="S3.E1.m1.6.6.4.2.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.8.8.6.4.7" xref="S3.E1.m1.8.8.6.5.cmml">,</mo><msub id="S3.E1.m1.7.7.5.3.3" xref="S3.E1.m1.7.7.5.3.3.cmml"><mtext id="S3.E1.m1.7.7.5.3.3.2" xref="S3.E1.m1.7.7.5.3.3.2a.cmml">logit</mtext><mn id="S3.E1.m1.7.7.5.3.3.3" xref="S3.E1.m1.7.7.5.3.3.3.cmml">3</mn></msub><mo id="S3.E1.m1.8.8.6.4.8" xref="S3.E1.m1.8.8.6.5.cmml">,</mo><mi id="S3.E1.m1.2.2" mathvariant="normal" xref="S3.E1.m1.2.2.cmml">…</mi><mo id="S3.E1.m1.8.8.6.4.9" xref="S3.E1.m1.8.8.6.5.cmml">,</mo><msub id="S3.E1.m1.8.8.6.4.4" xref="S3.E1.m1.8.8.6.4.4.cmml"><mtext id="S3.E1.m1.8.8.6.4.4.2" xref="S3.E1.m1.8.8.6.4.4.2a.cmml">logit</mtext><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">V</mi><mo id="S3.E1.m1.1.1.1.3.2" stretchy="false" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></msub><mo id="S3.E1.m1.8.8.6.4.10" stretchy="false" xref="S3.E1.m1.8.8.6.5.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.8b"><apply id="S3.E1.m1.8.8.cmml" xref="S3.E1.m1.8.8"><and id="S3.E1.m1.8.8a.cmml" xref="S3.E1.m1.8.8"></and><apply id="S3.E1.m1.8.8b.cmml" xref="S3.E1.m1.8.8"><eq id="S3.E1.m1.8.8.8.cmml" xref="S3.E1.m1.8.8.8"></eq><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><apply id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.3.1.cmml" xref="S3.E1.m1.3.3.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.3.2.cmml" xref="S3.E1.m1.3.3.1.3.2">𝑓</ci><ci id="S3.E1.m1.3.3.1.3.3.cmml" xref="S3.E1.m1.3.3.1.3.3">𝑙</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">𝑡</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">𝑚</ci></apply></apply><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><times id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2"></times><apply id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3"><ci id="S3.E1.m1.4.4.2.3.1.cmml" xref="S3.E1.m1.4.4.2.3.1">⋅</ci><apply id="S3.E1.m1.4.4.2.3.2.cmml" xref="S3.E1.m1.4.4.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.3.2.1.cmml" xref="S3.E1.m1.4.4.2.3.2">subscript</csymbol><ci id="S3.E1.m1.4.4.2.3.2.2.cmml" xref="S3.E1.m1.4.4.2.3.2.2">𝑊</ci><ci id="S3.E1.m1.4.4.2.3.2.3.cmml" xref="S3.E1.m1.4.4.2.3.2.3">𝑈</ci></apply><apply id="S3.E1.m1.4.4.2.3.3.cmml" xref="S3.E1.m1.4.4.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.3.3.1.cmml" xref="S3.E1.m1.4.4.2.3.3">subscript</csymbol><ci id="S3.E1.m1.4.4.2.3.3.2.cmml" xref="S3.E1.m1.4.4.2.3.3.2">ℎ</ci><ci id="S3.E1.m1.4.4.2.3.3.3.cmml" xref="S3.E1.m1.4.4.2.3.3.3">𝑙</ci></apply></apply><apply id="S3.E1.m1.4.4.2.1.1.1.cmml" xref="S3.E1.m1.4.4.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.1.1.1.1.cmml" xref="S3.E1.m1.4.4.2.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.2.1.1.1.2.cmml" xref="S3.E1.m1.4.4.2.1.1.1.2">𝑡</ci><ci id="S3.E1.m1.4.4.2.1.1.1.3.cmml" xref="S3.E1.m1.4.4.2.1.1.1.3">𝑚</ci></apply></apply></apply><apply id="S3.E1.m1.8.8c.cmml" xref="S3.E1.m1.8.8"><eq id="S3.E1.m1.8.8.9.cmml" xref="S3.E1.m1.8.8.9"></eq><share href="https://arxiv.org/html/2410.02762v1#S3.E1.m1.4.4.2.cmml" id="S3.E1.m1.8.8d.cmml" xref="S3.E1.m1.8.8"></share><list id="S3.E1.m1.8.8.6.5.cmml" xref="S3.E1.m1.8.8.6.4"><apply id="S3.E1.m1.5.5.3.1.1.cmml" xref="S3.E1.m1.5.5.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.3.1.1.1.cmml" xref="S3.E1.m1.5.5.3.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.3.1.1.2a.cmml" xref="S3.E1.m1.5.5.3.1.1.2"><mtext id="S3.E1.m1.5.5.3.1.1.2.cmml" xref="S3.E1.m1.5.5.3.1.1.2">logit</mtext></ci><cn id="S3.E1.m1.5.5.3.1.1.3.cmml" type="integer" xref="S3.E1.m1.5.5.3.1.1.3">1</cn></apply><apply id="S3.E1.m1.6.6.4.2.2.cmml" xref="S3.E1.m1.6.6.4.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.2.2.1.cmml" xref="S3.E1.m1.6.6.4.2.2">subscript</csymbol><ci id="S3.E1.m1.6.6.4.2.2.2a.cmml" xref="S3.E1.m1.6.6.4.2.2.2"><mtext id="S3.E1.m1.6.6.4.2.2.2.cmml" xref="S3.E1.m1.6.6.4.2.2.2">logit</mtext></ci><cn id="S3.E1.m1.6.6.4.2.2.3.cmml" type="integer" xref="S3.E1.m1.6.6.4.2.2.3">2</cn></apply><apply id="S3.E1.m1.7.7.5.3.3.cmml" xref="S3.E1.m1.7.7.5.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.5.3.3.1.cmml" xref="S3.E1.m1.7.7.5.3.3">subscript</csymbol><ci id="S3.E1.m1.7.7.5.3.3.2a.cmml" xref="S3.E1.m1.7.7.5.3.3.2"><mtext id="S3.E1.m1.7.7.5.3.3.2.cmml" xref="S3.E1.m1.7.7.5.3.3.2">logit</mtext></ci><cn id="S3.E1.m1.7.7.5.3.3.3.cmml" type="integer" xref="S3.E1.m1.7.7.5.3.3.3">3</cn></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">…</ci><apply id="S3.E1.m1.8.8.6.4.4.cmml" xref="S3.E1.m1.8.8.6.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.6.4.4.1.cmml" xref="S3.E1.m1.8.8.6.4.4">subscript</csymbol><ci id="S3.E1.m1.8.8.6.4.4.2a.cmml" xref="S3.E1.m1.8.8.6.4.4.2"><mtext id="S3.E1.m1.8.8.6.4.4.2.cmml" xref="S3.E1.m1.8.8.6.4.4.2">logit</mtext></ci><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.3"><abs id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></abs><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑉</ci></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.8c">f_{l}(t_{m})=W_{U}\cdot h_{l}(t_{m})=[\text{logit}_{1},\text{logit}_{2},\text{%
logit}_{3},\ldots,\text{logit}_{|V|}]</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.8d">italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) = italic_W start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT ⋅ italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) = [ logit start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , logit start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , logit start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , … , logit start_POSTSUBSCRIPT | italic_V | end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.6">This is the logit distribution representing the predictions of the model after <math alttext="l" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m1.1"><semantics id="S3.SS1.p3.4.m1.1a"><mi id="S3.SS1.p3.4.m1.1.1" xref="S3.SS1.p3.4.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m1.1b"><ci id="S3.SS1.p3.4.m1.1.1.cmml" xref="S3.SS1.p3.4.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m1.1d">italic_l</annotation></semantics></math> layers, where <math alttext="\text{logit}_{j}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m2.1"><semantics id="S3.SS1.p3.5.m2.1a"><msub id="S3.SS1.p3.5.m2.1.1" xref="S3.SS1.p3.5.m2.1.1.cmml"><mtext id="S3.SS1.p3.5.m2.1.1.2" xref="S3.SS1.p3.5.m2.1.1.2a.cmml">logit</mtext><mi id="S3.SS1.p3.5.m2.1.1.3" xref="S3.SS1.p3.5.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m2.1b"><apply id="S3.SS1.p3.5.m2.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m2.1.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m2.1.1.2a.cmml" xref="S3.SS1.p3.5.m2.1.1.2"><mtext id="S3.SS1.p3.5.m2.1.1.2.cmml" xref="S3.SS1.p3.5.m2.1.1.2">logit</mtext></ci><ci id="S3.SS1.p3.5.m2.1.1.3.cmml" xref="S3.SS1.p3.5.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m2.1c">\text{logit}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m2.1d">logit start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> corresponds to the token <math alttext="j" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m3.1"><semantics id="S3.SS1.p3.6.m3.1a"><mi id="S3.SS1.p3.6.m3.1.1" xref="S3.SS1.p3.6.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m3.1b"><ci id="S3.SS1.p3.6.m3.1.1.cmml" xref="S3.SS1.p3.6.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m3.1d">italic_j</annotation></semantics></math> in the vocabulary.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Applying Logit Lens on VLMs</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We apply the logit lens to probe the language model as it processes the image representations. This enables us to interpret the image features’ output distributions as they are transformed by the layers of the language model and localize objects spatially within the image.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.8"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.8.1">Extracting probability distributions from intermediate image representations</span>. We apply logit lens on the <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.8.2">image representations</span> in the VLM. For a given image embedding <math alttext="k_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">k</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑘</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">k_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, we find the latent representation of the image embedding at layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_l</annotation></semantics></math>, <math alttext="h_{l}(k_{i})" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><msub id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">h</mi><mi id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml">l</mi></msub><mo id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.3.m3.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.3.m3.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.3.m3.1.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.1.1.1.2" xref="S3.SS2.p2.3.m3.1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS2.p2.3.m3.1.1.1.1.1.3" xref="S3.SS2.p2.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p2.3.m3.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"></times><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">ℎ</ci><ci id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">𝑙</ci></apply><apply id="S3.SS2.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.2">𝑘</ci><ci id="S3.SS2.p2.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">h_{l}(k_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>, taking the logit lens to get the probability distribution over the vocabulary, <math alttext="\text{softmax}(f_{l}(k_{i}))" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mtext id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3a.cmml">softmax</mtext><mo id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.cmml"><msub id="S3.SS2.p2.4.m4.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3.3.cmml">l</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"></times><ci id="S3.SS2.p2.4.m4.1.1.3a.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><mtext id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">softmax</mtext></ci><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.2"></times><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.3.3">𝑙</ci></apply><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\text{softmax}(f_{l}(k_{i}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">softmax ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )</annotation></semantics></math>. We define an object <math alttext="o" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">o</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_o</annotation></semantics></math>, an object word composed of tokens from the vocabulary. We inspect the probability of a specific object <math alttext="o" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">o</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_o</annotation></semantics></math>, <math alttext="\text{softmax}(f_{l}(k_{i}))_{o}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mtext id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3a.cmml">softmax</mtext><mo id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">⁢</mo><msub id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml"><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml"><msub id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3.cmml">l</mi></msub><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS2.p2.7.m7.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.3.cmml">o</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><times id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2"></times><ci id="S3.SS2.p2.7.m7.1.1.3a.cmml" xref="S3.SS2.p2.7.m7.1.1.3"><mtext id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">softmax</mtext></ci><apply id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1">subscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1"><times id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2"></times><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.3">𝑙</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="S3.SS2.p2.7.m7.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\text{softmax}(f_{l}(k_{i}))_{o}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">softmax ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>. For multi-token objects, we take the maximum probability value over the object tokens. This provides a generalizable framework for analyzing specific latent image representations via text, with respect to specific objects. Next, we find the maximum probability over all image representations over all layers. For object <math alttext="o" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m8.1"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">o</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.1d">italic_o</annotation></semantics></math>, we compute:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="c_{o}=\max_{\begin{subarray}{c}1\leq l\leq L\\
1\leq i\leq n\end{subarray}}\{\text{softmax}(f_{l}(k_{i}))_{o}\}" class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msub id="S3.E2.m1.3.3.4" xref="S3.E2.m1.3.3.4.cmml"><mi id="S3.E2.m1.3.3.4.2" xref="S3.E2.m1.3.3.4.2.cmml">c</mi><mi id="S3.E2.m1.3.3.4.3" xref="S3.E2.m1.3.3.4.3.cmml">o</mi></msub><mo id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml">=</mo><mrow id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.3.cmml"><munder id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">max</mi><mtable id="S3.E2.m1.1.1.1.1.1.1" rowspacing="0pt" xref="S3.E2.m1.1.1.1.2.cmml"><mtr id="S3.E2.m1.1.1.1.1.1.1a" xref="S3.E2.m1.1.1.1.2.cmml"><mtd id="S3.E2.m1.1.1.1.1.1.1b" xref="S3.E2.m1.1.1.1.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">≤</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml">l</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.cmml">≤</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.6.cmml">L</mi></mrow></mtd></mtr><mtr id="S3.E2.m1.1.1.1.1.1.1c" xref="S3.E2.m1.1.1.1.2.cmml"><mtd id="S3.E2.m1.1.1.1.1.1.1d" xref="S3.E2.m1.1.1.1.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.2.1.1" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.2.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.2.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.3.cmml">≤</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.4.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.1.1.2.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.5.cmml">≤</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.6.cmml">n</mi></mrow></mtd></mtr></mtable></munder><mo id="S3.E2.m1.3.3.2.2a" xref="S3.E2.m1.3.3.2.3.cmml">⁡</mo><mrow id="S3.E2.m1.3.3.2.2.2" xref="S3.E2.m1.3.3.2.3.cmml"><mo id="S3.E2.m1.3.3.2.2.2.2" stretchy="false" xref="S3.E2.m1.3.3.2.3.cmml">{</mo><mrow id="S3.E2.m1.3.3.2.2.2.1" xref="S3.E2.m1.3.3.2.2.2.1.cmml"><mtext id="S3.E2.m1.3.3.2.2.2.1.3" xref="S3.E2.m1.3.3.2.2.2.1.3a.cmml">softmax</mtext><mo id="S3.E2.m1.3.3.2.2.2.1.2" xref="S3.E2.m1.3.3.2.2.2.1.2.cmml">⁢</mo><msub id="S3.E2.m1.3.3.2.2.2.1.1" xref="S3.E2.m1.3.3.2.2.2.1.1.cmml"><mrow id="S3.E2.m1.3.3.2.2.2.1.1.1.1" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.3.cmml">l</mi></msub><mo id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.2" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.E2.m1.3.3.2.2.2.1.1.3" xref="S3.E2.m1.3.3.2.2.2.1.1.3.cmml">o</mi></msub></mrow><mo id="S3.E2.m1.3.3.2.2.2.3" stretchy="false" xref="S3.E2.m1.3.3.2.3.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"></eq><apply id="S3.E2.m1.3.3.4.cmml" xref="S3.E2.m1.3.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.4.1.cmml" xref="S3.E2.m1.3.3.4">subscript</csymbol><ci id="S3.E2.m1.3.3.4.2.cmml" xref="S3.E2.m1.3.3.4.2">𝑐</ci><ci id="S3.E2.m1.3.3.4.3.cmml" xref="S3.E2.m1.3.3.4.3">𝑜</ci></apply><apply id="S3.E2.m1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.2.2"><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1">subscript</csymbol><max id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></max><list id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><matrix id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><matrixrow id="S3.E2.m1.1.1.1.1.1.1a.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><and id="S3.E2.m1.1.1.1.1.1.1.1.1.1a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"></and><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1b.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><leq id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"></leq><cn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.4">𝑙</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1c.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><leq id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5"></leq><share href="https://arxiv.org/html/2410.02762v1#S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1d.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"></share><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.6">𝐿</ci></apply></apply></matrixrow><matrixrow id="S3.E2.m1.1.1.1.1.1.1b.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1"><and id="S3.E2.m1.1.1.1.1.1.1.2.1.1a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1"></and><apply id="S3.E2.m1.1.1.1.1.1.1.2.1.1b.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1"><leq id="S3.E2.m1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.3"></leq><cn id="S3.E2.m1.1.1.1.1.1.1.2.1.1.2.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.2.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.4">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.2.1.1c.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1"><leq id="S3.E2.m1.1.1.1.1.1.1.2.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.5"></leq><share href="https://arxiv.org/html/2410.02762v1#S3.E2.m1.1.1.1.1.1.1.2.1.1.4.cmml" id="S3.E2.m1.1.1.1.1.1.1.2.1.1d.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1"></share><ci id="S3.E2.m1.1.1.1.1.1.1.2.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1.1.6">𝑛</ci></apply></apply></matrixrow></matrix></list></apply><apply id="S3.E2.m1.3.3.2.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.2.1"><times id="S3.E2.m1.3.3.2.2.2.1.2.cmml" xref="S3.E2.m1.3.3.2.2.2.1.2"></times><ci id="S3.E2.m1.3.3.2.2.2.1.3a.cmml" xref="S3.E2.m1.3.3.2.2.2.1.3"><mtext id="S3.E2.m1.3.3.2.2.2.1.3.cmml" xref="S3.E2.m1.3.3.2.2.2.1.3">softmax</mtext></ci><apply id="S3.E2.m1.3.3.2.2.2.1.1.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1"><times id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.2"></times><apply id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.3.3">𝑙</ci></apply><apply id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="S3.E2.m1.3.3.2.2.2.1.1.3.cmml" xref="S3.E2.m1.3.3.2.2.2.1.1.3">𝑜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">c_{o}=\max_{\begin{subarray}{c}1\leq l\leq L\\
1\leq i\leq n\end{subarray}}\{\text{softmax}(f_{l}(k_{i}))_{o}\}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = roman_max start_POSTSUBSCRIPT start_ARG start_ROW start_CELL 1 ≤ italic_l ≤ italic_L end_CELL end_ROW start_ROW start_CELL 1 ≤ italic_i ≤ italic_n end_CELL end_ROW end_ARG end_POSTSUBSCRIPT { softmax ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.4">We define <math alttext="c_{o}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝑐</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">c_{o}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> as the VLMs <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.4.1">internal confidence</span> of an object <math alttext="o" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">o</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_o</annotation></semantics></math> existing in the image: the highest probability of object presence across <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_n</annotation></semantics></math> image representations through <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">italic_L</annotation></semantics></math> layers of the language model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.2.1">Comparing the internal confidence of present and not present objects.</span> To determine if internal confidence provides meaningful information about objects in the image, we examine <math alttext="c_{o}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">c_{o}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> for objects present and not present in an image. We use InstructBLIP and LLaVA to caption 5000 random COCO2014 images in the Karpathy validation split <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib31" title="">2015</a>)</cite> and determine <math alttext="c_{o}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">𝑐</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">c_{o}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> for all 80 COCO objects, only a few of which are present in each image. Since there are many more objects not present than present, we randomly sample a subset of the internal confidences for objects not present. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.F2" title="Figure 2 ‣ 3.2 Applying Logit Lens on VLMs ‣ 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">2</span></a> exhibits the internal confidences for objects present and not present in the image. We empirically find that the VLMs’ internal confidences are higher for present objects than not present ones. We use this claim later to classify objects as hallucinations in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS1" title="5.1 Hallucination Detection ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S3.F2.g1" src="x2.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.2.1">Comparison of internal confidence in objects present and not present in the image</span>. We examine the internal confidence of COCO objects that exist and do not exist in the image within intermediate VLM image representations. We observe that objects that do not exist in the image have lower internal confidence.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F3.g1" src="x3.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.F3.2.1">Localizing objects using internal confidence values</span>. We find the probabilities of objects through layers of the language model for every image embedding in LLaVA. We use the highest layer probability per image embedding to localize an object within the image.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.3"><span class="ltx_text ltx_font_bold" id="S3.SS2.p6.3.1">Object localization</span>. Given that the language model can distinguish between objects present and not present in an image, we examine whether it can attribute high object internal confidence to specific patches in an image. For each image embedding <math alttext="k_{i}" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><msub id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">k</mi><mi id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">𝑘</ci><ci id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">k_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><mi id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><ci id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">italic_n</annotation></semantics></math> image embeddings, we find the maximum softmax probability of an object within the layers of the model, <math alttext="\max_{1\leq l\leq L}\{\text{softmax}(f_{l}(k_{i}))_{o}\}" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.2"><semantics id="S3.SS2.p6.3.m3.2a"><mrow id="S3.SS2.p6.3.m3.2.2.2" xref="S3.SS2.p6.3.m3.2.2.3.cmml"><msub id="S3.SS2.p6.3.m3.1.1.1.1" xref="S3.SS2.p6.3.m3.1.1.1.1.cmml"><mi id="S3.SS2.p6.3.m3.1.1.1.1.2" xref="S3.SS2.p6.3.m3.1.1.1.1.2.cmml">max</mi><mrow id="S3.SS2.p6.3.m3.1.1.1.1.3" xref="S3.SS2.p6.3.m3.1.1.1.1.3.cmml"><mn id="S3.SS2.p6.3.m3.1.1.1.1.3.2" xref="S3.SS2.p6.3.m3.1.1.1.1.3.2.cmml">1</mn><mo id="S3.SS2.p6.3.m3.1.1.1.1.3.3" xref="S3.SS2.p6.3.m3.1.1.1.1.3.3.cmml">≤</mo><mi id="S3.SS2.p6.3.m3.1.1.1.1.3.4" xref="S3.SS2.p6.3.m3.1.1.1.1.3.4.cmml">l</mi><mo id="S3.SS2.p6.3.m3.1.1.1.1.3.5" xref="S3.SS2.p6.3.m3.1.1.1.1.3.5.cmml">≤</mo><mi id="S3.SS2.p6.3.m3.1.1.1.1.3.6" xref="S3.SS2.p6.3.m3.1.1.1.1.3.6.cmml">L</mi></mrow></msub><mo id="S3.SS2.p6.3.m3.2.2.2a" xref="S3.SS2.p6.3.m3.2.2.3.cmml">⁡</mo><mrow id="S3.SS2.p6.3.m3.2.2.2.2" xref="S3.SS2.p6.3.m3.2.2.3.cmml"><mo id="S3.SS2.p6.3.m3.2.2.2.2.2" stretchy="false" xref="S3.SS2.p6.3.m3.2.2.3.cmml">{</mo><mrow id="S3.SS2.p6.3.m3.2.2.2.2.1" xref="S3.SS2.p6.3.m3.2.2.2.2.1.cmml"><mtext id="S3.SS2.p6.3.m3.2.2.2.2.1.3" xref="S3.SS2.p6.3.m3.2.2.2.2.1.3a.cmml">softmax</mtext><mo id="S3.SS2.p6.3.m3.2.2.2.2.1.2" xref="S3.SS2.p6.3.m3.2.2.2.2.1.2.cmml">⁢</mo><msub id="S3.SS2.p6.3.m3.2.2.2.2.1.1" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.cmml"><mrow id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.cmml"><mo id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.cmml"><msub id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.2" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.3" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.3.cmml">l</mi></msub><mo id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.2" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS2.p6.3.m3.2.2.2.2.1.1.3" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.3.cmml">o</mi></msub></mrow><mo id="S3.SS2.p6.3.m3.2.2.2.2.3" stretchy="false" xref="S3.SS2.p6.3.m3.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.2b"><apply id="S3.SS2.p6.3.m3.2.2.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2"><apply id="S3.SS2.p6.3.m3.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1">subscript</csymbol><max id="S3.SS2.p6.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.2"></max><apply id="S3.SS2.p6.3.m3.1.1.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3"><and id="S3.SS2.p6.3.m3.1.1.1.1.3a.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3"></and><apply id="S3.SS2.p6.3.m3.1.1.1.1.3b.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3"><leq id="S3.SS2.p6.3.m3.1.1.1.1.3.3.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3.3"></leq><cn id="S3.SS2.p6.3.m3.1.1.1.1.3.2.cmml" type="integer" xref="S3.SS2.p6.3.m3.1.1.1.1.3.2">1</cn><ci id="S3.SS2.p6.3.m3.1.1.1.1.3.4.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3.4">𝑙</ci></apply><apply id="S3.SS2.p6.3.m3.1.1.1.1.3c.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3"><leq id="S3.SS2.p6.3.m3.1.1.1.1.3.5.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3.5"></leq><share href="https://arxiv.org/html/2410.02762v1#S3.SS2.p6.3.m3.1.1.1.1.3.4.cmml" id="S3.SS2.p6.3.m3.1.1.1.1.3d.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3"></share><ci id="S3.SS2.p6.3.m3.1.1.1.1.3.6.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1.3.6">𝐿</ci></apply></apply></apply><apply id="S3.SS2.p6.3.m3.2.2.2.2.1.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1"><times id="S3.SS2.p6.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.2"></times><ci id="S3.SS2.p6.3.m3.2.2.2.2.1.3a.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.3"><mtext id="S3.SS2.p6.3.m3.2.2.2.2.1.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.3">softmax</mtext></ci><apply id="S3.SS2.p6.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.2.2.2.2.1.1.2.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1">subscript</csymbol><apply id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1"><times id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.2"></times><apply id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.3.3">𝑙</ci></apply><apply id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="S3.SS2.p6.3.m3.2.2.2.2.1.1.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2.1.1.3">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.2c">\max_{1\leq l\leq L}\{\text{softmax}(f_{l}(k_{i}))_{o}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.2d">roman_max start_POSTSUBSCRIPT 1 ≤ italic_l ≤ italic_L end_POSTSUBSCRIPT { softmax ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT }</annotation></semantics></math>. Using these internal confidence values, we localize the objects in the image patches, each of which maps to an image embedding. We focus on LLaVA for this task, since its image encoder preserves the spatial mapping of image patches to image features.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">We observe that image representations that exhibit higher internal confidence for specific objects correspond to the image patches in which those objects are visually present (examples in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.F3" title="In 3.2 Applying Logit Lens on VLMs ‣ 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>). Building on our previous observation, we see that the intermediate image representations semantically align with latent token representations of objects present in them while maintaining their spatial locality. We use this unique finding for zero-shot segmentation in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS3" title="5.3 Zero-shot Segmentation ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1">While the model is not directly trained to map the image representations closer to the text representations of objects within them, we can unembed the image representations in the text vocabulary for localization and find differences in internal confidence for present and hallucinated objects. In <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS1" title="5.1 Hallucination Detection ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>, we will use this observation for various applications including hallucination detection and zero-short segmentation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Erasing knowledge from VLMs</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Recognizing that image embeddings are directly interpretable (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.SS2" title="3.2 Applying Logit Lens on VLMs ‣ 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>), we edit these embeddings to erase the presence of objects from image captions. We propose a linear editing algorithm that subtracts the text embedding of a target object from all image embeddings. When applied on singular and multiple object removals, we find that it erases hallucinated objects more effectively than correctly detected (CD) objects (i.e. real objects that the model correctly detects).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Erasing objects from image representations</h3>
<figure class="ltx_figure ltx_minipage ltx_align_middle ltx_align_floatright" id="S4.F4.fig1" style="width:433.6pt;">
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.3.1.1">Algorithm 1</span> </span> <span class="ltx_text ltx_font_smallcaps" id="alg1.4.2">ProjectAway</span></figcaption>
<div class="ltx_listing ltx_listing" id="alg1.5">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_text ltx_font_bold" id="alg1.l1.1">Input:</span> A set of image embeddings <math alttext="K" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_K</annotation></semantics></math>, text embedding <math alttext="\vec{t}" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><mover accent="true" id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">t</mi><mo id="alg1.l1.m2.1.1.1" stretchy="false" xref="alg1.l1.m2.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><ci id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1.1">→</ci><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">\vec{t}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">over→ start_ARG italic_t end_ARG</annotation></semantics></math>, and weight factor <math alttext="\alpha" class="ltx_Math" display="inline" id="alg1.l1.m3.1"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m3.1d">italic_α</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_text ltx_font_bold" id="alg1.l2.1">Output:</span> A set of modified image embeddings <math alttext="K^{\prime}" class="ltx_Math" display="inline" id="alg1.l2.m1.1"><semantics id="alg1.l2.m1.1a"><msup id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">K</mi><mo id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><csymbol cd="ambiguous" id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1">superscript</csymbol><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝐾</ci><ci id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">K^{\prime}</annotation><annotation encoding="application/x-llamapun" id="alg1.l2.m1.1d">italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> projected away from the text embedding

</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_text ltx_font_bold" id="alg1.l3.1">Initialization:</span> <math alttext="K^{\prime}\leftarrow\emptyset" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><msup id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml"><mi id="alg1.l3.m1.1.1.2.2" xref="alg1.l3.m1.1.1.2.2.cmml">K</mi><mo id="alg1.l3.m1.1.1.2.3" xref="alg1.l3.m1.1.1.2.3.cmml">′</mo></msup><mo id="alg1.l3.m1.1.1.1" stretchy="false" xref="alg1.l3.m1.1.1.1.cmml">←</mo><mi id="alg1.l3.m1.1.1.3" mathvariant="normal" xref="alg1.l3.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><ci id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1">←</ci><apply id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.2.1.cmml" xref="alg1.l3.m1.1.1.2">superscript</csymbol><ci id="alg1.l3.m1.1.1.2.2.cmml" xref="alg1.l3.m1.1.1.2.2">𝐾</ci><ci id="alg1.l3.m1.1.1.2.3.cmml" xref="alg1.l3.m1.1.1.2.3">′</ci></apply><emptyset id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">K^{\prime}\leftarrow\emptyset</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ← ∅</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_text ltx_font_bold" id="alg1.l4.1">for</span> <math alttext="\vec{k}\in K" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mover accent="true" id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml"><mi id="alg1.l4.m1.1.1.2.2" xref="alg1.l4.m1.1.1.2.2.cmml">k</mi><mo id="alg1.l4.m1.1.1.2.1" stretchy="false" xref="alg1.l4.m1.1.1.2.1.cmml">→</mo></mover><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">∈</mo><mi id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><in id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1"></in><apply id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2"><ci id="alg1.l4.m1.1.1.2.1.cmml" xref="alg1.l4.m1.1.1.2.1">→</ci><ci id="alg1.l4.m1.1.1.2.2.cmml" xref="alg1.l4.m1.1.1.2.2">𝑘</ci></apply><ci id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\vec{k}\in K</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">over→ start_ARG italic_k end_ARG ∈ italic_K</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l4.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l5">     <math alttext="p\leftarrow\vec{k}\cdot\vec{t}" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">p</mi><mo id="alg1.l5.m1.1.1.1" stretchy="false" xref="alg1.l5.m1.1.1.1.cmml">←</mo><mrow id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"><mover accent="true" id="alg1.l5.m1.1.1.3.2" xref="alg1.l5.m1.1.1.3.2.cmml"><mi id="alg1.l5.m1.1.1.3.2.2" xref="alg1.l5.m1.1.1.3.2.2.cmml">k</mi><mo id="alg1.l5.m1.1.1.3.2.1" stretchy="false" xref="alg1.l5.m1.1.1.3.2.1.cmml">→</mo></mover><mo id="alg1.l5.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="alg1.l5.m1.1.1.3.1.cmml">⋅</mo><mover accent="true" id="alg1.l5.m1.1.1.3.3" xref="alg1.l5.m1.1.1.3.3.cmml"><mi id="alg1.l5.m1.1.1.3.3.2" xref="alg1.l5.m1.1.1.3.3.2.cmml">t</mi><mo id="alg1.l5.m1.1.1.3.3.1" stretchy="false" xref="alg1.l5.m1.1.1.3.3.1.cmml">→</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">←</ci><ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">𝑝</ci><apply id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3"><ci id="alg1.l5.m1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.3.1">⋅</ci><apply id="alg1.l5.m1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.3.2"><ci id="alg1.l5.m1.1.1.3.2.1.cmml" xref="alg1.l5.m1.1.1.3.2.1">→</ci><ci id="alg1.l5.m1.1.1.3.2.2.cmml" xref="alg1.l5.m1.1.1.3.2.2">𝑘</ci></apply><apply id="alg1.l5.m1.1.1.3.3.cmml" xref="alg1.l5.m1.1.1.3.3"><ci id="alg1.l5.m1.1.1.3.3.1.cmml" xref="alg1.l5.m1.1.1.3.3.1">→</ci><ci id="alg1.l5.m1.1.1.3.3.2.cmml" xref="alg1.l5.m1.1.1.3.3.2">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">p\leftarrow\vec{k}\cdot\vec{t}</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">italic_p ← over→ start_ARG italic_k end_ARG ⋅ over→ start_ARG italic_t end_ARG</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">     <span class="ltx_text ltx_font_bold" id="alg1.l6.1">if</span> <math alttext="p&gt;0" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">p</mi><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">&gt;</mo><mn id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><gt id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></gt><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑝</ci><cn id="alg1.l6.m1.1.1.3.cmml" type="integer" xref="alg1.l6.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">p&gt;0</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_p &gt; 0</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l6.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l7">         <math alttext="K^{\prime}\leftarrow K^{\prime}\cup\{\vec{k}-\alpha\cdot\frac{p}{\lVert\vec{t}%
\rVert_{2}^{2}}\cdot\vec{t}\}" class="ltx_Math" display="inline" id="alg1.l7.m1.2"><semantics id="alg1.l7.m1.2a"><mrow id="alg1.l7.m1.2.2" xref="alg1.l7.m1.2.2.cmml"><msup id="alg1.l7.m1.2.2.3" xref="alg1.l7.m1.2.2.3.cmml"><mi id="alg1.l7.m1.2.2.3.2" xref="alg1.l7.m1.2.2.3.2.cmml">K</mi><mo id="alg1.l7.m1.2.2.3.3" xref="alg1.l7.m1.2.2.3.3.cmml">′</mo></msup><mo id="alg1.l7.m1.2.2.2" stretchy="false" xref="alg1.l7.m1.2.2.2.cmml">←</mo><mrow id="alg1.l7.m1.2.2.1" xref="alg1.l7.m1.2.2.1.cmml"><msup id="alg1.l7.m1.2.2.1.3" xref="alg1.l7.m1.2.2.1.3.cmml"><mi id="alg1.l7.m1.2.2.1.3.2" xref="alg1.l7.m1.2.2.1.3.2.cmml">K</mi><mo id="alg1.l7.m1.2.2.1.3.3" xref="alg1.l7.m1.2.2.1.3.3.cmml">′</mo></msup><mo id="alg1.l7.m1.2.2.1.2" xref="alg1.l7.m1.2.2.1.2.cmml">∪</mo><mrow id="alg1.l7.m1.2.2.1.1.1" xref="alg1.l7.m1.2.2.1.1.2.cmml"><mo id="alg1.l7.m1.2.2.1.1.1.2" stretchy="false" xref="alg1.l7.m1.2.2.1.1.2.cmml">{</mo><mrow id="alg1.l7.m1.2.2.1.1.1.1" xref="alg1.l7.m1.2.2.1.1.1.1.cmml"><mover accent="true" id="alg1.l7.m1.2.2.1.1.1.1.2" xref="alg1.l7.m1.2.2.1.1.1.1.2.cmml"><mi id="alg1.l7.m1.2.2.1.1.1.1.2.2" xref="alg1.l7.m1.2.2.1.1.1.1.2.2.cmml">k</mi><mo id="alg1.l7.m1.2.2.1.1.1.1.2.1" stretchy="false" xref="alg1.l7.m1.2.2.1.1.1.1.2.1.cmml">→</mo></mover><mo id="alg1.l7.m1.2.2.1.1.1.1.1" xref="alg1.l7.m1.2.2.1.1.1.1.1.cmml">−</mo><mrow id="alg1.l7.m1.2.2.1.1.1.1.3" xref="alg1.l7.m1.2.2.1.1.1.1.3.cmml"><mi id="alg1.l7.m1.2.2.1.1.1.1.3.2" xref="alg1.l7.m1.2.2.1.1.1.1.3.2.cmml">α</mi><mo id="alg1.l7.m1.2.2.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="alg1.l7.m1.2.2.1.1.1.1.3.1.cmml">⋅</mo><mfrac id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">p</mi><msubsup id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml"><mrow id="alg1.l7.m1.1.1.1.3.2.2" xref="alg1.l7.m1.1.1.1.3.2.1.cmml"><mo fence="true" id="alg1.l7.m1.1.1.1.3.2.2.1" rspace="0em" xref="alg1.l7.m1.1.1.1.3.2.1.1.cmml">∥</mo><mover accent="true" id="alg1.l7.m1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.cmml"><mi id="alg1.l7.m1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.2.cmml">t</mi><mo id="alg1.l7.m1.1.1.1.1.1" stretchy="false" xref="alg1.l7.m1.1.1.1.1.1.cmml">→</mo></mover><mo fence="true" id="alg1.l7.m1.1.1.1.3.2.2.2" lspace="0em" xref="alg1.l7.m1.1.1.1.3.2.1.1.cmml">∥</mo></mrow><mn id="alg1.l7.m1.1.1.1.3.3" xref="alg1.l7.m1.1.1.1.3.3.cmml">2</mn><mn id="alg1.l7.m1.1.1.1.4" xref="alg1.l7.m1.1.1.1.4.cmml">2</mn></msubsup></mfrac><mo id="alg1.l7.m1.2.2.1.1.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="alg1.l7.m1.2.2.1.1.1.1.3.1.cmml">⋅</mo><mover accent="true" id="alg1.l7.m1.2.2.1.1.1.1.3.3" xref="alg1.l7.m1.2.2.1.1.1.1.3.3.cmml"><mi id="alg1.l7.m1.2.2.1.1.1.1.3.3.2" xref="alg1.l7.m1.2.2.1.1.1.1.3.3.2.cmml">t</mi><mo id="alg1.l7.m1.2.2.1.1.1.1.3.3.1" stretchy="false" xref="alg1.l7.m1.2.2.1.1.1.1.3.3.1.cmml">→</mo></mover></mrow></mrow><mo id="alg1.l7.m1.2.2.1.1.1.3" stretchy="false" xref="alg1.l7.m1.2.2.1.1.2.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.2b"><apply id="alg1.l7.m1.2.2.cmml" xref="alg1.l7.m1.2.2"><ci id="alg1.l7.m1.2.2.2.cmml" xref="alg1.l7.m1.2.2.2">←</ci><apply id="alg1.l7.m1.2.2.3.cmml" xref="alg1.l7.m1.2.2.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.3.1.cmml" xref="alg1.l7.m1.2.2.3">superscript</csymbol><ci id="alg1.l7.m1.2.2.3.2.cmml" xref="alg1.l7.m1.2.2.3.2">𝐾</ci><ci id="alg1.l7.m1.2.2.3.3.cmml" xref="alg1.l7.m1.2.2.3.3">′</ci></apply><apply id="alg1.l7.m1.2.2.1.cmml" xref="alg1.l7.m1.2.2.1"><union id="alg1.l7.m1.2.2.1.2.cmml" xref="alg1.l7.m1.2.2.1.2"></union><apply id="alg1.l7.m1.2.2.1.3.cmml" xref="alg1.l7.m1.2.2.1.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.1.3.1.cmml" xref="alg1.l7.m1.2.2.1.3">superscript</csymbol><ci id="alg1.l7.m1.2.2.1.3.2.cmml" xref="alg1.l7.m1.2.2.1.3.2">𝐾</ci><ci id="alg1.l7.m1.2.2.1.3.3.cmml" xref="alg1.l7.m1.2.2.1.3.3">′</ci></apply><set id="alg1.l7.m1.2.2.1.1.2.cmml" xref="alg1.l7.m1.2.2.1.1.1"><apply id="alg1.l7.m1.2.2.1.1.1.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1"><minus id="alg1.l7.m1.2.2.1.1.1.1.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.1"></minus><apply id="alg1.l7.m1.2.2.1.1.1.1.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2"><ci id="alg1.l7.m1.2.2.1.1.1.1.2.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.1">→</ci><ci id="alg1.l7.m1.2.2.1.1.1.1.2.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.2">𝑘</ci></apply><apply id="alg1.l7.m1.2.2.1.1.1.1.3.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3"><ci id="alg1.l7.m1.2.2.1.1.1.1.3.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3.1">⋅</ci><ci id="alg1.l7.m1.2.2.1.1.1.1.3.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3.2">𝛼</ci><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><divide id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1"></divide><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">𝑝</ci><apply id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1">superscript</csymbol><apply id="alg1.l7.m1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.3.1.cmml" xref="alg1.l7.m1.1.1.1">subscript</csymbol><apply id="alg1.l7.m1.1.1.1.3.2.1.cmml" xref="alg1.l7.m1.1.1.1.3.2.2"><csymbol cd="latexml" id="alg1.l7.m1.1.1.1.3.2.1.1.cmml" xref="alg1.l7.m1.1.1.1.3.2.2.1">delimited-∥∥</csymbol><apply id="alg1.l7.m1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1"><ci id="alg1.l7.m1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1">→</ci><ci id="alg1.l7.m1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.2">𝑡</ci></apply></apply><cn id="alg1.l7.m1.1.1.1.3.3.cmml" type="integer" xref="alg1.l7.m1.1.1.1.3.3">2</cn></apply><cn id="alg1.l7.m1.1.1.1.4.cmml" type="integer" xref="alg1.l7.m1.1.1.1.4">2</cn></apply></apply><apply id="alg1.l7.m1.2.2.1.1.1.1.3.3.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3.3"><ci id="alg1.l7.m1.2.2.1.1.1.1.3.3.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3.3.1">→</ci><ci id="alg1.l7.m1.2.2.1.1.1.1.3.3.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3.3.2">𝑡</ci></apply></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.2c">K^{\prime}\leftarrow K^{\prime}\cup\{\vec{k}-\alpha\cdot\frac{p}{\lVert\vec{t}%
\rVert_{2}^{2}}\cdot\vec{t}\}</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.2d">italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ← italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∪ { over→ start_ARG italic_k end_ARG - italic_α ⋅ divide start_ARG italic_p end_ARG start_ARG ∥ over→ start_ARG italic_t end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ⋅ over→ start_ARG italic_t end_ARG }</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">     <span class="ltx_text ltx_font_bold" id="alg1.l8.1">else</span>
</div>
<div class="ltx_listingline" id="alg1.l9">         <math alttext="K^{\prime}\leftarrow K^{\prime}\cup\{\vec{k}\}" class="ltx_Math" display="inline" id="alg1.l9.m1.1"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.2" xref="alg1.l9.m1.1.2.cmml"><msup id="alg1.l9.m1.1.2.2" xref="alg1.l9.m1.1.2.2.cmml"><mi id="alg1.l9.m1.1.2.2.2" xref="alg1.l9.m1.1.2.2.2.cmml">K</mi><mo id="alg1.l9.m1.1.2.2.3" xref="alg1.l9.m1.1.2.2.3.cmml">′</mo></msup><mo id="alg1.l9.m1.1.2.1" stretchy="false" xref="alg1.l9.m1.1.2.1.cmml">←</mo><mrow id="alg1.l9.m1.1.2.3" xref="alg1.l9.m1.1.2.3.cmml"><msup id="alg1.l9.m1.1.2.3.2" xref="alg1.l9.m1.1.2.3.2.cmml"><mi id="alg1.l9.m1.1.2.3.2.2" xref="alg1.l9.m1.1.2.3.2.2.cmml">K</mi><mo id="alg1.l9.m1.1.2.3.2.3" xref="alg1.l9.m1.1.2.3.2.3.cmml">′</mo></msup><mo id="alg1.l9.m1.1.2.3.1" xref="alg1.l9.m1.1.2.3.1.cmml">∪</mo><mrow id="alg1.l9.m1.1.2.3.3.2" xref="alg1.l9.m1.1.2.3.3.1.cmml"><mo id="alg1.l9.m1.1.2.3.3.2.1" stretchy="false" xref="alg1.l9.m1.1.2.3.3.1.cmml">{</mo><mover accent="true" id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mi id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml">k</mi><mo id="alg1.l9.m1.1.1.1" stretchy="false" xref="alg1.l9.m1.1.1.1.cmml">→</mo></mover><mo id="alg1.l9.m1.1.2.3.3.2.2" stretchy="false" xref="alg1.l9.m1.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.2.cmml" xref="alg1.l9.m1.1.2"><ci id="alg1.l9.m1.1.2.1.cmml" xref="alg1.l9.m1.1.2.1">←</ci><apply id="alg1.l9.m1.1.2.2.cmml" xref="alg1.l9.m1.1.2.2"><csymbol cd="ambiguous" id="alg1.l9.m1.1.2.2.1.cmml" xref="alg1.l9.m1.1.2.2">superscript</csymbol><ci id="alg1.l9.m1.1.2.2.2.cmml" xref="alg1.l9.m1.1.2.2.2">𝐾</ci><ci id="alg1.l9.m1.1.2.2.3.cmml" xref="alg1.l9.m1.1.2.2.3">′</ci></apply><apply id="alg1.l9.m1.1.2.3.cmml" xref="alg1.l9.m1.1.2.3"><union id="alg1.l9.m1.1.2.3.1.cmml" xref="alg1.l9.m1.1.2.3.1"></union><apply id="alg1.l9.m1.1.2.3.2.cmml" xref="alg1.l9.m1.1.2.3.2"><csymbol cd="ambiguous" id="alg1.l9.m1.1.2.3.2.1.cmml" xref="alg1.l9.m1.1.2.3.2">superscript</csymbol><ci id="alg1.l9.m1.1.2.3.2.2.cmml" xref="alg1.l9.m1.1.2.3.2.2">𝐾</ci><ci id="alg1.l9.m1.1.2.3.2.3.cmml" xref="alg1.l9.m1.1.2.3.2.3">′</ci></apply><set id="alg1.l9.m1.1.2.3.3.1.cmml" xref="alg1.l9.m1.1.2.3.3.2"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><ci id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1">→</ci><ci id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">𝑘</ci></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">K^{\prime}\leftarrow K^{\prime}\cup\{\vec{k}\}</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.1d">italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ← italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∪ { over→ start_ARG italic_k end_ARG }</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l10">     <span class="ltx_text ltx_font_bold" id="alg1.l10.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l10.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_text ltx_font_bold" id="alg1.l11.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l11.2">for</span>
</div>
</div>
</figure>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Our editing algorithm erases the presence of an object from image embeddings by orthogonalizing them with respect to the object’s text embedding.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We present an algorithm, <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p1.1.1">ProjectAway</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.F4.fig1" title="In 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>), that orthogonalizes image representations with respect to text representations in order to erase objects in image captions, applying it to remove objects one at a time and all at once.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.14">Given an image and an object to remove, we edit the latent representations <math alttext="h_{l^{I}}(k_{i})" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><msub id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">h</mi><msup id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.2.cmml">l</mi><mi id="S4.SS1.p2.1.m1.1.1.3.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.3.cmml">I</mi></msup></msub><mo id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.1.m1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.1.m1.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.1.m1.1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.1.1.1.2.cmml">k</mi><mi id="S4.SS1.p2.1.m1.1.1.1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.1.m1.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2"></times><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">ℎ</ci><apply id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3">superscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.2">𝑙</ci><ci id="S4.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3">𝐼</ci></apply></apply><apply id="S4.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1.2">𝑘</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">h_{l^{I}}(k_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> at a hidden layer <math alttext="l^{I}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><msup id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">l</mi><mi id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">I</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝑙</ci><ci id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">l^{I}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT</annotation></semantics></math> across all image embeddings <math alttext="k_{i}" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">k</mi><mi id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑘</ci><ci id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">k_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. We do not modify any latent representations outside of those belonging to image features. We compute the dot product, <math alttext="p" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_p</annotation></semantics></math>, of <math alttext="h_{l^{I}}(k_{i})" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.1"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><msub id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml"><mi id="S4.SS1.p2.5.m5.1.1.3.2" xref="S4.SS1.p2.5.m5.1.1.3.2.cmml">h</mi><msup id="S4.SS1.p2.5.m5.1.1.3.3" xref="S4.SS1.p2.5.m5.1.1.3.3.cmml"><mi id="S4.SS1.p2.5.m5.1.1.3.3.2" xref="S4.SS1.p2.5.m5.1.1.3.3.2.cmml">l</mi><mi id="S4.SS1.p2.5.m5.1.1.3.3.3" xref="S4.SS1.p2.5.m5.1.1.3.3.3.cmml">I</mi></msup></msub><mo id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.5.m5.1.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.5.m5.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.5.m5.1.1.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.5.m5.1.1.1.1.1.2" xref="S4.SS1.p2.5.m5.1.1.1.1.1.2.cmml">k</mi><mi id="S4.SS1.p2.5.m5.1.1.1.1.1.3" xref="S4.SS1.p2.5.m5.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.5.m5.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><times id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2"></times><apply id="S4.SS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.1.1.3.1.cmml" xref="S4.SS1.p2.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.5.m5.1.1.3.2.cmml" xref="S4.SS1.p2.5.m5.1.1.3.2">ℎ</ci><apply id="S4.SS1.p2.5.m5.1.1.3.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.p2.5.m5.1.1.3.3">superscript</csymbol><ci id="S4.SS1.p2.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.p2.5.m5.1.1.3.3.2">𝑙</ci><ci id="S4.SS1.p2.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3.3.3">𝐼</ci></apply></apply><apply id="S4.SS1.p2.5.m5.1.1.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.5.m5.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1.1.2">𝑘</ci><ci id="S4.SS1.p2.5.m5.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">h_{l^{I}}(k_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.1d">italic_h start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> and the object’s text embedding <math alttext="\vec{t}" class="ltx_Math" display="inline" id="S4.SS1.p2.6.m6.1"><semantics id="S4.SS1.p2.6.m6.1a"><mover accent="true" id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">t</mi><mo id="S4.SS1.p2.6.m6.1.1.1" stretchy="false" xref="S4.SS1.p2.6.m6.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><ci id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1">→</ci><ci id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\vec{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.1d">over→ start_ARG italic_t end_ARG</annotation></semantics></math>, subtracting a weighted <math alttext="\vec{t}" class="ltx_Math" display="inline" id="S4.SS1.p2.7.m7.1"><semantics id="S4.SS1.p2.7.m7.1a"><mover accent="true" id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">t</mi><mo id="S4.SS1.p2.7.m7.1.1.1" stretchy="false" xref="S4.SS1.p2.7.m7.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><ci id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1.1">→</ci><ci id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">\vec{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.7.m7.1d">over→ start_ARG italic_t end_ARG</annotation></semantics></math> from <math alttext="h_{l^{I}}(k_{i})" class="ltx_Math" display="inline" id="S4.SS1.p2.8.m8.1"><semantics id="S4.SS1.p2.8.m8.1a"><mrow id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><msub id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml"><mi id="S4.SS1.p2.8.m8.1.1.3.2" xref="S4.SS1.p2.8.m8.1.1.3.2.cmml">h</mi><msup id="S4.SS1.p2.8.m8.1.1.3.3" xref="S4.SS1.p2.8.m8.1.1.3.3.cmml"><mi id="S4.SS1.p2.8.m8.1.1.3.3.2" xref="S4.SS1.p2.8.m8.1.1.3.3.2.cmml">l</mi><mi id="S4.SS1.p2.8.m8.1.1.3.3.3" xref="S4.SS1.p2.8.m8.1.1.3.3.3.cmml">I</mi></msup></msub><mo id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.8.m8.1.1.1.1" xref="S4.SS1.p2.8.m8.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.8.m8.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.8.m8.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.8.m8.1.1.1.1.1" xref="S4.SS1.p2.8.m8.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.8.m8.1.1.1.1.1.2" xref="S4.SS1.p2.8.m8.1.1.1.1.1.2.cmml">k</mi><mi id="S4.SS1.p2.8.m8.1.1.1.1.1.3" xref="S4.SS1.p2.8.m8.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.8.m8.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><times id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2"></times><apply id="S4.SS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.8.m8.1.1.3.1.cmml" xref="S4.SS1.p2.8.m8.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.8.m8.1.1.3.2.cmml" xref="S4.SS1.p2.8.m8.1.1.3.2">ℎ</ci><apply id="S4.SS1.p2.8.m8.1.1.3.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.8.m8.1.1.3.3.1.cmml" xref="S4.SS1.p2.8.m8.1.1.3.3">superscript</csymbol><ci id="S4.SS1.p2.8.m8.1.1.3.3.2.cmml" xref="S4.SS1.p2.8.m8.1.1.3.3.2">𝑙</ci><ci id="S4.SS1.p2.8.m8.1.1.3.3.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3.3.3">𝐼</ci></apply></apply><apply id="S4.SS1.p2.8.m8.1.1.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.8.m8.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.8.m8.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.1.1.1.2">𝑘</ci><ci id="S4.SS1.p2.8.m8.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">h_{l^{I}}(k_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.8.m8.1d">italic_h start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> only if the dot product is positive. At <math alttext="\alpha=1" class="ltx_Math" display="inline" id="S4.SS1.p2.9.m9.1"><semantics id="S4.SS1.p2.9.m9.1a"><mrow id="S4.SS1.p2.9.m9.1.1" xref="S4.SS1.p2.9.m9.1.1.cmml"><mi id="S4.SS1.p2.9.m9.1.1.2" xref="S4.SS1.p2.9.m9.1.1.2.cmml">α</mi><mo id="S4.SS1.p2.9.m9.1.1.1" xref="S4.SS1.p2.9.m9.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.9.m9.1.1.3" xref="S4.SS1.p2.9.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.1b"><apply id="S4.SS1.p2.9.m9.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1"><eq id="S4.SS1.p2.9.m9.1.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1.1"></eq><ci id="S4.SS1.p2.9.m9.1.1.2.cmml" xref="S4.SS1.p2.9.m9.1.1.2">𝛼</ci><cn id="S4.SS1.p2.9.m9.1.1.3.cmml" type="integer" xref="S4.SS1.p2.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.1c">\alpha=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.9.m9.1d">italic_α = 1</annotation></semantics></math>, <span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.SS1.p2.14.1">ProjectAway</span> is equivalent to orthogonalizing the image representations with respect to the text representation. To compute text representation <math alttext="\vec{t}" class="ltx_Math" display="inline" id="S4.SS1.p2.11.m11.1"><semantics id="S4.SS1.p2.11.m11.1a"><mover accent="true" id="S4.SS1.p2.11.m11.1.1" xref="S4.SS1.p2.11.m11.1.1.cmml"><mi id="S4.SS1.p2.11.m11.1.1.2" xref="S4.SS1.p2.11.m11.1.1.2.cmml">t</mi><mo id="S4.SS1.p2.11.m11.1.1.1" stretchy="false" xref="S4.SS1.p2.11.m11.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.11.m11.1b"><apply id="S4.SS1.p2.11.m11.1.1.cmml" xref="S4.SS1.p2.11.m11.1.1"><ci id="S4.SS1.p2.11.m11.1.1.1.cmml" xref="S4.SS1.p2.11.m11.1.1.1">→</ci><ci id="S4.SS1.p2.11.m11.1.1.2.cmml" xref="S4.SS1.p2.11.m11.1.1.2">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.11.m11.1c">\vec{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.11.m11.1d">over→ start_ARG italic_t end_ARG</annotation></semantics></math>, we pass the object (e.g. “hot dog”) into the VLM’s text model and extract <math alttext="h_{l^{T}}(t_{\text{-1}})" class="ltx_Math" display="inline" id="S4.SS1.p2.12.m12.1"><semantics id="S4.SS1.p2.12.m12.1a"><mrow id="S4.SS1.p2.12.m12.1.1" xref="S4.SS1.p2.12.m12.1.1.cmml"><msub id="S4.SS1.p2.12.m12.1.1.3" xref="S4.SS1.p2.12.m12.1.1.3.cmml"><mi id="S4.SS1.p2.12.m12.1.1.3.2" xref="S4.SS1.p2.12.m12.1.1.3.2.cmml">h</mi><msup id="S4.SS1.p2.12.m12.1.1.3.3" xref="S4.SS1.p2.12.m12.1.1.3.3.cmml"><mi id="S4.SS1.p2.12.m12.1.1.3.3.2" xref="S4.SS1.p2.12.m12.1.1.3.3.2.cmml">l</mi><mi id="S4.SS1.p2.12.m12.1.1.3.3.3" xref="S4.SS1.p2.12.m12.1.1.3.3.3.cmml">T</mi></msup></msub><mo id="S4.SS1.p2.12.m12.1.1.2" xref="S4.SS1.p2.12.m12.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.12.m12.1.1.1.1" xref="S4.SS1.p2.12.m12.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.12.m12.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.12.m12.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.12.m12.1.1.1.1.1" xref="S4.SS1.p2.12.m12.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.12.m12.1.1.1.1.1.2" xref="S4.SS1.p2.12.m12.1.1.1.1.1.2.cmml">t</mi><mtext id="S4.SS1.p2.12.m12.1.1.1.1.1.3" xref="S4.SS1.p2.12.m12.1.1.1.1.1.3a.cmml">-1</mtext></msub><mo id="S4.SS1.p2.12.m12.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.12.m12.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.12.m12.1b"><apply id="S4.SS1.p2.12.m12.1.1.cmml" xref="S4.SS1.p2.12.m12.1.1"><times id="S4.SS1.p2.12.m12.1.1.2.cmml" xref="S4.SS1.p2.12.m12.1.1.2"></times><apply id="S4.SS1.p2.12.m12.1.1.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m12.1.1.3.1.cmml" xref="S4.SS1.p2.12.m12.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.12.m12.1.1.3.2.cmml" xref="S4.SS1.p2.12.m12.1.1.3.2">ℎ</ci><apply id="S4.SS1.p2.12.m12.1.1.3.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m12.1.1.3.3.1.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3">superscript</csymbol><ci id="S4.SS1.p2.12.m12.1.1.3.3.2.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.2">𝑙</ci><ci id="S4.SS1.p2.12.m12.1.1.3.3.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3">𝑇</ci></apply></apply><apply id="S4.SS1.p2.12.m12.1.1.1.1.1.cmml" xref="S4.SS1.p2.12.m12.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m12.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.12.m12.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.12.m12.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.12.m12.1.1.1.1.1.2">𝑡</ci><ci id="S4.SS1.p2.12.m12.1.1.1.1.1.3a.cmml" xref="S4.SS1.p2.12.m12.1.1.1.1.1.3"><mtext id="S4.SS1.p2.12.m12.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p2.12.m12.1.1.1.1.1.3">-1</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.12.m12.1c">h_{l^{T}}(t_{\text{-1}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.12.m12.1d">italic_h start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT -1 end_POSTSUBSCRIPT )</annotation></semantics></math> at hidden layer <math alttext="l^{T}" class="ltx_Math" display="inline" id="S4.SS1.p2.13.m13.1"><semantics id="S4.SS1.p2.13.m13.1a"><msup id="S4.SS1.p2.13.m13.1.1" xref="S4.SS1.p2.13.m13.1.1.cmml"><mi id="S4.SS1.p2.13.m13.1.1.2" xref="S4.SS1.p2.13.m13.1.1.2.cmml">l</mi><mi id="S4.SS1.p2.13.m13.1.1.3" xref="S4.SS1.p2.13.m13.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.13.m13.1b"><apply id="S4.SS1.p2.13.m13.1.1.cmml" xref="S4.SS1.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.13.m13.1.1.1.cmml" xref="S4.SS1.p2.13.m13.1.1">superscript</csymbol><ci id="S4.SS1.p2.13.m13.1.1.2.cmml" xref="S4.SS1.p2.13.m13.1.1.2">𝑙</ci><ci id="S4.SS1.p2.13.m13.1.1.3.cmml" xref="S4.SS1.p2.13.m13.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.13.m13.1c">l^{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.13.m13.1d">italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="t_{\text{-1}}" class="ltx_Math" display="inline" id="S4.SS1.p2.14.m14.1"><semantics id="S4.SS1.p2.14.m14.1a"><msub id="S4.SS1.p2.14.m14.1.1" xref="S4.SS1.p2.14.m14.1.1.cmml"><mi id="S4.SS1.p2.14.m14.1.1.2" xref="S4.SS1.p2.14.m14.1.1.2.cmml">t</mi><mtext id="S4.SS1.p2.14.m14.1.1.3" xref="S4.SS1.p2.14.m14.1.1.3a.cmml">-1</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.14.m14.1b"><apply id="S4.SS1.p2.14.m14.1.1.cmml" xref="S4.SS1.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.14.m14.1.1.1.cmml" xref="S4.SS1.p2.14.m14.1.1">subscript</csymbol><ci id="S4.SS1.p2.14.m14.1.1.2.cmml" xref="S4.SS1.p2.14.m14.1.1.2">𝑡</ci><ci id="S4.SS1.p2.14.m14.1.1.3a.cmml" xref="S4.SS1.p2.14.m14.1.1.3"><mtext id="S4.SS1.p2.14.m14.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p2.14.m14.1.1.3">-1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.14.m14.1c">t_{\text{-1}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.14.m14.1d">italic_t start_POSTSUBSCRIPT -1 end_POSTSUBSCRIPT</annotation></semantics></math> is the last token of the object. We use the last token of the object to capture the whole of the object’s meaning.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Removing objects one by one</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We evaluate the <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.p1.1.1">ProjectAway</span> algorithm’s effectiveness at erasing individual objects from captions across multiple images and objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.4.1">Experimental setting.</span> We apply <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.p2.4.2">ProjectAway</span> on 5000 random images from the COCO2014 training set on all mentioned COCO objects (i.e. hallucination and CD) individually and measure the removal rate at which objects no longer appear in the caption. For InstructBLIP, we set <math alttext="(l^{I},l^{T},\alpha)=(1,2,1.5)" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.1.m1.6"><semantics id="S4.SS1.SSS1.p2.1.m1.6a"><mrow id="S4.SS1.SSS1.p2.1.m1.6.6" xref="S4.SS1.SSS1.p2.1.m1.6.6.cmml"><mrow id="S4.SS1.SSS1.p2.1.m1.6.6.2.2" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.3.cmml"><mo id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.3" stretchy="false" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.3.cmml">(</mo><msup id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.cmml"><mi id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.2" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.2.cmml">l</mi><mi id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.3" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.3.cmml">I</mi></msup><mo id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.4" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.3.cmml">,</mo><msup id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.cmml"><mi id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.2" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.2.cmml">l</mi><mi id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.3" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.3.cmml">T</mi></msup><mo id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.5" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.3.cmml">,</mo><mi id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">α</mi><mo id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.6" stretchy="false" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.3.cmml">)</mo></mrow><mo id="S4.SS1.SSS1.p2.1.m1.6.6.3" xref="S4.SS1.SSS1.p2.1.m1.6.6.3.cmml">=</mo><mrow id="S4.SS1.SSS1.p2.1.m1.6.6.4.2" xref="S4.SS1.SSS1.p2.1.m1.6.6.4.1.cmml"><mo id="S4.SS1.SSS1.p2.1.m1.6.6.4.2.1" stretchy="false" xref="S4.SS1.SSS1.p2.1.m1.6.6.4.1.cmml">(</mo><mn id="S4.SS1.SSS1.p2.1.m1.2.2" xref="S4.SS1.SSS1.p2.1.m1.2.2.cmml">1</mn><mo id="S4.SS1.SSS1.p2.1.m1.6.6.4.2.2" xref="S4.SS1.SSS1.p2.1.m1.6.6.4.1.cmml">,</mo><mn id="S4.SS1.SSS1.p2.1.m1.3.3" xref="S4.SS1.SSS1.p2.1.m1.3.3.cmml">2</mn><mo id="S4.SS1.SSS1.p2.1.m1.6.6.4.2.3" xref="S4.SS1.SSS1.p2.1.m1.6.6.4.1.cmml">,</mo><mn id="S4.SS1.SSS1.p2.1.m1.4.4" xref="S4.SS1.SSS1.p2.1.m1.4.4.cmml">1.5</mn><mo id="S4.SS1.SSS1.p2.1.m1.6.6.4.2.4" stretchy="false" xref="S4.SS1.SSS1.p2.1.m1.6.6.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.6b"><apply id="S4.SS1.SSS1.p2.1.m1.6.6.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6"><eq id="S4.SS1.SSS1.p2.1.m1.6.6.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.3"></eq><vector id="S4.SS1.SSS1.p2.1.m1.6.6.2.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2"><apply id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1">superscript</csymbol><ci id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.2">𝑙</ci><ci id="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.5.5.1.1.1.3">𝐼</ci></apply><apply id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2">superscript</csymbol><ci id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.2">𝑙</ci><ci id="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.2.2.2.3">𝑇</ci></apply><ci id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">𝛼</ci></vector><vector id="S4.SS1.SSS1.p2.1.m1.6.6.4.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.6.6.4.2"><cn id="S4.SS1.SSS1.p2.1.m1.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p2.1.m1.2.2">1</cn><cn id="S4.SS1.SSS1.p2.1.m1.3.3.cmml" type="integer" xref="S4.SS1.SSS1.p2.1.m1.3.3">2</cn><cn id="S4.SS1.SSS1.p2.1.m1.4.4.cmml" type="float" xref="S4.SS1.SSS1.p2.1.m1.4.4">1.5</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.6c">(l^{I},l^{T},\alpha)=(1,2,1.5)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.1.m1.6d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , italic_α ) = ( 1 , 2 , 1.5 )</annotation></semantics></math>. For LLaVA, we set <math alttext="(l^{I},l^{T},\alpha)=(19,21,3.5)" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.2.m2.6"><semantics id="S4.SS1.SSS1.p2.2.m2.6a"><mrow id="S4.SS1.SSS1.p2.2.m2.6.6" xref="S4.SS1.SSS1.p2.2.m2.6.6.cmml"><mrow id="S4.SS1.SSS1.p2.2.m2.6.6.2.2" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.3.cmml"><mo id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.3" stretchy="false" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.3.cmml">(</mo><msup id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.cmml"><mi id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.2" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.2.cmml">l</mi><mi id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.3" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.3.cmml">I</mi></msup><mo id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.4" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.3.cmml">,</mo><msup id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.cmml"><mi id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.2" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.2.cmml">l</mi><mi id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.3" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.3.cmml">T</mi></msup><mo id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.5" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.3.cmml">,</mo><mi id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml">α</mi><mo id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.6" stretchy="false" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.3.cmml">)</mo></mrow><mo id="S4.SS1.SSS1.p2.2.m2.6.6.3" xref="S4.SS1.SSS1.p2.2.m2.6.6.3.cmml">=</mo><mrow id="S4.SS1.SSS1.p2.2.m2.6.6.4.2" xref="S4.SS1.SSS1.p2.2.m2.6.6.4.1.cmml"><mo id="S4.SS1.SSS1.p2.2.m2.6.6.4.2.1" stretchy="false" xref="S4.SS1.SSS1.p2.2.m2.6.6.4.1.cmml">(</mo><mn id="S4.SS1.SSS1.p2.2.m2.2.2" xref="S4.SS1.SSS1.p2.2.m2.2.2.cmml">19</mn><mo id="S4.SS1.SSS1.p2.2.m2.6.6.4.2.2" xref="S4.SS1.SSS1.p2.2.m2.6.6.4.1.cmml">,</mo><mn id="S4.SS1.SSS1.p2.2.m2.3.3" xref="S4.SS1.SSS1.p2.2.m2.3.3.cmml">21</mn><mo id="S4.SS1.SSS1.p2.2.m2.6.6.4.2.3" xref="S4.SS1.SSS1.p2.2.m2.6.6.4.1.cmml">,</mo><mn id="S4.SS1.SSS1.p2.2.m2.4.4" xref="S4.SS1.SSS1.p2.2.m2.4.4.cmml">3.5</mn><mo id="S4.SS1.SSS1.p2.2.m2.6.6.4.2.4" stretchy="false" xref="S4.SS1.SSS1.p2.2.m2.6.6.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.6b"><apply id="S4.SS1.SSS1.p2.2.m2.6.6.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6"><eq id="S4.SS1.SSS1.p2.2.m2.6.6.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.3"></eq><vector id="S4.SS1.SSS1.p2.2.m2.6.6.2.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2"><apply id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1">superscript</csymbol><ci id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.2">𝑙</ci><ci id="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.5.5.1.1.1.3">𝐼</ci></apply><apply id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2">superscript</csymbol><ci id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.2.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.2">𝑙</ci><ci id="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.2.2.2.3">𝑇</ci></apply><ci id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1">𝛼</ci></vector><vector id="S4.SS1.SSS1.p2.2.m2.6.6.4.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.6.6.4.2"><cn id="S4.SS1.SSS1.p2.2.m2.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p2.2.m2.2.2">19</cn><cn id="S4.SS1.SSS1.p2.2.m2.3.3.cmml" type="integer" xref="S4.SS1.SSS1.p2.2.m2.3.3">21</cn><cn id="S4.SS1.SSS1.p2.2.m2.4.4.cmml" type="float" xref="S4.SS1.SSS1.p2.2.m2.4.4">3.5</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.6c">(l^{I},l^{T},\alpha)=(19,21,3.5)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.2.m2.6d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , italic_α ) = ( 19 , 21 , 3.5 )</annotation></semantics></math>. These parameters are fixed irrespective of image and are chosen for their maximal effect (see ablations in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS2" title="4.2 Ablation Study: mass-removing hallucinations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). To differentiate hallucinations from CD, we compute CHAIR <cite class="ltx_cite ltx_citemacro_citep">(Rohrbach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib40" title="">2019</a>)</cite>, an evaluation criteria that compares model-generated captions to ground-truth human annotations. CHAIR provides two main scores, <math alttext="\text{CHAIR}_{I}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.3.m3.1"><semantics id="S4.SS1.SSS1.p2.3.m3.1a"><msub id="S4.SS1.SSS1.p2.3.m3.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.cmml"><mtext id="S4.SS1.SSS1.p2.3.m3.1.1.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.2a.cmml">CHAIR</mtext><mi id="S4.SS1.SSS1.p2.3.m3.1.1.3" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.3.m3.1b"><apply id="S4.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.3.m3.1.1.2a.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.2"><mtext id="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.2">CHAIR</mtext></ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.3.m3.1c">\text{CHAIR}_{I}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.3.m3.1d">CHAIR start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\text{CHAIR}_{S}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.4.m4.1"><semantics id="S4.SS1.SSS1.p2.4.m4.1a"><msub id="S4.SS1.SSS1.p2.4.m4.1.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.cmml"><mtext id="S4.SS1.SSS1.p2.4.m4.1.1.2" xref="S4.SS1.SSS1.p2.4.m4.1.1.2a.cmml">CHAIR</mtext><mi id="S4.SS1.SSS1.p2.4.m4.1.1.3" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.4.m4.1b"><apply id="S4.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.4.m4.1.1.2a.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.2"><mtext id="S4.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.2">CHAIR</mtext></ci><ci id="S4.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.4.m4.1c">\text{CHAIR}_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.4.m4.1d">CHAIR start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>, that quantify hallucinations for instances and sentences, respectively:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{CHAIR}_{S}=\frac{|\{\text{captions with hallucinated objects}\}|}{|\{%
\text{all captions}\}|},\text{CHAIR}_{I}=\frac{|\{\text{hallucinated objects}%
\}|}{|\{\text{all objects mentioned}\}|}" class="ltx_Math" display="block" id="S4.E3.m1.10"><semantics id="S4.E3.m1.10a"><mrow id="S4.E3.m1.10.10.2" xref="S4.E3.m1.10.10.3.cmml"><mrow id="S4.E3.m1.9.9.1.1" xref="S4.E3.m1.9.9.1.1.cmml"><msub id="S4.E3.m1.9.9.1.1.2" xref="S4.E3.m1.9.9.1.1.2.cmml"><mtext id="S4.E3.m1.9.9.1.1.2.2" xref="S4.E3.m1.9.9.1.1.2.2a.cmml">CHAIR</mtext><mi id="S4.E3.m1.9.9.1.1.2.3" xref="S4.E3.m1.9.9.1.1.2.3.cmml">S</mi></msub><mo id="S4.E3.m1.9.9.1.1.1" xref="S4.E3.m1.9.9.1.1.1.cmml">=</mo><mfrac id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml"><mrow id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.3.cmml"><mo id="S4.E3.m1.2.2.2.2.2" stretchy="false" xref="S4.E3.m1.2.2.2.3.1.cmml">|</mo><mrow id="S4.E3.m1.2.2.2.2.1.2" xref="S4.E3.m1.2.2.2.2.1.1.cmml"><mo id="S4.E3.m1.2.2.2.2.1.2.1" stretchy="false" xref="S4.E3.m1.2.2.2.2.1.1.cmml">{</mo><mtext id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1a.cmml">captions with hallucinated objects</mtext><mo id="S4.E3.m1.2.2.2.2.1.2.2" stretchy="false" xref="S4.E3.m1.2.2.2.2.1.1.cmml">}</mo></mrow><mo id="S4.E3.m1.2.2.2.2.3" stretchy="false" xref="S4.E3.m1.2.2.2.3.1.cmml">|</mo></mrow><mrow id="S4.E3.m1.4.4.4.2" xref="S4.E3.m1.4.4.4.3.cmml"><mo id="S4.E3.m1.4.4.4.2.2" stretchy="false" xref="S4.E3.m1.4.4.4.3.1.cmml">|</mo><mrow id="S4.E3.m1.4.4.4.2.1.2" xref="S4.E3.m1.4.4.4.2.1.1.cmml"><mo id="S4.E3.m1.4.4.4.2.1.2.1" stretchy="false" xref="S4.E3.m1.4.4.4.2.1.1.cmml">{</mo><mtext id="S4.E3.m1.3.3.3.1" xref="S4.E3.m1.3.3.3.1a.cmml">all captions</mtext><mo id="S4.E3.m1.4.4.4.2.1.2.2" stretchy="false" xref="S4.E3.m1.4.4.4.2.1.1.cmml">}</mo></mrow><mo id="S4.E3.m1.4.4.4.2.3" stretchy="false" xref="S4.E3.m1.4.4.4.3.1.cmml">|</mo></mrow></mfrac></mrow><mo id="S4.E3.m1.10.10.2.3" xref="S4.E3.m1.10.10.3a.cmml">,</mo><mrow id="S4.E3.m1.10.10.2.2" xref="S4.E3.m1.10.10.2.2.cmml"><msub id="S4.E3.m1.10.10.2.2.2" xref="S4.E3.m1.10.10.2.2.2.cmml"><mtext id="S4.E3.m1.10.10.2.2.2.2" xref="S4.E3.m1.10.10.2.2.2.2a.cmml">CHAIR</mtext><mi id="S4.E3.m1.10.10.2.2.2.3" xref="S4.E3.m1.10.10.2.2.2.3.cmml">I</mi></msub><mo id="S4.E3.m1.10.10.2.2.1" xref="S4.E3.m1.10.10.2.2.1.cmml">=</mo><mfrac id="S4.E3.m1.8.8" xref="S4.E3.m1.8.8.cmml"><mrow id="S4.E3.m1.6.6.2.2" xref="S4.E3.m1.6.6.2.3.cmml"><mo id="S4.E3.m1.6.6.2.2.2" stretchy="false" xref="S4.E3.m1.6.6.2.3.1.cmml">|</mo><mrow id="S4.E3.m1.6.6.2.2.1.2" xref="S4.E3.m1.6.6.2.2.1.1.cmml"><mo id="S4.E3.m1.6.6.2.2.1.2.1" stretchy="false" xref="S4.E3.m1.6.6.2.2.1.1.cmml">{</mo><mtext id="S4.E3.m1.5.5.1.1" xref="S4.E3.m1.5.5.1.1a.cmml">hallucinated objects</mtext><mo id="S4.E3.m1.6.6.2.2.1.2.2" stretchy="false" xref="S4.E3.m1.6.6.2.2.1.1.cmml">}</mo></mrow><mo id="S4.E3.m1.6.6.2.2.3" stretchy="false" xref="S4.E3.m1.6.6.2.3.1.cmml">|</mo></mrow><mrow id="S4.E3.m1.8.8.4.2" xref="S4.E3.m1.8.8.4.3.cmml"><mo id="S4.E3.m1.8.8.4.2.2" stretchy="false" xref="S4.E3.m1.8.8.4.3.1.cmml">|</mo><mrow id="S4.E3.m1.8.8.4.2.1.2" xref="S4.E3.m1.8.8.4.2.1.1.cmml"><mo id="S4.E3.m1.8.8.4.2.1.2.1" stretchy="false" xref="S4.E3.m1.8.8.4.2.1.1.cmml">{</mo><mtext id="S4.E3.m1.7.7.3.1" xref="S4.E3.m1.7.7.3.1a.cmml">all objects mentioned</mtext><mo id="S4.E3.m1.8.8.4.2.1.2.2" stretchy="false" xref="S4.E3.m1.8.8.4.2.1.1.cmml">}</mo></mrow><mo id="S4.E3.m1.8.8.4.2.3" stretchy="false" xref="S4.E3.m1.8.8.4.3.1.cmml">|</mo></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.10b"><apply id="S4.E3.m1.10.10.3.cmml" xref="S4.E3.m1.10.10.2"><csymbol cd="ambiguous" id="S4.E3.m1.10.10.3a.cmml" xref="S4.E3.m1.10.10.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.9.9.1.1.cmml" xref="S4.E3.m1.9.9.1.1"><eq id="S4.E3.m1.9.9.1.1.1.cmml" xref="S4.E3.m1.9.9.1.1.1"></eq><apply id="S4.E3.m1.9.9.1.1.2.cmml" xref="S4.E3.m1.9.9.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.9.9.1.1.2.1.cmml" xref="S4.E3.m1.9.9.1.1.2">subscript</csymbol><ci id="S4.E3.m1.9.9.1.1.2.2a.cmml" xref="S4.E3.m1.9.9.1.1.2.2"><mtext id="S4.E3.m1.9.9.1.1.2.2.cmml" xref="S4.E3.m1.9.9.1.1.2.2">CHAIR</mtext></ci><ci id="S4.E3.m1.9.9.1.1.2.3.cmml" xref="S4.E3.m1.9.9.1.1.2.3">𝑆</ci></apply><apply id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4"><divide id="S4.E3.m1.4.4.5.cmml" xref="S4.E3.m1.4.4"></divide><apply id="S4.E3.m1.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2"><abs id="S4.E3.m1.2.2.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2"></abs><set id="S4.E3.m1.2.2.2.2.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.2"><ci id="S4.E3.m1.1.1.1.1a.cmml" xref="S4.E3.m1.1.1.1.1"><mtext id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1">captions with hallucinated objects</mtext></ci></set></apply><apply id="S4.E3.m1.4.4.4.3.cmml" xref="S4.E3.m1.4.4.4.2"><abs id="S4.E3.m1.4.4.4.3.1.cmml" xref="S4.E3.m1.4.4.4.2.2"></abs><set id="S4.E3.m1.4.4.4.2.1.1.cmml" xref="S4.E3.m1.4.4.4.2.1.2"><ci id="S4.E3.m1.3.3.3.1a.cmml" xref="S4.E3.m1.3.3.3.1"><mtext id="S4.E3.m1.3.3.3.1.cmml" xref="S4.E3.m1.3.3.3.1">all captions</mtext></ci></set></apply></apply></apply><apply id="S4.E3.m1.10.10.2.2.cmml" xref="S4.E3.m1.10.10.2.2"><eq id="S4.E3.m1.10.10.2.2.1.cmml" xref="S4.E3.m1.10.10.2.2.1"></eq><apply id="S4.E3.m1.10.10.2.2.2.cmml" xref="S4.E3.m1.10.10.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.10.10.2.2.2.1.cmml" xref="S4.E3.m1.10.10.2.2.2">subscript</csymbol><ci id="S4.E3.m1.10.10.2.2.2.2a.cmml" xref="S4.E3.m1.10.10.2.2.2.2"><mtext id="S4.E3.m1.10.10.2.2.2.2.cmml" xref="S4.E3.m1.10.10.2.2.2.2">CHAIR</mtext></ci><ci id="S4.E3.m1.10.10.2.2.2.3.cmml" xref="S4.E3.m1.10.10.2.2.2.3">𝐼</ci></apply><apply id="S4.E3.m1.8.8.cmml" xref="S4.E3.m1.8.8"><divide id="S4.E3.m1.8.8.5.cmml" xref="S4.E3.m1.8.8"></divide><apply id="S4.E3.m1.6.6.2.3.cmml" xref="S4.E3.m1.6.6.2.2"><abs id="S4.E3.m1.6.6.2.3.1.cmml" xref="S4.E3.m1.6.6.2.2.2"></abs><set id="S4.E3.m1.6.6.2.2.1.1.cmml" xref="S4.E3.m1.6.6.2.2.1.2"><ci id="S4.E3.m1.5.5.1.1a.cmml" xref="S4.E3.m1.5.5.1.1"><mtext id="S4.E3.m1.5.5.1.1.cmml" xref="S4.E3.m1.5.5.1.1">hallucinated objects</mtext></ci></set></apply><apply id="S4.E3.m1.8.8.4.3.cmml" xref="S4.E3.m1.8.8.4.2"><abs id="S4.E3.m1.8.8.4.3.1.cmml" xref="S4.E3.m1.8.8.4.2.2"></abs><set id="S4.E3.m1.8.8.4.2.1.1.cmml" xref="S4.E3.m1.8.8.4.2.1.2"><ci id="S4.E3.m1.7.7.3.1a.cmml" xref="S4.E3.m1.7.7.3.1"><mtext id="S4.E3.m1.7.7.3.1.cmml" xref="S4.E3.m1.7.7.3.1">all objects mentioned</mtext></ci></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.10c">\text{CHAIR}_{S}=\frac{|\{\text{captions with hallucinated objects}\}|}{|\{%
\text{all captions}\}|},\text{CHAIR}_{I}=\frac{|\{\text{hallucinated objects}%
\}|}{|\{\text{all objects mentioned}\}|}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.10d">CHAIR start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = divide start_ARG | { captions with hallucinated objects } | end_ARG start_ARG | { all captions } | end_ARG , CHAIR start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT = divide start_ARG | { hallucinated objects } | end_ARG start_ARG | { all objects mentioned } | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">Results.</span> <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.T1" title="In 4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows that <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.p3.1.2">ProjectAway</span> is significantly more effective in erasing individual hallucinated objects at an individual level than CD objects for both InstructBLIP and LLaVA. Along with the insight that hallucinated objects have lower softmax scores (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.F2" title="In 3.2 Applying Logit Lens on VLMs ‣ 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>), these results suggest that hallucinated objects manifest more weakly in image embeddings and are hence easier to remove than CD objects.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Mass-removing objects</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We iteratively apply <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS2.p1.1.1">ProjectAway</span> to a <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.2">set</span> of objects, following the same experimental setup and observing similarly different removal rates for hallucinated objects and CD objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">Mass-removing hallucinations.</span> We mass-remove hallucinations identified with ground truth annotations using <span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.SS1.SSS2.p2.1.2">ProjectAway</span>. <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.T1" title="In 4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows that editing out all the hallucinations of an image yields a similar removal rate as individually editing out and, importantly, that erasing hallucinated objects together does not interfere with each other. We achieve a hallucination reduction rate of 41.3% for InstructBLIP and 23.3% for LLaVA (see <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.T4" title="In A.1 Mass-removing objects ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>). Recall count slightly <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.3">increases</span> for both models, indicating that caption accuracy is preserved. This may be because removed hallucinations are replaced with objects the model is more confident in. Qualitative results are in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.F5" title="In 4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S4.T1.19.1">Removing mentioned objects individually &amp; in-mass.</span> Using <span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.T1.20.2">ProjectAway</span>, we remove hallucinated objects and observe high hallucination reduction with CHAIR, mass-removal rate (Mass RR), and individual removal rate (Individual RR). We also remove correctly detected (CD) objects but find that they are more resistant to linear editing. Denote CHAIR<sub class="ltx_sub" id="S4.T1.21.3"><span class="ltx_text ltx_font_italic" id="S4.T1.21.3.1">S</span></sub> as <math alttext="C_{S}" class="ltx_Math" display="inline" id="S4.T1.8.m3.1"><semantics id="S4.T1.8.m3.1b"><msub id="S4.T1.8.m3.1.1" xref="S4.T1.8.m3.1.1.cmml"><mi id="S4.T1.8.m3.1.1.2" xref="S4.T1.8.m3.1.1.2.cmml">C</mi><mi id="S4.T1.8.m3.1.1.3" xref="S4.T1.8.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.8.m3.1c"><apply id="S4.T1.8.m3.1.1.cmml" xref="S4.T1.8.m3.1.1"><csymbol cd="ambiguous" id="S4.T1.8.m3.1.1.1.cmml" xref="S4.T1.8.m3.1.1">subscript</csymbol><ci id="S4.T1.8.m3.1.1.2.cmml" xref="S4.T1.8.m3.1.1.2">𝐶</ci><ci id="S4.T1.8.m3.1.1.3.cmml" xref="S4.T1.8.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.m3.1d">C_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.m3.1e">italic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> and CHAIR<sub class="ltx_sub" id="S4.T1.22.4"><span class="ltx_text ltx_font_italic" id="S4.T1.22.4.1">I</span></sub> as <math alttext="C_{I}" class="ltx_Math" display="inline" id="S4.T1.10.m5.1"><semantics id="S4.T1.10.m5.1b"><msub id="S4.T1.10.m5.1.1" xref="S4.T1.10.m5.1.1.cmml"><mi id="S4.T1.10.m5.1.1.2" xref="S4.T1.10.m5.1.1.2.cmml">C</mi><mi id="S4.T1.10.m5.1.1.3" xref="S4.T1.10.m5.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.10.m5.1c"><apply id="S4.T1.10.m5.1.1.cmml" xref="S4.T1.10.m5.1.1"><csymbol cd="ambiguous" id="S4.T1.10.m5.1.1.1.cmml" xref="S4.T1.10.m5.1.1">subscript</csymbol><ci id="S4.T1.10.m5.1.1.2.cmml" xref="S4.T1.10.m5.1.1.2">𝐶</ci><ci id="S4.T1.10.m5.1.1.3.cmml" xref="S4.T1.10.m5.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.m5.1d">C_{I}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.m5.1e">italic_C start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.14.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.14.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T1.14.4.4.5.1">Edit Scope</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.14.4.4.6.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.4.4.7"><span class="ltx_text ltx_font_bold" id="S4.T1.14.4.4.7.1">Individual RR (%)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.4.4.8"><span class="ltx_text ltx_font_bold" id="S4.T1.14.4.4.8.1">Mass RR (%)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.4.4.9"><span class="ltx_text ltx_font_bold" id="S4.T1.14.4.4.9.1">CD change (%)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.12.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.12.2.2.2.2">C<sub class="ltx_sub" id="S4.T1.12.2.2.2.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.12.2.2.2.2.1.1">i</span></sub> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.12.2.2.2.2.m2.1"><semantics id="S4.T1.12.2.2.2.2.m2.1a"><mo id="S4.T1.12.2.2.2.2.m2.1.1" stretchy="false" xref="S4.T1.12.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.2.2.2.2.m2.1b"><ci id="S4.T1.12.2.2.2.2.m2.1.1.cmml" xref="S4.T1.12.2.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.2.2.2.2.m2.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.14.4.4.4.2">C<sub class="ltx_sub" id="S4.T1.14.4.4.4.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.14.4.4.4.2.1.1">s</span></sub> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.14.4.4.4.2.m2.1"><semantics id="S4.T1.14.4.4.4.2.m2.1a"><mo id="S4.T1.14.4.4.4.2.m2.1.1" stretchy="false" xref="S4.T1.14.4.4.4.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.4.4.4.2.m2.1b"><ci id="S4.T1.14.4.4.4.2.m2.1.1.cmml" xref="S4.T1.14.4.4.4.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.4.4.4.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.4.4.4.2.m2.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.14.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.14.4.5.1.1.1">No edits</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.2">InstructBLIP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.6">15.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.5.1.7">54.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.4.6.2">
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.6.2.1">LLaVA</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.6.2.2">-</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.6.2.3">-</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.6.2.4">-</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.6.2.5">14.6</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.6.2.6">51.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.1" rowspan="2"><span class="ltx_text" id="S4.T1.14.4.7.3.1.1">Hallucinations</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.2">InstructBLIP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.3">83.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.4">74.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.5">+0.07</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.6">8.94</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.7.3.7">33.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.4.8.4">
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.8.4.1">LLaVA</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.8.4.2">86.0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.8.4.3">72.8</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.8.4.4">+0.01</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.8.4.5">11.2</td>
<td class="ltx_td ltx_align_left" id="S4.T1.14.4.8.4.6">35.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.4.9.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.14.4.9.5.1" rowspan="2"><span class="ltx_text" id="S4.T1.14.4.9.5.1.1">CD</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.9.5.2">InstructBLIP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.9.5.3">16.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.9.5.4">15.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.9.5.5">-2.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.9.5.6">17.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.14.4.9.5.7">58.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.4.10.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.4.10.6.1">LLaVA</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.4.10.6.2">6.9</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.4.10.6.3">8.3</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.4.10.6.4">-1.6</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.4.10.6.5">15.2</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.4.10.6.6">52.4</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="483" id="S4.F5.g1" src="x4.png" width="761"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S4.F5.3.1">Qualitative results for mass object removal.</span> We present example images and their captions after mass-removing hallucinations (red) with <span class="ltx_text ltx_font_smallcaps" id="S4.F5.4.2">ProjectAway.</span>, which can effectively remove hallucinations while preserving, even increasing, correctly detected objects (green).
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F5.5">.</p>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p3.1.1">Mass removing CD.</span> We similarly find that applying <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS2.p3.1.2">ProjectAway</span> can successfully remove CD objects when edited all together in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.T1" title="In 4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Furthermore, CHAIR scores minimally change, which indicates that this mass-removal merely erases object presence without eroding caption accuracy. While the removal rate is lower than for hallucinated objects, this insight proves useful when we apply <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS2.p3.1.3">ProjectAway</span> for hallucination reduction in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS2" title="5.2 Hallucination Removal ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study: mass-removing hallucinations</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We perform ablations on parameters of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.1.1">ProjectAway</span> to improve object removal rate for erasing hallucinations in-mass.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.5"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.5.1">Experimental setting.</span> We ablate the three parameters of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.5.2">ProjectAway</span>: layer <math alttext="l^{I}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><msup id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">l</mi><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">I</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑙</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">l^{I}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT</annotation></semantics></math> to edit at, layer <math alttext="l^{T}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><msup id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">l</mi><mi id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑙</ci><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">l^{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> to retrieve the text representation, and weight factor <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_α</annotation></semantics></math>. At <math alttext="l^{T}=-1" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><msup id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2.2" xref="S4.SS2.p2.4.m4.1.1.2.2.cmml">l</mi><mi id="S4.SS2.p2.4.m4.1.1.2.3" xref="S4.SS2.p2.4.m4.1.1.2.3.cmml">T</mi></msup><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml"><mo id="S4.SS2.p2.4.m4.1.1.3a" xref="S4.SS2.p2.4.m4.1.1.3.cmml">−</mo><mn id="S4.SS2.p2.4.m4.1.1.3.2" xref="S4.SS2.p2.4.m4.1.1.3.2.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><eq id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></eq><apply id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.2.1.cmml" xref="S4.SS2.p2.4.m4.1.1.2">superscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.2.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2.2">𝑙</ci><ci id="S4.SS2.p2.4.m4.1.1.2.3.cmml" xref="S4.SS2.p2.4.m4.1.1.2.3">𝑇</ci></apply><apply id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3"><minus id="S4.SS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3"></minus><cn id="S4.SS2.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S4.SS2.p2.4.m4.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">l^{T}=-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = - 1</annotation></semantics></math>, we average together the object’s constituent token embeddings. At <math alttext="l^{I}=-1" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5.1"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><msup id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2.2" xref="S4.SS2.p2.5.m5.1.1.2.2.cmml">l</mi><mi id="S4.SS2.p2.5.m5.1.1.2.3" xref="S4.SS2.p2.5.m5.1.1.2.3.cmml">I</mi></msup><mo id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml"><mo id="S4.SS2.p2.5.m5.1.1.3a" xref="S4.SS2.p2.5.m5.1.1.3.cmml">−</mo><mn id="S4.SS2.p2.5.m5.1.1.3.2" xref="S4.SS2.p2.5.m5.1.1.3.2.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><eq id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></eq><apply id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.2.1.cmml" xref="S4.SS2.p2.5.m5.1.1.2">superscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.2.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2.2">𝑙</ci><ci id="S4.SS2.p2.5.m5.1.1.2.3.cmml" xref="S4.SS2.p2.5.m5.1.1.2.3">𝐼</ci></apply><apply id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3"><minus id="S4.SS2.p2.5.m5.1.1.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3"></minus><cn id="S4.SS2.p2.5.m5.1.1.3.2.cmml" type="integer" xref="S4.SS2.p2.5.m5.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">l^{I}=-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.1d">italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT = - 1</annotation></semantics></math>, we edit the image embeddings directly inputted to the text model. We evaluate across 500 training samples from COCO 2014 that have at least one hallucination.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.4"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.4.1">Hidden layers.</span> <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.F7" title="In 4.2 Ablation Study: mass-removing hallucinations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows hallucination reduction rate on LLaVA from mass-removing hallucinations on every combination of <math alttext="l^{I}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><msup id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">l</mi><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">I</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">𝑙</ci><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">l^{I}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="l^{T}" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><msup id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mi id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">l</mi><mi id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">𝑙</ci><ci id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">l^{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> (each from -1 to 31). As a core concern is that editing erodes caption accuracy, we gray out any combination that reduces CD objects. For InstructBLIP (see <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F11" title="In A.2 Ablations for InstructBLIP ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>), the best parameters <math alttext="(l^{I}=1,l^{T}=2)" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.1.1"><mo id="S4.SS2.p3.3.m3.1.1.1.2" stretchy="false">(</mo><mrow id="S4.SS2.p3.3.m3.1.1.1.1.2" xref="S4.SS2.p3.3.m3.1.1.1.1.3.cmml"><mrow id="S4.SS2.p3.3.m3.1.1.1.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><msup id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.2" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.2.cmml">l</mi><mi id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.3" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.3.cmml">I</mi></msup><mo id="S4.SS2.p3.3.m3.1.1.1.1.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.3.m3.1.1.1.1.1.1.3" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS2.p3.3.m3.1.1.1.1.2.3" xref="S4.SS2.p3.3.m3.1.1.1.1.3a.cmml">,</mo><mrow id="S4.SS2.p3.3.m3.1.1.1.1.2.2" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.cmml"><msup id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.cmml"><mi id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.2" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.2.cmml">l</mi><mi id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.3" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.3.cmml">T</mi></msup><mo id="S4.SS2.p3.3.m3.1.1.1.1.2.2.1" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.1.cmml">=</mo><mn id="S4.SS2.p3.3.m3.1.1.1.1.2.2.3" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.3.cmml">2</mn></mrow></mrow><mo id="S4.SS2.p3.3.m3.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.1.1.3a.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S4.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1"><eq id="S4.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.1"></eq><apply id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.2">𝑙</ci><ci id="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.2.3">𝐼</ci></apply><cn id="S4.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.3.m3.1.1.1.1.1.1.3">1</cn></apply><apply id="S4.SS2.p3.3.m3.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2"><eq id="S4.SS2.p3.3.m3.1.1.1.1.2.2.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.1"></eq><apply id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2">superscript</csymbol><ci id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.2.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.2">𝑙</ci><ci id="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.3.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.2.3">𝑇</ci></apply><cn id="S4.SS2.p3.3.m3.1.1.1.1.2.2.3.cmml" type="integer" xref="S4.SS2.p3.3.m3.1.1.1.1.2.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">(l^{I}=1,l^{T}=2)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT = 1 , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = 2 )</annotation></semantics></math> reduces hallucinations by 38.5%. For LLaVA, our best parameters <math alttext="(l^{I}=19,l^{T}=21)" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mrow id="S4.SS2.p3.4.m4.1.1.1"><mo id="S4.SS2.p3.4.m4.1.1.1.2" stretchy="false">(</mo><mrow id="S4.SS2.p3.4.m4.1.1.1.1.2" xref="S4.SS2.p3.4.m4.1.1.1.1.3.cmml"><mrow id="S4.SS2.p3.4.m4.1.1.1.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.cmml"><msup id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.2" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.2.cmml">l</mi><mi id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.3" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.3.cmml">I</mi></msup><mo id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.4.m4.1.1.1.1.1.1.3" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.3.cmml">19</mn></mrow><mo id="S4.SS2.p3.4.m4.1.1.1.1.2.3" xref="S4.SS2.p3.4.m4.1.1.1.1.3a.cmml">,</mo><mrow id="S4.SS2.p3.4.m4.1.1.1.1.2.2" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.cmml"><msup id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.cmml"><mi id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.2" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.2.cmml">l</mi><mi id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.3" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.3.cmml">T</mi></msup><mo id="S4.SS2.p3.4.m4.1.1.1.1.2.2.1" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.1.cmml">=</mo><mn id="S4.SS2.p3.4.m4.1.1.1.1.2.2.3" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.3.cmml">21</mn></mrow></mrow><mo id="S4.SS2.p3.4.m4.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.1.3a.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S4.SS2.p3.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1"><eq id="S4.SS2.p3.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.1"></eq><apply id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.2">𝑙</ci><ci id="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.2.3">𝐼</ci></apply><cn id="S4.SS2.p3.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.4.m4.1.1.1.1.1.1.3">19</cn></apply><apply id="S4.SS2.p3.4.m4.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2"><eq id="S4.SS2.p3.4.m4.1.1.1.1.2.2.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.1"></eq><apply id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2">superscript</csymbol><ci id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.2">𝑙</ci><ci id="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.2.3">𝑇</ci></apply><cn id="S4.SS2.p3.4.m4.1.1.1.1.2.2.3.cmml" type="integer" xref="S4.SS2.p3.4.m4.1.1.1.1.2.2.3">21</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">(l^{I}=19,l^{T}=21)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT = 19 , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = 21 )</annotation></semantics></math> reduce hallucinations by 25.7%, and the middle layers are the best to edit and extract latent text embeddings from. Our results also provide a wide range of reasonable parameter alternatives to use if this reduction rate does not generalize beyond our samples.</p>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F7.3">
<span class="ltx_ERROR undefined" id="S4.F7.3.1">{floatrow}</span><span class="ltx_ERROR undefined" id="S4.F7.3.2">\ffigbox</span><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="333" id="S4.F7.g1" src="x5.png" width="381"/>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span class="ltx_text ltx_font_bold" id="S4.F7.5.1">Hidden layer ablations</span> for LLaVA. We track hallucination reduction (%) across different layers to edit at and extract latent embeddings for the text embedding, crossing out (red) parameters from consideration where there is a decrease in correctly detected objects.

</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S4.F7.6">
<span class="ltx_ERROR undefined" id="S4.F7.6.1">\ffigbox</span><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S4.F7.g2" src="x6.png" width="380"/>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span class="ltx_text ltx_font_bold" id="S4.F7.8.1">Weight ablations</span> for LLaVA. We vary the weight factor <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.F7.2.m1.1"><semantics id="S4.F7.2.m1.1a"><mi id="S4.F7.2.m1.1.1" xref="S4.F7.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.F7.2.m1.1b"><ci id="S4.F7.2.m1.1.1.cmml" xref="S4.F7.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.F7.2.m1.1d">italic_α</annotation></semantics></math> and measure changes in correctly detected objects, removal rate, and hallucination reduction. We observe a decline in hallucinations as weight grows and mark a weight where there is no loss in correctly detected objects.

</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.3"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.3.1">Weight factor.</span> Using the best-reduced hidden layers, we ablate the weight factor <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_α</annotation></semantics></math> for <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p4.3.2">ProjectAway</span> across the same 500 randomly selected COCO images. <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.F7" title="In 4.2 Ablation Study: mass-removing hallucinations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows that as <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mi id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">italic_α</annotation></semantics></math> increases, hallucinations are removed at a higher rate, and the overall hallucination count drops significantly. At high <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.p4.3.m3.1"><semantics id="S4.SS2.p4.3.m3.1a"><mi id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><ci id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.3.m3.1d">italic_α</annotation></semantics></math>, we observe through anecdotal examples that captions become nonsensical, as quantitatively shown by the complete loss of both correctly detected and hallucinated objects from the caption.
Therefore, as a pre-caution, we only select weight factors that do not reduce CD objects when we apply <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p4.3.3">ProjectAway</span> to erase hallucinated objects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Hallucination Detection</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">When extracting knowledge from VLMs in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.SS2" title="3.2 Applying Logit Lens on VLMs ‣ 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, we found that applying logit lens on in-context image representations exhibit useful information about visual objects present in the image. Using these observations, we present an approach for object presence classification that only relies on the VLMs own parameters. We utilize the internal confidence <math alttext="c_{o}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">c</mi><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝑐</ci><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">c_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> value to classify object presence, since the internal confidence for objects that are not present in the image, or hallucinated, are lower within the image representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.2.1">Experimental setting.</span> We evaluate the strength of the internal confidence <math alttext="c_{o}" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><msub id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">c</mi><mi id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">𝑐</ci><ci id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">c_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> as an indicator of object presence. We sample 5000 images from the MSCOCO training set, using the image captioning objective to caption methods with both InstructBLIP and LLaVA. We use the <math alttext="c_{o}" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><msub id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">c</mi><mi id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">𝑐</ci><ci id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">c_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> for present objects and hallucinations within the captions generated by each VLM. We assess how well the internal confidence aligns with the ground truth labels of object presence, where a negative sample is a hallucination and a positive sample is a present object.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Baseline.</span> As a baseline, we use the maximum output probability of the object’s tokens. This is the confidence of the model prediction. Previous works such as  <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib52" title="">2024</a>)</cite> have found that hallucinations occur more frequently on objects characterized by high uncertainty during generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Results.</span> We present quantitative results in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.F8" title="In 5.1 Hallucination Detection ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.T5" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>. We show qualitative results for LLaVA (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F12" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a>) and InstructBLIP (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F13" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>) in the Appendix. We find that utilizing internal confidence to classify object hallucinations provides a 47.17% improvement in mAP in InstructBLIP and 22.45% in LLaVA. Furthermore, the ROC AUC improves over the baseline by 50.10% in InstructBLIP and 44.68% in LLaVA, indicating stronger object presence classification.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S5.F8.g1" src="x7.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="S5.F8.2.1">Object Presence Classification Curves for InstructBLIP and LLaVA.</span> We show the Precision-Recall and ROC curves of our confidence measure for present object-hallucination classification on the COCO training subset. Classifying object presence with the internal confidence outperforms the baseline, indicating that the language model’s image representations know which objects are hallucinations and which are truly present.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Hallucination Removal</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We use the mass editing technique to remove hallucinations detected by the prior method. <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS1.SSS2" title="4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1.2</span></a> successfully removes a significant portion of hallucinations but presupposes a knowledge of what the hallucinations are. We threshold on the internal confidence of each object to identify hallucinations and mass-remove them using <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.1">ProjectAway</span>. Our chosen threshold prioritizes precision over recall (i.e. we allow classification of some CD objects as hallucinations) because CD objects are less affected by the removal method, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS1.SSS2" title="4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1.2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.4"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.4.1">Experimental setting.</span> We threshold hallucinations as <math alttext="c_{o}&lt;0.2" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><msub id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2.2" xref="S5.SS2.p2.1.m1.1.1.2.2.cmml">c</mi><mi id="S5.SS2.p2.1.m1.1.1.2.3" xref="S5.SS2.p2.1.m1.1.1.2.3.cmml">o</mi></msub><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><lt id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></lt><apply id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.2.1.cmml" xref="S5.SS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2.2">𝑐</ci><ci id="S5.SS2.p2.1.m1.1.1.2.3.cmml" xref="S5.SS2.p2.1.m1.1.1.2.3">𝑜</ci></apply><cn id="S5.SS2.p2.1.m1.1.1.3.cmml" type="float" xref="S5.SS2.p2.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">c_{o}&lt;0.2</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT &lt; 0.2</annotation></semantics></math> for InstructBLIP and <math alttext="c_{o}&lt;0.1" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><msub id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2.2" xref="S5.SS2.p2.2.m2.1.1.2.2.cmml">c</mi><mi id="S5.SS2.p2.2.m2.1.1.2.3" xref="S5.SS2.p2.2.m2.1.1.2.3.cmml">o</mi></msub><mo id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><lt id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1"></lt><apply id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.2.1.cmml" xref="S5.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.2.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2.2">𝑐</ci><ci id="S5.SS2.p2.2.m2.1.1.2.3.cmml" xref="S5.SS2.p2.2.m2.1.1.2.3">𝑜</ci></apply><cn id="S5.SS2.p2.2.m2.1.1.3.cmml" type="float" xref="S5.SS2.p2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">c_{o}&lt;0.1</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT &lt; 0.1</annotation></semantics></math> for LLaVA. Based on prior ablations (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS2" title="4.2 Ablation Study: mass-removing hallucinations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>), we select <math alttext="(l^{I}=1,l^{T}=2,\alpha=1.5)" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mrow id="S5.SS2.p2.3.m3.1.1.1"><mo id="S5.SS2.p2.3.m3.1.1.1.2" stretchy="false">(</mo><mrow id="S5.SS2.p2.3.m3.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.3.cmml"><mrow id="S5.SS2.p2.3.m3.1.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.cmml"><msup id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.cmml"><mi id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.2" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.2.cmml">l</mi><mi id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.3" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.3.cmml">I</mi></msup><mo id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.3.m3.1.1.1.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S5.SS2.p2.3.m3.1.1.1.1.2.3" xref="S5.SS2.p2.3.m3.1.1.1.1.3a.cmml">,</mo><mrow id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.3.cmml"><mrow id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.cmml"><msup id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.cmml"><mi id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.2.cmml">l</mi><mi id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.3" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.3.cmml">T</mi></msup><mo id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.3.cmml">2</mn></mrow><mo id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.3" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.3a.cmml">,</mo><mrow id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.cmml"><mi id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.2.cmml">α</mi><mo id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.1" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.1.cmml">=</mo><mn id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.3" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.3.cmml">1.5</mn></mrow></mrow></mrow><mo id="S5.SS2.p2.3.m3.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.1.3a.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S5.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1"><eq id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1"></eq><apply id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2">superscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.2">𝑙</ci><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.3">𝐼</ci></apply><cn id="S5.SS2.p2.3.m3.1.1.1.1.1.1.3.cmml" type="integer" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.3">1</cn></apply><apply id="S5.SS2.p2.3.m3.1.1.1.1.2.2.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.1.2.2.3a.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1"><eq id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.1"></eq><apply id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2">superscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.2">𝑙</ci><ci id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.2.3">𝑇</ci></apply><cn id="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.3.cmml" type="integer" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.1.1.3">2</cn></apply><apply id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2"><eq id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.1"></eq><ci id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.2">𝛼</ci><cn id="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.3.cmml" type="float" xref="S5.SS2.p2.3.m3.1.1.1.1.2.2.2.2.3">1.5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">(l^{I}=1,l^{T}=2,\alpha=1.5)</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT = 1 , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = 2 , italic_α = 1.5 )</annotation></semantics></math> for InstructBLIP and <math alttext="(l^{I}=19,l^{T}=21,\alpha=3.5)" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mrow id="S5.SS2.p2.4.m4.1.1.1"><mo id="S5.SS2.p2.4.m4.1.1.1.2" stretchy="false">(</mo><mrow id="S5.SS2.p2.4.m4.1.1.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.1.3.cmml"><mrow id="S5.SS2.p2.4.m4.1.1.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.cmml"><msup id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.cmml"><mi id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.2" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.2.cmml">l</mi><mi id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.3" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.3.cmml">I</mi></msup><mo id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.4.m4.1.1.1.1.1.1.3" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml">19</mn></mrow><mo id="S5.SS2.p2.4.m4.1.1.1.1.2.3" xref="S5.SS2.p2.4.m4.1.1.1.1.3a.cmml">,</mo><mrow id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.3.cmml"><mrow id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.cmml"><msup id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.cmml"><mi id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.2" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.2.cmml">l</mi><mi id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.3" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.3.cmml">T</mi></msup><mo id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.3" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.3.cmml">21</mn></mrow><mo id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.3" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.3a.cmml">,</mo><mrow id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.cmml"><mi id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.2" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.2.cmml">α</mi><mo id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.1" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.1.cmml">=</mo><mn id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.3" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.3.cmml">3.5</mn></mrow></mrow></mrow><mo id="S5.SS2.p2.4.m4.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><apply id="S5.SS2.p2.4.m4.1.1.1.1.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.1.1.1.1.3a.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S5.SS2.p2.4.m4.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1"><eq id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1"></eq><apply id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2">superscript</csymbol><ci id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.2">𝑙</ci><ci id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.2.3">𝐼</ci></apply><cn id="S5.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.3">19</cn></apply><apply id="S5.SS2.p2.4.m4.1.1.1.1.2.2.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.1.1.1.1.2.2.3a.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1"><eq id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.1"></eq><apply id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2">superscript</csymbol><ci id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.2">𝑙</ci><ci id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.2.3">𝑇</ci></apply><cn id="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.3.cmml" type="integer" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.1.1.3">21</cn></apply><apply id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2"><eq id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.1"></eq><ci id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.2">𝛼</ci><cn id="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.3.cmml" type="float" xref="S5.SS2.p2.4.m4.1.1.1.1.2.2.2.2.3">3.5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">(l^{I}=19,l^{T}=21,\alpha=3.5)</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT = 19 , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = 21 , italic_α = 3.5 )</annotation></semantics></math> for LLaVA. Our prompt is “Please describe this image in detail.”</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.3"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.3.1">Baselines.</span> Since our method intervenes during the decoder step, we compare our method with 3 standard decoding algorithms. Greedy decoding predicts the next token based on the highest logit probability. Beam search maintains a tree of beams and selects the best beam at generation end. Nucleus sampling selects the next token from a set of high probability tokens whose cumulative probability reaches a threshold <math alttext="p" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><mi id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">italic_p</annotation></semantics></math>. We also evaluate against OPERA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib24" title="">2024</a>)</cite>, which mitigates hallucinations by adding an overtrust penalty during decoder generation. We set <math alttext="p=0.9" class="ltx_Math" display="inline" id="S5.SS2.p3.2.m2.1"><semantics id="S5.SS2.p3.2.m2.1a"><mrow id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mi id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS2.p3.2.m2.1.1.1" xref="S5.SS2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><eq id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1.1"></eq><ci id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">𝑝</ci><cn id="S5.SS2.p3.2.m2.1.1.3.cmml" type="float" xref="S5.SS2.p3.2.m2.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">p=0.9</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.2.m2.1d">italic_p = 0.9</annotation></semantics></math> for nucleus sampling. We use beam search in our method and unify <math alttext="N_{\text{beam}}=5" class="ltx_Math" display="inline" id="S5.SS2.p3.3.m3.1"><semantics id="S5.SS2.p3.3.m3.1a"><mrow id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml"><msub id="S5.SS2.p3.3.m3.1.1.2" xref="S5.SS2.p3.3.m3.1.1.2.cmml"><mi id="S5.SS2.p3.3.m3.1.1.2.2" xref="S5.SS2.p3.3.m3.1.1.2.2.cmml">N</mi><mtext id="S5.SS2.p3.3.m3.1.1.2.3" xref="S5.SS2.p3.3.m3.1.1.2.3a.cmml">beam</mtext></msub><mo id="S5.SS2.p3.3.m3.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS2.p3.3.m3.1.1.3" xref="S5.SS2.p3.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><apply id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1"><eq id="S5.SS2.p3.3.m3.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1"></eq><apply id="S5.SS2.p3.3.m3.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.1.1.2.1.cmml" xref="S5.SS2.p3.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS2.p3.3.m3.1.1.2.2.cmml" xref="S5.SS2.p3.3.m3.1.1.2.2">𝑁</ci><ci id="S5.SS2.p3.3.m3.1.1.2.3a.cmml" xref="S5.SS2.p3.3.m3.1.1.2.3"><mtext id="S5.SS2.p3.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S5.SS2.p3.3.m3.1.1.2.3">beam</mtext></ci></apply><cn id="S5.SS2.p3.3.m3.1.1.3.cmml" type="integer" xref="S5.SS2.p3.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">N_{\text{beam}}=5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.3.m3.1d">italic_N start_POSTSUBSCRIPT beam end_POSTSUBSCRIPT = 5</annotation></semantics></math> for the baseline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Results.</span> We apply these parameters to 500 COCO images from the Karpathy validation set. We provide qualitative results in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F15" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">15</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F14" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>. Quantitative results in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.T2" title="In 5.2 Hallucination Removal ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> show that we outperform our baselines and reduce hallucinations by 25.7% on InstructBLIP and 23.8% on LLaVA compared to beam search. Our approach achieves a similar hallucination reduction rate as <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS1.SSS2" title="4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1.2</span></a>, despite not precisely differentiating hallucinations and some CD objects being incorrectly edited out. Notably, our method relies on no training or external models, effectively offering a “free lunch.”</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S5.T2.5.1">Hallucination intervention performance.</span> We mass-remove hallucinations detected by the method in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS1" title="5.1 Hallucination Detection ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a> and outperform other baselines. We observe a considerable drop in the raw count of hallucinated objects.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.3.5.1">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">CHAIR<math alttext="{}_{i}\downarrow" class="ltx_math_unparsed" display="inline" id="S5.T2.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mmultiscripts id="S5.T2.1.1.1.1.1.m1.1.1"><mo id="S5.T2.1.1.1.1.1.m1.1.1.2" stretchy="false">↓</mo><mprescripts id="S5.T2.1.1.1.1.1.m1.1.1a"></mprescripts><mi id="S5.T2.1.1.1.1.1.m1.1.1.3">i</mi><mrow id="S5.T2.1.1.1.1.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1b">{}_{i}\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.m1.1c">start_FLOATSUBSCRIPT italic_i end_FLOATSUBSCRIPT ↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.1">CHAIR<math alttext="{}_{s}\downarrow" class="ltx_math_unparsed" display="inline" id="S5.T2.2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.2.1.m1.1a"><mmultiscripts id="S5.T2.2.2.2.2.1.m1.1.1"><mo id="S5.T2.2.2.2.2.1.m1.1.1.2" stretchy="false">↓</mo><mprescripts id="S5.T2.2.2.2.2.1.m1.1.1a"></mprescripts><mi id="S5.T2.2.2.2.2.1.m1.1.1.3">s</mi><mrow id="S5.T2.2.2.2.2.1.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.1b">{}_{s}\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.1.m1.1c">start_FLOATSUBSCRIPT italic_s end_FLOATSUBSCRIPT ↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.3.3.1">Hallucinated Objects <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.1.m1.1"><semantics id="S5.T2.3.3.3.3.1.m1.1a"><mo id="S5.T2.3.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.1" rowspan="5"><span class="ltx_text" id="S5.T2.3.3.4.1.1.1">InstructBLIP</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.2">Greedy</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.3">57.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.4">23.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.5">512</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.5.2">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.1">Nucleus</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.2">58.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.3">24.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.4">508</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.6.3">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.1">Beam Search</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.2">53.4</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.3">14.6</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.4">564</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.7.4">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.7.4.1">OPERA</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.7.4.2">45.6</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.7.4.3">13.9</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.7.4.4">472</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.8.5">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.8.5.1">Ours</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.8.5.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.8.5.2.1">43.8</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.8.5.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.8.5.3.1">12.5</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.8.5.4.1">419</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.9.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.3.3.9.6.1" rowspan="5"><span class="ltx_text" id="S5.T2.3.3.9.6.1.1">LLaVA</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.9.6.2">Greedy</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.9.6.3">49.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.9.6.4">14.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.9.6.5">532</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.10.7">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.10.7.1">Nucleus</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.10.7.2">55.8</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.10.7.3">17.1</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.10.7.4">618</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.11.8">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.11.8.1">Beam Search</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.11.8.2">52.4</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.11.8.3">15.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.11.8.4">583</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.12.9">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.12.9.1">OPERA</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.12.9.2">44.8</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.12.9.3">12.8</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.12.9.4">462</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.13.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.13.10.1">Ours</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.13.10.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.13.10.2.1">42.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.13.10.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.13.10.3.1">12.2</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.13.10.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.13.10.4.1">444</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Zero-shot Segmentation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Building upon our findings in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S3.SS2" title="3.2 Applying Logit Lens on VLMs ‣ 3 Extracting Knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, we utilize the internal confidence per image feature for zero-shot image segmentation. This application leverages the spatial information encoded in the image representations and demonstrates how VLMs internally represent and localize objects within images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.2.1">Method.</span> Our approach leverages the spatial correspondence between image patches and their associated image embeddings. We use LLaVA to generate the name of the class in the image and we focus on the internal confidence of that class per image patch. We take the mean internal confidence for tokens comprising a class word. We resize the set of <math alttext="24\times 24" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mn id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">24</mn><mo id="S5.SS3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">24</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><times id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></times><cn id="S5.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS3.p2.1.m1.1.1.2">24</cn><cn id="S5.SS3.p2.1.m1.1.1.3.cmml" type="integer" xref="S5.SS3.p2.1.m1.1.1.3">24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">24\times 24</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">24 × 24</annotation></semantics></math> internal confidence values per image patch back into a fixed image size of <math alttext="336\times 366" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.1"><semantics id="S5.SS3.p2.2.m2.1a"><mrow id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mn id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">336</mn><mo id="S5.SS3.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">366</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><times id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1"></times><cn id="S5.SS3.p2.2.m2.1.1.2.cmml" type="integer" xref="S5.SS3.p2.2.m2.1.1.2">336</cn><cn id="S5.SS3.p2.2.m2.1.1.3.cmml" type="integer" xref="S5.SS3.p2.2.m2.1.1.3">366</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">336\times 366</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.1d">336 × 366</annotation></semantics></math> pixels. We then apply a threshold to these confidence values to binarize them into a foreground/background segmentation for the object in the image.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="517" id="S5.F9.g1" src="x8.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="S5.F9.2.1">Zero-shot segmentation.</span> Warmer areas indicate higher internal confidence for the class at that image patch. We binarize these values with a threshold to generate segmentations.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">Baseline.</span> As a baseline, we extract the attention values of generated tokens with the image embeddings from LLaVA. We also compare to the segmentation method introduced by <cite class="ltx_cite ltx_citemacro_cite">Gandelsman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib18" title="">2024b</a>)</cite>, which utilizes the attention heads of the image encoder without the additional VLM processing, using the same image encoder (CLIP-ViT-L/14 at 336px).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Results.</span> We evaluate our method on the Imagenet validation set. Qualitative results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.F9" title="In 5.3 Zero-shot Segmentation ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a> and quantitative comparisons with the baselines in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.T3" title="In 5.3 Zero-shot Segmentation ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>. We improve mAP by 8.03% over using the VLMs raw attention values and provide better and/or comparable performance to other state-of-the-art methods that utilize just the image encoder. While the VLM is not directly trained for segmentation, our technique reveals that it still encodes significant <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.2">spatial</span> information about objects within its intermediate image representations.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.5.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Pixel Acc. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.2.2.2.1">mIoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.1.m1.1"><semantics id="S5.T3.2.2.2.2.1.m1.1a"><mo id="S5.T3.2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T3.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.3.1">mAP <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.1.m1.1"><semantics id="S5.T3.3.3.3.3.1.m1.1a"><mo id="S5.T3.3.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T3.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.1.m1.1b"><ci id="S5.T3.3.3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.3.3.4.1.1">raw attention (CLIP)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.3.4.1.2">Image Encoder</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.3.4.1.3">69.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.3.4.1.4">45.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.3.4.1.5">77.30</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.3.3.5.2.1">TextSpan <cite class="ltx_cite ltx_citemacro_citep">(Gandelsman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#bib.bib18" title="">2024b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.5.2.2">Image Encoder</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.5.2.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.3.3.5.2.3.1">75.57</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.5.2.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.3.3.5.2.4.1">53.60</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.5.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.5.2.5.1">80.22</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.3.3.6.3.1">raw attention (VLM)</th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.6.3.2">VLM</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.6.3.3">67.28</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.6.3.4">39.27</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.6.3.5">73.96</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.3.3.7.4.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.3.7.4.2">VLM</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.7.4.3.1">76.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.3.7.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.7.4.4.1">54.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.3.7.4.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.3.3.7.4.5.1">79.90</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S5.T3.5.1">Segmentation Performance on ImageNet-segmentation.</span> Localizing objects using their probabilities within the image representations results in more accurate zero-shot segmentation than previous vision-encoders-based and VLM-based methods.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and limitations</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We interpreted VLMs’ image representations through the language model layers and discovered that linear editing of these representations can selectively remove object information via a simple orthogonalization. Our findings enabled hallucination reduction and improved zero-shot segmentation. We present two limitations of our work and conclude with future directions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Multi-token objects.</span> Our method simplifies the use of object words that may be composed of multiple tokens, such as by taking the max internal confidence over object tokens or utilizing the average token embedding for editing. This can introduce noise to the internal confidence if certain tokens are common in multiple different words and lead to an approximation of the object’s latent representations when editing.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Fine-grained edits.</span> The editing approach may struggle with highly abstract or longer sentences that involve attributes or interactions of objects. Removing a full sentence, for example, is not something we assessed in this paper, since our focus is on the removal of individual objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Future work.</span> While our focus was on interpreting objects and object hallucinations in VLMs, we believe that our approach can be extended to other key elements of visual scenes, such as people, attributes, and actions. We also focused on object removal, but we believe that editing can also be extended to inject objects into a caption (by adding instead of subtracting the text embedding). We hope to explore the applications of our approach in other multimodal architectures. Our insights may help design better VLMs that are more robust to hallucinations and have improved spatial understanding. We plan to explore these directions in our future work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Acknowledgments</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We thank Kayo Yin for her comments and feedback on our paper. YG is supported by the Google Fellowship. Authors, as part of their affiliation with UC Berkeley, were supported in part by the the Berkeley Artificial Intelligence Research (BAIR) commons program.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azaria &amp; Mitchell (2023)</span>
<span class="ltx_bibblock">
Amos Azaria and Tom Mitchell.

</span>
<span class="ltx_bibblock">The internal state of an LLM knows when it’s lying.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pp.  967–976, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-emnlp.68</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-emnlp.68" title="">https://aclanthology.org/2023.findings-emnlp.68</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.12966" title="">https://arxiv.org/abs/2308.12966</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bau et al. (2019)</span>
<span class="ltx_bibblock">
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Gan dissection: Visualizing and understanding generative adversarial networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bau et al. (2020)</span>
<span class="ltx_bibblock">
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Understanding the role of individual units in a deep neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the National Academy of Sciences</em>, 2020.

</span>
<span class="ltx_bibblock">ISSN 0027-8424.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1073/pnas.1907375117</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/content/early/2020/08/31/1907375117" title="">https://www.pnas.org/content/early/2020/08/31/1907375117</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belrose et al. (2023)</span>
<span class="ltx_bibblock">
Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Eliciting latent predictions from transformers with the tuned lens, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08112" title="">https://arxiv.org/abs/2303.08112</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bricken et al. (2023)</span>
<span class="ltx_bibblock">
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.

</span>
<span class="ltx_bibblock">Towards monosemanticity: Decomposing language models with dictionary learning.

</span>
<span class="ltx_bibblock">Transformer Circuits Thread, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" title="">https://transformer-circuits.pub/2023/monosemantic-features/index.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bronzini et al. (2024)</span>
<span class="ltx_bibblock">
Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, and Andrea Passerini.

</span>
<span class="ltx_bibblock">Unveiling llms: The evolution of latent representations in a dynamic knowledge graph, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.03623" title="">https://arxiv.org/abs/2404.03623</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chefer et al. (2021)</span>
<span class="ltx_bibblock">
Hila Chefer, Shir Gur, and Lior Wolf.

</span>
<span class="ltx_bibblock">Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2103.15679" title="">https://arxiv.org/abs/2103.15679</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye.

</span>
<span class="ltx_bibblock">Inside: Llms’ internal states retain the power of hallucination detection, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.03744" title="">https://arxiv.org/abs/2402.03744</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Haozhe Chen, Junfeng Yang, Carl Vondrick, and Chengzhi Mao.

</span>
<span class="ltx_bibblock">Interpreting and controlling vision foundation models via text explanations, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2310.10591" title="">https://arxiv.org/pdf/2310.10591</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conmy et al. (2023)</span>
<span class="ltx_bibblock">
Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso.

</span>
<span class="ltx_bibblock">Towards automated circuit discovery for mechanistic interpretability, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.14997" title="">https://arxiv.org/abs/2304.14997</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cunningham et al. (2023)</span>
<span class="ltx_bibblock">
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey.

</span>
<span class="ltx_bibblock">Sparse autoencoders find highly interpretable features in language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.08600" title="">https://arxiv.org/abs/2309.08600</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2022)</span>
<span class="ltx_bibblock">
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.

</span>
<span class="ltx_bibblock">Knowledge neurons in pretrained transformers, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2104.08696" title="">https://arxiv.org/abs/2104.08696</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.06500" title="">https://arxiv.org/abs/2305.06500</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dravid et al. (2023)</span>
<span class="ltx_bibblock">
Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher.

</span>
<span class="ltx_bibblock">Rosetta neurons: Mining the common units in a model zoo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  1934–1943, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Esser, Robin Rombach, and Bjorn Ommer.

</span>
<span class="ltx_bibblock">A disentangling invertible interpretation network for explaining latent representations, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2004.13166" title="">https://arxiv.org/pdf/2004.13166</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandelsman et al. (2024a)</span>
<span class="ltx_bibblock">
Yossi Gandelsman, Alexei A. Efros, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Interpreting the second-order effects of neurons in clip, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.04341" title="">https://arxiv.org/abs/2406.04341</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandelsman et al. (2024b)</span>
<span class="ltx_bibblock">
Yossi Gandelsman, Alexei A. Efros, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Interpreting clip’s image representation via text-based decomposition, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2310.05916" title="">https://arxiv.org/pdf/2310.05916</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghandeharioun et al. (2024)</span>
<span class="ltx_bibblock">
Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva.

</span>
<span class="ltx_bibblock">Patchscopes: A unifying framework for inspecting hidden representations of language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.06102" title="">https://arxiv.org/abs/2401.06102</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halawi et al. (2024)</span>
<span class="ltx_bibblock">
Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Overthinking the truth: Understanding how language models process false demonstrations, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09476" title="">https://arxiv.org/abs/2307.09476</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2024)</span>
<span class="ltx_bibblock">
Jinwen He, Yujia Gong, Kai Chen, Zijin Lin, Chengan Wei, and Yue Zhao.

</span>
<span class="ltx_bibblock">Llm factoscope: Uncovering llms’ factual discernment through inner states analysis, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.16374" title="">https://arxiv.org/abs/2312.16374</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt &amp; Manning (2019)</span>
<span class="ltx_bibblock">
John Hewitt and Christopher D. Manning.

</span>
<span class="ltx_bibblock">A structural probe for finding syntax in word representations.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.  4129–4138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1419</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1419" title="">https://aclanthology.org/N19-1419</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and Xiang Li.

</span>
<span class="ltx_bibblock">Rsgpt: A remote sensing vision language model and benchmark, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.15266" title="">https://arxiv.org/abs/2307.15266</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.

</span>
<span class="ltx_bibblock">Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.17911" title="">https://arxiv.org/abs/2311.17911</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huo et al. (2024)</span>
<span class="ltx_bibblock">
Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, and Xuming Hu.

</span>
<span class="ltx_bibblock">Mmneuron: Discovering neuron-level domain-specific interpretation in multimodal large language model, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2406.11193v1" title="">https://arxiv.org/pdf/2406.11193v1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ACM Computing Surveys</em>, 55(12):1–38, March 2023.

</span>
<span class="ltx_bibblock">ISSN 1557-7341.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3571730</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1145/3571730" title="">http://dx.doi.org/10.1145/3571730</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobayashi et al. (2020)</span>
<span class="ltx_bibblock">
Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui.

</span>
<span class="ltx_bibblock">Attention is not only a weight: Analyzing transformers with vector norms.

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pp.  7057–7075, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.574</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.574" title="">https://aclanthology.org/2020.emnlp-main.574</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2301.12597" title="">https://arxiv.org/abs/2301.12597</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.

</span>
<span class="ltx_bibblock">Emergent world representations: Exploring a sequence model trained on a synthetic task, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.13382" title="">https://arxiv.org/abs/2210.13382</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.10355" title="">https://arxiv.org/abs/2305.10355</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2015)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1405.0312" title="">https://arxiv.org/abs/1405.0312</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.03744" title="">https://arxiv.org/abs/2310.03744</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2024)</span>
<span class="ltx_bibblock">
Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, and Yang Liu.

</span>
<span class="ltx_bibblock">Codis: Benchmarking context-dependent visual comprehension for multimodal large language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.13607" title="">https://arxiv.org/abs/2402.13607</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2023)</span>
<span class="ltx_bibblock">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.

</span>
<span class="ltx_bibblock">Locating and editing factual associations in gpt, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2202.05262" title="">https://arxiv.org/abs/2202.05262</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">nostalgebraist (2020)</span>
<span class="ltx_bibblock">
nostalgebraist.

</span>
<span class="ltx_bibblock">Interpreting GPT: The logit lens.

</span>
<span class="ltx_bibblock">LessWrong, Aug 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" title="">https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Palit et al. (2023)</span>
<span class="ltx_bibblock">
Vedant Palit, Rohan Pandey, Aryaman Arora, and Paul Pu Liang.

</span>
<span class="ltx_bibblock">Towards vision-language mechanistic interpretability: A causal tracing tool for blip, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2308.14179" title="">https://arxiv.org/pdf/2308.14179</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petsiuk et al. (2018)</span>
<span class="ltx_bibblock">
Vitali Petsiuk, Abir Das, and Kate Saenko.

</span>
<span class="ltx_bibblock">Rise: Randomized input sampling for explanation of black-box models, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/1806.07421" title="">https://arxiv.org/pdf/1806.07421</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2103.00020" title="">https://arxiv.org/pdf/2103.00020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravfogel et al. (2024)</span>
<span class="ltx_bibblock">
Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan Cotterell.

</span>
<span class="ltx_bibblock">Linear adversarial concept erasure, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.12091" title="">https://arxiv.org/abs/2201.12091</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al. (2019)</span>
<span class="ltx_bibblock">
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko.

</span>
<span class="ltx_bibblock">Object hallucination in image captioning, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1809.02156" title="">https://arxiv.org/abs/1809.02156</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwettmann et al. (2023)</span>
<span class="ltx_bibblock">
Sarah Schwettmann, Neil Chowdhury, Samuel Klein, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Multimodal neurons in pretrained text-only transformers, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2308.01544" title="">https://arxiv.org/pdf/2308.01544</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaham et al. (2024)</span>
<span class="ltx_bibblock">
Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, and Antonio Torralba.

</span>
<span class="ltx_bibblock">A multimodal automated interpretability agent, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2404.14394" title="">https://arxiv.org/pdf/2404.14394</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2024)</span>
<span class="ltx_bibblock">
Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, and Lei Ma.

</span>
<span class="ltx_bibblock">Luna: A model-based universal analysis framework for large language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.14211" title="">https://arxiv.org/abs/2310.14211</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024)</span>
<span class="ltx_bibblock">
Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, and Yiqun Liu.

</span>
<span class="ltx_bibblock">Unsupervised real-time hallucination detection based on the internal states of large language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.06448" title="">https://arxiv.org/abs/2403.06448</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tucker et al. (2021)</span>
<span class="ltx_bibblock">
Mycal Tucker, Peng Qian, and Roger Levy.

</span>
<span class="ltx_bibblock">What if this modified that? syntactic interventions via counterfactual embeddings, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2105.14002" title="">https://arxiv.org/abs/2105.14002</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varshney et al. (2023)</span>
<span class="ltx_bibblock">
Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu.

</span>
<span class="ltx_bibblock">A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.03987" title="">https://arxiv.org/abs/2307.03987</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Rutte et al. (2024)</span>
<span class="ltx_bibblock">
Dimitri von Rutte, Sotiris Anagnostidis, Gregor Bachmann, and Thomas Hofmann.

</span>
<span class="ltx_bibblock">A language model’s guide through latent space, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2402.14433" title="">https://arxiv.org/pdf/2402.14433</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.

</span>
<span class="ltx_bibblock">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.04257" title="">https://arxiv.org/abs/2311.04257</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2023)</span>
<span class="ltx_bibblock">
Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen.

</span>
<span class="ltx_bibblock">Woodpecker: Hallucination correction for multimodal large language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.16045" title="">https://arxiv.org/abs/2310.16045</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, and Feng Zheng.

</span>
<span class="ltx_bibblock">Reflective instruction tuning: Mitigating hallucinations in large vision-language models, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.11422" title="">https://arxiv.org/abs/2407.11422</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Shaolei Zhang, Tian Yu, and Yang Feng.

</span>
<span class="ltx_bibblock">Truthx: Alleviating hallucinations by editing large language models in truthful space, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2402.17811" title="">https://arxiv.org/pdf/2402.17811</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.

</span>
<span class="ltx_bibblock">Analyzing and mitigating object hallucination in large vision-language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.00754" title="">https://arxiv.org/abs/2310.00754</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Mass-removing objects</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We mass-remove mentioned objects (hallucinations and correctly detected) with <span class="ltx_text ltx_font_smallcaps" id="A1.SS1.p1.1.1">ProjectAway</span> and tally up the total number of unique hallucinated and CD objects in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.T4" title="In A.1 Mass-removing objects ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="A1.T4.2.1">Supplemental metrics for <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.T1" title="In 4.1.2 Mass-removing objects ‣ 4.1 Erasing objects from image representations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</span> We measure unique hallucinated and correctly detected (CD) objects.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.3.1.1.1.1">Edit Scope</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T4.3.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.3.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T4.3.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T4.3.1.1.3.1">Hallucinations</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T4.3.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T4.3.1.1.4.1">CD</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.3.2.1.1" rowspan="2"><span class="ltx_text" id="A1.T4.3.2.1.1.1">No edits</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.3.2.1.2">InstructBLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.3.2.1.3">4545</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.3.2.1.4">14178</td>
</tr>
<tr class="ltx_tr" id="A1.T4.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.3.3.2.1">LLaVA</th>
<td class="ltx_td ltx_align_left" id="A1.T4.3.3.2.2">4372</td>
<td class="ltx_td ltx_align_left" id="A1.T4.3.3.2.3">15053</td>
</tr>
<tr class="ltx_tr" id="A1.T4.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.3.4.3.1" rowspan="2"><span class="ltx_text" id="A1.T4.3.4.3.1.1">Hallucinations</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.3.4.3.2">InstructBLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.3.4.3.3">2672</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.3.4.3.4">14189</td>
</tr>
<tr class="ltx_tr" id="A1.T4.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.3.5.4.1">LLaVA</th>
<td class="ltx_td ltx_align_left" id="A1.T4.3.5.4.2">3348</td>
<td class="ltx_td ltx_align_left" id="A1.T4.3.5.4.3">15061</td>
</tr>
<tr class="ltx_tr" id="A1.T4.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T4.3.6.5.1" rowspan="2"><span class="ltx_text" id="A1.T4.3.6.5.1.1">CD</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.3.6.5.2">InstructBLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.3.6.5.3">5078</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.3.6.5.4">13864</td>
</tr>
<tr class="ltx_tr" id="A1.T4.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T4.3.7.6.1">LLaVA</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T4.3.7.6.2">4583</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T4.3.7.6.3">14826</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Ablations for InstructBLIP</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.3">We show hidden layer and weight ablations for mass-removing hallucinations in InstructBLIP referenced in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.SS2" title="4.2 Ablation Study: mass-removing hallucinations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. The hidden layer ablations indicate that most of the parameter space is too sensitive to edit and leads to losses in correctly detected objects. We find that smaller <math alttext="l^{T}" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1.1"><semantics id="A1.SS2.p1.1.m1.1a"><msup id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><mi id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml">l</mi><mi id="A1.SS2.p1.1.m1.1.1.3" xref="A1.SS2.p1.1.m1.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="A1.SS2.p1.1.m1.1.1.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2">𝑙</ci><ci id="A1.SS2.p1.1.m1.1.1.3.cmml" xref="A1.SS2.p1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">l^{T}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.1.m1.1d">italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="l^{I}" class="ltx_Math" display="inline" id="A1.SS2.p1.2.m2.1"><semantics id="A1.SS2.p1.2.m2.1a"><msup id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><mi id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml">l</mi><mi id="A1.SS2.p1.2.m2.1.1.3" xref="A1.SS2.p1.2.m2.1.1.3.cmml">I</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="A1.SS2.p1.2.m2.1.1.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2">𝑙</ci><ci id="A1.SS2.p1.2.m2.1.1.3.cmml" xref="A1.SS2.p1.2.m2.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">l^{I}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.2.m2.1d">italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT</annotation></semantics></math> parameters are the most effective for reducing hallucinations. Our best parameters <math alttext="(l^{I}=1,l^{T}=2)" class="ltx_Math" display="inline" id="A1.SS2.p1.3.m3.1"><semantics id="A1.SS2.p1.3.m3.1a"><mrow id="A1.SS2.p1.3.m3.1.1.1"><mo id="A1.SS2.p1.3.m3.1.1.1.2" stretchy="false">(</mo><mrow id="A1.SS2.p1.3.m3.1.1.1.1.2" xref="A1.SS2.p1.3.m3.1.1.1.1.3.cmml"><mrow id="A1.SS2.p1.3.m3.1.1.1.1.1.1" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.cmml"><msup id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.cmml"><mi id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.2" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.2.cmml">l</mi><mi id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.3" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.3.cmml">I</mi></msup><mo id="A1.SS2.p1.3.m3.1.1.1.1.1.1.1" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.1.cmml">=</mo><mn id="A1.SS2.p1.3.m3.1.1.1.1.1.1.3" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A1.SS2.p1.3.m3.1.1.1.1.2.3" xref="A1.SS2.p1.3.m3.1.1.1.1.3a.cmml">,</mo><mrow id="A1.SS2.p1.3.m3.1.1.1.1.2.2" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.cmml"><msup id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.cmml"><mi id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.2" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.2.cmml">l</mi><mi id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.3" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.3.cmml">T</mi></msup><mo id="A1.SS2.p1.3.m3.1.1.1.1.2.2.1" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.1.cmml">=</mo><mn id="A1.SS2.p1.3.m3.1.1.1.1.2.2.3" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.3.cmml">2</mn></mrow></mrow><mo id="A1.SS2.p1.3.m3.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.1b"><apply id="A1.SS2.p1.3.m3.1.1.1.1.3.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p1.3.m3.1.1.1.1.3a.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="A1.SS2.p1.3.m3.1.1.1.1.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1"><eq id="A1.SS2.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.1"></eq><apply id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2">superscript</csymbol><ci id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.2.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.2">𝑙</ci><ci id="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.3.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.2.3">𝐼</ci></apply><cn id="A1.SS2.p1.3.m3.1.1.1.1.1.1.3.cmml" type="integer" xref="A1.SS2.p1.3.m3.1.1.1.1.1.1.3">1</cn></apply><apply id="A1.SS2.p1.3.m3.1.1.1.1.2.2.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2"><eq id="A1.SS2.p1.3.m3.1.1.1.1.2.2.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.1"></eq><apply id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2">superscript</csymbol><ci id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.2.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.2">𝑙</ci><ci id="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.3.cmml" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.2.3">𝑇</ci></apply><cn id="A1.SS2.p1.3.m3.1.1.1.1.2.2.3.cmml" type="integer" xref="A1.SS2.p1.3.m3.1.1.1.1.2.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.1c">(l^{I}=1,l^{T}=2)</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.3.m3.1d">( italic_l start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT = 1 , italic_l start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = 2 )</annotation></semantics></math> reduce hallucinations by 38.5%. It is not fully understood why the majority of the parameter search space is invalid in comparison with LLaVA in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S4.F7" title="In 4.2 Ablation Study: mass-removing hallucinations ‣ 4 Erasing knowledge from VLMs ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>. It is possible that the fine-tuning step in LLaVA semantically aligns hidden image representations with text embeddings more than InstructBLIP, allowing linear edits to have the precise, intended effect.</p>
</div>
<figure class="ltx_figure" id="A1.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A1.F11.3">
<span class="ltx_ERROR undefined" id="A1.F11.3.1">{floatrow}</span><span class="ltx_ERROR undefined" id="A1.F11.3.2">\ffigbox</span><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="333" id="A1.F11.g1" src="x9.png" width="381"/>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
<span class="ltx_text ltx_font_bold" id="A1.F11.5.1">Hidden layer ablations for InstructBLIP</span>. We track hallucination reduction (%) across different layers to edit at and extract latent embeddings for the text embedding, crossing out (red) parameters from consideration where there is a decrease in correctly detected objects.

</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A1.F11.6">
<span class="ltx_ERROR undefined" id="A1.F11.6.1">\ffigbox</span><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="289" id="A1.F11.g2" src="x10.png" width="381"/>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>
<span class="ltx_text ltx_font_bold" id="A1.F11.8.1">Weight ablations for InstructBLIP.</span> We vary the weight factor <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.F11.2.m1.1"><semantics id="A1.F11.2.m1.1a"><mi id="A1.F11.2.m1.1.1" xref="A1.F11.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.F11.2.m1.1b"><ci id="A1.F11.2.m1.1.1.cmml" xref="A1.F11.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F11.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.F11.2.m1.1d">italic_α</annotation></semantics></math> and measure changes in correctly detected objects, object removal rate, and hallucination reduction. We observe a decline in hallucinations as weight increases and mark a weight where there is no loss in correctly detected objects.

</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Hallucination Detection</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We show quantitative comparisons from our hallucination detection approach using internal confidence (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS1" title="5.1 Hallucination Detection ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>) to the baseline in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.T5" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>. We also show qualitative examples for LLaVA in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F12" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a> and for InstructBLIP in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F13" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>. These samples exhibit model-generated captions, parsed objects, and whether they are classified as hallucinated or correctly detected based on their internal confidence score.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Hallucination Reduction</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">We exhibit sample results from our hallucination reduction approach (<a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#S5.SS2" title="5.2 Hallucination Removal ‣ 5 Applications ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>), which linearly removes text representations of hallucinations from image representations, in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F15" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">15</span></a> for InstructBLIP and <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F14" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a> for LLaVA. We show the image caption before and after our linear editing method, removing objects detected as hallucinations.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Object Localization</h3>
<div class="ltx_para ltx_noindent" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1">We show qualitative examples for localization with internal confidence for specific image representations, specifically for the LLaVA model, in <a class="ltx_ref" href="https://arxiv.org/html/2410.02762v1#A1.F16" title="In A.5 Object Localization ‣ Appendix A Appendix ‣ Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">16</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.4.4.5.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T5.4.4.5.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="A1.T5.4.4.5.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.4.4.5.1.2.1">InstructBLIP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T5.4.4.5.1.3"><span class="ltx_text ltx_font_bold" id="A1.T5.4.4.5.1.3.1">LLaVA</span></th>
</tr>
<tr class="ltx_tr" id="A1.T5.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A1.T5.4.4.4.5"><span class="ltx_text ltx_font_bold" id="A1.T5.4.4.4.5.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1">mAP</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.m1.1a"><mo id="A1.T5.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T5.2.2.2.2">
<span class="ltx_text ltx_font_bold" id="A1.T5.2.2.2.2.1">ROC AUC</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.2.2.2.2.m1.1"><semantics id="A1.T5.2.2.2.2.m1.1a"><mo id="A1.T5.2.2.2.2.m1.1.1" stretchy="false" xref="A1.T5.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.2.m1.1b"><ci id="A1.T5.2.2.2.2.m1.1.1.cmml" xref="A1.T5.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.3.3.3.3">
<span class="ltx_text ltx_font_bold" id="A1.T5.3.3.3.3.1">mAP</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.3.3.3.3.m1.1"><semantics id="A1.T5.3.3.3.3.m1.1a"><mo id="A1.T5.3.3.3.3.m1.1.1" stretchy="false" xref="A1.T5.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.3.m1.1b"><ci id="A1.T5.3.3.3.3.m1.1.1.cmml" xref="A1.T5.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.4.4.4.4">
<span class="ltx_text ltx_font_bold" id="A1.T5.4.4.4.4.1">ROC AUC</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.4.4.4.4.m1.1"><semantics id="A1.T5.4.4.4.4.m1.1a"><mo id="A1.T5.4.4.4.4.m1.1.1" stretchy="false" xref="A1.T5.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.4.m1.1b"><ci id="A1.T5.4.4.4.4.m1.1.1.cmml" xref="A1.T5.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.4.4.6.1.1">Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.4.6.1.2">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.4.4.6.1.3">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.4.6.1.4">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.4.6.1.5">0.47</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T5.4.4.7.2.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.4.7.2.2">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T5.4.4.7.2.3">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.4.7.2.4">0.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.4.7.2.5">0.68</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span class="ltx_text ltx_font_bold" id="A1.T5.8.1">Object Presence Classification performance.</span> We use internal confidence <math alttext="c_{o}" class="ltx_Math" display="inline" id="A1.T5.6.6.m1.1"><semantics id="A1.T5.6.6.m1.1b"><msub id="A1.T5.6.6.m1.1.1" xref="A1.T5.6.6.m1.1.1.cmml"><mi id="A1.T5.6.6.m1.1.1.2" xref="A1.T5.6.6.m1.1.1.2.cmml">c</mi><mi id="A1.T5.6.6.m1.1.1.3" xref="A1.T5.6.6.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T5.6.6.m1.1c"><apply id="A1.T5.6.6.m1.1.1.cmml" xref="A1.T5.6.6.m1.1.1"><csymbol cd="ambiguous" id="A1.T5.6.6.m1.1.1.1.cmml" xref="A1.T5.6.6.m1.1.1">subscript</csymbol><ci id="A1.T5.6.6.m1.1.1.2.cmml" xref="A1.T5.6.6.m1.1.1.2">𝑐</ci><ci id="A1.T5.6.6.m1.1.1.3.cmml" xref="A1.T5.6.6.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.6.6.m1.1d">c_{o}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.6.6.m1.1e">italic_c start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> as a confidence score to classify whether the object is present in the image. We evaluate the mAP and ROC AUC of our classification method against the baseline for both the InstructBLIP and LLaVA models.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="935" id="A1.F12.g1" src="x11.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold" id="A1.F12.2.1">LLaVA Object Presence Classification</span>. Sample image captions from LLaVA and the internal confidence scores for objects in the caption used for classification as correctly detected objects or hallucinations.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="935" id="A1.F13.g1" src="x12.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span class="ltx_text ltx_font_bold" id="A1.F13.2.1">InstructBLIP Object Presence Classification</span>.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="943" id="A1.F14.g1" src="x13.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span class="ltx_text ltx_font_bold" id="A1.F14.2.1">Qualitative results for LLaVA hallucination intervention</span>. Our algorithm removes hallucinations and, at times, adds correctly detected objects.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="933" id="A1.F15.g1" src="x14.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span><span class="ltx_text ltx_font_bold" id="A1.F15.2.1">Qualitative results for InstructBLIP hallucination intervention</span>.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="622" id="A1.F16.g1" src="x15.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span><span class="ltx_text ltx_font_bold" id="A1.F16.2.1">Object Localization Samples</span>.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 17:36:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
