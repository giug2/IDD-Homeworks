<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models</title>
<!--Generated on Fri Aug  2 21:50:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.01585v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S1" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S2" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background and Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S2.SS1" title="In II Background and Related Work ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Background</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S2.SS2" title="In II Background and Related Work ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Related Work</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Approach</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.SS1" title="In III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Log Grouping Based on Commonality</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.SS2" title="In III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">LLM-based Unsupervised Log Parsing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.SS3" title="In III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Template Memory for Efficient Log Parsing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S4" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S6" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Threats to Validity</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S7" title="In OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zeyang Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Software PErformance, Analysis 
<br class="ltx_break"/>and Reliability (SPEAR) Lab
<br class="ltx_break"/>Concordia University
<br class="ltx_break"/>Montreal, Quebec, Canada
<br class="ltx_break"/>m_zeyang@encs.concordia.ca
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dong Jae Kim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">DePaul University
<br class="ltx_break"/>Chicago, Illinois, USA
<br class="ltx_break"/>djaekim086@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tse-Hsun (Peter) Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Software PErformance, Analysis 
<br class="ltx_break"/>and Reliability (SPEAR) Lab
<br class="ltx_break"/>Concordia University
<br class="ltx_break"/>Montreal, Quebec, Canada
<br class="ltx_break"/>peterc@encs.concordia.ca
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Log parsing is a critical step that transforms unstructured log data into structured formats, facilitating subsequent log-based analysis.
Traditional syntax-based log parsers are efficient and effective, but they often experience decreased accuracy when processing logs that deviate from the predefined rules.
Recently, large language models (LLM) based log parsers have shown superior parsing accuracy.
However, existing LLM-based parsers face three main challenges: 1) time-consuming and labor-intensive manual labeling for fine-tuning or in-context learning, 2) increased parsing costs due to the vast volume of log data and limited context size of LLMs, and 3) privacy risks from using commercial models like ChatGPT with sensitive log information.
To overcome these limitations, this paper introduces OpenLogParser, an unsupervised log parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance privacy and reduce operational costs while achieving state-of-the-art parsing accuracy. OpenLogParser first groups logs with similar static text but varying dynamic variables using a fixed-depth grouping tree. It then parses logs within these groups using three components: i) similarity scoring-based retrieval augmented generation: selects diverse logs within each group based on Jaccard similarity, helping the LLM distinguish between static text and dynamic variables; ii) self-reflection: iteratively query LLMs to refine log templates to improve parsing accuracy; and iii) log template memory: stores parsed templates to reduce LLM queries for improved parsing efficiency. Our evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higher parsing accuracy and processes logs 2.7 times faster compared to state-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacy and cost concerns of using commercial LLMs while achieving state-of-the-arts parsing efficiency and accuracy.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Real-world software systems generate large amounts of logs, often hundreds of gigabytes or even terabytes per day <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib48" title="">48</a>]</cite>. These logs provide developers with invaluable runtime information, essential for understanding system execution and debugging. To manage and analyze this vast amount of data, researchers and practitioners have proposed many automated approaches, such as monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib43" title="">43</a>]</cite>, anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib40" title="">40</a>]</cite>, and root cause analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib33" title="">33</a>]</cite>.
However, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, logs are semi-structured, containing a mixture of static text and dynamically generated variables (e.g., port number <span class="ltx_text ltx_font_sansserif" id="S1.p1.1.1">62267</span>), which makes direct analysis challenging.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="382" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of log parsing result from Hadoop.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Log parsing is a critical first step in log analysis that transforms unstructured logs into log templates, dividing logs into static parts (static messages) and dynamic parts (variables). As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, log templates represent the event structure of logs, providing a standardized format that simplifies further analysis. By distinguishing between static and dynamic components, log parsing enables more efficient and accurate downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib26" title="">26</a>]</cite>.
Given the sheer volume and diversity of generated logs, prior research has proposed various syntax-based parsers for efficient and effective log parsing. These parsers, such as Drain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>]</cite> and AEL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib18" title="">18</a>]</cite>, use manually crafted heuristics or predefined rules to identify and extract log templates. Although promising, these log parsers often experience decreased accuracy when processing logs that deviate from predefined rules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recent advances in large language models (LLMs) have enabled researchers to leverage these models for log parsing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib9" title="">9</a>]</cite>. LLMs exhibit superior capabilities in understanding and generating text, making them particularly effective for parsing semi-structured log data. Consequently, LLM-based log parsers often achieve higher accuracy than traditional syntax-based parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>]</cite>. However, the sheer volume of log data and the limited context size of LLMs lead to increased parsing costs, both in terms of time and money, as token consumption grows linearly with log size. This makes practical adoption challenging. Additionally, these parsers frequently require manually derived log template pairs for in-context learning, adding significant manual overhead.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A further complication arises from the reliance on commercial LLMs like ChatGPT by many LLM-based log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib23" title="">23</a>]</cite>. While powerful, using commercial models poses potential privacy risks, as logs often contain sensitive information about the software’s runtime behavior and data. Uploading logs and other sensitive information (e.g., code for refactoring and bug fixes) to commercial LLMs can expose a company’s sensitive data to potential privacy breaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we propose an unsupervised log parsing technique, OpenLogParser, which does not need any manual labels. OpenLogParser leverages smaller-size open-source LLMs (e.g., Llama3-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite>) to enhance privacy and reduce operational costs while achieving state-of-the-art parsing accuracy and efficiency.
Inspired by the effective grouping capabilities of syntax-based unsupervised log parsing methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>]</cite>, OpenLogParser first groups logs that share syntactic similarity in the static text, but vary in the dynamic variable, using a fixed-depth grouping tree. Then, OpenLogParser parses logs within individual groups through three key steps: (i) OpenLogParser uses similarity scoring-based <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p5.1.1">retrieval augmented generation (RAG)</span> to select the most diverse logs based on Jaccard similarity within each log group. This step helps LLMs separate dynamic and static text by highlighting variability in dynamic variables among logs in the same group.
(ii) OpenLogParser uses <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p5.1.2">self-reflection</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib38" title="">38</a>]</cite> to improve LLM responses, thereby improving parsing results. iii) OpenLogParser uses <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p5.1.3">log template memory</span> to store parsed log templates. This approach allows logs to be parsed by first matching them with stored templates, minimizing the number of LLM queries and significantly enhancing parsing efficiency.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The paper makes the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce, OpenLogParser, an unsupervised log parsing technique that effectively addresses the limitations of existing LLM-based and syntax-based parsers.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">OpenLogParser employs open-source LLMs, specifically Llama3-8B, to enhance data privacy and reduce operational costs associated with commercial models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Through extensive evaluations on over 50 million logs from LogHub2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>]</cite>, OpenLogParser demonstrated a 25% or higher parsing accuracy compared to state-of-the-art LLM-based log parsers (i.e., LILAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>]</cite> and LLMParser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>]</cite>). Moreover, it is 2.75 to 40 times faster, showcasing its superior efficiency and effectiveness.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">OpenLogParser’s self-reflection mechanism helps improve parsing accuracy by over 7%, showcasing the effectiveness of our prompting technique.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">Our experiment using four small-size LLMs shows that Llama3-8B achieves the best overall result, highlighting its potential in log analysis.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In short, the paper provides a novel unsupervised log parsing approach that is both efficient and effective while ensuring data privacy and reducing operational costs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><span class="ltx_text ltx_font_bold" id="S1.p9.1.1">Paper Organization.</span> Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S2" title="II Background and Related Work ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">II</span></a> discusses background and related work. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3" title="III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a> provides the design details of OpenLogParser. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S4" title="IV Experiment Setup ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">IV</span></a> outlines evaluation setup. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5" title="V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">V</span></a> presents evaluation results. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S6" title="VI Threats to Validity ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">VI</span></a> discusses threats to validity. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S7" title="VII Conclusion ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">VII</span></a> concludes the paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background and Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we discuss the background of LLM and its privacy concerns. We then discuss related log parsing research.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Background</span>
</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">Large Language Models.</span>
Large Language Models (LLMs), primarily built on the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib34" title="">34</a>]</cite>, have significantly advanced the field of natural language processing (NLP). These LLMs, such as the widely recognized GPT-3 model with its 175 billion parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib7" title="">7</a>]</cite>, are trained on diverse text data from various sources, including source code. The training involves self-supervised learning objectives that enable these models to develop a deep understanding of language and generate text that is contextually relevant and semantically coherent. LLMs have shown substantial capability in tasks that involve complex language comprehension and generation, such as code recognition and generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib5" title="">5</a>]</cite>.
Due to logs being semi-structured texts composed of natural language and code elements, researchers have adopted LLMs to tackle log analysis tasks, such as anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib40" title="">40</a>]</cite>, root cause analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib33" title="">33</a>]</cite>, and log parsing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib9" title="">9</a>]</cite>. Log parsing is one of the primary tasks of focus in this area, given its crucial role for more accurate and insightful downstream log analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib37" title="">37</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Privacy Issues Related to LLM.</span> While LLMs demonstrate remarkable capabilities in processing and generating natural language and code, their application on sensitive data such as logs presents notable privacy risks, particularly with commercial models such as ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib2" title="">2</a>]</cite>. One major concern is that data transmitted to these models–such as system logs–could be retained and used in the model’s further training cycles without explicit consent or knowledge of the data owners <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib6" title="">6</a>]</cite>. More importantly, sensitive data uploaded to the LLM providers could potentially be exposed through inadvertent data leaks or malicious attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib16" title="">16</a>]</cite>, posing significant privacy risks. To avoid such risks, an industry norm is to restrict the use of commercial LLMs despite their advanced capabilities. For example, Samsung bans ChatGPT and other commercial chatbots after a sensitive code leak <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib1" title="">1</a>]</cite>. Major financial institutions like Citigroup and Goldman Sachs have restricted the use of ChatGPT due to concerns over data privacy and security <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib3" title="">3</a>]</cite>. In contrast, open-source LLMs, such as those developed by Meta’s Llama series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib34" title="">34</a>]</cite>, offer greater privacy and security. Users can adopt the LLMs for local deployment to ensure data privacy, aligning with stringent data protection standards.
Thus, open-source LLMs are more secure and trustworthy for handling confidential data such as logs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Related Work</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Current automated log parsers can be broadly categorized into two types: syntax-based log parsers and semantic-based log parsers. Syntax-based log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>]</cite> typically employ heuristic rules or conduct comparisons among logs to identify common components that serve as templates. Semantic-based log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib30" title="">30</a>]</cite> focus on analyzing the textual content within logs to distinguish between static and dynamic segments (i.e., using LLMs), thereby deriving the log templates. Semantic-based parsers often require a data-driven approach to better grasp the semantic nuances inherent in the specific system logs they analyze.
Below, we discuss related work and the limitations of these two groups of parsers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Syntax-based Log parsing approaches.</span>
Syntax-based log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib12" title="">12</a>]</cite> generally utilize manually crafted heuristics or compare syntactic features between logs to extract log templates.
Different from general text data, log messages have some unique characteristics. Heuristic-based log parsers extract log templates by identifying features in the logs.
For example, AEL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib18" title="">18</a>]</cite> uses heuristics to remove potential dynamic variables and extract log templates. Drain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>]</cite> employs a fixed-depth parsing tree structure alongside specifically designed parsing rules (i.e., top-k prefix tokens) to identify common templates.
However, these log parsers often suffer from decreased accuracy when processing logs that do not conform to the predefined rules.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Logs with the same log template share the same static messages in the log. Based on this observation, several log parsers leverage frequent pattern mining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>]</cite> to parse the logs by identifying common textual content within logs.
For instance, Spell <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib12" title="">12</a>]</cite> uses the Longest Common Subsequence to parse logs, and Logram <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>]</cite> identifies frequent <math alttext="n-gram" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">n</mi><mo id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">−</mo><mrow id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml"><mi id="S2.SS2.p3.1.m1.1.1.3.2" xref="S2.SS2.p3.1.m1.1.1.3.2.cmml">g</mi><mo id="S2.SS2.p3.1.m1.1.1.3.1" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p3.1.m1.1.1.3.3" xref="S2.SS2.p3.1.m1.1.1.3.3.cmml">r</mi><mo id="S2.SS2.p3.1.m1.1.1.3.1a" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p3.1.m1.1.1.3.4" xref="S2.SS2.p3.1.m1.1.1.3.4.cmml">a</mi><mo id="S2.SS2.p3.1.m1.1.1.3.1b" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p3.1.m1.1.1.3.5" xref="S2.SS2.p3.1.m1.1.1.3.5.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><minus id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1"></minus><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">𝑛</ci><apply id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3"><times id="S2.SS2.p3.1.m1.1.1.3.1.cmml" xref="S2.SS2.p3.1.m1.1.1.3.1"></times><ci id="S2.SS2.p3.1.m1.1.1.3.2.cmml" xref="S2.SS2.p3.1.m1.1.1.3.2">𝑔</ci><ci id="S2.SS2.p3.1.m1.1.1.3.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3.3">𝑟</ci><ci id="S2.SS2.p3.1.m1.1.1.3.4.cmml" xref="S2.SS2.p3.1.m1.1.1.3.4">𝑎</ci><ci id="S2.SS2.p3.1.m1.1.1.3.5.cmml" xref="S2.SS2.p3.1.m1.1.1.3.5">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">n-gram</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">italic_n - italic_g italic_r italic_a italic_m</annotation></semantics></math> patterns within logs, using these recurring patterns to parse logs. While these frequent pattern mining-based parsers do not require manually defined rules, the templates they generate are highly dependent on the structure of the input logs. Logs with complex structures may lead to poor frequent pattern mining results, resulting in low parsing accuracy.
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.p3.1.1">In short, while syntax-based parsers benefit from simplicity and efficiency in identifying common templates, their performance varies depending on the structure of logs.
</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">Semantic-based log parsing approaches.</span>
Semantic-based log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib9" title="">9</a>]</cite> use language models to analyze the semantics of the log messages for log parsing. Recently, they have shown superior parsing accuracy compared to syntax-based log parsers, largely due to significant advancements in language models.
For instance, models like ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib23" title="">23</a>]</cite> can analyze the context of log messages and dynamically generate log templates without prior knowledge, enhancing accuracy and adaptability across different log formats. DivLog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib44" title="">44</a>]</cite> enhances log parsing by extracting similar logs from a candidate set of labeled logs for in-context learning using GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib7" title="">7</a>]</cite>.
Due to the high cost of commercial LLMs such as ChatGPT, LILAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>]</cite> enhances the efficiency of LLM-based log parsing by incorporating an Adaptive Parsing Cache that stores parsing results. LILAC adopts in-context learning with log-parsing demonstrations (i.e., manually created log templates) for enhanced parsing accuracy.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Some parsers also aim to use open-source LLMs for log parsing.
Hooglle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib9" title="">9</a>]</cite> adopted an LLM pre-trained on labeled logs for log parsing. LogPPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>]</cite> utilizes a masked language model (RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib29" title="">29</a>]</cite>) and adopts few-shot learning to classify tokens in log messages based on few-shot examples. As an initial attempt to apply LLMs for log parsing, LogPPT showed improved accuracy over traditional syntax-based log parsers.
LLMParser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>]</cite> explores the performance of various LLMs after a few-shot fine-tuning on log parsing. Results indicate that fine-tuning small open-source LLMs with a few demonstrations can also achieve high log parsing accuracy.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.p6.1.1">Although the results are promising, recent works in semantic-based log parsers have two main limitations: 1) privacy and monetary costs of using commercial LLMs and 2) requiring manually derived log templates for LLMs to learn</span>. First, most log parsers are based on commercial LLMs such as ChatGPT, which makes real-world adoption a challenge due to the privacy issues and monetary costs of parsing large volumes of logs.
Second, many parsers, especially the ones that aim to improve efficiency and accuracy (e.g., LILAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>]</cite>) or the ones that use smaller open-source models (e.g., LogPPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>]</cite> and LLMParser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>]</cite>) require some log-template pairs as the demonstration. Deriving such templates requires significant manual efforts, and the provided demonstrations may affect the parser’s accuracy on logs with unseen templates.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1">In this paper, we propose OpenLogParser that addresses the two above-mentioned limitations. We deployed a relatively small open-source LLM (i.e., Llama3-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite>) on log parsing to avoid privacy issues and monetary costs. Additionally, OpenLogParser enhances LLM-based log parsing by capitalizing on the commonalities and variabilities within logs to provide a demonstration-free prompt for the LLM.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Approach</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce OpenLogParser, an efficient unsupervised log parser, leveraging memory capabilities and advanced prompting techniques to maximize efficiency and parsing accuracy. OpenLogParser leverages a smaller-size open-source LLM to enhance privacy and reduce operation costs. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.F2" title="Figure 2 ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the overall architecture of OpenLogParser, which primarily comprises of three components:
(i) <span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.p1.1.1">log grouping</span>, which groups logs that share a commonality in their text. Such log groups can then be used as input to LLM to uncover dynamic variables.
(ii) An <span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.p1.1.2">unsupervised LLM-based log parser</span> that uses retrieval-augmented generation (RAG), followed by an iterative self-reflection mechanism to accurately parse the grouped logs into log templates.
(iii) An <span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.p1.1.3">efficient log template memory</span>, which memorizes the parsed log templates for future query. The core idea is to enhance efficiency by storing parsed log templates in memory, thereby avoiding the need for repeated LLM queries.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="377" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of OpenLogParser.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Log Grouping Based on Commonality</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">OpenLogParser achieves unsupervised and zero-shot log parsing by first applying an effective grouping strategy. This strategy aims to group logs that share commonality in their static text, yet are different in their dynamic variables. Such log groups can then be used as input to LLMs to generate log templates by prompting LLMs to identify the dynamic variables among logs in the same group. To group the logs, we adapt the efficient unsupervised methodology proposed by Drain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>]</cite>, which applies a fixed-depth parsing tree and parsing rules (i.e., <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_K</annotation></semantics></math> prefix tokens) to identify log groups. The fixed depth in our grouping tree provides a structured and predictable framework that enhances efficiency. By limiting the depth, we reduce the complexity of the tree traversal, which speeds up the grouping process.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">Our fixed-depth tree implementation for grouping consists of three key steps: (i) group by <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS1.p2.3.1">length</span>, (ii) group by <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS1.p2.3.2">K prefix tokens</span>, and (iii) group by <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS1.p2.3.3">token string similarity</span>. In step (i), we first group the logs based on <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.3.4">token length</span>, which partitions the logs into subsets of logs that are similar in token length. This initial grouping significantly reduces the computational complexity in the subsequent grouping phases.
In step (ii), the grouped logs are then kept at a fixed depth which stores <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_K</annotation></semantics></math> prefix tokens.
Since logs are initially grouped based on <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.3.5">token length</span>, truncating <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_K</annotation></semantics></math> prefix tokens (default the first three tokens of the log) can limit the number of nodes visited during the subsequent traversal process for step (iii), significantly improving grouping efficiency.
Prior to step (iii), it is important to note that we abstract the numerical literals in the logs with a wildcard symbol (*). This is done to prevent the issue of grouping explosion in step (iii), which can make grouping inefficient. Finally, in step (iii), we calculate the similarity between the new logs and the log groups stored in the fixed-depth tree. This step determines whether the incoming log fits into an existing group or necessitates the creation of a new log group. If a suitable group is found based on the similarity threshold, i.e., <math alttext="\frac{\text{\# of common tokens}}{\text{total number of tokens}}&gt;0.5" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mfrac id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml"><mtext id="S3.SS1.p2.3.m3.1.1.2.2" xref="S3.SS1.p2.3.m3.1.1.2.2a.cmml"># of common tokens</mtext><mtext id="S3.SS1.p2.3.m3.1.1.2.3" xref="S3.SS1.p2.3.m3.1.1.2.3a.cmml">total number of tokens</mtext></mfrac><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">&gt;</mo><mn id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><gt id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></gt><apply id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"><divide id="S3.SS1.p2.3.m3.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.2"></divide><ci id="S3.SS1.p2.3.m3.1.1.2.2a.cmml" xref="S3.SS1.p2.3.m3.1.1.2.2"><mtext id="S3.SS1.p2.3.m3.1.1.2.2.cmml" mathsize="70%" xref="S3.SS1.p2.3.m3.1.1.2.2"># of common tokens</mtext></ci><ci id="S3.SS1.p2.3.m3.1.1.2.3a.cmml" xref="S3.SS1.p2.3.m3.1.1.2.3"><mtext id="S3.SS1.p2.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S3.SS1.p2.3.m3.1.1.2.3">total number of tokens</mtext></ci></apply><cn id="S3.SS1.p2.3.m3.1.1.3.cmml" type="float" xref="S3.SS1.p2.3.m3.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\frac{\text{\# of common tokens}}{\text{total number of tokens}}&gt;0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">divide start_ARG # of common tokens end_ARG start_ARG total number of tokens end_ARG &gt; 0.5</annotation></semantics></math>, the log is inserted into existing log groups. If not, a new group is created, and the tree is dynamically updated to accommodate this new log pattern. This adaptive approach ensures that our system evolves with the incoming data, continuously optimizing both the accuracy and efficiency of the log grouping process.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">LLM-based Unsupervised Log Parsing</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our prompts to LLMs contain representative logs (based on variability) retrieved from each log group (from Section <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.SS1" title="III-A Log Grouping Based on Commonality ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>) to guide LLMs in separating dynamic variables and static text. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.F3" title="Figure 3 ‣ III-B LLM-based Unsupervised Log Parsing ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the prompt template that OpenLogParser uses. Below, we discuss the composition of our prompt in detail.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S3.SS2.p2.1.1">Prompt Instruction.</span>
In the instruction part of our prompt, we define the goal of the log parsing task to the LLM (highlighted in green in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.F3" title="Figure 3 ‣ III-B LLM-based Unsupervised Log Parsing ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>). We emphasize that all the provided logs should share one common template that matches all selected logs. This specification is crucial to ensure that the LLM can effectively identify the commonalities and variability within the provided logs, thereby preventing any difficulties in parsing due to inconsistent log templates.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S3.SS2.p3.1.1">Standardizing LLM Response by Input and Output Example.</span>
Since our LLM is not instruction fine-tuned <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>]</cite>, it is crucial to clearly describe our task instruction and include an input-output example in the prompt. This explicit guidance helps the LLM understand the desired input and output formats.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.F3" title="Figure 3 ‣ III-B LLM-based Unsupervised Log Parsing ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>, we provide one example to illustrate the input/output form. The example remains unchanged for all systems.
This approach effectively guides the LLM in understanding the objective and input-output formats without the need of instruction fine-tuning or labeled data.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1055" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of the prompt template used for OpenLogParser. The green block illustrates the task instruction provided to the LLM. The blue block highlights the input and output examples used to standardize the log response format. The yellow block depicts the retrieval-augmented selection process that enhances log parsing accuracy by incorporating representative variability.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S3.SS2.p4.1.1">Retrieval-Augmented Log Parsing.</span>
To parse logs accurately, we select representative logs that showcase variabilities within a log group based on commonality. By presenting the LLM with logs sharing the same structure but varying in dynamic variables, it can more effectively distinguish between fixed and dynamic elements to identify the log template. We developed a retrieval argument generation (RAG) approach based on Jaccard Similarity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib41" title="">41</a>]</cite>.
Jaccard similarity measures the similarity between two given sets by calculating the ratio of the number of elements (e.g., tokens) in their intersection to the number of elements in their union.
For log data, each log is split into a set of tokens (i.e., words), and then these tokens are used to determine the sizes of the intersection and union. The resulting ratio is the Jaccard similarity between two given logs, with a ratio closer to one indicating higher similarity.
We aim to identify the logs with the greatest variability within the same group. Hence, we select logs with the lowest Jaccard similarity score. This approach helps create accurate log templates by focusing on logs that are most indicative of the entire group’s characteristics.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Our selection process starts by selecting the longest log (based on the number of characters) within the group as the initial reference. We then calculate the Jaccard similarity between this log and every other log in the group. The log with the lowest similarity to the reference log is added to the selection set. We continue computing pairwise Jaccard similarity between the selected logs and the remaining unselected logs, sequentially adding the log with the lowest similarity. This iterative process is repeated until <span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">K</span> (default <math alttext="K=3" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">K</mi><mo id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><eq id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"></eq><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝐾</ci><cn id="S3.SS2.p5.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p5.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">K=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_K = 3</annotation></semantics></math>) logs have been selected, ensuring the selected logs effectively represent both the commonality and diversity within the log group. Given the computation costs and to ensure efficiency, we randomly select at most 200 logs from each group (or all the logs if the number is less than 200) for our log selection process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">Specifically, the logs selected from the log group are listed in the format of a Python list within the prompt for parsing. We use a prefix (i.e., ‘<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p6.1.1">Log list:</span>’) to help the LLM identify the logs that require parsing (highlighted in yellow in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.F3" title="Figure 3 ‣ III-B LLM-based Unsupervised Log Parsing ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>). This consistency in input format, mirroring the “Input and Output Example”, also guides the LLM to respond with the log template in a fixed format as demonstrated in the example, facilitating accurate template generation and extraction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1"><span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S3.SS2.p7.1.1">Post-processing Template Standardization.</span>
We use a post-processing technique to further standardize the log template generated by LLM. We employ string manipulation techniques to remove non-template content from the response (i.e., prefixes and backticks). To facilitate the verification of the accuracy of log templates, we replace the placeholder ”<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p7.1.2">&lt;*&gt;</span>” within the templates with the regular expression pattern ”<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p7.1.3">(.*?)</span>”. The regex template enables a direct matching process when comparing the generated templates with logs, and can be directly applied to abstract logs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1"><span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S3.SS2.p8.1.1">Self-Reflection for Verifying Log Template.</span> After generating a log template, we verify whether the template can match each log within the group. If a log is correctly matched by a log template, we consider it to be parsed successfully. The log template is then added to the log template memory for future use. After all logs in the group have been checked, any unparsed logs undergo a <span class="ltx_text ltx_font_sansserif" id="S3.SS2.p8.1.2">self-reflection</span> process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib38" title="">38</a>]</cite>, which aims to revise the templates and improve parsing results. Similar to the initial parsing attempt, we first select these unparsed logs and then utilize the prompt described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S3.F3" title="Figure 3 ‣ III-B LLM-based Unsupervised Log Parsing ‣ III Approach ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> to generate a new log template using LLMs. This step is repeated until all logs in the group can be matched/parsed by the generated templates. Note that, to prevent the LLM from entering a parsing loop (i.e., repeatedly generating incorrect templates), we limit the self-reflection process to three iterations.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Template Memory for Efficient Log Parsing</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Repeatedly using LLM to parse logs with identical groupings and templates significantly increases the frequency of LLM queries, thereby reducing the efficiency of the log parsing process. To address this issue, we introduce <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.p1.1.1">log template memory</span> in OpenLogParser, which stores the parsed log templates for future parsing, avoiding redundant LLM queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S3.SS3.p2.1.1">Efficient Log Template Memory Search and Matching.</span>
When a log group requires parsing, we first check whether a matching log template exists within the memory. If some logs within the group find a matching template in the memory, we apply this log template to parse the logs, mitigating the need for LLM queries. However, it is possible that some logs within the same group may match while others may not (e.g., due to limitations in the grouping step or limitation of the log template). Hence, the logs that remain unparsed are then sent to LLM for parsing. The new log template generated from this process is then added to the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.p2.1.2">log template memory</span> for future reference. This design significantly reduces the number of LLM queries during the log parsing process.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.4">To efficiently utilize log templates in the memory, there is a need for an efficient <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.p3.4.1">search</span> mechanism to verify whether or not the given logs match existing log templates in the memory. This is crucial since the memory can be large, consisting of many log templates. For every log, we need potentially at most <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_N</annotation></semantics></math> searches for <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_N</annotation></semantics></math> log templates.
To improve efficiency, we put forward one key observation: the token length of log templates is always less than or equal to that of the original logs, as multiple tokens may be treated as a single variable during log parsing. For instance, consider the log <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.4.2">‘sent 100 bytes data’</span>. After parsing, the corresponding log template is generated as <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.4.3">‘sent &lt;*&gt; data’</span>. The original log consists of four tokens, whereas the parsed template has three. This reduction in token count occurs because <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.4.4">‘100 bytes’</span> is treated as a single variable, thus decreasing the overall length of the template compared to the original log. Consequently, when searching for log templates in the memory, we first sort the templates based on the number of tokens. This sorting allows us to efficiently check new logs by first calculating the token length of the log to be parsed, then using binary search to find all templates with a token count less than or equal to the log length. This design reduces the number of match checks required from <math alttext="O(N)" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.2" xref="S3.SS3.p3.3.m3.1.2.cmml"><mi id="S3.SS3.p3.3.m3.1.2.2" xref="S3.SS3.p3.3.m3.1.2.2.cmml">O</mi><mo id="S3.SS3.p3.3.m3.1.2.1" xref="S3.SS3.p3.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.3.m3.1.2.3.2" xref="S3.SS3.p3.3.m3.1.2.cmml"><mo id="S3.SS3.p3.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.3.m3.1.2.cmml">(</mo><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">N</mi><mo id="S3.SS3.p3.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.2.cmml" xref="S3.SS3.p3.3.m3.1.2"><times id="S3.SS3.p3.3.m3.1.2.1.cmml" xref="S3.SS3.p3.3.m3.1.2.1"></times><ci id="S3.SS3.p3.3.m3.1.2.2.cmml" xref="S3.SS3.p3.3.m3.1.2.2">𝑂</ci><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">O(N)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">italic_O ( italic_N )</annotation></semantics></math> to <math alttext="O(LogN)" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><mrow id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">O</mi><mo id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS3.p3.4.m4.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS3.p3.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS3.p3.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p3.4.m4.1.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.1.1.1.2" xref="S3.SS3.p3.4.m4.1.1.1.1.1.2.cmml">L</mi><mo id="S3.SS3.p3.4.m4.1.1.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p3.4.m4.1.1.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.1.1.3.cmml">o</mi><mo id="S3.SS3.p3.4.m4.1.1.1.1.1.1a" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p3.4.m4.1.1.1.1.1.4" xref="S3.SS3.p3.4.m4.1.1.1.1.1.4.cmml">g</mi><mo id="S3.SS3.p3.4.m4.1.1.1.1.1.1b" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p3.4.m4.1.1.1.1.1.5" xref="S3.SS3.p3.4.m4.1.1.1.1.1.5.cmml">N</mi></mrow><mo id="S3.SS3.p3.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS3.p3.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><times id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2"></times><ci id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">𝑂</ci><apply id="S3.SS3.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1"><times id="S3.SS3.p3.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1"></times><ci id="S3.SS3.p3.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.2">𝐿</ci><ci id="S3.SS3.p3.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.3">𝑜</ci><ci id="S3.SS3.p3.4.m4.1.1.1.1.1.4.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.4">𝑔</ci><ci id="S3.SS3.p3.4.m4.1.1.1.1.1.5.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.5">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">O(LogN)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_O ( italic_L italic_o italic_g italic_N )</annotation></semantics></math>, thereby enhancing the efficiency of the search process.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Our log-template matching process is efficient. Unlike traditional log templates that use placeholders (i.e., <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p4.1.1">‘‘&lt;*&gt;’’</span>) to abstract dynamic variables within logs, we store log templates in memory as regular expression patterns (i.e., use <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p4.1.2">‘‘(.?)’’</span> instead of placeholders). This adjustment allows us to use regular expressions to efficiently verify whether logs match with log templates in memory and improve matching efficiency.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiment Setup</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we discuss our experiment setup to answer our research questions and OpenLogParser’s implementation details.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Studied Dataset.</span>
We conduct our experiment on the log parsing benchmark LogHub-2.0 provided by He et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>]</cite>.
This benchmark contains logs from 14 open-source systems of different types, such as distributed systems, supercomputer systems, and server-side applications. LogHub is widely used to evaluate and compare the accuracy of log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>]</cite>.
Compared to LogHub-1.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib15" title="">15</a>]</cite>, the number of logs has increased significantly in LogHub-2.0, increasing from 28K (2K logs per system) to more than 50 million logs with a total of 3,488 different log templates.
LogHub-2.0 also provides the groundtruth log template for each log.
With this large-scale LogHub-2.0 dataset, researchers can better evaluate the efficiency and effectiveness of log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Environment and Implementation.</span>
Our experiments were conducted on an Ubuntu server with an NVIDIA Tesla A100 GPU, AMD EPYC 7763 64-core CPU, and 256GB RAM using Python 3.9. We execute the baselines using their default parameters under the same environment to compare the efficiency. We use Llama3 8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite> for OpenLogParser’s underlying LLM because it is a relatively small yet powerful model, balancing performance and efficiency effectively. We set the temperature value to 0 to improve the stability of the model output.
Note that it is easy to switch to other LLMs. In RQ4, we evaluate OpenLogParser by replacing Llama3 with other open-source LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Evaluation Metrics for Log Parsing. </span>
Following prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>]</cite>, we use two most commonly used metrics to evaluate the effectiveness of log parsers: Group Accuracy and Parsing Accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic ltx_framed ltx_framed_underline" id="S4.p5.1.1">Group Accuracy (GA):</span>
Grouping Accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib48" title="">48</a>]</cite> is a metric used in log parsing to evaluate the extent to which log messages belonging to the same template are correctly grouped together by a parser.
GA is defined as the ratio of correctly grouped log messages to the total number of log messages. For a log message to be considered correctly grouped, it must be assigned to the same group as other log messages that share the same underlying template.
High GA indicates that the parser can effectively discern patterns within the log data and group similar log messages together. This can be crucial for various downstream log analysis tasks such as anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib37" title="">37</a>]</cite>.
Despite its usefulness, GA has limitations.
GA can remain high even if the parsed templates are flawed. Namely, a high GA score might obscure errors in dynamic variable extraction and template identification within the logs, leading to a misleading perception of overall parsing accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic ltx_framed ltx_framed_underline" id="S4.p6.1.1">Parsing Accuracy (PA):</span>
Parsing Accuracy (PA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib30" title="">30</a>]</cite> complements GA and is calculated as the ratio of accurately parsed log messages to the total number of log messages.
For a log message to be deemed correctly parsed, both extracted static text and dynamic variables must match exactly with those specified in the ground truth.
PA is a stricter metric because it requires a comprehensive match of all log components, not just their correct grouping. This distinction is crucial, as GA primarily evaluates the correct clustering of logs, while PA ensures precise parsing accuracy at the individual log message level.
Precise log parsing of the variables can also significantly impact the effectiveness of downstream log-based analyses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we evaluate OpenLogParser by answering four research questions (RQs).</p>
</div>
<section class="ltx_subsection" id="S5.SSx1">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">RQ1: What is the effectiveness of OpenLogParser? </h3>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p1">
<p class="ltx_p" id="S5.SSx1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SSx1.p1.1.1">Motivation.</span> Accuracy is the most critical factor for evaluating the effectiveness of log parsers. High accuracy in log parsing aids downstream log analysis tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib37" title="">37</a>]</cite>. In this RQ, we study the effectiveness of OpenLogParser.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>A comparison of the grouping accuracy (GA) and parsing accuracy (PA) for the state-of-the-art parsers and OpenLogParser.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:539.3pt;height:239.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.1pt,33.7pt) scale(0.78,0.78) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.1.1">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.2" style="padding-left:15.1pt;padding-right:15.1pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T1.1.1.1.3" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1">AEL</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T1.1.1.1.4" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.4.1">Drain</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T1.1.1.1.5" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.5.1">LILAC</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" colspan="2" id="S5.T1.1.1.1.1" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">LLMParser<sub class="ltx_sub" id="S5.T1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T1.1.1.1.1.1.1.1">T5Base</span></sub></span>      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T1.1.1.1.6" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.6.1">OpenLogParser</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.2">
<td class="ltx_td ltx_border_r" id="S5.T1.1.1.2.1" style="padding-left:15.1pt;padding-right:15.1pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.2" style="padding-left:15.1pt;padding-right:15.1pt;">     GA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.3" style="padding-left:15.1pt;padding-right:15.1pt;">     PA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.4" style="padding-left:15.1pt;padding-right:15.1pt;">     GA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.5" style="padding-left:15.1pt;padding-right:15.1pt;">     PA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.6" style="padding-left:15.1pt;padding-right:15.1pt;">     GA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.7" style="padding-left:15.1pt;padding-right:15.1pt;">     PA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.8" style="padding-left:15.1pt;padding-right:15.1pt;">     GA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.1.1.2.9" style="padding-left:15.1pt;padding-right:15.1pt;">     PA      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.10" style="padding-left:15.1pt;padding-right:15.1pt;">     GA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.11" style="padding-left:15.1pt;padding-right:15.1pt;">     PA</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.1.1.3.1" style="padding-left:15.1pt;padding-right:15.1pt;">     HDFS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9994</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6213</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9990</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6210</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.6" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.3.6.1">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9480</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8295</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.1.1.3.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9663      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.3.10.1">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.3.11.1">1.0000</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.4.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Hadoop</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8230</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5350</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9210</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5410</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9240</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7850</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8376</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.4.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8370      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.4.10.1">0.9625</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.4.11.1">0.8706</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.5.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Spark</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.2" style="padding-left:15.1pt;padding-right:15.1pt;">     –</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.3" style="padding-left:15.1pt;padding-right:15.1pt;">     –</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.1">0.8880</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3940</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8700</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7080</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.2589</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.5.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4231      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8586</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.11.1">0.8887</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.6.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Zookeeper</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.2" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.2.1">0.9960</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.6.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8420</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9940</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.6.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8430</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9970</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.6.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3780</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7192</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.6.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8408      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9932</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.11.1">0.8499</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.7.1" style="padding-left:15.1pt;padding-right:15.1pt;">     BGL</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.7.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9146</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4062</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.7.4" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.4.1">0.9190</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4070</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.7.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8335</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8239</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.7.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.1439</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.7.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.2440      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.7.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9024</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.7.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.11.1">0.9293</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.8.1" style="padding-left:15.1pt;padding-right:15.1pt;">     HPC</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.8.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7480</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7410</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.8.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7930</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7210</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.8.6" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.8.6.1">0.8450</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7350</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.8.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6423</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.8.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7070      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.8.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8440</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.8.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.8.11.1">0.9730</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.9.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Thunderbird</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.9.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7859</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.9.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.1635</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.9.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8310</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.9.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.2160</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.9.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7940</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.9.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3860</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.9.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5790</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.9.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3472      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.9.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.10.1">0.8699</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.9.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.11.1">0.6940</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.10.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Linux</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.10.2" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.10.2.1">0.9160</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.10.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.0820</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.10.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6860</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.10.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.1110</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.10.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7636</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.10.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7001</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.10.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4999</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.10.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8802      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.10.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9120</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.10.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.10.11.1">0.9017</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.11.1" style="padding-left:15.1pt;padding-right:15.1pt;">     HealthApp</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.11.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7250</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.11.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3110</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.11.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8620</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.11.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3120</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.11.6" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.11.6.1">0.9930</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.11.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6730</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.11.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8364</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.11.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9674      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.11.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8617</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.11.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.11.11.1">0.9735</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.12.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Apache</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.12.2" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.12.2.1">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.12.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7270</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.12.4" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.12.4.1">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.12.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7270</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.12.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9970</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.12.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9920</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.12.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8680</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.12.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9900      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.12.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.12.10.1">1.0000</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.12.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.12.11.1">0.9960</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.13.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Proxifier</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.13.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.9740</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.13.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6770</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.13.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6920</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.13.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6880</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.13.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5060</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.13.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7780</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.13.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8276</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.13.9" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.13.9.1">0.9817</span>      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.13.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5101</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.13.11" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8970</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.14.1" style="padding-left:15.1pt;padding-right:15.1pt;">     OpenSSH</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.14.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7050</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.14.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3640</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.14.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7070</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.14.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5860</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.14.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7480</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.14.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6550</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.14.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.2183</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.14.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7812      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.14.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.14.10.1">0.8678</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.14.11" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4955</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.15.1" style="padding-left:15.1pt;padding-right:15.1pt;">     OpenStack</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.15.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7430</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.15.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.0290</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.15.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7520</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.15.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.0290</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.15.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.5240</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.15.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4860</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.15.8" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.15.8.1">0.9517</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.15.9" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.15.9.1">0.9412</span>      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.15.10" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8114</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.15.11" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8308</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.1.16.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Mac</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.16.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7970</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.16.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.2450</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.16.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7610</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.16.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.3570</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.16.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8090</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.16.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4480</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.16.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6710</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.1.16.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6131      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.16.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.16.10.1">0.8141</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.16.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.16.11.1">0.6538</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.17">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.1.17.1" style="padding-left:15.1pt;padding-right:15.1pt;">     Average</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.2" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8559</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.1.17.3" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4418</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.4" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8432</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.1.17.5" style="padding-left:15.1pt;padding-right:15.1pt;">     0.4681</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.6" style="padding-left:15.1pt;padding-right:15.1pt;">     0.8289</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.1.17.7" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6783</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.8" style="padding-left:15.1pt;padding-right:15.1pt;">     0.6345</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.9" style="padding-left:15.1pt;padding-right:15.1pt;">     0.7514      <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.10" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.17.10.1">0.8720</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.17.11" style="padding-left:15.1pt;padding-right:15.1pt;">     <span class="ltx_text ltx_font_bold" id="S5.T1.1.1.17.11.1">0.8538</span></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text" id="S5.I1.i1.p1.1.1" style="font-size:80%;">Note: The highest values of GA and PA for each system are highlighted in <span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1.1">bold</span>. The accuracy of AEL on the Spark dataset is excluded because it cannot complete parsing the whole dataset after running for 10 days.</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p2">
<p class="ltx_p" id="S5.SSx1.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SSx1.p2.2.1">Approach.</span>
We compare OpenLogParser with other state-of-the-art log parsers, including AEL, Drain, LILAC, and LLMParser<sub class="ltx_sub" id="S5.SSx1.p2.2.2"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p2.2.2.1">T5Base</span></sub>.
AEL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib18" title="">18</a>]</cite> and Drain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib14" title="">14</a>]</cite> are two leading traditional syntax-based approaches that are efficient and perform better than most other syntax-based parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>]</cite>.
LILAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>]</cite> and LLMParser<sub class="ltx_sub" id="S5.SSx1.p2.2.3"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p2.2.3.1">T5Base</span></sub> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>]</cite> are recently proposed LLM-based parsers with high parsing accuracy.
Since LILAC uses ChatGPT as the underlying LLM, for a fair comparison, we replace ChatGPT with the same open-source LLM (Llama3-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite>) that OpenLogParser uses. We use T5-base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib10" title="">10</a>]</cite> (240M parameters) as the LLM for LLMParser by following the prior work. Note that both LILAC and LLMParser require manually derived log templates as a few shot demonstrations. We follow the steps described in the papers to obtain these demonstrations. We evaluate the parsers using the LogHub-2.0 dataset and report both Grouping Accuracy (GA) and Parsing Accuracy (PA).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p3">
<p class="ltx_p" id="S5.SSx1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SSx1.p3.1.1">Results.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5.T1" title="TABLE I ‣ RQ1: What is the effectiveness of OpenLogParser? ‣ V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">I</span></a> shows the GA and PA for each log parser across different systems. OpenLogParser achieved the highest GA and PA values for most systems, indicating superior performance in both grouping and parsing logs. Across all systems, OpenLogParser achieved an average GA of 0.8720 and an average PA of 0.8538, outperforming all other parsers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p4">
<p class="ltx_p" id="S5.SSx1.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx1.p4.1.1">OpenLogParser shows superior GA and PA compared to the semi-supervised LLM-based parser – LILAC.
</span>
Compared to OpenLogParser, LILAC demonstrated lower performance with a GA of 0.8289 and PA of 0.6783. LILAC uses manually labeled logs as demonstrations for in-context learning to enhance parsing accuracy. However, when utilizing less powerful open-source LLMs with smaller parameter sizes (i.e., as opposed to ChatGPT), LILAC’s performance declines significantly due to the limited ability of these models to capture complex log patterns with only a few demonstrations.
Consequently, this can lead to inaccurate parsing of variables within the logs (a PA of 0.6783, while OpenLogParser’s PA is 0.8538).
Unlike LILAC and LLMParser<sub class="ltx_sub" id="S5.SSx1.p4.1.2"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p4.1.2.1">T5Base</span></sub>, OpenLogParser is an unsupervised log parser, eliminating the need for labeled logs to enhance the LLM’s log parsing capabilities. The performance of OpenLogParser is not dependent on the number of labeled logs, thus avoiding the limitations faced by semi-supervised approaches that require labeled logs for fine-tuning or in-context learning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p5">
<p class="ltx_p" id="S5.SSx1.p5.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx1.p5.1.1">Among all three LLM-based log parsers, LLMParser<sub class="ltx_sub" id="S5.SSx1.p5.1.1.1"><span class="ltx_text ltx_font_medium" id="S5.SSx1.p5.1.1.1.1">T5Base</span></sub> shows the lowest GA, and the reason may be the limited number of fine-tuning samples that makes it hard to generalize to large-scale datasets.</span>
Among the three LLM-based log parsers (LLMParser<sub class="ltx_sub" id="S5.SSx1.p5.5.2"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p5.5.2.1">T5Base</span></sub>, LILAC, and OpenLogParser), LLMParser<sub class="ltx_sub" id="S5.SSx1.p5.5.3"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p5.5.3.1">T5Base</span></sub> exhibited the lowest GA of 0.6345 and the second-highest PA of 0.7514. When parsing large-scale datasets, logs may exhibit many different variations, even if they share the same log template. Given that LLMParser<sub class="ltx_sub" id="S5.SSx1.p5.5.4"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p5.5.4.1">T5Base</span></sub> is fine-tuned using a small, labeled sample set from the target system, the limited number of log samples likely contributes to its inability to robustly identify logs with the same template across all instances and, thus, lower GA. This limitation becomes particularly evident in systems with more logs, such as BGL and Spark, where LLMParser<sub class="ltx_sub" id="S5.SSx1.p5.5.5"><span class="ltx_text ltx_font_italic" id="S5.SSx1.p5.5.5.1">T5Base</span></sub> struggles to achieve high GA (0.1439 and 0.2589, respectively). Nevertheless, it is still able to identify all dynamic variables in a log with the second-highest PA among all five parsers, which shows the potentials of LLM-based parsers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p6">
<p class="ltx_p" id="S5.SSx1.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx1.p6.1.1">Syntax-based log parsers generally have significantly lower PAs compared to LLM-based parsers, showing challenges in accurately identifying variables.</span>
While AEL and Drain, as syntax-based parsers, show results similar to each other, they both exhibit lower GA compared to OpenLogParser (1.84% and 3.3% lower, respectively) and significantly lower PA (48.25% and 45.17% lower, respectively). This performance disparity is likely linked to their heuristic-based nature, which relies on predefined rules to identify log features. While these rules can effectively classify logs with similar features, achieving reasonable GAs, their generic nature often fails to accurately recognize variables within different log templates, leading to poor PAs. In contrast, OpenLogParser leverages pre-grouping and uses memory mechanisms to achieve high GA, and its LLM-based parsing process accurately identifies variables within grouped logs, resulting in superior PA.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p7">
<svg class="ltx_picture" height="74" id="S5.SSx1.p7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,74) matrix(1 0 0 -1 0 0)"><g fill="#333333" fill-opacity="1.0"><path d="M 0 5.91 L 0 68.1 C 0 71.36 2.64 74 5.91 74 L 594.09 74 C 597.36 74 600 71.36 600 68.1 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9F9F9" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 68.1 C 1.97 70.27 3.73 72.03 5.91 72.03 L 594.09 72.03 C 596.27 72.03 598.03 70.27 598.03 68.1 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 7.29)"><foreignobject color="#000000" height="59.42" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="585.42">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SSx1.p7.pic1.1.1.1.1.1" style="width:423.1pt;">
<span class="ltx_p" id="S5.SSx1.p7.pic1.1.1.1.1.1.1">OpenLogParser achieves superior GA and PA compared to state-of-the-art parsers. Despite not relying on labeled logs, OpenLogParser outperforms other LLM-based parsers that are semi-supervised. Additionally, OpenLogParser significantly enhances PA compared to syntax-based approaches.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx2">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">RQ2: What is the efficiency of OpenLogParser?</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p1">
<p class="ltx_p" id="S5.SSx2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SSx2.p1.1.1">Motivation.</span> Efficiency is crucial in log parsing since it
directly impacts the practical usability of the parser in real-world applications. In this RQ, we study the parsers’ efficiency.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Number of logs and parsing time, in seconds, for the state-of-the-art (first four columns) and OpenLogParser.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:545.7pt;height:243pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-90.9pt,40.5pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.1.1">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.2" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.3" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.4" style="padding-left:6.0pt;padding-right:6.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1">AEL</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.5" style="padding-left:6.0pt;padding-right:6.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.5.1">Drain</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.6" style="padding-left:6.0pt;padding-right:6.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.6.1">LILAC</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T2.1.1.1.1" style="padding-left:6.0pt;padding-right:6.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">LLMParser<sub class="ltx_sub" id="S5.T2.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.1.1.1.1.1.1.1">T5Base</span></sub></span>   <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T2.1.1.1.7" style="padding-left:6.0pt;padding-right:6.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.7.1">OpenLogParser</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.2">
<td class="ltx_td ltx_border_r" id="S5.T2.1.1.2.1" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.2.2" style="padding-left:6.0pt;padding-right:6.0pt;">Log count</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.2.3" style="padding-left:6.0pt;padding-right:6.0pt;">Total time</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.2.4" style="padding-left:6.0pt;padding-right:6.0pt;">Total time</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.2.5" style="padding-left:6.0pt;padding-right:6.0pt;">Total time</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.2.6" style="padding-left:6.0pt;padding-right:6.0pt;">Total time  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.2.7" style="padding-left:6.0pt;padding-right:6.0pt;">Total time</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.2.8" style="padding-left:6.0pt;padding-right:6.0pt;">LLM query time</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.2.9" style="padding-left:6.0pt;padding-right:6.0pt;">Grouping time</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.2.10" style="padding-left:6.0pt;padding-right:6.0pt;">Memory search time</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.1.3.1" style="padding-left:6.0pt;padding-right:6.0pt;">HDFS</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.3.2" style="padding-left:6.0pt;padding-right:6.0pt;">11,167,740</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.3.3" style="padding-left:6.0pt;padding-right:6.0pt;">5,711.52</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.3.4" style="padding-left:6.0pt;padding-right:6.0pt;">1,343.56</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.3.5" style="padding-left:6.0pt;padding-right:6.0pt;">1,162.20</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.6" style="padding-left:6.0pt;padding-right:6.0pt;">148,097.72  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.7" style="padding-left:6.0pt;padding-right:6.0pt;">1,252.62</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.8" style="padding-left:6.0pt;padding-right:6.0pt;">273.67</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.9" style="padding-left:6.0pt;padding-right:6.0pt;">867.36</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.10" style="padding-left:6.0pt;padding-right:6.0pt;">111.59</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.4.1" style="padding-left:6.0pt;padding-right:6.0pt;">Hadoop</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.4.2" style="padding-left:6.0pt;padding-right:6.0pt;">179,993</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.4.3" style="padding-left:6.0pt;padding-right:6.0pt;">361.54</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.4.4" style="padding-left:6.0pt;padding-right:6.0pt;">19.54</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.4.5" style="padding-left:6.0pt;padding-right:6.0pt;">4,747.52</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.6" style="padding-left:6.0pt;padding-right:6.0pt;">4,034.32  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.7" style="padding-left:6.0pt;padding-right:6.0pt;">285.76</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.8" style="padding-left:6.0pt;padding-right:6.0pt;">268.81</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.9" style="padding-left:6.0pt;padding-right:6.0pt;">11.95</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.10" style="padding-left:6.0pt;padding-right:6.0pt;">5.01</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.5.1" style="padding-left:6.0pt;padding-right:6.0pt;">Spark</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.5.2" style="padding-left:6.0pt;padding-right:6.0pt;">16,075,117</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.5.3" style="padding-left:6.0pt;padding-right:6.0pt;">10 days+</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.5.4" style="padding-left:6.0pt;padding-right:6.0pt;">1,539.88</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.5.5" style="padding-left:6.0pt;padding-right:6.0pt;">3,346.08</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.5.6" style="padding-left:6.0pt;padding-right:6.0pt;">225,046.88  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.5.7" style="padding-left:6.0pt;padding-right:6.0pt;">1,752.40</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.5.8" style="padding-left:6.0pt;padding-right:6.0pt;">631.66</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.5.9" style="padding-left:6.0pt;padding-right:6.0pt;">764.12</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.5.10" style="padding-left:6.0pt;padding-right:6.0pt;">356.62</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.6.1" style="padding-left:6.0pt;padding-right:6.0pt;">Zookeeper</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.6.2" style="padding-left:6.0pt;padding-right:6.0pt;">74,273</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.6.3" style="padding-left:6.0pt;padding-right:6.0pt;">3.22</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.6.4" style="padding-left:6.0pt;padding-right:6.0pt;">7.12</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.6.5" style="padding-left:6.0pt;padding-right:6.0pt;">1,702.46</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.6" style="padding-left:6.0pt;padding-right:6.0pt;">1,585.52  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.7" style="padding-left:6.0pt;padding-right:6.0pt;">52.23</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.8" style="padding-left:6.0pt;padding-right:6.0pt;">47.06</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.9" style="padding-left:6.0pt;padding-right:6.0pt;">4.75</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.10" style="padding-left:6.0pt;padding-right:6.0pt;">0.42</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.7.1" style="padding-left:6.0pt;padding-right:6.0pt;">BGL</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.7.2" style="padding-left:6.0pt;padding-right:6.0pt;">4,631,261</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.7.3" style="padding-left:6.0pt;padding-right:6.0pt;">29,917.35</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.7.4" style="padding-left:6.0pt;padding-right:6.0pt;">501.09</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.7.5" style="padding-left:6.0pt;padding-right:6.0pt;">8,624.70</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.7.6" style="padding-left:6.0pt;padding-right:6.0pt;">90,526.27  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.7.7" style="padding-left:6.0pt;padding-right:6.0pt;">1,244.64</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.7.8" style="padding-left:6.0pt;padding-right:6.0pt;">857.82</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.7.9" style="padding-left:6.0pt;padding-right:6.0pt;">298.23</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.7.10" style="padding-left:6.0pt;padding-right:6.0pt;">88.59</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.8.1" style="padding-left:6.0pt;padding-right:6.0pt;">HPC</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.8.2" style="padding-left:6.0pt;padding-right:6.0pt;">429,987</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.8.3" style="padding-left:6.0pt;padding-right:6.0pt;">18.00</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.8.4" style="padding-left:6.0pt;padding-right:6.0pt;">39.02</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.8.5" style="padding-left:6.0pt;padding-right:6.0pt;">388.88</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.6" style="padding-left:6.0pt;padding-right:6.0pt;">4,634.87  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.7" style="padding-left:6.0pt;padding-right:6.0pt;">539.76</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.8" style="padding-left:6.0pt;padding-right:6.0pt;">510.97</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.9" style="padding-left:6.0pt;padding-right:6.0pt;">27.45</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.10" style="padding-left:6.0pt;padding-right:6.0pt;">1.33</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.9.1" style="padding-left:6.0pt;padding-right:6.0pt;">Thunderbird</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.9.2" style="padding-left:6.0pt;padding-right:6.0pt;">16,601,745</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.9.3" style="padding-left:6.0pt;padding-right:6.0pt;">25,199.44</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.9.4" style="padding-left:6.0pt;padding-right:6.0pt;">2,132.20</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.9.5" style="padding-left:6.0pt;padding-right:6.0pt;">16,316.03</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.9.6" style="padding-left:6.0pt;padding-right:6.0pt;">421,864.78  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.9.7" style="padding-left:6.0pt;padding-right:6.0pt;">8,659.29</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.9.8" style="padding-left:6.0pt;padding-right:6.0pt;">5,343.56</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.9.9" style="padding-left:6.0pt;padding-right:6.0pt;">1,466.77</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.9.10" style="padding-left:6.0pt;padding-right:6.0pt;">1,848.96</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.10.1" style="padding-left:6.0pt;padding-right:6.0pt;">Linux</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.10.2" style="padding-left:6.0pt;padding-right:6.0pt;">23,921</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.10.3" style="padding-left:6.0pt;padding-right:6.0pt;">4.53</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.10.4" style="padding-left:6.0pt;padding-right:6.0pt;">2.61</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.10.5" style="padding-left:6.0pt;padding-right:6.0pt;">2,374.83</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.6" style="padding-left:6.0pt;padding-right:6.0pt;">1,031.82  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.7" style="padding-left:6.0pt;padding-right:6.0pt;">216.03</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.8" style="padding-left:6.0pt;padding-right:6.0pt;">213.42</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.9" style="padding-left:6.0pt;padding-right:6.0pt;">1.78</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.10" style="padding-left:6.0pt;padding-right:6.0pt;">0.82</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.11.1" style="padding-left:6.0pt;padding-right:6.0pt;">HealthApp</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.11.2" style="padding-left:6.0pt;padding-right:6.0pt;">212,394</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.11.3" style="padding-left:6.0pt;padding-right:6.0pt;">976.74</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.11.4" style="padding-left:6.0pt;padding-right:6.0pt;">17.88</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.11.5" style="padding-left:6.0pt;padding-right:6.0pt;">1,182.27</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.11.6" style="padding-left:6.0pt;padding-right:6.0pt;">3,139.20  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.11.7" style="padding-left:6.0pt;padding-right:6.0pt;">103.33</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.11.8" style="padding-left:6.0pt;padding-right:6.0pt;">85.25</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.11.9" style="padding-left:6.0pt;padding-right:6.0pt;">10.38</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.11.10" style="padding-left:6.0pt;padding-right:6.0pt;">7.70</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.12.1" style="padding-left:6.0pt;padding-right:6.0pt;">Apache</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.12.2" style="padding-left:6.0pt;padding-right:6.0pt;">51,977</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.12.3" style="padding-left:6.0pt;padding-right:6.0pt;">3.20</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.12.4" style="padding-left:6.0pt;padding-right:6.0pt;">5.42</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.12.5" style="padding-left:6.0pt;padding-right:6.0pt;">122.79</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.6" style="padding-left:6.0pt;padding-right:6.0pt;">1,056.53  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.7" style="padding-left:6.0pt;padding-right:6.0pt;">18.92</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.8" style="padding-left:6.0pt;padding-right:6.0pt;">15.19</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.9" style="padding-left:6.0pt;padding-right:6.0pt;">3.36</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.10" style="padding-left:6.0pt;padding-right:6.0pt;">0.37</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.13.1" style="padding-left:6.0pt;padding-right:6.0pt;">Proxifier</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.13.2" style="padding-left:6.0pt;padding-right:6.0pt;">21,320</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.13.3" style="padding-left:6.0pt;padding-right:6.0pt;">1.69</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.13.4" style="padding-left:6.0pt;padding-right:6.0pt;">2.96</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.13.5" style="padding-left:6.0pt;padding-right:6.0pt;">681.65</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.13.6" style="padding-left:6.0pt;padding-right:6.0pt;">821.44  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.13.7" style="padding-left:6.0pt;padding-right:6.0pt;">871.52</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.13.8" style="padding-left:6.0pt;padding-right:6.0pt;">868.99</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.13.9" style="padding-left:6.0pt;padding-right:6.0pt;">2.47</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.13.10" style="padding-left:6.0pt;padding-right:6.0pt;">0.07</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.14.1" style="padding-left:6.0pt;padding-right:6.0pt;">OpenSSH</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.14.2" style="padding-left:6.0pt;padding-right:6.0pt;">638,946</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.14.3" style="padding-left:6.0pt;padding-right:6.0pt;">1,338.67</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.14.4" style="padding-left:6.0pt;padding-right:6.0pt;">74.12</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.14.5" style="padding-left:6.0pt;padding-right:6.0pt;">1,134.35</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.6" style="padding-left:6.0pt;padding-right:6.0pt;">15,262.29  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.7" style="padding-left:6.0pt;padding-right:6.0pt;">89.37</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.8" style="padding-left:6.0pt;padding-right:6.0pt;">36.94</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.9" style="padding-left:6.0pt;padding-right:6.0pt;">49.00</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.10" style="padding-left:6.0pt;padding-right:6.0pt;">3.44</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.15.1" style="padding-left:6.0pt;padding-right:6.0pt;">OpenStack</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.15.2" style="padding-left:6.0pt;padding-right:6.0pt;">207,632</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.15.3" style="padding-left:6.0pt;padding-right:6.0pt;">30.28</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.15.4" style="padding-left:6.0pt;padding-right:6.0pt;">60.18</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.15.5" style="padding-left:6.0pt;padding-right:6.0pt;">1,260.64</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.15.6" style="padding-left:6.0pt;padding-right:6.0pt;">7,558.24  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.15.7" style="padding-left:6.0pt;padding-right:6.0pt;">377.64</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.15.8" style="padding-left:6.0pt;padding-right:6.0pt;">330.66</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.15.9" style="padding-left:6.0pt;padding-right:6.0pt;">44.88</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.15.10" style="padding-left:6.0pt;padding-right:6.0pt;">2.09</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.16.1" style="padding-left:6.0pt;padding-right:6.0pt;">Mac</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.16.2" style="padding-left:6.0pt;padding-right:6.0pt;">100,314</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.16.3" style="padding-left:6.0pt;padding-right:6.0pt;">10.79</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.16.4" style="padding-left:6.0pt;padding-right:6.0pt;">16.94</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.1.1.16.5" style="padding-left:6.0pt;padding-right:6.0pt;">15,930.81</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.16.6" style="padding-left:6.0pt;padding-right:6.0pt;">4,873.72  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.16.7" style="padding-left:6.0pt;padding-right:6.0pt;">5,935.77</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.16.8" style="padding-left:6.0pt;padding-right:6.0pt;">5,922.19</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.16.9" style="padding-left:6.0pt;padding-right:6.0pt;">9.26</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.16.10" style="padding-left:6.0pt;padding-right:6.0pt;">4.32</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.17">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.1.17.1" style="padding-left:6.0pt;padding-right:6.0pt;">Average</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.17.2" style="padding-left:6.0pt;padding-right:6.0pt;">3,601,187</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.17.3" style="padding-left:6.0pt;padding-right:6.0pt;">4,890.54</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.17.4" style="padding-left:6.0pt;padding-right:6.0pt;">411.61</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.1.1.17.5" style="padding-left:6.0pt;padding-right:6.0pt;">4,212.52</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.17.6" style="padding-left:6.0pt;padding-right:6.0pt;">66,395.26  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.17.7" style="padding-left:6.0pt;padding-right:6.0pt;">1,528.52</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.17.8" style="padding-left:6.0pt;padding-right:6.0pt;">1,100.44</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.17.9" style="padding-left:6.0pt;padding-right:6.0pt;">254.41</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.17.10" style="padding-left:6.0pt;padding-right:6.0pt;">173.67</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.18">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.1.1.18.1" style="padding-left:6.0pt;padding-right:6.0pt;">Total</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.1.1.18.2" style="padding-left:6.0pt;padding-right:6.0pt;">50,416,620</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.1.1.18.3" style="padding-left:6.0pt;padding-right:6.0pt;">17.66 hours</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.1.1.18.4" style="padding-left:6.0pt;padding-right:6.0pt;">1.60 hours</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.1.1.18.5" style="padding-left:6.0pt;padding-right:6.0pt;">16.38 hours</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T2.1.1.18.6" style="padding-left:6.0pt;padding-right:6.0pt;">258.20 hours  <span class="ltx_rule" style="width:1.5pt;background:black;display:inline-block;"> </span>
</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T2.1.1.18.7" style="padding-left:6.0pt;padding-right:6.0pt;">5.94 hours</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T2.1.1.18.8" style="padding-left:6.0pt;padding-right:6.0pt;">4.28 hours</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T2.1.1.18.9" style="padding-left:6.0pt;padding-right:6.0pt;">0.99 hours</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T2.1.1.18.10" style="padding-left:6.0pt;padding-right:6.0pt;">0.68 hours</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p2">
<p class="ltx_p" id="S5.SSx2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SSx2.p2.1.1">Approach.</span> We measure the total parsing time required by OpenLogParser and its individual components (i.e., LLM queries, grouping, and memory search), and the four baseline parsers to process logs from the LogHub-2.0 dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p3">
<p class="ltx_p" id="S5.SSx2.p3.4"><span class="ltx_text ltx_font_bold" id="S5.SSx2.p3.4.2">Results.</span> <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx2.p3.1.1">OpenLogParser is 2.7 and 40 times faster than Lilac and LLMParser<sub class="ltx_sub" id="S5.SSx2.p3.1.1.1"><span class="ltx_text ltx_font_medium" id="S5.SSx2.p3.1.1.1.1">T5Base</span></sub>, respectively.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5.T2" title="TABLE II ‣ RQ2: What is the efficiency of OpenLogParser? ‣ V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">II</span></a> shows the parsing time for each log parser across different systems. OpenLogParser spends a total of 5.94 hours to parse logs from all 14 systems (50 million logs), which is significantly faster than other LLM-based parsers: LILAC (16 hours) and LLMParser<sub class="ltx_sub" id="S5.SSx2.p3.4.3"><span class="ltx_text ltx_font_italic" id="S5.SSx2.p3.4.3.1">T5Base</span></sub> (258 hours). The parsing time for OpenLogParser is mainly occupied by the LLM query time, which accounts for 72.05% of the total processing time, followed by the grouping time, which constitutes 16.67% of the overall duration.
LLMParser<sub class="ltx_sub" id="S5.SSx2.p3.4.4"><span class="ltx_text ltx_font_italic" id="S5.SSx2.p3.4.4.1">T5Base</span></sub> is the slowest among all LLM-based parsers because it processes each log individually, and the vast quantity of logs linearly increases the number of model queries required. Even with a relatively lightweight model like T5-base, which has only 240 million parameters, querying to parse the logs individually is still slow and impractical for real-world applications.
LILAC, with its cache design, eliminates the need to parse each log individually through an LLM, significantly speeding up the process compared to LLMParser<sub class="ltx_sub" id="S5.SSx2.p3.4.5"><span class="ltx_text ltx_font_italic" id="S5.SSx2.p3.4.5.1">T5Base</span></sub>. However, LILAC still requires frequent model queries to update the templates in the cache, which limits its efficiency. In contrast, OpenLogParser optimizes parsing times through its grouping and memory features, resulting in superior efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p4">
<p class="ltx_p" id="S5.SSx2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx2.p4.1.1">AEL exhibits significant efficiency issues when parsing logs beyond certain sizes, while Drain maintains high efficiency across all datasets.</span>
AEL can parse datasets with fewer than 100K logs within seconds but requires several hours or even days for datasets with over one million logs (e.g., we stopped AEL after running for 10 days when parsing the 16 million logs from Spark).
This inefficiency is due to AEL’s reliance on extensive comparisons between logs and identified templates, where the parsing time grows exponentially with respect to the number of logs and log templates.
In contrast, Drain, which uses a fixed-depth parsing tree, is the most efficient parser. OpenLogParser uses a grouping method similar to Drain’s, with a total grouping time amounting to 0.99 hours, which is less than Drain’s total parsing time of 1.6 hours. This highlights the efficiency of OpenLogParser’s grouping process. While there is a slight slowdown due to the additional processing involved (5.94 hours compared to Drain’s 1.6 hours), OpenLogParser shows superior parsing effectiveness compared to Drain and is the second fastest log parser among the evaluates parsers. </p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p5">
<svg class="ltx_picture" height="60.09" id="S5.SSx2.p5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,60.09) matrix(1 0 0 -1 0 0)"><g fill="#333333" fill-opacity="1.0"><path d="M 0 5.91 L 0 54.18 C 0 57.44 2.64 60.09 5.91 60.09 L 594.09 60.09 C 597.36 60.09 600 57.44 600 54.18 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9F9F9" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 54.18 C 1.97 56.36 3.73 58.12 5.91 58.12 L 594.09 58.12 C 596.27 58.12 598.03 56.36 598.03 54.18 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 7.29)"><foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="585.42">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SSx2.p5.pic1.1.1.1.1.1" style="width:423.1pt;">
<span class="ltx_p" id="S5.SSx2.p5.pic1.1.1.1.1.1.1">OpenLogParser enhances its efficiency by utilizing grouping and memory components, which reduces the number of LLM queries. OpenLogParser demonstrates the highest efficiency across LLM-based parsers.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx3">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">RQ3: How does different settings impact the result of OpenLogParser?</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p1">
<p class="ltx_p" id="S5.SSx3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SSx3.p1.1.1">Motivation.</span> OpenLogParser implements multiple components to achieve effective and efficient log parsing. In this RQ, we explore how various settings and configurations affect the performance of OpenLogParser.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p2">
<p class="ltx_p" id="S5.SSx3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SSx3.p2.1.1">Approach.</span> There are three general components in OpenLogParser that can be adjusted or replaced: log selection from each group for prompting, the number of selected logs, and the inclusion or exclusion of self-reflection processes.
To select diverse logs from the log group, we use Jaccard similarity to measure the similarity between every log pair. In this RQ, we also try random sampling and cosine similarity.
Furthermore, we evaluate how changing the number of selected logs from 1 to 10 impacts the effectiveness.
Finally, we compare the effect of removing the self-reflection component on the efficiency and effectiveness of OpenLogParser.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p3">
<p class="ltx_p" id="S5.SSx3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SSx3.p3.1.1">Results.</span>
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx3.p3.1.2">Selecting representative logs based on Jaccard similarity outperforms using cosine similarity and random sampling.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5.T3" title="TABLE III ‣ RQ3: How does different settings impact the result of OpenLogParser? ‣ V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a> shows the total time, GA, and PA of OpenLogParser compared to replacing the log selection process with cosine similarity and random sampling.
When employing cosine similarity to select representative logs, both GA and PA experienced declines of 2.5% and 4.8%, respectively, compared to using Jaccard similarity. This indicates that although cosine similarity is shown to be an effective similarity metric for text data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib39" title="">39</a>]</cite>, it does not necessarily select logs that are representative enough for LLM to generalize log templates. However, we notice a slight reduction in execution time (3.3%) when using cosine similarity.
Similarly, using random sampling further reduces the processing time (by 8.2%), but due to the lack of diversity in the sampled logs, both GA and PA are even lower, at 0.849 and 0.806, respectively.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>OpenLogParser performance under different settings. The numbers in the parenthesis indicate the percentage difference compared to the full version of OpenLogParser. </figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.1" style="width:258.7pt;height:71.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.4pt,9.4pt) scale(0.79,0.79) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.1">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T3.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Total Time</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">GA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1">PA</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1.1">OpenLogParser</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.2">5.944 hours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.3">0.872</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.4">0.859</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.1.3.1">w/ cosine similarity</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.2">5.745 (↓3.3%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.3">0.85 (↓2.5%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.4">0.818 (↓4.8%)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.4">
<td class="ltx_td ltx_align_left" id="S5.T3.1.1.4.1">w/ random sampling</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2">5.458 (↓8.2%)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3">0.849 (↓2.6%)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.4">0.806 (↓6.2%)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T3.1.1.5.1">w/o self-reflection</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.1.1.5.2">3.292 (↓44.6%)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.1.1.5.3">0.81 (↓7.1%)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.1.1.5.4">0.777 (↓9.5%)</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p4">
<p class="ltx_p" id="S5.SSx3.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx3.p4.1.1">Although the self-reflection mechanism requires additional processing time, it significantly enhances the parsing results of OpenLogParser.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5.T3" title="TABLE III ‣ RQ3: How does different settings impact the result of OpenLogParser? ‣ V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a> compares full version OpenLogParser and OpenLogParser without self-reflection in the total execution time, GA, and PA.
Excluding the self-reflection component from OpenLogParser results in a 44.6% reduction in parsing time (from around six to three hours). However, removing self-reflection greatly decreases both GA and PA by 7.1% and 9.5%, respectively. This shows that self-reflection significantly enhances the parsing effectiveness of OpenLogParser, although at the expense of increased overhead due to additional LLM queries. Therefore, in practical applications, the inclusion of the self-reflection component in OpenLogParser can be determined based on the specific needs of effectiveness or efficiency.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p5">
<p class="ltx_p" id="S5.SSx3.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx3.p5.1.1">During retrieval augmented log parsing, varying the number of selected logs affects the performance of OpenLogParser. Retrieving three logs into the prompt yields the highest effectiveness.</span>
Fig <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5.F4" title="Figure 4 ‣ RQ3: How does different settings impact the result of OpenLogParser? ‣ V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> shows the OpenLogParser performance with variations in the number of logs from a group retrieved into the prompts.
OpenLogParser maintains high accuracy across various sample sizes, with optimal performance achieved when the sample size is set to three, reaching peak values in both GA and PA. Notably, when the sample size is reduced to one, GA and PA drop to 0.80 and 0.70, respectively, representing a decline of 8.26% and 18% compared to a sample size of three. This reduction highlights the challenges LLM faces in parsing logs accurately without sufficient comparative data, such as multiple log comparisons or labeled logs. As the sample size increases from one to two, both GA and PA show significant improvements, peaking when the sample size reaches three. However, further increases in sample size from three to eight result in slight decreases in GA and PA, stabilizing around 0.865 and 0.835, respectively. This suggests that an excess of log samples may introduce noise, subsequently lowering performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib47" title="">47</a>]</cite>. Importantly, when the sample size reaches 10, both GA and PA decrease compared to a sample size of eight. This decrease is attributed to prompt truncation caused by an overload of retrieved logs, which exceeds the context size of the LLM, resulting in incomplete input data.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="470" id="S5.F4.g1" src="x4.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>GA and PA of OpenLogParser using different numbers of selected logs in the prompt.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p6">
<svg class="ltx_picture" height="76.69" id="S5.SSx3.p6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,76.69) matrix(1 0 0 -1 0 0)"><g fill="#333333" fill-opacity="1.0"><path d="M 0 5.91 L 0 70.79 C 0 74.05 2.64 76.69 5.91 76.69 L 594.09 76.69 C 597.36 76.69 600 74.05 600 70.79 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9F9F9" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 70.79 C 1.97 72.96 3.73 74.72 5.91 74.72 L 594.09 74.72 C 596.27 74.72 598.03 72.96 598.03 70.79 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 7.29)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="585.42">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SSx3.p6.pic1.1.1.1.1.1" style="width:423.1pt;">
<span class="ltx_p" id="S5.SSx3.p6.pic1.1.1.1.1.1.1">Using Jaccard similarity for log selection and LLM self-reflection enhances the parsing result, although they come with added overhead. Retrieving more logs within the prompt does not necessarily increase effectiveness; in fact, the optimal number of logs enables OpenLogParser to reach peak accuracy is three.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx4">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">RQ4: What is the effectiveness of OpenLogParser with different LLMs?</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p1">
<p class="ltx_p" id="S5.SSx4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SSx4.p1.1.1">Motivation.</span>
Unlike previous parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib44" title="">44</a>]</cite> that are based on commercial LLMs, OpenLogParser employs open-source LLM to mitigate privacy concerns and monetary costs. Different LLMs exhibit varying capabilities due to their distinct architectures and pre-training data. In this RQ, we evaluate the performance of OpenLogParser across various open-source LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p2">
<p class="ltx_p" id="S5.SSx4.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SSx4.p2.1.1">Approach.</span>
We selected three other open-source models (in addition to Llama3-8B) with similar parameter sizes to compare the log parsing performance with different LLMs, including Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib17" title="">17</a>]</cite>, CodeGemma-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib42" title="">42</a>]</cite>, and ChatGLM3-6B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib13" title="">13</a>]</cite>. These models are commonly used in research and practice. Mistral-7B shows strong text generation capabilities in small model sizes. CodeGemma-7B is pre-trained on code repositories and tailored for code-related tasks. ChatGLM3-6B is known for its bilingual conversational abilities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p3">
<p class="ltx_p" id="S5.SSx4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SSx4.p3.1.1">Results.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#S5.T4" title="TABLE IV ‣ RQ4: What is the effectiveness of OpenLogParser with different LLMs? ‣ V Evaluation ‣ OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the parsing performance using various LLMs. Among all, Llama3-8B achieved the best overall results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p4">
<p class="ltx_p" id="S5.SSx4.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx4.p4.1.1">Compared to Llama3-8B, Mistral-7B requires a slightly longer parsing time, yet achieves a similar GA and a noticeable decline in PA. </span>
Mistral-7B is a general model with training objectives and parameter sizes similar to Llama3-8B. However, it exhibits a lower PA, decreased by 14.6%, with comparable results in parsing time and GA. This discrepancy in PA may be attributed to Llama3-8B’s enhanced pre-training data, which includes more code <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite>, and its larger parameter size. These factors likely contribute to Llama3-8B’s superior ability to abstract variables within logs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p5">
<p class="ltx_p" id="S5.SSx4.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx4.p5.1.1">CodeGemma-7B has a better parsing speed, but both GA and PA face a decline compared to Llama3-8B.</span>
CodeGemma-7B completes the parsing of all logs in only 71.5% of the time required by Llama3-8B. However, CodeGemma-7B does not achieve comparably high accuracy, indicating that while it is capable of generating log templates that match the logs, it struggles to consistently and accurately abstract variables within these logs. Nevertheless, using CodeGemma-7B still achieves higher GA and PA than other LLM-based parsers: LILAC and LLMParser<sub class="ltx_sub" id="S5.SSx4.p5.1.2"><span class="ltx_text ltx_font_italic" id="S5.SSx4.p5.1.2.1">T5Base</span></sub>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p6">
<p class="ltx_p" id="S5.SSx4.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SSx4.p6.1.1">As a conversational model, ChatGLM3-6B shows the worst result in parsing effectiveness and efficiency.</span>
ChatGLM3-6B, pre-trained on a bilingual corpus in Chinese and English and optimized for conversations, does not include code in its pre-training data, which may have caused its bad parsing ability. This prevents ChatGLM3-6B from generating accurate log templates that can match the logs, necessitating increased model queries for self-reflection. Consequently, the parsing time for ChatGLM3-6B significantly increases by 145.1% compared to LLaMA3. Despite undergoing extensive self-reflection, ChatGLM3-6B still fails to generate correct log templates. This leads to inferior results in both effectiveness and efficiency compared to other models, illustrating a clear disparity in performance when the pre-training background of the model does not match the specific task requirements.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Parsing performance of OpenLogParser using different LLMs.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:265.8pt;height:69.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.7pt,10.4pt) scale(0.77,0.77) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.1.1">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T4.1.1.1.1" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">Total Time</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">GA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1">PA</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.2.1" style="padding-left:8.5pt;padding-right:8.5pt;">Llama3-8B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">5.94 hours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.872</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.854</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3">
<td class="ltx_td ltx_align_left" id="S5.T4.1.1.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2" style="padding-left:8.5pt;padding-right:8.5pt;">6.78 (↑14.1%)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.876 (↑0.5%)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.729 (↓14.6%)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4">
<td class="ltx_td ltx_align_left" id="S5.T4.1.1.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">CodeGemma-7B</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.2" style="padding-left:8.5pt;padding-right:8.5pt;">4.25 (↓28.5%)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.814 (↓6.7%)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.752 (↓11.9%)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.1.1.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">ChatGLM3-6B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.5.2" style="padding-left:8.5pt;padding-right:8.5pt;">14.56 (↑145.1%)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.5.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.837 (↓4%)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.5.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.600 (↓29.7%)</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p7">
<svg class="ltx_picture" height="43.48" id="S5.SSx4.p7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,43.48) matrix(1 0 0 -1 0 0)"><g fill="#333333" fill-opacity="1.0"><path d="M 0 5.91 L 0 37.58 C 0 40.84 2.64 43.48 5.91 43.48 L 594.09 43.48 C 597.36 43.48 600 40.84 600 37.58 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9F9F9" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 37.58 C 1.97 39.75 3.73 41.51 5.91 41.51 L 594.09 41.51 C 596.27 41.51 598.03 39.75 598.03 37.58 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 7.29)"><foreignobject color="#000000" height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="585.42">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SSx4.p7.pic1.1.1.1.1.1" style="width:423.1pt;">
<span class="ltx_p" id="S5.SSx4.p7.pic1.1.1.1.1.1.1">Replacing the LLM leads to variations in effectiveness and performance. Among the four open-source models of similar sizes, Llama3-8B shows the best overall results.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Threats to Validity</span>
</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">External validity.</span>
Data leakage is a potential risk of LLM-based log parsers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>]</cite>. Although OpenLogParser does not involve using labeled logs for fine-tuning or in-context learning, there is a possibility that the LLM might have been pre-trained on publicly available log data.
Our evaluation dataset with ground-truth templates was released on August 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib19" title="">19</a>]</cite> and Llama3-8B training knowledge cutoff from March 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite>, so the leakage risk should be minimal.
The log format may also affect our result, but the datasets used are large and cover logs from various systems in different formats. Future studies are needed to evaluate OpenLogParser on logs from other systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Internal validity.</span>
OpenLogParser employs Llama3-8B as its base model due to its promising results in many tasks and the relatively small size <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib4" title="">4</a>]</cite>. We also compared the results across various open-source LLMs and found differences. Future research is needed to evaluate LLM-based parsers’ performance when more advanced LLMs are released in the future. The effectiveness of OpenLogParser could be influenced by specific parameter settings (e.g., the number of logs selected for prompting). Our evaluations showed that these settings have an impact on the parsing results and discussed the optimal settings. Future studies are needed to evaluate the settings on other datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Construct validity.</span>
To mitigate the effects of randomness in evaluating OpenLogParser, the generation temperature of the model is set to zero. This adjustment ensures that experiments conducted under the same conditions are repeatable and that the results are stable.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduced OpenLogParser, an unsupervised log parsing technique utilizing open-source LLMs to effectively address the limitations of existing LLM-based and syntax-based parsers.
OpenLogParser first groups logs that share a syntactic similarity in the static text but vary in the dynamic variable, using a fixed-depth grouping tree.
It then parses logs in these groups with three components: i) retrieval augmented generation using similarity scoring: identifies diverse logs within each group based on Jaccard similarity, aiding the LLM in differentiating static text from dynamic variables; ii) self-reflection: iteratively queries LLMs to refine log templates and enhance parsing accuracy; and iii) log template memory: store parsed templates to minimize LLM queries, thereby boosting parsing efficiency.
Our comprehensive evaluations on LogHub-2.0, a public large-scale log dataset, demonstrate that OpenLogParser achieves an average GA of 0.8720 and an average PA of 0.8538, outperforming state-of-the-art parsers (i.e., ILIAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib20" title="">20</a>]</cite> and LLMParser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.01585v1#bib.bib31" title="">31</a>]</cite>) by 5% and 25%, respectively. OpenLogParser parses logs from all 14 systems (50 million logs) in a total of 5.94 hours, which is 2.75 and 40 times faster than other LLM-based parsers
This marks a substantial advancement over traditional semantic-based and LLM-based parsers in an unsupervised way, confirming the robustness and effectiveness of our approach. Additionally, OpenLogParser addresses the privacy and cost concerns associated with commercial LLMs, making it a highly efficient and secure solution for practical log parsing needs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sam [2023]</span>
<span class="ltx_bibblock">
Samsung bans chatgpt among employees after sensitive code leak.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/" title="">https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/</a>, 2023.

</span>
<span class="ltx_bibblock">(Accessed on 07/18/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sec [2024]</span>
<span class="ltx_bibblock">
Security and privacy: Closed source vs open source battle.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://medium.com/blue-orange-digital/security-and-privacy-closed-source-vs-open-source-battle-a8757487040e" title="">https://medium.com/blue-orange-digital/security-and-privacy-closed-source-vs-open-source-battle-a8757487040e</a>, 05 2024.

</span>
<span class="ltx_bibblock">(Accessed on 05/17/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wal [2024]</span>
<span class="ltx_bibblock">
Wall street banks are cracking down on ai-powered chatgpt - bloomberg.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bloomberg.com/news/articles/2023-02-24/citigroup-goldman-sachs-join-chatgpt-crackdown-fn-reports" title="">https://www.bloomberg.com/news/articles/2023-02-24/citigroup-goldman-sachs-join-chatgpt-crackdown-fn-reports</a>, 2024.

</span>
<span class="ltx_bibblock">(Accessed on 07/18/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">lla [2024]</span>
<span class="ltx_bibblock">
Introducing meta llama 3: The most capable openly available llm to date.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3/" title="">https://ai.meta.com/blog/meta-llama-3/</a>, 2024.

</span>
<span class="ltx_bibblock">(Accessed on 07/22/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abedu et al. [2024]</span>
<span class="ltx_bibblock">
Samuel Abedu, Ahmad Abdellatif, and Emad Shihab.

</span>
<span class="ltx_bibblock">Llm-based chatbots for mining software repositories: Challenges and opportunities.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aicardi et al. [2020]</span>
<span class="ltx_bibblock">
C Aicardi, L Bitsch, and S Datta Burton.

</span>
<span class="ltx_bibblock">Trust and transparency in artificial intelligence. ethics &amp; society opinion. european commission.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2019]</span>
<span class="ltx_bibblock">
Jinfu Chen, Weiyi Shang, Ahmed E Hassan, Yong Wang, and Jiangbin Lin.

</span>
<span class="ltx_bibblock">An experience report of generating load tests using log-recovered workloads at varying granularities of user behaviour.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)</em>, pages 669–681. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024]</span>
<span class="ltx_bibblock">
Xiaolei Chen, Jie Shi, Jia Chen, Peng Wang, and Wei Wang.

</span>
<span class="ltx_bibblock">High-precision online log parsing with large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings</em>, pages 354–355, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. [2022]</span>
<span class="ltx_bibblock">
Hyung Won Chung et al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2020]</span>
<span class="ltx_bibblock">
Hetong Dai, Heng Li, Che-Shao Chen, Weiyi Shang, and Tse-Hsun Chen.

</span>
<span class="ltx_bibblock">Logram: Efficient log parsing using <math alttext="n" class="ltx_Math" display="inline" id="bib.bib11.1.m1.1"><semantics id="bib.bib11.1.m1.1a"><mi id="bib.bib11.1.m1.1.1" xref="bib.bib11.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="bib.bib11.1.m1.1b"><ci id="bib.bib11.1.m1.1.1.cmml" xref="bib.bib11.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib11.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="bib.bib11.1.m1.1d">italic_n</annotation></semantics></math> n-gram dictionaries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.1">IEEE Transactions on Software Engineering</em>, 48(3):879–892, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du and Li [2019]</span>
<span class="ltx_bibblock">
Min Du and Feifei Li.

</span>
<span class="ltx_bibblock">Spell: Online streaming parsing of large unstructured system logs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 31(11):2213–2227, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TKDE.2018.2875442</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GLM et al. [2024]</span>
<span class="ltx_bibblock">
Team GLM et al.

</span>
<span class="ltx_bibblock">Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2017]</span>
<span class="ltx_bibblock">
Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu.

</span>
<span class="ltx_bibblock">Drain: An online log parsing approach with fixed depth tree.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2017 IEEE International Conference on Web Services (ICWS)</em>, pages 33–40, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICWS.2017.13</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
Shilin He, Jieming Zhu, Pinjia He, and Michael R Lyu.

</span>
<span class="ltx_bibblock">Loghub: a large collection of system log datasets towards automated log analytics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2008.06448</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2024]</span>
<span class="ltx_bibblock">
Yizhan Huang, Yichen Li, Weibin Wu, Jianping Zhang, and Michael R. Lyu.

</span>
<span class="ltx_bibblock">Your code secret belongs to me: Neural code completion tools can memorize hard-coded credentials.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proc. ACM Softw. Eng.</em>, 1(FSE), jul 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3660818</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023a]</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.

</span>
<span class="ltx_bibblock">Mistral 7b, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2008]</span>
<span class="ltx_bibblock">
Zhen Ming Jiang, Ahmed E. Hassan, Parminder Flora, and Gilbert Hamann.

</span>
<span class="ltx_bibblock">Abstracting execution logs to execution events for enterprise applications (short paper).

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">2008 The Eighth International Conference on Quality Software</em>, pages 181–186, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023b]</span>
<span class="ltx_bibblock">
Zhihan Jiang, Jinyang Liu, Junjie Huang, Yichen Li, Yintong Huo, Jiazhen Gu, Zhuangbin Chen, Jieming Zhu, and Michael R Lyu.

</span>
<span class="ltx_bibblock">A large-scale benchmark for log parsing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2308.10828</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2024]</span>
<span class="ltx_bibblock">
Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, and Michael R. Lyu.

</span>
<span class="ltx_bibblock">Lilac: Log parsing using llms with adaptive parsing cache.

</span>
<span class="ltx_bibblock">1(FSE), jul 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. [2022]</span>
<span class="ltx_bibblock">
Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand.

</span>
<span class="ltx_bibblock">Guidelines for assessing the accuracy of log message template identification techniques.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 44th International Conference on Software Engineering</em>, ICSE ’22, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. [2023]</span>
<span class="ltx_bibblock">
Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand.

</span>
<span class="ltx_bibblock">Impact of log parsing on log-based anomaly detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv:2305.15897</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le and Zhang [2023a]</span>
<span class="ltx_bibblock">
Van-Hoang Le and Hongyu Zhang.

</span>
<span class="ltx_bibblock">Log parsing: How far can chatgpt go?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)</em>, pages 1699–1704, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le and Zhang [2023b]</span>
<span class="ltx_bibblock">
Van-Hoang Le and Hongyu Zhang.

</span>
<span class="ltx_bibblock">Log parsing with prompt-based few-shot learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">45th International Conference on Software Engineering: Software Engineering in Practice (ICSE)</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2021]</span>
<span class="ltx_bibblock">
Yukyung Lee, Jina Kim, and Pilsung Kang.

</span>
<span class="ltx_bibblock">Lanobert: System log anomaly detection based on bert masked language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2111.09564</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Zhenhao Li, Chuan Luo, Tse-Hsun (Peter) Chen, Weiyi Shang, Shilin He, Qingwei Lin, and Dongmei Zhang.

</span>
<span class="ltx_bibblock">Did we miss something important? studying and exploring variable-aware log abstraction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 45th International Conference on Software Engineering</em>, ICSE ’23, page 830–842, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2024]</span>
<span class="ltx_bibblock">
Feng Lin, Dong Jae Kim, et al.

</span>
<span class="ltx_bibblock">When llm-based code generation meets the software development process.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2403.15852</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu, Zhuangbin Chen, Cong Feng, Minzhi Yan, and Michael R Lyu.

</span>
<span class="ltx_bibblock">Scalable and adaptive log-based anomaly detection with expert in the loop.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2306.05032</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized BERT pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">CoRR</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022]</span>
<span class="ltx_bibblock">
Yudong Liu, Xu Zhang, Shilin He, Hongyu Zhang, Liqun Li, Yu Kang, Yong Xu, Minghua Ma, Qingwei Lin, Yingnong Dang, et al.

</span>
<span class="ltx_bibblock">Uniparser: A unified log parser for heterogeneous log data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the ACM Web Conference 2022</em>, pages 1893–1901, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2024]</span>
<span class="ltx_bibblock">
Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Chen, and Shaowei Wang.

</span>
<span class="ltx_bibblock">Llmparser: An exploratory study on using large language models for log parsing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering</em>, ICSE ’24, 2024.

</span>
<span class="ltx_bibblock">ISBN 9798400702174.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3597503.3639150</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al. [2024]</span>
<span class="ltx_bibblock">
Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun.

</span>
<span class="ltx_bibblock">How trustworthy are open-source LLMs? an assessment under malicious demonstrations shows their vulnerabilities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, pages 2775–2792, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy et al. [2024]</span>
<span class="ltx_bibblock">
Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, and Saravan Rajmohan.

</span>
<span class="ltx_bibblock">Exploring llm-based agents for root cause analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering</em>, pages 208–219, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roziere et al. [2023]</span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2308.12950</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarda [2023]</span>
<span class="ltx_bibblock">
Komal Sarda.

</span>
<span class="ltx_bibblock">Leveraging large language models for auto-remediation in microservices architecture.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)</em>, pages 16–18. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarda et al. [2023]</span>
<span class="ltx_bibblock">
Komal Sarda, Zakeya Namrud, Raphael Rouf, Harit Ahuja, Mohammadreza Rasolroveicy, Marin Litoiu, Larisa Shwartz, and Ian Watts.

</span>
<span class="ltx_bibblock">Adarma auto-detection and auto-remediation of microservice anomalies by leveraging large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering</em>, pages 200–205, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. [2021]</span>
<span class="ltx_bibblock">
Donghwan Shin, Zanis Ali Khan, Domenico Bianculli, and Lionel Briand.

</span>
<span class="ltx_bibblock">A theoretical framework for understanding the relationship between log parsing and anomaly detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Runtime Verification: 21st International Conference, RV 2021, Virtual Event, October 11–14, 2021, Proceedings</em>, page 277–287, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. [2024]</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. [2001]</span>
<span class="ltx_bibblock">
Amit Singhal et al.

</span>
<span class="ltx_bibblock">Modern information retrieval: A brief overview.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE Data Eng. Bull.</em>, 24(4):35–43, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2024]</span>
<span class="ltx_bibblock">
Jing Su, Chufeng Jiang, Xin Jin, Yuxin Qiao, Tingsong Xiao, Hongda Ma, Rong Wei, Zhi Jing, Jiajun Xu, and Junhong Lin.

</span>
<span class="ltx_bibblock">Large language models for forecasting and anomaly detection: A systematic literature review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2402.10350</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2016]</span>
<span class="ltx_bibblock">
P.N. Tan, M. Steinbach, and V. Kumar.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Introduction to Data Mining</em>.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">ISBN 9789332586055.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. [2024]</span>
<span class="ltx_bibblock">
CodeGemma Team et al.

</span>
<span class="ltx_bibblock">Codegemma: Open code models based on gemma, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021]</span>
<span class="ltx_bibblock">
Zehao Wang, Haoxiang Zhang, Tse-Hsun Chen, and Shaowei Wang.

</span>
<span class="ltx_bibblock">Would you like a quick peek? providing logging support to monitor data processing in big data applications.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>, pages 516–526, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2024]</span>
<span class="ltx_bibblock">
Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu Zhang, and Pinjia He.

</span>
<span class="ltx_bibblock">Divlog: Log parsing with prompt enhanced in-context learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering</em>, ICSE ’24, 2024.

</span>
<span class="ltx_bibblock">ISBN 9798400702174.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2023]</span>
<span class="ltx_bibblock">
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang.

</span>
<span class="ltx_bibblock">A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv</em>, abs/2312.02003, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2010]</span>
<span class="ltx_bibblock">
Ding Yuan, Haohui Mai, Weiwei Xiong, Lin Tan, Yuanyuan Zhou, and Shankar Pasupathy.

</span>
<span class="ltx_bibblock">Sherlog: error diagnosis by connecting clues from run-time logs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 15th International Conference on Architectural support for programming languages and operating systems</em>, pages 143–154, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2024]</span>
<span class="ltx_bibblock">
Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, and Bo Han.

</span>
<span class="ltx_bibblock">Can large language models reason robustly with noisy rationales?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">ICLR 2024 Workshop on Reliable and Responsible Foundation Models</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2019]</span>
<span class="ltx_bibblock">
Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael R Lyu.

</span>
<span class="ltx_bibblock">Tools and benchmarks for automated log parsing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice</em>, pages 121–130, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug  2 21:50:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
