<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1505.00468] VQA: Visual Question Answering www.visualqa.org</title><meta property="og:description" content="We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring reaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VQA: Visual Question Answering www.visualqa.org">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VQA: Visual Question Answering www.visualqa.org">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1505.00468">

<!--Generated on Sat Mar 16 03:39:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VQA: Visual Question Answering
<br class="ltx_break"><a href="www.visualqa.org" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:120%;">www.visualqa.org</a>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aishwarya Agrawal<sup id="id8.5.id1" class="ltx_sup"><span id="id8.5.id1.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>, Jiasen Lu<sup id="id9.6.id2" class="ltx_sup"><span id="id9.6.id2.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>, Stanislaw Antol<sup id="id10.7.id3" class="ltx_sup"><span id="id10.7.id3.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>, 
<br class="ltx_break">Margaret Mitchell, C.Â LawrenceÂ Zitnick, Dhruv Batra, Devi Parikh
</span><span class="ltx_author_notes"><sup id="id11.8.id1" class="ltx_sup">âˆ—</sup>The first three authors contributed equally.
A. Agrawal, J. Lu and S. Antol are with Virginia Tech.
M. Mitchell is with Microsoft Research, Redmond. C.Â L.Â Zitnick is with Facebook AI Research.
D. Batra and D. Parikh are with Georgia Institute of Technology.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.3" class="ltx_p">We propose the task of <em id="id7.3.1" class="ltx_emph ltx_font_italic">free-form</em> and <em id="id7.3.2" class="ltx_emph ltx_font_italic">open-ended</em> Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions.
Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers
that can be provided in a multiple-choice format. We provide a dataset containing <math id="id5.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id5.1.m1.1a"><mo id="id5.1.m1.1.1" xref="id5.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="id5.1.m1.1b"><csymbol cd="latexml" id="id5.1.m1.1.1.cmml" xref="id5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id5.1.m1.1c">\sim</annotation></semantics></math>0.25M images, <math id="id6.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id6.2.m2.1a"><mo id="id6.2.m2.1.1" xref="id6.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="id6.2.m2.1b"><csymbol cd="latexml" id="id6.2.m2.1.1.cmml" xref="id6.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id6.2.m2.1c">\sim</annotation></semantics></math>0.76M questions, and <math id="id7.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id7.3.m3.1a"><mo id="id7.3.m3.1.1" xref="id7.3.m3.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="id7.3.m3.1b"><csymbol cd="latexml" id="id7.3.m3.1.1.cmml" xref="id7.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id7.3.m3.1c">\sim</annotation></semantics></math>10M answers (<a href="www.visualqa.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.visualqa.org</a>), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (<a target="_blank" href="http://cloudcv.org/vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://cloudcv.org/vqa</a>).</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">We are witnessing a renewed excitement in multi-discipline Artificial Intelligence (AI) research problems. In particular, research in image and video captioning that combines Computer Vision (CV),
Natural Language Processing (NLP), and Knowledge Representation &amp; Reasoning (KR) has dramatically increased in the past year <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. Part of this excitement stems from a belief that multi-discipline tasks like image captioning are a step towards solving AI. However, the current state of the art demonstrates that a coarse scene-level understanding of an image paired with word <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p1.1.m1.1a"><mi id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">n</annotation></semantics></math>-gram statistics suffices to generate reasonable image captions, which suggests image captioning may not be as â€œAI-completeâ€ as desired.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">What makes for a compelling â€œAI-completeâ€ task?
We believe that in order to spawn the next generation of AI algorithms,
an ideal task should

<span id="S1.I1" class="ltx_inline-enumerate">
<span id="S1.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span id="S1.I1.i1.1" class="ltx_text">require <em id="S1.I1.i1.1.1" class="ltx_emph ltx_font_italic">multi-modal knowledge</em> beyond a single sub-domain (such as CV) and
</span></span>
<span id="S1.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span id="S1.I1.i2.1" class="ltx_text">have a well-defined <em id="S1.I1.i2.1.1" class="ltx_emph ltx_font_italic">quantitative evaluation metric</em> to track progress.
</span></span>
</span>
For some tasks, such as image captioning, automatic evaluation is still a difficult and
open research problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this paper, we introduce the task of <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">free-form</em> and <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">open-ended</em>
Visual Question Answering (VQA).
A VQA system takes as input an image and a free-form, open-ended, natural-language question
about the image and produces a natural-language answer as the output.
This goal-driven task is applicable to scenarios encountered when visually-impaired
usersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> or intelligence analysts actively elicit visual information.
Example questions are shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1505.00468/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="364" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of free-form, open-ended questions collected for images via Amazon Mechanical Turk.
Note that commonsense knowledge is needed along with a visual understanding of the scene
to answer many questions.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Open-ended questions require a potentially vast
set of AI capabilities to answer â€“
fine-grained recognition (<em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.2" class="ltx_text"></span>, â€œWhat kind of cheese is on the pizza?â€),
object detection (<em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.4" class="ltx_text"></span>, â€œHow many bikes are there?â€),
activity recognition (<em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.6" class="ltx_text"></span>, â€œIs this man crying?â€),
knowledge base reasoning (<em id="S1.p4.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.8" class="ltx_text"></span>, â€œIs this a vegetarian pizza?â€),
and commonsense reasoning (<em id="S1.p4.1.9" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.10" class="ltx_text"></span>, â€œDoes this person have 20/20 vision?â€, â€œIs this person expecting company?â€).
VQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is also amenable to automatic quantitative evaluation, making it possible to effectively track progress on this task.
While the answer to many questions is simply â€œyesâ€ or â€œnoâ€, the process for determining a correct answer is typically far from trivial (e.g.Â in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, â€œDoes this person have 20/20 vision?â€).
Moreover, since questions about images often tend to seek specific information, simple one-to-three word answers are sufficient for many questions. In such scenarios, we can easily evaluate a proposed algorithm by the number of questions it answers correctly.
In this paper, we present both an open-ended answering task and a multiple-choice
task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Unlike the open-ended task that requires a
free-form response, the multiple-choice task only requires an algorithm to pick from a predefined
list of possible answers.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We present a large dataset that contains 204,721 images from the MS COCO
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and a newly created abstract scene dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
that contains 50,000 scenes. The MS COCO dataset has images depicting diverse and complex scenes that are effective at eliciting compelling and diverse questions. We collected a new dataset of â€œrealisticâ€ abstract scenes to enable research focused only on the high-level reasoning required for VQA by removing the need to parse real images. Three questions were collected for each image or scene. Each question was answered by ten subjects along with their confidence. The dataset contains over 760K questions with around 10M answers.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">While the use of open-ended questions offers many benefits, it is still useful to understand the types of questions that are being asked and which types various algorithms may be
good at answering. To this end, we analyze the types of questions asked and the types of answers provided.
Through several visualizations, we demonstrate the astonishing diversity of the questions asked. We also explore how the information content of questions and their answers differs from image captions.
For baselines, we offer several approaches that use a combination of both text and state-of-the-art
visual featuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. As part of the VQA initiative, we will organize an annual challenge and
associated workshop to discuss state-of-the-art methods and best practices.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">VQA poses a rich set of challenges, many of which have been viewed as the holy grail of
automatic image understanding and AI in general. However, it includes as building blocks
several components that the CV, NLP, and KR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> communities have made
significant progress on during the past few decades. VQA provides an attractive balance
between pushing the state of the art, while being accessible enough for the communities
to start making progress on the task.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">VQA Efforts.</span>
Several recent papers have begun to study visual question answeringÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
However, unlike our work, these are fairly restricted (sometimes synthetic) settings with small datasets.
For instance,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> only considers
questions whose answers come from a predefined closed world of
16 basic colors or 894 object categories.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
also considers questions generated from templates from a fixed vocabulary of objects, attributes,
relationships between objects, <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">etc</em>.
In contrast, our proposed task involves <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">open-ended</em>, <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">free-form</em>
questions and answers provided by humans.
Our goal is to increase the diversity of knowledge and
kinds of reasoning needed to provide correct answers.
Critical to
achieving success on this more difficult and unconstrained task, our VQA dataset is <em id="S2.p1.1.5" class="ltx_emph ltx_font_italic">two orders of magnitude</em> larger than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
(<math id="S2.p1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.p1.1.m1.1a"><mo id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><gt id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">&gt;</annotation></semantics></math>250,000 <em id="S2.p1.1.6" class="ltx_emph ltx_font_italic">vs</em>.<span id="S2.p1.1.7" class="ltx_text"></span> 2,591 and 1,449 images respectively). The proposed VQA task has connections to other related work: Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> has studied joint parsing of videos and corresponding text to answer queries on two datasets containing 15 video clips each. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> uses crowdsourced workers to answer questions about visual content asked by visually-impaired users.
In concurrent work, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposed combining an LSTM for the question
with a CNN for the image to generate an answer. In their model, the LSTM question representation is conditioned on the CNN image features at each time step, and the final LSTM hidden state is used to sequentially decode the answer phrase. In contrast, the model developed in this paper explores â€œlate fusionâ€ â€“ <em id="S2.p1.1.8" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p1.1.9" class="ltx_text"></span>, the LSTM question representation and the CNN image features are computed independently, <em id="S2.p1.1.10" class="ltx_emph ltx_font_italic">fused</em> via an element-wise multiplication, and then passed through fully-connected layers to generate a softmax distribution over output answer classes.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> generates abstract scenes to capture visual common sense relevant to answering (purely textual) fill-in-the-blank and visual paraphrasing questions. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> use visual information to assess the plausibility of common sense assertions. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> introduced a dataset of 10k images and prompted captions that describe specific aspects of a scene (<em id="S2.p1.1.11" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.12" class="ltx_text"></span>, individual objects, what will happen next).
Concurrent with our work, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> collected questions &amp; answers in Chinese (later translated to English by humans) for COCO images. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> automatically generated four types of questions (object, count, color, location) using COCO captions.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Text-based Q&amp;A</span>
is a well studied problem in the NLP and text processing communities (recent examples
beingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>).
Other related textual tasks include sentence
completion (<em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.3" class="ltx_text"></span>,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> with multiple-choice answers).
These approaches provide inspiration for VQA techniques.
One key concern in text is the <em id="S2.p2.1.4" class="ltx_emph ltx_font_italic">grounding</em> of questions.
For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> synthesized textual descriptions and QA-pairs
grounded in a simulation of actors and objects in a fixed set of locations.
VQA is naturally grounded in images â€“ requiring the understanding of both
text (questions) and vision (images).
Our questions are generated by humans, making the need for commonsense
knowledge and complex reasoning more essential.
</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Describing Visual Content.</span>
Related to VQA are the tasks of image taggingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, image
captioningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and video captioningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, where words or sentences are
generated to describe visual content. While these tasks require both visual and semantic knowledge,
captions can often be non-specific (e.g., observed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>). The questions in VQA require detailed specific information about the image for which generic
image captions are of little use <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Other Vision+Language Tasks.</span> Several recent papers have explored tasks at the intersection of vision and language
that are easier to evaluate than image captioning,
such as
coreference resolutionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
or generating referring expressionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> for a particular object in an image that would allow a human to identify
which object is being referred to (<em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p4.1.3" class="ltx_text"></span>, â€œthe one in a red shirtâ€,
â€œthe dog on the leftâ€).
While task-driven and concrete, a limited set of visual concepts
(<em id="S2.p4.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p4.1.5" class="ltx_text"></span>, color, location) tend to be captured by referring expressions. As we demonstrate, a
richer variety of visual concepts emerge from visual questions and their answers.

</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">VQA Dataset Collection</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/1505.00468/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the dataset. See the appendix for more examples.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We now describe the Visual Question Answering (VQA) dataset. We begin by
describing the real images and abstract scenes used to collect the questions. Next, we describe
our process of collecting questions and their corresponding answers. Analysis of the questions
and answers gathered as well as baselinesâ€™ &amp; methodsâ€™ results are provided in following sections.
</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Real Images.</span>
We use the 123,287 training and validation images and 81,434 test images from the newly-released Microsoft
Common Objects in Context (MS COCO)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> dataset. The MS COCO dataset was gathered to find images containing multiple objects and rich contextual information. Given the
visual complexity of these images, they are well-suited for our VQA task. The more diverse our
collection of images, the more diverse, comprehensive, and interesting the resultant set of
questions and their answers.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Abstract Scenes.</span>
The VQA task with real images requires the use of complex and often noisy
visual recognizers. To attract researchers interested in exploring the high-level reasoning
required for VQA, but not the low-level vision tasks, we create a new abstract scenes
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> containing 50K scenes.
The dataset contains 20 â€œpaperdollâ€ human models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> spanning genders, races,
and ages with 8 different expressions. The limbs are adjustable to allow
for continuous pose variations. The clipart may be used to depict both
indoor and outdoor scenes. The set contains over 100 objects and 31 animals in
various poses. The use of this clipart enables the creation of more realistic scenes (see bottom row of Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 VQA Dataset Collection â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) that more closely mirror
real images than previous papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
See the appendix
for the user interface, additional details, and examples.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Splits.</span>
For real images, we follow the same train/val/test split strategy as the MC COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
(including test-dev, test-standard, test-challenge, test-reserve). For the VQA challenge (see section <a href="#S6" title="6 VQA Challenge and Workshop â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), test-dev is used for debugging and validation experiments and allows for unlimited submission to the evaluation server. Test-standard is the â€˜defaultâ€™ test data for the VQA competition. When comparing to the state of the art (e.g., in papers), results should be reported on test-standard. Test-standard is also used to maintain a public leaderboard that is updated upon submission. Test-reserve is used to protect against possible overfitting. If there are substantial differences between a methodâ€™s scores on test-standard and test-reserve, this raises a red-flag and prompts further investigation. Results on test-reserve are not publicly revealed. Finally, test-challenge is used to determine the winners of the challenge.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">For abstract scenes, we created splits for standardization, separating the scenes into 20K/10K/20K for train/val/test splits, respectively. There are no subsplits (test-dev, test-standard, test-challenge, test-reserve) for abstract scenes.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Captions.</span>
The MS COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> already contains five single-sentence captions for all images.
We also collected five single-captions for all abstract scenes using the same user interface<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/tylin/coco-ui" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tylin/coco-ui</a></span></span></span> for collection.</p>
</div>
<div id="S3.p7" class="ltx_para ltx_noindent">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_font_bold">Questions.</span>
Collecting interesting, diverse, and well-posed questions is a significant challenge.
Many simple questions may only require low-level computer vision knowledge,
such as â€œWhat color is the cat?â€ or â€œHow many chairs are present in the scene?â€.
However, we also want questions that require commonsense knowledge about the scene,
such as â€œWhat sound does the pictured animal make?â€. Importantly, questions should also <em id="S3.p7.1.2" class="ltx_emph ltx_font_italic">require</em> the image to correctly answer and not be answerable using just commonsense information, <em id="S3.p7.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p7.1.4" class="ltx_text"></span>, inÂ Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, â€œWhat is the mustache made of?â€. By having a wide variety of
question types and difficulty, we may be able to measure the continual progress of both
visual understanding and commonsense reasoning.</p>
</div>
<div id="S3.p8" class="ltx_para ltx_noindent">
<p id="S3.p8.1" class="ltx_p">We tested and evaluated a number of user interfaces for collecting
such â€œinterestingâ€ questions. Specifically, we ran pilot studies asking human subjects to ask questions about a given image that they believe
a â€œtoddlerâ€, â€œalienâ€, or â€œsmart robotâ€ would have trouble answering.
We found the â€œsmart robotâ€ interface to elicit the most interesting and diverse questions.
As shown in the appendix,
our final interface stated:
</p>
<p id="S3.p8.2" class="ltx_p ltx_align_center">

<span id="S3.p8.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:195.1pt;">
<span id="S3.p8.2.1.1" class="ltx_p">â€œ<em id="S3.p8.2.1.1.1" class="ltx_emph ltx_font_italic">We have built a smart robot. It understands a lot about images. It can
recognize and name all the objects, it knows where the objects are, it can recognize the scene
(<em id="S3.p8.2.1.1.1.1" class="ltx_emph ltx_font_upright">e.g</em>.<span id="S3.p8.2.1.1.1.2" class="ltx_text"></span>, kitchen, beach), peopleâ€™s expressions and poses, and properties of objects (<em id="S3.p8.2.1.1.1.3" class="ltx_emph ltx_font_upright">e.g</em>.<span id="S3.p8.2.1.1.1.4" class="ltx_text"></span>, color of
objects, their texture). Your task is to stump this smart robot!</em></span>
<span id="S3.p8.2.1.2" class="ltx_p"><em id="S3.p8.2.1.2.1" class="ltx_emph ltx_font_italic">Ask a question about this scene that this smart robot probably can not answer, but any human can easily answer while looking at the scene in the image.</em>â€</span>
</span>
</p>
<p id="S3.p8.3" class="ltx_p">To bias against
generic image-independent questions,
subjects were instructed to ask questions that <em id="S3.p8.3.1" class="ltx_emph ltx_font_italic">require</em> the image to answer.</p>
</div>
<div id="S3.p9" class="ltx_para ltx_noindent">
<p id="S3.p9.1" class="ltx_p">The same user interface was used for both the real images and abstract scenes.
In total, three questions from unique workers were gathered for each image/scene.
When writing a question, the subjects were shown the previous questions already asked for that image to
increase the question diversity. In total, the dataset contains over
<math id="S3.p9.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.p9.1.m1.1a"><mo id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><csymbol cd="latexml" id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">\sim</annotation></semantics></math>0.76M questions.</p>
</div>
<div id="S3.p10" class="ltx_para ltx_noindent">
<p id="S3.p10.1" class="ltx_p"><span id="S3.p10.1.1" class="ltx_text ltx_font_bold">Answers.</span>
Open-ended questions result in a diverse set of possible answers.
For many questions, a simple â€œyesâ€ or â€œnoâ€ response is sufficient. However, other questions
may require a short phrase. Multiple different answers may also be correct. For instance, the
answers â€œwhiteâ€, â€œtanâ€, or â€œoff-whiteâ€ may all be correct answers to the same question.
Human subjects may also disagree on the â€œcorrectâ€ answer, <em id="S3.p10.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p10.1.3" class="ltx_text"></span>, some saying â€œyesâ€ while
others say â€œnoâ€. To handle these discrepancies, we gather <em id="S3.p10.1.4" class="ltx_emph ltx_font_italic">10 answers for each
question from unique workers</em>, while also ensuring that the worker answering a question did not ask it.
We ask the subjects to provide answers that are â€œa brief phrase and not a complete sentence.
Respond matter-of-factly and avoid using conversational language or inserting your opinion.â€
In addition to answering the questions, the subjects were asked â€œDo you think you were
able to answer the question correctly?â€ and given the choices of â€œnoâ€, â€œmaybeâ€, and â€œyesâ€. See the appendix for more details about the user interface to collect answers.
See SectionÂ <a href="#S4" title="4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for an analysis of the answers provided.</p>
</div>
<div id="S3.p11" class="ltx_para ltx_noindent">
<p id="S3.p11.1" class="ltx_p">For testing, we offer two modalities for answering the questions: (i) <span id="S3.p11.1.1" class="ltx_text ltx_font_bold">open-ended</span> and (ii) <span id="S3.p11.1.2" class="ltx_text ltx_font_bold">multiple-choice</span>.</p>
</div>
<div id="S3.p12" class="ltx_para ltx_noindent">
<p id="S3.p12.1" class="ltx_p">For the open-ended task, the generated answers are evaluated using the following accuracy metric:
</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.3" class="ltx_Math" alttext="\text{accuracy}=\min(\frac{\text{\# humans that provided that answer}}{3},1)" display="block"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.4" xref="S3.Ex1.m1.3.4.cmml"><mtext id="S3.Ex1.m1.3.4.2" xref="S3.Ex1.m1.3.4.2a.cmml">accuracy</mtext><mo id="S3.Ex1.m1.3.4.1" xref="S3.Ex1.m1.3.4.1.cmml">=</mo><mrow id="S3.Ex1.m1.3.4.3.2" xref="S3.Ex1.m1.3.4.3.1.cmml"><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">min</mi><mo id="S3.Ex1.m1.3.4.3.2a" xref="S3.Ex1.m1.3.4.3.1.cmml">â¡</mo><mrow id="S3.Ex1.m1.3.4.3.2.1" xref="S3.Ex1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.4.3.2.1.1" xref="S3.Ex1.m1.3.4.3.1.cmml">(</mo><mfrac id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mtext id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.2a.cmml"># humans that provided that answer</mtext><mn id="S3.Ex1.m1.2.2.3" xref="S3.Ex1.m1.2.2.3.cmml">3</mn></mfrac><mo id="S3.Ex1.m1.3.4.3.2.1.2" xref="S3.Ex1.m1.3.4.3.1.cmml">,</mo><mn id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">1</mn><mo stretchy="false" id="S3.Ex1.m1.3.4.3.2.1.3" xref="S3.Ex1.m1.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.4.cmml" xref="S3.Ex1.m1.3.4"><eq id="S3.Ex1.m1.3.4.1.cmml" xref="S3.Ex1.m1.3.4.1"></eq><ci id="S3.Ex1.m1.3.4.2a.cmml" xref="S3.Ex1.m1.3.4.2"><mtext id="S3.Ex1.m1.3.4.2.cmml" xref="S3.Ex1.m1.3.4.2">accuracy</mtext></ci><apply id="S3.Ex1.m1.3.4.3.1.cmml" xref="S3.Ex1.m1.3.4.3.2"><min id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"></min><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><divide id="S3.Ex1.m1.2.2.1.cmml" xref="S3.Ex1.m1.2.2"></divide><ci id="S3.Ex1.m1.2.2.2a.cmml" xref="S3.Ex1.m1.2.2.2"><mtext id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2"># humans that provided that answer</mtext></ci><cn type="integer" id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.3">3</cn></apply><cn type="integer" id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">\text{accuracy}=\min(\frac{\text{\# humans that provided that answer}}{3},1)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.p12.2" class="ltx_p"><em id="S3.p12.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p12.2.2" class="ltx_text"></span>, an answer is deemed 100% accurate if at least 3 workers provided that exact answer.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In order
to be consistent with â€˜human accuraciesâ€™ reported in SectionÂ <a href="#S4" title="4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, machine accuracies are
averaged over all <math id="footnote2.m1.3" class="ltx_Math" alttext="{10\choose 9}" display="inline"><semantics id="footnote2.m1.3b"><mrow id="footnote2.m1.3.3.5" xref="footnote2.m1.3.3.4.cmml"><mo id="footnote2.m1.3.3.5.1" xref="footnote2.m1.1.1.1.1.1.cmml">(</mo><mfrac linethickness="0pt" id="footnote2.m1.3.3.3.3" xref="footnote2.m1.3.3.4.cmml"><mn id="footnote2.m1.2.2.2.2.2" xref="footnote2.m1.2.2.2.2.2.cmml">10</mn><mn id="footnote2.m1.3.3.3.3.3" xref="footnote2.m1.3.3.3.3.3.cmml">9</mn></mfrac><mo id="footnote2.m1.3.3.5.2" xref="footnote2.m1.1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m1.3c"><apply id="footnote2.m1.3.3.4.cmml" xref="footnote2.m1.3.3.5"><csymbol cd="latexml" id="footnote2.m1.1.1.1.1.1.cmml" xref="footnote2.m1.3.3.5.1">binomial</csymbol><cn type="integer" id="footnote2.m1.2.2.2.2.2.cmml" xref="footnote2.m1.2.2.2.2.2">10</cn><cn type="integer" id="footnote2.m1.3.3.3.3.3.cmml" xref="footnote2.m1.3.3.3.3.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.3d">{10\choose 9}</annotation></semantics></math> sets of human annotators</span></span></span>
Before comparison, all responses are made lowercase, numbers converted to digits,
and punctuation &amp; articles removed. We avoid using soft metrics
such as Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, since they often group together
words that we wish to distinguish, such as â€œleftâ€ and â€œrightâ€. We also avoid using evaluation metrics from machine translation such as BLEU and ROUGE because such metrics are typically applicable and reliable for sentences containing multiple words. In VQA, most answers (89.32%) are single word; thus there no high-order n-gram matches between predicted answers and ground-truth answers, and low-order n-gram matches degenerate to exact-string matching. Moreover, these automatic metrics such as BLEU and ROUGE have been found to poorly correlate with human judgement for tasks such as image caption evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S3.p13" class="ltx_para ltx_noindent">
<p id="S3.p13.1" class="ltx_p">For multiple-choice task, 18 candidate answers are created for each question. As with the open-ended task,
the accuracy of a chosen option is computed based on the number of human subjects who provided
that answer (divided by 3 and clipped at 1). We generate a candidate set of correct
and incorrect answers from four sets of answers:
<span id="S3.p13.1.1" class="ltx_text ltx_font_bold">Correct:</span> The most common (out of ten) correct answer.
<span id="S3.p13.1.2" class="ltx_text ltx_font_bold">Plausible:</span> To generate incorrect, but still plausible answers we ask
three subjects to answer the questions without seeing the image. See the appendix for more details about the user interface to collect these answers. If three unique answers
are not found, we gather additional answers from nearest neighbor questions
using a bag-of-words model. The use of these answers helps ensure the image,
and not just commonsense knowledge, is necessary to answer the question.
<span id="S3.p13.1.3" class="ltx_text ltx_font_bold">Popular:</span> These are the 10 most popular answers. For instance, these are â€œyesâ€,
â€œnoâ€, â€œ2â€, â€œ1â€, â€œwhiteâ€, â€œ3â€, â€œredâ€, â€œblueâ€, â€œ4â€, â€œgreenâ€ for real images.
The inclusion of the most popular answers makes it more difficult for algorithms to infer the type of
question from the set of answers provided, <em id="S3.p13.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p13.1.5" class="ltx_text"></span>, learning that it is a â€œyes or noâ€ question just because
â€œyesâ€ and â€œnoâ€ are present in the answers.
<span id="S3.p13.1.6" class="ltx_text ltx_font_bold">Random:</span> Correct answers from random questions in the dataset.
To generate a total of 18 candidate answers, we first find the union of the correct, plausible, and popular answers.
We include random answers until 18 unique answers are found.
The order of the answers is randomized. Example multiple choice questions are in the appendix.
</p>
</div>
<div id="S3.p14" class="ltx_para ltx_noindent">
<p id="S3.p14.1" class="ltx_p">Note that all 18 candidate answers are unique. But since 10 different subjects answered every question, it is possible that more than one of those 10 answers be present in the 18 choices. In such cases, according to the accuracy metric, multiple options could have a non-zero accuracy.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">VQA Dataset Analysis</span>
</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/1505.00468/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of questions by their first four words for a random sample of 60K questions for real images (left) and all questions for abstract scenes (right). The ordering of the words starts towards the center and radiates outwards. The arc length is proportional to the number of questions containing the word. White areas are words with contributions too small to show. </figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this section, we provide an analysis of the questions and answers in the VQA train dataset.
To gain an understanding of the types of questions asked and answers provided, we visualize
the distribution of question types and answers. We also explore how often the questions may
be answered without the image using just commonsense information. Finally, we analyze whether
the information contained in an image caption is sufficient to answer the questions.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">The dataset includes 614,163 questions
and 7,984,119 answers (including answers provided by workers with and without
looking at the image)
for 204,721 images from the MS COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and 150,000 questions with 1,950,000 answers for <math id="S4.p2.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S4.p2.1.m1.2a"><mrow id="S4.p2.1.m1.2.3.2" xref="S4.p2.1.m1.2.3.1.cmml"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">50</mn><mo id="S4.p2.1.m1.2.3.2.1" xref="S4.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.2b"><list id="S4.p2.1.m1.2.3.1.cmml" xref="S4.p2.1.m1.2.3.2"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">50</cn><cn type="integer" id="S4.p2.1.m1.2.2.cmml" xref="S4.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.2c">50,000</annotation></semantics></math> abstract scenes.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Questions</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.6" class="ltx_p"><span id="S4.SS1.p1.6.1" class="ltx_text ltx_font_bold">Types of Question.</span>
Given the structure of questions generated in the English language,
we can cluster questions into different types based on the words that start the question.
Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the distribution of questions based on the first four
words of the questions for both the real images (left) and abstract scenes (right).
Interestingly, the distribution of questions is quite similar for both real images and abstract scenes.
This helps demonstrate that the type of questions elicited by the abstract scenes is similar to
those elicited by the real images. There exists a surprising variety of question types,
including â€œWhat is<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi mathvariant="normal" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\ldots</annotation></semantics></math>â€, â€œIs there<math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi mathvariant="normal" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\ldots</annotation></semantics></math>â€, â€œHow many<math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi mathvariant="normal" id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\ldots</annotation></semantics></math>â€, and â€œDoes the<math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi mathvariant="normal" id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\ldots</annotation></semantics></math>â€.
Quantitatively, the percentage of questions for different types is shown in TableÂ <a href="#S5.T3" title="TABLE III â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Several example questions and answers are shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 VQA Dataset Collection â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
A particularly interesting type of question is the â€œWhat is<math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mi mathvariant="normal" id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\ldots</annotation></semantics></math>â€ questions, since they have a
diverse set of possible answers. See the appendix for visualizations for â€œWhat is<math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mi mathvariant="normal" id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\ldots</annotation></semantics></math>â€ questions.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Lengths.</span>
Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4.1 Questions â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the distribution of question lengths.
We see that most questions range from four to ten words.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1505.00468/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Percentage of questions with different word lengths for real images and abstract scenes.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1505.00468/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="420" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution of answers per question type for a random sample of 60K questions for real images when subjects provide answers when given the image (top) and when not given the image (bottom).</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Answers</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Typical Answers.</span>
Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.1 Questions â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (top) shows the distribution of answers for several question types.
We can see that a number of question types, such as â€œIs theâ€¦â€, â€œAreâ€¦â€, and â€œDoesâ€¦â€ are
typically answered using â€œyesâ€ and â€œnoâ€ as answers.
Other questions such as â€œWhat isâ€¦â€ and â€œWhat typeâ€¦â€ have a rich diversity
of responses. Other question types such as â€œWhat colorâ€¦â€ or â€œWhichâ€¦â€ have more specialized responses,
such as colors, or â€œleftâ€ and â€œrightâ€.
See the appendix for a list of the most popular answers.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.6" class="ltx_p"><span id="S4.SS2.p2.6.1" class="ltx_text ltx_font_bold">Lengths.</span>
Most answers consist of a single word, with the distribution of answers containing one, two, or three words, respectively being <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="89.32\%" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">89.32</mn><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">89.32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">89.32\%</annotation></semantics></math>, <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="6.91\%" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">6.91</mn><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">6.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">6.91\%</annotation></semantics></math>, and <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="2.74\%" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">2.74</mn><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">2.74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">2.74\%</annotation></semantics></math> for real images and <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="90.51\%" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">90.51</mn><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">90.51</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">90.51\%</annotation></semantics></math>, <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="5.89\%" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mn id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">5.89</mn><mo id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">5.89</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">5.89\%</annotation></semantics></math>, and <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="2.49\%" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mn id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">2.49</mn><mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">2.49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">2.49\%</annotation></semantics></math> for abstract scenes.
The brevity of answers is not surprising, since the questions tend to elicit specific
information from the images. This is in contrast with image captions that generically
describe the entire image and hence tend to be longer. The brevity of our answers makes
automatic evaluation feasible. While it may be tempting to believe the brevity of the answers
makes the problem easier, recall that they are human-provided open-ended answers to
open-ended questions. The questions typically require complex reasoning to arrive at these
deceptively simple answers (see Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 VQA Dataset Collection â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
There are currently 23,234 unique one-word answers in our dataset for real images and 3,770 for abstract scenes.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.8" class="ltx_p"><span id="S4.SS2.p3.8.1" class="ltx_text ltx_font_bold">â€˜Yes/Noâ€™ and â€˜Numberâ€™ Answers.</span>
Many questions are answered using either â€œyesâ€ or â€œnoâ€ (or sometimes â€œmaybeâ€) â€“
<math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="38.37\%" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">38.37</mn><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">38.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">38.37\%</annotation></semantics></math> and <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="40.66\%" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">40.66</mn><mo id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">40.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">40.66\%</annotation></semantics></math> of the questions on real images and abstract scenes respectively.
Among these â€˜yes/noâ€™ questions, there is a bias towards â€œyesâ€ â€“ <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="58.83\%" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml"><mn id="S4.SS2.p3.3.m3.1.1.2" xref="S4.SS2.p3.3.m3.1.1.2.cmml">58.83</mn><mo id="S4.SS2.p3.3.m3.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2">58.83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">58.83\%</annotation></semantics></math> and <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="55.86\%" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mrow id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><mn id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml">55.86</mn><mo id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2">55.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">55.86\%</annotation></semantics></math> of â€˜yes/noâ€™ answers are â€œyesâ€ for real images and abstract scenes.
Question types such as â€œHow manyâ€¦â€ are answered using numbers â€“
<math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="12.31\%" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mn id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml">12.31</mn><mo id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2">12.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">12.31\%</annotation></semantics></math> and <math id="S4.SS2.p3.6.m6.1" class="ltx_Math" alttext="14.48\%" display="inline"><semantics id="S4.SS2.p3.6.m6.1a"><mrow id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml"><mn id="S4.SS2.p3.6.m6.1.1.2" xref="S4.SS2.p3.6.m6.1.1.2.cmml">14.48</mn><mo id="S4.SS2.p3.6.m6.1.1.1" xref="S4.SS2.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><apply id="S4.SS2.p3.6.m6.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.p3.6.m6.1.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.6.m6.1.1.2.cmml" xref="S4.SS2.p3.6.m6.1.1.2">14.48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">14.48\%</annotation></semantics></math> of the questions on real images and abstract scenes are â€˜numberâ€™ questions.
â€œ2â€ is the most popular answer among the â€˜numberâ€™ questions, making up
<math id="S4.SS2.p3.7.m7.1" class="ltx_Math" alttext="26.04\%" display="inline"><semantics id="S4.SS2.p3.7.m7.1a"><mrow id="S4.SS2.p3.7.m7.1.1" xref="S4.SS2.p3.7.m7.1.1.cmml"><mn id="S4.SS2.p3.7.m7.1.1.2" xref="S4.SS2.p3.7.m7.1.1.2.cmml">26.04</mn><mo id="S4.SS2.p3.7.m7.1.1.1" xref="S4.SS2.p3.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m7.1b"><apply id="S4.SS2.p3.7.m7.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1"><csymbol cd="latexml" id="S4.SS2.p3.7.m7.1.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.7.m7.1.1.2.cmml" xref="S4.SS2.p3.7.m7.1.1.2">26.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m7.1c">26.04\%</annotation></semantics></math> of the â€˜numberâ€™ answers for real images and <math id="S4.SS2.p3.8.m8.1" class="ltx_Math" alttext="39.85\%" display="inline"><semantics id="S4.SS2.p3.8.m8.1a"><mrow id="S4.SS2.p3.8.m8.1.1" xref="S4.SS2.p3.8.m8.1.1.cmml"><mn id="S4.SS2.p3.8.m8.1.1.2" xref="S4.SS2.p3.8.m8.1.1.2.cmml">39.85</mn><mo id="S4.SS2.p3.8.m8.1.1.1" xref="S4.SS2.p3.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m8.1b"><apply id="S4.SS2.p3.8.m8.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1"><csymbol cd="latexml" id="S4.SS2.p3.8.m8.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.8.m8.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.2">39.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m8.1c">39.85\%</annotation></semantics></math> for abstract scenes.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Subject Confidence.</span>
When the subjects answered the questions, we asked
â€œDo you think you were able to answer the question correctly?â€.
Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.2 Answers â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the distribution of responses. A majority of the answers
were labeled as confident for both real images and abstract scenes.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.3" class="ltx_p"><span id="S4.SS2.p5.3.1" class="ltx_text ltx_font_bold">Inter-human Agreement.</span>
Does the self-judgment of confidence correspond to the answer agreement between subjects?
Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.2 Answers â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the percentage of questions in which
(i) <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mn id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><cn type="integer" id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">7</annotation></semantics></math> or more,
(ii) <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="3-7" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mrow id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml"><mn id="S4.SS2.p5.2.m2.1.1.2" xref="S4.SS2.p5.2.m2.1.1.2.cmml">3</mn><mo id="S4.SS2.p5.2.m2.1.1.1" xref="S4.SS2.p5.2.m2.1.1.1.cmml">âˆ’</mo><mn id="S4.SS2.p5.2.m2.1.1.3" xref="S4.SS2.p5.2.m2.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><apply id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1"><minus id="S4.SS2.p5.2.m2.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1.1"></minus><cn type="integer" id="S4.SS2.p5.2.m2.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.2">3</cn><cn type="integer" id="S4.SS2.p5.2.m2.1.1.3.cmml" xref="S4.SS2.p5.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">3-7</annotation></semantics></math>, or
(iii) less than <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><mn id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><cn type="integer" id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">3</annotation></semantics></math> subjects agree on the answers given their average confidence score
(0 = not confident, 1 = confident).
As expected, the agreement between subjects increases with confidence.
However, even if all of the subjects are confident the answers may still vary.
This is not surprising since some answers may vary, yet have very similar meaning, such as â€œhappyâ€ and â€œjoyfulâ€.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1505.00468/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Number of questions per average confidence score (0 = not confident, 1 = confident) for real images and abstract scenes (black lines). Percentage of questions where 7 or more answers are same, 3-7 are same, less than 3 are same (color bars). </figcaption>
</figure>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.6" class="ltx_p">As shown in TableÂ <a href="#S4.T1" title="TABLE I â€£ 4.3 Commonsense Knowledge â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> (Question + Image), there is significant inter-human
agreement in the answers for both real images (<math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="83.30\%" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mrow id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mn id="S4.SS2.p6.1.m1.1.1.2" xref="S4.SS2.p6.1.m1.1.1.2.cmml">83.30</mn><mo id="S4.SS2.p6.1.m1.1.1.1" xref="S4.SS2.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p6.1.m1.1.1.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2">83.30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">83.30\%</annotation></semantics></math>) and abstract scenes (<math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="87.49\%" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><mrow id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mn id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">87.49</mn><mo id="S4.SS2.p6.2.m2.1.1.1" xref="S4.SS2.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p6.2.m2.1.1.2.cmml" xref="S4.SS2.p6.2.m2.1.1.2">87.49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">87.49\%</annotation></semantics></math>).
Note that on average each question has <math id="S4.SS2.p6.3.m3.1" class="ltx_Math" alttext="2.70" display="inline"><semantics id="S4.SS2.p6.3.m3.1a"><mn id="S4.SS2.p6.3.m3.1.1" xref="S4.SS2.p6.3.m3.1.1.cmml">2.70</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.3.m3.1b"><cn type="float" id="S4.SS2.p6.3.m3.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1">2.70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.3.m3.1c">2.70</annotation></semantics></math> unique answers for real images and <math id="S4.SS2.p6.4.m4.1" class="ltx_Math" alttext="2.39" display="inline"><semantics id="S4.SS2.p6.4.m4.1a"><mn id="S4.SS2.p6.4.m4.1.1" xref="S4.SS2.p6.4.m4.1.1.cmml">2.39</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.4.m4.1b"><cn type="float" id="S4.SS2.p6.4.m4.1.1.cmml" xref="S4.SS2.p6.4.m4.1.1">2.39</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.4.m4.1c">2.39</annotation></semantics></math> for abstract scenes.
The agreement is significantly higher (<math id="S4.SS2.p6.5.m5.1" class="ltx_Math" alttext="&gt;95\%" display="inline"><semantics id="S4.SS2.p6.5.m5.1a"><mrow id="S4.SS2.p6.5.m5.1.1" xref="S4.SS2.p6.5.m5.1.1.cmml"><mi id="S4.SS2.p6.5.m5.1.1.2" xref="S4.SS2.p6.5.m5.1.1.2.cmml"></mi><mo id="S4.SS2.p6.5.m5.1.1.1" xref="S4.SS2.p6.5.m5.1.1.1.cmml">&gt;</mo><mrow id="S4.SS2.p6.5.m5.1.1.3" xref="S4.SS2.p6.5.m5.1.1.3.cmml"><mn id="S4.SS2.p6.5.m5.1.1.3.2" xref="S4.SS2.p6.5.m5.1.1.3.2.cmml">95</mn><mo id="S4.SS2.p6.5.m5.1.1.3.1" xref="S4.SS2.p6.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.5.m5.1b"><apply id="S4.SS2.p6.5.m5.1.1.cmml" xref="S4.SS2.p6.5.m5.1.1"><gt id="S4.SS2.p6.5.m5.1.1.1.cmml" xref="S4.SS2.p6.5.m5.1.1.1"></gt><csymbol cd="latexml" id="S4.SS2.p6.5.m5.1.1.2.cmml" xref="S4.SS2.p6.5.m5.1.1.2">absent</csymbol><apply id="S4.SS2.p6.5.m5.1.1.3.cmml" xref="S4.SS2.p6.5.m5.1.1.3"><csymbol cd="latexml" id="S4.SS2.p6.5.m5.1.1.3.1.cmml" xref="S4.SS2.p6.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS2.p6.5.m5.1.1.3.2.cmml" xref="S4.SS2.p6.5.m5.1.1.3.2">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.5.m5.1c">&gt;95\%</annotation></semantics></math>) for â€œyes/noâ€ questions and lower for other questions (<math id="S4.SS2.p6.6.m6.1" class="ltx_Math" alttext="&lt;76\%" display="inline"><semantics id="S4.SS2.p6.6.m6.1a"><mrow id="S4.SS2.p6.6.m6.1.1" xref="S4.SS2.p6.6.m6.1.1.cmml"><mi id="S4.SS2.p6.6.m6.1.1.2" xref="S4.SS2.p6.6.m6.1.1.2.cmml"></mi><mo id="S4.SS2.p6.6.m6.1.1.1" xref="S4.SS2.p6.6.m6.1.1.1.cmml">&lt;</mo><mrow id="S4.SS2.p6.6.m6.1.1.3" xref="S4.SS2.p6.6.m6.1.1.3.cmml"><mn id="S4.SS2.p6.6.m6.1.1.3.2" xref="S4.SS2.p6.6.m6.1.1.3.2.cmml">76</mn><mo id="S4.SS2.p6.6.m6.1.1.3.1" xref="S4.SS2.p6.6.m6.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.6.m6.1b"><apply id="S4.SS2.p6.6.m6.1.1.cmml" xref="S4.SS2.p6.6.m6.1.1"><lt id="S4.SS2.p6.6.m6.1.1.1.cmml" xref="S4.SS2.p6.6.m6.1.1.1"></lt><csymbol cd="latexml" id="S4.SS2.p6.6.m6.1.1.2.cmml" xref="S4.SS2.p6.6.m6.1.1.2">absent</csymbol><apply id="S4.SS2.p6.6.m6.1.1.3.cmml" xref="S4.SS2.p6.6.m6.1.1.3"><csymbol cd="latexml" id="S4.SS2.p6.6.m6.1.1.3.1.cmml" xref="S4.SS2.p6.6.m6.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS2.p6.6.m6.1.1.3.2.cmml" xref="S4.SS2.p6.6.m6.1.1.3.2">76</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.6.m6.1c">&lt;76\%</annotation></semantics></math>), possibly due to the fact that we perform exact string matching and do not account for synonyms, plurality, <em id="S4.SS2.p6.6.1" class="ltx_emph ltx_font_italic">etc</em>. Note that the automatic determination of synonyms is a difficult problem, since the level of answer granularity can vary across questions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_italic">Commonsense Knowledge</span>
</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/1505.00468/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_img_landscape" width="461" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S4.F7.2.1" class="ltx_text" style="font-size:90%;">Example questions judged by Mturk workers to be answerable by different age groups. The percentage of questions falling into each age group is shown in parentheses.</span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Is the Image Necessary?</span>
Clearly, some questions can sometimes be
answered correctly using commonsense knowledge alone without the need for an image,
<em id="S4.SS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p1.1.3" class="ltx_text"></span>, â€œWhat is the color of the fire hydrant?â€.
We explore this issue by asking three subjects to answer
the questions <em id="S4.SS3.p1.1.4" class="ltx_emph ltx_font_italic">without seeing the image</em> (see the examples in blue in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 VQA Dataset Collection â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
In TableÂ <a href="#S4.T1" title="TABLE I â€£ 4.3 Commonsense Knowledge â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> (Question), we show the percentage of questions for which
the correct answer is provided over all questions, â€œyes/noâ€ questions, and the other questions that
are not â€œyes/noâ€. For â€œyes/noâ€ questions, the human subjects respond better than chance.
For other questions, humans are only correct about <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="21\%" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">21</mn><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">21\%</annotation></semantics></math> of the time. This demonstrates that
understanding the visual information is critical to VQA and that commonsense information alone is not sufficient.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">To show the qualitative difference in answers provided with and without images,
we show the distribution of answers for various question types in Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.1 Questions â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (bottom).
The distribution of colors, numbers, and even â€œyes/noâ€ responses is surprisingly different for answers
with and without images.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.8" class="ltx_p"><span id="S4.SS3.p3.8.1" class="ltx_text ltx_font_bold">Which Questions Require Common Sense?</span>
In order to identify questions that require commonsense reasoning to answer, we conducted
two AMT studies (on a subset 10K questions from the real images of VQA trainval) asking subjects â€“</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Whether or not they believed a question required commonsense to answer the question, and</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">The youngest age group that they believe a person must be in order to be able to correctly answer the question â€“
toddler (3-4),
younger child (5-8),
older child (9-12),
teenager (13-17),
adult (18+).</p>
</div>
</li>
</ol>
<p id="S4.SS3.p3.7" class="ltx_p">Each question was shown to 10 subjects. We found that
for <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="47.43\%" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">47.43</mn><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">47.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">47.43\%</annotation></semantics></math> of questions 3 or more subjects voted â€˜yesâ€™ to commonsense,
(<math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="18.14\%" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mn id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">18.14</mn><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">18.14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">18.14\%</annotation></semantics></math>: 6 or more).
In the â€˜perceived human age required to answer questionâ€™ study, we found the following distribution of responses:
toddler: <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="15.3\%" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mn id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">15.3</mn><mo id="S4.SS3.p3.3.m3.1.1.1" xref="S4.SS3.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">15.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">15.3\%</annotation></semantics></math>,
younger child: <math id="S4.SS3.p3.4.m4.1" class="ltx_Math" alttext="39.7\%" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mrow id="S4.SS3.p3.4.m4.1.1" xref="S4.SS3.p3.4.m4.1.1.cmml"><mn id="S4.SS3.p3.4.m4.1.1.2" xref="S4.SS3.p3.4.m4.1.1.2.cmml">39.7</mn><mo id="S4.SS3.p3.4.m4.1.1.1" xref="S4.SS3.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m4.1b"><apply id="S4.SS3.p3.4.m4.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p3.4.m4.1.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.4.m4.1.1.2.cmml" xref="S4.SS3.p3.4.m4.1.1.2">39.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">39.7\%</annotation></semantics></math>,
older child: <math id="S4.SS3.p3.5.m5.1" class="ltx_Math" alttext="28.4\%" display="inline"><semantics id="S4.SS3.p3.5.m5.1a"><mrow id="S4.SS3.p3.5.m5.1.1" xref="S4.SS3.p3.5.m5.1.1.cmml"><mn id="S4.SS3.p3.5.m5.1.1.2" xref="S4.SS3.p3.5.m5.1.1.2.cmml">28.4</mn><mo id="S4.SS3.p3.5.m5.1.1.1" xref="S4.SS3.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.m5.1b"><apply id="S4.SS3.p3.5.m5.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS3.p3.5.m5.1.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.5.m5.1.1.2.cmml" xref="S4.SS3.p3.5.m5.1.1.2">28.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m5.1c">28.4\%</annotation></semantics></math>,
teenager: <math id="S4.SS3.p3.6.m6.1" class="ltx_Math" alttext="11.2\%" display="inline"><semantics id="S4.SS3.p3.6.m6.1a"><mrow id="S4.SS3.p3.6.m6.1.1" xref="S4.SS3.p3.6.m6.1.1.cmml"><mn id="S4.SS3.p3.6.m6.1.1.2" xref="S4.SS3.p3.6.m6.1.1.2.cmml">11.2</mn><mo id="S4.SS3.p3.6.m6.1.1.1" xref="S4.SS3.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.6.m6.1b"><apply id="S4.SS3.p3.6.m6.1.1.cmml" xref="S4.SS3.p3.6.m6.1.1"><csymbol cd="latexml" id="S4.SS3.p3.6.m6.1.1.1.cmml" xref="S4.SS3.p3.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.6.m6.1.1.2.cmml" xref="S4.SS3.p3.6.m6.1.1.2">11.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.6.m6.1c">11.2\%</annotation></semantics></math>,
adult: <math id="S4.SS3.p3.7.m7.1" class="ltx_Math" alttext="5.5\%" display="inline"><semantics id="S4.SS3.p3.7.m7.1a"><mrow id="S4.SS3.p3.7.m7.1.1" xref="S4.SS3.p3.7.m7.1.1.cmml"><mn id="S4.SS3.p3.7.m7.1.1.2" xref="S4.SS3.p3.7.m7.1.1.2.cmml">5.5</mn><mo id="S4.SS3.p3.7.m7.1.1.1" xref="S4.SS3.p3.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.7.m7.1b"><apply id="S4.SS3.p3.7.m7.1.1.cmml" xref="S4.SS3.p3.7.m7.1.1"><csymbol cd="latexml" id="S4.SS3.p3.7.m7.1.1.1.cmml" xref="S4.SS3.p3.7.m7.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.7.m7.1.1.2.cmml" xref="S4.SS3.p3.7.m7.1.1.2">5.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.7.m7.1c">5.5\%</annotation></semantics></math>.
In Figure <a href="#S4.F7" title="Figure 7 â€£ 4.3 Commonsense Knowledge â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we show several questions for which a majority of subjects picked the specified age range. Surprisingly the perceived age needed to answer the questions is fairly well distributed across the different age ranges. As expected the questions that were judged answerable by an adult (18+) generally need specialized knowledge, whereas those answerable by a toddler (3-4) are more generic.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.3" class="ltx_p">We measure the degree of commonsense required to answer a question as the percentage of subjects (out of 10) who voted â€œyesâ€ in our â€œwhether or not a question requires commonsenseâ€ study.
A fine-grained breakdown of average age and average degree of common sense (on a scale of <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="0-100" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mn id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">0</mn><mo id="S4.SS3.p4.1.m1.1.1.1" xref="S4.SS3.p4.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><minus id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">0</cn><cn type="integer" id="S4.SS3.p4.1.m1.1.1.3.cmml" xref="S4.SS3.p4.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">0-100</annotation></semantics></math>) required to answer a question is shown in TableÂ <a href="#S5.T3" title="TABLE III â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. The average age and the average degree of commonsense across all questions is <math id="S4.SS3.p4.2.m2.1" class="ltx_Math" alttext="8.92" display="inline"><semantics id="S4.SS3.p4.2.m2.1a"><mn id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">8.92</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><cn type="float" id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">8.92</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">8.92</annotation></semantics></math> and <math id="S4.SS3.p4.3.m3.1" class="ltx_Math" alttext="31.01\%" display="inline"><semantics id="S4.SS3.p4.3.m3.1a"><mrow id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml"><mn id="S4.SS3.p4.3.m3.1.1.2" xref="S4.SS3.p4.3.m3.1.1.2.cmml">31.01</mn><mo id="S4.SS3.p4.3.m3.1.1.1" xref="S4.SS3.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b"><apply id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p4.3.m3.1.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p4.3.m3.1.1.2.cmml" xref="S4.SS3.p4.3.m3.1.1.2">31.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">31.01\%</annotation></semantics></math> respectively.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p">It is important to distinguish between:</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">How old someone needs to be to be able to answer a question correctly, and</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">How old people <em id="S4.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">think</em> someone needs to be to be able to answer a question correctly.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS3.p6" class="ltx_para ltx_noindent">
<p id="S4.SS3.p6.1" class="ltx_p">Our age annotations capture the latter â€“ perceptions of MTurk workers in an uncontrolled environment. As such, the relative ordering of question types in TableÂ <a href="#S5.T3" title="TABLE III â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> is more important than absolute age numbers.
The two rankings of questions in terms of common sense required according to the two studies
were largely correlated (Pearsonâ€™s rank correlation: 0.58).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Input</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">All</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Yes/No</span></th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Number</span></th>
<th id="S4.T1.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Question</span></th>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">40.81</span></td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">67.60</span></td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.2.1.5.1" class="ltx_text" style="font-size:90%;">25.77</span></td>
<td id="S4.T1.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.2.1.6.1" class="ltx_text" style="font-size:90%;">21.22</span></td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Real</span></th>
<th id="S4.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">Question + Caption*</span></th>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">57.47</span></td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">78.97</span></td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">39.68</span></td>
<td id="S4.T1.1.3.2.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.3.2.6.1" class="ltx_text" style="font-size:90%;">44.41</span></td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.4.3.2.1" class="ltx_text" style="font-size:90%;">Question + Image</span></th>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.4.3.3.1" class="ltx_text" style="font-size:90%;">83.30</span></td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.4.3.4.1" class="ltx_text" style="font-size:90%;">95.77</span></td>
<td id="S4.T1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.4.3.5.1" class="ltx_text" style="font-size:90%;">83.39</span></td>
<td id="S4.T1.1.4.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.4.3.6.1" class="ltx_text" style="font-size:90%;">72.67</span></td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">Question</span></th>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">43.27</span></td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.5.4.4.1" class="ltx_text" style="font-size:90%;">66.65</span></td>
<td id="S4.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.5.4.5.1" class="ltx_text" style="font-size:90%;">28.52</span></td>
<td id="S4.T1.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.5.4.6.1" class="ltx_text" style="font-size:90%;">23.66</span></td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Abstract</span></th>
<th id="S4.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">Question + Caption*</span></th>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">54.34</span></td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.6.5.4.1" class="ltx_text" style="font-size:90%;">74.70</span></td>
<td id="S4.T1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">41.19</span></td>
<td id="S4.T1.1.6.5.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.6.5.6.1" class="ltx_text" style="font-size:90%;">40.18</span></td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">Question + Image</span></th>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">87.49</span></td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.7.6.4.1" class="ltx_text" style="font-size:90%;">95.96</span></td>
<td id="S4.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">95.04</span></td>
<td id="S4.T1.1.7.6.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T1.1.7.6.6.1" class="ltx_text" style="font-size:90%;">75.33</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Test-standard accuracy of human subjects when asked to answer the
question without seeing the image (Question),
seeing just a caption of the image and not the image itself (Question + Caption),
and seeing the image (Question + Image).
Results are shown for all questions, â€œyes/noâ€ &amp; â€œnumberâ€ questions, and other questions
that are neither answered â€œyes/noâ€ nor number.
All answers are free-form and not multiple-choice.
*â€†These accuracies are evaluated on a subset of 3K train questions (1K images).</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span><span id="S4.SS4.1.1" class="ltx_text ltx_font_italic">Captions <em id="S4.SS4.1.1.1" class="ltx_emph ltx_font_bold">vs</em></span><span id="S4.SS4.2.2" class="ltx_text ltx_font_bold">.<span id="S4.SS4.2.2.1" class="ltx_text"></span></span><span id="S4.SS4.3.3" class="ltx_text ltx_font_italic"> Questions</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">Do generic image captions provide enough information to answer the questions?
TableÂ <a href="#S4.T1" title="TABLE I â€£ 4.3 Commonsense Knowledge â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> (Question + Caption) shows the percentage of questions answered
correctly when human subjects are given the question and a human-provided caption
describing the image, but not the image. As expected, the results are better than when humans are shown the questions alone.
However, the accuracies are significantly lower than when subjects are shown the actual image.
This demonstrates that in order to answer the questions correctly, deeper image understanding
(beyond what image captions typically capture) is necessary. In fact, we find that the distributions of nouns, verbs, and adjectives mentioned in captions is statistically significantly different from those mentioned in our questions + answers (Kolmogorov-Smirnov test, <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="p&lt;.001" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><lt id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></lt><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">p&lt;.001</annotation></semantics></math>) for both real images and abstract scenes. See the appendix for details.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">VQA Baselines and Methods</span>
</h2>

<figure id="S5.F8" class="ltx_figure"><img src="/html/1505.00468/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Our best performing model (deeper LSTM Q + norm I). This model uses a two layer LSTM to encode the questions and the last hidden layer of VGGNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> to encode the images. The image features are then <math id="S5.F8.2.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S5.F8.2.m1.1b"><msub id="S5.F8.2.m1.1.1" xref="S5.F8.2.m1.1.1.cmml"><mi mathvariant="normal" id="S5.F8.2.m1.1.1.2" xref="S5.F8.2.m1.1.1.2.cmml">â„“</mi><mn id="S5.F8.2.m1.1.1.3" xref="S5.F8.2.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.F8.2.m1.1c"><apply id="S5.F8.2.m1.1.1.cmml" xref="S5.F8.2.m1.1.1"><csymbol cd="ambiguous" id="S5.F8.2.m1.1.1.1.cmml" xref="S5.F8.2.m1.1.1">subscript</csymbol><ci id="S5.F8.2.m1.1.1.2.cmml" xref="S5.F8.2.m1.1.1.2">â„“</ci><cn type="integer" id="S5.F8.2.m1.1.1.3.cmml" xref="S5.F8.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F8.2.m1.1d">\ell_{2}</annotation></semantics></math> normalized. Both the question and image features are transformed to a common space and fused via element-wise multiplication, which is then passed through a fully connected layer followed by a softmax layer to obtain a distribution over answers.</figcaption>
</figure>
<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this section, we explore the difficulty of the VQA dataset for the MS COCO images using several baselines
and novel methods. We train on VQA train+val. Unless stated otherwise, all human accuracies are on test-standard, machine accuracies are on test-dev, and results involving human captions (in gray font) are trained on train and tested on val (because captions are not available for test).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">Baselines</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">We implemented the following baselines:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">random:</span> We randomly choose an answer from the top 1K answers of the VQA train/val dataset.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">prior (â€œyesâ€):</span> We always select the most popular answer (â€œyesâ€) for both the open-ended and multiple-choice tasks. Note that â€œyesâ€ is always one of the choices for the multiple-choice questions.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">per Q-type prior:</span> For the open-ended task, we pick the most popular answer per question type (see the appendix for details). For the multiple-choice task, we pick the answer (from the provided choices) that is most similar to the picked answer for the open-ended task using cosine similarity in Word2Vec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> feature space.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">nearest neighbor:</span> Given a test image, question pair, we first find the <math id="S5.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.I1.i4.p1.1.m1.1a"><mi id="S5.I1.i4.p1.1.m1.1.1" xref="S5.I1.i4.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.1.m1.1b"><ci id="S5.I1.i4.p1.1.m1.1.1.cmml" xref="S5.I1.i4.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.1.m1.1c">K</annotation></semantics></math> nearest neighbor questions and associated images from the training set. See appendix for details on how neighbors are found. Next, for the open-ended task, we pick the most frequent ground truth answer from this set of nearest neighbor question, image pairs. Similar to the â€œper Q-type priorâ€ baseline, for the multiple-choice task, we pick the answer (from the provided choices) that is most similar to the picked answer for the open-ended task using cosine similarity in 
<br class="ltx_break">Word2Vec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> feature space.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">Methods</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.3" class="ltx_p">For our methods, we develop a 2-channel vision (image) + language (question) model that culminates with a softmax over <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">K</annotation></semantics></math> possible outputs. We choose the top <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="K=1000" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">K</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><eq id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></eq><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">ğ¾</ci><cn type="integer" id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">K=1000</annotation></semantics></math> most frequent answers as possible outputs. This set of answers covers <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="82.67\%" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mn id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">82.67</mn><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">82.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">82.67\%</annotation></semantics></math> of the train+val answers. We describe the different components of our model below:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Image Channel:</span> This channel provides an embedding for the image. We experiment with two embeddings â€“</p>
<ol id="S5.I2" class="ltx_enumerate">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">I:</span> The activations from the last hidden layer of VGGNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> are used as 4096-dim image embedding.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">norm I:</span> These are <math id="S5.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S5.I2.i2.p1.1.m1.1a"><msub id="S5.I2.i2.p1.1.m1.1.1" xref="S5.I2.i2.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.I2.i2.p1.1.m1.1.1.2" xref="S5.I2.i2.p1.1.m1.1.1.2.cmml">â„“</mi><mn id="S5.I2.i2.p1.1.m1.1.1.3" xref="S5.I2.i2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.1.m1.1b"><apply id="S5.I2.i2.p1.1.m1.1.1.cmml" xref="S5.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I2.i2.p1.1.m1.1.1.1.cmml" xref="S5.I2.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.I2.i2.p1.1.m1.1.1.2.cmml" xref="S5.I2.i2.p1.1.m1.1.1.2">â„“</ci><cn type="integer" id="S5.I2.i2.p1.1.m1.1.1.3.cmml" xref="S5.I2.i2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.1.m1.1c">\ell_{2}</annotation></semantics></math> normalized activations from the last hidden layer of VGGNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Question Channel:</span> This channel provides an embedding for the question. We experiment with three embeddings â€“</p>
<ol id="S5.I3" class="ltx_enumerate">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Bag-of-Words Question (BoW Q)</span>: The top 1,000 words in the questions are used to create a bag-of-words representation. Since there is a strong correlation between the words that start a question and the answer (see Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.1 Questions â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), we find the top 10 first, second, and third words of the questions and create a 30 dimensional bag-of-words representation. These features are concatenated to get a 1,030-dim embedding for the question.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">LSTM Q:</span> An LSTM with one hidden layer is used to obtain 1024-dim embedding for the question. The embedding obtained from the LSTM is a concatenation of last cell state and last hidden state representations (each being 512-dim) from the hidden layer of the LSTM. Each question word is encoded with 300-dim embedding by a fully-connected layer + tanh non-linearity which is then fed to the LSTM. The input vocabulary to the embedding layer consists of all the question words seen in the training dataset.</p>
</div>
</li>
<li id="S5.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I3.i3.p1" class="ltx_para">
<p id="S5.I3.i3.p1.1" class="ltx_p"><span id="S5.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">deeper LSTM Q:</span> An LSTM with two hidden layers is used to obtain 2048-dim embedding for the question. The embedding obtained from the LSTM is a concatenation of last cell state and last hidden state representations (each being 512-dim) from each of the two hidden layers of the LSTM. Hence 2 (hidden layers) x 2 (cell state and hidden state) x 512 (dimensionality of each of the cell states, as well as hidden states) in Fig.Â <a href="#S5.F8" title="Figure 8 â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. This is followed by a fully-connected layer + tanh non-linearity to transform 2048-dim embedding to 1024-dim. The question words are encoded in the same way as in LSTM Q.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.2" class="ltx_p"><span id="S5.SS2.p4.2.1" class="ltx_text ltx_font_bold">Multi-Layer Perceptron (MLP):</span> The image and question embeddings
are combined to obtain a single embedding.</p>
<ol id="S5.I4" class="ltx_enumerate">
<li id="S5.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I4.i1.p1" class="ltx_para">
<p id="S5.I4.i1.p1.1" class="ltx_p">For <span id="S5.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">BoW Q + I</span> method, we simply concatenate the BoW Q and I embeddings.</p>
</div>
</li>
<li id="S5.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I4.i2.p1" class="ltx_para">
<p id="S5.I4.i2.p1.1" class="ltx_p">For <span id="S5.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">LSTM Q + I</span>, and <span id="S5.I4.i2.p1.1.2" class="ltx_text ltx_font_bold">deeper LSTM Q + norm I</span> (Fig.Â <a href="#S5.F8" title="Figure 8 â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) methods, the image embedding is first transformed to 1024-dim by a fully-connected layer + tanh non-linearity to match the LSTM embedding of the question. The transformed image and LSTM embeddings (being in a common space) are then fused via element-wise multiplication.</p>
</div>
</li>
</ol>
<p id="S5.SS2.p4.1" class="ltx_p">This combined image + question embedding is then passed to an MLP â€“ a fully connected neural network classifier with 2 hidden layers and 1000 hidden units (dropout 0.5) in each layer with tanh non-linearity, followed by a softmax layer to obtain a distribution over <math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><mi id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><ci id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">K</annotation></semantics></math> answers. The entire model is learned end-to-end with a cross-entropy loss. VGGNet parameters are frozen to those learned for ImageNet classification and not fine-tuned in the image channel.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p">We also experimented with providing captions as input to our model. Similar to TableÂ <a href="#S4.T1" title="TABLE I â€£ 4.3 Commonsense Knowledge â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we assume that a human-generated caption is given as input. We use a bag-of-words representation containing the 1,000 most popular words in the captions as the caption embedding (<span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Caption</span>). For <span id="S5.SS2.p5.1.2" class="ltx_text ltx_font_bold">BoW Question + Caption (BoW Q + C)</span> method, we simply concatenate the BoW Q and C embeddings.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p">For testing, we report the result on two different tasks: open-ended selects the answer with highest activation from all possible <math id="S5.SS2.p6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS2.p6.1.m1.1a"><mi id="S5.SS2.p6.1.m1.1.1" xref="S5.SS2.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><ci id="S5.SS2.p6.1.m1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">K</annotation></semantics></math> answers and multiple-choice picks the answer that has the highest activation from the potential answers.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">Results</span>
</h3>

<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;"></th>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;" colspan="4"><span id="S5.T2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Open-Ended</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;" colspan="4"><span id="S5.T2.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Multiple-Choice</span></td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"></th>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.2.1" class="ltx_text" style="font-size:70%;">All</span></td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Yes/No</span></td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Number</span></td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Other</span></td>
<td id="S5.T2.1.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.6.1" class="ltx_text" style="font-size:70%;">All</span></td>
<td id="S5.T2.1.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.7.1" class="ltx_text" style="font-size:70%;">Yes/No</span></td>
<td id="S5.T2.1.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.8.1" class="ltx_text" style="font-size:70%;">Number</span></td>
<td id="S5.T2.1.2.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.2.2.9.1" class="ltx_text" style="font-size:70%;">Other</span></td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<th id="S5.T2.1.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.1.1" class="ltx_text" style="font-size:70%;">prior (â€œyesâ€)</span></th>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.2.1" class="ltx_text" style="font-size:70%;">29.66</span></td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.3.1" class="ltx_text" style="font-size:70%;">70.81</span></td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.4.1" class="ltx_text" style="font-size:70%;">00.39</span></td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.5.1" class="ltx_text" style="font-size:70%;">01.15</span></td>
<td id="S5.T2.1.3.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.6.1" class="ltx_text" style="font-size:70%;">29.66</span></td>
<td id="S5.T2.1.3.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.7.1" class="ltx_text" style="font-size:70%;">70.81</span></td>
<td id="S5.T2.1.3.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.8.1" class="ltx_text" style="font-size:70%;">00.39</span></td>
<td id="S5.T2.1.3.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.3.3.9.1" class="ltx_text" style="font-size:70%;">01.15</span></td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<th id="S5.T2.1.4.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.1.1" class="ltx_text" style="font-size:70%;">per Q-type prior</span></th>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.2.1" class="ltx_text" style="font-size:70%;">37.54</span></td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.3.1" class="ltx_text" style="font-size:70%;">71.03</span></td>
<td id="S5.T2.1.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.4.1" class="ltx_text" style="font-size:70%;">35.77</span></td>
<td id="S5.T2.1.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.5.1" class="ltx_text" style="font-size:70%;">09.38</span></td>
<td id="S5.T2.1.4.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.6.1" class="ltx_text" style="font-size:70%;">39.45</span></td>
<td id="S5.T2.1.4.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.7.1" class="ltx_text" style="font-size:70%;">71.02</span></td>
<td id="S5.T2.1.4.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.8.1" class="ltx_text" style="font-size:70%;">35.86</span></td>
<td id="S5.T2.1.4.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.4.4.9.1" class="ltx_text" style="font-size:70%;">13.34</span></td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<th id="S5.T2.1.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.1.1" class="ltx_text" style="font-size:70%;">nearest neighbor</span></th>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.2.1" class="ltx_text" style="font-size:70%;">42.70</span></td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.3.1" class="ltx_text" style="font-size:70%;">71.89</span></td>
<td id="S5.T2.1.5.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.4.1" class="ltx_text" style="font-size:70%;">24.36</span></td>
<td id="S5.T2.1.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.5.1" class="ltx_text" style="font-size:70%;">21.94</span></td>
<td id="S5.T2.1.5.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.6.1" class="ltx_text" style="font-size:70%;">48.49</span></td>
<td id="S5.T2.1.5.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.7.1" class="ltx_text" style="font-size:70%;">71.94</span></td>
<td id="S5.T2.1.5.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.8.1" class="ltx_text" style="font-size:70%;">26.00</span></td>
<td id="S5.T2.1.5.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.5.5.9.1" class="ltx_text" style="font-size:70%;">33.56</span></td>
</tr>
<tr id="S5.T2.1.6.6" class="ltx_tr">
<th id="S5.T2.1.6.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.1.1" class="ltx_text" style="font-size:70%;">BoW Q</span></th>
<td id="S5.T2.1.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.2.1" class="ltx_text" style="font-size:70%;color:#000000;">48.09</span></td>
<td id="S5.T2.1.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.3.1" class="ltx_text" style="font-size:70%;color:#000000;">75.66</span></td>
<td id="S5.T2.1.6.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.4.1" class="ltx_text" style="font-size:70%;color:#000000;">36.70</span></td>
<td id="S5.T2.1.6.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.5.1" class="ltx_text" style="font-size:70%;color:#000000;">27.14</span></td>
<td id="S5.T2.1.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.6.1" class="ltx_text" style="font-size:70%;color:#000000;">53.68</span></td>
<td id="S5.T2.1.6.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.7.1" class="ltx_text" style="font-size:70%;color:#000000;">75.71</span></td>
<td id="S5.T2.1.6.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.8.1" class="ltx_text" style="font-size:70%;color:#000000;">37.05</span></td>
<td id="S5.T2.1.6.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.6.6.9.1" class="ltx_text" style="font-size:70%;color:#000000;">38.64</span></td>
</tr>
<tr id="S5.T2.1.7.7" class="ltx_tr">
<th id="S5.T2.1.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.1.1" class="ltx_text" style="font-size:70%;">I</span></th>
<td id="S5.T2.1.7.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.2.1" class="ltx_text" style="font-size:70%;color:#000000;">28.13</span></td>
<td id="S5.T2.1.7.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.3.1" class="ltx_text" style="font-size:70%;color:#000000;">64.01</span></td>
<td id="S5.T2.1.7.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.4.1" class="ltx_text" style="font-size:70%;">00.42</span></td>
<td id="S5.T2.1.7.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.5.1" class="ltx_text" style="font-size:70%;color:#000000;">03.77</span></td>
<td id="S5.T2.1.7.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.6.1" class="ltx_text" style="font-size:70%;color:#000000;">30.53</span></td>
<td id="S5.T2.1.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.7.1" class="ltx_text" style="font-size:70%;color:#000000;">69.87</span></td>
<td id="S5.T2.1.7.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.8.1" class="ltx_text" style="font-size:70%;">00.45</span></td>
<td id="S5.T2.1.7.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.7.7.9.1" class="ltx_text" style="font-size:70%;color:#000000;">03.76</span></td>
</tr>
<tr id="S5.T2.1.8.8" class="ltx_tr">
<th id="S5.T2.1.8.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.1.1" class="ltx_text" style="font-size:70%;">BoW Q + I</span></th>
<td id="S5.T2.1.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.2.1" class="ltx_text" style="font-size:70%;color:#000000;">52.64</span></td>
<td id="S5.T2.1.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.3.1" class="ltx_text" style="font-size:70%;color:#000000;">75.55</span></td>
<td id="S5.T2.1.8.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.4.1" class="ltx_text" style="font-size:70%;">33.67</span></td>
<td id="S5.T2.1.8.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.5.1" class="ltx_text" style="font-size:70%;color:#000000;">37.37</span></td>
<td id="S5.T2.1.8.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.6.1" class="ltx_text" style="font-size:70%;color:#000000;">58.97</span></td>
<td id="S5.T2.1.8.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.7.1" class="ltx_text" style="font-size:70%;color:#000000;">75.59</span></td>
<td id="S5.T2.1.8.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.8.1" class="ltx_text" style="font-size:70%;">34.35</span></td>
<td id="S5.T2.1.8.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.8.8.9.1" class="ltx_text" style="font-size:70%;color:#000000;">50.33</span></td>
</tr>
<tr id="S5.T2.1.9.9" class="ltx_tr">
<th id="S5.T2.1.9.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.1.1" class="ltx_text" style="font-size:70%;">LSTM Q</span></th>
<td id="S5.T2.1.9.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.2.1" class="ltx_text" style="font-size:70%;color:#000000;">48.76</span></td>
<td id="S5.T2.1.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.3.1" class="ltx_text" style="font-size:70%;color:#000000;">78.20</span></td>
<td id="S5.T2.1.9.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.4.1" class="ltx_text" style="font-size:70%;">35.68</span></td>
<td id="S5.T2.1.9.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.5.1" class="ltx_text" style="font-size:70%;color:#000000;">26.59</span></td>
<td id="S5.T2.1.9.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.6.1" class="ltx_text" style="font-size:70%;color:#000000;">54.75</span></td>
<td id="S5.T2.1.9.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.7.1" class="ltx_text" style="font-size:70%;color:#000000;">78.22</span></td>
<td id="S5.T2.1.9.9.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.8.1" class="ltx_text" style="font-size:70%;">36.82</span></td>
<td id="S5.T2.1.9.9.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.9.9.9.1" class="ltx_text" style="font-size:70%;color:#000000;">38.78</span></td>
</tr>
<tr id="S5.T2.1.10.10" class="ltx_tr">
<th id="S5.T2.1.10.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.1.1" class="ltx_text" style="font-size:70%;">LSTM Q + I</span></th>
<td id="S5.T2.1.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.2.1" class="ltx_text" style="font-size:70%;color:#000000;">53.74</span></td>
<td id="S5.T2.1.10.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.3.1" class="ltx_text" style="font-size:70%;color:#000000;">78.94</span></td>
<td id="S5.T2.1.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.4.1" class="ltx_text" style="font-size:70%;">35.24</span></td>
<td id="S5.T2.1.10.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.5.1" class="ltx_text" style="font-size:70%;color:#000000;">36.42</span></td>
<td id="S5.T2.1.10.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.6.1" class="ltx_text" style="font-size:70%;color:#000000;">57.17</span></td>
<td id="S5.T2.1.10.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.7.1" class="ltx_text" style="font-size:70%;color:#000000;">78.95</span></td>
<td id="S5.T2.1.10.10.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.8.1" class="ltx_text" style="font-size:70%;">35.80</span></td>
<td id="S5.T2.1.10.10.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.10.10.9.1" class="ltx_text" style="font-size:70%;color:#000000;">43.41</span></td>
</tr>
<tr id="S5.T2.1.11.11" class="ltx_tr">
<th id="S5.T2.1.11.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.1.1" class="ltx_text" style="font-size:70%;">deeper LSTM Q</span></th>
<td id="S5.T2.1.11.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.2.1" class="ltx_text" style="font-size:70%;color:#000000;">50.39</span></td>
<td id="S5.T2.1.11.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.3.1" class="ltx_text" style="font-size:70%;color:#000000;">78.41</span></td>
<td id="S5.T2.1.11.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.4.1" class="ltx_text" style="font-size:70%;">34.68</span></td>
<td id="S5.T2.1.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.5.1" class="ltx_text" style="font-size:70%;color:#000000;">30.03</span></td>
<td id="S5.T2.1.11.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.6.1" class="ltx_text" style="font-size:70%;color:#000000;">55.88</span></td>
<td id="S5.T2.1.11.11.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.7.1" class="ltx_text" style="font-size:70%;color:#000000;">78.45</span></td>
<td id="S5.T2.1.11.11.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.8.1" class="ltx_text" style="font-size:70%;">35.91</span></td>
<td id="S5.T2.1.11.11.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.11.11.9.1" class="ltx_text" style="font-size:70%;color:#000000;">41.13</span></td>
</tr>
<tr id="S5.T2.1.12.12" class="ltx_tr">
<th id="S5.T2.1.12.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.1.1" class="ltx_text" style="font-size:70%;">deeper LSTM Q + norm I</span></th>
<td id="S5.T2.1.12.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">57.75</span></td>
<td id="S5.T2.1.12.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">80.50</span></td>
<td id="S5.T2.1.12.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">36.77</span></td>
<td id="S5.T2.1.12.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">43.08</span></td>
<td id="S5.T2.1.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">62.70</span></td>
<td id="S5.T2.1.12.12.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">80.52</span></td>
<td id="S5.T2.1.12.12.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.8.1" class="ltx_text ltx_font_bold" style="font-size:70%;">38.22</span></td>
<td id="S5.T2.1.12.12.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.12.12.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">53.01</span></td>
</tr>
<tr id="S5.T2.1.13.13" class="ltx_tr">
<th id="S5.T2.1.13.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.1.1" class="ltx_text" style="font-size:70%;color:#808080;">Caption</span></th>
<td id="S5.T2.1.13.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.2.1" class="ltx_text" style="font-size:70%;color:#808080;">26.70</span></td>
<td id="S5.T2.1.13.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.3.1" class="ltx_text" style="font-size:70%;color:#808080;">65.50</span></td>
<td id="S5.T2.1.13.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.4.1" class="ltx_text" style="font-size:70%;color:#808080;">02.03</span></td>
<td id="S5.T2.1.13.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.5.1" class="ltx_text" style="font-size:70%;color:#808080;">03.86</span></td>
<td id="S5.T2.1.13.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.6.1" class="ltx_text" style="font-size:70%;color:#808080;">28.29</span></td>
<td id="S5.T2.1.13.13.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.7.1" class="ltx_text" style="font-size:70%;color:#808080;">69.79</span></td>
<td id="S5.T2.1.13.13.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.8.1" class="ltx_text" style="font-size:70%;color:#808080;">02.06</span></td>
<td id="S5.T2.1.13.13.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.13.13.9.1" class="ltx_text" style="font-size:70%;color:#808080;">03.82</span></td>
</tr>
<tr id="S5.T2.1.14.14" class="ltx_tr">
<th id="S5.T2.1.14.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.1.1" class="ltx_text" style="font-size:70%;color:#808080;">BoW Q + C</span></th>
<td id="S5.T2.1.14.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.2.1" class="ltx_text" style="font-size:70%;color:#808080;">54.70</span></td>
<td id="S5.T2.1.14.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.3.1" class="ltx_text" style="font-size:70%;color:#808080;">75.82</span></td>
<td id="S5.T2.1.14.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.4.1" class="ltx_text" style="font-size:70%;color:#808080;">40.12</span></td>
<td id="S5.T2.1.14.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.5.1" class="ltx_text" style="font-size:70%;color:#808080;">42.56</span></td>
<td id="S5.T2.1.14.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.6.1" class="ltx_text" style="font-size:70%;color:#808080;">59.85</span></td>
<td id="S5.T2.1.14.14.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.7.1" class="ltx_text" style="font-size:70%;color:#808080;">75.89</span></td>
<td id="S5.T2.1.14.14.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.8.1" class="ltx_text" style="font-size:70%;color:#808080;">41.16</span></td>
<td id="S5.T2.1.14.14.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T2.1.14.14.9.1" class="ltx_text" style="font-size:70%;color:#808080;">52.53</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Accuracy of our methods for the open-ended and multiple-choice tasks on the VQA test-dev for real images.
Q = Question, I = Image, C = Caption. (Caption and BoW Q + C results are on val).
See text for details.
</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the accuracy of our baselines and methods for both the open-ended and multiple-choice tasks on the VQA test-dev for real images.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">As expected, the vision-alone model (I) that completely ignores the question performs rather poorly (open-ended: 28.13% / multiple-choice: 30.53%). In fact, on open-ended task, the vision-alone model (I) performs worse than the prior (â€œyesâ€) baseline, which ignores both the image <em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">and</em> question (responding to every question with a â€œyesâ€).</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p">Interestingly, the language-alone methods (per Q-type prior, BoW Q, LSTM Q) that ignore the image perform surprisingly well, with BoW Q achieving 48.09% on open-ended (53.68% on multiple-choice) and LSTM Q achieving 48.76% on open-ended (54.75% on multiple-choice); both outperforming the nearest neighbor baseline (open-ended: 42.70%, multiple-choice: 48.49%). Our quantitative results and analyses suggest that this might be due to the language-model exploiting subtle statistical priors about the question types (e.g. â€œWhat color is the banana?â€ can be answered with â€œyellowâ€ without looking at the image). For a detailed discussion of the subtle biases in the questions, please see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p">The accuracy of our <span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_bold">best model</span> (deeper LSTM Q + norm I (Fig.Â <a href="#S5.F8" title="Figure 8 â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>), selected using VQA test-dev accuracies) on VQA test-standard is 58.16% (open-ended) / 63.09% (multiple-choice). We can see that our model is able to significantly outperform both the vision-alone and language-alone baselines. As a general trend, results on multiple-choice are better than open-ended. All methods are significantly worse than human performance.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p">Our VQA demo is available on CloudCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> â€“ <a target="_blank" href="http://cloudcv.org/vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://cloudcv.org/vqa</a>. This will be updated with newer models as we develop them.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para ltx_noindent">
<p id="S5.SS3.p6.1" class="ltx_p">To gain further insights into these results, we computed accuracies by question type in TableÂ <a href="#S5.T3" title="TABLE III â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Interestingly, for question types that require more reasoning, such as â€œIs theâ€ or â€œHow manyâ€, the scene-level image features do not provide any additional information. However, for questions that can be answered using scene-level information, such as â€œWhat sport,â€
we do see an improvement. Similarly, for questions whose answer may be contained in a generic caption we see improvement, such as â€œWhat animalâ€. For all question types, the results are worse than human accuracies.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para ltx_noindent">
<p id="S5.SS3.p7.1" class="ltx_p">We also analyzed the accuracies of our best model (deeper LSTM Q + norm I) on a subset of questions with certain specific (ground truth) answers. In Fig.Â <a href="#S5.F9" title="Figure 9 â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we show the average accuracy of the model on questions with 50 most frequent ground truth answers on the VQA validation set (plot is sorted by accuracy, not frequency). We can see that the model performs well for answers that are common visual objects such as â€œwiiâ€, â€œtennisâ€, â€œbathroomâ€ while the performance is somewhat underwhelming for counts (<em id="S5.SS3.p7.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS3.p7.1.2" class="ltx_text"></span>, â€œ2â€, â€œ1â€, â€œ3â€), and particularly poor for higher counts (<em id="S5.SS3.p7.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS3.p7.1.4" class="ltx_text"></span>, â€œ5â€, â€œ6â€, â€œ10â€, â€œ8â€, â€œ7â€).</p>
</div>
<div id="S5.SS3.p8" class="ltx_para ltx_noindent">
<p id="S5.SS3.p8.1" class="ltx_p">In Fig.Â <a href="#S5.F10" title="Figure 10 â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we show the distribution of 50 most frequently predicted answers when the system is correct on the VQA validation set (plot is sorted by prediction frequency, not accuracy). In this analysis, â€œsystem is correctâ€ implies that it has VQA accuracy <math id="S5.SS3.p8.1.m1.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S5.SS3.p8.1.m1.1a"><mn id="S5.SS3.p8.1.m1.1.1" xref="S5.SS3.p8.1.m1.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p8.1.m1.1b"><cn type="float" id="S5.SS3.p8.1.m1.1.1.cmml" xref="S5.SS3.p8.1.m1.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p8.1.m1.1c">1.0</annotation></semantics></math> (see section <a href="#S3" title="3 VQA Dataset Collection â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for accuracy metric). We can see that the frequent ground truth answers (<em id="S5.SS3.p8.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS3.p8.1.2" class="ltx_text"></span>, â€œyesâ€, â€œnoâ€, â€œ2â€, â€œwhiteâ€, â€œredâ€, â€œblueâ€, â€œ1â€, â€œgreenâ€) are more frequently predicted than others when the model is correct.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="5"><span id="S5.T3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Open-Ended</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Human Age</span></th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">Commonsense</span></th>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<th id="S5.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.2.2.1.1" class="ltx_text" style="font-size:70%;">Question</span></th>
<th id="S5.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="3"><span id="S5.T3.1.2.2.2.1" class="ltx_text" style="font-size:70%;">K = 1000</span></th>
<th id="S5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="2"><span id="S5.T3.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Human</span></th>
<th id="S5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.2.2.4.1" class="ltx_text" style="font-size:70%;">To Be Able</span></th>
<th id="S5.T3.1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.2.2.5.1" class="ltx_text" style="font-size:70%;">To Be Able</span></th>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<th id="S5.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.1.1" class="ltx_text" style="font-size:70%;">Type</span></th>
<th id="S5.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.2.1" class="ltx_text" style="font-size:70%;">Q</span></th>
<th id="S5.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.3.1" class="ltx_text" style="font-size:70%;">Q + I</span></th>
<th id="S5.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.4.1" class="ltx_text" style="font-size:70%;">Q + C</span></th>
<th id="S5.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.5.1" class="ltx_text" style="font-size:70%;">Q</span></th>
<th id="S5.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.6.1" class="ltx_text" style="font-size:70%;">Q + I</span></th>
<th id="S5.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.7.1" class="ltx_text" style="font-size:70%;">To Answer</span></th>
<th id="S5.T3.1.3.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.3.3.8.1" class="ltx_text" style="font-size:70%;">To Answer (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.4.1" class="ltx_tr">
<th id="S5.T3.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.4.1.1.1" class="ltx_text" style="font-size:70%;">what is </span><span id="S5.T3.1.4.1.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(13.84)</span>
</th>
<td id="S5.T3.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.2.1" class="ltx_text" style="font-size:70%;color:#000000;">23.57</span></td>
<td id="S5.T3.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.3.1" class="ltx_text" style="font-size:70%;color:#000000;">34.28</span></td>
<td id="S5.T3.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.4.1" class="ltx_text" style="font-size:70%;color:#808080;">43.88</span></td>
<td id="S5.T3.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.5.1" class="ltx_text" style="font-size:70%;color:#000000;">16.86</span></td>
<td id="S5.T3.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.6.1" class="ltx_text" style="font-size:70%;color:#000000;">73.68</span></td>
<td id="S5.T3.1.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.07</span></td>
<td id="S5.T3.1.4.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.4.1.8.1" class="ltx_text" style="font-size:70%;">27.52</span></td>
</tr>
<tr id="S5.T3.1.5.2" class="ltx_tr">
<th id="S5.T3.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.5.2.1.1" class="ltx_text" style="font-size:70%;">what color </span><span id="S5.T3.1.5.2.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(08.98)</span>
</th>
<td id="S5.T3.1.5.2.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.2.1" class="ltx_text" style="font-size:70%;color:#000000;">33.37</span></td>
<td id="S5.T3.1.5.2.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.3.1" class="ltx_text" style="font-size:70%;color:#000000;">43.53</span></td>
<td id="S5.T3.1.5.2.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.4.1" class="ltx_text" style="font-size:70%;color:#808080;">48.61</span></td>
<td id="S5.T3.1.5.2.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.5.1" class="ltx_text" style="font-size:70%;color:#000000;">28.71</span></td>
<td id="S5.T3.1.5.2.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.6.1" class="ltx_text" style="font-size:70%;color:#000000;">86.06</span></td>
<td id="S5.T3.1.5.2.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.7.1" class="ltx_text" style="font-size:70%;color:#000000;">06.60</span></td>
<td id="S5.T3.1.5.2.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.5.2.8.1" class="ltx_text" style="font-size:70%;">13.22</span></td>
</tr>
<tr id="S5.T3.1.6.3" class="ltx_tr">
<th id="S5.T3.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.6.3.1.1" class="ltx_text" style="font-size:70%;">what kind </span><span id="S5.T3.1.6.3.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(02.49)</span>
</th>
<td id="S5.T3.1.6.3.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.2.1" class="ltx_text" style="font-size:70%;color:#000000;">27.78</span></td>
<td id="S5.T3.1.6.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.3.1" class="ltx_text" style="font-size:70%;color:#000000;">42.72</span></td>
<td id="S5.T3.1.6.3.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.4.1" class="ltx_text" style="font-size:70%;color:#808080;">43.88</span></td>
<td id="S5.T3.1.6.3.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.5.1" class="ltx_text" style="font-size:70%;color:#000000;">19.10</span></td>
<td id="S5.T3.1.6.3.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.6.1" class="ltx_text" style="font-size:70%;color:#000000;">70.11</span></td>
<td id="S5.T3.1.6.3.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.7.1" class="ltx_text" style="font-size:70%;color:#000000;">10.55</span></td>
<td id="S5.T3.1.6.3.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.6.3.8.1" class="ltx_text" style="font-size:70%;">40.34</span></td>
</tr>
<tr id="S5.T3.1.7.4" class="ltx_tr">
<th id="S5.T3.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.7.4.1.1" class="ltx_text" style="font-size:70%;">what are </span><span id="S5.T3.1.7.4.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(02.32)</span>
</th>
<td id="S5.T3.1.7.4.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.2.1" class="ltx_text" style="font-size:70%;color:#000000;">25.47</span></td>
<td id="S5.T3.1.7.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.3.1" class="ltx_text" style="font-size:70%;color:#000000;">39.10</span></td>
<td id="S5.T3.1.7.4.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.4.1" class="ltx_text" style="font-size:70%;color:#808080;">47.27</span></td>
<td id="S5.T3.1.7.4.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.5.1" class="ltx_text" style="font-size:70%;color:#000000;">17.72</span></td>
<td id="S5.T3.1.7.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.6.1" class="ltx_text" style="font-size:70%;color:#000000;">69.49</span></td>
<td id="S5.T3.1.7.4.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.03</span></td>
<td id="S5.T3.1.7.4.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.7.4.8.1" class="ltx_text" style="font-size:70%;">28.72</span></td>
</tr>
<tr id="S5.T3.1.8.5" class="ltx_tr">
<th id="S5.T3.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.8.5.1.1" class="ltx_text" style="font-size:70%;">what type </span><span id="S5.T3.1.8.5.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(01.78)</span>
</th>
<td id="S5.T3.1.8.5.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.2.1" class="ltx_text" style="font-size:70%;color:#000000;">27.68</span></td>
<td id="S5.T3.1.8.5.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.3.1" class="ltx_text" style="font-size:70%;color:#000000;">42.62</span></td>
<td id="S5.T3.1.8.5.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.4.1" class="ltx_text" style="font-size:70%;color:#808080;">44.32</span></td>
<td id="S5.T3.1.8.5.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.5.1" class="ltx_text" style="font-size:70%;color:#000000;">19.53</span></td>
<td id="S5.T3.1.8.5.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.6.1" class="ltx_text" style="font-size:70%;color:#000000;">70.65</span></td>
<td id="S5.T3.1.8.5.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.7.1" class="ltx_text" style="font-size:70%;color:#000000;">11.04</span></td>
<td id="S5.T3.1.8.5.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.8.5.8.1" class="ltx_text" style="font-size:70%;">38.92</span></td>
</tr>
<tr id="S5.T3.1.9.6" class="ltx_tr">
<th id="S5.T3.1.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.9.6.1.1" class="ltx_text" style="font-size:70%;">is the </span><span id="S5.T3.1.9.6.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(10.16)</span>
</th>
<td id="S5.T3.1.9.6.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.2.1" class="ltx_text" style="font-size:70%;color:#000000;">70.76</span></td>
<td id="S5.T3.1.9.6.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.3.1" class="ltx_text" style="font-size:70%;color:#000000;">69.87</span></td>
<td id="S5.T3.1.9.6.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.4.1" class="ltx_text" style="font-size:70%;color:#808080;">70.50</span></td>
<td id="S5.T3.1.9.6.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.5.1" class="ltx_text" style="font-size:70%;color:#000000;">65.24</span></td>
<td id="S5.T3.1.9.6.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.6.1" class="ltx_text" style="font-size:70%;color:#000000;">95.67</span></td>
<td id="S5.T3.1.9.6.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.7.1" class="ltx_text" style="font-size:70%;color:#000000;">08.51</span></td>
<td id="S5.T3.1.9.6.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.9.6.8.1" class="ltx_text" style="font-size:70%;">30.30</span></td>
</tr>
<tr id="S5.T3.1.10.7" class="ltx_tr">
<th id="S5.T3.1.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.10.7.1.1" class="ltx_text" style="font-size:70%;">is this </span><span id="S5.T3.1.10.7.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(08.26)</span>
</th>
<td id="S5.T3.1.10.7.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.2.1" class="ltx_text" style="font-size:70%;color:#000000;">70.34</span></td>
<td id="S5.T3.1.10.7.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.3.1" class="ltx_text" style="font-size:70%;color:#000000;">70.79</span></td>
<td id="S5.T3.1.10.7.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.4.1" class="ltx_text" style="font-size:70%;color:#808080;">71.54</span></td>
<td id="S5.T3.1.10.7.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.5.1" class="ltx_text" style="font-size:70%;color:#000000;">63.35</span></td>
<td id="S5.T3.1.10.7.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.6.1" class="ltx_text" style="font-size:70%;color:#000000;">95.43</span></td>
<td id="S5.T3.1.10.7.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.7.1" class="ltx_text" style="font-size:70%;color:#000000;">10.13</span></td>
<td id="S5.T3.1.10.7.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.10.7.8.1" class="ltx_text" style="font-size:70%;">45.32</span></td>
</tr>
<tr id="S5.T3.1.11.8" class="ltx_tr">
<th id="S5.T3.1.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.11.8.1.1" class="ltx_text" style="font-size:70%;">how many </span><span id="S5.T3.1.11.8.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(10.28)</span>
</th>
<td id="S5.T3.1.11.8.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.2.1" class="ltx_text" style="font-size:70%;color:#000000;">43.78</span></td>
<td id="S5.T3.1.11.8.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.3.1" class="ltx_text" style="font-size:70%;color:#000000;">40.33</span></td>
<td id="S5.T3.1.11.8.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.4.1" class="ltx_text" style="font-size:70%;color:#808080;">47.52</span></td>
<td id="S5.T3.1.11.8.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.5.1" class="ltx_text" style="font-size:70%;color:#000000;">30.45</span></td>
<td id="S5.T3.1.11.8.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.6.1" class="ltx_text" style="font-size:70%;color:#000000;">86.32</span></td>
<td id="S5.T3.1.11.8.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.7.1" class="ltx_text" style="font-size:70%;color:#000000;">07.67</span></td>
<td id="S5.T3.1.11.8.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.11.8.8.1" class="ltx_text" style="font-size:70%;">15.93</span></td>
</tr>
<tr id="S5.T3.1.12.9" class="ltx_tr">
<th id="S5.T3.1.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.12.9.1.1" class="ltx_text" style="font-size:70%;">are </span><span id="S5.T3.1.12.9.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(07.57)</span>
</th>
<td id="S5.T3.1.12.9.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.2.1" class="ltx_text" style="font-size:70%;color:#000000;">73.96</span></td>
<td id="S5.T3.1.12.9.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.3.1" class="ltx_text" style="font-size:70%;color:#000000;">73.58</span></td>
<td id="S5.T3.1.12.9.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.4.1" class="ltx_text" style="font-size:70%;color:#808080;">72.43</span></td>
<td id="S5.T3.1.12.9.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.5.1" class="ltx_text" style="font-size:70%;color:#000000;">67.10</span></td>
<td id="S5.T3.1.12.9.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.6.1" class="ltx_text" style="font-size:70%;color:#000000;">95.24</span></td>
<td id="S5.T3.1.12.9.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.7.1" class="ltx_text" style="font-size:70%;color:#000000;">08.65</span></td>
<td id="S5.T3.1.12.9.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.12.9.8.1" class="ltx_text" style="font-size:70%;">30.63</span></td>
</tr>
<tr id="S5.T3.1.13.10" class="ltx_tr">
<th id="S5.T3.1.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.13.10.1.1" class="ltx_text" style="font-size:70%;">does </span><span id="S5.T3.1.13.10.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(02.75)</span>
</th>
<td id="S5.T3.1.13.10.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.2.1" class="ltx_text" style="font-size:70%;color:#000000;">76.81</span></td>
<td id="S5.T3.1.13.10.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.3.1" class="ltx_text" style="font-size:70%;color:#000000;">75.81</span></td>
<td id="S5.T3.1.13.10.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.4.1" class="ltx_text" style="font-size:70%;color:#808080;">75.88</span></td>
<td id="S5.T3.1.13.10.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.5.1" class="ltx_text" style="font-size:70%;color:#000000;">69.96</span></td>
<td id="S5.T3.1.13.10.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.6.1" class="ltx_text" style="font-size:70%;color:#000000;">95.70</span></td>
<td id="S5.T3.1.13.10.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.29</span></td>
<td id="S5.T3.1.13.10.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.13.10.8.1" class="ltx_text" style="font-size:70%;">38.97</span></td>
</tr>
<tr id="S5.T3.1.14.11" class="ltx_tr">
<th id="S5.T3.1.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.14.11.1.1" class="ltx_text" style="font-size:70%;">where </span><span id="S5.T3.1.14.11.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(02.90)</span>
</th>
<td id="S5.T3.1.14.11.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.2.1" class="ltx_text" style="font-size:70%;color:#000000;">16.21</span></td>
<td id="S5.T3.1.14.11.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.3.1" class="ltx_text" style="font-size:70%;color:#000000;">23.49</span></td>
<td id="S5.T3.1.14.11.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.4.1" class="ltx_text" style="font-size:70%;color:#808080;">29.47</span></td>
<td id="S5.T3.1.14.11.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.5.1" class="ltx_text" style="font-size:70%;color:#000000;">11.09</span></td>
<td id="S5.T3.1.14.11.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.6.1" class="ltx_text" style="font-size:70%;color:#000000;">43.56</span></td>
<td id="S5.T3.1.14.11.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.54</span></td>
<td id="S5.T3.1.14.11.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.14.11.8.1" class="ltx_text" style="font-size:70%;">36.51</span></td>
</tr>
<tr id="S5.T3.1.15.12" class="ltx_tr">
<th id="S5.T3.1.15.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.15.12.1.1" class="ltx_text" style="font-size:70%;">is there </span><span id="S5.T3.1.15.12.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(03.60)</span>
</th>
<td id="S5.T3.1.15.12.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.2.1" class="ltx_text" style="font-size:70%;color:#000000;">86.50</span></td>
<td id="S5.T3.1.15.12.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.3.1" class="ltx_text" style="font-size:70%;color:#000000;">86.37</span></td>
<td id="S5.T3.1.15.12.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.4.1" class="ltx_text" style="font-size:70%;color:#808080;">85.88</span></td>
<td id="S5.T3.1.15.12.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.5.1" class="ltx_text" style="font-size:70%;color:#000000;">72.48</span></td>
<td id="S5.T3.1.15.12.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.6.1" class="ltx_text" style="font-size:70%;color:#000000;">96.43</span></td>
<td id="S5.T3.1.15.12.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.7.1" class="ltx_text" style="font-size:70%;color:#000000;">08.25</span></td>
<td id="S5.T3.1.15.12.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.15.12.8.1" class="ltx_text" style="font-size:70%;">19.88</span></td>
</tr>
<tr id="S5.T3.1.16.13" class="ltx_tr">
<th id="S5.T3.1.16.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.16.13.1.1" class="ltx_text" style="font-size:70%;">why </span><span id="S5.T3.1.16.13.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(01.20)</span>
</th>
<td id="S5.T3.1.16.13.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.2.1" class="ltx_text" style="font-size:70%;color:#000000;">16.24</span></td>
<td id="S5.T3.1.16.13.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.3.1" class="ltx_text" style="font-size:70%;color:#000000;">13.94</span></td>
<td id="S5.T3.1.16.13.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.4.1" class="ltx_text" style="font-size:70%;color:#808080;">14.54</span></td>
<td id="S5.T3.1.16.13.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.5.1" class="ltx_text" style="font-size:70%;color:#000000;">11.80</span></td>
<td id="S5.T3.1.16.13.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.6.1" class="ltx_text" style="font-size:70%;color:#000000;">21.50</span></td>
<td id="S5.T3.1.16.13.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.7.1" class="ltx_text" style="font-size:70%;color:#000000;">11.18</span></td>
<td id="S5.T3.1.16.13.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.16.13.8.1" class="ltx_text" style="font-size:70%;">73.56</span></td>
</tr>
<tr id="S5.T3.1.17.14" class="ltx_tr">
<th id="S5.T3.1.17.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.17.14.1.1" class="ltx_text" style="font-size:70%;">which </span><span id="S5.T3.1.17.14.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(01.21)</span>
</th>
<td id="S5.T3.1.17.14.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.2.1" class="ltx_text" style="font-size:70%;color:#000000;">29.50</span></td>
<td id="S5.T3.1.17.14.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.3.1" class="ltx_text" style="font-size:70%;color:#000000;">34.83</span></td>
<td id="S5.T3.1.17.14.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.4.1" class="ltx_text" style="font-size:70%;color:#808080;">40.84</span></td>
<td id="S5.T3.1.17.14.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.5.1" class="ltx_text" style="font-size:70%;color:#000000;">25.64</span></td>
<td id="S5.T3.1.17.14.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.6.1" class="ltx_text" style="font-size:70%;color:#000000;">67.44</span></td>
<td id="S5.T3.1.17.14.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.27</span></td>
<td id="S5.T3.1.17.14.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.17.14.8.1" class="ltx_text" style="font-size:70%;">30.00</span></td>
</tr>
<tr id="S5.T3.1.18.15" class="ltx_tr">
<th id="S5.T3.1.18.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.18.15.1.1" class="ltx_text" style="font-size:70%;">do </span><span id="S5.T3.1.18.15.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(01.15)</span>
</th>
<td id="S5.T3.1.18.15.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.2.1" class="ltx_text" style="font-size:70%;color:#000000;">77.73</span></td>
<td id="S5.T3.1.18.15.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.3.1" class="ltx_text" style="font-size:70%;color:#000000;">79.31</span></td>
<td id="S5.T3.1.18.15.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.4.1" class="ltx_text" style="font-size:70%;color:#808080;">74.63</span></td>
<td id="S5.T3.1.18.15.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.5.1" class="ltx_text" style="font-size:70%;color:#000000;">71.33</span></td>
<td id="S5.T3.1.18.15.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.6.1" class="ltx_text" style="font-size:70%;color:#000000;">95.44</span></td>
<td id="S5.T3.1.18.15.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.23</span></td>
<td id="S5.T3.1.18.15.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.18.15.8.1" class="ltx_text" style="font-size:70%;">37.68</span></td>
</tr>
<tr id="S5.T3.1.19.16" class="ltx_tr">
<th id="S5.T3.1.19.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.19.16.1.1" class="ltx_text" style="font-size:70%;">what does </span><span id="S5.T3.1.19.16.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(01.12)</span>
</th>
<td id="S5.T3.1.19.16.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.2.1" class="ltx_text" style="font-size:70%;color:#000000;">19.58</span></td>
<td id="S5.T3.1.19.16.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.3.1" class="ltx_text" style="font-size:70%;color:#000000;">20.00</span></td>
<td id="S5.T3.1.19.16.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.4.1" class="ltx_text" style="font-size:70%;color:#808080;">23.19</span></td>
<td id="S5.T3.1.19.16.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.5.1" class="ltx_text" style="font-size:70%;color:#000000;">11.12</span></td>
<td id="S5.T3.1.19.16.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.6.1" class="ltx_text" style="font-size:70%;color:#000000;">75.88</span></td>
<td id="S5.T3.1.19.16.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.7.1" class="ltx_text" style="font-size:70%;color:#000000;">10.02</span></td>
<td id="S5.T3.1.19.16.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.19.16.8.1" class="ltx_text" style="font-size:70%;">33.27</span></td>
</tr>
<tr id="S5.T3.1.20.17" class="ltx_tr">
<th id="S5.T3.1.20.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.20.17.1.1" class="ltx_text" style="font-size:70%;">what time </span><span id="S5.T3.1.20.17.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(00.67)</span>
</th>
<td id="S5.T3.1.20.17.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.2.1" class="ltx_text" style="font-size:70%;color:#000000;">8.35</span></td>
<td id="S5.T3.1.20.17.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.3.1" class="ltx_text" style="font-size:70%;color:#000000;">14.00</span></td>
<td id="S5.T3.1.20.17.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.4.1" class="ltx_text" style="font-size:70%;color:#808080;">18.28</span></td>
<td id="S5.T3.1.20.17.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.5.1" class="ltx_text" style="font-size:70%;color:#000000;">07.64</span></td>
<td id="S5.T3.1.20.17.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.6.1" class="ltx_text" style="font-size:70%;color:#000000;">58.98</span></td>
<td id="S5.T3.1.20.17.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.81</span></td>
<td id="S5.T3.1.20.17.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.20.17.8.1" class="ltx_text" style="font-size:70%;">31.83</span></td>
</tr>
<tr id="S5.T3.1.21.18" class="ltx_tr">
<th id="S5.T3.1.21.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.21.18.1.1" class="ltx_text" style="font-size:70%;">who </span><span id="S5.T3.1.21.18.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(00.77)</span>
</th>
<td id="S5.T3.1.21.18.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.2.1" class="ltx_text" style="font-size:70%;color:#000000;">19.75</span></td>
<td id="S5.T3.1.21.18.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.3.1" class="ltx_text" style="font-size:70%;color:#000000;">20.43</span></td>
<td id="S5.T3.1.21.18.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.4.1" class="ltx_text" style="font-size:70%;color:#808080;">27.28</span></td>
<td id="S5.T3.1.21.18.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.5.1" class="ltx_text" style="font-size:70%;color:#000000;">14.69</span></td>
<td id="S5.T3.1.21.18.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.6.1" class="ltx_text" style="font-size:70%;color:#000000;">56.93</span></td>
<td id="S5.T3.1.21.18.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.7.1" class="ltx_text" style="font-size:70%;color:#000000;">09.49</span></td>
<td id="S5.T3.1.21.18.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.21.18.8.1" class="ltx_text" style="font-size:70%;">43.82</span></td>
</tr>
<tr id="S5.T3.1.22.19" class="ltx_tr">
<th id="S5.T3.1.22.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.22.19.1.1" class="ltx_text" style="font-size:70%;">what sport </span><span id="S5.T3.1.22.19.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(00.81)</span>
</th>
<td id="S5.T3.1.22.19.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.2.1" class="ltx_text" style="font-size:70%;color:#000000;">37.96</span></td>
<td id="S5.T3.1.22.19.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.3.1" class="ltx_text" style="font-size:70%;color:#000000;">81.12</span></td>
<td id="S5.T3.1.22.19.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.4.1" class="ltx_text" style="font-size:70%;color:#808080;">93.87</span></td>
<td id="S5.T3.1.22.19.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.5.1" class="ltx_text" style="font-size:70%;color:#000000;">17.86</span></td>
<td id="S5.T3.1.22.19.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.6.1" class="ltx_text" style="font-size:70%;color:#000000;">95.59</span></td>
<td id="S5.T3.1.22.19.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.7.1" class="ltx_text" style="font-size:70%;color:#000000;">08.07</span></td>
<td id="S5.T3.1.22.19.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.22.19.8.1" class="ltx_text" style="font-size:70%;">31.87</span></td>
</tr>
<tr id="S5.T3.1.23.20" class="ltx_tr">
<th id="S5.T3.1.23.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.23.20.1.1" class="ltx_text" style="font-size:70%;">what animal </span><span id="S5.T3.1.23.20.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(00.53)</span>
</th>
<td id="S5.T3.1.23.20.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.2.1" class="ltx_text" style="font-size:70%;color:#000000;">23.12</span></td>
<td id="S5.T3.1.23.20.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.3.1" class="ltx_text" style="font-size:70%;color:#000000;">59.70</span></td>
<td id="S5.T3.1.23.20.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.4.1" class="ltx_text" style="font-size:70%;color:#808080;">71.02</span></td>
<td id="S5.T3.1.23.20.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.5.1" class="ltx_text" style="font-size:70%;color:#000000;">17.67</span></td>
<td id="S5.T3.1.23.20.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.6.1" class="ltx_text" style="font-size:70%;color:#000000;">92.51</span></td>
<td id="S5.T3.1.23.20.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.7.1" class="ltx_text" style="font-size:70%;color:#000000;">06.75</span></td>
<td id="S5.T3.1.23.20.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.23.20.8.1" class="ltx_text" style="font-size:70%;">18.04</span></td>
</tr>
<tr id="S5.T3.1.24.21" class="ltx_tr">
<th id="S5.T3.1.24.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T3.1.24.21.1.1" class="ltx_text" style="font-size:70%;">what brand </span><span id="S5.T3.1.24.21.1.2" class="ltx_text" style="font-size:70%;color:#000000;">(00.36)</span>
</th>
<td id="S5.T3.1.24.21.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.2.1" class="ltx_text" style="font-size:70%;color:#000000;">40.13</span></td>
<td id="S5.T3.1.24.21.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.3.1" class="ltx_text" style="font-size:70%;color:#000000;">36.84</span></td>
<td id="S5.T3.1.24.21.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.4.1" class="ltx_text" style="font-size:70%;color:#808080;">32.19</span></td>
<td id="S5.T3.1.24.21.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.5.1" class="ltx_text" style="font-size:70%;color:#000000;">25.34</span></td>
<td id="S5.T3.1.24.21.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.6.1" class="ltx_text" style="font-size:70%;color:#000000;">80.95</span></td>
<td id="S5.T3.1.24.21.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.7.1" class="ltx_text" style="font-size:70%;color:#000000;">12.50</span></td>
<td id="S5.T3.1.24.21.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T3.1.24.21.8.1" class="ltx_text" style="font-size:70%;">41.33</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Open-ended test-dev results for different question types on real images (Q+C is reported on val).
Machine performance is reported using the bag-of-words representation for questions.
Questions types are determined by the one or two words that start the question.
The percentage of questions for each type is shown in parentheses.
Last and second last columns respectively show the average human age and average degree of commonsense required to answer the questions
(as reported by AMT workers), respectively.
See text for details.</figcaption>
</figure>
<div id="S5.SS3.p9" class="ltx_para ltx_noindent">
<p id="S5.SS3.p9.4" class="ltx_p">Finally, evaluating our best model (deeper LSTM Q + norm I) on the validation questions for which we have age annotations (how old a human needs to be to answer the question correctly), we estimate that our model performs as well as a <math id="S5.SS3.p9.1.m1.1" class="ltx_Math" alttext="4.74" display="inline"><semantics id="S5.SS3.p9.1.m1.1a"><mn id="S5.SS3.p9.1.m1.1.1" xref="S5.SS3.p9.1.m1.1.1.cmml">4.74</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p9.1.m1.1b"><cn type="float" id="S5.SS3.p9.1.m1.1.1.cmml" xref="S5.SS3.p9.1.m1.1.1">4.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p9.1.m1.1c">4.74</annotation></semantics></math> year old child! The average age required on the same set of questions is <math id="S5.SS3.p9.2.m2.1" class="ltx_Math" alttext="8.98" display="inline"><semantics id="S5.SS3.p9.2.m2.1a"><mn id="S5.SS3.p9.2.m2.1.1" xref="S5.SS3.p9.2.m2.1.1.cmml">8.98</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p9.2.m2.1b"><cn type="float" id="S5.SS3.p9.2.m2.1.1.cmml" xref="S5.SS3.p9.2.m2.1.1">8.98</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p9.2.m2.1c">8.98</annotation></semantics></math>. Evaluating the same model on the validation questions for which we have commonsense annotations (whether the question requires commonsense to answer it), we estimate that it has degree of commonsense of <math id="S5.SS3.p9.3.m3.1" class="ltx_Math" alttext="17.35\%" display="inline"><semantics id="S5.SS3.p9.3.m3.1a"><mrow id="S5.SS3.p9.3.m3.1.1" xref="S5.SS3.p9.3.m3.1.1.cmml"><mn id="S5.SS3.p9.3.m3.1.1.2" xref="S5.SS3.p9.3.m3.1.1.2.cmml">17.35</mn><mo id="S5.SS3.p9.3.m3.1.1.1" xref="S5.SS3.p9.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p9.3.m3.1b"><apply id="S5.SS3.p9.3.m3.1.1.cmml" xref="S5.SS3.p9.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p9.3.m3.1.1.1.cmml" xref="S5.SS3.p9.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p9.3.m3.1.1.2.cmml" xref="S5.SS3.p9.3.m3.1.1.2">17.35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p9.3.m3.1c">17.35\%</annotation></semantics></math>. The average degree of commonsense required on same set of questions is <math id="S5.SS3.p9.4.m4.1" class="ltx_Math" alttext="31.23\%" display="inline"><semantics id="S5.SS3.p9.4.m4.1a"><mrow id="S5.SS3.p9.4.m4.1.1" xref="S5.SS3.p9.4.m4.1.1.cmml"><mn id="S5.SS3.p9.4.m4.1.1.2" xref="S5.SS3.p9.4.m4.1.1.2.cmml">31.23</mn><mo id="S5.SS3.p9.4.m4.1.1.1" xref="S5.SS3.p9.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p9.4.m4.1b"><apply id="S5.SS3.p9.4.m4.1.1.cmml" xref="S5.SS3.p9.4.m4.1.1"><csymbol cd="latexml" id="S5.SS3.p9.4.m4.1.1.1.cmml" xref="S5.SS3.p9.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p9.4.m4.1.1.2.cmml" xref="S5.SS3.p9.4.m4.1.1.2">31.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p9.4.m4.1c">31.23\%</annotation></semantics></math>. Again, these estimates reflect the age and commonsense perceived by MTurk workers that would be required to answer the question. See the appendix for details.</p>
</div>
<div id="S5.SS3.p10" class="ltx_para ltx_noindent">
<p id="S5.SS3.p10.4" class="ltx_p">We further analyzed the performance of the model for different age groups on the validation questions for which we have age annotations. In Fig.Â <a href="#S5.F11" title="Figure 11 â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, we computed the average accuracy of the predictions made by the model for questions belonging to different age groups. Perhaps as expected, the accuracy of the model decreases as the age of the question increases (from <math id="S5.SS3.p10.1.m1.1" class="ltx_Math" alttext="61.07\%" display="inline"><semantics id="S5.SS3.p10.1.m1.1a"><mrow id="S5.SS3.p10.1.m1.1.1" xref="S5.SS3.p10.1.m1.1.1.cmml"><mn id="S5.SS3.p10.1.m1.1.1.2" xref="S5.SS3.p10.1.m1.1.1.2.cmml">61.07</mn><mo id="S5.SS3.p10.1.m1.1.1.1" xref="S5.SS3.p10.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p10.1.m1.1b"><apply id="S5.SS3.p10.1.m1.1.1.cmml" xref="S5.SS3.p10.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p10.1.m1.1.1.1.cmml" xref="S5.SS3.p10.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p10.1.m1.1.1.2.cmml" xref="S5.SS3.p10.1.m1.1.1.2">61.07</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p10.1.m1.1c">61.07\%</annotation></semantics></math> at <math id="S5.SS3.p10.2.m2.1" class="ltx_Math" alttext="3-4" display="inline"><semantics id="S5.SS3.p10.2.m2.1a"><mrow id="S5.SS3.p10.2.m2.1.1" xref="S5.SS3.p10.2.m2.1.1.cmml"><mn id="S5.SS3.p10.2.m2.1.1.2" xref="S5.SS3.p10.2.m2.1.1.2.cmml">3</mn><mo id="S5.SS3.p10.2.m2.1.1.1" xref="S5.SS3.p10.2.m2.1.1.1.cmml">âˆ’</mo><mn id="S5.SS3.p10.2.m2.1.1.3" xref="S5.SS3.p10.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p10.2.m2.1b"><apply id="S5.SS3.p10.2.m2.1.1.cmml" xref="S5.SS3.p10.2.m2.1.1"><minus id="S5.SS3.p10.2.m2.1.1.1.cmml" xref="S5.SS3.p10.2.m2.1.1.1"></minus><cn type="integer" id="S5.SS3.p10.2.m2.1.1.2.cmml" xref="S5.SS3.p10.2.m2.1.1.2">3</cn><cn type="integer" id="S5.SS3.p10.2.m2.1.1.3.cmml" xref="S5.SS3.p10.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p10.2.m2.1c">3-4</annotation></semantics></math> age group to <math id="S5.SS3.p10.3.m3.1" class="ltx_Math" alttext="47.83\%" display="inline"><semantics id="S5.SS3.p10.3.m3.1a"><mrow id="S5.SS3.p10.3.m3.1.1" xref="S5.SS3.p10.3.m3.1.1.cmml"><mn id="S5.SS3.p10.3.m3.1.1.2" xref="S5.SS3.p10.3.m3.1.1.2.cmml">47.83</mn><mo id="S5.SS3.p10.3.m3.1.1.1" xref="S5.SS3.p10.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p10.3.m3.1b"><apply id="S5.SS3.p10.3.m3.1.1.cmml" xref="S5.SS3.p10.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p10.3.m3.1.1.1.cmml" xref="S5.SS3.p10.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p10.3.m3.1.1.2.cmml" xref="S5.SS3.p10.3.m3.1.1.2">47.83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p10.3.m3.1c">47.83\%</annotation></semantics></math> at <math id="S5.SS3.p10.4.m4.1" class="ltx_Math" alttext="18+" display="inline"><semantics id="S5.SS3.p10.4.m4.1a"><mrow id="S5.SS3.p10.4.m4.1.1" xref="S5.SS3.p10.4.m4.1.1.cmml"><mn id="S5.SS3.p10.4.m4.1.1.2" xref="S5.SS3.p10.4.m4.1.1.2.cmml">18</mn><mo id="S5.SS3.p10.4.m4.1.1.3" xref="S5.SS3.p10.4.m4.1.1.3.cmml">+</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p10.4.m4.1b"><apply id="S5.SS3.p10.4.m4.1.1.cmml" xref="S5.SS3.p10.4.m4.1.1"><csymbol cd="latexml" id="S5.SS3.p10.4.m4.1.1.1.cmml" xref="S5.SS3.p10.4.m4.1.1">limit-from</csymbol><cn type="integer" id="S5.SS3.p10.4.m4.1.1.2.cmml" xref="S5.SS3.p10.4.m4.1.1.2">18</cn><plus id="S5.SS3.p10.4.m4.1.1.3.cmml" xref="S5.SS3.p10.4.m4.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p10.4.m4.1c">18+</annotation></semantics></math> age group).</p>
</div>
<div id="S5.SS3.p11" class="ltx_para ltx_noindent">
<p id="S5.SS3.p11.1" class="ltx_p">In Fig.Â <a href="#S5.F12" title="Figure 12 â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, we show the distribution of age of questions for different levels of accuracies achieved by our system on the validation questions for which we have age annotations. It is interesting to see that the relative proportions of different age groups is consistent across all accuracy bins with questions belonging to the age group 5-8 comprising the majority of the predictions which is expected because 5-8 is the most common age group in the dataset (see Fig.Â <a href="#S4.F7" title="Figure 7 â€£ 4.3 Commonsense Knowledge â€£ 4 VQA Dataset Analysis â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/1505.00468/assets/x9.png" id="S5.F9.g1" class="ltx_graphics ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><math id="S5.F9.2.m1.1" class="ltx_Math" alttext="\Pr\text{(system is correct }|\text{ answer)}" display="inline"><semantics id="S5.F9.2.m1.1b"><mrow id="S5.F9.2.m1.1.1" xref="S5.F9.2.m1.1.1.cmml"><mrow id="S5.F9.2.m1.1.1.2" xref="S5.F9.2.m1.1.1.2.cmml"><mi id="S5.F9.2.m1.1.1.2.1" xref="S5.F9.2.m1.1.1.2.1.cmml">Pr</mi><mo lspace="0.167em" id="S5.F9.2.m1.1.1.2b" xref="S5.F9.2.m1.1.1.2.cmml">â¡</mo><mtext id="S5.F9.2.m1.1.1.2.2" xref="S5.F9.2.m1.1.1.2.2a.cmml">(system is correctÂ </mtext></mrow><mo fence="false" id="S5.F9.2.m1.1.1.1" xref="S5.F9.2.m1.1.1.1.cmml">|</mo><mtext id="S5.F9.2.m1.1.1.3" xref="S5.F9.2.m1.1.1.3a.cmml">Â answer)</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.F9.2.m1.1c"><apply id="S5.F9.2.m1.1.1.cmml" xref="S5.F9.2.m1.1.1"><csymbol cd="latexml" id="S5.F9.2.m1.1.1.1.cmml" xref="S5.F9.2.m1.1.1.1">conditional</csymbol><apply id="S5.F9.2.m1.1.1.2.cmml" xref="S5.F9.2.m1.1.1.2"><ci id="S5.F9.2.m1.1.1.2.1.cmml" xref="S5.F9.2.m1.1.1.2.1">Pr</ci><ci id="S5.F9.2.m1.1.1.2.2a.cmml" xref="S5.F9.2.m1.1.1.2.2"><mtext id="S5.F9.2.m1.1.1.2.2.cmml" xref="S5.F9.2.m1.1.1.2.2">(system is correctÂ </mtext></ci></apply><ci id="S5.F9.2.m1.1.1.3a.cmml" xref="S5.F9.2.m1.1.1.3"><mtext id="S5.F9.2.m1.1.1.3.cmml" xref="S5.F9.2.m1.1.1.3">Â answer)</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.2.m1.1d">\Pr\text{(system is correct }|\text{ answer)}</annotation></semantics></math> for 50 most frequent ground truth answers on the VQA validation set (plot is sorted by accuracy, not frequency). System refers to our best model (deeper LSTM Q + norm I).</figcaption>
</figure>
<figure id="S5.F10" class="ltx_figure"><img src="/html/1505.00468/assets/x10.png" id="S5.F10.g1" class="ltx_graphics ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><math id="S5.F10.2.m1.1" class="ltx_Math" alttext="\Pr\text{(answer }|\text{ system is correct)}" display="inline"><semantics id="S5.F10.2.m1.1b"><mrow id="S5.F10.2.m1.1.1" xref="S5.F10.2.m1.1.1.cmml"><mrow id="S5.F10.2.m1.1.1.2" xref="S5.F10.2.m1.1.1.2.cmml"><mi id="S5.F10.2.m1.1.1.2.1" xref="S5.F10.2.m1.1.1.2.1.cmml">Pr</mi><mo lspace="0.167em" id="S5.F10.2.m1.1.1.2b" xref="S5.F10.2.m1.1.1.2.cmml">â¡</mo><mtext id="S5.F10.2.m1.1.1.2.2" xref="S5.F10.2.m1.1.1.2.2a.cmml">(answerÂ </mtext></mrow><mo fence="false" id="S5.F10.2.m1.1.1.1" xref="S5.F10.2.m1.1.1.1.cmml">|</mo><mtext id="S5.F10.2.m1.1.1.3" xref="S5.F10.2.m1.1.1.3a.cmml">Â system is correct)</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.F10.2.m1.1c"><apply id="S5.F10.2.m1.1.1.cmml" xref="S5.F10.2.m1.1.1"><csymbol cd="latexml" id="S5.F10.2.m1.1.1.1.cmml" xref="S5.F10.2.m1.1.1.1">conditional</csymbol><apply id="S5.F10.2.m1.1.1.2.cmml" xref="S5.F10.2.m1.1.1.2"><ci id="S5.F10.2.m1.1.1.2.1.cmml" xref="S5.F10.2.m1.1.1.2.1">Pr</ci><ci id="S5.F10.2.m1.1.1.2.2a.cmml" xref="S5.F10.2.m1.1.1.2.2"><mtext id="S5.F10.2.m1.1.1.2.2.cmml" xref="S5.F10.2.m1.1.1.2.2">(answerÂ </mtext></ci></apply><ci id="S5.F10.2.m1.1.1.3a.cmml" xref="S5.F10.2.m1.1.1.3"><mtext id="S5.F10.2.m1.1.1.3.cmml" xref="S5.F10.2.m1.1.1.3">Â system is correct)</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F10.2.m1.1d">\Pr\text{(answer }|\text{ system is correct)}</annotation></semantics></math> for 50 most frequently predicted answers on the VQA validation set (plot is sorted by prediction frequency, not accuracy). System refers to our best model (deeper LSTM Q + norm I).</figcaption>
</figure>
<figure id="S5.F11" class="ltx_figure"><img src="/html/1505.00468/assets/x11.png" id="S5.F11.g1" class="ltx_graphics ltx_img_landscape" width="461" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><math id="S5.F11.2.m1.1" class="ltx_Math" alttext="\Pr\text{(system is correct }|\text{ age of question)}" display="inline"><semantics id="S5.F11.2.m1.1b"><mrow id="S5.F11.2.m1.1.1" xref="S5.F11.2.m1.1.1.cmml"><mrow id="S5.F11.2.m1.1.1.2" xref="S5.F11.2.m1.1.1.2.cmml"><mi id="S5.F11.2.m1.1.1.2.1" xref="S5.F11.2.m1.1.1.2.1.cmml">Pr</mi><mo lspace="0.167em" id="S5.F11.2.m1.1.1.2b" xref="S5.F11.2.m1.1.1.2.cmml">â¡</mo><mtext id="S5.F11.2.m1.1.1.2.2" xref="S5.F11.2.m1.1.1.2.2a.cmml">(system is correctÂ </mtext></mrow><mo fence="false" id="S5.F11.2.m1.1.1.1" xref="S5.F11.2.m1.1.1.1.cmml">|</mo><mtext id="S5.F11.2.m1.1.1.3" xref="S5.F11.2.m1.1.1.3a.cmml">Â age of question)</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.F11.2.m1.1c"><apply id="S5.F11.2.m1.1.1.cmml" xref="S5.F11.2.m1.1.1"><csymbol cd="latexml" id="S5.F11.2.m1.1.1.1.cmml" xref="S5.F11.2.m1.1.1.1">conditional</csymbol><apply id="S5.F11.2.m1.1.1.2.cmml" xref="S5.F11.2.m1.1.1.2"><ci id="S5.F11.2.m1.1.1.2.1.cmml" xref="S5.F11.2.m1.1.1.2.1">Pr</ci><ci id="S5.F11.2.m1.1.1.2.2a.cmml" xref="S5.F11.2.m1.1.1.2.2"><mtext id="S5.F11.2.m1.1.1.2.2.cmml" xref="S5.F11.2.m1.1.1.2.2">(system is correctÂ </mtext></ci></apply><ci id="S5.F11.2.m1.1.1.3a.cmml" xref="S5.F11.2.m1.1.1.3"><mtext id="S5.F11.2.m1.1.1.3.cmml" xref="S5.F11.2.m1.1.1.3">Â age of question)</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F11.2.m1.1d">\Pr\text{(system is correct }|\text{ age of question)}</annotation></semantics></math> on the VQA validation set. System refers to our best model (deeper LSTM Q + norm I).</figcaption>
</figure>
<figure id="S5.F12" class="ltx_figure"><img src="/html/1505.00468/assets/x12.png" id="S5.F12.g1" class="ltx_graphics ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><math id="S5.F12.2.m1.1" class="ltx_Math" alttext="\Pr\text{(age of question }|\text{ system is correct)}" display="inline"><semantics id="S5.F12.2.m1.1b"><mrow id="S5.F12.2.m1.1.1" xref="S5.F12.2.m1.1.1.cmml"><mrow id="S5.F12.2.m1.1.1.2" xref="S5.F12.2.m1.1.1.2.cmml"><mi id="S5.F12.2.m1.1.1.2.1" xref="S5.F12.2.m1.1.1.2.1.cmml">Pr</mi><mo lspace="0.167em" id="S5.F12.2.m1.1.1.2b" xref="S5.F12.2.m1.1.1.2.cmml">â¡</mo><mtext id="S5.F12.2.m1.1.1.2.2" xref="S5.F12.2.m1.1.1.2.2a.cmml">(age of questionÂ </mtext></mrow><mo fence="false" id="S5.F12.2.m1.1.1.1" xref="S5.F12.2.m1.1.1.1.cmml">|</mo><mtext id="S5.F12.2.m1.1.1.3" xref="S5.F12.2.m1.1.1.3a.cmml">Â system is correct)</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.F12.2.m1.1c"><apply id="S5.F12.2.m1.1.1.cmml" xref="S5.F12.2.m1.1.1"><csymbol cd="latexml" id="S5.F12.2.m1.1.1.1.cmml" xref="S5.F12.2.m1.1.1.1">conditional</csymbol><apply id="S5.F12.2.m1.1.1.2.cmml" xref="S5.F12.2.m1.1.1.2"><ci id="S5.F12.2.m1.1.1.2.1.cmml" xref="S5.F12.2.m1.1.1.2.1">Pr</ci><ci id="S5.F12.2.m1.1.1.2.2a.cmml" xref="S5.F12.2.m1.1.1.2.2"><mtext id="S5.F12.2.m1.1.1.2.2.cmml" xref="S5.F12.2.m1.1.1.2.2">(age of questionÂ </mtext></ci></apply><ci id="S5.F12.2.m1.1.1.3a.cmml" xref="S5.F12.2.m1.1.1.3"><mtext id="S5.F12.2.m1.1.1.3.cmml" xref="S5.F12.2.m1.1.1.3">Â system is correct)</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F12.2.m1.1d">\Pr\text{(age of question }|\text{ system is correct)}</annotation></semantics></math> on the VQA validation set. System refers to our best model (deeper LSTM Q + norm I).</figcaption>
</figure>
<div id="S5.SS3.p12" class="ltx_para ltx_noindent">
<p id="S5.SS3.p12.1" class="ltx_p">TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the accuracy of different ablated versions of our best model (deeper LSTM Q + norm I) for both the open-ended and multiple-choice tasks on the VQA test-dev for real images. The different ablated versions are as follows â€“</p>
</div>
<div id="S5.SS3.p13" class="ltx_para ltx_noindent">
<ol id="S5.I5" class="ltx_enumerate">
<li id="S5.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I5.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i1.p1.2" class="ltx_p"><span id="S5.I5.i1.p1.2.1" class="ltx_text ltx_font_bold">Without I Norm:</span> In this model, the activations from the last hidden layer of VGGNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> are not <math id="S5.I5.i1.p1.1.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S5.I5.i1.p1.1.m1.1a"><msub id="S5.I5.i1.p1.1.m1.1.1" xref="S5.I5.i1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.I5.i1.p1.1.m1.1.1.2" xref="S5.I5.i1.p1.1.m1.1.1.2.cmml">â„“</mi><mn id="S5.I5.i1.p1.1.m1.1.1.3" xref="S5.I5.i1.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.I5.i1.p1.1.m1.1b"><apply id="S5.I5.i1.p1.1.m1.1.1.cmml" xref="S5.I5.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I5.i1.p1.1.m1.1.1.1.cmml" xref="S5.I5.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.I5.i1.p1.1.m1.1.1.2.cmml" xref="S5.I5.i1.p1.1.m1.1.1.2">â„“</ci><cn type="integer" id="S5.I5.i1.p1.1.m1.1.1.3.cmml" xref="S5.I5.i1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I5.i1.p1.1.m1.1c">\ell_{2}</annotation></semantics></math>-normalized. Comparing the accuracies in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that <math id="S5.I5.i1.p1.2.m2.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S5.I5.i1.p1.2.m2.1a"><msub id="S5.I5.i1.p1.2.m2.1.1" xref="S5.I5.i1.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.I5.i1.p1.2.m2.1.1.2" xref="S5.I5.i1.p1.2.m2.1.1.2.cmml">â„“</mi><mn id="S5.I5.i1.p1.2.m2.1.1.3" xref="S5.I5.i1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.I5.i1.p1.2.m2.1b"><apply id="S5.I5.i1.p1.2.m2.1.1.cmml" xref="S5.I5.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.I5.i1.p1.2.m2.1.1.1.cmml" xref="S5.I5.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.I5.i1.p1.2.m2.1.1.2.cmml" xref="S5.I5.i1.p1.2.m2.1.1.2">â„“</ci><cn type="integer" id="S5.I5.i1.p1.2.m2.1.1.3.cmml" xref="S5.I5.i1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I5.i1.p1.2.m2.1c">\ell_{2}</annotation></semantics></math>-normalization of image features boosts the performance by 0.16% for open-ended task and by 0.24% for multiple-choice task.</p>
</div>
</li>
<li id="S5.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I5.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i2.p1.1" class="ltx_p"><span id="S5.I5.i2.p1.1.1" class="ltx_text ltx_font_bold">Concatenation:</span> In this model, the transformed image and LSTM embeddings are concatenated (instead of element-wise multiplied), resulting in doubling the number of parameters in the following fully-connected layer. Comparing the accuracies in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that element-wise fusion performs better by 0.95% for open-ended task and by 1.24% for multiple-choice task.</p>
</div>
</li>
<li id="S5.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I5.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i3.p1.1" class="ltx_p"><span id="S5.I5.i3.p1.1.1" class="ltx_text ltx_font_bold">K = 500:</span> In this model, we use K = 500 most frequent answers as possible outputs. Comparing the accuracies in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that K = 1000 performs better than K = 500 by 0.82% for open-ended task and by 1.92% for multiple-choice task.</p>
</div>
</li>
<li id="S5.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I5.i4.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i4.p1.1" class="ltx_p"><span id="S5.I5.i4.p1.1.1" class="ltx_text ltx_font_bold">K = 2000:</span> In this model, we use K = 2000 most frequent answers as possible outputs. Comparing the accuracies in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that K = 2000 performs better then K = 1000 by 0.40% for open-ended task and by 1.16% for multiple-choice task.</p>
</div>
</li>
<li id="S5.I5.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S5.I5.i5.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i5.p1.2" class="ltx_p"><span id="S5.I5.i5.p1.1.1" class="ltx_text ltx_font_bold">Truncated Q Vocab <math id="S5.I5.i5.p1.1.1.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.I5.i5.p1.1.1.m1.1a"><mi mathvariant="normal" id="S5.I5.i5.p1.1.1.m1.1.1" xref="S5.I5.i5.p1.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.I5.i5.p1.1.1.m1.1b"><ci id="S5.I5.i5.p1.1.1.m1.1.1.cmml" xref="S5.I5.i5.p1.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I5.i5.p1.1.1.m1.1c">@</annotation></semantics></math> 5:</span> In this model, the input vocabulary to the embedding layer (which encodes the question words) consists of only those question words which occur atleast 5 times in the training dataset, thus reducing the vocabulary size from 14770 (when all question words are used) to 5134 (65.24% reduction). Remaining question words are replaced with UNK (unknown) tokens. Comparing the accuracies in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that truncating the question vocabulary <math id="S5.I5.i5.p1.2.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.I5.i5.p1.2.m1.1a"><mi mathvariant="normal" id="S5.I5.i5.p1.2.m1.1.1" xref="S5.I5.i5.p1.2.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.I5.i5.p1.2.m1.1b"><ci id="S5.I5.i5.p1.2.m1.1.1.cmml" xref="S5.I5.i5.p1.2.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I5.i5.p1.2.m1.1c">@</annotation></semantics></math> 5 performs better than using all questions words by 0.24% for open-ended task and by 0.17% for multiple-choice task.</p>
</div>
</li>
<li id="S5.I5.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S5.I5.i6.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i6.p1.2" class="ltx_p"><span id="S5.I5.i6.p1.1.1" class="ltx_text ltx_font_bold">Truncated Q Vocab <math id="S5.I5.i6.p1.1.1.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.I5.i6.p1.1.1.m1.1a"><mi mathvariant="normal" id="S5.I5.i6.p1.1.1.m1.1.1" xref="S5.I5.i6.p1.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.I5.i6.p1.1.1.m1.1b"><ci id="S5.I5.i6.p1.1.1.m1.1.1.cmml" xref="S5.I5.i6.p1.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I5.i6.p1.1.1.m1.1c">@</annotation></semantics></math> 11:</span> In this model, the input vocabulary to the embedding layer (which encodes the question words) consists of only those question words which occur atleast 11 times in the training dataset, thus reducing the vocabulary size from 14770 (when all question words are used) to 3561 (75.89% reduction). Remaining question words are replaced with UNK (unknown) tokens. Comparing the accuracies in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that truncating the question vocabulary <math id="S5.I5.i6.p1.2.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.I5.i6.p1.2.m1.1a"><mi mathvariant="normal" id="S5.I5.i6.p1.2.m1.1.1" xref="S5.I5.i6.p1.2.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.I5.i6.p1.2.m1.1b"><ci id="S5.I5.i6.p1.2.m1.1.1.cmml" xref="S5.I5.i6.p1.2.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I5.i6.p1.2.m1.1c">@</annotation></semantics></math> 11 performs better than using all questions words by 0.06% for open-ended task and by 0.02% for multiple-choice task.</p>
</div>
</li>
<li id="S5.I5.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S5.I5.i7.p1" class="ltx_para">
<p id="S5.I5.i7.p1.1" class="ltx_p"><span id="S5.I5.i7.p1.1.1" class="ltx_text ltx_font_bold">Filtered Dataset:</span> We created a filtered version of the VQA train + val dataset in which we only keep the answers with subject confidence â€œyesâ€. Also, we keep only those questions for which at least 50% (5 out of 10) answers are annotated with subject confidence â€œyesâ€. The resulting filtered dataset consists of 344600 questions, compared to 369861 questions in the original dataset, thus leading to only 6.83% reduction in the size of the dataset. The filtered dataset has 8.77 answers per question on average. We did not filter the test set so that accuracies of the model trained on the filtered dataset can be compared with that of the model trained on the original dataset. The row â€œFiltered Datasetâ€ in TableÂ <a href="#S5.T4" title="TABLE IV â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the performance of the deeper LSTM Q + norm I model when trained on the filtered dataset. Comparing these accuracies with the corresponding accuracies in TableÂ <a href="#S5.T2" title="TABLE II â€£ 5.3 Results â€£ 5 VQA Baselines and Methods â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that the model trained on filtered version performs worse by 1.13% for open-ended task and by 1.88% for multiple-choice task.</p>
</div>
</li>
</ol>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.2.2.3.1" class="ltx_tr">
<th id="S5.T4.2.2.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;"></th>
<th id="S5.T4.2.2.3.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;" colspan="4"><span id="S5.T4.2.2.3.1.2.1" class="ltx_text" style="font-size:70%;">Open-Ended</span></th>
<th id="S5.T4.2.2.3.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;" colspan="4"><span id="S5.T4.2.2.3.1.3.1" class="ltx_text" style="font-size:70%;">Multiple-Choice</span></th>
</tr>
<tr id="S5.T4.2.2.4.2" class="ltx_tr">
<th id="S5.T4.2.2.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"></th>
<th id="S5.T4.2.2.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.2.1" class="ltx_text" style="font-size:70%;">All</span></th>
<th id="S5.T4.2.2.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.3.1" class="ltx_text" style="font-size:70%;">Yes/No</span></th>
<th id="S5.T4.2.2.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.4.1" class="ltx_text" style="font-size:70%;">Number</span></th>
<th id="S5.T4.2.2.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.5.1" class="ltx_text" style="font-size:70%;">Other</span></th>
<th id="S5.T4.2.2.4.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.6.1" class="ltx_text" style="font-size:70%;">All</span></th>
<th id="S5.T4.2.2.4.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.7.1" class="ltx_text" style="font-size:70%;">Yes/No</span></th>
<th id="S5.T4.2.2.4.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.8.1" class="ltx_text" style="font-size:70%;">Number</span></th>
<th id="S5.T4.2.2.4.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.4.2.9.1" class="ltx_text" style="font-size:70%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.2.2.5.1" class="ltx_tr">
<th id="S5.T4.2.2.5.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.1.1" class="ltx_text" style="font-size:70%;">Without I Norm</span></th>
<td id="S5.T4.2.2.5.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.2.1" class="ltx_text" style="font-size:70%;">57.59</span></td>
<td id="S5.T4.2.2.5.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.3.1" class="ltx_text" style="font-size:70%;">80.41</span></td>
<td id="S5.T4.2.2.5.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.4.1" class="ltx_text" style="font-size:70%;">36.63</span></td>
<td id="S5.T4.2.2.5.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.5.1" class="ltx_text" style="font-size:70%;">42.84</span></td>
<td id="S5.T4.2.2.5.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.6.1" class="ltx_text" style="font-size:70%;">62.46</span></td>
<td id="S5.T4.2.2.5.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.7.1" class="ltx_text" style="font-size:70%;">80.43</span></td>
<td id="S5.T4.2.2.5.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.8.1" class="ltx_text" style="font-size:70%;">38.10</span></td>
<td id="S5.T4.2.2.5.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.5.1.9.1" class="ltx_text" style="font-size:70%;">52.62</span></td>
</tr>
<tr id="S5.T4.2.2.6.2" class="ltx_tr">
<th id="S5.T4.2.2.6.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></th>
<td id="S5.T4.2.2.6.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.2.1" class="ltx_text" style="font-size:70%;">56.80</span></td>
<td id="S5.T4.2.2.6.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.3.1" class="ltx_text" style="font-size:70%;">78.49</span></td>
<td id="S5.T4.2.2.6.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.4.1" class="ltx_text" style="font-size:70%;">35.08</span></td>
<td id="S5.T4.2.2.6.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.5.1" class="ltx_text" style="font-size:70%;">43.19</span></td>
<td id="S5.T4.2.2.6.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.6.1" class="ltx_text" style="font-size:70%;">61.46</span></td>
<td id="S5.T4.2.2.6.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.7.1" class="ltx_text" style="font-size:70%;">78.52</span></td>
<td id="S5.T4.2.2.6.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.8.1" class="ltx_text" style="font-size:70%;">36.43</span></td>
<td id="S5.T4.2.2.6.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.6.2.9.1" class="ltx_text" style="font-size:70%;">52.54</span></td>
</tr>
<tr id="S5.T4.2.2.7.3" class="ltx_tr">
<th id="S5.T4.2.2.7.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.1.1" class="ltx_text" style="font-size:70%;">K = 500</span></th>
<td id="S5.T4.2.2.7.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.2.1" class="ltx_text" style="font-size:70%;">56.93</span></td>
<td id="S5.T4.2.2.7.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.3.1" class="ltx_text" style="font-size:70%;">80.61</span></td>
<td id="S5.T4.2.2.7.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.4.1" class="ltx_text" style="font-size:70%;">36.24</span></td>
<td id="S5.T4.2.2.7.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.5.1" class="ltx_text" style="font-size:70%;">41.39</span></td>
<td id="S5.T4.2.2.7.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.6.1" class="ltx_text" style="font-size:70%;">60.78</span></td>
<td id="S5.T4.2.2.7.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.7.1" class="ltx_text" style="font-size:70%;">80.64</span></td>
<td id="S5.T4.2.2.7.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.8.1" class="ltx_text" style="font-size:70%;">37.44</span></td>
<td id="S5.T4.2.2.7.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.7.3.9.1" class="ltx_text" style="font-size:70%;">49.10</span></td>
</tr>
<tr id="S5.T4.2.2.8.4" class="ltx_tr">
<th id="S5.T4.2.2.8.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.1.1" class="ltx_text" style="font-size:70%;">K = 2000</span></th>
<td id="S5.T4.2.2.8.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.2.1" class="ltx_text" style="font-size:70%;">58.15</span></td>
<td id="S5.T4.2.2.8.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.3.1" class="ltx_text" style="font-size:70%;">80.56</span></td>
<td id="S5.T4.2.2.8.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.4.1" class="ltx_text" style="font-size:70%;">37.04</span></td>
<td id="S5.T4.2.2.8.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.5.1" class="ltx_text" style="font-size:70%;">43.79</span></td>
<td id="S5.T4.2.2.8.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.6.1" class="ltx_text" style="font-size:70%;">63.86</span></td>
<td id="S5.T4.2.2.8.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.7.1" class="ltx_text" style="font-size:70%;">80.59</span></td>
<td id="S5.T4.2.2.8.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.8.1" class="ltx_text" style="font-size:70%;">38.97</span></td>
<td id="S5.T4.2.2.8.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.8.4.9.1" class="ltx_text" style="font-size:70%;">55.20</span></td>
</tr>
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;">
<span id="S5.T4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Truncated Q Vocab </span><math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mi mathsize="70%" mathvariant="normal" id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">@</annotation></semantics></math><span id="S5.T4.1.1.1.1.2" class="ltx_text" style="font-size:70%;"> 5</span>
</th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">57.99</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.3.1" class="ltx_text" style="font-size:70%;">80.67</span></td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.4.1" class="ltx_text" style="font-size:70%;">36.99</span></td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.5.1" class="ltx_text" style="font-size:70%;">43.38</span></td>
<td id="S5.T4.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.6.1" class="ltx_text" style="font-size:70%;">62.87</span></td>
<td id="S5.T4.1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.7.1" class="ltx_text" style="font-size:70%;">80.71</span></td>
<td id="S5.T4.1.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.8.1" class="ltx_text" style="font-size:70%;">38.22</span></td>
<td id="S5.T4.1.1.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.1.1.1.9.1" class="ltx_text" style="font-size:70%;">53.20</span></td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<th id="S5.T4.2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;">
<span id="S5.T4.2.2.2.1.1" class="ltx_text" style="font-size:70%;">Truncated Q Vocab </span><math id="S5.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.T4.2.2.2.1.m1.1a"><mi mathsize="70%" mathvariant="normal" id="S5.T4.2.2.2.1.m1.1.1" xref="S5.T4.2.2.2.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.m1.1b"><ci id="S5.T4.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.m1.1c">@</annotation></semantics></math><span id="S5.T4.2.2.2.1.2" class="ltx_text" style="font-size:70%;"> 11</span>
</th>
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.2.1" class="ltx_text" style="font-size:70%;">57.81</span></td>
<td id="S5.T4.2.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.3.1" class="ltx_text" style="font-size:70%;">80.42</span></td>
<td id="S5.T4.2.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.4.1" class="ltx_text" style="font-size:70%;">36.97</span></td>
<td id="S5.T4.2.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.5.1" class="ltx_text" style="font-size:70%;">43.22</span></td>
<td id="S5.T4.2.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.6.1" class="ltx_text" style="font-size:70%;">62.72</span></td>
<td id="S5.T4.2.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.7.1" class="ltx_text" style="font-size:70%;">80.45</span></td>
<td id="S5.T4.2.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.8.1" class="ltx_text" style="font-size:70%;">38.30</span></td>
<td id="S5.T4.2.2.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.2.9.1" class="ltx_text" style="font-size:70%;">53.09</span></td>
</tr>
<tr id="S5.T4.2.2.9.5" class="ltx_tr">
<th id="S5.T4.2.2.9.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.1.1" class="ltx_text" style="font-size:70%;">Filtered Dataset</span></th>
<td id="S5.T4.2.2.9.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.2.1" class="ltx_text" style="font-size:70%;">56.62</span></td>
<td id="S5.T4.2.2.9.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.3.1" class="ltx_text" style="font-size:70%;">80.19</span></td>
<td id="S5.T4.2.2.9.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.4.1" class="ltx_text" style="font-size:70%;">37.48</span></td>
<td id="S5.T4.2.2.9.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.5.1" class="ltx_text" style="font-size:70%;">40.95</span></td>
<td id="S5.T4.2.2.9.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.6.1" class="ltx_text" style="font-size:70%;">60.82</span></td>
<td id="S5.T4.2.2.9.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.7.1" class="ltx_text" style="font-size:70%;">80.19</span></td>
<td id="S5.T4.2.2.9.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.8.1" class="ltx_text" style="font-size:70%;">37.48</span></td>
<td id="S5.T4.2.2.9.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S5.T4.2.2.9.5.9.1" class="ltx_text" style="font-size:70%;">49.57</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Accuracy of ablated versions of our best model (deeper LSTM Q + norm I) for the open-ended and multiple-choice tasks on the VQA test-dev for real images.
Q = Question, I = Image.
See text for details.
</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">VQA Challenge and Workshop</span>
</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We have set up an evaluation server<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="http://visualqa.org/challenge.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://visualqa.org/challenge.html</a></span></span></span> where results may be uploaded for the test set and it returns an accuracy breakdown. We are organizing an annual challenge and workshop to facilitate systematic progress in this area; the first instance of the workshop will be held at CVPR 2016<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="http://www.visualqa.org/workshop.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.visualqa.org/workshop.html</a></span></span></span>. We suggest that papers reporting results on the VQA dataset â€“</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i1.p1.3" class="ltx_p">Report test-standard accuracies, which can be calculated using either of the non-test-dev phases, i.e., â€œtest2015â€ or â€œChallenge test2015â€ on the following links: [<a target="_blank" href="https://www.codalab.org/competitions/6961" title="" class="ltx_ref ltx_href">oe-real</a> <math id="S6.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S6.I1.i1.p1.1.m1.1a"><mo fence="false" stretchy="false" id="S6.I1.i1.p1.1.m1.1.1" xref="S6.I1.i1.p1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S6.I1.i1.p1.1.m1.1b"><ci id="S6.I1.i1.p1.1.m1.1.1.cmml" xref="S6.I1.i1.p1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i1.p1.1.m1.1c">|</annotation></semantics></math> <a target="_blank" href="https://www.codalab.org/competitions/6981" title="" class="ltx_ref ltx_href">oe-abstract</a> <math id="S6.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S6.I1.i1.p1.2.m2.1a"><mo fence="false" stretchy="false" id="S6.I1.i1.p1.2.m2.1.1" xref="S6.I1.i1.p1.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S6.I1.i1.p1.2.m2.1b"><ci id="S6.I1.i1.p1.2.m2.1.1.cmml" xref="S6.I1.i1.p1.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i1.p1.2.m2.1c">|</annotation></semantics></math> <a target="_blank" href="https://www.codalab.org/competitions/6971" title="" class="ltx_ref ltx_href">mc-real</a> <math id="S6.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S6.I1.i1.p1.3.m3.1a"><mo fence="false" stretchy="false" id="S6.I1.i1.p1.3.m3.1.1" xref="S6.I1.i1.p1.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S6.I1.i1.p1.3.m3.1b"><ci id="S6.I1.i1.p1.3.m3.1.1.cmml" xref="S6.I1.i1.p1.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i1.p1.3.m3.1c">|</annotation></semantics></math> <a target="_blank" href="https://www.codalab.org/competitions/6991" title="" class="ltx_ref ltx_href">mc-abstract</a>].</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i2.p1.3" class="ltx_p">Compare their test-standard accuracies with those on the corresponding test2015 leaderboards [<a target="_blank" href="http://www.visualqa.org/roe.html" title="" class="ltx_ref ltx_href">oe-real-leaderboard</a> <math id="S6.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S6.I1.i2.p1.1.m1.1a"><mo fence="false" stretchy="false" id="S6.I1.i2.p1.1.m1.1.1" xref="S6.I1.i2.p1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S6.I1.i2.p1.1.m1.1b"><ci id="S6.I1.i2.p1.1.m1.1.1.cmml" xref="S6.I1.i2.p1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i2.p1.1.m1.1c">|</annotation></semantics></math> <a target="_blank" href="http://www.visualqa.org/aoe.html" title="" class="ltx_ref ltx_href">oe-abstract-leaderboard</a> <math id="S6.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S6.I1.i2.p1.2.m2.1a"><mo fence="false" stretchy="false" id="S6.I1.i2.p1.2.m2.1.1" xref="S6.I1.i2.p1.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S6.I1.i2.p1.2.m2.1b"><ci id="S6.I1.i2.p1.2.m2.1.1.cmml" xref="S6.I1.i2.p1.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i2.p1.2.m2.1c">|</annotation></semantics></math> <a target="_blank" href="http://www.visualqa.org/rmc.html" title="" class="ltx_ref ltx_href">mc-real-leaderboard</a> <math id="S6.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S6.I1.i2.p1.3.m3.1a"><mo fence="false" stretchy="false" id="S6.I1.i2.p1.3.m3.1.1" xref="S6.I1.i2.p1.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S6.I1.i2.p1.3.m3.1b"><ci id="S6.I1.i2.p1.3.m3.1.1.cmml" xref="S6.I1.i2.p1.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i2.p1.3.m3.1c">|</annotation></semantics></math> <a target="_blank" href="http://www.visualqa.org/amc.html" title="" class="ltx_ref ltx_href">mc-abstract-leaderboard</a>].</p>
</div>
</li>
</ol>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">For more details, please see the challenge page<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="http://visualqa.org/challenge.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://visualqa.org/challenge.html</a></span></span></span>. Screenshots of leaderboards for open-ended-real and multiple-choice-real are shown in Fig.Â <a href="#S6.F13" title="Figure 13 â€£ 6 VQA Challenge and Workshop â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.
We also compare the test-standard accuracies of our best model (deeper LSTM Q + norm I) for both open-ended and multiple-choice tasks (real images) with other entries (as of ) on the corresponding leaderboards in TableÂ <a href="#S6.T5" title="TABLE V â€£ 6 VQA Challenge and Workshop â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;"></th>
<th id="S6.T5.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;" colspan="4"><span id="S6.T5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Open-Ended</span></th>
<th id="S6.T5.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.8pt;padding-right:1.8pt;" colspan="4"><span id="S6.T5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Multiple-Choice</span></th>
</tr>
<tr id="S6.T5.1.2.2" class="ltx_tr">
<th id="S6.T5.1.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"></th>
<th id="S6.T5.1.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.2.1" class="ltx_text" style="font-size:70%;">All</span></th>
<th id="S6.T5.1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Yes/No</span></th>
<th id="S6.T5.1.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Number</span></th>
<th id="S6.T5.1.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Other</span></th>
<th id="S6.T5.1.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.6.1" class="ltx_text" style="font-size:70%;">All</span></th>
<th id="S6.T5.1.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.7.1" class="ltx_text" style="font-size:70%;">Yes/No</span></th>
<th id="S6.T5.1.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.8.1" class="ltx_text" style="font-size:70%;">Number</span></th>
<th id="S6.T5.1.2.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.2.2.9.1" class="ltx_text" style="font-size:70%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.3.1" class="ltx_tr">
<th id="S6.T5.1.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.1.1" class="ltx_text" style="font-size:70%;">snubi-naverlabs</span></th>
<td id="S6.T5.1.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.2.1" class="ltx_text" style="font-size:70%;">60.60</span></td>
<td id="S6.T5.1.3.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.3.1" class="ltx_text" style="font-size:70%;">82.23</span></td>
<td id="S6.T5.1.3.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.4.1" class="ltx_text" style="font-size:70%;">38.22</span></td>
<td id="S6.T5.1.3.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.5.1" class="ltx_text" style="font-size:70%;">46.99</span></td>
<td id="S6.T5.1.3.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.6.1" class="ltx_text" style="font-size:70%;">64.95</span></td>
<td id="S6.T5.1.3.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.7.1" class="ltx_text" style="font-size:70%;">82.25</span></td>
<td id="S6.T5.1.3.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.8.1" class="ltx_text" style="font-size:70%;">39.56</span></td>
<td id="S6.T5.1.3.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.3.1.9.1" class="ltx_text" style="font-size:70%;">55.68</span></td>
</tr>
<tr id="S6.T5.1.4.2" class="ltx_tr">
<th id="S6.T5.1.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.1.1" class="ltx_text" style="font-size:70%;">MM_PaloAlto</span></th>
<td id="S6.T5.1.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.2.1" class="ltx_text" style="font-size:70%;">60.36</span></td>
<td id="S6.T5.1.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.3.1" class="ltx_text" style="font-size:70%;">80.43</span></td>
<td id="S6.T5.1.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.4.1" class="ltx_text" style="font-size:70%;">36.82</span></td>
<td id="S6.T5.1.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.5.1" class="ltx_text" style="font-size:70%;">48.33</span></td>
<td id="S6.T5.1.4.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.6.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.4.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.7.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.4.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.8.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.4.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.4.2.9.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
</tr>
<tr id="S6.T5.1.5.3" class="ltx_tr">
<th id="S6.T5.1.5.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.1.1" class="ltx_text" style="font-size:70%;">LV-NUS</span></th>
<td id="S6.T5.1.5.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.2.1" class="ltx_text" style="font-size:70%;">59.54</span></td>
<td id="S6.T5.1.5.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.3.1" class="ltx_text" style="font-size:70%;">81.34</span></td>
<td id="S6.T5.1.5.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.4.1" class="ltx_text" style="font-size:70%;">35.67</span></td>
<td id="S6.T5.1.5.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.5.1" class="ltx_text" style="font-size:70%;">46.10</span></td>
<td id="S6.T5.1.5.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.6.1" class="ltx_text" style="font-size:70%;">64.18</span></td>
<td id="S6.T5.1.5.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.7.1" class="ltx_text" style="font-size:70%;">81.25</span></td>
<td id="S6.T5.1.5.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.8.1" class="ltx_text" style="font-size:70%;">38.30</span></td>
<td id="S6.T5.1.5.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.5.3.9.1" class="ltx_text" style="font-size:70%;">55.20</span></td>
</tr>
<tr id="S6.T5.1.6.4" class="ltx_tr">
<th id="S6.T5.1.6.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.1.1" class="ltx_text" style="font-size:70%;">ACVT_Adelaide</span></th>
<td id="S6.T5.1.6.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.2.1" class="ltx_text" style="font-size:70%;">59.44</span></td>
<td id="S6.T5.1.6.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.3.1" class="ltx_text" style="font-size:70%;">81.07</span></td>
<td id="S6.T5.1.6.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.4.1" class="ltx_text" style="font-size:70%;">37.12</span></td>
<td id="S6.T5.1.6.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.5.1" class="ltx_text" style="font-size:70%;">45.83</span></td>
<td id="S6.T5.1.6.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.6.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.6.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.7.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.6.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.8.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.6.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.6.4.9.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
</tr>
<tr id="S6.T5.1.7.5" class="ltx_tr">
<th id="S6.T5.1.7.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.1.1" class="ltx_text" style="font-size:70%;">global_vision</span></th>
<td id="S6.T5.1.7.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.2.1" class="ltx_text" style="font-size:70%;">58.43</span></td>
<td id="S6.T5.1.7.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.3.1" class="ltx_text" style="font-size:70%;">78.24</span></td>
<td id="S6.T5.1.7.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.4.1" class="ltx_text" style="font-size:70%;">36.27</span></td>
<td id="S6.T5.1.7.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.5.1" class="ltx_text" style="font-size:70%;">46.32</span></td>
<td id="S6.T5.1.7.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.6.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.7.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.7.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.7.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.8.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.7.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.7.5.9.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
</tr>
<tr id="S6.T5.1.8.6" class="ltx_tr">
<th id="S6.T5.1.8.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.1.1" class="ltx_text" style="font-size:70%;">deeper LSTM Q + norm I</span></th>
<td id="S6.T5.1.8.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.2.1" class="ltx_text" style="font-size:70%;">58.16</span></td>
<td id="S6.T5.1.8.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.3.1" class="ltx_text" style="font-size:70%;">80.56</span></td>
<td id="S6.T5.1.8.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.4.1" class="ltx_text" style="font-size:70%;">36.53</span></td>
<td id="S6.T5.1.8.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.5.1" class="ltx_text" style="font-size:70%;">43.73</span></td>
<td id="S6.T5.1.8.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.6.1" class="ltx_text" style="font-size:70%;">63.09</span></td>
<td id="S6.T5.1.8.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.7.1" class="ltx_text" style="font-size:70%;">80.59</span></td>
<td id="S6.T5.1.8.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.8.1" class="ltx_text" style="font-size:70%;">37.70</span></td>
<td id="S6.T5.1.8.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.8.6.9.1" class="ltx_text" style="font-size:70%;">53.64</span></td>
</tr>
<tr id="S6.T5.1.9.7" class="ltx_tr">
<th id="S6.T5.1.9.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.1.1" class="ltx_text" style="font-size:70%;">iBOWIMG</span></th>
<td id="S6.T5.1.9.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.2.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.9.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.3.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.9.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.4.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.9.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.5.1" class="ltx_text" style="font-size:70%;">â€“</span></td>
<td id="S6.T5.1.9.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.6.1" class="ltx_text" style="font-size:70%;">61.97</span></td>
<td id="S6.T5.1.9.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.7.1" class="ltx_text" style="font-size:70%;">76.86</span></td>
<td id="S6.T5.1.9.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.8.1" class="ltx_text" style="font-size:70%;">37.30</span></td>
<td id="S6.T5.1.9.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.8pt;padding-right:1.8pt;"><span id="S6.T5.1.9.7.9.1" class="ltx_text" style="font-size:70%;">54.60</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Test-standard accuracy of our best model (deeper LSTM Q + norm I) compared to test-standard accuracies of other entries for the open-ended and multiple-choice tasks in the respective VQA Real Image Challenge leaderboards (as of ).</figcaption>
</figure>
<figure id="S6.F13" class="ltx_figure"><img src="/html/1505.00468/assets/x13.png" id="S6.F13.g1" class="ltx_graphics ltx_img_landscape" width="461" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Leaderboard showing test-standard accuracies for VQA Real Image Challenge (Open-Ended) on left and leaderboard showing test-standard accuracies for VQA Real Image Challenge (Multiple-Choice) on right (snapshot from ).</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Discussion</span>
</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">In conclusion, we introduce the task of Visual Question Answering (VQA). Given an image and an open-ended, natural language question about the image, the task is to provide an accurate natural language answer. We provide a dataset containing over 250K images, 760K questions, and around 10M answers. We demonstrate the wide variety of questions and answers in our dataset, as well as the diverse set of AI capabilities in computer vision, natural language processing, and commonsense reasoning required to answer these questions accurately.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">The questions we solicited from our human subjects were open-ended and not task-specific. For some application domains, it would be useful to collect task-specific questions. For instance, questions may be gathered from subjects who are visually impaired <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or the questions could focused on one specific domain (say sports).
Bigham <em id="S7.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p2.1.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> created an application that allows the visually impaired to capture images and ask open-ended questions that are answered by human subjects. Interestingly, these questions can rarely be answered using generic captions. Training on task-specific datasets may help enable practical VQA applications.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p">We believe VQA has the distinctive advantage of pushing the frontiers on â€œAI-completeâ€ problems, while being amenable to automatic evaluation. Given the recent progress in the community, we believe the time is ripe to take on such an endeavor.</p>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">Acknowledgements.</span>
We would like to acknowledge the countless hours of effort provided by the workers on Amazon Mechanical Turk. This work was supported in part by the The Paul G. Allen Family Foundation via an award to D.P., ICTAS at Virginia Tech via awards to D.B. and D.P., Google Faculty Research Awards to D.P. and D.B., the National Science Foundation CAREER award to D.B., the Army Research Office YIP Award to D.B., and a Office of Naval Research grant to D.B.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix Overview</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">In the appendix, we provide:
</p>
<ol id="Sx1.I1" class="ltx_enumerate">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">- Additional analysis comparing captions and Q&amp;A data</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">- Qualitative visualizations for â€œWhat isâ€ questions</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">- Human accuracy on multiple-choice questions</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">IV</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">- <span id="Sx1.I1.i4.p1.1.1" class="ltx_text" style="color:#000000;">Details on VQA baselines</span></p>
</div>
</li>
<li id="Sx1.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">V</span> 
<div id="Sx1.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i5.p1.1" class="ltx_p">- <span id="Sx1.I1.i5.p1.1.1" class="ltx_text" style="color:#000000;">â€œAgeâ€ and â€œCommonsenseâ€ of our model</span></p>
</div>
</li>
<li id="Sx1.I1.i6" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">VI</span> 
<div id="Sx1.I1.i6.p1" class="ltx_para">
<p id="Sx1.I1.i6.p1.1" class="ltx_p">- Details on the abstract scene dataset</p>
</div>
</li>
<li id="Sx1.I1.i7" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">VII</span> 
<div id="Sx1.I1.i7.p1" class="ltx_para">
<p id="Sx1.I1.i7.p1.1" class="ltx_p">- User interfaces used to collect the dataset</p>
</div>
</li>
<li id="Sx1.I1.i8" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">VIII</span> 
<div id="Sx1.I1.i8.p1" class="ltx_para">
<p id="Sx1.I1.i8.p1.1" class="ltx_p">- List of the top answers in the dataset</p>
</div>
</li>
<li id="Sx1.I1.i9" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">IX</span> 
<div id="Sx1.I1.i9.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i9.p1.1" class="ltx_p">- Additional examples from the VQA dataset</p>
</div>
</li>
</ol>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix I: Captions <em id="Sx2.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="Sx2.2.2" class="ltx_text"></span> Questions</h2>

<div id="Sx2.p1" class="ltx_para ltx_noindent">
<p id="Sx2.p1.1" class="ltx_p">Do questions and answers provide further information about the visual world beyond that captured by captions? One method for determining whether the information captured by questions &amp; answers is different from the information captured by captions is to measure some of the differences in the word distributions from the two datasets. We cast this comparison in terms of nouns, verbs, and adjectives by extracting all words from the caption data <span id="Sx2.p1.1.1" class="ltx_text" style="color:#000000;">(MS COCO captions for real images and captions collected by us for abstract scenes)</span> using the Stanford part-of-speech (POS)<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Noun tags begin with NN, verb tags begin with VB, adjective tags begin with JJ, and prepositions are tagged as IN.</span></span></span> taggerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. We normalize the word frequencies from captions, questions, and answers per image, and compare captions <em id="Sx2.p1.1.2" class="ltx_emph ltx_font_italic">vs</em>.<span id="Sx2.p1.1.3" class="ltx_text"></span> questions and answers combined. Using a Kolmogorov-Smirnov test to determine whether the underlying distributions of the two datasets differ, we find a significant difference for all three parts of speech (p <math id="Sx2.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="Sx2.p1.1.m1.1a"><mo id="Sx2.p1.1.m1.1.1" xref="Sx2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Sx2.p1.1.m1.1b"><lt id="Sx2.p1.1.m1.1.1.cmml" xref="Sx2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p1.1.m1.1c">&lt;</annotation></semantics></math> .001) <span id="Sx2.p1.1.4" class="ltx_text" style="color:#000000;">for both real images and abstract scenes</span>. This helps motivate the VQA task as a way to learn information about visual scenes; although both captions and questions &amp; answers provide information about the visual world, they do it from different perspectives, with different underlying biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and can function as complementary to one another.</p>
</div>
<div id="Sx2.p2" class="ltx_para ltx_noindent">
<p id="Sx2.p2.1" class="ltx_p">We illustrate the similarities and differences between the word distributions in captions vs. questions &amp; answers as Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with size indicating the normalized count â€“ Fig.Â <a href="#Sx2.F15" title="Figure 15 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> (nouns), Fig.Â <a href="#Sx2.F16" title="Figure 16 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> (verbs), and Fig.Â <a href="#Sx2.F17" title="Figure 17 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> (adjectives) <span id="Sx2.p2.1.1" class="ltx_text" style="color:#000000;">for real images and Fig.Â <a href="#Sx2.F18" title="Figure 18 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> (nouns), Fig.Â <a href="#Sx2.F19" title="Figure 19 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> (verbs), and Fig.Â <a href="#Sx2.F20" title="Figure 20 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> (adjectives) for abstract scenes</span>.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Visualization created using <a target="_blank" href="http://worditout.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://worditout.com/</a>.</span></span></span> The left side shows the top words in questions &amp; answers, the right the top words in captions, and the center the words common to both, with size indicating the harmonic mean of the counts.</p>
</div>
<div id="Sx2.p3" class="ltx_para ltx_noindent">
<p id="Sx2.p3.1" class="ltx_p">We see that adjectives in captions capture some clearly visual properties discussed in previous work on vision to language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, such as material and pattern, while the questions &amp; answers have more adjectives that capture what is usual (<em id="Sx2.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="Sx2.p3.1.2" class="ltx_text"></span>, <span id="Sx2.p3.1.3" class="ltx_text" style="color:#000000;">â€œdominantâ€, â€œapproximateâ€, â€œhigherâ€</span>) and other kinds of commonsense properties (<em id="Sx2.p3.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="Sx2.p3.1.5" class="ltx_text"></span>, <span id="Sx2.p3.1.6" class="ltx_text" style="color:#000000;">â€œedibleâ€, â€œpossibleâ€, â€œunsafeâ€, â€œacceptableâ€</span>). Interestingly, we see that question &amp; answer nouns capture information about <span id="Sx2.p3.1.7" class="ltx_text" style="color:#000000;">â€œethnicityâ€ and â€œhairstyleâ€</span>, while caption nouns capture information about <span id="Sx2.p3.1.8" class="ltx_text" style="color:#000000;">pluralized visible objects (<em id="Sx2.p3.1.8.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="Sx2.p3.1.8.2" class="ltx_text"></span>, â€œcellphonesâ€, â€œdaughtersâ€) and groups (<em id="Sx2.p3.1.8.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="Sx2.p3.1.8.4" class="ltx_text"></span>, â€œtrioâ€, â€œsomeâ€), among other differences.</span> â€œManâ€ and â€œpeopleâ€ are common in both captions and questions &amp; answers.</p>
</div>
<div id="Sx2.p4" class="ltx_para ltx_noindent">
<p id="Sx2.p4.1" class="ltx_p">One key piece to understanding the visual world is understanding spatial relationships, and so we additionally extract spatial prepositions and plot their proportions in the captions <em id="Sx2.p4.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="Sx2.p4.1.2" class="ltx_text"></span>Â the questions &amp; answers data in Fig.Â <a href="#Sx2.F14" title="Figure 14 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> (left) for real images and Fig.Â <a href="#Sx2.F14" title="Figure 14 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> (right) for abstract scenes. We see that questions &amp; answers have a higher proportion of specific spatial relations (<em id="Sx2.p4.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="Sx2.p4.1.4" class="ltx_text"></span>, â€œinâ€, â€œonâ€) compared to captions, which have a higher proportion of general spatial relations (<em id="Sx2.p4.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="Sx2.p4.1.6" class="ltx_text"></span>, â€œwithâ€, <span id="Sx2.p4.1.7" class="ltx_text" style="color:#000000;">â€œnearâ€</span>).</p>
</div>
<figure id="Sx2.F14" class="ltx_figure"><img src="/html/1505.00468/assets/x14.png" id="Sx2.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Proportions of spatial prepositions in the captions and question &amp; answers for real images (left) and abstract scenes (right).</figcaption>
</figure>
<figure id="Sx2.F15" class="ltx_figure"><img src="/html/1505.00468/assets/figures/coco_nouns.png" id="Sx2.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for nouns with size indicating the normalized count <span id="Sx2.F15.2.1" class="ltx_text" style="color:#000000;">for real images</span>.</figcaption>
</figure>
<figure id="Sx2.F16" class="ltx_figure"><img src="/html/1505.00468/assets/figures/coco_verbs.png" id="Sx2.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for verbs with size indicating the normalized count <span id="Sx2.F16.2.1" class="ltx_text" style="color:#000000;">for real images</span>.</figcaption>
</figure>
<figure id="Sx2.F17" class="ltx_figure"><img src="/html/1505.00468/assets/figures/coco_adjectives.png" id="Sx2.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for adjectives with size indicating the normalized count <span id="Sx2.F17.2.1" class="ltx_text" style="color:#000000;">for real images</span>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="Sx2.F18" class="ltx_figure"><img src="/html/1505.00468/assets/figures/abstract_nouns.png" id="Sx2.F18.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for nouns with size indicating the normalized count <span id="Sx2.F18.2.1" class="ltx_text" style="color:#000000;">for abstract scenes</span>.</figcaption>
</figure>
<figure id="Sx2.F19" class="ltx_figure"><img src="/html/1505.00468/assets/figures/abstract_verbs.png" id="Sx2.F19.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for verbs with size indicating the normalized count <span id="Sx2.F19.2.1" class="ltx_text" style="color:#000000;">for abstract scenes</span>.</figcaption>
</figure>
<figure id="Sx2.F20" class="ltx_figure"><img src="/html/1505.00468/assets/figures/abstract_adjectives.png" id="Sx2.F20.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Venn-style word clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for adjectives with size indicating the normalized count <span id="Sx2.F20.2.1" class="ltx_text" style="color:#000000;">for abstract scenes</span>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="Sx2.F21" class="ltx_figure"><img src="/html/1505.00468/assets/x15.png" id="Sx2.F21.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span><span id="Sx2.F21.2.1" class="ltx_text" style="color:#000000;">Distribution of questions starting with â€œWhat isâ€ by their first five words for a random sample of 60K questions for real images (left) and all questions for abstract scenes (right). The ordering of the words starts towards the center and radiates outwards. The arc length is proportional to the number of questions containing the word. White areas are words with contributions too small to show.</span></figcaption>
</figure>
<figure id="Sx2.F22" class="ltx_figure"><img src="/html/1505.00468/assets/x16.png" id="Sx2.F22.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span><span id="Sx2.F22.2.1" class="ltx_text" style="color:#000000;">Distribution of answers for questions starting with â€œWhat isâ€ for a random sample of 60K questions for real images (top) and all questions for abstract scenes (bottom). Each column corresponds to questions ending in different words, such as â€œdoing?â€, â€œon?â€, <em id="Sx2.F22.2.1.1" class="ltx_emph ltx_font_italic">etc</em>.</span></figcaption>
</figure>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix II: â€œWhat isâ€ Analysis</h2>

<div id="Sx3.p1" class="ltx_para ltx_noindent">
<p id="Sx3.p1.1" class="ltx_p">In Fig.Â <a href="#Sx2.F21" title="Figure 21 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>, we show the distribution of questions starting with â€œWhat isâ€ by their first five words for <span id="Sx3.p1.1.1" class="ltx_text" style="color:#000000;">both real images and abstract scenes</span>. Note the diversity of objects referenced in the questions, as well as, the relations between objects, such as â€œholdingâ€ and <span id="Sx3.p1.1.2" class="ltx_text" style="color:#000000;">â€œsitting onâ€</span>. In Fig.Â <a href="#Sx2.F22" title="Figure 22 â€£ Appendix I: Captions vs. Questions â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>, we show the distribution of answers for â€œWhat isâ€ questions ending in different words. For instance, questions ending in â€œeatingâ€ have answers such as â€œpizzaâ€, <span id="Sx3.p1.1.3" class="ltx_text" style="color:#000000;">â€œwatermelonâ€ and â€œhot dogâ€</span>. Notice the diversity in answers for some questions, such as those that end with â€œfor?â€ or <span id="Sx3.p1.1.4" class="ltx_text" style="color:#000000;">â€œpicture?â€</span>. Other questions result in intuitive responses, such as â€œholding?â€ and the response â€œumbrellaâ€.</p>
</div>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix III: Multiple-Choice Human Accuracy</h2>

<div id="Sx4.p1" class="ltx_para ltx_noindent">
<p id="Sx4.p1.2" class="ltx_p">To compute human accuracy for multiple-choice questions, we collected <span id="Sx4.p1.2.3" class="ltx_text" style="color:#000000;">three</span> human answers per question on a random subset of 3,000 questions <span id="Sx4.p1.2.4" class="ltx_text" style="color:#000000;">for both real images and abstract scenes.</span> In TableÂ <a href="#Sx4.T6" title="TABLE VI â€£ Appendix III: Multiple-Choice Human Accuracy â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, we show the human accuracies for multiple choice questions. TableÂ <a href="#Sx4.T6" title="TABLE VI â€£ Appendix III: Multiple-Choice Human Accuracy â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> also shows the inter-human agreement for open-ended answer task. In comparison to open-ended answer, the multiple-choice accuracies are <span id="Sx4.p1.2.5" class="ltx_text" style="color:#000000;">more or less same</span>
for â€œyes/noâ€ questions and significantly better <span id="Sx4.p1.2.2" class="ltx_text" style="color:#000000;">(<math id="Sx4.p1.1.1.m1.1" class="ltx_Math" alttext="\approx 15\%" display="inline"><semantics id="Sx4.p1.1.1.m1.1a"><mrow id="Sx4.p1.1.1.m1.1.1" xref="Sx4.p1.1.1.m1.1.1.cmml"><mi id="Sx4.p1.1.1.m1.1.1.2" xref="Sx4.p1.1.1.m1.1.1.2.cmml"></mi><mo mathcolor="#000000" id="Sx4.p1.1.1.m1.1.1.1" xref="Sx4.p1.1.1.m1.1.1.1.cmml">â‰ˆ</mo><mrow id="Sx4.p1.1.1.m1.1.1.3" xref="Sx4.p1.1.1.m1.1.1.3.cmml"><mn mathcolor="#000000" id="Sx4.p1.1.1.m1.1.1.3.2" xref="Sx4.p1.1.1.m1.1.1.3.2.cmml">15</mn><mo mathcolor="#000000" id="Sx4.p1.1.1.m1.1.1.3.1" xref="Sx4.p1.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx4.p1.1.1.m1.1b"><apply id="Sx4.p1.1.1.m1.1.1.cmml" xref="Sx4.p1.1.1.m1.1.1"><approx id="Sx4.p1.1.1.m1.1.1.1.cmml" xref="Sx4.p1.1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="Sx4.p1.1.1.m1.1.1.2.cmml" xref="Sx4.p1.1.1.m1.1.1.2">absent</csymbol><apply id="Sx4.p1.1.1.m1.1.1.3.cmml" xref="Sx4.p1.1.1.m1.1.1.3"><csymbol cd="latexml" id="Sx4.p1.1.1.m1.1.1.3.1.cmml" xref="Sx4.p1.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="Sx4.p1.1.1.m1.1.1.3.2.cmml" xref="Sx4.p1.1.1.m1.1.1.3.2">15</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p1.1.1.m1.1c">\approx 15\%</annotation></semantics></math> increase for real images and <math id="Sx4.p1.2.2.m2.1" class="ltx_Math" alttext="\approx 11\%" display="inline"><semantics id="Sx4.p1.2.2.m2.1a"><mrow id="Sx4.p1.2.2.m2.1.1" xref="Sx4.p1.2.2.m2.1.1.cmml"><mi id="Sx4.p1.2.2.m2.1.1.2" xref="Sx4.p1.2.2.m2.1.1.2.cmml"></mi><mo mathcolor="#000000" id="Sx4.p1.2.2.m2.1.1.1" xref="Sx4.p1.2.2.m2.1.1.1.cmml">â‰ˆ</mo><mrow id="Sx4.p1.2.2.m2.1.1.3" xref="Sx4.p1.2.2.m2.1.1.3.cmml"><mn mathcolor="#000000" id="Sx4.p1.2.2.m2.1.1.3.2" xref="Sx4.p1.2.2.m2.1.1.3.2.cmml">11</mn><mo mathcolor="#000000" id="Sx4.p1.2.2.m2.1.1.3.1" xref="Sx4.p1.2.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx4.p1.2.2.m2.1b"><apply id="Sx4.p1.2.2.m2.1.1.cmml" xref="Sx4.p1.2.2.m2.1.1"><approx id="Sx4.p1.2.2.m2.1.1.1.cmml" xref="Sx4.p1.2.2.m2.1.1.1"></approx><csymbol cd="latexml" id="Sx4.p1.2.2.m2.1.1.2.cmml" xref="Sx4.p1.2.2.m2.1.1.2">absent</csymbol><apply id="Sx4.p1.2.2.m2.1.1.3.cmml" xref="Sx4.p1.2.2.m2.1.1.3"><csymbol cd="latexml" id="Sx4.p1.2.2.m2.1.1.3.1.cmml" xref="Sx4.p1.2.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="Sx4.p1.2.2.m2.1.1.3.2.cmml" xref="Sx4.p1.2.2.m2.1.1.3.2">11</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p1.2.2.m2.1c">\approx 11\%</annotation></semantics></math> increase for abstract scenes)</span> for â€œotherâ€ questions. Since â€œotherâ€ questions may be ambiguous, the increase in accuracy using multiple choice is not surprising.</p>
</div>
<figure id="Sx4.T6" class="ltx_table">
<table id="Sx4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T6.1.1.1" class="ltx_tr">
<th id="Sx4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="Sx4.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Accuracy Metric</span></th>
<th id="Sx4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.1.1.3.1" class="ltx_text" style="font-size:90%;">All</span></th>
<th id="Sx4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Yes/No</span></th>
<th id="Sx4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Number</span></th>
<th id="Sx4.T6.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T6.1.2.1" class="ltx_tr">
<th id="Sx4.T6.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="Sx4.T6.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.2.1.2.1" class="ltx_text" style="font-size:90%;">MC majority vote</span></th>
<td id="Sx4.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.2.1.3.1" class="ltx_text" style="font-size:90%;color:#000000;">91.54</span></td>
<td id="Sx4.T6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.2.1.4.1" class="ltx_text" style="font-size:90%;color:#000000;">97.40</span></td>
<td id="Sx4.T6.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.2.1.5.1" class="ltx_text" style="font-size:90%;color:#000000;">86.97</span></td>
<td id="Sx4.T6.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.2.1.6.1" class="ltx_text" style="font-size:90%;color:#000000;">87.91</span></td>
</tr>
<tr id="Sx4.T6.1.3.2" class="ltx_tr">
<th id="Sx4.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Real</span></th>
<th id="Sx4.T6.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.3.2.2.1" class="ltx_text" style="font-size:90%;">MC average</span></th>
<td id="Sx4.T6.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.3.2.3.1" class="ltx_text" style="font-size:90%;color:#000000;">88.53</span></td>
<td id="Sx4.T6.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.3.2.4.1" class="ltx_text" style="font-size:90%;color:#000000;">94.40</span></td>
<td id="Sx4.T6.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.3.2.5.1" class="ltx_text" style="font-size:90%;color:#000000;">84.99</span></td>
<td id="Sx4.T6.1.3.2.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.3.2.6.1" class="ltx_text" style="font-size:90%;color:#000000;">84.64</span></td>
</tr>
<tr id="Sx4.T6.1.4.3" class="ltx_tr">
<th id="Sx4.T6.1.4.3.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="Sx4.T6.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.4.3.2.1" class="ltx_text" style="font-size:90%;color:#000000;">Open-Ended</span></th>
<td id="Sx4.T6.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.4.3.3.1" class="ltx_text" style="font-size:90%;color:#000000;">80.62</span></td>
<td id="Sx4.T6.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.4.3.4.1" class="ltx_text" style="font-size:90%;color:#000000;">94.78</span></td>
<td id="Sx4.T6.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.4.3.5.1" class="ltx_text" style="font-size:90%;color:#000000;">78.46</span></td>
<td id="Sx4.T6.1.4.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.4.3.6.1" class="ltx_text" style="font-size:90%;color:#000000;">69.69</span></td>
</tr>
<tr id="Sx4.T6.1.5.4" class="ltx_tr">
<th id="Sx4.T6.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="Sx4.T6.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.5.4.2.1" class="ltx_text" style="font-size:90%;">MC majority vote</span></th>
<td id="Sx4.T6.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.5.4.3.1" class="ltx_text" style="font-size:90%;color:#000000;">93.57</span></td>
<td id="Sx4.T6.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.5.4.4.1" class="ltx_text" style="font-size:90%;color:#000000;">97.78</span></td>
<td id="Sx4.T6.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.5.4.5.1" class="ltx_text" style="font-size:90%;color:#000000;">96.71</span></td>
<td id="Sx4.T6.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.5.4.6.1" class="ltx_text" style="font-size:90%;color:#000000;">88.73</span></td>
</tr>
<tr id="Sx4.T6.1.6.5" class="ltx_tr">
<th id="Sx4.T6.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Abstract</span></th>
<th id="Sx4.T6.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.6.5.2.1" class="ltx_text" style="font-size:90%;">MC average</span></th>
<td id="Sx4.T6.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.6.5.3.1" class="ltx_text" style="font-size:90%;color:#000000;">90.40</span></td>
<td id="Sx4.T6.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.6.5.4.1" class="ltx_text" style="font-size:90%;color:#000000;">94.59</span></td>
<td id="Sx4.T6.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.6.5.5.1" class="ltx_text" style="font-size:90%;color:#000000;">94.36</span></td>
<td id="Sx4.T6.1.6.5.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.6.5.6.1" class="ltx_text" style="font-size:90%;color:#000000;">85.32</span></td>
</tr>
<tr id="Sx4.T6.1.7.6" class="ltx_tr">
<th id="Sx4.T6.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="Sx4.T6.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.7.6.2.1" class="ltx_text" style="font-size:90%;color:#000000;">Open-Ended</span></th>
<td id="Sx4.T6.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.7.6.3.1" class="ltx_text" style="font-size:90%;color:#000000;">85.66</span></td>
<td id="Sx4.T6.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.7.6.4.1" class="ltx_text" style="font-size:90%;color:#000000;">95.32</span></td>
<td id="Sx4.T6.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.7.6.5.1" class="ltx_text" style="font-size:90%;color:#000000;">94.17</span></td>
<td id="Sx4.T6.1.7.6.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="Sx4.T6.1.7.6.6.1" class="ltx_text" style="font-size:90%;color:#000000;">74.12</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span><span id="Sx4.T6.3.1" class="ltx_text" style="color:#000000;">For each of the two datasets, real and abstract, first two rows are the human accuracies for multiple-choice questions when subjects were shown both the image and the question. Majority vote means we consider the answer picked by majority of the three subjects to be the predicted answer by humans and compute accuracy of that answer for each question. Average means we compute the accuracy of each of the answers picked by the subjects and record their average for each question. The last row is the inter-human agreement for open-ended answers task when subjects were shown both the image and the question. All accuracies are evaluated on a random subset of 3000 questions.</span></figcaption>
</figure>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix IV: Details on VQA baselines</h2>

<div id="Sx5.p1" class="ltx_para ltx_noindent">
<p id="Sx5.p1.1" class="ltx_p"><span id="Sx5.p1.1.1" class="ltx_text ltx_font_bold">â€œper Q-type priorâ€ baseline.</span> We decide on different question types based on first few words of questions in the real images training set and ensure that each question type has at least 30 questions in the training dataset. The most popular answer for each question type is also computed on real images training set.</p>
</div>
<div id="Sx5.p2" class="ltx_para ltx_noindent">
<p id="Sx5.p2.3" class="ltx_p"><span id="Sx5.p2.3.1" class="ltx_text ltx_font_bold">â€œnearest neighborâ€ baseline.</span> For every question in the VQA test-standard set, we find its <math id="Sx5.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="Sx5.p2.1.m1.1a"><mi id="Sx5.p2.1.m1.1.1" xref="Sx5.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Sx5.p2.1.m1.1b"><ci id="Sx5.p2.1.m1.1.1.cmml" xref="Sx5.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.1.m1.1c">k</annotation></semantics></math> nearest neighbor questions in the training set using cosine similarity in Skip-Thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> feature space. We also experimented with bag of words and Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> feature spaces but we obtained the best performance with Skip-Thought. In this set of <math id="Sx5.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="Sx5.p2.2.m2.1a"><mi id="Sx5.p2.2.m2.1.1" xref="Sx5.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Sx5.p2.2.m2.1b"><ci id="Sx5.p2.2.m2.1.1.cmml" xref="Sx5.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.2.m2.1c">k</annotation></semantics></math> questions and their associated images, we find the image which is most similar to the query image using cosine similarity in fc7 feature space. We use the fc7 features from the caffenet model in BVLC Caffe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The most common ground truth answer of this most similar image and question pair is the predicted answer for the query image and question pair. We pick <math id="Sx5.p2.3.m3.1" class="ltx_Math" alttext="k=4" display="inline"><semantics id="Sx5.p2.3.m3.1a"><mrow id="Sx5.p2.3.m3.1.1" xref="Sx5.p2.3.m3.1.1.cmml"><mi id="Sx5.p2.3.m3.1.1.2" xref="Sx5.p2.3.m3.1.1.2.cmml">k</mi><mo id="Sx5.p2.3.m3.1.1.1" xref="Sx5.p2.3.m3.1.1.1.cmml">=</mo><mn id="Sx5.p2.3.m3.1.1.3" xref="Sx5.p2.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx5.p2.3.m3.1b"><apply id="Sx5.p2.3.m3.1.1.cmml" xref="Sx5.p2.3.m3.1.1"><eq id="Sx5.p2.3.m3.1.1.1.cmml" xref="Sx5.p2.3.m3.1.1.1"></eq><ci id="Sx5.p2.3.m3.1.1.2.cmml" xref="Sx5.p2.3.m3.1.1.2">ğ‘˜</ci><cn type="integer" id="Sx5.p2.3.m3.1.1.3.cmml" xref="Sx5.p2.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.3.m3.1c">k=4</annotation></semantics></math> on the test-dev set.</p>
</div>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix V: â€œAgeâ€ and â€œCommonsenseâ€ of our model</h2>

<div id="Sx6.p1" class="ltx_para ltx_noindent">
<p id="Sx6.p1.1" class="ltx_p">We estimate the age and degree of commonsense of our <span id="Sx6.p1.1.1" class="ltx_text ltx_font_bold">best model</span> (deeper LSTM Q + norm I), selected using VQA test-dev accuracies). To estimate the age, we compute a weighted average of the average age per question, weighted by the accuracy of the modelâ€™s predicted answer for that question, on the subset of questions in the VQA validation set for which we have age annotations (how old a human needs to be to answer the question correctly). To estimate the degree of commonsense, we compute a weighted average of the average degree of commonsense per question, weighted by the accuracy of the modelâ€™s predicted answer for that question, on the subset of questions in the VQA validation set for which we have commonsense annotations (whether the question requires commonsense to answer it).</p>
</div>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix VI: Abstract Scenes Dataset</h2>

<div id="Sx7.p1" class="ltx_para ltx_noindent">
<p id="Sx7.p1.1" class="ltx_p">In Fig.Â <a href="#Sx7.F23" title="Figure 23 â€£ Appendix VI: Abstract Scenes Dataset â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a> (left), we show a subset of the objects that are present in the abstract scenes dataset. For more examples of the scenes generated, please see Fig.Â <a href="#Sx10.F28" title="Figure 28 â€£ Appendix IX: Additional Examples â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28</span></a>. The user interface used to create the scenes is shown in Fig.Â <a href="#Sx7.F23" title="Figure 23 â€£ Appendix VI: Abstract Scenes Dataset â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a> (right). Subjects used a drag-and-drop interface to create the scenes. Each object could be flipped horizontally and scaled. The scale of the object determined the rendering order of the objects. Many objects have different attributes corresponding to different poses or types. Most animals have five different discrete poses. Humans have eight discrete expressions and their poses may be continuously adjusted using a â€œpaperdollâ€ model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<figure id="Sx7.F23" class="ltx_figure"><img src="/html/1505.00468/assets/x17.png" id="Sx7.F23.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Left: A small subset of the objects present in the abstract scene dataset. Right: The AMT interface for collecting abstract scenes.
The light green circles indicate where users can select to manipulate a personâ€™s pose. Different objects may be added to the scene using the folders to the right.</figcaption>
</figure>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix VII: User Interfaces</h2>

<div id="Sx8.p1" class="ltx_para ltx_noindent">
<p id="Sx8.p1.1" class="ltx_p">In Fig.Â <a href="#Sx8.F24" title="Figure 24 â€£ Appendix VII: User Interfaces â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>, we show the AMT interface that we used to collect questions for images. Note that we tell the workers that the robot already knows the answer to the previously asked question(s), inspiring them to ask different kinds of questions, thereby increasing the diversity of our dataset.</p>
</div>
<div id="Sx8.p2" class="ltx_para ltx_noindent">
<p id="Sx8.p2.1" class="ltx_p">Fig.Â <a href="#Sx8.F25" title="Figure 25 â€£ Appendix VII: User Interfaces â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a> shows the AMT interface used for collecting answers to the previously collected questions when subjects were shown the corresponding images. Fig.Â <a href="#Sx8.F26" title="Figure 26 â€£ Appendix VII: User Interfaces â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">26</span></a> shows the interface that was used to collect answers to questions when subjects were not shown the corresponding image (<em id="Sx8.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="Sx8.p2.1.2" class="ltx_text"></span>, to help in gathering incorrect, but plausible, answers for the multiple-choice task and to assess how accurately the questions can be answered using common sense knowledge alone).</p>
</div>
<figure id="Sx8.F24" class="ltx_figure"><img src="/html/1505.00468/assets/figures/question_stage03.png" id="Sx8.F24.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="412" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Our AMT interface for collecting the third question for an image, when subjects were shown previous questions that were collected and were asked to ask a question different from previous questions.</figcaption>
</figure>
<figure id="Sx8.F25" class="ltx_figure"><img src="/html/1505.00468/assets/figures/answer_with_image.png" id="Sx8.F25.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>The AMT interface used to collect answers to a question when subjects were shown the image while answering the question.</figcaption>
</figure>
<figure id="Sx8.F26" class="ltx_figure"><img src="/html/1505.00468/assets/figures/answer_without_image.png" id="Sx8.F26.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="331" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>The AMT interface used to collect answers to a question when subjects were not shown the image while answering the question using only commonsense to collect the plausible, but incorrect, multiple-choice answers.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="Sx9" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix VIII: Answer Distribution</h2>

<div id="Sx9.p1" class="ltx_para ltx_noindent">
<p id="Sx9.p1.1" class="ltx_p">The top <math id="Sx9.p1.1.m1.1" class="ltx_Math" alttext="250" display="inline"><semantics id="Sx9.p1.1.m1.1a"><mn id="Sx9.p1.1.m1.1.1" xref="Sx9.p1.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="Sx9.p1.1.m1.1b"><cn type="integer" id="Sx9.p1.1.m1.1.1.cmml" xref="Sx9.p1.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx9.p1.1.m1.1c">250</annotation></semantics></math> answers in our <span id="Sx9.p1.1.1" class="ltx_text" style="color:#000000;">real images</span> dataset along with their counts and percentage counts are given below. The answers have been presented in different colors to show the different Part-of-Speech (POS) tagging of the answers with the following color code: <span id="Sx9.p1.1.2" class="ltx_text" style="color:#FF00FF;">yes/no</span>, <span id="Sx9.p1.1.3" class="ltx_text" style="color:#00FF00;">noun</span>, <span id="Sx9.p1.1.4" class="ltx_text" style="color:#0000FF;">verb</span>, <span id="Sx9.p1.1.5" class="ltx_text" style="color:#DCD20A;">adjective</span>, <span id="Sx9.p1.1.6" class="ltx_text" style="color:#00FFFF;">adverb</span>, and <span id="Sx9.p1.1.7" class="ltx_text" style="color:#FF0000;">numeral</span>.</p>
</div>
<div id="Sx9.p2" class="ltx_para ltx_noindent">
<p id="Sx9.p2.1" class="ltx_p"><span id="Sx9.p2.1.1" class="ltx_text" style="color:#FF00FF;">â€œyesâ€</span> (566613, 22.82%), <span id="Sx9.p2.1.2" class="ltx_text" style="color:#FF00FF;">â€œnoâ€</span> (381307, 15.35%), <span id="Sx9.p2.1.3" class="ltx_text" style="color:#FF0000;">â€œ2â€</span> (80031, 3.22%), <span id="Sx9.p2.1.4" class="ltx_text" style="color:#FF0000;">â€œ1â€</span> (46537, 1.87%), <span id="Sx9.p2.1.5" class="ltx_text" style="color:#DCD20A;">â€œwhiteâ€</span> (41753, 1.68%), <span id="Sx9.p2.1.6" class="ltx_text" style="color:#FF0000;">â€œ3â€</span> (41334, 1.66%), <span id="Sx9.p2.1.7" class="ltx_text" style="color:#DCD20A;">â€œredâ€</span> (33834, 1.36%), <span id="Sx9.p2.1.8" class="ltx_text" style="color:#DCD20A;">â€œblueâ€</span> (28881, 1.16%), <span id="Sx9.p2.1.9" class="ltx_text" style="color:#FF0000;">â€œ4â€</span> (27174, 1.09%), <span id="Sx9.p2.1.10" class="ltx_text" style="color:#DCD20A;">â€œgreenâ€</span> (22453, 0.9%), <span id="Sx9.p2.1.11" class="ltx_text" style="color:#DCD20A;">â€œblackâ€</span> (21852, 0.88%), <span id="Sx9.p2.1.12" class="ltx_text" style="color:#DCD20A;">â€œyellowâ€</span> (17312, 0.7%), <span id="Sx9.p2.1.13" class="ltx_text" style="color:#DCD20A;">â€œbrownâ€</span> (14488, 0.58%), <span id="Sx9.p2.1.14" class="ltx_text" style="color:#FF0000;">â€œ5â€</span> (14373, 0.58%), <span id="Sx9.p2.1.15" class="ltx_text" style="color:#00FF00;">â€œtennisâ€</span> (10941, 0.44%),<span id="Sx9.p2.1.16" class="ltx_text" style="color:#00FF00;">â€œbaseballâ€</span> (10299, 0.41%), <span id="Sx9.p2.1.17" class="ltx_text" style="color:#FF0000;">â€œ6â€</span> (10103, 0.41%), <span id="Sx9.p2.1.18" class="ltx_text" style="color:#00FF00;">â€œorangeâ€</span> (9136, 0.37%), <span id="Sx9.p2.1.19" class="ltx_text" style="color:#FF0000;">â€œ0â€</span> (8812, 0.35%), <span id="Sx9.p2.1.20" class="ltx_text" style="color:#00FF00;">â€œbathroomâ€</span> (8473, 0.34%), <span id="Sx9.p2.1.21" class="ltx_text" style="color:#00FF00;">â€œwoodâ€</span> (8219, 0.33%), <span id="Sx9.p2.1.22" class="ltx_text" style="color:#00FFFF;">â€œrightâ€</span> (8209, 0.33%), <span id="Sx9.p2.1.23" class="ltx_text" style="color:#00FFFF;">â€œleftâ€</span> (8058, 0.32%), <span id="Sx9.p2.1.24" class="ltx_text" style="color:#00FF00;">â€œfrisbeeâ€</span> (7671, 0.31%), <span id="Sx9.p2.1.25" class="ltx_text" style="color:#DCD20A;">â€œpinkâ€</span> (7519, 0.3%), <span id="Sx9.p2.1.26" class="ltx_text" style="color:#DCD20A;">â€œgrayâ€</span> (7385, 0.3%), <span id="Sx9.p2.1.27" class="ltx_text" style="color:#00FF00;">â€œpizzaâ€</span> (6892, 0.28%), <span id="Sx9.p2.1.28" class="ltx_text" style="color:#FF0000;">â€œ7â€</span> (6005, 0.24%), <span id="Sx9.p2.1.29" class="ltx_text" style="color:#00FF00;">â€œkitchenâ€</span> (5926, 0.24%), <span id="Sx9.p2.1.30" class="ltx_text" style="color:#FF0000;">â€œ8â€</span> (5592, 0.23%), <span id="Sx9.p2.1.31" class="ltx_text" style="color:#00FF00;">â€œcatâ€</span> (5514, 0.22%), <span id="Sx9.p2.1.32" class="ltx_text" style="color:#00FF00;">â€œskiingâ€</span> (5189, 0.21%), <span id="Sx9.p2.1.33" class="ltx_text" style="color:#0000FF;">â€œskateboardingâ€</span> (5122, 0.21%), <span id="Sx9.p2.1.34" class="ltx_text" style="color:#00FF00;">â€œdogâ€</span> (5092, 0.21%), <span id="Sx9.p2.1.35" class="ltx_text" style="color:#00FF00;">â€œsnowâ€</span> (4867, 0.2%), <span id="Sx9.p2.1.36" class="ltx_text" style="color:#DCD20A;">â€œblack and whiteâ€</span> (4852, 0.2%), <span id="Sx9.p2.1.37" class="ltx_text" style="color:#00FF00;">â€œskateboardâ€</span> (4697, 0.19%), <span id="Sx9.p2.1.38" class="ltx_text" style="color:#0000FF;">â€œsurfingâ€</span> (4544, 0.18%), <span id="Sx9.p2.1.39" class="ltx_text" style="color:#00FF00;">â€œwaterâ€</span> (4513, 0.18%), <span id="Sx9.p2.1.40" class="ltx_text" style="color:#00FF00;">â€œgiraffeâ€</span> (4027, 0.16%), <span id="Sx9.p2.1.41" class="ltx_text" style="color:#00FF00;">â€œgrassâ€</span> (3979, 0.16%), <span id="Sx9.p2.1.42" class="ltx_text" style="color:#00FF00;">â€œsurfboardâ€</span> (3934, 0.16%), <span id="Sx9.p2.1.43" class="ltx_text" style="color:#00FF00;">â€œwiiâ€</span> (3898, 0.16%), <span id="Sx9.p2.1.44" class="ltx_text" style="color:#00FF00;">â€œkiteâ€</span> (3852, 0.16%), <span id="Sx9.p2.1.45" class="ltx_text" style="color:#FF0000;">â€œ10â€</span> (3756, 0.15%), <span id="Sx9.p2.1.46" class="ltx_text" style="color:#DCD20A;">â€œpurpleâ€</span> (3722, 0.15%), <span id="Sx9.p2.1.47" class="ltx_text" style="color:#00FF00;">â€œelephantâ€</span> (3646, 0.15%), <span id="Sx9.p2.1.48" class="ltx_text" style="color:#00FF00;">â€œbroccoliâ€</span> (3604, 0.15%), <span id="Sx9.p2.1.49" class="ltx_text" style="color:#00FF00;">â€œmanâ€</span> (3590, 0.14%), <span id="Sx9.p2.1.50" class="ltx_text" style="color:#00FF00;">â€œwinterâ€</span> (3490, 0.14%), <span id="Sx9.p2.1.51" class="ltx_text" style="color:#00FF00;">â€œstopâ€</span> (3413, 0.14%), <span id="Sx9.p2.1.52" class="ltx_text" style="color:#00FF00;">â€œtrainâ€</span> (3226, 0.13%), <span id="Sx9.p2.1.53" class="ltx_text" style="color:#FF0000;">â€œ9â€</span> (3217, 0.13%), <span id="Sx9.p2.1.54" class="ltx_text" style="color:#00FF00;">â€œappleâ€</span> (3189, 0.13%), <span id="Sx9.p2.1.55" class="ltx_text" style="color:#00FF00;">â€œsilverâ€</span> (3186, 0.13%), <span id="Sx9.p2.1.56" class="ltx_text" style="color:#00FF00;">â€œhorseâ€</span> (3159, 0.13%), <span id="Sx9.p2.1.57" class="ltx_text" style="color:#00FF00;">â€œbananaâ€</span> (3151, 0.13%), <span id="Sx9.p2.1.58" class="ltx_text" style="color:#00FF00;">â€œumbrellaâ€</span> (3139, 0.13%), <span id="Sx9.p2.1.59" class="ltx_text" style="color:#0000FF;">â€œeatingâ€</span> (3117, 0.13%), <span id="Sx9.p2.1.60" class="ltx_text" style="color:#00FF00;">â€œsheepâ€</span> (2927, 0.12%), <span id="Sx9.p2.1.61" class="ltx_text" style="color:#00FF00;">â€œbearâ€</span> (2803, 0.11%), <span id="Sx9.p2.1.62" class="ltx_text" style="color:#00FF00;">â€œphoneâ€</span> (2772, 0.11%), <span id="Sx9.p2.1.63" class="ltx_text" style="color:#FF0000;">â€œ12â€</span> (2633, 0.11%), <span id="Sx9.p2.1.64" class="ltx_text" style="color:#00FF00;">â€œmotorcycleâ€</span> (2608, 0.11%), <span id="Sx9.p2.1.65" class="ltx_text" style="color:#00FF00;">â€œcakeâ€</span> (2602, 0.1%), <span id="Sx9.p2.1.66" class="ltx_text" style="color:#00FF00;">â€œwineâ€</span> (2574, 0.1%), <span id="Sx9.p2.1.67" class="ltx_text" style="color:#00FF00;">â€œbeachâ€</span> (2536, 0.1%), <span id="Sx9.p2.1.68" class="ltx_text" style="color:#00FF00;">â€œsoccerâ€</span> (2504, 0.1%), <span id="Sx9.p2.1.69" class="ltx_text" style="color:#DCD20A;">â€œsunnyâ€</span> (2475, 0.1%), <span id="Sx9.p2.1.70" class="ltx_text" style="color:#00FF00;">â€œzebraâ€</span> (2403, 0.1%), <span id="Sx9.p2.1.71" class="ltx_text" style="color:#DCD20A;">â€œtanâ€</span> (2402, 0.1%), <span id="Sx9.p2.1.72" class="ltx_text" style="color:#00FF00;">â€œbrickâ€</span> (2395, 0.1%), <span id="Sx9.p2.1.73" class="ltx_text" style="color:#00FF00;">â€œfemaleâ€</span> (2372, 0.1%), <span id="Sx9.p2.1.74" class="ltx_text" style="color:#00FF00;">â€œbananasâ€</span> (2350, 0.09%), <span id="Sx9.p2.1.75" class="ltx_text" style="color:#00FF00;">â€œtableâ€</span> (2331, 0.09%), <span id="Sx9.p2.1.76" class="ltx_text" style="color:#00FF00;">â€œlaptopâ€</span> (2316, 0.09%), <span id="Sx9.p2.1.77" class="ltx_text" style="color:#00FF00;">â€œhatâ€</span> (2277, 0.09%), <span id="Sx9.p2.1.78" class="ltx_text" style="color:#00FF00;">â€œbenchâ€</span> (2259, 0.09%), <span id="Sx9.p2.1.79" class="ltx_text" style="color:#00FF00;">â€œflowersâ€</span> (2219, 0.09%), <span id="Sx9.p2.1.80" class="ltx_text" style="color:#00FF00;">â€œwomanâ€</span> (2197, 0.09%), <span id="Sx9.p2.1.81" class="ltx_text" style="color:#00FF00;">â€œmaleâ€</span> (2170, 0.09%), <span id="Sx9.p2.1.82" class="ltx_text" style="color:#00FF00;">â€œcowâ€</span> (2084, 0.08%), <span id="Sx9.p2.1.83" class="ltx_text" style="color:#00FF00;">â€œfoodâ€</span> (2083, 0.08%), <span id="Sx9.p2.1.84" class="ltx_text" style="color:#00FF00;">â€œliving roomâ€</span> (2022, 0.08%), <span id="Sx9.p2.1.85" class="ltx_text" style="color:#00FF00;">â€œbusâ€</span> (2011, 0.08%), <span id="Sx9.p2.1.86" class="ltx_text" style="color:#00FF00;">â€œsnowboardingâ€</span> (1990, 0.08%), <span id="Sx9.p2.1.87" class="ltx_text" style="color:#00FF00;">â€œkitesâ€</span> (1979, 0.08%), <span id="Sx9.p2.1.88" class="ltx_text" style="color:#00FF00;">â€œcell phoneâ€</span> (1943, 0.08%), <span id="Sx9.p2.1.89" class="ltx_text" style="color:#00FF00;">â€œhelmetâ€</span> (1885, 0.08%), <span id="Sx9.p2.1.90" class="ltx_text" style="color:#00FFFF;">â€œmaybeâ€</span> (1853, 0.07%), <span id="Sx9.p2.1.91" class="ltx_text" style="color:#00FFFF;">â€œoutsideâ€</span> (1846, 0.07%), <span id="Sx9.p2.1.92" class="ltx_text" style="color:#00FF00;">â€œhot dogâ€</span> (1809, 0.07%), <span id="Sx9.p2.1.93" class="ltx_text" style="color:#00FF00;">â€œnightâ€</span> (1805, 0.07%), <span id="Sx9.p2.1.94" class="ltx_text" style="color:#00FF00;">â€œtreesâ€</span> (1785, 0.07%), <span id="Sx9.p2.1.95" class="ltx_text" style="color:#FF0000;">â€œ11â€</span> (1753, 0.07%), <span id="Sx9.p2.1.96" class="ltx_text" style="color:#00FF00;">â€œbirdâ€</span> (1739, 0.07%), <span id="Sx9.p2.1.97" class="ltx_text" style="color:#00FFFF;">â€œdownâ€</span> (1732, 0.07%), <span id="Sx9.p2.1.98" class="ltx_text" style="color:#00FF00;">â€œbedâ€</span> (1587, 0.06%), <span id="Sx9.p2.1.99" class="ltx_text" style="color:#00FF00;">â€œcameraâ€</span> (1560, 0.06%), <span id="Sx9.p2.1.100" class="ltx_text" style="color:#00FF00;">â€œtreeâ€</span> (1547, 0.06%), <span id="Sx9.p2.1.101" class="ltx_text" style="color:#00FF00;">â€œchristmasâ€</span> (1544, 0.06%), <span id="Sx9.p2.1.102" class="ltx_text" style="color:#00FF00;">â€œfenceâ€</span> (1543, 0.06%), <span id="Sx9.p2.1.103" class="ltx_text" style="color:#00FF00;">â€œnothingâ€</span> (1538, 0.06%), <span id="Sx9.p2.1.104" class="ltx_text" style="color:#DCD20A;">â€œunknownâ€</span> (1532, 0.06%), <span id="Sx9.p2.1.105" class="ltx_text" style="color:#00FF00;">â€œtennis racketâ€</span> (1525, 0.06%), <span id="Sx9.p2.1.106" class="ltx_text" style="color:#DCD20A;">â€œred and whiteâ€</span> (1518, 0.06%), <span id="Sx9.p2.1.107" class="ltx_text" style="color:#00FF00;">â€œbedroomâ€</span> (1500, 0.06%), <span id="Sx9.p2.1.108" class="ltx_text" style="color:#00FF00;">â€œbatâ€</span> (1494, 0.06%), <span id="Sx9.p2.1.109" class="ltx_text" style="color:#00FF00;">â€œglassesâ€</span> (1491, 0.06%), <span id="Sx9.p2.1.110" class="ltx_text" style="color:#00FF00;">â€œtileâ€</span> (1487, 0.06%), <span id="Sx9.p2.1.111" class="ltx_text" style="color:#00FF00;">â€œmetalâ€</span> (1470, 0.06%), <span id="Sx9.p2.1.112" class="ltx_text" style="color:#DCD20A;">â€œblue and whiteâ€</span> (1440, 0.06%), <span id="Sx9.p2.1.113" class="ltx_text" style="color:#00FF00;">â€œforkâ€</span> (1439, 0.06%), <span id="Sx9.p2.1.114" class="ltx_text" style="color:#00FF00;">â€œplaneâ€</span> (1439, 0.06%), <span id="Sx9.p2.1.115" class="ltx_text" style="color:#00FF00;">â€œairportâ€</span> (1422, 0.06%), <span id="Sx9.p2.1.116" class="ltx_text" style="color:#DCD20A;">â€œcloudyâ€</span> (1413, 0.06%), <span id="Sx9.p2.1.117" class="ltx_text" style="color:#FF0000;">â€œ15â€</span> (1407, 0.06%), <span id="Sx9.p2.1.118" class="ltx_text" style="color:#00FFFF;">â€œupâ€</span> (1399, 0.06%), <span id="Sx9.p2.1.119" class="ltx_text" style="color:#DCD20A;">â€œblondeâ€</span> (1398, 0.06%), <span id="Sx9.p2.1.120" class="ltx_text" style="color:#00FF00;">â€œdayâ€</span> (1396, 0.06%), <span id="Sx9.p2.1.121" class="ltx_text" style="color:#00FF00;">â€œteddy bearâ€</span> (1386, 0.06%), <span id="Sx9.p2.1.122" class="ltx_text" style="color:#00FF00;">â€œglassâ€</span> (1379, 0.06%), <span id="Sx9.p2.1.123" class="ltx_text" style="color:#FF0000;">â€œ20â€</span> (1365, 0.05%), <span id="Sx9.p2.1.124" class="ltx_text" style="color:#00FF00;">â€œbeerâ€</span> (1345, 0.05%), <span id="Sx9.p2.1.125" class="ltx_text" style="color:#00FF00;">â€œcarâ€</span> (1331, 0.05%), <span id="Sx9.p2.1.126" class="ltx_text" style="color:#0000FF;">â€œsittingâ€</span> (1328, 0.05%), <span id="Sx9.p2.1.127" class="ltx_text" style="color:#00FF00;">â€œboatâ€</span> (1326, 0.05%), <span id="Sx9.p2.1.128" class="ltx_text" style="color:#0000FF;">â€œstandingâ€</span> (1326, 0.05%), <span id="Sx9.p2.1.129" class="ltx_text" style="color:#DCD20A;">â€œclearâ€</span> (1318, 0.05%), <span id="Sx9.p2.1.130" class="ltx_text" style="color:#FF0000;">â€œ13â€</span> (1318, 0.05%), <span id="Sx9.p2.1.131" class="ltx_text" style="color:#00FF00;">â€œnikeâ€</span> (1293, 0.05%), <span id="Sx9.p2.1.132" class="ltx_text" style="color:#00FF00;">â€œsandâ€</span> (1282, 0.05%), <span id="Sx9.p2.1.133" class="ltx_text" style="color:#DCD20A;">â€œopenâ€</span> (1279, 0.05%), <span id="Sx9.p2.1.134" class="ltx_text" style="color:#00FF00;">â€œcowsâ€</span> (1271, 0.05%), <span id="Sx9.p2.1.135" class="ltx_text" style="color:#00FF00;">â€œbikeâ€</span> (1267, 0.05%), <span id="Sx9.p2.1.136" class="ltx_text" style="color:#00FF00;">â€œchocolateâ€</span> (1266, 0.05%), <span id="Sx9.p2.1.137" class="ltx_text" style="color:#00FF00;">â€œdonutâ€</span> (1263, 0.05%), <span id="Sx9.p2.1.138" class="ltx_text" style="color:#00FF00;">â€œairplaneâ€</span> (1247, 0.05%), <span id="Sx9.p2.1.139" class="ltx_text" style="color:#00FF00;">â€œbirthdayâ€</span> (1241, 0.05%), <span id="Sx9.p2.1.140" class="ltx_text" style="color:#00FF00;">â€œcarrotsâ€</span> (1239, 0.05%), <span id="Sx9.p2.1.141" class="ltx_text" style="color:#00FF00;">â€œskisâ€</span> (1220, 0.05%), <span id="Sx9.p2.1.142" class="ltx_text" style="color:#00FF00;">â€œgirlâ€</span> (1220, 0.05%), <span id="Sx9.p2.1.143" class="ltx_text" style="color:#DCD20A;">â€œmanyâ€</span> (1211, 0.05%), <span id="Sx9.p2.1.144" class="ltx_text" style="color:#00FF00;">â€œzooâ€</span> (1204, 0.05%), <span id="Sx9.p2.1.145" class="ltx_text" style="color:#00FF00;">â€œsuitcaseâ€</span> (1199, 0.05%), <span id="Sx9.p2.1.146" class="ltx_text" style="color:#DCD20A;">â€œoldâ€</span> (1180, 0.05%), <span id="Sx9.p2.1.147" class="ltx_text" style="color:#00FF00;">â€œchairâ€</span> (1174, 0.05%), <span id="Sx9.p2.1.148" class="ltx_text" style="color:#DCD20A;">â€œbeigeâ€</span> (1170, 0.05%), <span id="Sx9.p2.1.149" class="ltx_text" style="color:#00FF00;">â€œballâ€</span> (1169, 0.05%), <span id="Sx9.p2.1.150" class="ltx_text" style="color:#00FF00;">â€œoceanâ€</span> (1168, 0.05%), <span id="Sx9.p2.1.151" class="ltx_text" style="color:#00FF00;">â€œsandwichâ€</span> (1168, 0.05%), <span id="Sx9.p2.1.152" class="ltx_text" style="color:#00FF00;">â€œtieâ€</span> (1166, 0.05%), <span id="Sx9.p2.1.153" class="ltx_text" style="color:#00FF00;">â€œhorsesâ€</span> (1163, 0.05%), <span id="Sx9.p2.1.154" class="ltx_text" style="color:#00FF00;">â€œpalmâ€</span> (1163, 0.05%), <span id="Sx9.p2.1.155" class="ltx_text" style="color:#00FF00;">â€œstripesâ€</span> (1155, 0.05%), <span id="Sx9.p2.1.156" class="ltx_text" style="color:#00FF00;">â€œfallâ€</span> (1146, 0.05%), <span id="Sx9.p2.1.157" class="ltx_text" style="color:#00FF00;">â€œcheeseâ€</span> (1142, 0.05%), <span id="Sx9.p2.1.158" class="ltx_text" style="color:#00FF00;">â€œscissorsâ€</span> (1134, 0.05%), <span id="Sx9.p2.1.159" class="ltx_text" style="color:#00FF00;">â€œroundâ€</span> (1125, 0.05%), <span id="Sx9.p2.1.160" class="ltx_text" style="color:#DCD20A;">â€œchineseâ€</span> (1123, 0.05%), <span id="Sx9.p2.1.161" class="ltx_text" style="color:#00FF00;">â€œknifeâ€</span> (1120, 0.05%), <span id="Sx9.p2.1.162" class="ltx_text" style="color:#FF0000;">â€œ14â€</span> (1110, 0.04%), <span id="Sx9.p2.1.163" class="ltx_text" style="color:#00FF00;">â€œtoiletâ€</span> (1099, 0.04%), <span id="Sx9.p2.1.164" class="ltx_text" style="color:#0000FF;">â€œdonâ€™t knowâ€</span> (1085, 0.04%), <span id="Sx9.p2.1.165" class="ltx_text" style="color:#00FF00;">â€œsnowboardâ€</span> (1083, 0.04%), <span id="Sx9.p2.1.166" class="ltx_text" style="color:#00FF00;">â€œtruckâ€</span> (1076, 0.04%), <span id="Sx9.p2.1.167" class="ltx_text" style="color:#00FF00;">â€œboyâ€</span> (1070, 0.04%), <span id="Sx9.p2.1.168" class="ltx_text" style="color:#00FF00;">â€œcoffeeâ€</span> (1070, 0.04%), <span id="Sx9.p2.1.169" class="ltx_text" style="color:#DCD20A;">â€œcoldâ€</span> (1064, 0.04%), <span id="Sx9.p2.1.170" class="ltx_text" style="color:#00FF00;">â€œfruitâ€</span> (1064, 0.04%), <span id="Sx9.p2.1.171" class="ltx_text" style="color:#0000FF;">â€œwalkingâ€</span> (1053, 0.04%), <span id="Sx9.p2.1.172" class="ltx_text" style="color:#00FF00;">â€œweddingâ€</span> (1051, 0.04%), <span id="Sx9.p2.1.173" class="ltx_text" style="color:#00FF00;">â€œlotâ€</span> (1050, 0.04%), <span id="Sx9.p2.1.174" class="ltx_text" style="color:#00FF00;">â€œsunglassesâ€</span> (1047, 0.04%), <span id="Sx9.p2.1.175" class="ltx_text" style="color:#00FF00;">â€œmountainsâ€</span> (1030, 0.04%), <span id="Sx9.p2.1.176" class="ltx_text" style="color:#00FF00;">â€œwallâ€</span> (1009, 0.04%), <span id="Sx9.p2.1.177" class="ltx_text" style="color:#00FF00;">â€œelephantsâ€</span> (1006, 0.04%), <span id="Sx9.p2.1.178" class="ltx_text" style="color:#00FF00;">â€œwetsuitâ€</span> (998, 0.04%), <span id="Sx9.p2.1.179" class="ltx_text" style="color:#00FF00;">â€œsquareâ€</span> (994, 0.04%), <span id="Sx9.p2.1.180" class="ltx_text" style="color:#00FF00;">â€œtoothbrushâ€</span> (989, 0.04%), <span id="Sx9.p2.1.181" class="ltx_text" style="color:#0000FF;">â€œsleepingâ€</span> (986, 0.04%), <span id="Sx9.p2.1.182" class="ltx_text" style="color:#00FF00;">â€œfire hydrantâ€</span> (977, 0.04%), <span id="Sx9.p2.1.183" class="ltx_text" style="color:#00FF00;">â€œbicycleâ€</span> (973, 0.04%), <span id="Sx9.p2.1.184" class="ltx_text" style="color:#00FF00;">â€œovercastâ€</span> (968, 0.04%), <span id="Sx9.p2.1.185" class="ltx_text" style="color:#00FF00;">â€œdonutsâ€</span> (961, 0.04%), <span id="Sx9.p2.1.186" class="ltx_text" style="color:#00FF00;">â€œplasticâ€</span> (961, 0.04%), <span id="Sx9.p2.1.187" class="ltx_text" style="color:#00FF00;">â€œbreakfastâ€</span> (955, 0.04%), <span id="Sx9.p2.1.188" class="ltx_text" style="color:#00FF00;">â€œtvâ€</span> (953, 0.04%), <span id="Sx9.p2.1.189" class="ltx_text" style="color:#00FF00;">â€œpaperâ€</span> (952, 0.04%), <span id="Sx9.p2.1.190" class="ltx_text" style="color:#00FF00;">â€œgroundâ€</span> (949, 0.04%), <span id="Sx9.p2.1.191" class="ltx_text" style="color:#DCD20A;">â€œasianâ€</span> (938, 0.04%), <span id="Sx9.p2.1.192" class="ltx_text" style="color:#00FF00;">â€œplaidâ€</span> (936, 0.04%), <span id="Sx9.p2.1.193" class="ltx_text" style="color:#00FF00;">â€œdirtâ€</span> (933, 0.04%), <span id="Sx9.p2.1.194" class="ltx_text" style="color:#00FF00;">â€œmirrorâ€</span> (928, 0.04%), <span id="Sx9.p2.1.195" class="ltx_text" style="color:#00FF00;">â€œusaâ€</span> (928, 0.04%), <span id="Sx9.p2.1.196" class="ltx_text" style="color:#00FF00;">â€œchickenâ€</span> (925, 0.04%), <span id="Sx9.p2.1.197" class="ltx_text" style="color:#00FF00;">â€œplateâ€</span> (920, 0.04%), <span id="Sx9.p2.1.198" class="ltx_text" style="color:#00FF00;">â€œclockâ€</span> (912, 0.04%), <span id="Sx9.p2.1.199" class="ltx_text" style="color:#00FF00;">â€œluggageâ€</span> (908, 0.04%), <span id="Sx9.p2.1.200" class="ltx_text" style="color:#00FF00;">â€œnoneâ€</span> (908, 0.04%), <span id="Sx9.p2.1.201" class="ltx_text" style="color:#00FF00;">â€œstreetâ€</span> (905, 0.04%), <span id="Sx9.p2.1.202" class="ltx_text" style="color:#00FFFF;">â€œon tableâ€</span> (904, 0.04%), <span id="Sx9.p2.1.203" class="ltx_text" style="color:#00FF00;">â€œspoonâ€</span> (899, 0.04%), <span id="Sx9.p2.1.204" class="ltx_text" style="color:#0000FF;">â€œcookingâ€</span> (898, 0.04%), <span id="Sx9.p2.1.205" class="ltx_text" style="color:#DCD20A;">â€œdaytimeâ€</span> (896, 0.04%), <span id="Sx9.p2.1.206" class="ltx_text" style="color:#FF0000;">â€œ16â€</span> (893, 0.04%), <span id="Sx9.p2.1.207" class="ltx_text" style="color:#00FF00;">â€œafricaâ€</span> (890, 0.04%), <span id="Sx9.p2.1.208" class="ltx_text" style="color:#00FF00;">â€œstoneâ€</span> (884, 0.04%), <span id="Sx9.p2.1.209" class="ltx_text" style="color:#DCD20A;">â€œnot sureâ€</span> (873, 0.04%), <span id="Sx9.p2.1.210" class="ltx_text" style="color:#00FF00;">â€œwindowâ€</span> (868, 0.03%), <span id="Sx9.p2.1.211" class="ltx_text" style="color:#00FF00;">â€œsunâ€</span> (865, 0.03%), <span id="Sx9.p2.1.212" class="ltx_text" style="color:#00FF00;">â€œgoldâ€</span> (860, 0.03%), <span id="Sx9.p2.1.213" class="ltx_text" style="color:#00FF00;">â€œpeopleâ€</span> (856, 0.03%), <span id="Sx9.p2.1.214" class="ltx_text" style="color:#00FF00;">â€œracketâ€</span> (847, 0.03%), <span id="Sx9.p2.1.215" class="ltx_text" style="color:#00FF00;">â€œzebrasâ€</span> (845, 0.03%), <span id="Sx9.p2.1.216" class="ltx_text" style="color:#00FF00;">â€œcarrotâ€</span> (841, 0.03%), <span id="Sx9.p2.1.217" class="ltx_text" style="color:#00FF00;">â€œpersonâ€</span> (835, 0.03%), <span id="Sx9.p2.1.218" class="ltx_text" style="color:#00FF00;">â€œfishâ€</span> (835, 0.03%), <span id="Sx9.p2.1.219" class="ltx_text" style="color:#DCD20A;">â€œhappyâ€</span> (824, 0.03%), <span id="Sx9.p2.1.220" class="ltx_text" style="color:#00FF00;">â€œcircleâ€</span> (822, 0.03%), <span id="Sx9.p2.1.221" class="ltx_text" style="color:#00FF00;">â€œorangesâ€</span> (817, 0.03%), <span id="Sx9.p2.1.222" class="ltx_text" style="color:#00FF00;">â€œbackpackâ€</span> (812, 0.03%), <span id="Sx9.p2.1.223" class="ltx_text" style="color:#FF0000;">â€œ25â€</span> (810, 0.03%), <span id="Sx9.p2.1.224" class="ltx_text" style="color:#00FF00;">â€œleavesâ€</span> (809, 0.03%), <span id="Sx9.p2.1.225" class="ltx_text" style="color:#00FF00;">â€œwatchâ€</span> (804, 0.03%), <span id="Sx9.p2.1.226" class="ltx_text" style="color:#00FF00;">â€œmountainâ€</span> (800, 0.03%), <span id="Sx9.p2.1.227" class="ltx_text" style="color:#00FF00;">â€œno oneâ€</span> (798, 0.03%), <span id="Sx9.p2.1.228" class="ltx_text" style="color:#00FF00;">â€œski polesâ€</span> (792, 0.03%), <span id="Sx9.p2.1.229" class="ltx_text" style="color:#00FF00;">â€œcityâ€</span> (791, 0.03%), <span id="Sx9.p2.1.230" class="ltx_text" style="color:#00FF00;">â€œcouchâ€</span> (790, 0.03%), <span id="Sx9.p2.1.231" class="ltx_text" style="color:#00FF00;">â€œafternoonâ€</span> (782, 0.03%), <span id="Sx9.p2.1.232" class="ltx_text" style="color:#00FF00;">â€œjeansâ€</span> (781, 0.03%), <span id="Sx9.p2.1.233" class="ltx_text" style="color:#DCD20A;">â€œbrown and whiteâ€</span> (779, 0.03%), <span id="Sx9.p2.1.234" class="ltx_text" style="color:#00FF00;">â€œsummerâ€</span> (774, 0.03%), <span id="Sx9.p2.1.235" class="ltx_text" style="color:#00FF00;">â€œgiraffesâ€</span> (772, 0.03%), <span id="Sx9.p2.1.236" class="ltx_text" style="color:#00FF00;">â€œcomputerâ€</span> (771, 0.03%), <span id="Sx9.p2.1.237" class="ltx_text" style="color:#00FF00;">â€œrefrigeratorâ€</span> (768, 0.03%), <span id="Sx9.p2.1.238" class="ltx_text" style="color:#00FF00;">â€œbirdsâ€</span> (762, 0.03%), <span id="Sx9.p2.1.239" class="ltx_text" style="color:#00FF00;">â€œchildâ€</span> (761, 0.03%), <span id="Sx9.p2.1.240" class="ltx_text" style="color:#00FF00;">â€œparkâ€</span> (759, 0.03%), <span id="Sx9.p2.1.241" class="ltx_text" style="color:#0000FF;">â€œflying kiteâ€</span> (756, 0.03%), <span id="Sx9.p2.1.242" class="ltx_text" style="color:#00FF00;">â€œrestaurantâ€</span> (747, 0.03%), <span id="Sx9.p2.1.243" class="ltx_text" style="color:#00FF00;">â€œeveningâ€</span> (738, 0.03%), <span id="Sx9.p2.1.244" class="ltx_text" style="color:#00FF00;">â€œgraffitiâ€</span> (736, 0.03%), <span id="Sx9.p2.1.245" class="ltx_text" style="color:#FF0000;">â€œ30â€</span> (730, 0.03%), <span id="Sx9.p2.1.246" class="ltx_text" style="color:#0000FF;">â€œgrazingâ€</span> (727, 0.03%), <span id="Sx9.p2.1.247" class="ltx_text" style="color:#00FF00;">â€œflowerâ€</span> (723, 0.03%), <span id="Sx9.p2.1.248" class="ltx_text" style="color:#DCD20A;">â€œremoteâ€</span> (720, 0.03%), <span id="Sx9.p2.1.249" class="ltx_text" style="color:#00FF00;">â€œhayâ€</span> (719, 0.03%), <span id="Sx9.p2.1.250" class="ltx_text" style="color:#FF0000;">â€œ50â€</span> (716, 0.03%).</p>
</div>
</section>
<section id="Sx10" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix IX: Additional Examples</h2>

<div id="Sx10.p1" class="ltx_para ltx_noindent">
<p id="Sx10.p1.1" class="ltx_p">To provide insight into the dataset, we provide additional examples. In Fig.Â <a href="#Sx10.F27" title="Figure 27 â€£ Appendix IX: Additional Examples â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27</span></a>, Fig.Â <a href="#Sx10.F28" title="Figure 28 â€£ Appendix IX: Additional Examples â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28</span></a>, and Fig.Â <a href="#Sx10.F29" title="Figure 29 â€£ Appendix IX: Additional Examples â€£ VQA: Visual Question Answering www.visualqa.org" class="ltx_ref"><span class="ltx_text ltx_ref_tag">29</span></a>, we show a random selection of the VQA dataset for the MS COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> images, abstract scenes, and multiple-choice questions, respectively.</p>
</div>
<figure id="Sx10.F27" class="ltx_figure"><img src="/html/1505.00468/assets/x18.png" id="Sx10.F27.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="600" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 27: </span>Random examples of questions (black), <span id="Sx10.F27.2.1" class="ltx_text" style="color:#000000;">(a subset of the)</span> answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the real image dataset.</figcaption>
</figure>
<figure id="Sx10.F28" class="ltx_figure"><img src="/html/1505.00468/assets/x19.png" id="Sx10.F28.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="600" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 28: </span>Random examples of questions (black), <span id="Sx10.F28.2.1" class="ltx_text" style="color:#000000;">(a subset of the)</span> answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the abstract scene dataset.</figcaption>
</figure>
<figure id="Sx10.F29" class="ltx_figure"><img src="/html/1505.00468/assets/x20.png" id="Sx10.F29.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="593" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 29: </span>Random examples of multiple-choice questions for numerous representative examples of the real and abstract scene dataset.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
H.Â Agrawal, C.Â S. Mathialagan, Y.Â Goyal, N.Â Chavali, P.Â Banik, A.Â Mohapatra,
A.Â Osman, and D.Â Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:80%;">Cloudcv: Large-scale distributed computer vision as a cloud service.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Mobile Cloud Visual Media Computing</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:80%;">, pages 265â€“290.
Springer International Publishing, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
S.Â Antol, C.Â L. Zitnick, and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:80%;">Zero-Shot Learning via Visual Abstraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ECCV</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
J.Â P. Bigham, C.Â Jayant, H.Â Ji, G.Â Little, A.Â Miller, R.Â C. Miller, R.Â Miller,
A.Â Tatarowicz, B.Â White, S.Â White, and T.Â Yeh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:80%;">VizWiz: Nearly Real-time Answers to Visual Questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">User Interface Software and Technology</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:80%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
K.Â Bollacker, C.Â Evans, P.Â Paritosh, T.Â Sturge, and J.Â Taylor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:80%;">Freebase: A Collaboratively Created Graph Database for Structuring
Human Knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">International Conference on Management of Data</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:80%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
A.Â Carlson, J.Â Betteridge, B.Â Kisiel, B.Â Settles, E.Â R.Â H. Jr., and T.Â M.
Mitchell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:80%;">Toward an Architecture for Never-Ending Language Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">AAAI</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:80%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
X.Â Chen, H.Â Fang, T.Â Lin, R.Â Vedantam, S.Â Gupta, P.Â DollÃ¡r, and C.Â L.
Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:80%;">Microsoft COCO captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:80%;">, abs/1504.00325, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
X.Â Chen, H.Â Fang, T.-Y. Lin, R.Â Vedantam, S.Â Gupta, P.Â DollÃ¡r, and C.Â L.
Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:80%;">Microsoft COCO Captions: Data Collection and Evaluation Server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1504.00325</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
X.Â Chen, A.Â Shrivastava, and A.Â Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:80%;">NEIL: Extracting Visual Knowledge from Web Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
X.Â Chen and C.Â L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:80%;">Mindâ€™s Eye: A Recurrent Visual Representation for Image Caption
Generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
G.Â Coppersmith and E.Â Kelly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:80%;">Dynamic wordclouds and vennclouds for exploratory data analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL Workshop on Interactive Language Learning and
Visualization</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
J.Â Deng, A.Â C. Berg, and L.Â Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:80%;">Hierarchical Semantic Indexing for Large Scale Image Retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:80%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
J.Â Donahue, L.Â A. Hendricks, S.Â Guadarrama, M.Â Rohrbach, S.Â Venugopalan,
K.Â Saenko, and T.Â Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:80%;">Long-term Recurrent Convolutional Networks for Visual Recognition
and Description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
D.Â Elliott and F.Â Keller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:80%;">Comparing Automatic Evaluation Measures for Image Description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
A.Â Fader, L.Â Zettlemoyer, and O.Â Etzioni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:80%;">Paraphrase-Driven Learning for Open Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
A.Â Fader, L.Â Zettlemoyer, and O.Â Etzioni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:80%;">Open Question Answering over Curated and Extracted Knowledge Bases.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">International Conference on Knowledge Discovery and Data
Mining</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
H.Â Fang, S.Â Gupta, F.Â N. Iandola, R.Â Srivastava, L.Â Deng, P.Â DollÃ¡r,
J.Â Gao, X.Â He, M.Â Mitchell, J.Â C. Platt, C.Â L. Zitnick, and G.Â Zweig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:80%;">From Captions to Visual Concepts and Back.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
A.Â Farhadi, M.Â Hejrati, A.Â Sadeghi, P.Â Young, C.Â Rashtchian, J.Â Hockenmaier,
and D.Â Forsyth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:80%;">Every Picture Tells a Story: Generating Sentences for Images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ECCV</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:80%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
H.Â Gao, J.Â Mao, J.Â Zhou, Z.Â Huang, and A.Â Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:80%;">Are you talking to a machine? dataset and methods for multilingual
image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NIPS</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
D.Â Geman, S.Â Geman, N.Â Hallonquist, and L.Â Younes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:80%;">A Visual Turing Test for Computer Vision Systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">PNAS</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
J.Â Gordon and B.Â V. Durme.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:80%;">Reporting bias and knowledge extraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 3rd Workshop on Knowledge Extraction, at
CIKM 2013</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
S.Â Guadarrama, N.Â Krishnamoorthy, G.Â Malkarnenkar, S.Â Venugopalan, R.Â Mooney,
T.Â Darrell, and K.Â Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:80%;">YouTube2Text: Recognizing and Describing Arbitrary Activities Using
Semantic Hierarchies and Zero-Shot Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:80%;">, December 2013.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
M.Â Hodosh, P.Â Young, and J.Â Hockenmaier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:80%;">Framing Image Description as a Ranking Task: Data, Models and
Evaluation Metrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">JAIR</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
Y.Â Jia, E.Â Shelhamer, J.Â Donahue, S.Â Karayev, J.Â Long, R.Â Girshick,
S.Â Guadarrama, and T.Â Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:80%;">Caffe: Convolutional architecture for fast feature embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1408.5093</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
A.Â Karpathy and L.Â Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:80%;">Deep Visual-Semantic Alignments for Generating Image Descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
S.Â Kazemzadeh, V.Â Ordonez, M.Â Matten, and T.Â L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:80%;">ReferItGame: Referring to Objects in Photographs of Natural Scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">EMNLP</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
R.Â Kiros, R.Â Salakhutdinov, and R.Â S. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:80%;">Unifying Visual-Semantic Embeddings with Multimodal Neural Language
Models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">TACL</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
R.Â Kiros, Y.Â Zhu, R.Â Salakhutdinov, R.Â S. Zemel, A.Â Torralba, R.Â Urtasun, and
S.Â Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:80%;">Skip-thought vectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1506.06726</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
C.Â Kong, D.Â Lin, M.Â Bansal, R.Â Urtasun, and S.Â Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:80%;">What Are You Talking About? Text-to-Image Coreference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
A.Â Krizhevsky, I.Â Sutskever, and G.Â E. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:80%;">ImageNet Classification with Deep Convolutional Neural Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NIPS</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:80%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
G.Â Kulkarni, V.Â Premraj, S.Â L. Sagnik DharÂ and, Y.Â Choi, A.Â C. Berg, and T.Â L.
Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:80%;">Baby Talk: Understanding and Generating Simple Image
Descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:80%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
D.Â B. Lenat and R.Â V. Guha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Building Large Knowledge-Based Systems; Representation and
Inference in the Cyc Project</span><span id="bib.bib31.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:80%;">Addison-Wesley Longman Publishing Co., Inc., 1989.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
T.-Y. Lin, M.Â Maire, S.Â Belongie, J.Â Hays, P.Â Perona, D.Â Ramanan,
P.Â DollÃ¡r, and C.Â L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:80%;">Microsoft COCO: Common Objects in Context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ECCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
X.Â Lin and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:80%;">Donâ€™t Just Listen, Use Your Imagination: Leveraging Visual Common
Sense for Non-Visual Tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:80%;">
X.Â Lin and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:80%;">Donâ€™t just listen, use your imagination: Leveraging visual common
sense for non-visual tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:80%;">
H.Â Liu and P.Â Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:80%;">ConceptNet â€” A Practical Commonsense Reasoning Tool-Kit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">BT Technology Journal</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:80%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:80%;">
M.Â Malinowski and M.Â Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:80%;">A Multi-World Approach to Question Answering about Real-World Scenes
based on Uncertain Input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NIPS</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:80%;">
M.Â Malinowski, M.Â Rohrbach, and M.Â Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:80%;">Ask your neurons: A neural-based approach to answering questions
about images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:80%;">
J.Â Mao, W.Â Xu, Y.Â Yang, J.Â Wang, and A.Â L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:80%;">Explain Images with Multimodal Recurrent Neural Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:80%;">, abs/1410.1090, 2014.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:80%;">
T.Â Mikolov, I.Â Sutskever, K.Â Chen, G.Â S. Corrado, and J.Â Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:80%;">Distributed Representations of Words and Phrases and their
Compositionality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NIPS</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:80%;">
M.Â Mitchell, J.Â Dodge, A.Â Goyal, K.Â Yamaguchi, K.Â Stratos, X.Â Han, A.Â Mensch,
A.Â Berg, T.Â L. Berg, and H.Â DaumeÂ III.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:80%;">Midge: Generating Image Descriptions From Computer Vision
Detections.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:80%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:80%;">
M.Â Mitchell, K.Â van Deemter, and E.Â Reiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:80%;">Attributes in visual reference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">PRE-CogSci</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:80%;">
M.Â Mitchell, K.Â VanÂ Deemter, and E.Â Reiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:80%;">Generating Expressions that Refer to Visible Objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">HLT-NAACL</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:80%;">
V.Â Ramanathan, A.Â Joulin, P.Â Liang, and L.Â Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:80%;">Linking People with â€œTheirâ€ Names using Coreference Resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ECCV</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:80%;">
M.Â Ren, R.Â Kiros, and R.Â Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:80%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NIPS</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:80%;">
M.Â Richardson, C.Â J. Burges, and E.Â Renshaw.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:80%;">MCTest: A Challenge Dataset for the Open-Domain Machine
Comprehension of Text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">EMNLP</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:80%;">
M.Â Rohrbach, W.Â Qiu, I.Â Titov, S.Â Thater, M.Â Pinkal, and B.Â Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:80%;">Translating Video Content to Natural Language Descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:80%;">
F.Â Sadeghi, S.Â K. KumarÂ Divvala, and A.Â Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:80%;">Viske: Visual knowledge extraction and question answering by visual
verification of relation phrases.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:80%;">
K.Â Simonyan and A.Â Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:80%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:80%;">, abs/1409.1556, 2014.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:80%;">
K.Â Toutanova, D.Â Klein, C.Â D. Manning, and Y.Â Singer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:80%;">Feature-rich part-of-speech tagging with a cyclic dependency network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:80%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:80%;">
K.Â Tu, M.Â Meng, M.Â W. Lee, T.Â E. Choe, and S.Â C. Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:80%;">Joint Video and Text Parsing for Understanding Events and Answering
Queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE MultiMedia</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:80%;">
R.Â Vedantam, C.Â L. Zitnick, and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:80%;">CIDEr: Consensus-based Image Description Evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:80%;">
R.Â Vendantam, X.Â Lin, T.Â Batra, C.Â L. Zitnick, and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:80%;">Learning common sense through visual abstraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:80%;">
O.Â Vinyals, A.Â Toshev, S.Â Bengio, and D.Â Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:80%;">Show and Tell: A Neural Image Caption Generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:80%;">
J.Â Weston, A.Â Bordes, S.Â Chopra, and T.Â Mikolov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:80%;">Towards AI-Complete Question Answering: A Set of Prerequisite Toy
Tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:80%;">, abs/1502.05698, 2015.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:80%;">
L.Â Yu, E.Â Park, A.Â C. Berg, and T.Â L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:80%;">Visual madlibs: Fill-in-the-blank description generation and question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:80%;">
P.Â Zhang, Y.Â Goyal, D.Â Summers-Stay, D.Â Batra, and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:80%;">Yin and yang: Balancing and answering binary visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:80%;">, abs/1511.05099, 2015.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:80%;">
C.Â L. Zitnick and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:80%;">Bringing Semantics Into Focus Using Visual Abstraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">CVPR</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:80%;">
C.Â L. Zitnick, D.Â Parikh, and L.Â Vanderwende.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:80%;">Learning the Visual Interpretation of Sentences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:80%;">
C.Â L. Zitnick, R.Â Vedantam, and D.Â Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:80%;">Adopting Abstract Images for Semantic Scene Understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">PAMI</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1505.00467" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1505.00468" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1505.00468">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1505.00468" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1505.00469" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 03:39:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
