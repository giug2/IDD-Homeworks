<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction</title>
<!--Generated on Thu Aug 22 09:36:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.12249v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx1" title="In LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx2" title="In LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx3" title="In LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx3.SSx1" title="In Methodology ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx3.SSx1.SSSx1" title="In Datasets ‣ Methodology ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Classification.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx3.SSx1.SSSx2" title="In Datasets ‣ Methodology ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">NER.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx3.SSx2" title="In Methodology ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx3.SSx3" title="In Methodology ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Techniques</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4" title="In LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.SSx1" title="In Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Overview of results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.SSx2" title="In Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Detailed Comparison of Prompting Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.SSx3" title="In Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Detailed Per-dataset analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx5" title="In LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title">Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#A1" title="In LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix B: Compute Details</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Aishik Nagar<sup class="ltx_sup" id="id1.1.id1">1</sup>, Viktor Schlegel<sup class="ltx_sup" id="id2.2.id2">*2,3</sup>,
Thanh-Tung Nguyen,
<br class="ltx_break"/>Hao Li<sup class="ltx_sup" id="id3.3.id3">3</sup>,
Yuping Wu<sup class="ltx_sup" id="id4.4.id4">3</sup>,
Kuluhan Binici<sup class="ltx_sup" id="id5.5.id5">4</sup>,
Stefan Winkler<sup class="ltx_sup" id="id6.6.id6">4</sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Large Language Models (LLMs) are increasingly adopted for applications in healthcare, reaching the performance of domain experts on tasks such as question answering and document summarisation. Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extration. To breach this gap, in this paper, we systematically benchmark LLM performance in Medical Classification and Named Entity Recognition (NER) tasks. We aim to disentangle the contribution of different factors to the performance, particularly the impact of LLMs’ task knowledge and reasoning capabilities, their (parametric) domain knowledge, and addition of external knowledge. To this end we evaluate various open LLMs—including BioMistral and Llama-2 models—on a diverse set of biomedical datasets, using standard prompting, Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter-intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks, laying bare the limitations in the current application of CoT, self-consistency and RAG in the biomedical domain. Our findings suggest that advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required. This highlights the need for more effective integration of external knowledge and reasoning mechanisms in LLMs to enhance their performance in real-world biomedical applications.</p>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The success of Large Language Models (LLMs) promises to reshape the landscape of AI healthcare applications, especially for scenarios relying on Question Answering <cite class="ltx_cite ltx_citemacro_citep">(Budler, Gosak, and Stiglic <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib5" title="">2023</a>; Subramanian et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib60" title="">2024</a>)</cite>, summarisation <cite class="ltx_cite ltx_citemacro_citep">(Schlegel et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib53" title="">2023</a>)</cite> and extracting insights from unstructured patient-generated health data <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib34" title="">2023</a>)</cite>. While considerable progress has been made in leveraging LLMs for tasks requiring free-text outputs, much of the focus has been on optimizing the parametric knowledge—the information stored in the model’s weights and learned during training. Recent works explore methods such as fine-tuning on task-specific data and in-context learning (ICL) and reporting significant improvements in model performance.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">However, these approaches primarily enhance the models’ internal knowledge representation. As such, they rely on <em class="ltx_emph ltx_font_italic" id="Sx1.p2.1.1">readily available data</em> for the structured tasks at hand, be it in form of training sets for task-specific fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Abburi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib1" title="">2023</a>)</cite>, or for selecting good-quality representative few-shot examples for ICL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib77" title="">2024</a>; Gutierrez et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib16" title="">2022</a>)</cite>. In the biomedical domain, such resources for structured prediction tasks are typically not available, as requirements might arise ad-hoc—for example when researchers need to process a set of medical records to find patients satisfying inclusion criteria for a clinical trial <cite class="ltx_cite ltx_citemacro_citep">(Jullien et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib24" title="">2023</a>)</cite> (e.g., whether they’re a smoker). But even for well established tasks, such as medication name extraction, for which resources exist <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib69" title="">2020</a>)</cite>, these resources often prove to be insufficient in a practical context, due to the domain shift between public resources and internal hospital data <cite class="ltx_cite ltx_citemacro_citep">(Hadi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib17" title="">2023</a>)</cite>. Therefore, solely training-set reliant improvements parametric knowledge of LLMs as driver of performance for structured prediction tasks is often infeasible and approaches need to be able to perform well in zero-shot scenarios. Despite this, the literature currently lacks a systematic investigation of other crucial aspects of knowledge utilization.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">In order to address this research gap, we first postulate that the performance of LLMs in medical reasoning and information extraction tasks in “true” zero-shot setting<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>by “true zero-shot” we refer to the scenario where no examples are available to solve the task and no information beyond the labels and their semantically meaningful names is made available to the model <cite class="ltx_cite ltx_citemacro_citep">(Lampert, Nickisch, and Harmeling <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib32" title="">2014</a>)</cite>.</span></span></span> hinges on three distinct categories of knowledge:</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="Sx1.I1.i1.p1.1.1">Parametric Knowledge</span>: The inherent knowledge embedded within the model’s parameters.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="Sx1.I1.i2.p1.1.1">Task Knowledge</span>: The model’s ability to reason about the specific task, including understanding relevant labels and the context of the task.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="Sx1.I1.i3.p1.1.1">External Knowledge</span>: Additional information and context retrieved to supplement the model’s understanding and decision-making process.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Research in evaluating these aspects specifically in the medical domain <cite class="ltx_cite ltx_citemacro_citep">(Nori et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib39" title="">2023</a>; Subramanian et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib60" title="">2024</a>)</cite> is being conducted vividly, but these works have mostly focused on knowledge-intensive prerequisite tasks, such as Multiple-Choice Question Answering. While useful to evaluate the medical <em class="ltx_emph ltx_font_italic" id="Sx1.p4.1.1">knowledge</em> of LLMs, they do not address the question of the medical <em class="ltx_emph ltx_font_italic" id="Sx1.p4.1.2">capabilities</em> of LLMs to succeed on tasks that are more reflective of real applications, such as medical text classification or information extraction. As such, it is necessary to evaluate, whether advancements derived from methods that enhance performance, such as (zero-shot) Chain-of-Thought (CoT) reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib68" title="">2022</a>; Wang and Zhou <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib65" title="">2024</a>)</cite>, self-consistency <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib64" title="">2022</a>)</cite> and Retrieval-augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib35" title="">2024</a>)</cite> carry over to such structured prediction tasks.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Moreover, these studies often employ large, commercial models like ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Biswas <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib2" title="">2023</a>)</cite> or GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib44" title="">2023</a>)</cite> which present significant challenges in real-world applications due to their computational cost and privacy concerns associated with sending sensitive data to third-party APIs. Furthermore, there is a growing concern regarding the reliability of LLMs in medical applications, as even the most powerful models are prone to generating hallucinations, compromising the truthfulness of the outputs. Although constrained generations have shown promise in mitigating these issues, their application in medical information extraction tasks has been limited.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">Thus, there are three problems that currently inhibit our understanding of the capabilities of LLMs on structured prediction tasks in the medical domain, and, as a consequence, their improvement:

<span class="ltx_inline-enumerate" id="Sx1.I2">
<span class="ltx_inline-item" id="Sx1.I2.i1"><span class="ltx_tag ltx_tag_inline-item"><em class="ltx_emph ltx_font_italic" id="Sx1.I2.i1.1.1.1">(i)</em></span> <span class="ltx_text" id="Sx1.I2.i1.4">Existing approaches to structured prediction tasks in the medical domain typically enhance parametric knowledge and rely on the availability of training sets, which might not be realistic;
</span></span>
<span class="ltx_inline-item" id="Sx1.I2.i2"><span class="ltx_tag ltx_tag_inline-item"><em class="ltx_emph ltx_font_italic" id="Sx1.I2.i2.1.1.1">(ii)</em></span> <span class="ltx_text" id="Sx1.I2.i2.4">“True zero-shot” studies and methods to improve performance in such settings are mostly carried out surrogate tasks such as Question Answering and whether they can be adapted to structured prediction tasks on is unknown;
</span></span>
<span class="ltx_inline-item" id="Sx1.I2.i3"><span class="ltx_tag ltx_tag_inline-item"><em class="ltx_emph ltx_font_italic" id="Sx1.I2.i3.1.1.1">(iii)</em></span> <span class="ltx_text" id="Sx1.I2.i3.4">Advancements are typically reported on large-scale, proprietary LLMs which might be unusable due to privacy concerns and inaccessibility to logits for constrained decoding.
</span></span>
</span></p>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">In this paper, we aim to address these gaps by systematically benchmarking the performance of LLMs in medical classification and NER tasks as a representative selection of structured prediction tasks. We focus on assessing the impact of task knowledge and external knowledge while maintaining the parametric knowledge at a reasonable yet static level. Our approach involves exploring a range of techniques, including CoT reasoning, Retrieval-Augmented Generation (RAG), and constrained generation, which have not been extensively applied in these settings. By providing a comprehensive evaluation of these methods, we seek to offer new insights into the practical deployment of LLMs in the medical domain, highlighting both the challenges and potential solutions.</p>
</div>
<div class="ltx_para" id="Sx1.p8">
<p class="ltx_p" id="Sx1.p8.1">To summarise, this paper makes the following novel contributions: <span class="ltx_text ltx_font_bold" id="Sx1.p8.1.1">First</span>, to our knowledge, we present the first comprehensive benchmark for LLMs in medical classification and Named Entity Recognition (NER) tasks, providing a systematic evaluation of their information extraction performance in these critical structured prediction tasks within the medical domain. <span class="ltx_text ltx_font_bold" id="Sx1.p8.1.2">Second</span>, we investigate the impact of various knowledge enhancement techniques, including Chain of Thought (CoT) reasoning, Self-Consistency, Retrieval-Augmented Generation (RAG), and constrained generation, which have not been extensively explored in medical information extraction settings. Notably, we demonstrate that parametric knowledge capacity, i.e., model size, is a primary and often sole driver of performance in zero-shot settings, offering insights into the limitations and potential of current LLM architectures.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Related Work</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We briefly survey the existing benchmarking literature in the medical domain, outlining the lack of studies focusing on structured prediction tasks. Furthermore, we cover recent prompting techniques that were proposed to elicit reasoning in LLMs, and augment their domain knowledge, either by better tapping into their parametric knowledge or by explicitly providing them with relevant external context. Notably, we omit approaches that rely on existence of training sets, such as few-shot prompting <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib66" title="">2023</a>)</cite> or model fine-tuning, as one of the key challenges in the medical domain is the lack of annotated task data, due to privacy concerns over sharing medical records. Instead, as outlines in the introduction, we focus on “true” zero-shot capabilities of LLMs.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx2.p2.1.1">Existing LLMs Benchmarks:</span> With the rising popularity of LLMs, many works evaluated their performance in the biomedical and clinical domains.
These works typically focus on evaluating domain-knowledge by means of Question Answering <cite class="ltx_cite ltx_citemacro_citep">(Singhal et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib56" title="">2023</a>; Harris <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib18" title="">2023</a>; Subramanian et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib60" title="">2024</a>)</cite>, or focus directly on possible application scenarios, such as summarisation <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib34" title="">2023</a>; Yim et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib75" title="">2023</a>)</cite> or clinical coding <cite class="ltx_cite ltx_citemacro_citep">(Kaur, Ginige, and Obst <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib25" title="">2023</a>)</cite>. Many works combine these two directions in an effort to provide more comprehensive benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib59" title="">2024</a>; Xiong et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib73" title="">2024</a>; Feng et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib13" title="">2024</a>; Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib7" title="">2020</a>; Manes et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib37" title="">2024</a>)</cite>. However, many of these works overlook the wealth of existing literature and plethora of available resources for traditional structured prediction tasks in the biomedical domain, such as document classification, entity recognition and linking and event and relation extraction (e.g., <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib49" title="">Pyysalo et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib46" title="">2007</a>; <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib49" title="">2012</a></cite>) to name a few). <cite class="ltx_cite ltx_citemacro_citet">Fries et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib14" title="">2022</a>)</cite> have provided a comprehensive and unified collection of these resources, however their work prioritises reportage of the resource collection over benchmarking results. Their preliminary evaluations suggest that their evaluated pre-LLM era models barely surpass the random guess baseline in the zero-shot setting. We build upon their work by providing a detailed analysis to what extent approaches to enhance reasoning and knowledge in LLMs help to challenge this status quo.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx2.p3.1.1">Reasoning- and Knowledge-enhancing approaches:</span> Current work attempts to improve the performance of LLMs from different knowledge utilization perspectives. One of the obvious methods is full parameter domain-specific pre-training <cite class="ltx_cite ltx_citemacro_citep">(Xie et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib71" title="">2024</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib8" title="">2023</a>)</cite> propose the largest medical foundation model, trained on both biomedical and clinical data, up to 70B. <cite class="ltx_cite ltx_citemacro_citet">Bolton et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib3" title="">2024</a>)</cite>, on the other hand, believe larger LLMs are computationally expensive to run, proposing a 2.7B LLM specific for biomedical NLP tasks. When fine-tuned, the relatively small model compete with larger LLMs. In our study, we compare domain-generalist models with those adapted to the medical domain.
Since full parameter tuning is costly, many works focus on domain knowledge adaptation by pre-training <cite class="ltx_cite ltx_citemacro_citep">(Shi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib55" title="">2024</a>; Song et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib57" title="">2024</a>)</cite> or instruction tuning <cite class="ltx_cite ltx_citemacro_citep">(Willard and Louf <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib70" title="">2023</a>)</cite> with adapters. Training-free approaches encompass chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib68" title="">2022</a>; Jeong et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib21" title="">2024</a>)</cite>, self-consistency<cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib64" title="">2022</a>)</cite>, Concerned with lack of grounding resulting in hallucination, recent work introduce RAG methods <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib35" title="">2024</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib67" title="">2024b</a>; Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib76" title="">2023</a>; Munnangi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib38" title="">2024</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib63" title="">2024a</a>; Soong et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib58" title="">2023</a>)</cite>. However, most of these efforts have focused on performance in a particular knowledge paradigm and have lacked a systematic assessment of how performance on structured prediction, which we address in our study.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Methodology</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Our methodology is designed to answer the following two research questions: <em class="ltx_emph ltx_font_italic" id="Sx3.p1.1.1">“How well do LLMs perform on structured prediction tasks?”</em> and <em class="ltx_emph ltx_font_italic" id="Sx3.p1.1.2">“To what extent can approaches that enhance task and external knowledge improve their performance?”</em>
To answer the first research question, we benchmark a representative sample of LLMs on a large collection of biomedical text classification and NER datasets. More specifically, we choose the task of Medical Text Classification and NER as representative structured predictions tasks. We focus on the “true” zero shot setting, since, as discussed before, this allows us to establish the level of models’ original <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.3">parametric knowledge</span>, which is desirable as it more closely reflects real-world application scenarios, because annotated training data for such tasks in the biomedical domain is usually not available due to the ad-hoc nature of task requirements and privacy constraints of medical records. Thus improving parametric knowledge is often infeasible in practice.
To answer the second question, we compare their zero-shot performance to various methods that aim to enhance <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.4">task knowledge</span> and <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.5">external knowledge</span>, while keeping the <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.6">parametric knowledge</span> static.</p>
</div>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Datasets</h3>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">Since we evaluate different prompting techniques, we restrict the choice of tasks to those where the number of possible labels is small enough to fit in the evaluated LLMs’ context window.
We restrict the number of labels to ten and the mean length of the input documents to at most 2048 tokens. This leaves us with 14 different classification datasets from the BigBio collection<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>for the GAD dataset, we only select 1 fold out of the 10 available, as the folds feature the same task for different data, unlike other datasets. We also skipped the Chinese subset of meddialog as we had difficulties loading the dataset</span></span></span>. For the NER task, we sample 12 datasets from the pool of those that satisfy the criteria. The resulting dataset sample features four non-English datasets and six non-public classification datasets, which allows us to investigate whether LLMs perform better on minority languages or on data that is less likely to be found in public pre-training corpora.
We run the evaluation on the official test-set split where available, otherwise we consider the full dataset. For datasets with more than 500 instances, we sample 500 random but fixed instances to speed up the experiments. Overall, our selection spans English and non-english source data, publicly available and private datasets, and various domains such as scientific papers, medical notes and social media. The overview of the datasets follows below,
with full details to be found in the technical appendix.
</p>
</div>
<section class="ltx_subsubsection" id="Sx3.SSx1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Classification.</h4>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p1.1">The datasets used for classification tasks include both single-label and multi-label datasets, covering a wide range of biomedical and clinical domains. For single-label classification, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.1">GAD</span> dataset focuses on identifying associations between genes and diseases <cite class="ltx_cite ltx_citemacro_citep">(Bravo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib4" title="">2015</a>)</cite>, while the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.2">GEO</span> dataset is concerned with classifying microarray, transcriptomics, and single-cell experiments from the Gene Expression Omnibus (GEO) database <cite class="ltx_cite ltx_citemacro_citep">(Elucidata <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib12" title="">2022</a>)</cite>. The <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.3">MedDialog</span> dataset aims to classify dialogue snippets as either being said by a doctor or a patient <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib7" title="">2020</a>)</cite>. Furthermore, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.4">CZIDrsm</span> dataset has several subsets, including one for classifying research articles based on aspects of disease research (<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.5">CZIBase</span>), and others for identifying whether a paper describes substantive research into Quality of Life (<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.6">CZIQoL</span>) or is a natural history study (<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p1.1.7">CZINatHist</span>).</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p2">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p2.1">In multi-label classification, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.1">LitCovid</span> dataset is used for the classification of COVID-19-related articles <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib6" title="">2021</a>)</cite>. The <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.2">CAS</span> and <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.3">ESSAI</span> datasets are utilized for identify negation and uncertainty clinical cases from French-speaking countries <cite class="ltx_cite ltx_citemacro_citep">(Grabar, Claveau, and Dalloux <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib15" title="">2018</a>)</cite>. The <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.4">NTCIR13</span> datasets include subsets for disease classification of tweets in Japanese (<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.5">*-Ja</span>), English (<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.6">*-En</span>), and Chinese (<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.7">*-Zh</span>) <cite class="ltx_cite ltx_citemacro_citep">(Iso et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib20" title="">2017</a>)</cite>. Additionally, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.8">PsyTAR</span> dataset is used for sentence classification of various drug-related effects, such as Adverse Drug Reactions (ADR) and Withdrawal Symptoms (WDs) <cite class="ltx_cite ltx_citemacro_citep">(Zolnoori et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib80" title="">2019</a>)</cite>, while the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx1.p2.1.9">SciCite</span> dataset is used for citation intent classification based on the context within computer science and biomedical domains <cite class="ltx_cite ltx_citemacro_citep">(Cohan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib9" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx3.SSx1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">NER.</h4>
<div class="ltx_para" id="Sx3.SSx1.SSSx2.p1">
<p class="ltx_p" id="Sx3.SSx1.SSSx2.p1.1">The datasets for Named Entity Recognition (NER) tasks are similarly divided into entity recognition (single entity type) and classification (multiple entity types). In the single-type category, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p1.1.1">GeneTag</span> dataset is used for gene/protein NER, with two annotation versions: the original <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p1.1.2">GeneTag-G</span> and the corrected <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p1.1.3">GeneTag-C</span> <cite class="ltx_cite ltx_citemacro_citep">(Tanabe et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib61" title="">2005</a>)</cite>. Additionally, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p1.1.4">GENIA-PPI</span> dataset focuses on protein-protein interactions or gene regulatory relations within the GENIA corpus, capturing primarily static relations <cite class="ltx_cite ltx_citemacro_citep">(Pyysalo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib48" title="">2009</a>; Hoehndorf et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib19" title="">2010</a>; Ohta et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib41" title="">2010</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx2.p2">
<p class="ltx_p" id="Sx3.SSx1.SSSx2.p2.1">The multiple-type NER datasets encompass various complex biomedical tasks. The <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.1">AnEm</span> dataset targets anatomical entity recognition <cite class="ltx_cite ltx_citemacro_citep">(Ohta et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib43" title="">2012</a>)</cite>, while the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.2">BioInfer</span> dataset focuses on recognizing proteins, genes, and RNA entities <cite class="ltx_cite ltx_citemacro_citep">(Pyysalo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib46" title="">2007</a>)</cite>. The <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.3">Genia-EE</span> dataset is used for the GENIA Event corpus <cite class="ltx_cite ltx_citemacro_citep">(Kim et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib27" title="">2009</a>)</cite>, and the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.4">BioNLP11-REL</span> dataset is employed for extracting part-of relations between genes/proteins and associated entities <cite class="ltx_cite ltx_citemacro_citep">(Pyysalo, Ohta, and Tsujii <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib50" title="">2011</a>)</cite>. Furthermore, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.5">BioNLP-13-CG</span> dataset is used for Cancer Genetics (CG) information extraction, focusing on recognizing events represented as structured n-ary associations of given physical entities <cite class="ltx_cite ltx_citemacro_citep">(Pyysalo, Ohta, and Ananiadou <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib47" title="">2013</a>)</cite>. The <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.6">BioNLP-13-GRO</span> dataset aims to populate the Gene Regulation Ontology with events and relations <cite class="ltx_cite ltx_citemacro_citep">(Kim et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib28" title="">2013</a>)</cite>, and the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.7">BioNLP-13-PC</span> dataset is used for the automatic extraction of biomolecular reactions from text <cite class="ltx_cite ltx_citemacro_citep">(Ohta et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib42" title="">2013</a>)</cite>. Lastly, the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.8">PICO</span> dataset deals with recognizing (P)articipants, (I)nterventions, and (O)utcomes <cite class="ltx_cite ltx_citemacro_citep">(Nye et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib40" title="">2018</a>)</cite>, and the <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx1.SSSx2.p2.1.9">MLEE</span> dataset is used for event extraction related to angiogenesis <cite class="ltx_cite ltx_citemacro_citep">(Pyysalo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib49" title="">2012</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Models</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">For our experiments, we employed two instruction-tuned variants of the Llama-2 model—7B and 70B—both <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib62" title="">2023</a>)</cite>, alongside the BioMistral-7B model <cite class="ltx_cite ltx_citemacro_citep">(Labrak et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib31" title="">2024</a>)</cite> which was further pre-trained on the biomedical domain.
Since we make use of constrained generation to generate model outputs and guide the models decoding process, we retrict the evaluation to open source models since this process is not possible for proprietary models such as GPT-4.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">Techniques</h3>
<div class="ltx_para" id="Sx3.SSx3.p1">
<p class="ltx_p" id="Sx3.SSx3.p1.1">Standard prompting was used as a baseline for both the Classification as well as the NER tasks.
<span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.1">Chain-of-thought reasoning</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib68" title="">2022</a>)</cite> has been shown to improve performance, particularly in QA and logical reasoning tasks. Thus, we also ran experiments with <span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.2">chain-of-thought</span> reasoning to measure its impact on model performance. For the NER task, we adapted a more guided, <span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.3">two-stage approach</span> <cite class="ltx_cite ltx_citemacro_citep">(Shen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib54" title="">2021</a>)</cite> to implement a novel chain-of-thought reasoning approach. Here, The first stage involves inducing a generic entity name from a datasets’ known entity labels—e.g., “Bodypart” for the NER labels describing different bodyparts—and then labelling the input document with that generic entity type.
In the second stage all entities labelled in this way are further disambiguated with their respective fine-grained dataset NER labels.
<span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.4">Retrieval Augmented Generation</span> <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib33" title="">2020</a>)</cite> has been established as an effective technique to improve model performance by introducing relevant non-parameteric knowledge to models and thus grounding the generated outputs to factual information. <cite class="ltx_cite ltx_citemacro_citet">Xiong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib73" title="">2024</a>)</cite> conducted a systematic study of RAG on medical QA, and we incorporate their findings into our study.
We used PubMed abstracts <cite class="ltx_cite ltx_citemacro_citep">(Sanyal, Bhowmick, and Das <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib52" title="">2021</a>)</cite> and Wikipedia articles as knowledge corpora, because <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib73" title="">Xiong et al.</a></cite>’s (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib73" title="">2024</a></cite>) experiments found that using PubMed improved performance over non RAG techniques, while using Wikipedia reduced performance in medical QA tasks. Our goal was to evaluate whether the same holds true for structured prediction tasks as well.
For the RAG module, we made use of FAISS <cite class="ltx_cite ltx_citemacro_citep">(Douze et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib11" title="">2024</a>; Johnson, Douze, and Jégou <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib23" title="">2019</a>)</cite>, which allows retrieval of most similar documents based on semantic similarity, where we used the <span class="ltx_text ltx_font_typewriter" id="Sx3.SSx3.p1.1.5">all-MiniLM-L6-v2</span> sentence transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib51" title="">2019</a>)</cite> model for embedding input documents and corpora. For each experiment, the number of retrieved documents was computed based on the maximum possible documents which could be used without exceeding the token limit of the model. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.6">Self-consistency</span>, proposed by <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib64" title="">2022</a>)</cite>, improves chain-of-thought reasoning of LLMs by sampling reasoning paths for a given problem, followed by a majority vote for the final answer. We also conduct a set of experiments employing <span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.7">self-consistency</span> to investigate whether such improvements can be observed on structured prediction tasks in the medical domain as well. For classification tasks, self consistency was employed to generate multiple reasoning chains for the given problem, followed by answer extraction from each reasoning chain and majority voting to select the final answer. For NER tasks, since we follow the two-stage approach, self-consistency was employed in both stages. Multiple general entity labels were generated in the first stage, and entities were extracted for each such label. In the second stage, self consistency was again used for the entity selection phase as well as the entity label determination step. Majority voting was utilised in final label or class selection in each case <cite class="ltx_cite ltx_citemacro_citep">(Xie et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib72" title="">2023</a>)</cite>.
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx3.SSx3.p1.1.8">Constrained decoding</span> in LLMs <cite class="ltx_cite ltx_citemacro_citep">(Willard and Louf <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib70" title="">2023</a>)</cite> was used to ensure structured information extraction and text generation. This allowed us to evaluate the LLMs for the task at hand without the added variability due to the aleatoric uncertainties brought about by the probabilistic language generation fundamental to the architectures of the models. More specifically, for classification tasks, we ensured the presense of at least one label in the generated outputs. For NER we restricted the generation of spans occurring in text in the first step, and in the second step, for each of the spans we restricted the generation to any of the possible labels. This is also one of the reasons why we opted against evaluating API-based closed-source LLMs<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The other reason being their intransparancy with regard to training data, which violates our “true” zero-shot setting.</span></span></span>, as in our initial experiments the hallucinations in generated outputs created problems with reliably parsing the structured outputs.</p>
</div>
<div class="ltx_para" id="Sx3.SSx3.p2">
<p class="ltx_p" id="Sx3.SSx3.p2.1">We refer to chain of thought as <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx3.p2.1.1">CoT</span>, Self-consistency as <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx3.p2.1.2">SC</span>, RAG as <span class="ltx_text" id="Sx3.SSx3.p2.1.3"><span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx3.p2.1.3.1">RAG-</span>{<span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx3.p2.1.3.2">P|W</span>}</span> for PubMed and Wikipedia corpora, respectively, and to standard prompting as <span class="ltx_text ltx_font_smallcaps" id="Sx3.SSx3.p2.1.4">Vanilla</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Evaluation Results</h2>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Overview of results</h3>
<figure class="ltx_figure" id="Sx4.F1"><svg class="ltx_picture ltx_centering" height="189.05" id="Sx4.F1.pic1" overflow="visible" version="1.1" width="605.38"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,189.05) matrix(1 0 0 -1 0 0) translate(37.37,0) translate(0,83.37) matrix(1.0 0.0 0.0 1.0 -37.37 -83.37)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(37.37,0) translate(0,83.37)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 0 0 L 0 99.63 M 40.55 0 L 40.55 99.63 M 81.1 0 L 81.1 99.63 M 121.66 0 L 121.66 99.63 M 162.21 0 L 162.21 99.63 M 202.76 0 L 202.76 99.63 M 243.31 0 L 243.31 99.63 M 283.87 0 L 283.87 99.63 M 324.42 0 L 324.42 99.63 M 364.97 0 L 364.97 99.63 M 405.52 0 L 405.52 99.63 M 446.08 0 L 446.08 99.63 M 486.63 0 L 486.63 99.63 M 527.18 0 L 527.18 99.63 M 567.73 0 L 567.73 99.63" style="fill:none"></path></g><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.4pt"><path d="M 0 0 L 567.73 0 M 0 23.17 L 567.73 23.17 M 0 46.34 L 567.73 46.34 M 0 69.51 L 567.73 69.51 M 0 92.68 L 567.73 92.68" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 -5.91 L 0 0 M 40.55 -5.91 L 40.55 0 M 81.1 -5.91 L 81.1 0 M 121.66 -5.91 L 121.66 0 M 162.21 -5.91 L 162.21 0 M 202.76 -5.91 L 202.76 0 M 243.31 -5.91 L 243.31 0 M 283.87 -5.91 L 283.87 0 M 324.42 -5.91 L 324.42 0 M 364.97 -5.91 L 364.97 0 M 405.52 -5.91 L 405.52 0 M 446.08 -5.91 L 446.08 0 M 486.63 -5.91 L 486.63 0 M 527.18 -5.91 L 527.18 0 M 567.73 -5.91 L 567.73 0 M 0 105.54 L 0 99.63 M 40.55 105.54 L 40.55 99.63 M 81.1 105.54 L 81.1 99.63 M 121.66 105.54 L 121.66 99.63 M 162.21 105.54 L 162.21 99.63 M 202.76 105.54 L 202.76 99.63 M 243.31 105.54 L 243.31 99.63 M 283.87 105.54 L 283.87 99.63 M 324.42 105.54 L 324.42 99.63 M 364.97 105.54 L 364.97 99.63 M 405.52 105.54 L 405.52 99.63 M 446.08 105.54 L 446.08 99.63 M 486.63 105.54 L 486.63 99.63 M 527.18 105.54 L 527.18 99.63 M 567.73 105.54 L 567.73 99.63" style="fill:none"></path></g><g color="#808080" fill="#808080" fill-opacity="0" stroke="#808080" stroke-opacity="0" stroke-width="0.2pt"><path d="M 0 0 L 5.91 0 M 0 23.17 L 5.91 23.17 M 0 46.34 L 5.91 46.34 M 0 69.51 L 5.91 69.51 M 0 92.68 L 5.91 92.68 M 567.73 0 L 561.83 0 M 567.73 23.17 L 561.83 23.17 M 567.73 46.34 L 561.83 46.34 M 567.73 69.51 L 561.83 69.51 M 567.73 92.68 L 561.83 92.68" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 99.63 L 567.73 99.63 L 567.73 0 L 0 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 7.12 -39.35)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="28.06"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.1.1.1.1.1.1.1">CAS</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 37.07 -49.95)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="43.05"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.2.2.2.2.2.1.1">SciCite</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 51.51 -76.28)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="80.14"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.3.3.3.3.3.1.1">NTCIR13-Zh</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 91.27 -76.85)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.4.4.4.4.4.1.1">NTCIR13-En</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 134 -74.68)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="78.03"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.5.5.5.5.5.1.1">NTCIR13-Ja</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 193.11 -56.11)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="51.77"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.6.6.6.6.6.1.1">CZIBase</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 241.88 -47.91)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="40.17"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.7.7.7.7.7.1.1">ESSAI</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 288.88 -41.45)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="31.04"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.8.8.8.8.8.1.1">GEO</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 329.16 -41.72)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="31.42"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.9.9.9.9.9.1.1">GDA</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 354.4 -57.26)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="53.23"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.10.10.10.10.10.1.1">LitCovid</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 386.26 -65.95)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="65.53"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.11.11.11.11.11.1.1">MedDialog</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 423.52 -69.02)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.03"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.12.12.12.12.12.1.1">CZiNatHist</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 477.12 -55.98)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="52.73"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.13.13.13.13.13.1.1">PsyTAR</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 518.95 -54.7)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.77"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.14.14.14.14.14.1.1">CZIQoL</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 -3.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_italic" id="Sx4.F1.pic1.15.15.15.15.15.1.1">0</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 16.25)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F1.pic1.16.16.16.16.16.1.1">20</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 39.42)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F1.pic1.17.17.17.17.17.1.1">40</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 62.59)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F1.pic1.18.18.18.18.18.1.1">60</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 85.76)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F1.pic1.19.19.19.19.19.1.1">80</span></foreignobject></g><clippath id="pgfcp1"><path d="M 0 0 L 567.73 0 L 567.73 99.63 L 0 99.63 Z"></path></clippath><g clip-path="url(#pgfcp1)"><g color="#4F81BD" fill="#4F81BD" stroke="#4F81BD"><path d="M 13.36 0 h 4.15 v 23.93 h -4.15 Z M 53.91 0 h 4.15 v 31.38 h -4.15 Z M 94.46 0 h 4.15 v 47.28 h -4.15 Z M 135.02 0 h 4.15 v 60.79 h -4.15 Z M 175.57 0 h 4.15 v 54.77 h -4.15 Z M 216.12 0 h 4.15 v 29.19 h -4.15 Z M 256.67 0 h 4.15 v 22.08 h -4.15 Z M 297.22 0 h 4.15 v 60.94 h -4.15 Z M 337.78 0 h 4.15 v 60.94 h -4.15 Z M 378.33 0 h 4.15 v 32.68 h -4.15 Z M 418.88 0 h 4.15 v 75.01 h -4.15 Z M 459.43 0 h 4.15 v 29.89 h -4.15 Z M 499.99 0 h 4.15 v 32.88 h -4.15 Z M 540.54 0 h 4.15 v 29.89 h -4.15 Z"></path></g><g></g><g color="#9BBB59" fill="#9BBB59" stroke="#9BBB59"><path d="M 18.2 0 h 4.15 v 36.25 h -4.15 Z M 58.75 0 h 4.15 v 37.88 h -4.15 Z M 99.31 0 h 4.15 v 40.37 h -4.15 Z M 139.86 0 h 4.15 v 67.76 h -4.15 Z M 180.41 0 h 4.15 v 47.88 h -4.15 Z M 220.96 0 h 4.15 v 47.96 h -4.15 Z M 261.52 0 h 4.15 v 24.72 h -4.15 Z M 302.07 0 h 4.15 v 99.17 h -4.15 Z M 342.62 0 h 4.15 v 53.06 h -4.15 Z M 383.17 0 h 4.15 v 10.82 h -4.15 Z M 423.73 0 h 4.15 v 77.86 h -4.15 Z M 464.28 0 h 4.15 v 44.49 h -4.15 Z M 504.83 0 h 4.15 v 28.07 h -4.15 Z M 545.38 0 h 4.15 v 38 h -4.15 Z"></path></g><g></g><g color="#FFB303" fill="#FFB303" stroke="#FFB303"><path d="M 23.04 0 h 4.15 v 25.43 h -4.15 Z M 63.6 0 h 4.15 v 45.42 h -4.15 Z M 104.15 0 h 4.15 v 39.9 h -4.15 Z M 144.7 0 h 4.15 v 64.18 h -4.15 Z M 185.25 0 h 4.15 v 53.2 h -4.15 Z M 225.81 0 h 4.15 v 32.67 h -4.15 Z M 266.36 0 h 4.15 v 19.97 h -4.15 Z M 306.91 0 h 4.15 v 80.17 h -4.15 Z M 347.46 0 h 4.15 v 54.45 h -4.15 Z M 388.02 0 h 4.15 v 14.78 h -4.15 Z M 428.57 0 h 4.15 v 61.73 h -4.15 Z M 469.12 0 h 4.15 v 22.94 h -4.15 Z M 509.67 0 h 4.15 v 24.43 h -4.15 Z M 550.23 0 h 4.15 v 27.11 h -4.15 Z"></path></g><g></g></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -23.31 22.03)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.58"><em class="ltx_emph ltx_font_italic" id="Sx4.F1.pic1.20.20.20.20.20.1.1">Micro-F1</em></foreignobject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 17.31 73.72 h 142.97 v 22.65 h -142.97 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 21.46 76.49)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.555)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.58)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#4F81BD" fill="#4F81BD" stroke="#4F81BD" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.28 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="20.56"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.21.21.21.21.21.1.1.1.1.1">Bio</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#9BBB59" fill="#9BBB59" stroke="#9BBB59" transform="matrix(1 0 0 -1 39.38 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 52.67 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="32.29"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.22.22.22.22.22.2.2.2.1.1">L70B</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FFB303" fill="#FFB303" stroke="#FFB303" transform="matrix(1 0 0 -1 90.49 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 103.77 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="25.37"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F1.pic1.23.23.23.23.23.3.3.3.1.1">L7B</span></foreignobject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Best-performing <em class="ltx_emph ltx_font_italic" id="Sx4.F1.7.1">Standard Prompting</em> method for <span class="ltx_text ltx_font_bold" id="Sx4.F1.8.2" style="color:#4F81BD;">Bio</span>Mistral 7B, <span class="ltx_text ltx_font_bold" id="Sx4.F1.9.3" style="color:#9BBB59;">L</span>lama-<span class="ltx_text ltx_font_bold" id="Sx4.F1.10.4" style="color:#9BBB59;">70B</span> and <span class="ltx_text ltx_font_bold" id="Sx4.F1.11.5" style="color:#FFB303;">L</span>lama-<span class="ltx_text ltx_font_bold" id="Sx4.F1.12.6" style="color:#FFB303;">7B</span> for all classification tasks.</figcaption>
</figure>
<figure class="ltx_table" id="Sx4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx4.T1.1">
<tr class="ltx_tr" id="Sx4.T1.1.1">
<td class="ltx_td ltx_border_tt" id="Sx4.T1.1.1.1" rowspan="2"></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx4.T1.1.1.2" rowspan="2"><span class="ltx_text" id="Sx4.T1.1.1.2.1">Technique</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T1.1.1.3">CLS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Sx4.T1.1.1.4">NER</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.2">
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.2.1">F1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.2.2">F1-S</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.2.3">F1-L</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.3.1" rowspan="7"><span class="ltx_text" id="Sx4.T1.1.3.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx4.T1.1.3.1.1.1" style="width:6.9pt;height:61.7pt;vertical-align:-27.4pt;"><span class="ltx_transformed_inner" style="width:61.8pt;transform:translate(-27.4pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="Sx4.T1.1.3.1.1.1.1">BioMistral-7B</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.3.2"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.3.2.1">Vanilla</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.3.3">36.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.3.4">3.3</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.3.5">2.2</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.4">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.4.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.4.1.1">CoT</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.4.2">31.3</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.4.3">1.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.4.4">1.3</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.5">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.5.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.5.1.1">SC-CoT</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.5.2">20.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.5.3">0.8</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.5.4">0.4</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.6">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.6.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.6.1.1">CoT-RAG-P</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.6.2">14.7</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.6.3">1.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.6.4">1.2</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.7">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.7.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.7.1.1">CoT-RAG-W</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.7.2">15.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.7.3">1.3</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.7.4">1.0</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.8">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.8.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.8.1.1">SC-CoT-RAG-P</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.8.2">19.2</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.8.3">0.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.8.4">0.4</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.9">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.9.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.9.1.1">SC-CoT-RAG-W</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.9.2">21.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.9.3">0.4</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.9.4">0.3</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.10.1" rowspan="7"><span class="ltx_text" id="Sx4.T1.1.10.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx4.T1.1.10.1.1.1" style="width:6.9pt;height:56.1pt;vertical-align:-24.6pt;"><span class="ltx_transformed_inner" style="width:56.1pt;transform:translate(-24.58pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="Sx4.T1.1.10.1.1.1.1">Llama-2-70B</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.10.2"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.10.2.1">Vanilla</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.10.3">40.3</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.10.4">8.6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.10.5">5.8</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.11">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.11.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.11.1.1">CoT</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.11.2">35.9</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.11.3">10.3</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.11.4">7.3</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.12">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.12.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.12.1.1">SC-CoT</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.12.2">28.0</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.12.3">9.1</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.12.4">5.4</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.13">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.13.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.13.1.1">CoT-RAG-P</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.13.2">16.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.13.3">9.9</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.13.4">7.1</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.14">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.14.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.14.1.1">CoT-RAG-W</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.14.2">15.7</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.14.3">10.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.14.4">7.2</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.15">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.15.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.15.1.1">SC-CoT-RAG-P</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.15.2">27.2</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.15.3">9.0</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.15.4">5.4</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.16">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.16.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.16.1.1">SC-CoT-RAG-W</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.16.2">26.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.16.3">9.1</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.16.4">5.3</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.17">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Sx4.T1.1.17.1" rowspan="7"><span class="ltx_text" id="Sx4.T1.1.17.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx4.T1.1.17.1.1.1" style="width:6.9pt;height:51.1pt;vertical-align:-22.1pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-22.08pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="Sx4.T1.1.17.1.1.1.1">Llama-2-7B</span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.17.2"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.17.2.1">Vanilla</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.17.3">34.9</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.17.4">6.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="Sx4.T1.1.17.5">5.2</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.18">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.18.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.18.1.1">CoT</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.18.2">30.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.18.3">4.9</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.18.4">2.5</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.19">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.19.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.19.1.1">SC-CoT</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.19.2">24.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.19.3">5.1</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.19.4">3.0</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.20">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.20.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.20.1.1">CoT-RAG-P</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.20.2">14.3</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.20.3">4.6</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.20.4">2.3</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.21">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.21.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.21.1.1">CoT-RAG-W</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.21.2">14.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.21.3">4.2</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.21.4">1.7</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.22">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.22.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.22.1.1">SC-CoT-RAG-P</span></td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.22.2">25.5</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.22.3">5.7</td>
<td class="ltx_td ltx_align_right" id="Sx4.T1.1.22.4">2.9</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.23">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx4.T1.1.23.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.T1.1.23.1.1">SC-CoT-RAG-W</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="Sx4.T1.1.23.2">11.1</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="Sx4.T1.1.23.3">5.6</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="Sx4.T1.1.23.4">3.2</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of each model and technique combination across Classification and NER datasets. For classification, we report Micro-F1 and for NER we report both Span-Identification Micro-F1 performance as well as full Micro-F1 performance, including recognizing correct types.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx1.p1.1.1">Reasoning and knowledge enhancing techniques seem to not improve performance</span>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F1" title="Figure 1 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">1</span></a> and Figure  <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F2" title="Figure 2 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">2</span></a> compare the results of the best performing techniques for each model for classification and NER, respectively. As seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.T1" title="Table 1 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">1</span></a>, perhaps counter-intuitively, Standard Prompting consistently achieves the highest average F1 scores across all models for classification task, with BioMistral-7B obtaining 36.48%, Llama-2-70B-Chat-AWQ achieving 40.34%, and Llama-2-7b-chat-hf scoring 34.92%. This result indicates that for structured prediction tasks, more complex reasoning techniques such as Chain of Thought (CoT) Prompting or Retrieval-Augmented Generation (RAG), do not outperform simpler approaches like Standard Prompting. For NER tasks, the results present a more nuanced picture compared to the classification tasks. While Standard Prompting remains effective, there is a noticeable shift in performance across different models and datasets.
Notably, the scores are significantly lower than typical F1 scores in biomedical NER benchmarks. For instance, the NCBI disease corpus <cite class="ltx_cite ltx_citemacro_citep">(Doğan, Leaman, and Lu <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib10" title="">2014</a>; Krallinger et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib30" title="">2015</a>)</cite> and CHEMDNER dataset usually yield higher performances with specialized models or extensive pre-training. State-of-the-art models on these benchmarks can achieve Span F1 scores up to 0.90 for the NCBI disease corpus <cite class="ltx_cite ltx_citemacro_citep">(Kocaman and Talby <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib29" title="">2021</a>; Zhou et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib79" title="">2023</a>)</cite>. However, similar to our findings, in true zero-shot setting, NER scores have been reported to be markedly low, even for the general domain <cite class="ltx_cite ltx_citemacro_citep">(Shen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib54" title="">2021</a>)</cite> and when supplying label descriptions <cite class="ltx_cite ltx_citemacro_citep">(Picco et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib45" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p2">
<p class="ltx_p" id="Sx4.SSx1.p2.1">A possible reason for poor performance might be that these approaches have been tailored towards—and shown to work well on—knowledge- and reasoning-intensive tasks, such as Question Answering <cite class="ltx_cite ltx_citemacro_citep">(Nori et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib39" title="">2023</a>)</cite> or Mathematical Reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wang and Zhou <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib65" title="">2024</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib64" title="">2022</a>; Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib35" title="">2024</a>)</cite>. Meanwhile more narrowly defined tasks like information extraction or classification require the understanding of specific task semantics over generic reasoning capabilities. They seem to not require broad knowledge, as it could be found in biomedical paper abstracts or Wikipedia articles, but rather require application of domain knowledge in a specific and highly contextualized tasks, contained within the input document and task description. Models need to be able to handle highly specialized vocabulary, including jargon, acronyms, and synonyms that can vary widely between subfields <cite class="ltx_cite ltx_citemacro_citep">(Kim et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib26" title="">2007</a>; Zheng, Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib78" title="">2018</a>; Jiang and Xu <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib22" title="">2024</a>)</cite>. There is a fundamental requirement for context dependent disambiguation of ambiguity and polysemy as well as nuances and variablity in syntax and expressions of biomedical concepts. This is often developed through specialized pre-training or domain-specific enhancements, which the LLMs have not been able to capture. These challenges necessitate models that not only have robust general NER capabilities but also an intricate understanding of biomedical context which can very for different subtasks within the domain.</p>
</div>
<figure class="ltx_figure" id="Sx4.F2"><svg class="ltx_picture ltx_centering" height="207.1" id="Sx4.F2.pic1" overflow="visible" version="1.1" width="605.38"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,207.1) matrix(1 0 0 -1 0 0) translate(37.37,0) translate(0,95.94) matrix(1.0 0.0 0.0 1.0 -37.37 -95.94)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(37.37,0) translate(0,95.94)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 0 0 L 0 99.63 M 47.31 0 L 47.31 99.63 M 94.62 0 L 94.62 99.63 M 141.93 0 L 141.93 99.63 M 189.24 0 L 189.24 99.63 M 236.56 0 L 236.56 99.63 M 283.87 0 L 283.87 99.63 M 331.18 0 L 331.18 99.63 M 378.49 0 L 378.49 99.63 M 425.8 0 L 425.8 99.63 M 473.11 0 L 473.11 99.63 M 520.42 0 L 520.42 99.63 M 567.73 0 L 567.73 99.63" style="fill:none"></path></g><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.4pt"><path d="M 0 0 L 567.73 0 M 0 33.21 L 567.73 33.21 M 0 66.42 L 567.73 66.42 M 0 99.63 L 567.73 99.63" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 -5.91 L 0 0 M 47.31 -5.91 L 47.31 0 M 94.62 -5.91 L 94.62 0 M 141.93 -5.91 L 141.93 0 M 189.24 -5.91 L 189.24 0 M 236.56 -5.91 L 236.56 0 M 283.87 -5.91 L 283.87 0 M 331.18 -5.91 L 331.18 0 M 378.49 -5.91 L 378.49 0 M 425.8 -5.91 L 425.8 0 M 473.11 -5.91 L 473.11 0 M 520.42 -5.91 L 520.42 0 M 567.73 -5.91 L 567.73 0 M 0 105.53 L 0 99.63 M 47.31 105.53 L 47.31 99.63 M 94.62 105.53 L 94.62 99.63 M 141.93 105.53 L 141.93 99.63 M 189.24 105.53 L 189.24 99.63 M 236.56 105.53 L 236.56 99.63 M 283.87 105.53 L 283.87 99.63 M 331.18 105.53 L 331.18 99.63 M 378.49 105.53 L 378.49 99.63 M 425.8 105.53 L 425.8 99.63 M 473.11 105.53 L 473.11 99.63 M 520.42 105.53 L 520.42 99.63 M 567.73 105.53 L 567.73 99.63" style="fill:none"></path></g><g color="#808080" fill="#808080" fill-opacity="0" stroke="#808080" stroke-opacity="0" stroke-width="0.2pt"><path d="M 0 0 L 5.91 0 M 0 33.21 L 5.91 33.21 M 0 66.42 L 5.91 66.42 M 0 99.63 L 5.91 99.63 M 567.73 0 L 561.83 0 M 567.73 33.21 L 561.83 33.21 M 567.73 66.42 L 561.83 66.42 M 567.73 99.63 L 561.83 99.63" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 99.63 L 567.73 99.63 L 567.73 0 L 0 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 2.76 -47.09)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="39.01"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.1.1.1.1.1.1.1">AnEm</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 49.25 -47.91)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="40.17"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.2.2.2.2.2.1.1">MLEE</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 63.54 -80.93)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="86.87"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.3.3.3.3.3.1.1">BioNLP13-PC</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 109.83 -81.95)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="88.31"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.4.4.4.4.4.1.1">BioNLP13-CG</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 171.41 -67.68)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="69.28"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.5.5.5.5.5.1.1">GeneTag-C</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 217.77 -68.63)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="69.47"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.6.6.6.6.6.1.1">GENIA-EE</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 265.42 -68.29)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.15"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.7.7.7.7.7.1.1">GeneTag-G</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 308.86 -72.16)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="74.47"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.8.8.8.8.8.1.1">GENIA-PPI</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 338.91 -89.42)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="98.88"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.9.9.9.9.9.1.1">BioNLP13-GRO</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 431.27 -44.37)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="35.17"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.10.10.10.10.10.1.1">PICO</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 468.88 -54.29)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.04"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.11.11.11.11.11.1.1">BioInfer</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 483.09 -87.18)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="95.71"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.12.12.12.12.12.1.1">BioNLP11-REL</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 -3.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_italic" id="Sx4.F2.pic1.13.13.13.13.13.1.1">0</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 29.75)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_italic" id="Sx4.F2.pic1.14.14.14.14.14.1.1">5</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 59.5)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F2.pic1.15.15.15.15.15.1.1">10</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 92.71)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F2.pic1.16.16.16.16.16.1.1">15</span></foreignobject></g><clippath id="pgfcp2"><path d="M 0 0 L 567.73 0 L 567.73 99.63 L 0 99.63 Z"></path></clippath><g clip-path="url(#pgfcp2)"><g color="#4F81BD" fill="#4F81BD" stroke="#4F81BD"><path d="M 16.74 0 h 4.15 v 4.65 h -4.15 Z M 64.05 0 h 4.15 v 4.38 h -4.15 Z M 111.36 0 h 4.15 v 18.26 h -4.15 Z M 158.67 0 h 4.15 v 10.56 h -4.15 Z M 205.98 0 h 4.15 v 21.85 h -4.15 Z M 300.6 0 h 4.15 v 0 h -4.15 Z M 253.29 0 h 4.15 v 15.61 h -4.15 Z M 347.91 0 h 4.15 v 47.82 h -4.15 Z M 395.23 0 h 4.15 v 0.33 h -4.15 Z M 442.54 0 h 4.15 v 2.19 h -4.15 Z M 489.85 0 h 4.15 v 26.23 h -4.15 Z M 537.16 0 h 4.15 v 20.92 h -4.15 Z"></path></g><g></g><g color="#9BBB59" fill="#9BBB59" stroke="#9BBB59"><path d="M 21.58 0 h 4.15 v 14.61 h -4.15 Z M 68.89 0 h 4.15 v 30.29 h -4.15 Z M 116.2 0 h 4.15 v 42.77 h -4.15 Z M 163.51 0 h 4.15 v 37.26 h -4.15 Z M 210.82 0 h 4.15 v 60.84 h -4.15 Z M 305.45 0 h 4.15 v 85.35 h -4.15 Z M 258.14 0 h 4.15 v 59.24 h -4.15 Z M 352.76 0 h 4.15 v 68.48 h -4.15 Z M 400.07 0 h 4.15 v 19.06 h -4.15 Z M 447.38 0 h 4.15 v 27.43 h -4.15 Z M 494.69 0 h 4.15 v 66.22 h -4.15 Z M 542 0 h 4.15 v 70.6 h -4.15 Z"></path></g><g></g><g color="#FFB303" fill="#FFB303" stroke="#FFB303"><path d="M 26.42 0 h 4.15 v 3.72 h -4.15 Z M 73.73 0 h 4.15 v 15.74 h -4.15 Z M 121.05 0 h 4.15 v 25.37 h -4.15 Z M 168.36 0 h 4.15 v 17.53 h -4.15 Z M 215.67 0 h 4.15 v 40.12 h -4.15 Z M 310.29 0 h 4.15 v 67.01 h -4.15 Z M 262.98 0 h 4.15 v 61.44 h -4.15 Z M 357.6 0 h 4.15 v 57.52 h -4.15 Z M 404.91 0 h 4.15 v 0.8 h -4.15 Z M 452.22 0 h 4.15 v 11.69 h -4.15 Z M 499.53 0 h 4.15 v 57.65 h -4.15 Z M 546.85 0 h 4.15 v 55.72 h -4.15 Z"></path></g><g></g></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -23.31 22.02)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.58"><em class="ltx_emph ltx_font_italic" id="Sx4.F2.pic1.17.17.17.17.17.1.1">Micro-F1</em></foreignobject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 17.31 73.71 h 142.97 v 22.65 h -142.97 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 21.46 76.48)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.555)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.58)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#4F81BD" fill="#4F81BD" stroke="#4F81BD" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.28 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="20.56"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.18.18.18.18.18.1.1.1.1.1">Bio</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#9BBB59" fill="#9BBB59" stroke="#9BBB59" transform="matrix(1 0 0 -1 39.38 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 52.67 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="32.29"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.19.19.19.19.19.2.2.2.1.1">L70B</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FFB303" fill="#FFB303" stroke="#FFB303" transform="matrix(1 0 0 -1 90.49 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 103.77 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="25.37"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F2.pic1.20.20.20.20.20.3.3.3.1.1">L7B</span></foreignobject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Best-performing <em class="ltx_emph ltx_font_italic" id="Sx4.F2.7.1">Standard Prompting</em> method for <span class="ltx_text ltx_font_bold" id="Sx4.F2.8.2" style="color:#4F81BD;">Bio</span>Mistral 7B, <span class="ltx_text ltx_font_bold" id="Sx4.F2.9.3" style="color:#9BBB59;">L</span>lama-<span class="ltx_text ltx_font_bold" id="Sx4.F2.10.4" style="color:#9BBB59;">70B</span> and <span class="ltx_text ltx_font_bold" id="Sx4.F2.11.5" style="color:#FFB303;">L</span>lama-<span class="ltx_text ltx_font_bold" id="Sx4.F2.12.6" style="color:#FFB303;">7B</span> for all NER tasks.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx1.p3">
<p class="ltx_p" id="Sx4.SSx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx1.p3.1.1">Scale drives improvements.</span> In line with previous observations, we find that the 70B model also shows a considerable improvement (5.4% for classification, 2.2% for NER Span F1) over the 7B model. The most significant difference in performance between the Llama 7B and 70B Models is observed when using Self-Consistency with Chain of Thought and RAG (Wikipedia), where the 70B model outperforms the 7B model by 15.45% on classification and on NER tasks. This suggests that the larger model is significantly better at leveraging external knowledge when combined with self-consistency and chain of thought prompting. The larger model’s increased capacity might be particularly advantageous in handling these complexities, resulting in a more significant performance gap compared to simpler techniques. Methods like Chain-of-Thought Prompting and Self-Consistency with Chain-of-Thought and RAG involve complex reasoning and knowledge integration processes<cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib68" title="">2022</a>)</cite>. This is further demonstrated by the fact that Llama 70B improves performance by 10.91% when using Self Consistency is added to Wikipedia based RAG, indicating that self consistency helps model combat the drop in performance when adding potentially irrelevant external information for the larger model. Unlike in classification tasks, where Standard Prompting was universally superior, NER performance does not degrade as much when using advanced prompting techniques, particularly when using larger models like Llama-2-70B, likely due to the general lack of epistemic confidence in the answers in the first place.</p>
</div>
<figure class="ltx_figure" id="Sx4.F3"><svg class="ltx_picture ltx_centering" height="230.56" id="Sx4.F3.pic1" overflow="visible" version="1.1" width="710.38"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,230.56) matrix(1 0 0 -1 0 0) translate(37.37,0) translate(0,83.37) matrix(1.0 0.0 0.0 1.0 -37.37 -83.37)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(37.37,0) translate(0,83.37)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 0 0 L 0 141.14 M 48.05 0 L 48.05 141.14 M 96.1 0 L 96.1 141.14 M 144.16 0 L 144.16 141.14 M 192.21 0 L 192.21 141.14 M 240.26 0 L 240.26 141.14 M 288.31 0 L 288.31 141.14 M 336.37 0 L 336.37 141.14 M 384.42 0 L 384.42 141.14 M 432.47 0 L 432.47 141.14 M 480.52 0 L 480.52 141.14 M 528.58 0 L 528.58 141.14 M 576.63 0 L 576.63 141.14 M 624.68 0 L 624.68 141.14 M 672.73 0 L 672.73 141.14" style="fill:none"></path></g><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.4pt"><path d="M 0 0 L 672.73 0 M 0 32.82 L 672.73 32.82 M 0 65.65 L 672.73 65.65 M 0 98.47 L 672.73 98.47 M 0 131.29 L 672.73 131.29" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 -5.91 L 0 0 M 48.05 -5.91 L 48.05 0 M 96.1 -5.91 L 96.1 0 M 144.16 -5.91 L 144.16 0 M 192.21 -5.91 L 192.21 0 M 240.26 -5.91 L 240.26 0 M 288.31 -5.91 L 288.31 0 M 336.37 -5.91 L 336.37 0 M 384.42 -5.91 L 384.42 0 M 432.47 -5.91 L 432.47 0 M 480.52 -5.91 L 480.52 0 M 528.58 -5.91 L 528.58 0 M 576.63 -5.91 L 576.63 0 M 624.68 -5.91 L 624.68 0 M 672.73 -5.91 L 672.73 0 M 0 147.05 L 0 141.14 M 48.05 147.05 L 48.05 141.14 M 96.1 147.05 L 96.1 141.14 M 144.16 147.05 L 144.16 141.14 M 192.21 147.05 L 192.21 141.14 M 240.26 147.05 L 240.26 141.14 M 288.31 147.05 L 288.31 141.14 M 336.37 147.05 L 336.37 141.14 M 384.42 147.05 L 384.42 141.14 M 432.47 147.05 L 432.47 141.14 M 480.52 147.05 L 480.52 141.14 M 528.58 147.05 L 528.58 141.14 M 576.63 147.05 L 576.63 141.14 M 624.68 147.05 L 624.68 141.14 M 672.73 147.05 L 672.73 141.14" style="fill:none"></path></g><g color="#808080" fill="#808080" fill-opacity="0" stroke="#808080" stroke-opacity="0" stroke-width="0.2pt"><path d="M 0 0 L 5.91 0 M 0 32.82 L 5.91 32.82 M 0 65.65 L 5.91 65.65 M 0 98.47 L 5.91 98.47 M 0 131.29 L 5.91 131.29 M 672.73 0 L 666.83 0 M 672.73 32.82 L 666.83 32.82 M 672.73 65.65 L 666.83 65.65 M 672.73 98.47 L 666.83 98.47 M 672.73 131.29 L 666.83 131.29" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 141.14 L 672.73 141.14 L 672.73 0 L 0 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 10.87 -39.35)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="28.06"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.1.1.1.1.1.1.1">CAS</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 48.32 -49.95)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="43.05"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.2.2.2.2.2.1.1">SciCite</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 70.26 -76.28)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="80.14"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.3.3.3.3.3.1.1">NTCIR13-Zh</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 117.52 -76.85)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.1"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.4.4.4.4.4.1.1">NTCIR13-En</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 167.75 -74.68)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="78.03"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.5.5.5.5.5.1.1">NTCIR13-Ja</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 234.36 -56.11)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="51.77"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.6.6.6.6.6.1.1">CZIBase</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 290.63 -47.91)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="40.17"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.7.7.7.7.7.1.1">ESSAI</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 345.13 -41.45)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="31.04"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.8.8.8.8.8.1.1">GEO</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 392.91 -41.72)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="31.42"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.9.9.9.9.9.1.1">GDA</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 425.65 -57.26)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="53.23"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.10.10.10.10.10.1.1">LitCovid</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 465.01 -65.95)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="65.53"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.11.11.11.11.11.1.1">MedDialog</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 509.77 -69.02)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.03"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.12.12.12.12.12.1.1">CZiNatHist</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 570.87 -55.98)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="52.73"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.13.13.13.13.13.1.1">PsyTAR</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 620.2 -54.7)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.77"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.14.14.14.14.14.1.1">CZIQoL</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 -3.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_italic" id="Sx4.F3.pic1.15.15.15.15.15.1.1">0</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 25.9)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F3.pic1.16.16.16.16.16.1.1">20</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 58.73)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F3.pic1.17.17.17.17.17.1.1">40</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 91.55)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F3.pic1.18.18.18.18.18.1.1">60</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 124.37)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F3.pic1.19.19.19.19.19.1.1">80</span></foreignobject></g><clippath id="pgfcp3"><path d="M 0 0 L 672.73 0 L 672.73 141.14 L 0 141.14 Z"></path></clippath><g clip-path="url(#pgfcp3)"><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M -11.81 29.84 L 47.85 29.84" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 48.25 66.3 L 95.9 66.3" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 96.31 24.95 L 143.96 24.95" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 144.36 24.95 L 192.01 24.95" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 192.41 24.95 L 240.06 24.95" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 240.46 30.2 L 288.11 30.2" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 288.52 14.7 L 336.16 14.7" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 336.57 111.93 L 384.22 111.93" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 384.62 80.09 L 432.27 80.09" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 432.67 53.35 L 480.32 53.35" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 480.73 86.1 L 528.37 86.1" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 528.78 49.56 L 576.43 49.56" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 576.83 43.01 L 624.48 43.01" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 624.68 50.55 L 672.73 50.55"></path></g><g></g><g color="#4F81BD" fill="#4F81BD" stroke="#4F81BD"><path d="M 9.84 0 h 3.46 v 44.67 h -3.46 Z M 57.9 0 h 3.46 v 62.81 h -3.46 Z M 105.95 0 h 3.46 v 54.7 h -3.46 Z M 154 0 h 3.46 v 64.69 h -3.46 Z M 202.05 0 h 3.46 v 57.24 h -3.46 Z M 250.11 0 h 3.46 v 53.5 h -3.46 Z M 298.16 0 h 3.46 v 34.5 h -3.46 Z M 346.21 0 h 3.46 v 110.61 h -3.46 Z M 394.26 0 h 3.46 v 72.87 h -3.46 Z M 442.31 0 h 3.46 v 33.87 h -3.46 Z M 490.37 0 h 3.46 v 98.21 h -3.46 Z M 538.42 0 h 3.46 v 51.2 h -3.46 Z M 586.47 0 h 3.46 v 43.34 h -3.46 Z M 634.52 0 h 3.46 v 41.36 h -3.46 Z"></path></g><g></g><g color="#9BBB59" fill="#9BBB59" stroke="#9BBB59"><path d="M 13.99 0 h 3.46 v 9.37 h -3.46 Z M 62.05 0 h 3.46 v 4.94 h -3.46 Z M 110.1 0 h 3.46 v 0 h -3.46 Z M 158.15 0 h 3.46 v 3.3 h -3.46 Z M 206.2 0 h 3.46 v 0.71 h -3.46 Z M 254.26 0 h 3.46 v 56.78 h -3.46 Z M 302.31 0 h 3.46 v 8.21 h -3.46 Z M 350.36 0 h 3.46 v 76.48 h -3.46 Z M 398.41 0 h 3.46 v 83.7 h -3.46 Z M 446.47 0 h 3.46 v 0.9 h -3.46 Z M 494.52 0 h 3.46 v 102.24 h -3.46 Z M 542.57 0 h 3.46 v 16.41 h -3.46 Z M 590.62 0 h 3.46 v 0.49 h -3.46 Z M 638.68 0 h 3.46 v 16.08 h -3.46 Z"></path></g><g></g><g color="#FFB303" fill="#FFB303" stroke="#FFB303"><path d="M 18.15 0 h 3.46 v 0 h -3.46 Z M 66.2 0 h 3.46 v 6.48 h -3.46 Z M 114.25 0 h 3.46 v 2.79 h -3.46 Z M 162.3 0 h 3.46 v 3.32 h -3.46 Z M 210.36 0 h 3.46 v 0.71 h -3.46 Z M 258.41 0 h 3.46 v 50.22 h -3.46 Z M 306.46 0 h 3.46 v 2.87 h -3.46 Z M 354.51 0 h 3.46 v 82.39 h -3.46 Z M 402.56 0 h 3.46 v 79.76 h -3.46 Z M 450.62 0 h 3.46 v 1.82 h -3.46 Z M 498.67 0 h 3.46 v 104.92 h -3.46 Z M 546.72 0 h 3.46 v 14.44 h -3.46 Z M 594.77 0 h 3.46 v 0.98 h -3.46 Z M 642.83 0 h 3.46 v 8.86 h -3.46 Z"></path></g><g></g><g color="#FFBFBF" fill="#FFBFBF" stroke="#FFBFBF"><path d="M 22.3 0 h 3.46 v 40.9 h -3.46 Z M 70.35 0 h 3.46 v 50.25 h -3.46 Z M 118.4 0 h 3.46 v 0 h -3.46 Z M 166.45 0 h 3.46 v 73.29 h -3.46 Z M 214.51 0 h 3.46 v 0 h -3.46 Z M 262.56 0 h 3.46 v 46.28 h -3.46 Z M 310.61 0 h 3.46 v 36.7 h -3.46 Z M 358.66 0 h 3.46 v 96.17 h -3.46 Z M 406.72 0 h 3.46 v 82.06 h -3.46 Z M 454.77 0 h 3.46 v 25.32 h -3.46 Z M 502.82 0 h 3.46 v 96.86 h -3.46 Z M 550.87 0 h 3.46 v 43.65 h -3.46 Z M 598.93 0 h 3.46 v 5.88 h -3.46 Z M 646.98 0 h 3.46 v 46.28 h -3.46 Z"></path></g><g></g><g color="#008080" fill="#008080" stroke="#008080"><path d="M 26.45 0 h 3.46 v 33.17 h -3.46 Z M 74.5 0 h 3.46 v 49.3 h -3.46 Z M 122.55 0 h 3.46 v 0 h -3.46 Z M 170.6 0 h 3.46 v 76.4 h -3.46 Z M 218.66 0 h 3.46 v 0 h -3.46 Z M 266.71 0 h 3.46 v 42.67 h -3.46 Z M 314.76 0 h 3.46 v 28.95 h -3.46 Z M 362.81 0 h 3.46 v 94.86 h -3.46 Z M 410.87 0 h 3.46 v 81.73 h -3.46 Z M 458.92 0 h 3.46 v 24.62 h -3.46 Z M 506.97 0 h 3.46 v 96.86 h -3.46 Z M 555.02 0 h 3.46 v 43.33 h -3.46 Z M 603.08 0 h 3.46 v 8.44 h -3.46 Z M 651.13 0 h 3.46 v 44.97 h -3.46 Z"></path></g><g></g><g color="#BF0040" fill="#BF0040" stroke="#BF0040"><path d="M 30.6 0 h 3.46 v 38.35 h -3.46 Z M 78.65 0 h 3.46 v 51.09 h -3.46 Z M 126.7 0 h 3.46 v 0 h -3.46 Z M 174.76 0 h 3.46 v 73.44 h -3.46 Z M 222.81 0 h 3.46 v 0 h -3.46 Z M 270.86 0 h 3.46 v 44.31 h -3.46 Z M 318.91 0 h 3.46 v 30.38 h -3.46 Z M 366.97 0 h 3.46 v 84.68 h -3.46 Z M 415.02 0 h 3.46 v 79.43 h -3.46 Z M 463.07 0 h 3.46 v 28.79 h -3.46 Z M 511.12 0 h 3.46 v 94.17 h -3.46 Z M 559.18 0 h 3.46 v 41.69 h -3.46 Z M 607.23 0 h 3.46 v 8.98 h -3.46 Z M 655.28 0 h 3.46 v 35.12 h -3.46 Z"></path></g><g></g><g color="#579C35" fill="#579C35" stroke="#579C35"><path d="M 34.75 0 h 3.46 v 51.35 h -3.46 Z M 82.8 0 h 3.46 v 53.67 h -3.46 Z M 130.85 0 h 3.46 v 57.19 h -3.46 Z M 178.91 0 h 3.46 v 95.99 h -3.46 Z M 226.96 0 h 3.46 v 67.83 h -3.46 Z M 275.01 0 h 3.46 v 67.94 h -3.46 Z M 323.06 0 h 3.46 v 35.02 h -3.46 Z M 371.12 0 h 3.46 v 140.48 h -3.46 Z M 419.17 0 h 3.46 v 75.17 h -3.46 Z M 467.22 0 h 3.46 v 15.33 h -3.46 Z M 515.27 0 h 3.46 v 110.3 h -3.46 Z M 563.33 0 h 3.46 v 63.02 h -3.46 Z M 611.38 0 h 3.46 v 39.77 h -3.46 Z M 659.43 0 h 3.46 v 53.83 h -3.46 Z"></path></g><g></g></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -23.31 42.78)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.58"><em class="ltx_emph ltx_font_italic" id="Sx4.F3.pic1.20.20.20.20.20.1.1">Micro-F1</em></foreignobject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 20.46 96.76 h 411.92 v 39.87 h -411.92 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 24.61 99.53)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 25.72)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.58)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1 0 0 -1 2.08 0) translate(0.28,0)"><path d="M 0 8.3 L 8.3 5.53"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.28 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="35.61"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.21.21.21.21.21.1.1.1.1.1">Guess</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#4F81BD" fill="#4F81BD" stroke="#4F81BD" transform="matrix(1 0 0 -1 68.02 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 81.3 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.22.22.22.22.22.2.2.2.1.1">CoT</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#9BBB59" fill="#9BBB59" stroke="#9BBB59" transform="matrix(1 0 0 -1 185.71 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 199 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="76.58"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.23.23.23.23.23.3.3.3.1.1">CoT-RAG-P</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FFB303" fill="#FFB303" stroke="#FFB303" transform="matrix(1 0 0 -1 303.41 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 316.69 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.39"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.24.24.24.24.24.4.4.4.1.1">CoT-RAG-W</span></foreignobject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.72)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FFBFBF" fill="#FFBFBF" stroke="#FFBFBF" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.28 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.2"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.25.25.25.25.25.5.5.1.1.1">SC-CoT</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#008080" fill="#008080" stroke="#008080" transform="matrix(1 0 0 -1 68.02 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 81.3 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="98.88"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.26.26.26.26.26.6.6.2.1.1">SC-CoT-RAG-P</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#BF0040" fill="#BF0040" stroke="#BF0040" transform="matrix(1 0 0 -1 185.71 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 199 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="98.88"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.27.27.27.27.27.7.7.3.1.1">SC-CoT-RAG-P</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#579C35" fill="#579C35" stroke="#579C35" transform="matrix(1 0 0 -1 303.41 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 316.69 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="42.28"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F3.pic1.28.28.28.28.28.8.8.4.1.1">Vanilla</span></foreignobject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Breakdown of the Micro-F1 performance of each technique and the random guess baseline for all classification datasets, compared against the random guess baseline.</figcaption>
</figure>
<figure class="ltx_figure" id="Sx4.F4"><svg class="ltx_picture ltx_centering" height="248.61" id="Sx4.F4.pic1" overflow="visible" version="1.1" width="710.38"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,248.61) matrix(1 0 0 -1 0 0) translate(37.37,0) translate(0,95.94) matrix(1.0 0.0 0.0 1.0 -37.37 -95.94)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(37.37,0) translate(0,95.94)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 0 0 L 0 141.14 M 56.06 0 L 56.06 141.14 M 112.12 0 L 112.12 141.14 M 168.18 0 L 168.18 141.14 M 224.24 0 L 224.24 141.14 M 280.31 0 L 280.31 141.14 M 336.37 0 L 336.37 141.14 M 392.43 0 L 392.43 141.14 M 448.49 0 L 448.49 141.14 M 504.55 0 L 504.55 141.14 M 560.61 0 L 560.61 141.14 M 616.67 0 L 616.67 141.14 M 672.73 0 L 672.73 141.14" style="fill:none"></path></g><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.4pt"><path d="M 0 0 L 672.73 0 M 0 47.05 L 672.73 47.05 M 0 94.09 L 672.73 94.09 M 0 141.14 L 672.73 141.14" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 -5.91 L 0 0 M 56.06 -5.91 L 56.06 0 M 112.12 -5.91 L 112.12 0 M 168.18 -5.91 L 168.18 0 M 224.24 -5.91 L 224.24 0 M 280.31 -5.91 L 280.31 0 M 336.37 -5.91 L 336.37 0 M 392.43 -5.91 L 392.43 0 M 448.49 -5.91 L 448.49 0 M 504.55 -5.91 L 504.55 0 M 560.61 -5.91 L 560.61 0 M 616.67 -5.91 L 616.67 0 M 672.73 -5.91 L 672.73 0 M 0 147.04 L 0 141.14 M 56.06 147.04 L 56.06 141.14 M 112.12 147.04 L 112.12 141.14 M 168.18 147.04 L 168.18 141.14 M 224.24 147.04 L 224.24 141.14 M 280.31 147.04 L 280.31 141.14 M 336.37 147.04 L 336.37 141.14 M 392.43 147.04 L 392.43 141.14 M 448.49 147.04 L 448.49 141.14 M 504.55 147.04 L 504.55 141.14 M 560.61 147.04 L 560.61 141.14 M 616.67 147.04 L 616.67 141.14 M 672.73 147.04 L 672.73 141.14" style="fill:none"></path></g><g color="#808080" fill="#808080" fill-opacity="0" stroke="#808080" stroke-opacity="0" stroke-width="0.2pt"><path d="M 0 0 L 5.91 0 M 0 47.05 L 5.91 47.05 M 0 94.09 L 5.91 94.09 M 0 141.14 L 5.91 141.14 M 672.73 0 L 666.83 0 M 672.73 47.05 L 666.83 47.05 M 672.73 94.09 L 666.83 94.09 M 672.73 141.14 L 666.83 141.14" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 141.14 L 672.73 141.14 L 672.73 0 L 0 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 7.13 -47.09)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="39.01"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.1.1.1.1.1.1.1">AnEm</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 62.38 -47.91)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="40.17"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.2.2.2.2.2.1.1">MLEE</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 85.42 -80.93)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="86.87"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.3.3.3.3.3.1.1">BioNLP13-PC</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 140.46 -81.95)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="88.31"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.4.4.4.4.4.1.1">BioNLP13-CG</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 210.79 -67.68)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="69.28"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.5.5.5.5.5.1.1">GeneTag-C</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 265.9 -68.63)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="69.47"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.6.6.6.6.6.1.1">GENIA-EE</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 322.3 -68.29)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.15"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.7.7.7.7.7.1.1">GeneTag-G</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 374.49 -72.16)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="74.47"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.8.8.8.8.8.1.1">GENIA-PPI</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 413.29 -89.42)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="98.88"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.9.9.9.9.9.1.1">BioNLP13-GRO</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 514.4 -44.37)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="35.17"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.10.10.10.10.10.1.1">PICO</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 560.76 -54.29)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.04"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.11.11.11.11.11.1.1">BioInfer</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 583.72 -87.18)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="95.71"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.12.12.12.12.12.1.1">BioNLP11-REL</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 -3.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_italic" id="Sx4.F4.pic1.13.13.13.13.13.1.1">0</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 43.59)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_italic" id="Sx4.F4.pic1.14.14.14.14.14.1.1">5</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 87.17)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F4.pic1.15.15.15.15.15.1.1">10</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -4.89 134.22)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text ltx_font_italic" id="Sx4.F4.pic1.16.16.16.16.16.1.1">15</span></foreignobject></g><clippath id="pgfcp4"><path d="M 0 0 L 672.73 0 L 672.73 141.14 L 0 141.14 Z"></path></clippath><g clip-path="url(#pgfcp4)"><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 0 0 L 56.06 0"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 56.06 0 L 112.12 0"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 112.12 1.31 L 168.18 1.31"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 168.18 0.12 L 224.24 0.12"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 224.24 61.79 L 280.31 61.79"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 280.31 38.76 L 336.37 38.76"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 336.37 2.02 L 392.43 2.02"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 392.43 2.01 L 448.49 2.01"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 448.49 0.02 L 504.55 0.02"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 504.55 0.09 L 560.61 0.09"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 560.61 2.76 L 616.67 2.76"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 616.67 2.01 L 672.73 2.01"></path></g><g></g><g color="#4F81BD" fill="#4F81BD" stroke="#4F81BD"><path d="M 11.43 0 h 4.15 v 20.7 h -4.15 Z M 67.49 0 h 4.15 v 42.91 h -4.15 Z M 123.55 0 h 4.15 v 60.59 h -4.15 Z M 179.61 0 h 4.15 v 52.78 h -4.15 Z M 235.67 0 h 4.15 v 86.19 h -4.15 Z M 347.79 0 h 4.15 v 120.91 h -4.15 Z M 291.73 0 h 4.15 v 83.93 h -4.15 Z M 403.85 0 h 4.15 v 97.01 h -4.15 Z M 459.92 0 h 4.15 v 27 h -4.15 Z M 515.98 0 h 4.15 v 38.86 h -4.15 Z M 572.04 0 h 4.15 v 93.81 h -4.15 Z M 628.1 0 h 4.15 v 100.02 h -4.15 Z"></path></g><g></g><g color="#9BBB59" fill="#9BBB59" stroke="#9BBB59"><path d="M 16.27 0 h 4.15 v 45.26 h -4.15 Z M 72.33 0 h 4.15 v 32.46 h -4.15 Z M 128.39 0 h 4.15 v 61.91 h -4.15 Z M 184.45 0 h 4.15 v 32.46 h -4.15 Z M 240.51 0 h 4.15 v 10.73 h -4.15 Z M 352.64 0 h 4.15 v 62.1 h -4.15 Z M 296.58 0 h 4.15 v 65.11 h -4.15 Z M 408.7 0 h 4.15 v 96.16 h -4.15 Z M 464.76 0 h 4.15 v 9.88 h -4.15 Z M 520.82 0 h 4.15 v 22.02 h -4.15 Z M 576.88 0 h 4.15 v 95.03 h -4.15 Z M 632.94 0 h 4.15 v 80.17 h -4.15 Z"></path></g><g></g><g color="#FFB303" fill="#FFB303" stroke="#FFB303"><path d="M 21.11 0 h 4.15 v 63.7 h -4.15 Z M 77.17 0 h 4.15 v 50.62 h -4.15 Z M 133.23 0 h 4.15 v 23.05 h -4.15 Z M 189.3 0 h 4.15 v 34.34 h -4.15 Z M 245.36 0 h 4.15 v 71.51 h -4.15 Z M 357.48 0 h 4.15 v 127.78 h -4.15 Z M 301.42 0 h 4.15 v 85.62 h -4.15 Z M 413.54 0 h 4.15 v 97.2 h -4.15 Z M 469.6 0 h 4.15 v 21.45 h -4.15 Z M 525.66 0 h 4.15 v 33.59 h -4.15 Z M 581.72 0 h 4.15 v 93.34 h -4.15 Z M 637.79 0 h 4.15 v 94.09 h -4.15 Z"></path></g><g></g><g color="#FFBFBF" fill="#FFBFBF" stroke="#FFBFBF"><path d="M 25.96 0 h 4.15 v 43.19 h -4.15 Z M 82.02 0 h 4.15 v 32.27 h -4.15 Z M 138.08 0 h 4.15 v 61.06 h -4.15 Z M 194.14 0 h 4.15 v 32.08 h -4.15 Z M 250.2 0 h 4.15 v 7.9 h -4.15 Z M 362.32 0 h 4.15 v 19.76 h -4.15 Z M 306.26 0 h 4.15 v 78.57 h -4.15 Z M 418.38 0 h 4.15 v 104.82 h -4.15 Z M 474.44 0 h 4.15 v 16 h -4.15 Z M 530.51 0 h 4.15 v 28.98 h -4.15 Z M 586.57 0 h 4.15 v 112.63 h -4.15 Z M 642.63 0 h 4.15 v 76.5 h -4.15 Z"></path></g><g></g><g color="#008080" fill="#008080" stroke="#008080"><path d="M 30.8 0 h 4.15 v 47.33 h -4.15 Z M 86.86 0 h 4.15 v 51 h -4.15 Z M 142.92 0 h 4.15 v 14.49 h -4.15 Z M 198.98 0 h 4.15 v 64.64 h -4.15 Z M 255.04 0 h 4.15 v 69.72 h -4.15 Z M 367.17 0 h 4.15 v 116.11 h -4.15 Z M 311.1 0 h 4.15 v 82.24 h -4.15 Z M 423.23 0 h 4.15 v 106.32 h -4.15 Z M 479.29 0 h 4.15 v 16.18 h -4.15 Z M 535.35 0 h 4.15 v 30.96 h -4.15 Z M 591.41 0 h 4.15 v 114.98 h -4.15 Z M 647.47 0 h 4.15 v 101.34 h -4.15 Z"></path></g><g></g><g color="#BF0040" fill="#BF0040" stroke="#BF0040"><path d="M 35.64 0 h 4.15 v 31.9 h -4.15 Z M 91.7 0 h 4.15 v 36.04 h -4.15 Z M 147.76 0 h 4.15 v 61.63 h -4.15 Z M 203.82 0 h 4.15 v 26.63 h -4.15 Z M 259.89 0 h 4.15 v 9.88 h -4.15 Z M 372.01 0 h 4.15 v 33.03 h -4.15 Z M 315.95 0 h 4.15 v 68.03 h -4.15 Z M 428.07 0 h 4.15 v 98.42 h -4.15 Z M 484.13 0 h 4.15 v 15.43 h -4.15 Z M 540.19 0 h 4.15 v 25.78 h -4.15 Z M 596.25 0 h 4.15 v 113.1 h -4.15 Z M 652.31 0 h 4.15 v 80.26 h -4.15 Z"></path></g><g></g><g color="#579C35" fill="#579C35" stroke="#579C35"><path d="M 40.48 0 h 4.15 v 12.04 h -4.15 Z M 96.55 0 h 4.15 v 70.19 h -4.15 Z M 152.61 0 h 4.15 v 58.81 h -4.15 Z M 208.67 0 h 4.15 v 39.71 h -4.15 Z M 264.73 0 h 4.15 v 82.33 h -4.15 Z M 376.85 0 h 4.15 v 105.38 h -4.15 Z M 320.79 0 h 4.15 v 37.54 h -4.15 Z M 432.91 0 h 4.15 v 66.8 h -4.15 Z M 488.97 0 h 4.15 v 9.97 h -4.15 Z M 545.03 0 h 4.15 v 27.76 h -4.15 Z M 601.1 0 h 4.15 v 72.54 h -4.15 Z M 657.16 0 h 4.15 v 74.52 h -4.15 Z"></path></g><g></g></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -23.31 42.78)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.58"><em class="ltx_emph ltx_font_italic" id="Sx4.F4.pic1.17.17.17.17.17.1.1">Micro-F1</em></foreignobject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 20.46 96.76 h 411.92 v 39.87 h -411.92 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 24.61 99.53)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 25.72)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.58)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1 0 0 -1 2.08 0) translate(0.28,0)"><path d="M 0 8.3 L 8.3 5.53"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.28 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="35.61"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.18.18.18.18.18.1.1.1.1.1">Guess</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#4F81BD" fill="#4F81BD" stroke="#4F81BD" transform="matrix(1 0 0 -1 68.02 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 81.3 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.19.19.19.19.19.2.2.2.1.1">CoT</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#9BBB59" fill="#9BBB59" stroke="#9BBB59" transform="matrix(1 0 0 -1 185.71 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 199 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="76.58"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.20.20.20.20.20.3.3.3.1.1">CoT-RAG-P</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FFB303" fill="#FFB303" stroke="#FFB303" transform="matrix(1 0 0 -1 303.41 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 316.69 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.39"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.21.21.21.21.21.4.4.4.1.1">CoT-RAG-W</span></foreignobject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.72)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FFBFBF" fill="#FFBFBF" stroke="#FFBFBF" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.28 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.2"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.22.22.22.22.22.5.5.1.1.1">SC-CoT</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#008080" fill="#008080" stroke="#008080" transform="matrix(1 0 0 -1 68.02 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 81.3 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="98.88"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.23.23.23.23.23.6.6.2.1.1">SC-CoT-RAG-P</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#BF0040" fill="#BF0040" stroke="#BF0040" transform="matrix(1 0 0 -1 185.71 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 199 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="98.88"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.24.24.24.24.24.7.7.3.1.1">SC-CoT-RAG-P</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#579C35" fill="#579C35" stroke="#579C35" transform="matrix(1 0 0 -1 303.41 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 316.69 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="42.28"><span class="ltx_text ltx_font_smallcaps" id="Sx4.F4.pic1.25.25.25.25.25.8.8.4.1.1">Vanilla</span></foreignobject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Breakdown of each technique and the random guess baseline on all NER datasets as measured by the Micro-F1 scores. A prediction is counted as correct when both the span and its assigned label are found in the ground truth</figcaption>
</figure>
<figure class="ltx_figure" id="Sx4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="396" id="Sx4.F5.g1" src="extracted/5807097/single_vs_multi_cls.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of performance of each model in single label vs multi label datasets. Random baseline for single class classification is 0.415 and multi class classification is 0.215.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Detailed Comparison of Prompting Techniques</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx2.p1.1.1">The use of CoT and Self Consistency are not helpful if there is a lack of parametric knowledge about the task.</span> For BioMistral-7B, using Self-Consistency CoT prompting leads to the biggest reduction of about 16% for classification tasks. One possible reason is the domain-specific pre-training equips the model to better follow the instructions directly without needing additional reasoning structures, which seem detrimental. Similar to the RAG case, self-consistency seems to not consistently improve performance for NER. While Self Consistency aims to improve the reliability of Chain of Thought prompting by generating multiple reasoning paths and selecting the most consistent one, it might introduce additional complexity leading to errors or inconsistencies. This is especially true, if the model’s answers have low confidence scores due to insufficient parametric knowledge which prevents them to reliably solve these problems and would explain the observed performance drop. For NER tasks, the combination of Chain of Thought (CoT) and Self-Consistency prompting with RAG (Wikipedia) shows the most substantial performance difference between the 70B and 7B models. This suggests that larger models are more adept at leveraging external knowledge and complex reasoning strategies for entity recognition tasks if there is lack of parametric knowledge.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p2">
<p class="ltx_p" id="Sx4.SSx2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx2.p2.1.1">RAG does not help information extraction.</span> The quality and relevance of the retrieved information can significantly impact performance, as seen from the fact that there is an average drop of 16.91% when using RAG with PubMed Corpora and 16.47% when using RAG with Wikipedia corpora as compared to the best performing technique for classification. While incorporating external knowledge through RAG can be generally beneficial for QA based tasks <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib73" title="">2024</a>)</cite> where incorporating relevant facts to the given question can append relevant knowledge into the model, it is not as straightforward in classification and information extraction tasks. This has to especially be considered in the given task setting, where the model could be confused by the presence of irrelevant knowledge information which adds an additional layer of complexity in extracting the relevant information for answering the relevant questions.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p3">
<p class="ltx_p" id="Sx4.SSx2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx2.p3.1.1">SC helps models filter out irrelevant noise in case of RAG, but does not help CoT</span> While Self Consistency aims to improve the reliability of Chain of Thought prompting by generating multiple reasoning paths and selecting the most consistent one, is fundamentally dependent on the models epistemic certainty <cite class="ltx_cite ltx_citemacro_citep">(Yadkori et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib74" title="">2024</a>; Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib36" title="">2024</a>)</cite>. This hinders performance if the model’s answers have low confidence scores due to insufficient parametric knowledge which prevents them to reliably solve these problems and would explain the observed performance drop.
For BioMistral-7B, using Self-Consistency CoT prompting leads to the biggest reduction of about 16% for classification tasks. One possible reason is the domain-specific pre-training equips the model to better follow the instructions directly without needing additional reasoning structures, which seem detrimental. Similar to the RAG case, self-consistency seems to not consistently improve performance for NER. The combination of Chain of Thought (CoT) and Self-Consistency prompting with RAG (Wikipedia) shows the most substantial performance difference between the 70B and 7B models.
This suggests that larger models are more adept at leveraging external knowledge and complex reasoning strategies for entity recognition tasks to augment the lack of epistemic uncertainty.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Detailed Per-dataset analysis</h3>
<div class="ltx_para" id="Sx4.SSx3.p1">
<p class="ltx_p" id="Sx4.SSx3.p1.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx3.p1.1.1">Models Perform Significantly better on public datasets.</span> Models perform significantly better on public datasets (average accuracy of 30%) compared to private datasets (average accuracy of 12%). This might hint at possible data leakage during pre-training or instruction-tuning, as publicly available datasets are more likely to be included in a web-crawl or a dedicated instruction tuning dataset.
This might suggest that model performance on ‘unseen’ (yet publicly available) tasks could be a result of unintentional data leakage rather than a by product of reasoning or generalisation.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.p2">
<p class="ltx_p" id="Sx4.SSx3.p2.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx3.p2.1.1">Multilingual Performance is not Scale Dependent.</span> As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F1" title="Figure 1 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">1</span></a>, smaller models can match or even outperform larger models on Chinese and Japanese datasets but not on English datasets. This may be due to the heavy reliance on large English corpora during training, with limited exposure to medical contexts in other languages. This forces models to generalize compressed language representations to specialized domains, where overfitting on sparse languages may hinder larger models’ performance.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.p3">
<p class="ltx_p" id="Sx4.SSx3.p3.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx3.p3.1.1">LLMs struggle on tasks high complexity tasks</span> As seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F5" title="Figure 5 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">5</span></a>, LLMs seem to struggle to outperform random baselines for both single and multi class classification tasks. However, Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F3" title="Figure 3 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">3</span></a> paints a more nuanced picture: guessing baseine remains unbeaten only on two of 14 datasets, which drags down the average performance significantly.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.p4">
<p class="ltx_p" id="Sx4.SSx3.p4.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F3" title="Figure 3 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#Sx4.F4" title="Figure 4 ‣ Overview of results ‣ Evaluation Results ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">4</span></a> show that Llama2 70B demonstrates good performance in low-complexity tasks such as disease and symptom classification (<span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.1">CZIBase</span>, <span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.2">NTCIR13-En</span>) and medium-complexity tasks like Gene Expression classification (<span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.3">Geo</span>). However, the model is challenged by higher-complexity problems, such as the <span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.4">BioNLP13-CG</span> and <span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.5">GENIA-EE</span> datasets.Specifically, in datasets that demand nuanced understanding and interpretation, such as the extraction of participants and outcomes from abstracts and gene ontology population (<span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.6">PICO</span>, <span class="ltx_text ltx_font_smallcaps" id="Sx4.SSx3.p4.1.7">BioNLP13-GRO</span>) the performance is low. When incorporating RAG (Retrieval-Augmented Generation) techniques, there are fluctuations in performance across datasets. While results improve on some datasets, RAG does not universally benefit the model’s ability to accurately extract and classify biomedical information.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">We provide a comprehensive benchmark and analysis of LLMs in Medical Classification and Named Entity Recognition tasks, revealing several key insights that have significant implications for the field.
We carry out a critical investigation of broad claims regarding LLM capabilities by replicating them in various contexts, domains and datasets.
We find that models suffer from fundamental drawbacks in generalizability, which hinder their performance in structured information extraction tasks on domain specific problems.
This leads to Standard prompting outperforming more advanced methods across both the tasks.
Our findings underscore the paramount importance of parametric knowledge capacity in zero-shot settings, regardless of advanced techniques used to augment external knowledge or model reasoning.</p>
</div>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. The authors thank Abhinav Ramesh Kashyap, Andy T. Liu and Vijay Prakash Dwivedi for their comments and useful feedback during the work. The authors further acknowledge and are thankful for the use of Imperial College Research Computing Service (DOI: <span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">http://doi.org/10.14469/hpc/2232</span>), and the Computational Shared Facility at The University of Manchester.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abburi et al. (2023)</span>
<span class="ltx_bibblock">
Abburi, H.; Suesserman, M.; Pudota, N.; Veeramani, B.; Bowen, E.; and Bhattacharya, S. 2023.

</span>
<span class="ltx_bibblock">Generative AI Text Classification using Ensemble LLM Approaches.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IberLEF@SEPLN</em>, volume 3496 of <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">CEUR Workshop Proceedings</em>. CEUR-WS.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biswas (2023)</span>
<span class="ltx_bibblock">
Biswas, S. S. 2023.

</span>
<span class="ltx_bibblock">Role of chat gpt in public health.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Annals of biomedical engineering</em>, 51(5): 868–869.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolton et al. (2024)</span>
<span class="ltx_bibblock">
Bolton, E.; Venigalla, A.; Yasunaga, M.; Hall, D.; Xiong, B.; Lee, T.; Daneshjou, R.; Frankle, J.; Liang, P.; Carbin, M.; et al. 2024.

</span>
<span class="ltx_bibblock">Biomedlm: A 2.7 b parameter language model trained on biomedical text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2403.18421</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bravo et al. (2015)</span>
<span class="ltx_bibblock">
Bravo, À.; Piñero, J.; Queralt-Rosinach, N.; Rautschka, M.; and Furlong, L. I. 2015.

</span>
<span class="ltx_bibblock">Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">BMC Bioinformatics</em>, 16(1).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budler, Gosak, and Stiglic (2023)</span>
<span class="ltx_bibblock">
Budler, L. C.; Gosak, L.; and Stiglic, G. 2023.

</span>
<span class="ltx_bibblock">Review of artificial intelligence-based question-answering systems in healthcare.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">WIREs Data. Mining. Knowl. Discov.</em>, 13(2).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Chen, Q.; Allot, A.; Leaman, R.; Doğan, R. I.; and Lu, Z. 2021.

</span>
<span class="ltx_bibblock">Overview of the BioCreative VII LitCovid Track: multi-label topic classification for COVID-19 literature annotation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the seventh BioCreative challenge evaluation workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Chen, S.; Ju, Z.; Dong, X.; Fang, H.; Wang, S.; Yang, Y.; Zeng, J.; Zhang, R.; Zhang, R.; Zhou, M.; Zhu, P.; and Xie, P. 2020.

</span>
<span class="ltx_bibblock">MedDialog: A Large-scale Medical Dialogue Dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2004.03329.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Chen, Z.; Cano, A. H.; Romanou, A.; Bonnet, A.; Matoba, K.; Salvi, F.; Pagliardini, M.; Fan, S.; Köpf, A.; Mohtashami, A.; et al. 2023.

</span>
<span class="ltx_bibblock">Meditron-70b: Scaling medical pretraining for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2311.16079</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohan et al. (2019)</span>
<span class="ltx_bibblock">
Cohan, A.; Ammar, W.; van Zuylen, M.; and Cady, F. 2019.

</span>
<span class="ltx_bibblock">Structural Scaffolds for Citation Intent Classification in Scientific Publications.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Conference of the North American Chapter of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doğan, Leaman, and Lu (2014)</span>
<span class="ltx_bibblock">
Doğan, R. I.; Leaman, R.; and Lu, Z. 2014.

</span>
<span class="ltx_bibblock">NCBI disease corpus: a resource for disease name recognition and concept normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Journal of biomedical informatics</em>, 47: 1–10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze et al. (2024)</span>
<span class="ltx_bibblock">
Douze, M.; Guzhva, A.; Deng, C.; Johnson, J.; Szilvasy, G.; Mazaré, P.-E.; Lomeli, M.; Hosseini, L.; and Jégou, H. 2024.

</span>
<span class="ltx_bibblock">The Faiss library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elucidata (2022)</span>
<span class="ltx_bibblock">
Elucidata, I. 2022.

</span>
<span class="ltx_bibblock">GEOKhoj v1.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/ElucidataInc/GEOKhoj-datasets/tree/main/geokhoj_v1</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2024)</span>
<span class="ltx_bibblock">
Feng, H.; Ronzano, F.; LaFleur, J.; Garber, M.; de Oliveira, R.; Rough, K.; Roth, K.; Nanavati, J.; Zine El Abidine, K.; and Mack, C. 2024.

</span>
<span class="ltx_bibblock">Evaluation of large language model performance on the Biomedical Language Understanding and Reasoning Benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">medRxiv</em>, 2024–05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fries et al. (2022)</span>
<span class="ltx_bibblock">
Fries, J.; Weber, L.; Seelam, N.; Altay, G.; Datta, D.; Garda, S.; Kang, S.; Su, R.; Kusa, W.; Cahyawijaya, S.; et al. 2022.

</span>
<span class="ltx_bibblock">Bigbio: A framework for data-centric biomedical natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural Information Processing Systems</em>, 35: 25792–25806.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grabar, Claveau, and Dalloux (2018)</span>
<span class="ltx_bibblock">
Grabar, N.; Claveau, V.; and Dalloux, C. 2018.

</span>
<span class="ltx_bibblock">CAS: French Corpus with Clinical Cases.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</em>, 122–128. Brussels, Belgium: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutierrez et al. (2022)</span>
<span class="ltx_bibblock">
Gutierrez, B. J.; McNeal, N.; Washington, C.; Chen, Y.; Li, L.; Sun, H.; and Su, Y. 2022.

</span>
<span class="ltx_bibblock">Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">EMNLP (Findings)</em>, 4497–4512. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hadi et al. (2023)</span>
<span class="ltx_bibblock">
Hadi, M. U.; Qureshi, R.; Shah, A.; Irfan, M.; Zafar, A.; Shaikh, M. B.; Akhtar, N.; Wu, J.; Mirjalili, S.; et al. 2023.

</span>
<span class="ltx_bibblock">A survey on large language models: Applications, challenges, limitations, and practical usage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Authorea Preprints</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harris (2023)</span>
<span class="ltx_bibblock">
Harris, E. 2023.

</span>
<span class="ltx_bibblock">Large language models answer medical questions accurately, but can’t match clinicians’ knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">JAMA</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoehndorf et al. (2010)</span>
<span class="ltx_bibblock">
Hoehndorf, R.; cyrille Ngonga Ngomo, A.; Pyysalo, S.; Ohta, T.; Oellrich, A.; and Rebholz-schuhmann, D. 2010.

</span>
<span class="ltx_bibblock">Applying ontology design patterns to the implementation of relations in GENIA.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Fourth International Symposium for Semantic Mining in Biomedicine</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iso et al. (2017)</span>
<span class="ltx_bibblock">
Iso, H.; Ruiz, C.; Murayama, T.; Taguchi, K.; Takeuchi, R.; Yamamoto, H.; Wakamiya, S.; and Aramaki, E. 2017.

</span>
<span class="ltx_bibblock">NTCIR13 MedWeb Task: multi-label classification of tweets using an ensemble of neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">NTCIR</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al. (2024)</span>
<span class="ltx_bibblock">
Jeong, M.; Hwang, H.; Yoon, C.; Lee, T.; and Kang, J. 2024.

</span>
<span class="ltx_bibblock">OLAPH: Improving Factuality in Biomedical Long-form Question Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2405.12701</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang and Xu (2024)</span>
<span class="ltx_bibblock">
Jiang, C.; and Xu, W. 2024.

</span>
<span class="ltx_bibblock">MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2405.02144</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson, Douze, and Jégou (2019)</span>
<span class="ltx_bibblock">
Johnson, J.; Douze, M.; and Jégou, H. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">IEEE Transactions on Big Data</em>, 7(3): 535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jullien et al. (2023)</span>
<span class="ltx_bibblock">
Jullien, M.; Valentino, M.; Frost, H.; O’Regan, P.; Landers, D.; and Freitas, A. 2023.

</span>
<span class="ltx_bibblock">NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 16745–16764. Stroudsburg, PA, USA: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaur, Ginige, and Obst (2023)</span>
<span class="ltx_bibblock">
Kaur, R.; Ginige, J. A.; and Obst, O. 2023.

</span>
<span class="ltx_bibblock">AI-based ICD coding and classification approaches using discharge summaries: A systematic literature review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Expert Syst. Appl.</em>, 213(Part): 118997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2007)</span>
<span class="ltx_bibblock">
Kim, H.; Goryachev, S.; Rosemblat, G.; Browne, A.; Keselman, A.; and Zeng-Treitler, Q. 2007.

</span>
<span class="ltx_bibblock">Beyond surface characteristics: a new health text-specific readability measurement.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">AMIA Annual Symposium Proceedings</em>, volume 2007, 418. American Medical Informatics Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2009)</span>
<span class="ltx_bibblock">
Kim, J.-D.; Ohta, T.; Pyysalo, S.; Kano, Y.; and Tsujii, J. 2009.

</span>
<span class="ltx_bibblock">Overview of BioNLP’09 Shared Task on Event Extraction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task</em>, 1–9. Boulder, Colorado: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2013)</span>
<span class="ltx_bibblock">
Kim, J.-j.; Han, X.; Lee, V.; and Rebholz-Schuhmann, D. 2013.

</span>
<span class="ltx_bibblock">GRO Task: Populating the Gene Regulation Ontology with events and relations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the BioNLP Shared Task 2013 Workshop</em>, 50–57. Sofia, Bulgaria: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocaman and Talby (2021)</span>
<span class="ltx_bibblock">
Kocaman, V.; and Talby, D. 2021.

</span>
<span class="ltx_bibblock">Biomedical named entity recognition at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part I</em>, 635–646. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krallinger et al. (2015)</span>
<span class="ltx_bibblock">
Krallinger, M.; Leitner, F.; Rabal, O.; Vazquez, M.; Oyarzabal, J.; and Valencia, A. 2015.

</span>
<span class="ltx_bibblock">CHEMDNER: The drugs and chemical names extraction challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Journal of cheminformatics</em>, 7: 1–11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labrak et al. (2024)</span>
<span class="ltx_bibblock">
Labrak, Y.; Bazoge, A.; Morin, E.; Gourraud, P.-A.; Rouvier, M.; and Dufour, R. 2024.

</span>
<span class="ltx_bibblock">Biomistral: A collection of open-source pretrained large language models for medical domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2402.10373</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lampert, Nickisch, and Harmeling (2014)</span>
<span class="ltx_bibblock">
Lampert, C. H.; Nickisch, H.; and Harmeling, S. 2014.

</span>
<span class="ltx_bibblock">Attribute-Based Classification for Zero-Shot Visual Object Categorization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em>, 36(3): 453–465.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Advances in Neural Information Processing Systems</em>, 33: 9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Li, H.; Wu, Y.; Schlegel, V.; Batista-Navarro, R.; Nguyen, T.; Kashyap, A. R.; Zeng, X.; Beck, D.; Winkler, S.; and Nenadic, G. 2023.

</span>
<span class="ltx_bibblock">Team: PULSAR at ProbSum 2023: PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients’ Problems and Data Augmentation with Black-box Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">BioNLP@ACL</em>, 503–509. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Li, M.; Zhou, H.; Yang, H.; and Zhang, R. 2024.

</span>
<span class="ltx_bibblock">RT: a Retrieving and Chain-of-Thought framework for few-shot medical named entity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Journal of the American Medical Informatics Association</em>, ocae095.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Liu, L.; Pan, Y.; Li, X.; and Chen, G. 2024.

</span>
<span class="ltx_bibblock">Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2404.15993</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manes et al. (2024)</span>
<span class="ltx_bibblock">
Manes, I.; Ronn, N.; Cohen, D.; Ber, R. I.; Horowitz-Kugler, Z.; and Stanovsky, G. 2024.

</span>
<span class="ltx_bibblock">K-QA: A Real-World Medical Q&amp;A Benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">CoRR</em>, abs/2401.14493.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munnangi et al. (2024)</span>
<span class="ltx_bibblock">
Munnangi, M.; Feldman, S.; Wallace, B. C.; Amir, S.; Hope, T.; and Naik, A. 2024.

</span>
<span class="ltx_bibblock">On-the-fly Definition Augmentation of LLMs for Biomedical NER.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2404.00152</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nori et al. (2023)</span>
<span class="ltx_bibblock">
Nori, H.; Lee, Y. T.; Zhang, S.; Carignan, D.; Edgar, R.; Fusi, N.; King, N.; Larson, J.; Li, Y.; Liu, W.; Luo, R.; McKinney, S. M.; Ness, R. O.; Poon, H.; Qin, T.; Usuyama, N.; White, C.; and Horvitz, E. 2023.

</span>
<span class="ltx_bibblock">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">CoRR</em>, abs/2311.16452.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nye et al. (2018)</span>
<span class="ltx_bibblock">
Nye, B.; Li, J. J.; Patel, R.; Yang, Y.; Marshall, I.; Nenkova, A.; and Wallace, B. 2018.

</span>
<span class="ltx_bibblock">A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 197–207. Melbourne, Australia: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ohta et al. (2010)</span>
<span class="ltx_bibblock">
Ohta, T.; Pyysalo, S.; Kim, J.-D.; and Tsujii, J. 2010.

</span>
<span class="ltx_bibblock">A reevaluation of biomedical named entity - term relations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Journal of bioinformatics and computational biology</em>, 8: 917–28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ohta et al. (2013)</span>
<span class="ltx_bibblock">
Ohta, T.; Pyysalo, S.; Rak, R.; Rowley, A.; Chun, H.-W.; Jung, S.-J.; Choi, S.-P.; Ananiadou, S.; and Tsujii, J. 2013.

</span>
<span class="ltx_bibblock">Overview of the Pathway Curation (PC) task of BioNLP Shared Task 2013.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the BioNLP Shared Task 2013 Workshop</em>, 67–75. Sofia, Bulgaria: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ohta et al. (2012)</span>
<span class="ltx_bibblock">
Ohta, T.; Pyysalo, S.; Tsujii, J.; and Ananiadou, S. 2012.

</span>
<span class="ltx_bibblock">Open-domain Anatomical Entity Mention Detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the Workshop on Detecting Structure in Scholarly Discourse</em>, volume W12-43. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Picco et al. (2024)</span>
<span class="ltx_bibblock">
Picco, G.; Fuchs, L.; Galindo, M. M.; Purpura, A.; López, V.; and Lam, H. T. 2024.

</span>
<span class="ltx_bibblock">Description Boosting for Zero-Shot Entity and Relation Classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2406.02245.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pyysalo et al. (2007)</span>
<span class="ltx_bibblock">
Pyysalo, S.; Ginter, F.; Heimonen, J.; Bj"orne, J.; Boberg, J.; J"arvinen, J.; and Salakoski, T. 2007.

</span>
<span class="ltx_bibblock">BioInfer: a corpus for information extraction in the biomedical domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">BMC bioinformatics</em>, 8(1): 1–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pyysalo, Ohta, and Ananiadou (2013)</span>
<span class="ltx_bibblock">
Pyysalo, S.; Ohta, T.; and Ananiadou, S. 2013.

</span>
<span class="ltx_bibblock">Overview of the Cancer Genetics (CG) task of BioNLP Shared Task 2013.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the BioNLP Shared Task 2013 Workshop</em>, 58–66. Sofia, Bulgaria: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pyysalo et al. (2009)</span>
<span class="ltx_bibblock">
Pyysalo, S.; Ohta, T.; Kim, J.-D.; and Tsujii, J. 2009.

</span>
<span class="ltx_bibblock">Static Relations: a Piece in the Biomedical Information Extraction Puzzle.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the BioNLP 2009 Workshop</em>, 1–9. Boulder, Colorado: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pyysalo et al. (2012)</span>
<span class="ltx_bibblock">
Pyysalo, S.; Ohta, T.; Miwa, M.; Cho, H.-C.; Tsujii, J.; and Ananiadou, S. 2012.

</span>
<span class="ltx_bibblock">Event extraction across multiple levels of biological organization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Bioinformatics</em>, 28(18): i575–i581.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pyysalo, Ohta, and Tsujii (2011)</span>
<span class="ltx_bibblock">
Pyysalo, S.; Ohta, T.; and Tsujii, J. 2011.

</span>
<span class="ltx_bibblock">Overview of the Entity Relations (REL) Supporting Task of BioNLP Shared Task 2011.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the BioNLP Shared Task 2011 Workshop</em>, BioNLP Shared Task ’11, 83–88. USA: Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">ISBN 9781937284091.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Reimers, N.; and Gurevych, I. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">EMNLP/IJCNLP (1)</em>, 3980–3990. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanyal, Bhowmick, and Das (2021)</span>
<span class="ltx_bibblock">
Sanyal, D. K.; Bhowmick, P. K.; and Das, P. P. 2021.

</span>
<span class="ltx_bibblock">A review of author name disambiguation techniques for the PubMed bibliographic database.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">J. Inf. Sci.</em>, 47(2).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlegel et al. (2023)</span>
<span class="ltx_bibblock">
Schlegel, V.; Li, H.; Wu, Y.; Subramanian, A.; Nguyen, T.; Kashyap, A. R.; Beck, D.; Zeng, X.; Batista-Navarro, R. T.; Winkler, S.; and Nenadic, G. 2023.

</span>
<span class="ltx_bibblock">PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">CLEF (Working Notes)</em>, volume 3497 of <em class="ltx_emph ltx_font_italic" id="bib.bib53.2.2">CEUR Workshop Proceedings</em>, 1668–1679. CEUR-WS.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Shen, Y.; Ma, X.; Tan, Z.; Zhang, S.; Wang, W.; and Lu, W. 2021.

</span>
<span class="ltx_bibblock">Locate and label: A two-stage identifier for nested named entity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2105.06804</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2024)</span>
<span class="ltx_bibblock">
Shi, W.; Xu, R.; Zhuang, Y.; Yu, Y.; Wu, H.; Yang, C.; and Wang, M. D. 2024.

</span>
<span class="ltx_bibblock">MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning.

</span>
<span class="ltx_bibblock">arXiv:2405.03000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. (2023)</span>
<span class="ltx_bibblock">
Singhal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung, H. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; et al. 2023.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Nature</em>, 620(7972): 172–180.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2024)</span>
<span class="ltx_bibblock">
Song, Y.; Zhang, J.; Tian, Z.; Yang, Y.; Huang, M.; and Li, D. 2024.

</span>
<span class="ltx_bibblock">LLM-based privacy data augmentation guided by knowledge distillation with a distribution tutor for medical text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2402.16515</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soong et al. (2023)</span>
<span class="ltx_bibblock">
Soong, D.; Sridhar, S.; Si, H.; Wagner, J.-S.; Sá, A. C. C.; Yu, C. Y.; Karagoz, K.; Guan, M.; Hamadeh, H.; and Higgs, B. W. 2023.

</span>
<span class="ltx_bibblock">Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2305.17116</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2024)</span>
<span class="ltx_bibblock">
Srivastava, S.; PV, A.; Menon, S.; Sukumar, A.; Philipose, A.; Prince, S.; Thomas, S.; et al. 2024.

</span>
<span class="ltx_bibblock">Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2402.19450</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subramanian et al. (2024)</span>
<span class="ltx_bibblock">
Subramanian, A.; Schlegel, V.; Ramesh Kashyap, A.; Nguyen, T.-T.; Dwivedi, V. P.; and Winkler, S. 2024.

</span>
<span class="ltx_bibblock">M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering.

</span>
<span class="ltx_bibblock">In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Findings of the Association for Computational Linguistics ACL 2024</em>, 4002–4042. Bangkok, Thailand and virtual meeting: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanabe et al. (2005)</span>
<span class="ltx_bibblock">
Tanabe, L.; Xie, N.; Thom, L. H.; Matten, W.; and Wilbur, W. J. 2005.

</span>
<span class="ltx_bibblock">GENETAG: a tagged corpus for gene/protein named entity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">BMC Bioinformatics</em>, 6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Wang, J.; Yang, Z.; Yao, Z.; and Yu, H. 2024a.

</span>
<span class="ltx_bibblock">Jmlr: Joint medical llm and retrieval training for enhancing reasoning and professional question answering capability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2402.17887</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2203.11171</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Zhou (2024)</span>
<span class="ltx_bibblock">
Wang, X.; and Zhou, D. 2024.

</span>
<span class="ltx_bibblock">Chain-of-thought reasoning without prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2402.10200</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Wang, Y.; Peng, X.; Shen, T.; Clarke, A.; Schlegel, C.; Martin, P.; and Long, G. 2023.

</span>
<span class="ltx_bibblock">Soft Prompt Transfer for Zero-Shot and Few-Shot Learning in EHR Understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">ADMA (3)</em>, volume 14178 of <em class="ltx_emph ltx_font_italic" id="bib.bib66.2.2">Lecture Notes in Computer Science</em>, 18–32. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Wang, Z.; Liu, A.; Lin, H.; Li, J.; Ma, X.; and Liang, Y. 2024b.

</span>
<span class="ltx_bibblock">Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2403.05313</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Advances in neural information processing systems</em>, 35: 24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2020)</span>
<span class="ltx_bibblock">
Wei, Q.; Ji, Z.; Li, Z.; Du, J.; Wang, J.; Xu, J.; Xiang, Y.; Tiryaki, F.; Wu, S.; Zhang, Y.; Tao, C.; and Xu, H. 2020.

</span>
<span class="ltx_bibblock">A study of deep learning approaches for medication and adverse drug event extraction from clinical text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">J. Am. Medical Informatics Assoc.</em>, 27(1): 13–21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Willard and Louf (2023)</span>
<span class="ltx_bibblock">
Willard, B. T.; and Louf, R. 2023.

</span>
<span class="ltx_bibblock">Efficient guided generation for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv e-prints</em>, arXiv–2307.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2024)</span>
<span class="ltx_bibblock">
Xie, Q.; Chen, Q.; Chen, A.; Peng, C.; Hu, Y.; Lin, F.; Peng, X.; Huang, J.; Zhang, J.; Keloth, V.; et al. 2024.

</span>
<span class="ltx_bibblock">Me llama: Foundation large language models for medical applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2402.12749</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Xie, T.; Li, Q.; Zhang, J.; Zhang, Y.; Liu, Z.; and Wang, H. 2023.

</span>
<span class="ltx_bibblock">Empirical study of zero-shot ner with chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2310.10035</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. (2024)</span>
<span class="ltx_bibblock">
Xiong, G.; Jin, Q.; Lu, Z.; and Zhang, A. 2024.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2402.13178</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yadkori et al. (2024)</span>
<span class="ltx_bibblock">
Yadkori, Y. A.; Kuzborskij, I.; György, A.; and Szepesvári, C. 2024.

</span>
<span class="ltx_bibblock">To Believe or Not to Believe Your LLM.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2406.02543</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yim et al. (2023)</span>
<span class="ltx_bibblock">
Yim, W.; Fu, Y.; Abacha, A. B.; Snider, N.; Lin, T.; and Yetisgen, M. 2023.

</span>
<span class="ltx_bibblock">ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">CoRR</em>, abs/2306.02022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yu, G.; Liu, L.; Jiang, H.; Shi, S.; and Ao, X. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented few-shot text classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, 6721–6735.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Zhang, M.; Wang, B.; Fei, H.; and Zhang, M. 2024.

</span>
<span class="ltx_bibblock">In-Context Learning for Few-Shot Nested Named Entity Recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">ICASSP</em>, 10026–10030. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng, Yu et al. (2018)</span>
<span class="ltx_bibblock">
Zheng, J.; Yu, H.; et al. 2018.

</span>
<span class="ltx_bibblock">Assessing the readability of medical documents: a ranking approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">JMIR medical informatics</em>, 6(1): e8611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Zhou, W.; Zhang, S.; Gu, Y.; Chen, M.; and Poon, H. 2023.

</span>
<span class="ltx_bibblock">Universalner: Targeted distillation from large language models for open named entity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2308.03279</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zolnoori et al. (2019)</span>
<span class="ltx_bibblock">
Zolnoori, M.; Fung, K. W.; Patrick, T. B.; Fontelo, P.; Kharrazi, H.; Faiola, A.; Wu, Y. S. S.; Eldredge, C. E.; Luo, J.; Conway, M.; Zhu, J.; Park, S. K.; Xu, K.; Moayyed, H.; and Goudarzvand, S. 2019.

</span>
<span class="ltx_bibblock">A systematic approach for developing a corpus of patient reported adverse drug events: A case study for SSRI and SNRI medications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Journal of Biomedical Informatics</em>, 90.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix A: Datasets</h2>
<div class="ltx_para" id="Ax1.p1">
<p class="ltx_p" id="Ax1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#A1.T2" title="Table 2 ‣ Appendix A Appendix B: Compute Details ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#A1.T3" title="Table 3 ‣ Appendix A Appendix B: Compute Details ‣ LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction"><span class="ltx_text ltx_ref_tag">3</span></a> list the huggingface dataset cards and citations for each classification and ner dataset used in the paper respectively. 
<br class="ltx_break"/>For datasets considered private, we assume that models have not been trained on these datasets due to their restricted access, which requires Data Use Agreements (DUAs) and other permissions. Consequently, the likelihood of these datasets being included in common web crawls is low. 
<br class="ltx_break"/>We have signed all the relevant Data Use Agreements (DUAs) and strictly adhere to their provisions. We do not redistribute the data and advise those wishing to reproduce experiments involving private datasets to consult the corresponding Hugging Face dataset cards for guidance on obtaining the necessary data.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix B: Compute Details</h2>
<div class="ltx_para" id="A1.p1">
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">Hardware used (GPU/CPU): We used a mix of different shared computational facilities with nVidia A100-SXM4-80GB, RTX6000 with 24GB and L40S with 48GB. Debian OS was used for all the compute servers.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">Memory: The machines used had between 256 GB and 1TB of memory</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Software and libraries used: The environment can be reproduced from the
textttenvironment.yaml file in the supplementary material</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">Model details: The models used have been described in detail in the main paper submission under the Models subsection of the Methodology section.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">Random seed of 42 was used for all random sampling purposes</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="A1.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T2.1">
<tr class="ltx_tr" id="A1.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.1.1.1">
<span class="ltx_p" id="A1.T2.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.1.1.1">Dataset Name</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.1.2.1">
<span class="ltx_p" id="A1.T2.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.2.1.1.1">HuggingFace Card</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.1.3.1">
<span class="ltx_p" id="A1.T2.1.1.3.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.3.1.1.1">Citation</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.2.1.1">
<span class="ltx_p" id="A1.T2.1.2.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.2.1.1.1.1">GAD</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.2.2.1">
<span class="ltx_p" id="A1.T2.1.2.2.1.1" style="width:71.1pt;">bigbio/gad</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.2.3.1">
<span class="ltx_p" id="A1.T2.1.2.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Bravo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib4" title="">2015</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.3.1.1">
<span class="ltx_p" id="A1.T2.1.3.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.3.1.1.1.1">GEO</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.3.2.1">
<span class="ltx_p" id="A1.T2.1.3.2.1.1" style="width:71.1pt;">bigbio/geokhoj_v1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.3.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.3.3.1">
<span class="ltx_p" id="A1.T2.1.3.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Elucidata <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib12" title="">2022</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.4.1.1">
<span class="ltx_p" id="A1.T2.1.4.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.4.1.1.1.1">MedDialog</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.4.2.1">
<span class="ltx_p" id="A1.T2.1.4.2.1.1" style="width:71.1pt;">bigbio/meddialog</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.4.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.4.3.1">
<span class="ltx_p" id="A1.T2.1.4.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib7" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.5.1.1">
<span class="ltx_p" id="A1.T2.1.5.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.5.1.1.1.1">CZIBase</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.5.2.1">
<span class="ltx_p" id="A1.T2.1.5.2.1.1" style="width:71.1pt;">bigbio/czi_drsm</span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.5.3"></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.6.1.1">
<span class="ltx_p" id="A1.T2.1.6.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.6.1.1.1.1">CZIQoL</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.6.2.1">
<span class="ltx_p" id="A1.T2.1.6.2.1.1" style="width:71.1pt;">bigbio/czi_drsm</span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.6.3"></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.7.1.1">
<span class="ltx_p" id="A1.T2.1.7.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.7.1.1.1.1">CZINatHist</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.7.2.1">
<span class="ltx_p" id="A1.T2.1.7.2.1.1" style="width:71.1pt;">bigbio/czi_drsm</span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.7.3"></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.8.1.1">
<span class="ltx_p" id="A1.T2.1.8.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.8.1.1.1.1">LitCovid</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.8.2.1">
<span class="ltx_p" id="A1.T2.1.8.2.1.1" style="width:71.1pt;">bigbio/bc7_litcovid</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.8.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.8.3.1">
<span class="ltx_p" id="A1.T2.1.8.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib6" title="">2021</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.9.1.1">
<span class="ltx_p" id="A1.T2.1.9.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.9.1.1.1.1">CAS</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.9.2.1">
<span class="ltx_p" id="A1.T2.1.9.2.1.1" style="width:71.1pt;">bigbio/cas</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.9.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.9.3.1">
<span class="ltx_p" id="A1.T2.1.9.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Grabar, Claveau, and Dalloux <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib15" title="">2018</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.10.1.1">
<span class="ltx_p" id="A1.T2.1.10.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.10.1.1.1.1">ESSAI</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.10.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.10.2.1">
<span class="ltx_p" id="A1.T2.1.10.2.1.1" style="width:71.1pt;">bigbio/essai</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.10.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.10.3.1">
<span class="ltx_p" id="A1.T2.1.10.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Grabar, Claveau, and Dalloux <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib15" title="">2018</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.11.1.1">
<span class="ltx_p" id="A1.T2.1.11.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.11.1.1.1.1">NTCIR13-Ja</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.11.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.11.2.1">
<span class="ltx_p" id="A1.T2.1.11.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T2.1.11.2.1.1.1"></span><span class="ltx_text" id="A1.T2.1.11.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T2.1.11.2.1.1.2.1">
<span class="ltx_tr" id="A1.T2.1.11.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.11.2.1.1.2.1.1.1">bigbio/ntcir_13</span></span>
<span class="ltx_tr" id="A1.T2.1.11.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.11.2.1.1.2.1.2.1">_medweb</span></span>
</span></span><span class="ltx_text" id="A1.T2.1.11.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.11.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.11.3.1">
<span class="ltx_p" id="A1.T2.1.11.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Iso et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib20" title="">2017</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.12.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.12.1.1">
<span class="ltx_p" id="A1.T2.1.12.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.12.1.1.1.1">NTCIR13-En</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.12.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.12.2.1">
<span class="ltx_p" id="A1.T2.1.12.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T2.1.12.2.1.1.1"></span><span class="ltx_text" id="A1.T2.1.12.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T2.1.12.2.1.1.2.1">
<span class="ltx_tr" id="A1.T2.1.12.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.12.2.1.1.2.1.1.1">bigbio/ntcir_13</span></span>
<span class="ltx_tr" id="A1.T2.1.12.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.12.2.1.1.2.1.2.1">_medweb</span></span>
</span></span><span class="ltx_text" id="A1.T2.1.12.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.12.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.12.3.1">
<span class="ltx_p" id="A1.T2.1.12.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Iso et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib20" title="">2017</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.13.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.13.1.1">
<span class="ltx_p" id="A1.T2.1.13.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.13.1.1.1.1">NTCIR13-Zh</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.13.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.13.2.1">
<span class="ltx_p" id="A1.T2.1.13.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T2.1.13.2.1.1.1"></span><span class="ltx_text" id="A1.T2.1.13.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T2.1.13.2.1.1.2.1">
<span class="ltx_tr" id="A1.T2.1.13.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.13.2.1.1.2.1.1.1">bigbio/ntcir_13</span></span>
<span class="ltx_tr" id="A1.T2.1.13.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T2.1.13.2.1.1.2.1.2.1">_medweb</span></span>
</span></span><span class="ltx_text" id="A1.T2.1.13.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.13.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.13.3.1">
<span class="ltx_p" id="A1.T2.1.13.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Iso et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib20" title="">2017</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.14.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.14.1.1">
<span class="ltx_p" id="A1.T2.1.14.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.14.1.1.1.1">PsyTAR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.14.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.14.2.1">
<span class="ltx_p" id="A1.T2.1.14.2.1.1" style="width:71.1pt;">bigbio/psytar</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T2.1.14.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.14.3.1">
<span class="ltx_p" id="A1.T2.1.14.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Zolnoori et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib80" title="">2019</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.1.15.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.15.1.1">
<span class="ltx_p" id="A1.T2.1.15.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T2.1.15.1.1.1.1">SciCite</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A1.T2.1.15.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.15.2.1">
<span class="ltx_p" id="A1.T2.1.15.2.1.1" style="width:71.1pt;">bigbio/scicite</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A1.T2.1.15.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.1.15.3.1">
<span class="ltx_p" id="A1.T2.1.15.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Cohan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib9" title="">2019</a>)</cite></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Datasets used for classification tasks.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T3.1">
<tr class="ltx_tr" id="A1.T3.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.1.1.1">
<span class="ltx_p" id="A1.T3.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.1.1.1">Dataset Name</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.1.2.1">
<span class="ltx_p" id="A1.T3.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.2.1.1.1">HuggingFace Card</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.1.3.1">
<span class="ltx_p" id="A1.T3.1.1.3.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.3.1.1.1">Citation</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.2.1.1">
<span class="ltx_p" id="A1.T3.1.2.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.2.1.1.1.1">GeneTag-G</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.2.2.1">
<span class="ltx_p" id="A1.T3.1.2.2.1.1" style="width:71.1pt;">bigbio/genetag</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.2.3.1">
<span class="ltx_p" id="A1.T3.1.2.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Tanabe et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib61" title="">2005</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.3.1.1">
<span class="ltx_p" id="A1.T3.1.3.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.3.1.1.1.1">GeneTag-C</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.3.2.1">
<span class="ltx_p" id="A1.T3.1.3.2.1.1" style="width:71.1pt;">bigbio/genetag</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.3.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.3.3.1">
<span class="ltx_p" id="A1.T3.1.3.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Tanabe et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib61" title="">2005</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.4.1.1">
<span class="ltx_p" id="A1.T3.1.4.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.4.1.1.1.1">GENIA-PPI</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.4.2.1">
<span class="ltx_p" id="A1.T3.1.4.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.4.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.4.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.4.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.4.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.4.2.1.1.2.1.1.1">bigbio/genia</span></span>
<span class="ltx_tr" id="A1.T3.1.4.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.4.2.1.1.2.1.2.1">_relation_corpus</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.4.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.4.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.4.3.1">
<span class="ltx_p" id="A1.T3.1.4.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Pyysalo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib48" title="">2009</a>; Hoehndorf et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib19" title="">2010</a>; Ohta et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib41" title="">2010</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.5.1.1">
<span class="ltx_p" id="A1.T3.1.5.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.5.1.1.1.1">AnEm</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.5.2.1">
<span class="ltx_p" id="A1.T3.1.5.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.5.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.5.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.5.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.5.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.5.2.1.1.2.1.1.1">bigbio/an_em</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.5.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.5.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.5.3.1">
<span class="ltx_p" id="A1.T3.1.5.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Ohta et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib43" title="">2012</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.6.1.1">
<span class="ltx_p" id="A1.T3.1.6.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.6.1.1.1.1">BioInfer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.6.2.1">
<span class="ltx_p" id="A1.T3.1.6.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.6.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.6.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.6.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.6.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.6.2.1.1.2.1.1.1">bigbio/bioinfer</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.6.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.6.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.6.3.1">
<span class="ltx_p" id="A1.T3.1.6.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Pyysalo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib46" title="">2007</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.7.1.1">
<span class="ltx_p" id="A1.T3.1.7.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.7.1.1.1.1">Genia-EE</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.7.2.1">
<span class="ltx_p" id="A1.T3.1.7.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.7.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.7.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.7.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.7.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.7.2.1.1.2.1.1.1">bigbio/bionlp</span></span>
<span class="ltx_tr" id="A1.T3.1.7.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.7.2.1.1.2.1.2.1">_shared_task_2009</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.7.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.7.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.7.3.1">
<span class="ltx_p" id="A1.T3.1.7.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Kim et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib27" title="">2009</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.8.1.1">
<span class="ltx_p" id="A1.T3.1.8.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.8.1.1.1.1">BioNLP11-REL</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.8.2.1">
<span class="ltx_p" id="A1.T3.1.8.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.8.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.8.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.8.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.8.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.8.2.1.1.2.1.1.1">bigbio/bionlp_st</span></span>
<span class="ltx_tr" id="A1.T3.1.8.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.8.2.1.1.2.1.2.1">_2011_rel</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.8.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.8.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.8.3.1">
<span class="ltx_p" id="A1.T3.1.8.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Pyysalo, Ohta, and Tsujii <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib50" title="">2011</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.9.1.1">
<span class="ltx_p" id="A1.T3.1.9.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.9.1.1.1.1">BioNLP-13-CG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.9.2.1">
<span class="ltx_p" id="A1.T3.1.9.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.9.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.9.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.9.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.9.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.9.2.1.1.2.1.1.1">bigbio/bionlp_st</span></span>
<span class="ltx_tr" id="A1.T3.1.9.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.9.2.1.1.2.1.2.1">_2013_cg</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.9.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.9.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.9.3.1">
<span class="ltx_p" id="A1.T3.1.9.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Pyysalo, Ohta, and Ananiadou <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib47" title="">2013</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.10.1.1">
<span class="ltx_p" id="A1.T3.1.10.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.10.1.1.1.1">BioNLP-13-GRO</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.10.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.10.2.1">
<span class="ltx_p" id="A1.T3.1.10.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.10.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.10.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.10.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.10.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.10.2.1.1.2.1.1.1">bigbio/bionlp_st</span></span>
<span class="ltx_tr" id="A1.T3.1.10.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.10.2.1.1.2.1.2.1">_2013_gro</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.10.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.10.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.10.3.1">
<span class="ltx_p" id="A1.T3.1.10.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Kim et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib28" title="">2013</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.11.1.1">
<span class="ltx_p" id="A1.T3.1.11.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.11.1.1.1.1">BioNLP-13-PC</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.11.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.11.2.1">
<span class="ltx_p" id="A1.T3.1.11.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.11.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.11.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.11.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.11.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.11.2.1.1.2.1.1.1">bigbio/bionlp_st</span></span>
<span class="ltx_tr" id="A1.T3.1.11.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.11.2.1.1.2.1.2.1">_2013_pc</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.11.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.11.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.11.3.1">
<span class="ltx_p" id="A1.T3.1.11.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Ohta et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib42" title="">2013</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.12.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.12.1.1">
<span class="ltx_p" id="A1.T3.1.12.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.12.1.1.1.1">PICO</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.12.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.12.2.1">
<span class="ltx_p" id="A1.T3.1.12.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.12.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.12.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.12.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.12.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.12.2.1.1.2.1.1.1">bigbio/ebm_pico</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.12.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T3.1.12.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.12.3.1">
<span class="ltx_p" id="A1.T3.1.12.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Nye et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib40" title="">2018</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.1.13.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.13.1.1">
<span class="ltx_p" id="A1.T3.1.13.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_smallcaps" id="A1.T3.1.13.1.1.1.1">MLEE</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A1.T3.1.13.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.13.2.1">
<span class="ltx_p" id="A1.T3.1.13.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A1.T3.1.13.2.1.1.1"></span><span class="ltx_text" id="A1.T3.1.13.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T3.1.13.2.1.1.2.1">
<span class="ltx_tr" id="A1.T3.1.13.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T3.1.13.2.1.1.2.1.1.1">bigbio/mlee</span></span>
</span></span><span class="ltx_text" id="A1.T3.1.13.2.1.1.3"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A1.T3.1.13.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.13.3.1">
<span class="ltx_p" id="A1.T3.1.13.3.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citep">(Pyysalo et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.12249v1#bib.bib49" title="">2012</a>)</cite></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Datasets used for NER tasks.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 22 09:36:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
