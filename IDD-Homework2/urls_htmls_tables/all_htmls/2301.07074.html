<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.07074] SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations</title><meta property="og:description" content="Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, r‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.07074">

<!--Generated on Fri Mar  1 06:32:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning Partial annotations Segmentation.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>akanher1@jhu.edu</span></span></span> </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Maryland Medical Intelligent Imaging (UM2ii) Center, Baltimore, MD 
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{akanhere,pkulkarni,pyi,vparekh}@som.umaryland.edu</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adway Kanhere 
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pranav Kulkarni
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul H. Yi
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vishwa S. Parekh
</span><span class="ltx_author_notes">22</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation of liver, spleen, pancreas, and kidneys, respectively, significantly (<math id="id1.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">p</mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">&lt;</mo><mn id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><lt id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></lt><ci id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">ùëù</ci><cn type="float" id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">p&lt;0.05</annotation></semantics></math>) better (except spleen) than the dice scores of 0.87, 0.83, 0.42, and 0.48 for the baseline models. In contrast, the central aggregation model significantly (<math id="id2.2.m2.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">p</mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">&lt;</mo><mn id="id2.2.m2.1.1.3" xref="id2.2.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><lt id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"></lt><ci id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">ùëù</ci><cn type="float" id="id2.2.m2.1.1.3.cmml" xref="id2.2.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">p&lt;0.05</annotation></semantics></math>) performed poorly on the test dataset with dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the potential of the SegViz framework to train multi-task models from distributed datasets with partial labels. All our implementations are open-source and available at https://anonymous.4open.science/r/SegViz-B746</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated Learning Partial annotations Segmentation.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Medical image segmentation is one of the most fundamental tasks in automated medical image analysis as it forms the basis for many downstream applications, including diagnosis, prognosis and treatment planning, image reconstruction, and treatment response assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. As a result, many large-scale datasets have been curated and released for the segmentation of different organ types and tumor structures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, each of these datasets has been curated for a specific use case and therefore, focuses on segmenting only a particular organ or tumor subset in the body. Consequently, developing and deploying algorithms for each use case would potentially result in hundreds of models, thereby limiting their clinical utility ‚Äì imagine deploying a different algorithm for every type of cancer, injury, and other diseases. Considering the above limitations, the situation is further amplified by the time-consuming and expensive manual annotations required to build large-scale fully annotated multi-organ datasets.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The challenge of training several individual models separately and the need for large-scale multi-organ datasets can be addressed by training multi-task segmentation models from distributed datasets using collaborative learning. Federated learning (FL) has gained importance in recent years for solving this challenge by collaboratively training one global model from several local models without data sharing. However, the capability of FL in aggregating knowledge from datasets curated at different imaging centers is challenging as each imaging center may focus on related but different tasks; suppose one center is training a liver segmentation model while another center is training a spleen segmentation model from CT scans. These two datasets would contain images with a similar field of view but different, incomplete annotations, as illustrated in <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Such a situation, where one dataset has only a few organs annotated while another dataset contains no overlapping annotations with the first one is very common in medical imaging.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose SegViz, a federated learning (FL) based framework for aggregating knowledge from heterogeneous, distributed medical imaging datasets with distinct and partial annotations into a single ‚Äòglobal‚Äò model. We evaluated the SegViz framework for the task of segmenting four organs - liver, spleen, pancreas, and kidneys on CT scans using distributed nodes each containing one local dataset. We compare the performance of SegViz trained global models to models trained individually on each dataset as well as a model trained by centrally aggregating all the datasets.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2301.07074/assets/Segviz_introfig.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="314" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of an example federated learning setup with nodes containing datasets with a similar field of view but different and incomplete annotations.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. One potential solution is to curate datasets with partial annotations, wherein only a subset of structures is annotated for each image or volume. Furthermore, knowledge from similar partially annotated datasets from multiple groups can be aggregated to collaboratively train global models using Federated Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Knowledge aggregation would not only save time but also allow different groups to benefit from each other‚Äôs annotations without explicitly sharing them. Consequently, different techniques have been proposed in the literature for aggregating knowledge from heterogeneous datasets with partial, incomplete labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">There has been considerable research in the past on developing multi-task segmentation models using partial labels. The works of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> show how to create subsets of the partially labeled datasets to create fully labeled subsets. However, this strategy requires very heavy computational resources. Another approach as described by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is to design a multi-task head with a common encoder and task-specific decoders that are trained separately. Similarly, the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> has shown promise in developing multi-task segmentation models using multi-scale feature abstraction. However, these approachs require all the data to be hosted locally and is not realistic in a medical scenario not only because of privacy and data sharing restrictions but also because it is impossible to anticipate in advance how many distinct activities should the model be trained for.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the authors developed a multi-task multi-domain deep segmentation model for the segmentation of pediatric imaging datasets with excellent performance. However, the proposed technique was developed and evaluated for different anatomical regions in the body with no overlapping field of view or incomplete annotations. Similarly, the cross-domain medical image segmentation technique developed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> was focused on segmentation of the same anatomical structure and the proposed technique was not developed to tackle incomplete annotations</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduced a real FL setup for segmentation using partial labels where client nodes were trained on specific sub-networks for their specific tasks using a shared decoder. However, this method is not scalable and again, needs knowledge of all the tasks to be trained. It was for the first time in the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> that knowledge aggregation was introduced using a single network in a federated manner. The global federated learning framework developed in their work, however, failed to accurately segment different anatomical structures on the external test set. For optimal performance, the authors used an ensemble of multiple local federated learning models, making it computationally expensive and practically challenging.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Therefore, we developed SegViz to address the shortcomings of current techniques in efficiently aggregating knowledge from heterogeneous datasets with partial annotations. Our method does not rely on any heavy model or specific feature engineering methods and utilizes the intrinsic similarities between the different imaging datasets to learn a general representation across multiple tasks. Moreover, it does not require knowledge of the all the tasks in the participating datasets and is able to tackle domain shift between these datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We developed SegViz as a multi-task federated learning framework to learn a diverse set of tasks from distributed nodes with incomplete annotations, as illustrated in Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1 SegViz model architecture ‚Ä£ 3 Methods ‚Ä£ SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The global SegViz model is initialized at the server with two distinct blocks - a representation block and a task block. The goal of the representation block is to learn a generalized representation of the underlying dataset while the goal of the task block is to learn individual tasks distributed across different nodes. Every client is initialized with a subset of the SegViz model, comprising the representation block and a subset of the task block representing the client‚Äôs tasks. During training, the weights of the representation block are always aggregated by the server and redistributed back to the client nodes. On the other hand, the weights of the task block are directly copied from the corresponding client nodes containing the corresponding task, thereby preserving the task-related information for each node in their task block.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>SegViz model architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The backbone of the SegViz model architecture was constructed using a modified version of the multi-head 3D-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> configuration for all our experiments. Each U-Net has 5 layers with down/up-sampling at each layer by a factor of 2. Unlike how U-Net implementations typically operate, these down or up-sampling operations happen at the beginning of each block instead of at the end. The U-Net also contains 2 convolutional residual units at the layers and uses Batch Normalization at each layer. The task block comprised a multi-head architecture with each head consisting of two layers, including the final classification layer. The SegViz model was implemented using the MONAI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> framework and the pre-processing and training were done using Pytorch. The SegViz model architecture has been illustrated in the supplementary material.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">During training, all weights are initialized using LeCun initialization. The batch size was set to 2 and the learning rate was initially set to 1e-4 with the Adam optimizer and CosineAnnealingLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as the scheduler. The Dice Loss was used as the loss function. The average Dice Score was chosen as the final evaluation metric. Each model was trained for a total of 500 epochs.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2301.07074/assets/segviz.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="393" height="441" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the proposed SegViz framework: Client nodes update the global meta-model where knowledge aggregation occurs after every 10 iterations of the local model. The weights of the global model are then shared with the client models allowing both nodes to share knowledge without sharing data.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The SegViz framework was evaluated using four publicly available datasets from the Medical Segmentation Decathalon (MSD) challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The Spleen MSD dataset consists of 61 3D Computed tomographies (CT) volumes with spleen annotations out of which only the 41 training set volumes were used. The Liver MSD dataset consists of 201 3D CT volumes with liver and liver tumor annotations out of which only 131 training set volumes were considered. Similarly, the Pancreas MSD dataset consists of 420 3D CT volumes of which only 282 training volumes were used. Lastly, from the 2019 Kidney Tumor Segmentation Challenge dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we used 210 3D CT volumes from the training dataset. For this study, all tumor annotations were discarded and only organ annotations were used. The training and internal validation splits were considered from the overall training data in an 80:20 split.
We considered all 30 training image volumes from the Beyond the Cranial Vault (BTCV) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as an external test set for all our experiments.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">During pre-processing, all the image volumes were first resized to 256 √ó 256 √ó 128, and the intensity values normalized between 0 and 1. All the volumes were resampled to a constant spacing of (1.5, 1.5, 2.0). We extract random foreground patches of size 128 √ó128 √ó 32 from each volume such that the center voxel of each patch belonged to either the foreground or background class.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Individual baseline implementation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For every task, we trained a single U-Net model based on the Segviz model architecture on the training dataset after the 80:20 split. Hence we had a single model trained on the training dataset for the liver, spleen, pancreas, and kidneys.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Central aggregation implementation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As a lower bound for a multi-task segmentation setup, we combine all four datasets together to create a central repository of all the data. We consider this a lower bound because naive aggregation of the data in the case of partial annotations would lead to suboptimal performance compared to an individual model trained for each dataset separately. We setup our centrally aggregated model using the same steps as our baseline implementation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>SegViz implementation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">FedAvg: </span>We use the popular FedAvg algorithm to construct an FL setup where each client node in the setup represents an isolated group having one of the datasets. The same UNet configuration with the Segviz architecture was used at each client. Apart from the same pre-processing steps as the baseline implementation, we also added random affine transformations such as rotation and scaling. While training the local models, after every 10 epochs, following the FedAvg algorithm, the global model gets all but the last two convolutional layers‚Äô weights and averages them. The updated weights are then shared back to all the local models.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">FedBN: </span> We investigate the popular FedBN algorithm in a similar setup as our FedAvg implementation. Making sure that our global model is generalizable to non-i.i.d data is especially important in medical imaging as data from different centers is obtained using different scanners/protocols. FedBN has shown to be successful compared to other FL algorithms such as FedAvg and FedProx in creating a global model that is generalizable well to non-i.i.d data and it does so by not aggregating the batch norm layers during knowledge transfer.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Local fine-tuning</span>
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, the authors demonstrated the need for fine-tuning in FL models in order to reduce the effect of catastrophic forgetting and stabilize personalized performance. We also finetuned our FedAvg and FedBN models (keeping the representation block frozen) on the local datasets to improve task-specific performance of each task block while keeping the same representation block.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mean Dice score performance of all the experiments on the in-federation validation dataset. The standard deviation values are in parentheses. After a paired t-test, the entries underlined are significant against baseline, in bold against central agg, in italics against our best model (FedBN + FT)</figcaption><img src="/html/2301.07074/assets/int_valid_metrics.png" id="S4.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="[Uncaptioned image]">
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">As shown in Figure <a href="#S5.F3" title="Figure 3 ‚Ä£ 5 Results ‚Ä£ SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the FedBN model with fine-tuning performs the best on the in-federation internal validation set as well the out-of-federation BTCV test set. The SegViz framework using FedBN with fine-tuning segmented the BTCV test set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation of liver, spleen, pancreas, and kidneys, respectively, significantly (<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><lt id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></lt><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">ùëù</ci><cn type="float" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math>) better (except spleen) than the dice scores of 0.87, 0.83, 0.42, and 0.48 for the baseline models. In contrast, the central aggregation model performed significantly (<math id="S5.p1.2.m2.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">p</mi><mo id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><lt id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></lt><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">ùëù</ci><cn type="float" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">p&lt;0.05</annotation></semantics></math>) poorly on the test dataset with dice scores of 0.65, 0, 0.55, and 0.68. We note that the model trained on the centrally aggregated data did not generalize to the spleen label due to the overall model becoming biased toward the liver and pancreas labels, which contain more samples per label. We have included the statistical t-test results between the baseline and the best-performing models in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.3 SegViz implementation ‚Ä£ 4 Experiments ‚Ä£ SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S5.T2" title="Table 2 ‚Ä£ 5 Results ‚Ä£ SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Mean Dice score performance of all the experiments on the out-of-federation BTCV dataset. The standard deviation values are in parentheses. The same conventions as Table 1 are followed</figcaption><img src="/html/2301.07074/assets/btcv_metrics.png" id="S5.T2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="336" alt="[Uncaptioned image]">
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2301.07074/assets/Picture_SegViz.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A comparison of the ground truth segmentation masks with the masks generated by the baseline and SegViz models.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The SegViz framework proposed in this work demonstrated excellent performance in aggregating knowledge from heterogeneous datasets with different, incomplete labels. Our approach successfully aggregated knowledge from all nodes with little to no drop in the performance of the global meta-model in terms of the average dice score. The comparable performance between the SegViz segmentations and multiple baseline model segmentation illustrates a preliminary example of constructing a single global multi-task segmentation model with clinical applicability from dispersed datasets with disjoint partial annotations. It is important to note that the FedAvg global model can be extended to contain a multi-head classifier block while this is not true for the FedBN model.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.2" class="ltx_p">Image segmentation from heterogeneous datasets with incomplete annotations has many potential benefits. For example, SegViz can potentially reduce labeling time by <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="1/\eta" display="inline"><semantics id="S6.p2.1.m1.1a"><mrow id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mn id="S6.p2.1.m1.1.1.2" xref="S6.p2.1.m1.1.1.2.cmml">1</mn><mo id="S6.p2.1.m1.1.1.1" xref="S6.p2.1.m1.1.1.1.cmml">/</mo><mi id="S6.p2.1.m1.1.1.3" xref="S6.p2.1.m1.1.1.3.cmml">Œ∑</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><divide id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1"></divide><cn type="integer" id="S6.p2.1.m1.1.1.2.cmml" xref="S6.p2.1.m1.1.1.2">1</cn><ci id="S6.p2.1.m1.1.1.3.cmml" xref="S6.p2.1.m1.1.1.3">ùúÇ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">1/\eta</annotation></semantics></math> where <math id="S6.p2.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S6.p2.2.m2.1a"><mi id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml">Œ∑</mi><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><ci id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1">ùúÇ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">\eta</annotation></semantics></math> is the number of distinct labels in the distributed data sets by allowing the transfer of knowledge between each client. This would not only save time but also allow different research groups to potentially benefit from each others‚Äô annotations without explicitly sharing them.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">We believe the success of SegViz is attributed to several inherent advantages in its implementations, such as using a learning rate decay and random affine transformations during training which makes it more robust to non-i.i.d data. Moreover, extending our FL implementations with fine-tuning allows for creating stable, high-performing personalized local models. In the future, we would like to extend our experiments using a modality than is less stable than CT such as MRI. We would also like to investigate the real-world performance of our FL setup where client nodes can join and drop contact with the server at any point in time while maintaining no drop in performance.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Antonelli, M., Reinke, A., Bakas, S., Farahani, K., Kopp-Schneider, A.,
Landman, B.A., Litjens, G., Menze, B., Ronneberger, O., Summers, R.M.,
et¬†al.: The medical segmentation decathlon. Nature communications
<span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">13</span>(1), 1‚Äì13 (2022)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Boutillon, A., Conze, P.H., Pons, C., Burdin, V., Borotikar, B.: Generalizable
multi-task, multi-domain deep segmentation of sparse pediatric imaging
datasets via multi-scale contrastive regularization and multi-joint
anatomical priors. Medical Image Analysis <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">81</span>, 102556 (2022)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Cardoso, M.J., Li, W., Brown, R., Ma, N., Kerfoot, E., Wang, Y., Murrey, B.,
Myronenko, A., Zhao, C., Yang, D., et¬†al.: Monai: An open-source framework
for deep learning in healthcare. arXiv preprint arXiv:2211.02701 (2022)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, S., Ma, K., Zheng, Y.: Med3d: Transfer learning for 3d medical image
analysis. arXiv preprint arXiv:1904.00625 (2019)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, X., Sun, S., Bai, N., Han, K., Liu, Q., Yao, S., Tang, H., Zhang, C., Lu,
Z., Huang, Q., et¬†al.: A deep learning-based auto-segmentation system for
organs-at-risk on whole-body computed tomography images for radiation
therapy. Radiotherapy and Oncology <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">160</span>, 175‚Äì184 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chowdhury, A., Kassem, H., Padoy, N., Umeton, R., Karargyris, A.: A review of
medical federated learning: Applications in oncology and cancer research. In:
International MICCAI Brainlesion Workshop. pp. 3‚Äì24. Springer (2022)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
√ái√ßek, √ñ., Abdulkadir, A., Lienkamp, S.S., Brox, T.,
Ronneberger, O.: 3d u-net: learning dense volumetric segmentation from sparse
annotation. In: International conference on medical image computing and
computer-assisted intervention. pp. 424‚Äì432. Springer (2016)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Fang, X., Yan, P.: Multi-organ segmentation over partially labeled datasets
with multi-scale feature abstraction. IEEE Transactions on Medical Imaging
<span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">39</span>(11), 3619‚Äì3629 (2020)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Heller, N., Sathianathen, N., Kalapara, A., Walczak, E., Moore, K., Kaluzniak,
H., Rosenberg, J., Blake, P., Rengel, Z., Oestreich, M., et¬†al.: The kits19
challenge data: 300 kidney tumor cases with clinical context, ct semantic
segmentations, and surgical outcomes. arXiv preprint arXiv:1904.00445 (2019)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Huang, R., Zheng, Y., Hu, Z., Zhang, S., Li, H.: Multi-organ segmentation via
co-training weight-averaged models from few-organ datasets. In: Medical Image
Computing and Computer Assisted Intervention‚ÄìMICCAI 2020: 23rd International
Conference, Lima, Peru, October 4‚Äì8, 2020, Proceedings, Part IV 23. pp.
146‚Äì155. Springer (2020)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Isensee, F., J√§ger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.:
Automated design of deep learning methods for biomedical image segmentation.
arXiv preprint arXiv:1904.08128 (2019)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jiang, Y., Koneƒçn·ª≥, J., Rush, K., Kannan, S.: Improving federated
learning personalization via model agnostic meta learning. arXiv preprint
arXiv:1909.12488 (2019)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Landman, B., Xu, Z., Igelsias, J., Styner, M., Langerak, T., Klein, A.: Miccai
multi-atlas labeling beyond the cranial vault‚Äìworkshop and challenge. In:
Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault‚ÄîWorkshop Challenge.
vol.¬†5, p.¬†12 (2015)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Liang, P.P., Liu, T., Ziyin, L., Allen, N.B., Auerbach, R.P., Brent, D.,
Salakhutdinov, R., Morency, L.P.: Think locally, act globally: Federated
learning with local and global representations. arXiv preprint
arXiv:2001.01523 (2020)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm
restarts. arXiv preprint arXiv:1608.03983 (2016)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Parekh, V.S., Lai, S., Braverman, V., Leal, J., Rowe, S., Pillai, J.J., Jacobs,
M.A.: Cross-domain federated learning in medical imaging. arXiv preprint
arXiv:2112.10001 (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Shen, C., Wang, P., Roth, H.R., Yang, D., Xu, D., Oda, M., Wang, W., Fuh, C.S.,
Chen, P.T., Liu, K.L., et¬†al.: Multi-task federated learning for
heterogeneous pancreas segmentation. In: Clinical Image-Based Procedures,
Distributed and Collaborative Learning, Artificial Intelligence for Combating
COVID-19 and Secure and Privacy-Preserving Machine Learning, pp. 101‚Äì110.
Springer (2021)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Shen, C., Wang, P., Yang, D., Xu, D., Oda, M., Chen, P.T., Liu, K.L., Liao,
W.C., Fuh, C.S., Mori, K., et¬†al.: Joint multi organ and tumor segmentation
from partial labels using federated learning. In: International Workshop on
Distributed, Collaborative, and Federated Learning, Workshop on Affordable
Healthcare and AI for Resource Diverse Global Health. pp. 58‚Äì67. Springer
(2022)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N., Wu, Z., Ding, X.:
Embracing imperfect datasets: A review of deep learning solutions for medical
image segmentation. Medical Image Analysis <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">63</span>, 101693 (2020)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Xu, X., Yan, P.: Federated multi-organ segmentation with partially labeled
data. arXiv preprint arXiv:2206.07156 (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yu, Q., Shi, Y., Sun, J., Gao, Y., Zhu, J., Dai, Y.: Crossbar-net: A novel
convolutional neural network for kidney tumor segmentation in ct images. IEEE
transactions on image processing <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">28</span>(8), 4060‚Äì4074 (2019)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Zhang, J., Xie, Y., Xia, Y., Shen, C.: Dodnet: Learning to segment multi-organ
and tumors from multiple partially labeled datasets. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. pp.
1195‚Äì1204 (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.07073" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.07074" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.07074">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.07074" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.07075" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 06:32:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
