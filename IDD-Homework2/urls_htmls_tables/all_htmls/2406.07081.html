<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning</title>
<!--Generated on Tue Jun 11 08:26:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.07081v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S1" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S2" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>CAP: Context-Aware Prompting</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.SS1" title="In 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dynamic Context Window</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.SS2" title="In 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Example-Specific In-Context Demonstrations Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.SS3" title="In 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.SS1" title="In 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.SS2" title="In 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comparing Different Prompt Selection Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.SS3" title="In 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparing with Traditional NMT Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.SS4" title="In 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Analyzing the Effectiveness on ZPT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.SS5" title="In 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Analyzing the Effectiveness on Literary Translation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S5" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S5.SS1" title="In 5 Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S5.SS2" title="In 5 Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Visualization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S5.SS3" title="In 5 Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S6" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A1" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A2" title="In Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Case Study and Visualization Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A2.SS1" title="In Appendix B Case Study and Visualization Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Case Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A2.SS2" title="In Appendix B Case Study and Visualization Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Visualization</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Menglong Cui, Jiangcun Du<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>, Shaolin Zhu, Deyi Xiong  
<br class="ltx_break"/>College of Intelligence and Computing, Tianjin University, Tianjin, China
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{cuimenglongcs,d2000,zhushaolin,dyxiong}@tju.edu.cn</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Â Equal contributionÂ Corresponding author</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning.
In contrast to sentence-level translation, document-level translation (DOCMT) by LLMs based on in-context learning faces two major challenges: (1) document translations generated by LLMs are often incoherent; (2) the length of demonstration for in-context learning is usually limited.
To address these issues, we propose a <span class="ltx_text ltx_font_bold" id="id2.id1.1">C</span>ontext-<span class="ltx_text ltx_font_bold" id="id2.id1.2">A</span>ware <span class="ltx_text ltx_font_bold" id="id2.id1.3">P</span>rompting method (<span class="ltx_text ltx_font_bold" id="id2.id1.4">CAP</span>), which enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning.
CAP takes into account multi-level attention, selects the most relevant sentences to the current one as context, and then generates a summary from these collected sentences. Subsequently, sentences most similar to the summary are retrieved from the datastore as demonstrations, which effectively guide LLMs in generating cohesive and coherent translations.
We conduct extensive experiments across various DOCMT tasks, and the results demonstrate the effectiveness of our approach, particularly in zero pronoun translation (ZPT) and literary translation tasks.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{CJK}</span>
<p class="ltx_p" id="p1.2">UTF8gbsn</p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<div class="ltx_block ltx_align_bottom" id="p2.1">
<p class="ltx_p" id="p2.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1">Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p2.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.2.1.1.1.1.1.1">
Menglong Cui<span class="ltx_note ltx_role_thanks" id="p2.1.2.1.1.1.1.1.1.1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Â Equal contribution</span></span></span>, Jiangcun Du<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span>, Shaolin Zhu, Deyi Xiong <span class="ltx_note ltx_role_thanks" id="p2.1.2.1.1.1.1.1.1.2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Â Corresponding author</span></span></span></span></span></span>
<span class="ltx_tr" id="p2.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.2.2.1">College of Intelligence and Computing, Tianjin University, Tianjin, China</span></span>
<span class="ltx_tr" id="p2.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.2.1.1.3.3.1.1">{cuimenglongcs,d2000,zhushaolin,dyxiong}@tju.edu.cn</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the increasing scale of parameters and training corpus, large language models (LLMs) have gained remarkable abilities to handle a variety of tasks via in-context learning <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib12" title="">2023a</a>); Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib36" title="">2023b</a>)</cite>, which allows language models to perform tasks with a few given demonstrations and human-written prompts as context.
One particular area where LLMs have shown outstanding potential is machine translation (MT) <cite class="ltx_cite ltx_citemacro_cite">Moslem etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib23" title="">2023</a>); Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib45" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib49" title="">b</a>); Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib6" title="">2024</a>)</cite>.
However, most existing studies <cite class="ltx_cite ltx_citemacro_cite">Zhu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib50" title="">2024</a>)</cite> have focused on sentence-level translation, which makes LLM-based document-level machine translation underexplored <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib35" title="">2023a</a>); Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib14" title="">2024</a>)</cite>.
To bridge this gap, we adapt LLMs to document-level machine translation (DOCMT) in this paper.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As a complex task different from sentence-level translation, one major challenge of DOCMT with LLMs is that <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">the length of demonstrations for in-context learning is limited</span>.
For sentence-level MT, <cite class="ltx_cite ltx_citemacro_citet">Agrawal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib1" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib45" title="">2023a</a>)</cite> show that using 32 or more randomly sampled bilingual parallel sentence pairs as prompts can effectively enhance the translation abilities of LLMs.
However, for DOCMT, the length of the text segments to be translated or to be used as demonstrations inherently increase <cite class="ltx_cite ltx_citemacro_cite">Zeng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib43" title="">2024</a>)</cite>.
Consequently, the limited input window of large language models poses a significant obstacle <cite class="ltx_cite ltx_citemacro_cite">Pawar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib24" title="">2024</a>); Tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib33" title="">2023</a>)</cite>.
Unlike sentence-level translation, the constraints of DOCMT hinder the seamless integration of such demonstrations.
Moreover, the lengthier nature of document-level inputs exacerbates issues related to inference speed.
As the length of the demonstrations increases, the computational resources required for processing also escalate, potentially impeding real-time translation capabilities.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Previous efforts to DOCMT with LLMs, e.g., the method presented in <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib38" title="">2024</a>)</cite>, involve fine-tuning LLMs using two strategies: Parameter-Efficient Fine-Tuning and Full Parameter Fine-Tuning.
Such a framework may not be able to sufficiently overcome the limitation of the length of demonstration for in-context learning in LLMs.
And it also reveals a significant issue: <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">context-independent translations</span>, which can result in translations that lack coherence and disambiguation <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib44" title="">2022a</a>); MacÃ© and Servan (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib19" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address these issues, we propose a novel method called <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">CAP</span>, which stands for <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">C</span>ontext-<span class="ltx_text ltx_font_bold" id="S1.p4.1.3">A</span>ware <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">P</span>rompting.
It consists of three essential steps. Firstly, by combining token- and sentence-level attention scores, we obtain sentences with the highest sentence-level attention scores as context for each sentence. This step facilitates LLMs in generating coherent translations. Secondly, to address the limitation of the length of demonstration for in-context learning in LLMs, we summarize the context and retrieve parallel sentences from the datastore, which are most similar to the generated summary. Finally, we use the retrieved example pairs as demonstrations, prompting LLMs to produce document translations.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In summary, the contributions of this work are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We investigate document-level translation via LLMs. Specially, we explore linguistic phenomena in document-level translation with LLMs, such as ZPT.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a novel method, CAP, to enable LLMs to perform accurate and coherent translation via in-context learning. Our method addresses the issue of translation coherence deficiency by employing a dynamic context window and resolves the limitation concerning the length of demonstration for in-context learning by summarizing the context.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">To demonstrate the effectiveness of our method, we conduct experiments on various DOCMT tasks. Experimental results show that our method significantly outperforms the baselines, especially for the ZPT task.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Document-Level Machine Translation</span>
In recent years, numerous approaches have been proposed for DOCMT <cite class="ltx_cite ltx_citemacro_cite">Maruf etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib20" title="">2022</a>); Lei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib10" title="">2022</a>); Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib46" title="">2022b</a>); Tan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib30" title="">2022</a>); Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib48" title="">2020b</a>)</cite>.
These studies could be further roughly categorized into two groups.
Studies of the first group extend the source-side context from a sentence to the local context of few sentences <cite class="ltx_cite ltx_citemacro_cite">Tan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib32" title="">2021</a>); Lyu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib17" title="">2021</a>); Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib39" title="">2021</a>)</cite>.
Studies of the second group have focused on document-to-document (Doc2Doc) translation.
The initial exploration of Doc2Doc NMT has involved concatenating multiple sentences into a single unit <cite class="ltx_cite ltx_citemacro_cite">Ma etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib18" title="">2020</a>); Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib47" title="">2020a</a>); Tan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib31" title="">2019</a>)</cite>.
Recent studies have successfully trained vanilla Transformer models for Doc2Doc translation by leveraging either large augmented datasets <cite class="ltx_cite ltx_citemacro_cite">Lupo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib16" title="">2022</a>); Sun etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib29" title="">2022</a>)</cite> or pre-trained models <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib42" title="">2021</a>)</cite>.
Additionally, some latest work has achieved the truly Doc2Doc translation by enhancing the attention mechanism in the Transformer model <cite class="ltx_cite ltx_citemacro_cite">Bao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib3" title="">2021</a>); Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib13" title="">2023b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Translation Oriented LLMs</span> LLMs have demonstrated remarkable proficiency
across a wide range of NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Scao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib27" title="">2022</a>); Touvron etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib34" title="">2023</a>); Sun and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib28" title="">2022</a>); Min etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib22" title="">2024</a>); Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib37" title="">2022</a>)</cite>.
Recent research has shown that in-context learning can significantly enhance their performance when following general language instructions.
Specifically, there is a growing body of work exploring the translation capabilities of LLMs <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib15" title="">2023</a>); Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib45" title="">2023a</a>); Hendy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib8" title="">2023</a>)</cite>.
However, it is important to note that these efforts have primarily focused on sentence-level machine translation and have not delved into DOCMT <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib38" title="">2024</a>)</cite>.
Latest noteworthy studies in DOCMT are conducted by <cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib38" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib35" title="">2023a</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib35" title="">2023a</a>)</cite> investigate the document-level translation capabilities of GPT-3.5-TURBO, making it the most closely related work to ours that explores the DOCMT ability of LLMs.
<cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib38" title="">2024</a>)</cite> explore fine-tuning LLMs using two strategies: Parameter-Efficient Fine-Tuning and Full Parameter Fine-Tuning, to enhance the DOCMT ability of LLMs.
Such a framework may not be able to sufficiently overcome the limitation of the length of demonstration for in-context learning in LLMs.
We observe that the existing translation-oriented LLMs tend to struggle with effectively translating sentences in the middle and ending of a document.
Existing methods that explore DOCMT with LLMs are rarely considering the issues of (1) the length limitation of demonstrations for in-context learning and (2) maintaining coherence and resolving ambiguities.
In this work, we mainly concentrate on mitigating the length limitation of in-context learning for DOCMT and addressing the issues of incoherence and ambiguity in DOCMT with LLMs.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="S2.F1.1.g1" src="x1.png" width="563"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram of the proposed CAP. It consists of three essential steps. (1) Dynamic Context Window: selecting context by sentence-level attention scores; (2) Summarize and Retrieve: summarizing the context and retrieving parallel sentences from the datastore, which are most similar to the summary; (3) Inference: with parallel sentences serving as demonstrations, the LLM translates the input text via in-context learning.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CAP: Context-Aware Prompting</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, to address the challenges associated with utilizing LLMs for DOCMT via in-context learning, we propose a <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">C</span>ontext-<span class="ltx_text ltx_font_bold" id="S3.p1.1.2">A</span>ware <span class="ltx_text ltx_font_bold" id="S3.p1.1.3">P</span>rompting method.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">1</span></a>, our approach consists of three steps.
Firstly, we take into consideration multi-level attention, subsequently opting for the sentences with the highest sentence-level attention scores as the context.
Secondly, we input the context into an LLM to obtain its summary, then retrieve <math alttext="K" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_K</annotation></semantics></math> example pairs from the datastore, which are most similar to the summary.
Thirdly, we use the retrieved example pairs as demonstrations in the prompt, guiding the model to generate accurate and coherent translations through a few-shot approach.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dynamic Context Window</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In the context of document-level translation for LLMs, issues of incoherence and disambiguation may arise, leading to context-independent translations. In order to address this problem, we propose a Dynamic Context Window as a solution.
By extracting the most attended context surrounding the current sentence, it facilitates the model in generating accurate and coherent translation outputs.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Initially, we meticulously devise a method for computing sentence-level attention. Specifically, as demonstrated in Eq.â€„(<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.E1" title="In 3.1 Dynamic Context Window â€£ 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">1</span></a>), we extract the attention weights from the final layer of the LLM, followed by averaging across all attention heads to obtain the token-token attention scores.
Next, as illustrated in Eq.â€„(<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.E2" title="In 3.1 Dynamic Context Window â€£ 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">2</span></a>), the maximum value of token-token attention scores between tokens in the current sentence and tokens in all other sentences is computed as the token-sentence attention score.
Ultimately, as shown in Eq.â€„(<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.E3" title="In 3.1 Dynamic Context Window â€£ 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">3</span></a>), the average of attention scores between all tokens in the current sentence and other sentences is calculated to derive the sentence-level attention score.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\boldsymbol{attn}_{i,j}=\frac{1}{H}\sum\limits_{h=1}^{H}\boldsymbol{attn}^{h}_%
{i,j}" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.5" xref="S3.E1.m1.4.5.cmml"><mrow id="S3.E1.m1.4.5.2" xref="S3.E1.m1.4.5.2.cmml"><mi id="S3.E1.m1.4.5.2.2" xref="S3.E1.m1.4.5.2.2.cmml">ğ’‚</mi><mo id="S3.E1.m1.4.5.2.1" xref="S3.E1.m1.4.5.2.1.cmml">â¢</mo><mi id="S3.E1.m1.4.5.2.3" xref="S3.E1.m1.4.5.2.3.cmml">ğ’•</mi><mo id="S3.E1.m1.4.5.2.1a" xref="S3.E1.m1.4.5.2.1.cmml">â¢</mo><mi id="S3.E1.m1.4.5.2.4" xref="S3.E1.m1.4.5.2.4.cmml">ğ’•</mi><mo id="S3.E1.m1.4.5.2.1b" xref="S3.E1.m1.4.5.2.1.cmml">â¢</mo><msub id="S3.E1.m1.4.5.2.5" xref="S3.E1.m1.4.5.2.5.cmml"><mi id="S3.E1.m1.4.5.2.5.2" xref="S3.E1.m1.4.5.2.5.2.cmml">ğ’</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">j</mi></mrow></msub></mrow><mo id="S3.E1.m1.4.5.1" xref="S3.E1.m1.4.5.1.cmml">=</mo><mrow id="S3.E1.m1.4.5.3" xref="S3.E1.m1.4.5.3.cmml"><mfrac id="S3.E1.m1.4.5.3.2" xref="S3.E1.m1.4.5.3.2.cmml"><mn id="S3.E1.m1.4.5.3.2.2" xref="S3.E1.m1.4.5.3.2.2.cmml">1</mn><mi id="S3.E1.m1.4.5.3.2.3" xref="S3.E1.m1.4.5.3.2.3.cmml">H</mi></mfrac><mo id="S3.E1.m1.4.5.3.1" xref="S3.E1.m1.4.5.3.1.cmml">â¢</mo><mrow id="S3.E1.m1.4.5.3.3" xref="S3.E1.m1.4.5.3.3.cmml"><munderover id="S3.E1.m1.4.5.3.3.1" xref="S3.E1.m1.4.5.3.3.1.cmml"><mo id="S3.E1.m1.4.5.3.3.1.2.2" movablelimits="false" xref="S3.E1.m1.4.5.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.4.5.3.3.1.2.3" xref="S3.E1.m1.4.5.3.3.1.2.3.cmml"><mi id="S3.E1.m1.4.5.3.3.1.2.3.2" xref="S3.E1.m1.4.5.3.3.1.2.3.2.cmml">h</mi><mo id="S3.E1.m1.4.5.3.3.1.2.3.1" xref="S3.E1.m1.4.5.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.4.5.3.3.1.2.3.3" xref="S3.E1.m1.4.5.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.4.5.3.3.1.3" xref="S3.E1.m1.4.5.3.3.1.3.cmml">H</mi></munderover><mrow id="S3.E1.m1.4.5.3.3.2" xref="S3.E1.m1.4.5.3.3.2.cmml"><mi id="S3.E1.m1.4.5.3.3.2.2" xref="S3.E1.m1.4.5.3.3.2.2.cmml">ğ’‚</mi><mo id="S3.E1.m1.4.5.3.3.2.1" xref="S3.E1.m1.4.5.3.3.2.1.cmml">â¢</mo><mi id="S3.E1.m1.4.5.3.3.2.3" xref="S3.E1.m1.4.5.3.3.2.3.cmml">ğ’•</mi><mo id="S3.E1.m1.4.5.3.3.2.1a" xref="S3.E1.m1.4.5.3.3.2.1.cmml">â¢</mo><mi id="S3.E1.m1.4.5.3.3.2.4" xref="S3.E1.m1.4.5.3.3.2.4.cmml">ğ’•</mi><mo id="S3.E1.m1.4.5.3.3.2.1b" xref="S3.E1.m1.4.5.3.3.2.1.cmml">â¢</mo><msubsup id="S3.E1.m1.4.5.3.3.2.5" xref="S3.E1.m1.4.5.3.3.2.5.cmml"><mi id="S3.E1.m1.4.5.3.3.2.5.2.2" xref="S3.E1.m1.4.5.3.3.2.5.2.2.cmml">ğ’</mi><mrow id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi><mo id="S3.E1.m1.4.4.2.4.1" xref="S3.E1.m1.4.4.2.3.cmml">,</mo><mi id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">j</mi></mrow><mi id="S3.E1.m1.4.5.3.3.2.5.2.3" xref="S3.E1.m1.4.5.3.3.2.5.2.3.cmml">h</mi></msubsup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.5.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.1.cmml" xref="S3.E1.m1.4.5.1"></eq><apply id="S3.E1.m1.4.5.2.cmml" xref="S3.E1.m1.4.5.2"><times id="S3.E1.m1.4.5.2.1.cmml" xref="S3.E1.m1.4.5.2.1"></times><ci id="S3.E1.m1.4.5.2.2.cmml" xref="S3.E1.m1.4.5.2.2">ğ’‚</ci><ci id="S3.E1.m1.4.5.2.3.cmml" xref="S3.E1.m1.4.5.2.3">ğ’•</ci><ci id="S3.E1.m1.4.5.2.4.cmml" xref="S3.E1.m1.4.5.2.4">ğ’•</ci><apply id="S3.E1.m1.4.5.2.5.cmml" xref="S3.E1.m1.4.5.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.2.5.1.cmml" xref="S3.E1.m1.4.5.2.5">subscript</csymbol><ci id="S3.E1.m1.4.5.2.5.2.cmml" xref="S3.E1.m1.4.5.2.5.2">ğ’</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘—</ci></list></apply></apply><apply id="S3.E1.m1.4.5.3.cmml" xref="S3.E1.m1.4.5.3"><times id="S3.E1.m1.4.5.3.1.cmml" xref="S3.E1.m1.4.5.3.1"></times><apply id="S3.E1.m1.4.5.3.2.cmml" xref="S3.E1.m1.4.5.3.2"><divide id="S3.E1.m1.4.5.3.2.1.cmml" xref="S3.E1.m1.4.5.3.2"></divide><cn id="S3.E1.m1.4.5.3.2.2.cmml" type="integer" xref="S3.E1.m1.4.5.3.2.2">1</cn><ci id="S3.E1.m1.4.5.3.2.3.cmml" xref="S3.E1.m1.4.5.3.2.3">ğ»</ci></apply><apply id="S3.E1.m1.4.5.3.3.cmml" xref="S3.E1.m1.4.5.3.3"><apply id="S3.E1.m1.4.5.3.3.1.cmml" xref="S3.E1.m1.4.5.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.3.3.1.1.cmml" xref="S3.E1.m1.4.5.3.3.1">superscript</csymbol><apply id="S3.E1.m1.4.5.3.3.1.2.cmml" xref="S3.E1.m1.4.5.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.3.3.1.2.1.cmml" xref="S3.E1.m1.4.5.3.3.1">subscript</csymbol><sum id="S3.E1.m1.4.5.3.3.1.2.2.cmml" xref="S3.E1.m1.4.5.3.3.1.2.2"></sum><apply id="S3.E1.m1.4.5.3.3.1.2.3.cmml" xref="S3.E1.m1.4.5.3.3.1.2.3"><eq id="S3.E1.m1.4.5.3.3.1.2.3.1.cmml" xref="S3.E1.m1.4.5.3.3.1.2.3.1"></eq><ci id="S3.E1.m1.4.5.3.3.1.2.3.2.cmml" xref="S3.E1.m1.4.5.3.3.1.2.3.2">â„</ci><cn id="S3.E1.m1.4.5.3.3.1.2.3.3.cmml" type="integer" xref="S3.E1.m1.4.5.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.4.5.3.3.1.3.cmml" xref="S3.E1.m1.4.5.3.3.1.3">ğ»</ci></apply><apply id="S3.E1.m1.4.5.3.3.2.cmml" xref="S3.E1.m1.4.5.3.3.2"><times id="S3.E1.m1.4.5.3.3.2.1.cmml" xref="S3.E1.m1.4.5.3.3.2.1"></times><ci id="S3.E1.m1.4.5.3.3.2.2.cmml" xref="S3.E1.m1.4.5.3.3.2.2">ğ’‚</ci><ci id="S3.E1.m1.4.5.3.3.2.3.cmml" xref="S3.E1.m1.4.5.3.3.2.3">ğ’•</ci><ci id="S3.E1.m1.4.5.3.3.2.4.cmml" xref="S3.E1.m1.4.5.3.3.2.4">ğ’•</ci><apply id="S3.E1.m1.4.5.3.3.2.5.cmml" xref="S3.E1.m1.4.5.3.3.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.3.3.2.5.1.cmml" xref="S3.E1.m1.4.5.3.3.2.5">subscript</csymbol><apply id="S3.E1.m1.4.5.3.3.2.5.2.cmml" xref="S3.E1.m1.4.5.3.3.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.3.3.2.5.2.1.cmml" xref="S3.E1.m1.4.5.3.3.2.5">superscript</csymbol><ci id="S3.E1.m1.4.5.3.3.2.5.2.2.cmml" xref="S3.E1.m1.4.5.3.3.2.5.2.2">ğ’</ci><ci id="S3.E1.m1.4.5.3.3.2.5.2.3.cmml" xref="S3.E1.m1.4.5.3.3.2.5.2.3">â„</ci></apply><list id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.4"><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">ğ‘–</ci><ci id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2">ğ‘—</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\boldsymbol{attn}_{i,j}=\frac{1}{H}\sum\limits_{h=1}^{H}\boldsymbol{attn}^{h}_%
{i,j}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">bold_italic_a bold_italic_t bold_italic_t bold_italic_n start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_H end_ARG âˆ‘ start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT bold_italic_a bold_italic_t bold_italic_t bold_italic_n start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\rm{TS_{i,S}}=max(\boldsymbol{attn}_{i,k}),k\in tokens(S)" class="ltx_Math" display="block" id="S3.E2.m1.7"><semantics id="S3.E2.m1.7a"><mrow id="S3.E2.m1.7.7.2" xref="S3.E2.m1.7.7.3.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><msub id="S3.E2.m1.6.6.1.1.3" xref="S3.E2.m1.6.6.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.3.2" xref="S3.E2.m1.6.6.1.1.3.2.cmml">TS</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" mathvariant="normal" xref="S3.E2.m1.1.1.1.1.cmml">i</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" mathvariant="normal" xref="S3.E2.m1.2.2.2.2.cmml">S</mi></mrow></msub><mo id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml">max</mi><mo id="S3.E2.m1.6.6.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml">ğšğ­ğ­ğ§</mi><mrow id="S3.E2.m1.4.4.2.4" xref="S3.E2.m1.4.4.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1" mathvariant="normal" xref="S3.E2.m1.3.3.1.1.cmml">i</mi><mo id="S3.E2.m1.4.4.2.4.1" xref="S3.E2.m1.4.4.2.3.cmml">,</mo><mi id="S3.E2.m1.4.4.2.2" mathvariant="normal" xref="S3.E2.m1.4.4.2.2.cmml">k</mi></mrow></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.7.7.2.3" xref="S3.E2.m1.7.7.3a.cmml">,</mo><mrow id="S3.E2.m1.7.7.2.2" xref="S3.E2.m1.7.7.2.2.cmml"><mi id="S3.E2.m1.7.7.2.2.2" mathvariant="normal" xref="S3.E2.m1.7.7.2.2.2.cmml">k</mi><mo id="S3.E2.m1.7.7.2.2.1" xref="S3.E2.m1.7.7.2.2.1.cmml">âˆˆ</mo><mrow id="S3.E2.m1.7.7.2.2.3" xref="S3.E2.m1.7.7.2.2.3.cmml"><mi id="S3.E2.m1.7.7.2.2.3.2" xref="S3.E2.m1.7.7.2.2.3.2.cmml">tokens</mi><mo id="S3.E2.m1.7.7.2.2.3.1" xref="S3.E2.m1.7.7.2.2.3.1.cmml">â¢</mo><mrow id="S3.E2.m1.7.7.2.2.3.3.2" xref="S3.E2.m1.7.7.2.2.3.cmml"><mo id="S3.E2.m1.7.7.2.2.3.3.2.1" stretchy="false" xref="S3.E2.m1.7.7.2.2.3.cmml">(</mo><mi id="S3.E2.m1.5.5" mathvariant="normal" xref="S3.E2.m1.5.5.cmml">S</mi><mo id="S3.E2.m1.7.7.2.2.3.3.2.2" stretchy="false" xref="S3.E2.m1.7.7.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.7b"><apply id="S3.E2.m1.7.7.3.cmml" xref="S3.E2.m1.7.7.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.3a.cmml" xref="S3.E2.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1.1"><eq id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></eq><apply id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.3.2">TS</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">i</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">S</ci></list></apply><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.2"></times><ci id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3">max</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2">ğšğ­ğ­ğ§</ci><list id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.4"><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">i</ci><ci id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2">k</ci></list></apply></apply></apply><apply id="S3.E2.m1.7.7.2.2.cmml" xref="S3.E2.m1.7.7.2.2"><in id="S3.E2.m1.7.7.2.2.1.cmml" xref="S3.E2.m1.7.7.2.2.1"></in><ci id="S3.E2.m1.7.7.2.2.2.cmml" xref="S3.E2.m1.7.7.2.2.2">k</ci><apply id="S3.E2.m1.7.7.2.2.3.cmml" xref="S3.E2.m1.7.7.2.2.3"><times id="S3.E2.m1.7.7.2.2.3.1.cmml" xref="S3.E2.m1.7.7.2.2.3.1"></times><ci id="S3.E2.m1.7.7.2.2.3.2.cmml" xref="S3.E2.m1.7.7.2.2.3.2">tokens</ci><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">S</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.7c">\rm{TS_{i,S}}=max(\boldsymbol{attn}_{i,k}),k\in tokens(S)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.7d">roman_TS start_POSTSUBSCRIPT roman_i , roman_S end_POSTSUBSCRIPT = roman_max ( bold_attn start_POSTSUBSCRIPT roman_i , roman_k end_POSTSUBSCRIPT ) , roman_k âˆˆ roman_tokens ( roman_S )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\rm{SS_{I,S}}=avg(TS_{i,S}),i\in tokens(I)" class="ltx_Math" display="block" id="S3.E3.m1.7"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7.2" xref="S3.E3.m1.7.7.3.cmml"><mrow id="S3.E3.m1.6.6.1.1" xref="S3.E3.m1.6.6.1.1.cmml"><msub id="S3.E3.m1.6.6.1.1.3" xref="S3.E3.m1.6.6.1.1.3.cmml"><mi id="S3.E3.m1.6.6.1.1.3.2" xref="S3.E3.m1.6.6.1.1.3.2.cmml">SS</mi><mrow id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1" mathvariant="normal" xref="S3.E3.m1.1.1.1.1.cmml">I</mi><mo id="S3.E3.m1.2.2.2.4.1" xref="S3.E3.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3.m1.2.2.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.2.2.cmml">S</mi></mrow></msub><mo id="S3.E3.m1.6.6.1.1.2" xref="S3.E3.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.6.6.1.1.1" xref="S3.E3.m1.6.6.1.1.1.cmml"><mi id="S3.E3.m1.6.6.1.1.1.3" xref="S3.E3.m1.6.6.1.1.1.3.cmml">avg</mi><mo id="S3.E3.m1.6.6.1.1.1.2" xref="S3.E3.m1.6.6.1.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.6.6.1.1.1.1.1" xref="S3.E3.m1.6.6.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.6.6.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.6.6.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.6.6.1.1.1.1.1.1" xref="S3.E3.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.6.6.1.1.1.1.1.1.2" xref="S3.E3.m1.6.6.1.1.1.1.1.1.2.cmml">TS</mi><mrow id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1" mathvariant="normal" xref="S3.E3.m1.3.3.1.1.cmml">i</mi><mo id="S3.E3.m1.4.4.2.4.1" xref="S3.E3.m1.4.4.2.3.cmml">,</mo><mi id="S3.E3.m1.4.4.2.2" mathvariant="normal" xref="S3.E3.m1.4.4.2.2.cmml">S</mi></mrow></msub><mo id="S3.E3.m1.6.6.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.7.7.2.3" xref="S3.E3.m1.7.7.3a.cmml">,</mo><mrow id="S3.E3.m1.7.7.2.2" xref="S3.E3.m1.7.7.2.2.cmml"><mi id="S3.E3.m1.7.7.2.2.2" mathvariant="normal" xref="S3.E3.m1.7.7.2.2.2.cmml">i</mi><mo id="S3.E3.m1.7.7.2.2.1" xref="S3.E3.m1.7.7.2.2.1.cmml">âˆˆ</mo><mrow id="S3.E3.m1.7.7.2.2.3" xref="S3.E3.m1.7.7.2.2.3.cmml"><mi id="S3.E3.m1.7.7.2.2.3.2" xref="S3.E3.m1.7.7.2.2.3.2.cmml">tokens</mi><mo id="S3.E3.m1.7.7.2.2.3.1" xref="S3.E3.m1.7.7.2.2.3.1.cmml">â¢</mo><mrow id="S3.E3.m1.7.7.2.2.3.3.2" xref="S3.E3.m1.7.7.2.2.3.cmml"><mo id="S3.E3.m1.7.7.2.2.3.3.2.1" stretchy="false" xref="S3.E3.m1.7.7.2.2.3.cmml">(</mo><mi id="S3.E3.m1.5.5" mathvariant="normal" xref="S3.E3.m1.5.5.cmml">I</mi><mo id="S3.E3.m1.7.7.2.2.3.3.2.2" stretchy="false" xref="S3.E3.m1.7.7.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.3.cmml" xref="S3.E3.m1.7.7.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.3a.cmml" xref="S3.E3.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S3.E3.m1.6.6.1.1.cmml" xref="S3.E3.m1.6.6.1.1"><eq id="S3.E3.m1.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.2"></eq><apply id="S3.E3.m1.6.6.1.1.3.cmml" xref="S3.E3.m1.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.3.1.cmml" xref="S3.E3.m1.6.6.1.1.3">subscript</csymbol><ci id="S3.E3.m1.6.6.1.1.3.2.cmml" xref="S3.E3.m1.6.6.1.1.3.2">SS</ci><list id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.4"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">I</ci><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">S</ci></list></apply><apply id="S3.E3.m1.6.6.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1"><times id="S3.E3.m1.6.6.1.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.2"></times><ci id="S3.E3.m1.6.6.1.1.1.3.cmml" xref="S3.E3.m1.6.6.1.1.1.3">avg</ci><apply id="S3.E3.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.2">TS</ci><list id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.4"><ci id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1">i</ci><ci id="S3.E3.m1.4.4.2.2.cmml" xref="S3.E3.m1.4.4.2.2">S</ci></list></apply></apply></apply><apply id="S3.E3.m1.7.7.2.2.cmml" xref="S3.E3.m1.7.7.2.2"><in id="S3.E3.m1.7.7.2.2.1.cmml" xref="S3.E3.m1.7.7.2.2.1"></in><ci id="S3.E3.m1.7.7.2.2.2.cmml" xref="S3.E3.m1.7.7.2.2.2">i</ci><apply id="S3.E3.m1.7.7.2.2.3.cmml" xref="S3.E3.m1.7.7.2.2.3"><times id="S3.E3.m1.7.7.2.2.3.1.cmml" xref="S3.E3.m1.7.7.2.2.3.1"></times><ci id="S3.E3.m1.7.7.2.2.3.2.cmml" xref="S3.E3.m1.7.7.2.2.3.2">tokens</ci><ci id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5">I</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">\rm{SS_{I,S}}=avg(TS_{i,S}),i\in tokens(I)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.7d">roman_SS start_POSTSUBSCRIPT roman_I , roman_S end_POSTSUBSCRIPT = roman_avg ( roman_TS start_POSTSUBSCRIPT roman_i , roman_S end_POSTSUBSCRIPT ) , roman_i âˆˆ roman_tokens ( roman_I )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.11">where <math alttext="\rm{TS}_{i,S}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.2"><semantics id="S3.SS1.p4.1.m1.2a"><msub id="S3.SS1.p4.1.m1.2.3" xref="S3.SS1.p4.1.m1.2.3.cmml"><mi id="S3.SS1.p4.1.m1.2.3.2" xref="S3.SS1.p4.1.m1.2.3.2.cmml">TS</mi><mrow id="S3.SS1.p4.1.m1.2.2.2.4" xref="S3.SS1.p4.1.m1.2.2.2.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1" mathvariant="normal" xref="S3.SS1.p4.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p4.1.m1.2.2.2.4.1" xref="S3.SS1.p4.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p4.1.m1.2.2.2.2" mathvariant="normal" xref="S3.SS1.p4.1.m1.2.2.2.2.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><apply id="S3.SS1.p4.1.m1.2.3.cmml" xref="S3.SS1.p4.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.3">subscript</csymbol><ci id="S3.SS1.p4.1.m1.2.3.2.cmml" xref="S3.SS1.p4.1.m1.2.3.2">TS</ci><list id="S3.SS1.p4.1.m1.2.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.2.4"><ci id="S3.SS1.p4.1.m1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1">i</ci><ci id="S3.SS1.p4.1.m1.2.2.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2">S</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">\rm{TS}_{i,S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.2d">roman_TS start_POSTSUBSCRIPT roman_i , roman_S end_POSTSUBSCRIPT</annotation></semantics></math> is the attention score of <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.11.1">T</span>oken <math alttext="\rm{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">i</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\rm{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">roman_i</annotation></semantics></math> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.11.2">S</span>entence <math alttext="\rm{S}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p4.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">S</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\rm{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">roman_S</annotation></semantics></math>, <math alttext="\rm{SS}_{I,S}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4.2"><semantics id="S3.SS1.p4.4.m4.2a"><msub id="S3.SS1.p4.4.m4.2.3" xref="S3.SS1.p4.4.m4.2.3.cmml"><mi id="S3.SS1.p4.4.m4.2.3.2" xref="S3.SS1.p4.4.m4.2.3.2.cmml">SS</mi><mrow id="S3.SS1.p4.4.m4.2.2.2.4" xref="S3.SS1.p4.4.m4.2.2.2.3.cmml"><mi id="S3.SS1.p4.4.m4.1.1.1.1" mathvariant="normal" xref="S3.SS1.p4.4.m4.1.1.1.1.cmml">I</mi><mo id="S3.SS1.p4.4.m4.2.2.2.4.1" xref="S3.SS1.p4.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p4.4.m4.2.2.2.2" mathvariant="normal" xref="S3.SS1.p4.4.m4.2.2.2.2.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.2b"><apply id="S3.SS1.p4.4.m4.2.3.cmml" xref="S3.SS1.p4.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.2.3.1.cmml" xref="S3.SS1.p4.4.m4.2.3">subscript</csymbol><ci id="S3.SS1.p4.4.m4.2.3.2.cmml" xref="S3.SS1.p4.4.m4.2.3.2">SS</ci><list id="S3.SS1.p4.4.m4.2.2.2.3.cmml" xref="S3.SS1.p4.4.m4.2.2.2.4"><ci id="S3.SS1.p4.4.m4.1.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1.1.1">I</ci><ci id="S3.SS1.p4.4.m4.2.2.2.2.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2">S</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.2c">\rm{SS}_{I,S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m4.2d">roman_SS start_POSTSUBSCRIPT roman_I , roman_S end_POSTSUBSCRIPT</annotation></semantics></math> is the attention score between <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.11.3">S</span>entence <math alttext="\rm{I}" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5.1"><semantics id="S3.SS1.p4.5.m5.1a"><mi id="S3.SS1.p4.5.m5.1.1" mathvariant="normal" xref="S3.SS1.p4.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><ci id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">I</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\rm{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.5.m5.1d">roman_I</annotation></semantics></math> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.11.4">S</span>entence <math alttext="\rm{S}" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m6.1"><semantics id="S3.SS1.p4.6.m6.1a"><mi id="S3.SS1.p4.6.m6.1.1" mathvariant="normal" xref="S3.SS1.p4.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><ci id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">S</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">\rm{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.6.m6.1d">roman_S</annotation></semantics></math>.
<math alttext="\boldsymbol{attn}_{i,j}^{h}" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m7.2"><semantics id="S3.SS1.p4.7.m7.2a"><mrow id="S3.SS1.p4.7.m7.2.3" xref="S3.SS1.p4.7.m7.2.3.cmml"><mi id="S3.SS1.p4.7.m7.2.3.2" xref="S3.SS1.p4.7.m7.2.3.2.cmml">ğ’‚</mi><mo id="S3.SS1.p4.7.m7.2.3.1" xref="S3.SS1.p4.7.m7.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p4.7.m7.2.3.3" xref="S3.SS1.p4.7.m7.2.3.3.cmml">ğ’•</mi><mo id="S3.SS1.p4.7.m7.2.3.1a" xref="S3.SS1.p4.7.m7.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p4.7.m7.2.3.4" xref="S3.SS1.p4.7.m7.2.3.4.cmml">ğ’•</mi><mo id="S3.SS1.p4.7.m7.2.3.1b" xref="S3.SS1.p4.7.m7.2.3.1.cmml">â¢</mo><msubsup id="S3.SS1.p4.7.m7.2.3.5" xref="S3.SS1.p4.7.m7.2.3.5.cmml"><mi id="S3.SS1.p4.7.m7.2.3.5.2.2" xref="S3.SS1.p4.7.m7.2.3.5.2.2.cmml">ğ’</mi><mrow id="S3.SS1.p4.7.m7.2.2.2.4" xref="S3.SS1.p4.7.m7.2.2.2.3.cmml"><mi id="S3.SS1.p4.7.m7.1.1.1.1" xref="S3.SS1.p4.7.m7.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p4.7.m7.2.2.2.4.1" xref="S3.SS1.p4.7.m7.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p4.7.m7.2.2.2.2" xref="S3.SS1.p4.7.m7.2.2.2.2.cmml">j</mi></mrow><mi id="S3.SS1.p4.7.m7.2.3.5.3" xref="S3.SS1.p4.7.m7.2.3.5.3.cmml">h</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.2b"><apply id="S3.SS1.p4.7.m7.2.3.cmml" xref="S3.SS1.p4.7.m7.2.3"><times id="S3.SS1.p4.7.m7.2.3.1.cmml" xref="S3.SS1.p4.7.m7.2.3.1"></times><ci id="S3.SS1.p4.7.m7.2.3.2.cmml" xref="S3.SS1.p4.7.m7.2.3.2">ğ’‚</ci><ci id="S3.SS1.p4.7.m7.2.3.3.cmml" xref="S3.SS1.p4.7.m7.2.3.3">ğ’•</ci><ci id="S3.SS1.p4.7.m7.2.3.4.cmml" xref="S3.SS1.p4.7.m7.2.3.4">ğ’•</ci><apply id="S3.SS1.p4.7.m7.2.3.5.cmml" xref="S3.SS1.p4.7.m7.2.3.5"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.2.3.5.1.cmml" xref="S3.SS1.p4.7.m7.2.3.5">superscript</csymbol><apply id="S3.SS1.p4.7.m7.2.3.5.2.cmml" xref="S3.SS1.p4.7.m7.2.3.5"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.2.3.5.2.1.cmml" xref="S3.SS1.p4.7.m7.2.3.5">subscript</csymbol><ci id="S3.SS1.p4.7.m7.2.3.5.2.2.cmml" xref="S3.SS1.p4.7.m7.2.3.5.2.2">ğ’</ci><list id="S3.SS1.p4.7.m7.2.2.2.3.cmml" xref="S3.SS1.p4.7.m7.2.2.2.4"><ci id="S3.SS1.p4.7.m7.1.1.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1.1.1">ğ‘–</ci><ci id="S3.SS1.p4.7.m7.2.2.2.2.cmml" xref="S3.SS1.p4.7.m7.2.2.2.2">ğ‘—</ci></list></apply><ci id="S3.SS1.p4.7.m7.2.3.5.3.cmml" xref="S3.SS1.p4.7.m7.2.3.5.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.2c">\boldsymbol{attn}_{i,j}^{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.7.m7.2d">bold_italic_a bold_italic_t bold_italic_t bold_italic_n start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the attention score of the <math alttext="h" class="ltx_Math" display="inline" id="S3.SS1.p4.8.m8.1"><semantics id="S3.SS1.p4.8.m8.1a"><mi id="S3.SS1.p4.8.m8.1.1" xref="S3.SS1.p4.8.m8.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m8.1b"><ci id="S3.SS1.p4.8.m8.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.8.m8.1d">italic_h</annotation></semantics></math>-th head in the final transformer block, calculated between the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p4.9.m9.1"><semantics id="S3.SS1.p4.9.m9.1a"><mi id="S3.SS1.p4.9.m9.1.1" xref="S3.SS1.p4.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m9.1b"><ci id="S3.SS1.p4.9.m9.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m9.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.9.m9.1d">italic_i</annotation></semantics></math>-th token and the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS1.p4.10.m10.1"><semantics id="S3.SS1.p4.10.m10.1a"><mi id="S3.SS1.p4.10.m10.1.1" xref="S3.SS1.p4.10.m10.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m10.1b"><ci id="S3.SS1.p4.10.m10.1.1.cmml" xref="S3.SS1.p4.10.m10.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m10.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.10.m10.1d">italic_j</annotation></semantics></math>-th token in the current document.
<math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p4.11.m11.1"><semantics id="S3.SS1.p4.11.m11.1a"><mi id="S3.SS1.p4.11.m11.1.1" xref="S3.SS1.p4.11.m11.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.11.m11.1b"><ci id="S3.SS1.p4.11.m11.1.1.cmml" xref="S3.SS1.p4.11.m11.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.11.m11.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.11.m11.1d">italic_H</annotation></semantics></math> is the number of attention heads in each transformer block.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">After obtaining the sentence-level attention scores, we extract the top <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">italic_N</annotation></semantics></math> sentences with the highest sentence-level attention scores as dynamic context, while maintaining their original sequential positions within the document.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Example-Specific In-Context Demonstrations Selection</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Unlike sentence-level translation, the challenge in utilizing LLMs for DOCMT arises from the constrained context length, as the length of the text segments to be translated or used as demonstrations inherently increases.
To effectively address these challenges, it is necessary to further process the context and extract as much relevant information as possible from the preceding sentences. We then input this extracted context into an LLM to generate a concise summary that encapsulates key topics and information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.7">Subsequently, as shown in Eq.â€„(<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.E4" title="In 3.2 Example-Specific In-Context Demonstrations Selection â€£ 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">4</span></a>), we utilize sentence-transformers <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib26" title="">2020</a>)</cite> to retrieve carefully designed example pairs from a specified datastore. These examples are selected based on their cosine similarity with the embedding of the summary sentences, ensuring that the retrieved examples exhibit semantic similarity to the context and content of the document.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\rm{score}}(S,X)=cos(\boldsymbol{S},\boldsymbol{X})" class="ltx_Math" display="block" id="S3.E4.m1.4"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.5" xref="S3.E4.m1.4.5.cmml"><mrow id="S3.E4.m1.4.5.2" xref="S3.E4.m1.4.5.2.cmml"><mi id="S3.E4.m1.4.5.2.2" xref="S3.E4.m1.4.5.2.2.cmml">score</mi><mo id="S3.E4.m1.4.5.2.1" xref="S3.E4.m1.4.5.2.1.cmml">â¢</mo><mrow id="S3.E4.m1.4.5.2.3.2" xref="S3.E4.m1.4.5.2.3.1.cmml"><mo id="S3.E4.m1.4.5.2.3.2.1" stretchy="false" xref="S3.E4.m1.4.5.2.3.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">S</mi><mo id="S3.E4.m1.4.5.2.3.2.2" xref="S3.E4.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">X</mi><mo id="S3.E4.m1.4.5.2.3.2.3" stretchy="false" xref="S3.E4.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.4.5.1" xref="S3.E4.m1.4.5.1.cmml">=</mo><mrow id="S3.E4.m1.4.5.3" xref="S3.E4.m1.4.5.3.cmml"><mi id="S3.E4.m1.4.5.3.2" xref="S3.E4.m1.4.5.3.2.cmml">c</mi><mo id="S3.E4.m1.4.5.3.1" xref="S3.E4.m1.4.5.3.1.cmml">â¢</mo><mi id="S3.E4.m1.4.5.3.3" xref="S3.E4.m1.4.5.3.3.cmml">o</mi><mo id="S3.E4.m1.4.5.3.1a" xref="S3.E4.m1.4.5.3.1.cmml">â¢</mo><mi id="S3.E4.m1.4.5.3.4" xref="S3.E4.m1.4.5.3.4.cmml">s</mi><mo id="S3.E4.m1.4.5.3.1b" xref="S3.E4.m1.4.5.3.1.cmml">â¢</mo><mrow id="S3.E4.m1.4.5.3.5.2" xref="S3.E4.m1.4.5.3.5.1.cmml"><mo id="S3.E4.m1.4.5.3.5.2.1" stretchy="false" xref="S3.E4.m1.4.5.3.5.1.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">ğ‘º</mi><mo id="S3.E4.m1.4.5.3.5.2.2" xref="S3.E4.m1.4.5.3.5.1.cmml">,</mo><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">ğ‘¿</mi><mo id="S3.E4.m1.4.5.3.5.2.3" stretchy="false" xref="S3.E4.m1.4.5.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.5.cmml" xref="S3.E4.m1.4.5"><eq id="S3.E4.m1.4.5.1.cmml" xref="S3.E4.m1.4.5.1"></eq><apply id="S3.E4.m1.4.5.2.cmml" xref="S3.E4.m1.4.5.2"><times id="S3.E4.m1.4.5.2.1.cmml" xref="S3.E4.m1.4.5.2.1"></times><ci id="S3.E4.m1.4.5.2.2.cmml" xref="S3.E4.m1.4.5.2.2">score</ci><interval closure="open" id="S3.E4.m1.4.5.2.3.1.cmml" xref="S3.E4.m1.4.5.2.3.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ‘†</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">ğ‘‹</ci></interval></apply><apply id="S3.E4.m1.4.5.3.cmml" xref="S3.E4.m1.4.5.3"><times id="S3.E4.m1.4.5.3.1.cmml" xref="S3.E4.m1.4.5.3.1"></times><ci id="S3.E4.m1.4.5.3.2.cmml" xref="S3.E4.m1.4.5.3.2">ğ‘</ci><ci id="S3.E4.m1.4.5.3.3.cmml" xref="S3.E4.m1.4.5.3.3">ğ‘œ</ci><ci id="S3.E4.m1.4.5.3.4.cmml" xref="S3.E4.m1.4.5.3.4">ğ‘ </ci><interval closure="open" id="S3.E4.m1.4.5.3.5.1.cmml" xref="S3.E4.m1.4.5.3.5.2"><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">ğ‘º</ci><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">ğ‘¿</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">{\rm{score}}(S,X)=cos(\boldsymbol{S},\boldsymbol{X})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.4d">roman_score ( italic_S , italic_X ) = italic_c italic_o italic_s ( bold_italic_S , bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.6">where <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_S</annotation></semantics></math>, and <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_X</annotation></semantics></math> are the summary we obtained at the preceding step and the example sentence in datastore respectively. <math alttext="\boldsymbol{S}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">ğ‘º</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ‘º</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\boldsymbol{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">bold_italic_S</annotation></semantics></math> and <math alttext="\boldsymbol{X}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">ğ‘¿</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ğ‘¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\boldsymbol{X}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">bold_italic_X</annotation></semantics></math> are the embeddings of <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_S</annotation></semantics></math> and <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_X</annotation></semantics></math> respectively.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">By employing these selected examples as few-shot demonstrations, we provide the LLM with a more precise understanding of the document. This approach not only enhances the modelâ€™s accuracy in utilizing in-context learning for translation but also contributes to enhancing the robustness of document translation tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Inference</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We employ a prompt template, which is identical to that utilized in <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib7" title="">2023</a>)</cite>. In this approach, we integrate the retrieved example pairs from the preceding step seamlessly into the prompt template, which will then be input into the target LLM. This strategic integration serves to guide the LLMs in generating translations that exhibit enhanced cohesion and coherence.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Following the translation process, we employ a diverse set of evaluation metrics to thoroughly assess the quality of the translations generated by the LLM. This multifaceted evaluation allows us to gain comprehensive insights into the effectiveness and performance of the language models in producing accurate and appropriate translations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we conducted extensive experiments on various DOCMT tasks to examine the effectiveness of the proposed method.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Datasets:</span> We used OPUS-100<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://opus.nlpl.eu/opus-100.php</span></span></span> and news-commentary-v15 as the datastore to retrieve the in-context demonstrations on four language pairs.
To assess the effectiveness of our proposed method on LLMs, we employed WMT22 newstest as the test set.
Following <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib1" title="">2023</a>)</cite>, we normalized punctuation using Moses<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://www2.statmt.org/moses/</span></span></span> and removed sentence pairs with a source/target length ratio exceeding 1.5.
To assess the accuracy of our method in ZPT, we utilized the GuoFeng <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib40" title="">2022</a>)</cite> dataset, which covers five domains: movie subtitle, Q&amp;A forum, government news, web fiction, and personal profile.
To validate the capability of our approach in guiding the model to generate more cohesive and more coherent translations, we employed the WMT23 Literary Translation dataset,<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www2.statmt.org/wmt23/literary-translation-task.html</span></span></span> which comprises two test sets: an in-domain test set and an out-of-domain test set.
Additionally, we used its training set to retrieve in-context demonstrations.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1.1">methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T1.1.1.1.2">Qwen-1.8B-Chat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T1.1.1.1.3">Qwen-7B-Chat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T1.1.1.1.4">Qwen-14B-Chat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T1.1.1.1.5">Qwen-72B-Chat</th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.1">de-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.2">zh-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.3">de-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.4">zh-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.5">de-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.6">zh-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.7">de-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.8">zh-en</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.3.1.1">Zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.1.2.1">19.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.1.3.1">17.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.4">34.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.5">24.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.6">35.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.7">26.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.8">37.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.9">29.11</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.2.1">Random</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.2">17.58</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.3">15.12</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.4">32.91</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.5">23.58</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.6">35.28</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.7">26.42</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.8">37.55</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.9">28.40</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.5.3.1">BM25</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.2">17.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.3">14.92</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.4">32.90</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.5">23.54</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.6">34.86</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.7">25.90</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.8">37.49</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.9">28.59</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.6.4.1">Similar</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.2">17.81</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.3">15.34</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.4">33.43</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.5">23.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.6">35.29</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.7">26.42</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.8">37.72</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.9">28.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.7.5.1">Precedent</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.2">15.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.3">15.23</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.4">34.49</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.5">23.65</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.6">35.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.7">26.49</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.8">38.01</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5.9">29.15</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.1.8.6.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.2">18.56</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.3">15.51</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.6.4.1">34.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.6.5.1">24.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.6.6.1">36.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.6.7.1">26.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.8"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.6.8.1">38.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.6.9"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.6.9.1">29.32</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Document-level BLEU scores of variants of the Qwen chat version LLM, employing different prompt strategies, on the WMT22 newstest dataset.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Models:</span> To compare the translation performance of LLMs using our method, we compared against the following three encoder-decoder based strong NMT models as baselines.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">MarianMT: A framework for translation models, using the same models as BART <cite class="ltx_cite ltx_citemacro_cite">Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib11" title="">2020</a>)</cite>.
In our experiment, each language pair is associated with a distinct pre-trained model.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">M2M100 <cite class="ltx_cite ltx_citemacro_cite">Fan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib5" title="">2021</a>)</cite>: It is a multilingual encoder-decoder model, which is pre-trained on 100 languages and is capable of translating 9,900 directions.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">NLLB <cite class="ltx_cite ltx_citemacro_cite">Costa-jussÃ  etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib4" title="">2022</a>)</cite>: It is a neural multilingual machine translation model, which supports translation between any pair of 200 languages. There are numerous variants of it, and the one utilized in our experiments is NLLB-3.3B.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">To evaluate the translation abilities of LLMs, we utilized LLMs that are tuned with SFT, as they can follow instructions to generate translations.
Hence, we chose the following two LLMs, which support both Chinese and English and have undergone instruction tuning:</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Baichuan <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib41" title="">2023</a>)</cite>: It is trained on a high-quality corpus with 2.6 trillion tokens and has achieved the best performance in authoritative Chinese and English benchmarks of the same size.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Qwen <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib2" title="">2023</a>)</cite>: It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Prompt Selection Strategies:</span> To assess the effectiveness of the proposed method, we compared our method against five prompt selection strategies:</p>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Zero-shot: Zero-shot is a powerful baseline. Previous studies <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib25" title="">2019</a>)</cite> show that LLMs have excellent zero-shot abilities in many NLP tasks, including reading comprehension, translation, and summarization.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">Random: This strategy randomly selects <math alttext="K" class="ltx_Math" display="inline" id="S4.I3.i2.p1.1.m1.1"><semantics id="S4.I3.i2.p1.1.m1.1a"><mi id="S4.I3.i2.p1.1.m1.1.1" xref="S4.I3.i2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><ci id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.1.m1.1d">italic_K</annotation></semantics></math> sentence pairs from the datastore as demonstrations.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1">BM25:
It calculates the similarity between translated sentences and sentences in the corpus with BM25, and then selects top <math alttext="K" class="ltx_Math" display="inline" id="S4.I3.i3.p1.1.m1.1"><semantics id="S4.I3.i3.p1.1.m1.1a"><mi id="S4.I3.i3.p1.1.m1.1.1" xref="S4.I3.i3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i3.p1.1.m1.1b"><ci id="S4.I3.i3.p1.1.m1.1.1.cmml" xref="S4.I3.i3.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i3.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i3.p1.1.m1.1d">italic_K</annotation></semantics></math> sentences with the highest similarity as demonstrations.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i4.p1">
<p class="ltx_p" id="S4.I3.i4.p1.1">Similar: This method uses sentence embedding similarity-based retrieval to select <math alttext="K" class="ltx_Math" display="inline" id="S4.I3.i4.p1.1.m1.1"><semantics id="S4.I3.i4.p1.1.m1.1a"><mi id="S4.I3.i4.p1.1.m1.1.1" xref="S4.I3.i4.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i4.p1.1.m1.1b"><ci id="S4.I3.i4.p1.1.m1.1.1.cmml" xref="S4.I3.i4.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i4.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i4.p1.1.m1.1d">italic_K</annotation></semantics></math> demonstrations <cite class="ltx_cite ltx_citemacro_cite">Moslem etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib23" title="">2023</a>)</cite>. In our work, we used sentence-transformers <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib26" title="">2020</a>)</cite> to implement this method.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i5.p1">
<p class="ltx_p" id="S4.I3.i5.p1.1">Precedent: This approach guides large language models in generating translations by utilizing the preceding <math alttext="K" class="ltx_Math" display="inline" id="S4.I3.i5.p1.1.m1.1"><semantics id="S4.I3.i5.p1.1.m1.1a"><mi id="S4.I3.i5.p1.1.m1.1.1" xref="S4.I3.i5.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i5.p1.1.m1.1b"><ci id="S4.I3.i5.p1.1.m1.1.1.cmml" xref="S4.I3.i5.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i5.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i5.p1.1.m1.1d">italic_K</annotation></semantics></math> sentences of the current sentence within a document, along with those translations generated in previous steps as demonstrations.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">For prompt selection experiments, we employed models from four distinct sizes of the Qwen LLMs. Meanwhile, for a trade-off between performance and cost, we set <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p6.1.m1.1"><semantics id="S4.SS1.p6.1.m1.1a"><mi id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><ci id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.1.m1.1d">italic_K</annotation></semantics></math> to 3.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p7.1.1">Metrics:</span> We employed a variety of metrics to assess the quality of document-level translation generated by LLMs, including document-level BLEU(<math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.p7.1.m1.1"><semantics id="S4.SS1.p7.1.m1.1a"><mi id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.1b"><ci id="S4.SS1.p7.1.m1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.1.m1.1d">italic_d</annotation></semantics></math>-BLEU), and ChrF2 using SacreBLEU for document-level evaluation.
For the ZPT task, we employed accuracy to evaluate the precision of zero pronoun translation.
Additionally, we have also evaluated document-level translation quality with Blonde <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib9" title="">2022</a>)</cite>, which is an automatic evaluation metric for document-level machine translation.
More experimental results can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A1" title="Appendix A Additional Experimental Results â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">All experiments were performed on 4 NVIDIA A100 GPUs.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.4" style="width:420.4pt;height:206.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.1pt,5.4pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.4.4.5.1.1" rowspan="2"><span class="ltx_text" id="S4.T2.4.4.5.1.1.1">Methods</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.4.4.5.1.2">de-en</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.4.4.5.1.3">zh-en</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.4.4.5.1.4">en-de</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.4.4.5.1.5">en-zh</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1">
<math alttext="d" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">italic_d</annotation></semantics></math>-BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.5">chrF2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2">
<math alttext="d" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mi id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">italic_d</annotation></semantics></math>-BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.6">chrF2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3">
<math alttext="d" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.3.m1.1a"><mi id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.m1.1d">italic_d</annotation></semantics></math>-BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.7">chrF2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.4">
<math alttext="d" class="ltx_Math" display="inline" id="S4.T2.4.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.4.m1.1a"><mi id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.4.m1.1d">italic_d</annotation></semantics></math>-BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.8">chrF2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.6.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.6.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S4.T2.4.4.6.2.2">Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.4.4.7.3.1">MarianMT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.2">36.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.3">68.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.4">22.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.5">58.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.6">30.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.7">58.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.8">27.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.9">24.89</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.4.4.8.4.1">M2M100-1.2B</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.2">35.11</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.3">66.86</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.4">17.32</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.5">49.15</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.6">31.13</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.7">67.51</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.8">28.45</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.9">26.13</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.4.4.9.5.1">NLLB-3.3B</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.2">36.35</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.3">67.74</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.4">25.48</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.5">60.03</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.6"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.9.5.6.1">34.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.7"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.9.5.7.1">69.23</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.8">24.03</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.9">23.02</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.10.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.10.6.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S4.T2.4.4.10.6.2">LLMs</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.4.4.11.7.1">Baichuan-7B-Chat</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.2">33.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.3">68.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.4">25.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.5">60.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.6">20.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.7">63.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.8">33.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.9">33.57</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.4.4.12.8.1">Baichuan-13B-Chat</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.2">33.62</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.3">68.83</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.4">26.02</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.5">64.91</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.6">22.32</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.7">64.83</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.8">36.30</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.9">33.94</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.4.4.13.9.1">Qwen-7B-Chat</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.2">34.86</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.3">68.84</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.4">24.34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.5">62.57</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.6">24.01</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.7">64.72</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.8">36.59</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.13.9.9">33.67</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.4.4.14.10.1">Qwen-14B-Chat</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.2">36.57</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.3">70.00</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.4">26.77</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.5">64.24</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.6">29.36</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.7">67.88</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.8">42.28</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.9">37.85</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.15.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T2.4.4.15.11.1">Qwen-72B-Chat</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.2.1">38.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.3.1">70.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.4.1">29.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.5.1">65.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.6">31.48</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.7">68.11</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.8"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.8.1">46.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.4.4.15.11.9"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.9.1">40.90</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Document-level BLEU and chrF2 scores of models with different architectures on four language pairs from the WMT22 newstest dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1">methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.2">1.8B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.3">7B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.4">14B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.5">72B</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.2.1.1">Zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">29.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">36.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4">38.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5">36.53</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.3.2.1">Random</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">27.81</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">38.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4">37.55</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5">38.13</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.3.1">BM25</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2">28.13</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3">37.45</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.4">37.94</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.5">37.95</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.4.1">Similar</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.2">29.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.3">37.49</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.4">37.93</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.5">37.39</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.5.1">Precedent</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.2">29.91</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.3">42.02</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.4">40.45</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.5">39.07</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.1.7.6.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.6.2.1">32.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.6.3.1">42.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.6.4.1">41.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.6.5.1">41.65</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>ZPT accuracy of various variants of the Qwen chat version model on the GuoFeng dataset. 1.8B, 7B, 14B, and 72B represent the model sizes. We computed the average accuracy across five domains for the model.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparing Different Prompt Selection Strategies</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Experimental results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T1" title="Table 1 â€£ 4.1 Setup â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">1</span></a>. We find that the prompt guidance constructed using our approach yields the highest translation quality across LLMs of different sizes, except for Qwen-1.8B-Chat. We attribute this to the fact that small models often struggle to extract meaningful context.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Additionally, in the same few-shot experimental setup, the translation quality of selecting demonstrations based on sentence embedding similarity surpasses that of the random approach. This is also corroborated in <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib1" title="">2023</a>)</cite>.
Additionally, we observe experimental results consistent with <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib25" title="">2019</a>)</cite>, indicating that the quality of generative outputs guided in a â€œZero-shotâ€ method is generally superior to those generated through â€œRandomâ€ selection or â€œSimilarity-basedâ€ selection in a few-shot method.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We observe that â€œPrecedentâ€ serves as a strong baseline and exhibits a similar performance trend to our method: when the model size is too small, it struggles to produce high-quality translations.
As noted by <cite class="ltx_cite ltx_citemacro_citet">Agrawal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib1" title="">2023</a>)</cite>, the quality of demonstrations in prompt is crucial for translation performance.
Therefore, we attribute this to the cumulative effect of low-quality translations produced by small models, which leads to progressively worse translations.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Experiment results suggest that as the model size increases, our method continues to effectively guide LLMs in generating high-quality translations.
This also demonstrates the universality of our approach.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T4.1.1.1.1.1">methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T4.1.1.1.2">Qwen-1.8B-Chat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T4.1.1.1.3">Qwen-7B-Chat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T4.1.1.1.4">Qwen-14B-Chat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T4.1.1.1.5">Qwen-72B-Chat</th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.1">test01</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.2">test02</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.3">test01</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.4">test02</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.5">test01</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.6">test02</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.7">test01</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.8">test02</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.3.1.1">Zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.2">13.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.3">8.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.4">19.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.5">14.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.6">21.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.7">15.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.8">22.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.9">16.61</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.4.2.1">Random</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.2">12.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.3">8.24</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.4">18.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.5">14.21</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.6">21.03</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.7">15.46</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.8">21.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.9">16.68</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.5.3.1">Similar</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.2">12.48</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.3">7.96</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.4">19.48</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.5">14.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.6">21.02</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.7">15.24</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.8">21.67</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.9">17.55</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T4.1.6.4.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.2.1">14.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.3.1">9.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.4.1">19.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.5.1">14.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.6.1">21.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.7.1">15.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.8.1">23.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.9"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.9.1">17.85</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Document-level BLEU scores on the WMT literary translation dataset, which includes two test sets, namely â€œtest01â€ and â€œtest02â€. â€œtest01â€ and the datastores are within the same domain, while â€œtest02â€ is in a different domain from the datastores.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparing with Traditional NMT Models</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We compared our method against traditional NMT models. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T2" title="Table 2 â€£ 4.1 Setup â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">2</span></a>, the results show that when the model size is large enough, the translation abilities of LLMs comprehensively exceed those of dedicated translation models.
This is observed except in the en-de language pair, where we attribute the discrepancy to the insufficient understanding of the German language by the Baichuan and Qwen models, primarily trained on Chinese and English corpora.
Specifically, in the en-zh translation direction where Baichuan and Qwen excel, the translation results of LLMs significantly surpass those of specialized models.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Moreover, our experiments reveal interesting insights into the impact of model size on translation performance. As expected, larger LLMs, such as Qwen-72B-Chat, consistently outperforms smaller counterparts across various language pairs and evaluation metrics. This trend demonstrates the importance of model scale in harnessing the full potential of LLMs for translation tasks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">However, the most striking finding is the competitive performance of small LLMs, particularly those with parameters in the 7B to 14B range. Despite their smaller size compared to their larger counterparts, these models exhibit translation capabilities on par with or even outperforming dedicated translation models. This suggests that with an appropriate prompt strategy, small LLMs can offer a cost-effective solution for translation tasks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Our method has improved document translation performance on different LLMs, further verifying the robustness of our method. This underscores the generality and efficiency of our approach in leveraging LLMs for translation tasks across diverse domains and languages.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Analyzing the Effectiveness on ZPT</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The increasing interest among researchers in ZPT can be attributed to its increasing complexity and significance within the field of language translation. ZPT involves the intricate task of proficiently addressing the omission of zero pronouns in the source language text when it undergoes translation into the target language. The primary aim is to ensure the meticulous restoration or expression of these omitted pronouns in the translated output, thereby upholding linguistic accuracy, fluency, and contextual consistency.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">To assess the effectiveness of our proposed method in the context of the ZPT task, a series of experiments were conducted on the comprehensive GuoFeng dataset. Experimental results are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T3" title="Table 3 â€£ 4.1 Setup â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">3</span></a>.
Remarkably, across models of varying sizes, a discernible enhancement in ZPT accuracy is consistently observed when compared to the baseline performance.
It is noteworthy, however, that the relationship between the model size and the accuracy of ZPT is not characterized by continuous improvement.
This phenomenon can be ascribed to the realization that a high overall translation quality of the model does not necessarily translate into a proportionate increase in accuracy specifically related to ZPT.
Additionally, we found that the â€œPrecedentâ€ method outperforms similarity-based prompt selection strategies, indicating the importance of contextual information in the ZPT task. Our approach, which considers dynamic contextual information, effectively captures relevant context while filtering out irrelevant information. As a result, our method demonstrates superior performance in the ZPT task.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Hence, it becomes evident that a mere augmentation of the model size does not suffice for the improvement of ZPT accuracy. Consequently, the exploration and adoption of more effective methods become imperative in the pursuit of advancing the state-of-the-art in this domain.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="S4.F2.1.g1" src="x2.png" width="653"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Average attention scores of the final layer of the Qwen-14B-Chat model. The darker the color, the higher the attention score assigned to that token. It is noteworthy that the model attends to â€œä»–â€ (meaning â€œheâ€ in English) from the examples we extract when generating â€œheâ€. Our examples successfully help the model in translating the zero pronoun â€œä»–â€ (meaning â€œheâ€ in English).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Analyzing the Effectiveness on Literary Translation</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The significant challenges for translating literary works include entity consistency, anaphora resolution, and lexical choice <cite class="ltx_cite ltx_citemacro_cite">Matusov (<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#bib.bib21" title="">2019</a>)</cite>.
Literary works exhibit longer contextual spans compared to texts in other domains, such as news articles. Therefore, LLMs must possess the ability to model distant contexts in order to learn translation consistency and appropriate lexical choices.
Our approach, by focusing on the context relevant to the current sentence and subsequently summarizing the context, is able to encapsulate more contextual information within a limited-length prompt.
To validate whether our method can guide the model to generate more consistent translations, we conducted experiments on the WMT23 Literary Translation dataset.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">The experimental results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T4" title="Table 4 â€£ 4.2 Comparing Different Prompt Selection Strategies â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">4</span></a>.
We observe consistent improvements of our approach over the baseline method, both on in-domain and out-of-domain test sets.
Consistent with the above experimental results, the strategy of simply using sentence embedding similarity to select demonstrations often performs even worse than random selection. This implies that a straightforward prompt selection strategy based on similarity is not suitable for challenging tasks like literary translation.
However, our method is able to effectively extract contextual information, thereby guiding LLMs to generate higher-quality translations, specifically achieving better entity consistency.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.1.2.1.1">methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.2.1.2">de-en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.2.1.3">zh-en</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.3.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.1.3.1.1.1">w/o DCW</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.3.1.2">35.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.3.1.3">26.03</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.1.1.1.1">max <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.1.m1.1a"><mo id="S4.T5.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T5.1.1.1.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.1.m1.1d">â‡’</annotation></semantics></math> avg</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.2">36.05</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3">26.38</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T5.1.4.2.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.4.2.2.1">36.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.4.2.3.1">26.77</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation study on the de-en and zh-en WMT22 newstest dataset.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we conducted experiments to further analyze the effectiveness of our approach.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To verify the effectiveness of various factors on our method, we further compared our method with the following variants and present the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T5" title="Table 5 â€£ 4.5 Analyzing the Effectiveness on Literary Translation â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">5</span></a>:</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">(1) <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">w/o DCW</span>. In this variant, we directly used a Fixed Context Window to select the context. Specifically, we utilized the preceding and subsequent two sentences of the current sentence as the context. As reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T5" title="Table 5 â€£ 4.5 Analyzing the Effectiveness on Literary Translation â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">5</span></a>, this variant dramatically decreases performance on two language pair test sets.
It reveals the significance of dynamically selecting context through attention scores.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">(2) <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.1">max <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S5.SS1.p3.1.1.m1.1"><semantics id="S5.SS1.p3.1.1.m1.1a"><mo id="S5.SS1.p3.1.1.m1.1.1" stretchy="false" xref="S5.SS1.p3.1.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.1.m1.1b"><ci id="S5.SS1.p3.1.1.m1.1.1.cmml" xref="S5.SS1.p3.1.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.1.m1.1d">â‡’</annotation></semantics></math> avg</span>. In this variant, we replaced the max function with the average function in Eq.â€„(<a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S3.E2" title="In 3.1 Dynamic Context Window â€£ 3 CAP: Context-Aware Prompting â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">2</span></a>). From Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.T5" title="Table 5 â€£ 4.5 Analyzing the Effectiveness on Literary Translation â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">5</span></a>, we can observe that this variant performs worse than our method. It demonstrates the effectiveness of our approach in computing the sentence-level attention score.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Visualization</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In order to explain why our approach successfully translates omitted zero pronouns in Chinese, we extracted the attention weights from the final layer of the Qwen-14B-Chat model and visualized it using appropriate tools.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/alan-cooney/CircuitsVis</span></span></span>
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.F2" title="Figure 2 â€£ 4.4 Analyzing the Effectiveness on ZPT â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe that when the model attempts to translate the zero pronoun â€œHeâ€, it exhibits greater attention to the subject in the context and the associated information about the subject.
The visualized results reflect the effectiveness of our approach, showcasing its ability to capture relevant contextual information and guide the model in generating accurate and coherent translations.
This indicates that our approach involves more than merely translating words at the surface level; rather, it entails translating based on a profound comprehension of sentence structure and context, thereby offering guidance and support for the model to generate more precise translations, thereby demonstrating the effectiveness of our method.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T6.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.1.1">
<span class="ltx_p" id="S5.T6.1.1.1.1.1.1" style="width:45.5pt;">Source</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T6.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.2.1">
<span class="ltx_p" id="S5.T6.1.1.1.2.1.1" style="width:381.3pt;">ä¸»è¦ä»äº‹åª’ä½“å¤§æ•°æ®è®¡ç®—ã€ç½‘ç»œå¤šåª’ä½“ä¸è·¨åª’ä½“æ™ºèƒ½ç­‰ç ”ç©¶å·¥ä½œã€‚</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T6.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.2.1.1">
<span class="ltx_p" id="S5.T6.1.2.2.1.1.1" style="width:45.5pt;">Reference</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T6.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.2.2.1">
<span class="ltx_p" id="S5.T6.1.2.2.2.1.1" style="width:381.3pt;"><span class="ltx_text" id="S5.T6.1.2.2.2.1.1.1" style="color:#0000FF;">His</span> research fields primarily concentrate on Media Big Data Computing, Network Multimedia and Cross-Media Intelligence, etc.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T6.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.3.1.1">
<span class="ltx_p" id="S5.T6.1.3.3.1.1.1" style="width:45.5pt;">Zero-shot</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T6.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.3.2.1">
<span class="ltx_p" id="S5.T6.1.3.3.2.1.1" style="width:381.3pt;"><span class="ltx_text" id="S5.T6.1.3.3.2.1.1.1" style="color:#FF0000;">Our</span> research focuses on media big data computing, network multimedia, and cross-media intelligence.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T6.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.4.1.1">
<span class="ltx_p" id="S5.T6.1.4.4.1.1.1" style="width:45.5pt;">Random</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T6.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.4.2.1">
<span class="ltx_p" id="S5.T6.1.4.4.2.1.1" style="width:381.3pt;"><span class="ltx_text" id="S5.T6.1.4.4.2.1.1.1" style="color:#FF0000;">My</span> main research works involve media big data computation, network multimedia and cross-media intelligence.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T6.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.5.1.1">
<span class="ltx_p" id="S5.T6.1.5.5.1.1.1" style="width:45.5pt;">Similar</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T6.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.5.2.1">
<span class="ltx_p" id="S5.T6.1.5.5.2.1.1" style="width:381.3pt;"><span class="ltx_text" id="S5.T6.1.5.5.2.1.1.1" style="color:#FF0000;">The</span> main research areas include media big data computing, network multimedia and cross-media intelligence.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T6.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.6.1.1">
<span class="ltx_p" id="S5.T6.1.6.6.1.1.1" style="width:45.5pt;">Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T6.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.6.2.1">
<span class="ltx_p" id="S5.T6.1.6.6.2.1.1" style="width:381.3pt;"><span class="ltx_text" id="S5.T6.1.6.6.2.1.1.1" style="color:#0000FF;">His</span> main research focuses on media big data computation, network multimedia, and cross-media intelligence.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>A Case Study on the translation of Qwen-14B-Chat with various prompt selection strategies. <span class="ltx_text" id="S5.T6.4.1" style="color:#FF0000;">Red</span> text denotes incorrect translations of Zero Pronoun while <span class="ltx_text" id="S5.T6.5.2" style="color:#0000FF;">blue</span> text indicates accurate translations.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Case Study</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In order to assess the precision of ZPT across various methods in the ZPT task, a comprehensive case study was undertaken. As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S5.T6" title="Table 6 â€£ 5.2 Visualization â€£ 5 Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">6</span></a>, it is observed that the â€œZero-shotâ€, â€œRandomâ€, and â€œSimilarâ€ methods exhibit inaccuracies in translating omitted zero pronouns in Chinese. Conversely, our approach demonstrates proficiency by accurately translating the omitted zero pronouns as â€œHisâ€.
This success can be attributed to the methodâ€™s adeptness at extracting relevant contextual information, enabling the model to deduce the omitted zero pronouns effectively.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">In summary, our method stands out in its capacity to extract valuable contextual information, guiding the model towards generating translations that are not only more accurate but also more cohesive and coherent. Further examples and elaborate experimental results are available in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A2" title="Appendix B Case Study and Visualization Analysis â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">B</span></a> for reference.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we have presented a novel approach that guides the model to generate more accurate, cohesive, and coherent document-level translations via in-context learning.
By selecting relevant contexts using our carefully designed method, we can extract topic information from the context summaries. These summaries serve as a guide for selecting the most beneficial examples from the datastore to enhance the in-context learning procedure of LLMs, hence mitigating the length limitation issue of in-context learning for DOCMT.
Experimental results on various benchmarks demonstrate the effectiveness of our approach, particularly in ZPT and literary translation tasks, where our method outperforms the baseline significantly.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">By selecting better contexts and demonstrations, we have effectively increased the ZPT accuracy and BLEU score of DOCMT with LLMs via in-context learning. We have validated the effectiveness of our method using ZPT accuracy and BLEU score. However, beyond the general BLEU and Blonde metric, DOCMT encompasses challenges such as coreference consistency. We also need to make improvements to address these challenges. Moreover, we should also investigate the performance on more language pairs, which we consider as future work.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Yet another limitation in our work is the increased computational demand. Since our approach necessitates the computation of dynamic context and summarization of context, the computational load is slightly higher compared to the baseline. We intend to explore more efficient document-level machine translation methods in future work.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The present research was supported by the National Key Research and Development Program of China (Grant No. 2023YFE0116400). We would like to thank the anonymous reviewers for their insightful comments.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2023)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.564" title="">In-context examples selection for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 8857â€“8873. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, YuÂ Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, AnÂ Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.16609" title="">Qwen technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CoRR</em>, abs/2309.16609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, and Weihua Luo. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.ACL-LONG.267" title="">G-transformer for document-level machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 3442â€“3455. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussÃ  etÂ al. (2022)</span>
<span class="ltx_bibblock">
MartaÂ R. Costa-jussÃ , James Cross, Onur Ã‡elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, AlÂ Youngblood, Bapi Akula, LoÃ¯c Barrault, GabrielÂ Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, KaushikÂ Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, NecipÂ Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco GuzmÃ¡n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">CoRR</em>, abs/2207.04672.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v22/20-1307.html" title="">Beyond english-centric multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">J. Mach. Learn. Res.</em>, 22:107:1â€“107:48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Pengzhi Gao, Zhongjun He, Hua Wu, and Haifeng Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.05861" title="">Towards boosting many-to-many multilingual machine translation with large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, abs/2401.05861.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.04118" title="">Exploring human-like translation strategy with large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2305.04118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy etÂ al. (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, YoungÂ Jin Kim, Mohamed Afify, and HanyÂ Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2302.09210" title="">How good are GPT models at machine translation? A comprehensive evaluation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/2302.09210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, and Ming Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.NAACL-MAIN.111" title="">Blonde: An automatic evaluation metric for document-level machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022</em>, pages 1550â€“1565. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yikun Lei, Yuqi Ren, and Deyi Xiong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.462" title="">Codonmt: Modeling cohesion devices for document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022</em>, pages 5205â€“5216. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.ACL-MAIN.703" title="">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages 7871â€“7880. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Jia Li, GeÂ Li, Chongyang Tao, Jia Li, Huangzhao Zhang, Fang Liu, and Zhi Jin. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.09748" title="">Large language model-aware in-context learning for code generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2310.09748.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yachao Li, Junhui Li, Jing Jiang, Shimin Tao, Hao Yang, and Min Zhang. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TASLP.2023.3313445" title="">P-transformer: Towards better document-to-document neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE ACM Trans. Audio Speech Lang. Process.</em>, 31:3859â€“3870.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yachao Li, Junhui Li, Jing Jiang, and Min Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.08088" title="">Enhancing document-level translation of large language model via translation mixed-instructions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, abs/2401.08088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.06575" title="">Chain-of-dictionary prompting elicits translation in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2305.06575.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lupo etÂ al. (2022)</span>
<span class="ltx_bibblock">
Lorenzo Lupo, Marco Dinarelli, and Laurent Besacier. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.77" title="">Focused concatenation for context-aware neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022</em>, pages 830â€“842. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xinglin Lyu, Junhui Li, Zhengxian Gong, and Min Zhang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.262" title="">Encouraging lexical translation consistency for document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 3265â€“3277. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2020)</span>
<span class="ltx_bibblock">
Shuming Ma, Dongdong Zhang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.ACL-MAIN.321" title="">A simple and effective unified encoder for document-level machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages 3505â€“3511. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacÃ© and Servan (2019)</span>
<span class="ltx_bibblock">
Valentin MacÃ© and Christophe Servan. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2019.iwslt-1.21" title="">Using whole document context in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 16th International Conference on Spoken Language Translation, IWSLT 2019, Hong Kong, November 2-3, 2019</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maruf etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3441691" title="">A survey on document-level neural machine translation: Methods and evaluation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM Comput. Surv.</em>, 54(2):45:1â€“45:36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matusov (2019)</span>
<span class="ltx_bibblock">
Evgeny Matusov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W19-7302" title="">The challenges of using neural machine translation for literature</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Qualities of Literary Machine Translation</em>, pages 10â€“19, Dublin, Ireland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min etÂ al. (2024)</span>
<span class="ltx_bibblock">
Bonan Min, Hayley Ross, Elior Sulem, Amir PouranÂ Ben Veyseh, ThienÂ Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3605943" title="">Recent advances in natural language processing via large pre-trained language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ACM Comput. Surv.</em>, 56(2):30:1â€“30:40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, JohnÂ D. Kelleher, and Andy Way. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.22" title="">Adaptive machine translation with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation, EAMT 2023, Tampere, Finland, 12-15 June 2023</em>, pages 227â€“237. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pawar etÂ al. (2024)</span>
<span class="ltx_bibblock">
Saurav Pawar, S.Â M. TowhidulÂ Islam Tonmoy, S.Â M.Â Mehedi Zaman, Vinija Jain, Aman Chadha, and Amitava Das. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.07872" title="">The what, why, and how of context length extension techniques in large language models - A detailed survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">CoRR</em>, abs/2401.07872.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:160025533" title="">Language models are unsupervised multitask learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2020)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.09813" title="">Making monolingual sentence embeddings multilingual using knowledge distillation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/2004.09813.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao etÂ al. (2022)</span>
<span class="ltx_bibblock">
TevenÂ Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman CastagnÃ©, AlexandraÂ Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, Jonathan Tow, AlexanderÂ M. Rush, Stella Biderman, Albert Webson, PawanÂ Sasanka Ammanamanchi, Thomas Wang, BenoÃ®t Sagot, Niklas Muennighoff, AlbertÂ Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, IzÂ Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, PedroÂ Ortiz Suarez, Victor Sanh, Hugo LaurenÃ§on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, AlhamÂ Fikri Aji, Amit Alfassy, Anna Rogers, ArielÂ Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, DavidÂ Ifeoluwa Adelani, and etÂ al. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2211.05100" title="">BLOOM: A 176b-parameter open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">CoRR</em>, abs/2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Xiong (2022)</span>
<span class="ltx_bibblock">
Haoran Sun and Deyi Xiong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.447" title="">Language branch gated multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022</em>, pages 5046â€“5053. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Lei Li. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.FINDINGS-ACL.279" title="">Rethinking document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 3537â€“3548. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xin Tan, Longyin Zhang, Fang Kong, and Guodong Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.24963/IJCAI.2022/608" title="">Towards discourse-aware document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022</em>, pages 4383â€“4389. ijcai.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al. (2019)</span>
<span class="ltx_bibblock">
Xin Tan, Longyin Zhang, Deyi Xiong, and Guodong Zhou. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/D19-1168" title="">Hierarchical modeling of global context for document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, pages 1576â€“1585. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xin Tan, Longyin Zhang, and Guodong Zhou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.197" title="">Coupling context modeling with zero pronoun recovering for document-level natural language generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 2530â€“2540. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Dehan Kong, Longtao Huang, and Hui Xue. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.284" title="">Large language models can be lazy learners: Analyze shortcuts in in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 4645â€“4657. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">CoRR</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.1036" title="">Document-level machine translation with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 16646â€“16661. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Qinyong Wang, Zhenxiang Gao, and Rong Xu. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.01137" title="">Exploring the in-context learning ability of large language model for biomedical concept linking</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2307.01137.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, YiÂ Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, EdÂ H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=yzkSU5zdwD" title="">Emergent abilities of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Minghao Wu, Thuy-Trang Vu, Lizhen Qu, GeorgeÂ F. Foster, and Gholamreza Haffari. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.06468" title="">Adapting large language models for document-level machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">CoRR</em>, abs/2401.06468.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mingzhou Xu, Liangyou Li, DerekÂ F. Wong, Qun Liu, and LidiaÂ S. Chao. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.663" title="">Document graph for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 8435â€“8448. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Mingzhou Xu, Longyue Wang, DerekÂ F. Wong, Hongye Liu, Linfeng Song, LidiaÂ S. Chao, Shuming Shi, and Zhaopeng Tu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.EMNLP-MAIN.774" title="">Guofeng: A benchmark for zero pronoun recovery and translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 11266â€“11278. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, CeÂ Bian, Chao Yin, Chenxu Lv, DaÂ Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.10305" title="">Baichuan 2: Open large-scale language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">CoRR</em>, abs/2309.10305.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Pengcheng Yang, Pei Zhang, Boxing Chen, Jun Xie, and Weihua Luo. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.NAACL-MAIN.281" title="">Context-interactive pre-training for document machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, pages 3589â€“3595. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong Dai, Jintao Li, Zehao Wang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, and YuÂ Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.03868" title="">Flightllm: Efficient large language model inference with a complete mapping flow on fpgas</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">CoRR</em>, abs/2401.03868.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Biao Zhang, Ankur Bapna, Melvin Johnson, Ali Dabirmoghaddam, Naveen Arivazhagan, and Orhan Firat. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.ACL-LONG.287" title="">Multilingual document-level translation enables zero-shot transfer from sentences to documents</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 4176â€“4192. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/zhang23m.html" title="">Prompting large language model for machine translation: A case study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2">Proceedings of Machine Learning Research</em>, pages 41092â€“41110. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Linlin Zhang, Zhirui Zhang, Boxing Chen, Weihua Luo, and Luo Si. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP43922.2022.9746245" title="">Context-adaptive document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022</em>, pages 6232â€“6236. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Pei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.EMNLP-MAIN.81" title="">Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 1081â€“1087. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Pei Zhang, XuÂ Zhang, Wei Chen, Jian Yu, Yanfeng Wang, and Deyi Xiong. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3233/FAIA200358" title="">Learning contextualized sentence representations for document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ECAI 2020 - 24th European Conference on Artificial Intelligence, 29 August-8 September 2020, Santiago de Compostela, Spain, August 29 - September 8, 2020 - Including 10th Conference on Prestigious Applications of Artificial Intelligence (PAIS 2020)</em>, volume 325 of <em class="ltx_emph ltx_font_italic" id="bib.bib48.2.2">Frontiers in Artificial Intelligence and Applications</em>, pages 2298â€“2305. IOS Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.10968" title="">Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">CoRR</em>, abs/2306.10968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shaolin Zhu, Menglong Cui, and Deyi Xiong. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.lrec-main.1444" title="">Towards robust in-context learning for machine translation with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy</em>, pages 16619â€“16629. ELRA and ICCL.

</span>
</li>
</ul>
</section>
<figure class="ltx_table" id="Sx2.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx2.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx2.T7.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx2.T7.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.1.1.1.1">
<span class="ltx_p" id="Sx2.T7.1.1.1.1.1.1" style="width:45.5pt;">source</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx2.T7.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.1.1.2.1">
<span class="ltx_p" id="Sx2.T7.1.1.1.2.1.1" style="width:381.3pt;">æ‰‹åŠ¨è¿˜æ˜¯è‡ªåŠ¨ï¼Œç¡®å®šç¦»åˆæŒæ¡å¥½äº†ï¼Ÿ</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx2.T7.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx2.T7.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.2.2.1.1">
<span class="ltx_p" id="Sx2.T7.1.2.2.1.1.1" style="width:45.5pt;">reference</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx2.T7.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.2.2.2.1">
<span class="ltx_p" id="Sx2.T7.1.2.2.2.1.1" style="width:381.3pt;">Is your automatic or manual? Are <span class="ltx_text" id="Sx2.T7.1.2.2.2.1.1.1" style="color:#0000FF;">you</span> sure that you deal with your clutch well?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx2.T7.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx2.T7.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.3.3.1.1">
<span class="ltx_p" id="Sx2.T7.1.3.3.1.1.1" style="width:45.5pt;">zero-shot</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx2.T7.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.3.3.2.1">
<span class="ltx_p" id="Sx2.T7.1.3.3.2.1.1" style="width:381.3pt;">Should I use manual or automatic, and have <span class="ltx_text" id="Sx2.T7.1.3.3.2.1.1.1" style="color:#FF0000;">I</span> got a good grip on it?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx2.T7.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx2.T7.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.4.4.1.1">
<span class="ltx_p" id="Sx2.T7.1.4.4.1.1.1" style="width:45.5pt;">random</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx2.T7.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.4.4.2.1">
<span class="ltx_p" id="Sx2.T7.1.4.4.2.1.1" style="width:381.3pt;">Should we use manual or automatic methods to solve the problem? Have <span class="ltx_text" id="Sx2.T7.1.4.4.2.1.1.1" style="color:#FF0000;">we</span> grasped the situation well?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx2.T7.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx2.T7.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.5.5.1.1">
<span class="ltx_p" id="Sx2.T7.1.5.5.1.1.1" style="width:45.5pt;">similar</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx2.T7.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.5.5.2.1">
<span class="ltx_p" id="Sx2.T7.1.5.5.2.1.1" style="width:381.3pt;">Should I use manual or automatic control? Have <span class="ltx_text" id="Sx2.T7.1.5.5.2.1.1.1" style="color:#FF0000;">I</span> got a good grip on the distance and angle?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx2.T7.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="Sx2.T7.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.6.6.1.1">
<span class="ltx_p" id="Sx2.T7.1.6.6.1.1.1" style="width:45.5pt;">ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="Sx2.T7.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="Sx2.T7.1.6.6.2.1">
<span class="ltx_p" id="Sx2.T7.1.6.6.2.1.1" style="width:381.3pt;">Manual or automatic, have <span class="ltx_text" id="Sx2.T7.1.6.6.2.1.1.1" style="color:#0000FF;">you</span> determined the gear shift handle is in the correct position?</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>A Case Study on the translation of Qwen-14B-Chat with various prompt selection strategies. <span class="ltx_text" id="Sx2.T7.4.1" style="color:#FF0000;">Red</span> text denotes incorrect translations of Zero Pronoun while <span class="ltx_text" id="Sx2.T7.5.2" style="color:#0000FF;">blue</span> text indicates accurate translations.</figcaption>
</figure>
<figure class="ltx_figure" id="Sx2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="Sx2.F3.1.g1" src="x3.png" width="629"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average attention scores of the final layer of the Qwen-14B-Chat model. The darker the color, the higher the attention score assigned to that token.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Experimental Results</h2>
<figure class="ltx_table" id="A1.T8">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A1.T8.1.1.1.1" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.1.1.1">methods</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T8.1.1.1.2">Qwen-7B-Chat</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T8.1.1.1.3">Qwen-14B-Chat</th>
</tr>
<tr class="ltx_tr" id="A1.T8.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T8.1.2.2.1">de-en</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T8.1.2.2.2">zh-en</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T8.1.2.2.3">de-en</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T8.1.2.2.4">zh-en</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.3.1.1">Zero-shot</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.3.1.2">46.88</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.3.1.3">35.15</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.3.1.4">50.01</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.3.1.5">38.27</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.4.2">
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.2.1">Random</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.2.2">46.47</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.2.3">34.74</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.2.4">49.35</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.2.5">38.27</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.5.3">
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.3.1">BM25</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.3.2">45.94</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.3.3">34.63</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.3.4">49.37</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.3.5">38.00</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.6.4">
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.4.1">Similar</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.4.2">46.36</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.4.3">34.77</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.4.4">49.72</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.4.5">38.82</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.7.5">
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.5.1">Precedent</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.5.2">46.13</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.5.3">35.04</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.5.4">49.50</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.5.5">38.43</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T8.1.8.6.1">Ours</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T8.1.8.6.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.8.6.2.1">47.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T8.1.8.6.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.8.6.3.1">35.20</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T8.1.8.6.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.8.6.4.1">50.23</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T8.1.8.6.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.8.6.5.1">38.90</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Blonde scores of Qwen chat LLMs, with different prompt strategies, on the WMT22 newstest dataset.</figcaption>
</figure>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#S4.SS2" title="4.2 Comparing Different Prompt Selection Strategies â€£ 4 Experiments â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we have employed only <math alttext="d" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mi id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><ci id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">italic_d</annotation></semantics></math>-BLEU and ChrF2 to evaluate translation quality. We use Blonde scores here to further measure translation quality.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Specifically, we evaluated the translation performance of Qwen-7B-Chat and Qwen-14B-Chat on the WMT22 newtest test set using the Blonde metric under different prompt strategies. Experimental results, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#A1.T8" title="Table 8 â€£ Appendix A Additional Experimental Results â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">8</span></a>, indicate that our proposed method significantly outperforms the baselines, demonstrating its effectiveness.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Case Study and Visualization Analysis</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we present more examples to further validate our approach with case study and attention visualization.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Case Study</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#Sx2.T7" title="Table 7 â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">7</span></a>, we provide another example where in the original Chinese text, the subject â€œä½ â€ (you) is omitted due to language convention. In English, it should be correctly restored and translated as â€œyouâ€. We find that the â€œZero-shotâ€, â€œRandomâ€, and â€œSimilarâ€ methods all incorrectly recover zero pronouns, whereas our method successfully recovers the correct zero pronoun â€œyouâ€. This demonstrates that our method outperforms better than baselines in ZPT.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Visualization</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07081v1#Sx2.F3" title="Figure 3 â€£ Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that when the model is tasked with translating â€œHeâ€, it predominantly focuses on the subject of the context and its associated information, encompassing entities such as â€œç™½å´‡æ©â€ and the designation â€œæ•™æˆâ€.
This experimental result illustrates how our approach guides LLMs to generate more accurate translations via in-context learning, thus proving the effectiveness of our method.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 11 08:26:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
