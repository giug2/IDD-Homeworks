<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1809.05825] Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data</title><meta property="og:description" content="The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking.
Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment sâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1809.05825">

<!--Generated on Sat Mar  2 19:57:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\newfloatcommand</span>
<p id="p1.2" class="ltx_p">capbtabboxtable[][<span id="p1.2.1" class="ltx_ERROR undefined">\FBwidth</span>]


</p>
</div>
<h1 class="ltx_title ltx_title_document">Segmenting Unknown 3D Objects from Real Depth Images
<br class="ltx_break">using Mask R-CNN Trained on Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Danielczuk<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Matthew Matl<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Saurabh Gupta<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">1</span></sup>,
<br class="ltx_break">Andrew Li<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">1</span></sup>, Andrew Lee<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Jeffrey Mahler<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">1</span></sup>, Ken Goldberg<sup id="id17.17.id7" class="ltx_sup"><span id="id17.17.id7.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span><span class="ltx_author_notes"><sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Department of Electrical Engineering and Computer Science<sup id="id19.19.id1" class="ltx_sup"><span id="id19.19.id1.1" class="ltx_text ltx_font_italic">2</span></sup>Department of Industrial Engineering and Operations Research<sup id="id20.20.id1" class="ltx_sup"><span id="id20.20.id1.1" class="ltx_text ltx_font_italic">1-2</span></sup>The AUTOLAB at UC Berkeley; Berkeley, CA 94720, USA<span id="id21.21.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{mdanielczuk, mmatl, sgupta, andrewyli, andrew_lee, jmahler, goldberg}@berkeley.edu</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking.
Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available.
As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world.
We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models.
We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application.
Code, the synthetic training dataset, and supplementary material are available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://bit.ly/2letCuE</span>.</p>
</div>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Category-agnostic instance segmentation</span>, or the ability to mask the pixels belonging to each individual object in a scene regardless of the objectâ€™s class, has potential to enhance robotic perception pipelines for applications such as instance-specific grasping, where a target object must be identified and grasped among potentially unknown distractor objects in a cluttered environment.
For example, recent approaches to grasp planning on unknown objects have used deep learning to generate robot grasping policies from massive datasets of images, grasps, and reward labels Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
While these methods are effective at generalizing across a wide variety of objects, they search for high-quality grasp affordances across an entire scene and do not distinguish between the objects they are grasping.
Nonetheless, these methods may be extended to plan grasps for a particular target object by constraining grasp planning to an object mask produced by category-agnostic instance segmentation.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1809.05825/assets/images/money.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Color image (left) and depth image segmented by SD Mask RCNN (right) for a heap of objects. Despite clutter, occlusions, and complex geometries, SD Mask RCNN is able to correctly mask each of the objects.

</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Object segmentation without prior models of the objects is difficult due to sensor noise and occlusions.
Computer vision techniques for generating category-agnostic object proposalsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> often oversegment and require secondary pruning steps to find a set of valid independent objects.
A variety of recent methods have demonstrated the ability to accurately segment RGB images into pre-defined semantic classes such as humans, bicycles, and cars by training deep neural networks on massive, hand-labeled datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
These techniques require time-consuming human labeling to generate training dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and existing datasets consist of RGB images of natural scenes that are very different from the types of cluttered scenes commonly encountered in warehouses or fulfillment centers.
Adding new object classes or generating data for new types of environments requires additional manual labeling.
Thus, in robotics, pixel-wise object segmentation is often avoidedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or used for a small number of object classes, where semantic segmentation networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or predefined featuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> can be used.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address these issues, we present a method and dataset for training Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, a popular instance segmentation network, to perform category-agnostic instance segmentation on real depth images without training on hand-labeled data â€“ and, in fact, without training on real data at all.
We build on recent research which suggests that networks trained on synthetic depth images can transfer well from simulation to reality in some domainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and that depth cues can enhance instance segmentation in simulated imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
To learn an instance segmentation network that transfers from simulation to reality, we propose to train on a large synthetic dataset of depth images with domain randomizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> over a diverse set of 3D objects, camera poses, and camera intrinsic parameters.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper contributes:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A method for rapidly generating a synthetic dataset of depth images and segmentation masks using domain randomization for robust transfer from simulation to reality.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The Warehouse Instance Segmentation Dataset for Object Manipulation (WISDOM), a hybrid sim/real dataset designed for training and evaluating category-agnostic instance segmentation methods in the context of robotic bin picking.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Synthetic Depth Mask R-CNN (SD Mask R-CNN), a Mask R-CNN adaptation designed to perform deep category-agnostic object instance segmentation on depth images, trained on WISDOM-Sim.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Experiments evaluating the sim-to-real generalization abilities of SD Mask R-CNN and performance benchmarks comparing it against a set of baseline instance segmentation methods.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In an experimental evaluation on WISDOM-Realâ€™s high-resolution dataset, SD Mask R-CNN achieves significantly higher average precision and recall than baseline learning methods fine-tuned on WISDOM-Real training images, and also generalizes to a low-resolution sensor. We employ SD Mask R-CNN as part of an instance-specific grasping pipeline on an ABB YuMi bimanual industrial robot and find that it can increase success rate by 20% over standard point cloud segmentation techniques.</p>
</div>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This work builds on prior research in region proposal generation,
neural architectures for image segmentation, and use of synthetic data
for learning models in computer vision and robotics. The approach presented
here is motivated and informed by robotic grasping, manipulation, and bin-picking tasks.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph ltx_indentfirst">
<h5 class="ltx_title ltx_title_paragraph">Box and Region Proposals</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Early work in computer vision focused on using bottom-up cues for
generating box and region proposals in images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Such techniques typically detect contours in images to obtain a hierarchical segmentation.
Regions from such a hierarchical segmentation are combined together and
used with low-level objectness cues to produce a list of regions that cover the
objects present in the image. The focus of these techniques is on getting high recall, and
the soup of output region proposals is used with a classifier to detect or segment
objects of interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">More recently, given advances in learning image representations, researchers have
used feature learning techniques (specifically CNN based models) to tackle this problem of producing
bounding box proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and
region proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Unlike bottom-up segmentation methods, these techniques use data-driven methods
to learn high-level semantic markers for proposing and classifying object segments.
Some of these learning-based region proposal techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
have built upon advances in models for image segmentation and use fine-grained
information from early layers in CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
to produce high quality regions.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">While most work in computer vision has used RGB images to study these problems,
researchers have also studied similar problems with depth data. Once again there are
bottom-up techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
that use low-level geometry-based cues to come up with region proposals, as well as more recent
top-down learning-based techniques to produce proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
in the form of image segments or 3D bounding boxes that contain objects in the scene.Â <cite class="ltx_cite ltx_citemacro_citet">Shao etÂ al. [<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> combined color and depth modalities, featurizing objects and clustering the features to produce object instance segmentation masks on simulated RGB-D images.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p4.1" class="ltx_p">A parallel stream of work has tackled the problem of class-specific segmentation. Some
of these works ignore object instances and study semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>,
while others try to distinguish between instances
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
Similar research has also been done in context of input from depth sensors
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph ltx_indentfirst">
<h5 class="ltx_title ltx_title_paragraph">Synthetic Data for Training Models</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Our research is related to a number of recent efforts for
rapidly acquiring large training datasets containing image and
ground truth masks with limited or no human labeling.
The most natural way is to augment training with synthetic
color and depth images collected in simulation.
This idea has been explored extensively for training semantic segmentation networks
for autonomous drivingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and for estimating human and object pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.
Another approach is to use self-supervision to increase
training dataset size by first hand-aligning 3D models
to images with easy-to-use interfacesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
or algorithmically matching a set of 3D CAD models
to initial RGB-D imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>,
and then projecting each 3D model into a larger
set of images from camera viewpoints with known 6-DOF poses. In comparison, we generate synthetic training datasets for
category-agnostic object segmentation in a robot bin picking domain.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph ltx_indentfirst">
<h5 class="ltx_title ltx_title_paragraph">Robotics Applications</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Segmentation methods have been applied extensively to grasping target objects, most notably in the Amazon Robotics Challenge (ARC).
Many classical grasping pipelines consisted of an alignment phase, in which 3D CAD models or scans are matched to RGB-D point clouds, and an indexing phase, in which precomputed grasps are executed given the estimated object poseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
In the 2015 ARC, the winning team followed a similar strategy, using a histogram backprojection method to segment objects from shelves and point cloud heuristics for grasp planningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
In 2016, many teams used deep learning to segment objects for the alignment phase, training semantic segmentation networks with separate classes for each object instance on hand-labeledÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> or self-supervised datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Team ACRV, the winners of the 2017 ARC, fine-tuned RefineNet to segment and classify 40 unique known objects in a bin, with a system to quickly learn new items with a semi-automated procedureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
In contrast, our method uses deep learning for category-agnostic segmentation, which can can be used to segment a wide variety of objects not seen in training.

</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Problem Statement</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We consider the problem of depth-based category-agnostic instance segmentation, or finding subsets of pixels corresponding to unique unknown objects in a single depth image.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To formalize category-agnostic instance segmentation, we use the following definitions:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.8" class="ltx_p"><span id="S3.I1.i1.p1.8.1" class="ltx_text ltx_font_italic">States:</span> Let <math id="S3.I1.i1.p1.1.m1.7" class="ltx_Math" alttext="\mathbf{x}=\{\mathcal{O}_{1},\ldots,\mathcal{O}_{m},\mathcal{B}_{1},\ldots,\mathcal{B}_{n},\mathcal{C}\}" display="inline"><semantics id="S3.I1.i1.p1.1.m1.7a"><mrow id="S3.I1.i1.p1.1.m1.7.7" xref="S3.I1.i1.p1.1.m1.7.7.cmml"><mi id="S3.I1.i1.p1.1.m1.7.7.6" xref="S3.I1.i1.p1.1.m1.7.7.6.cmml">ğ±</mi><mo id="S3.I1.i1.p1.1.m1.7.7.5" xref="S3.I1.i1.p1.1.m1.7.7.5.cmml">=</mo><mrow id="S3.I1.i1.p1.1.m1.7.7.4.4" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml"><mo stretchy="false" id="S3.I1.i1.p1.1.m1.7.7.4.4.5" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">{</mo><msub id="S3.I1.i1.p1.1.m1.4.4.1.1.1" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.4.4.1.1.1.2" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1.2.cmml">ğ’ª</mi><mn id="S3.I1.i1.p1.1.m1.4.4.1.1.1.3" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1.3.cmml">1</mn></msub><mo id="S3.I1.i1.p1.1.m1.7.7.4.4.6" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">,</mo><mi mathvariant="normal" id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S3.I1.i1.p1.1.m1.7.7.4.4.7" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">,</mo><msub id="S3.I1.i1.p1.1.m1.5.5.2.2.2" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.5.5.2.2.2.2" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2.2.cmml">ğ’ª</mi><mi id="S3.I1.i1.p1.1.m1.5.5.2.2.2.3" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2.3.cmml">m</mi></msub><mo id="S3.I1.i1.p1.1.m1.7.7.4.4.8" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">,</mo><msub id="S3.I1.i1.p1.1.m1.6.6.3.3.3" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.6.6.3.3.3.2" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3.2.cmml">â„¬</mi><mn id="S3.I1.i1.p1.1.m1.6.6.3.3.3.3" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3.3.cmml">1</mn></msub><mo id="S3.I1.i1.p1.1.m1.7.7.4.4.9" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">,</mo><mi mathvariant="normal" id="S3.I1.i1.p1.1.m1.2.2" xref="S3.I1.i1.p1.1.m1.2.2.cmml">â€¦</mi><mo id="S3.I1.i1.p1.1.m1.7.7.4.4.10" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">,</mo><msub id="S3.I1.i1.p1.1.m1.7.7.4.4.4" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.7.7.4.4.4.2" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4.2.cmml">â„¬</mi><mi id="S3.I1.i1.p1.1.m1.7.7.4.4.4.3" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4.3.cmml">n</mi></msub><mo id="S3.I1.i1.p1.1.m1.7.7.4.4.11" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.3.3" xref="S3.I1.i1.p1.1.m1.3.3.cmml">ğ’</mi><mo stretchy="false" id="S3.I1.i1.p1.1.m1.7.7.4.4.12" xref="S3.I1.i1.p1.1.m1.7.7.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.7b"><apply id="S3.I1.i1.p1.1.m1.7.7.cmml" xref="S3.I1.i1.p1.1.m1.7.7"><eq id="S3.I1.i1.p1.1.m1.7.7.5.cmml" xref="S3.I1.i1.p1.1.m1.7.7.5"></eq><ci id="S3.I1.i1.p1.1.m1.7.7.6.cmml" xref="S3.I1.i1.p1.1.m1.7.7.6">ğ±</ci><set id="S3.I1.i1.p1.1.m1.7.7.4.5.cmml" xref="S3.I1.i1.p1.1.m1.7.7.4.4"><apply id="S3.I1.i1.p1.1.m1.4.4.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.4.4.1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1.2">ğ’ª</ci><cn type="integer" id="S3.I1.i1.p1.1.m1.4.4.1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.4.4.1.1.1.3">1</cn></apply><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">â€¦</ci><apply id="S3.I1.i1.p1.1.m1.5.5.2.2.2.cmml" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.5.5.2.2.2.1.cmml" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.5.5.2.2.2.2.cmml" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2.2">ğ’ª</ci><ci id="S3.I1.i1.p1.1.m1.5.5.2.2.2.3.cmml" xref="S3.I1.i1.p1.1.m1.5.5.2.2.2.3">ğ‘š</ci></apply><apply id="S3.I1.i1.p1.1.m1.6.6.3.3.3.cmml" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.6.6.3.3.3.1.cmml" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.6.6.3.3.3.2.cmml" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3.2">â„¬</ci><cn type="integer" id="S3.I1.i1.p1.1.m1.6.6.3.3.3.3.cmml" xref="S3.I1.i1.p1.1.m1.6.6.3.3.3.3">1</cn></apply><ci id="S3.I1.i1.p1.1.m1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.2.2">â€¦</ci><apply id="S3.I1.i1.p1.1.m1.7.7.4.4.4.cmml" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.7.7.4.4.4.1.cmml" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.7.7.4.4.4.2.cmml" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4.2">â„¬</ci><ci id="S3.I1.i1.p1.1.m1.7.7.4.4.4.3.cmml" xref="S3.I1.i1.p1.1.m1.7.7.4.4.4.3">ğ‘›</ci></apply><ci id="S3.I1.i1.p1.1.m1.3.3.cmml" xref="S3.I1.i1.p1.1.m1.3.3">ğ’</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.7c">\mathbf{x}=\{\mathcal{O}_{1},\ldots,\mathcal{O}_{m},\mathcal{B}_{1},\ldots,\mathcal{B}_{n},\mathcal{C}\}</annotation></semantics></math> be a ground truth state which contains (A) a set of <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">m</annotation></semantics></math> foreground objects in the environment, (B) a set of <math id="S3.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">n</annotation></semantics></math> background objects (e.g. bins, tables), and (C) a depth camera. Here, each object state <math id="S3.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{O}_{i}" display="inline"><semantics id="S3.I1.i1.p1.4.m4.1a"><msub id="S3.I1.i1.p1.4.m4.1.1" xref="S3.I1.i1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.4.m4.1.1.2" xref="S3.I1.i1.p1.4.m4.1.1.2.cmml">ğ’ª</mi><mi id="S3.I1.i1.p1.4.m4.1.1.3" xref="S3.I1.i1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.4.m4.1b"><apply id="S3.I1.i1.p1.4.m4.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m4.1.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.4.m4.1.1.2.cmml" xref="S3.I1.i1.p1.4.m4.1.1.2">ğ’ª</ci><ci id="S3.I1.i1.p1.4.m4.1.1.3.cmml" xref="S3.I1.i1.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.4.m4.1c">\mathcal{O}_{i}</annotation></semantics></math> or <math id="S3.I1.i1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{B}_{j}" display="inline"><semantics id="S3.I1.i1.p1.5.m5.1a"><msub id="S3.I1.i1.p1.5.m5.1.1" xref="S3.I1.i1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.5.m5.1.1.2" xref="S3.I1.i1.p1.5.m5.1.1.2.cmml">â„¬</mi><mi id="S3.I1.i1.p1.5.m5.1.1.3" xref="S3.I1.i1.p1.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.5.m5.1b"><apply id="S3.I1.i1.p1.5.m5.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.5.m5.1.1.2.cmml" xref="S3.I1.i1.p1.5.m5.1.1.2">â„¬</ci><ci id="S3.I1.i1.p1.5.m5.1.1.3.cmml" xref="S3.I1.i1.p1.5.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.5.m5.1c">\mathcal{B}_{j}</annotation></semantics></math> is defined by the objectâ€™s geometry and 6-DOF pose, while the camera state <math id="S3.I1.i1.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S3.I1.i1.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.6.m6.1.1" xref="S3.I1.i1.p1.6.m6.1.1.cmml">ğ’</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.6.m6.1b"><ci id="S3.I1.i1.p1.6.m6.1.1.cmml" xref="S3.I1.i1.p1.6.m6.1.1">ğ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.6.m6.1c">\mathcal{C}</annotation></semantics></math> is defined by its intrinsics matrix <math id="S3.I1.i1.p1.7.m7.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.I1.i1.p1.7.m7.1a"><mi id="S3.I1.i1.p1.7.m7.1.1" xref="S3.I1.i1.p1.7.m7.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.7.m7.1b"><ci id="S3.I1.i1.p1.7.m7.1.1.cmml" xref="S3.I1.i1.p1.7.m7.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.7.m7.1c">K</annotation></semantics></math> and its 6-DOF pose <math id="S3.I1.i1.p1.8.m8.3" class="ltx_Math" alttext="(R,\mathbf{t})\in SE(3)" display="inline"><semantics id="S3.I1.i1.p1.8.m8.3a"><mrow id="S3.I1.i1.p1.8.m8.3.4" xref="S3.I1.i1.p1.8.m8.3.4.cmml"><mrow id="S3.I1.i1.p1.8.m8.3.4.2.2" xref="S3.I1.i1.p1.8.m8.3.4.2.1.cmml"><mo stretchy="false" id="S3.I1.i1.p1.8.m8.3.4.2.2.1" xref="S3.I1.i1.p1.8.m8.3.4.2.1.cmml">(</mo><mi id="S3.I1.i1.p1.8.m8.1.1" xref="S3.I1.i1.p1.8.m8.1.1.cmml">R</mi><mo id="S3.I1.i1.p1.8.m8.3.4.2.2.2" xref="S3.I1.i1.p1.8.m8.3.4.2.1.cmml">,</mo><mi id="S3.I1.i1.p1.8.m8.2.2" xref="S3.I1.i1.p1.8.m8.2.2.cmml">ğ­</mi><mo stretchy="false" id="S3.I1.i1.p1.8.m8.3.4.2.2.3" xref="S3.I1.i1.p1.8.m8.3.4.2.1.cmml">)</mo></mrow><mo id="S3.I1.i1.p1.8.m8.3.4.1" xref="S3.I1.i1.p1.8.m8.3.4.1.cmml">âˆˆ</mo><mrow id="S3.I1.i1.p1.8.m8.3.4.3" xref="S3.I1.i1.p1.8.m8.3.4.3.cmml"><mi id="S3.I1.i1.p1.8.m8.3.4.3.2" xref="S3.I1.i1.p1.8.m8.3.4.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.8.m8.3.4.3.1" xref="S3.I1.i1.p1.8.m8.3.4.3.1.cmml">â€‹</mo><mi id="S3.I1.i1.p1.8.m8.3.4.3.3" xref="S3.I1.i1.p1.8.m8.3.4.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.8.m8.3.4.3.1a" xref="S3.I1.i1.p1.8.m8.3.4.3.1.cmml">â€‹</mo><mrow id="S3.I1.i1.p1.8.m8.3.4.3.4.2" xref="S3.I1.i1.p1.8.m8.3.4.3.cmml"><mo stretchy="false" id="S3.I1.i1.p1.8.m8.3.4.3.4.2.1" xref="S3.I1.i1.p1.8.m8.3.4.3.cmml">(</mo><mn id="S3.I1.i1.p1.8.m8.3.3" xref="S3.I1.i1.p1.8.m8.3.3.cmml">3</mn><mo stretchy="false" id="S3.I1.i1.p1.8.m8.3.4.3.4.2.2" xref="S3.I1.i1.p1.8.m8.3.4.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.8.m8.3b"><apply id="S3.I1.i1.p1.8.m8.3.4.cmml" xref="S3.I1.i1.p1.8.m8.3.4"><in id="S3.I1.i1.p1.8.m8.3.4.1.cmml" xref="S3.I1.i1.p1.8.m8.3.4.1"></in><interval closure="open" id="S3.I1.i1.p1.8.m8.3.4.2.1.cmml" xref="S3.I1.i1.p1.8.m8.3.4.2.2"><ci id="S3.I1.i1.p1.8.m8.1.1.cmml" xref="S3.I1.i1.p1.8.m8.1.1">ğ‘…</ci><ci id="S3.I1.i1.p1.8.m8.2.2.cmml" xref="S3.I1.i1.p1.8.m8.2.2">ğ­</ci></interval><apply id="S3.I1.i1.p1.8.m8.3.4.3.cmml" xref="S3.I1.i1.p1.8.m8.3.4.3"><times id="S3.I1.i1.p1.8.m8.3.4.3.1.cmml" xref="S3.I1.i1.p1.8.m8.3.4.3.1"></times><ci id="S3.I1.i1.p1.8.m8.3.4.3.2.cmml" xref="S3.I1.i1.p1.8.m8.3.4.3.2">ğ‘†</ci><ci id="S3.I1.i1.p1.8.m8.3.4.3.3.cmml" xref="S3.I1.i1.p1.8.m8.3.4.3.3">ğ¸</ci><cn type="integer" id="S3.I1.i1.p1.8.m8.3.3.cmml" xref="S3.I1.i1.p1.8.m8.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.8.m8.3c">(R,\mathbf{t})\in SE(3)</annotation></semantics></math>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.6" class="ltx_p"><span id="S3.I1.i2.p1.6.1" class="ltx_text ltx_font_italic">Observations:</span> Let <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{y}\in\mathbb{R}^{H\times W}_{+}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">ğ²</mi><mo id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msubsup id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.3.2.2" xref="S3.I1.i2.p1.1.m1.1.1.3.2.2.cmml">â„</mi><mo id="S3.I1.i2.p1.1.m1.1.1.3.3" xref="S3.I1.i2.p1.1.m1.1.1.3.3.cmml">+</mo><mrow id="S3.I1.i2.p1.1.m1.1.1.3.2.3" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.3.2.3.2" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.I1.i2.p1.1.m1.1.1.3.2.3.1" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.1.cmml">Ã—</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3.2.3.3" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.3.cmml">W</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><in id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></in><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">ğ²</ci><apply id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">subscript</csymbol><apply id="S3.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.3.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.3.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2.2">â„</ci><apply id="S3.I1.i2.p1.1.m1.1.1.3.2.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3"><times id="S3.I1.i2.p1.1.m1.1.1.3.2.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.1"></times><ci id="S3.I1.i2.p1.1.m1.1.1.3.2.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.2">ğ»</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.2.3.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2.3.3">ğ‘Š</ci></apply></apply><plus id="S3.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\mathbf{y}\in\mathbb{R}^{H\times W}_{+}</annotation></semantics></math> be a depth image observation of the state <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\mathbf{x}</annotation></semantics></math> generated from <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">ğ’</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">ğ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">\mathcal{C}</annotation></semantics></math> with height <math id="S3.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.I1.i2.p1.4.m4.1a"><mi id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><ci id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">H</annotation></semantics></math> and width <math id="S3.I1.i2.p1.5.m5.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.I1.i2.p1.5.m5.1a"><mi id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><ci id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">W</annotation></semantics></math>.
Let the pixel space <math id="S3.I1.i2.p1.6.m6.4" class="ltx_Math" alttext="\mathcal{U}=[0,H-1]\times[0,W-1]" display="inline"><semantics id="S3.I1.i2.p1.6.m6.4a"><mrow id="S3.I1.i2.p1.6.m6.4.4" xref="S3.I1.i2.p1.6.m6.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.6.m6.4.4.4" xref="S3.I1.i2.p1.6.m6.4.4.4.cmml">ğ’°</mi><mo id="S3.I1.i2.p1.6.m6.4.4.3" xref="S3.I1.i2.p1.6.m6.4.4.3.cmml">=</mo><mrow id="S3.I1.i2.p1.6.m6.4.4.2" xref="S3.I1.i2.p1.6.m6.4.4.2.cmml"><mrow id="S3.I1.i2.p1.6.m6.3.3.1.1.1" xref="S3.I1.i2.p1.6.m6.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.I1.i2.p1.6.m6.3.3.1.1.1.2" xref="S3.I1.i2.p1.6.m6.3.3.1.1.2.cmml">[</mo><mn id="S3.I1.i2.p1.6.m6.1.1" xref="S3.I1.i2.p1.6.m6.1.1.cmml">0</mn><mo id="S3.I1.i2.p1.6.m6.3.3.1.1.1.3" xref="S3.I1.i2.p1.6.m6.3.3.1.1.2.cmml">,</mo><mrow id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.cmml"><mi id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.2" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.2.cmml">H</mi><mo id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.1" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.1.cmml">âˆ’</mo><mn id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.3" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.I1.i2.p1.6.m6.3.3.1.1.1.4" xref="S3.I1.i2.p1.6.m6.3.3.1.1.2.cmml">]</mo></mrow><mo rspace="0.222em" id="S3.I1.i2.p1.6.m6.4.4.2.3" xref="S3.I1.i2.p1.6.m6.4.4.2.3.cmml">Ã—</mo><mrow id="S3.I1.i2.p1.6.m6.4.4.2.2.1" xref="S3.I1.i2.p1.6.m6.4.4.2.2.2.cmml"><mo stretchy="false" id="S3.I1.i2.p1.6.m6.4.4.2.2.1.2" xref="S3.I1.i2.p1.6.m6.4.4.2.2.2.cmml">[</mo><mn id="S3.I1.i2.p1.6.m6.2.2" xref="S3.I1.i2.p1.6.m6.2.2.cmml">0</mn><mo id="S3.I1.i2.p1.6.m6.4.4.2.2.1.3" xref="S3.I1.i2.p1.6.m6.4.4.2.2.2.cmml">,</mo><mrow id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.cmml"><mi id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.2" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.2.cmml">W</mi><mo id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.1" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.1.cmml">âˆ’</mo><mn id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.3" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.I1.i2.p1.6.m6.4.4.2.2.1.4" xref="S3.I1.i2.p1.6.m6.4.4.2.2.2.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.6.m6.4b"><apply id="S3.I1.i2.p1.6.m6.4.4.cmml" xref="S3.I1.i2.p1.6.m6.4.4"><eq id="S3.I1.i2.p1.6.m6.4.4.3.cmml" xref="S3.I1.i2.p1.6.m6.4.4.3"></eq><ci id="S3.I1.i2.p1.6.m6.4.4.4.cmml" xref="S3.I1.i2.p1.6.m6.4.4.4">ğ’°</ci><apply id="S3.I1.i2.p1.6.m6.4.4.2.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2"><times id="S3.I1.i2.p1.6.m6.4.4.2.3.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2.3"></times><interval closure="closed" id="S3.I1.i2.p1.6.m6.3.3.1.1.2.cmml" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1"><cn type="integer" id="S3.I1.i2.p1.6.m6.1.1.cmml" xref="S3.I1.i2.p1.6.m6.1.1">0</cn><apply id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.cmml" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1"><minus id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.1"></minus><ci id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.2.cmml" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.2">ğ»</ci><cn type="integer" id="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.3.cmml" xref="S3.I1.i2.p1.6.m6.3.3.1.1.1.1.3">1</cn></apply></interval><interval closure="closed" id="S3.I1.i2.p1.6.m6.4.4.2.2.2.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1"><cn type="integer" id="S3.I1.i2.p1.6.m6.2.2.cmml" xref="S3.I1.i2.p1.6.m6.2.2">0</cn><apply id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1"><minus id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.1.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.1"></minus><ci id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.2.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.2">ğ‘Š</ci><cn type="integer" id="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.3.cmml" xref="S3.I1.i2.p1.6.m6.4.4.2.2.1.1.3">1</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.6.m6.4c">\mathcal{U}=[0,H-1]\times[0,W-1]</annotation></semantics></math> be the set of all real-valued pixel coordinates in the depth image.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.4" class="ltx_p"><span id="S3.I1.i3.p1.4.1" class="ltx_text ltx_font_italic">Object Mask:</span> Let <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{M}_{i}\subseteq\mathcal{U}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><msub id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.1.m1.1.1.2.2" xref="S3.I1.i3.p1.1.m1.1.1.2.2.cmml">â„³</mi><mi id="S3.I1.i3.p1.1.m1.1.1.2.3" xref="S3.I1.i3.p1.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.I1.i3.p1.1.m1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.cmml">âŠ†</mo><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">ğ’°</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><subset id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1"></subset><apply id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.2">â„³</ci><ci id="S3.I1.i3.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\mathcal{M}_{i}\subseteq\mathcal{U}</annotation></semantics></math> be a mask for foreground object <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{O}_{i}" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><msub id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">ğ’ª</mi><mi id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2">ğ’ª</ci><ci id="S3.I1.i3.p1.2.m2.1.1.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">\mathcal{O}_{i}</annotation></semantics></math>, or the set of pixels in <math id="S3.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.I1.i3.p1.3.m3.1a"><mi id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><ci id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">\mathbf{y}</annotation></semantics></math> that were generated by the surface of <math id="S3.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{O}_{i}" display="inline"><semantics id="S3.I1.i3.p1.4.m4.1a"><msub id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.4.m4.1.1.2" xref="S3.I1.i3.p1.4.m4.1.1.2.cmml">ğ’ª</mi><mi id="S3.I1.i3.p1.4.m4.1.1.3" xref="S3.I1.i3.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><apply id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.4.m4.1.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.4.m4.1.1.2.cmml" xref="S3.I1.i3.p1.4.m4.1.1.2">ğ’ª</ci><ci id="S3.I1.i3.p1.4.m4.1.1.3.cmml" xref="S3.I1.i3.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">\mathcal{O}_{i}</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p">Every state <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\mathbf{x}</annotation></semantics></math> corresponds to a set of visible foreground object masks : <math id="S3.p3.2.m2.1" class="ltx_math_unparsed" alttext="\mathcal{M}=\{(\mathcal{M}_{i}:\mathcal{M}_{i}\neq\emptyset)\ \forall i\in\{1,\ldots,m\}\}" display="inline"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S3.p3.2.m2.1.1">â„³</mi><mo id="S3.p3.2.m2.1.2">=</mo><mrow id="S3.p3.2.m2.1.3"><mo stretchy="false" id="S3.p3.2.m2.1.3.1">{</mo><mrow id="S3.p3.2.m2.1.3.2"><mo stretchy="false" id="S3.p3.2.m2.1.3.2.1">(</mo><msub id="S3.p3.2.m2.1.3.2.2"><mi class="ltx_font_mathcaligraphic" id="S3.p3.2.m2.1.3.2.2.2">â„³</mi><mi id="S3.p3.2.m2.1.3.2.2.3">i</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S3.p3.2.m2.1.3.2.3">:</mo><msub id="S3.p3.2.m2.1.3.2.4"><mi class="ltx_font_mathcaligraphic" id="S3.p3.2.m2.1.3.2.4.2">â„³</mi><mi id="S3.p3.2.m2.1.3.2.4.3">i</mi></msub><mo id="S3.p3.2.m2.1.3.2.5">â‰ </mo><mi mathvariant="normal" id="S3.p3.2.m2.1.3.2.6">âˆ…</mi><mo rspace="0.667em" stretchy="false" id="S3.p3.2.m2.1.3.2.7">)</mo></mrow><mo rspace="0.167em" id="S3.p3.2.m2.1.3.3">âˆ€</mo><mi id="S3.p3.2.m2.1.3.4">i</mi><mo id="S3.p3.2.m2.1.3.5">âˆˆ</mo><mrow id="S3.p3.2.m2.1.3.6"><mo stretchy="false" id="S3.p3.2.m2.1.3.6.1">{</mo><mn id="S3.p3.2.m2.1.3.6.2">1</mn><mo id="S3.p3.2.m2.1.3.6.3">,</mo><mi mathvariant="normal" id="S3.p3.2.m2.1.3.6.4">â€¦</mi><mo id="S3.p3.2.m2.1.3.6.5">,</mo><mi id="S3.p3.2.m2.1.3.6.6">m</mi><mo stretchy="false" id="S3.p3.2.m2.1.3.6.7">}</mo></mrow><mo stretchy="false" id="S3.p3.2.m2.1.3.7">}</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">\mathcal{M}=\{(\mathcal{M}_{i}:\mathcal{M}_{i}\neq\emptyset)\ \forall i\in\{1,\ldots,m\}\}</annotation></semantics></math>.
The goal of category-agnostic object instance segmentation is to find <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">â„³</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">â„³</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">\mathcal{M}</annotation></semantics></math> given a depth image <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">\mathbf{y}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Synthetic Dataset Generation Method</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.4" class="ltx_p">To efficiently learn category-agnostic instance segmentation, we generate a synthetic training dataset of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">N</annotation></semantics></math> paired depth images and ground truth object masks: <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}=\left\{(\mathbf{y}_{k},\mathcal{M}_{k})\right\}_{k=1}^{N}" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml">ğ’Ÿ</mi><mo id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">=</mo><msubsup id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml"><mrow id="S4.p1.2.m2.1.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.2.cmml"><mo id="S4.p1.2.m2.1.1.1.1.1.1.2" xref="S4.p1.2.m2.1.1.1.1.1.2.cmml">{</mo><mrow id="S4.p1.2.m2.1.1.1.1.1.1.1.2" xref="S4.p1.2.m2.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.p1.2.m2.1.1.1.1.1.1.1.2.3" xref="S4.p1.2.m2.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.2" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.2.cmml">ğ²</mi><mi id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.3" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S4.p1.2.m2.1.1.1.1.1.1.1.2.4" xref="S4.p1.2.m2.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.2" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.2.cmml">â„³</mi><mi id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.3" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S4.p1.2.m2.1.1.1.1.1.1.1.2.5" xref="S4.p1.2.m2.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S4.p1.2.m2.1.1.1.1.1.1.3" xref="S4.p1.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.p1.2.m2.1.1.1.1.3" xref="S4.p1.2.m2.1.1.1.1.3.cmml"><mi id="S4.p1.2.m2.1.1.1.1.3.2" xref="S4.p1.2.m2.1.1.1.1.3.2.cmml">k</mi><mo id="S4.p1.2.m2.1.1.1.1.3.1" xref="S4.p1.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S4.p1.2.m2.1.1.1.1.3.3" xref="S4.p1.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.p1.2.m2.1.1.1.3" xref="S4.p1.2.m2.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><eq id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2"></eq><ci id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3">ğ’Ÿ</ci><apply id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.2.cmml" xref="S4.p1.2.m2.1.1.1">superscript</csymbol><apply id="S4.p1.2.m2.1.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.1.2.cmml" xref="S4.p1.2.m2.1.1.1">subscript</csymbol><set id="S4.p1.2.m2.1.1.1.1.1.2.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1"><interval closure="open" id="S4.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2"><apply id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.2">ğ²</ci><ci id="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.1.1.3">ğ‘˜</ci></apply><apply id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.2">â„³</ci><ci id="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1.2.2.3">ğ‘˜</ci></apply></interval></set><apply id="S4.p1.2.m2.1.1.1.1.3.cmml" xref="S4.p1.2.m2.1.1.1.1.3"><eq id="S4.p1.2.m2.1.1.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.1.1.3.1"></eq><ci id="S4.p1.2.m2.1.1.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.1.1.3.2">ğ‘˜</ci><cn type="integer" id="S4.p1.2.m2.1.1.1.1.3.3.cmml" xref="S4.p1.2.m2.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.p1.2.m2.1.1.1.3.cmml" xref="S4.p1.2.m2.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\mathcal{D}=\left\{(\mathbf{y}_{k},\mathcal{M}_{k})\right\}_{k=1}^{N}</annotation></semantics></math>.
The proposed dataset generation method samples training examples using two distributions: a task-specific state distribution, <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="p(\mathbf{x})" display="inline"><semantics id="S4.p1.3.m3.1a"><mrow id="S4.p1.3.m3.1.2" xref="S4.p1.3.m3.1.2.cmml"><mi id="S4.p1.3.m3.1.2.2" xref="S4.p1.3.m3.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p1.3.m3.1.2.1" xref="S4.p1.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S4.p1.3.m3.1.2.3.2" xref="S4.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S4.p1.3.m3.1.2.3.2.1" xref="S4.p1.3.m3.1.2.cmml">(</mo><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">ğ±</mi><mo stretchy="false" id="S4.p1.3.m3.1.2.3.2.2" xref="S4.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.2.cmml" xref="S4.p1.3.m3.1.2"><times id="S4.p1.3.m3.1.2.1.cmml" xref="S4.p1.3.m3.1.2.1"></times><ci id="S4.p1.3.m3.1.2.2.cmml" xref="S4.p1.3.m3.1.2.2">ğ‘</ci><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ğ±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">p(\mathbf{x})</annotation></semantics></math>, that randomizes over a diverse set of object geometries, object poses, and camera parameters; and an observation distribution, <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="p(\mathbf{y}|\mathbf{x})" display="inline"><semantics id="S4.p1.4.m4.1a"><mrow id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mi id="S4.p1.4.m4.1.1.3" xref="S4.p1.4.m4.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">â€‹</mo><mrow id="S4.p1.4.m4.1.1.1.1" xref="S4.p1.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p1.4.m4.1.1.1.1.2" xref="S4.p1.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S4.p1.4.m4.1.1.1.1.1" xref="S4.p1.4.m4.1.1.1.1.1.cmml"><mi id="S4.p1.4.m4.1.1.1.1.1.2" xref="S4.p1.4.m4.1.1.1.1.1.2.cmml">ğ²</mi><mo fence="false" id="S4.p1.4.m4.1.1.1.1.1.1" xref="S4.p1.4.m4.1.1.1.1.1.1.cmml">|</mo><mi id="S4.p1.4.m4.1.1.1.1.1.3" xref="S4.p1.4.m4.1.1.1.1.1.3.cmml">ğ±</mi></mrow><mo stretchy="false" id="S4.p1.4.m4.1.1.1.1.3" xref="S4.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><times id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2"></times><ci id="S4.p1.4.m4.1.1.3.cmml" xref="S4.p1.4.m4.1.1.3">ğ‘</ci><apply id="S4.p1.4.m4.1.1.1.1.1.cmml" xref="S4.p1.4.m4.1.1.1.1"><csymbol cd="latexml" id="S4.p1.4.m4.1.1.1.1.1.1.cmml" xref="S4.p1.4.m4.1.1.1.1.1.1">conditional</csymbol><ci id="S4.p1.4.m4.1.1.1.1.1.2.cmml" xref="S4.p1.4.m4.1.1.1.1.1.2">ğ²</ci><ci id="S4.p1.4.m4.1.1.1.1.1.3.cmml" xref="S4.p1.4.m4.1.1.1.1.1.3">ğ±</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">p(\mathbf{y}|\mathbf{x})</annotation></semantics></math>, that models sensor operation and noise.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1809.05825/assets/images/segmentation_datasets.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Dataset generation procedure for the WISDOM synthetic dataset. A subset of 3D CAD models from a training dataset of 1,600 objects are dropped into a virtual bin using dynamic simulation with pybullet. A virtual camera captures both a synthetic depth image of the scene and object segmasks based on the pixelwise projection of each unique 3D object. This process is repeated to generate 50,000 images.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">To sample a single datapoint, we first sample a state <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{x}_{k}\sim p(\mathbf{x})" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.2" xref="S4.p2.1.m1.1.2.cmml"><msub id="S4.p2.1.m1.1.2.2" xref="S4.p2.1.m1.1.2.2.cmml"><mi id="S4.p2.1.m1.1.2.2.2" xref="S4.p2.1.m1.1.2.2.2.cmml">ğ±</mi><mi id="S4.p2.1.m1.1.2.2.3" xref="S4.p2.1.m1.1.2.2.3.cmml">k</mi></msub><mo id="S4.p2.1.m1.1.2.1" xref="S4.p2.1.m1.1.2.1.cmml">âˆ¼</mo><mrow id="S4.p2.1.m1.1.2.3" xref="S4.p2.1.m1.1.2.3.cmml"><mi id="S4.p2.1.m1.1.2.3.2" xref="S4.p2.1.m1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.2.3.1" xref="S4.p2.1.m1.1.2.3.1.cmml">â€‹</mo><mrow id="S4.p2.1.m1.1.2.3.3.2" xref="S4.p2.1.m1.1.2.3.cmml"><mo stretchy="false" id="S4.p2.1.m1.1.2.3.3.2.1" xref="S4.p2.1.m1.1.2.3.cmml">(</mo><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">ğ±</mi><mo stretchy="false" id="S4.p2.1.m1.1.2.3.3.2.2" xref="S4.p2.1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.2.cmml" xref="S4.p2.1.m1.1.2"><csymbol cd="latexml" id="S4.p2.1.m1.1.2.1.cmml" xref="S4.p2.1.m1.1.2.1">similar-to</csymbol><apply id="S4.p2.1.m1.1.2.2.cmml" xref="S4.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.2.2.1.cmml" xref="S4.p2.1.m1.1.2.2">subscript</csymbol><ci id="S4.p2.1.m1.1.2.2.2.cmml" xref="S4.p2.1.m1.1.2.2.2">ğ±</ci><ci id="S4.p2.1.m1.1.2.2.3.cmml" xref="S4.p2.1.m1.1.2.2.3">ğ‘˜</ci></apply><apply id="S4.p2.1.m1.1.2.3.cmml" xref="S4.p2.1.m1.1.2.3"><times id="S4.p2.1.m1.1.2.3.1.cmml" xref="S4.p2.1.m1.1.2.3.1"></times><ci id="S4.p2.1.m1.1.2.3.2.cmml" xref="S4.p2.1.m1.1.2.3.2">ğ‘</ci><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">ğ±</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\mathbf{x}_{k}\sim p(\mathbf{x})</annotation></semantics></math> using a dataset of 3D CAD models, dynamic simulation, and domain randomizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> over the object states, camera intrinsic parameters, and camera pose for robust transfer from simulation to reality.
Next, we sample a synthetic depth image <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{y}_{k}\sim p(\mathbf{y}_{k}\mid\mathbf{x}_{k})" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><msub id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml"><mi id="S4.p2.2.m2.1.1.3.2" xref="S4.p2.2.m2.1.1.3.2.cmml">ğ²</mi><mi id="S4.p2.2.m2.1.1.3.3" xref="S4.p2.2.m2.1.1.3.3.cmml">k</mi></msub><mo id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">âˆ¼</mo><mrow id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml"><mi id="S4.p2.2.m2.1.1.1.3" xref="S4.p2.2.m2.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p2.2.m2.1.1.1.2" xref="S4.p2.2.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S4.p2.2.m2.1.1.1.1.1" xref="S4.p2.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p2.2.m2.1.1.1.1.1.2" xref="S4.p2.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p2.2.m2.1.1.1.1.1.1" xref="S4.p2.2.m2.1.1.1.1.1.1.cmml"><msub id="S4.p2.2.m2.1.1.1.1.1.1.2" xref="S4.p2.2.m2.1.1.1.1.1.1.2.cmml"><mi id="S4.p2.2.m2.1.1.1.1.1.1.2.2" xref="S4.p2.2.m2.1.1.1.1.1.1.2.2.cmml">ğ²</mi><mi id="S4.p2.2.m2.1.1.1.1.1.1.2.3" xref="S4.p2.2.m2.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="S4.p2.2.m2.1.1.1.1.1.1.1" xref="S4.p2.2.m2.1.1.1.1.1.1.1.cmml">âˆ£</mo><msub id="S4.p2.2.m2.1.1.1.1.1.1.3" xref="S4.p2.2.m2.1.1.1.1.1.1.3.cmml"><mi id="S4.p2.2.m2.1.1.1.1.1.1.3.2" xref="S4.p2.2.m2.1.1.1.1.1.1.3.2.cmml">ğ±</mi><mi id="S4.p2.2.m2.1.1.1.1.1.1.3.3" xref="S4.p2.2.m2.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S4.p2.2.m2.1.1.1.1.1.3" xref="S4.p2.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">similar-to</csymbol><apply id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.3.1.cmml" xref="S4.p2.2.m2.1.1.3">subscript</csymbol><ci id="S4.p2.2.m2.1.1.3.2.cmml" xref="S4.p2.2.m2.1.1.3.2">ğ²</ci><ci id="S4.p2.2.m2.1.1.3.3.cmml" xref="S4.p2.2.m2.1.1.3.3">ğ‘˜</ci></apply><apply id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"><times id="S4.p2.2.m2.1.1.1.2.cmml" xref="S4.p2.2.m2.1.1.1.2"></times><ci id="S4.p2.2.m2.1.1.1.3.cmml" xref="S4.p2.2.m2.1.1.1.3">ğ‘</ci><apply id="S4.p2.2.m2.1.1.1.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1.1.1"><csymbol cd="latexml" id="S4.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.1">conditional</csymbol><apply id="S4.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.p2.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.2.2">ğ²</ci><ci id="S4.p2.2.m2.1.1.1.1.1.1.2.3.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.2.3">ğ‘˜</ci></apply><apply id="S4.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.1.1.1.3.1.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p2.2.m2.1.1.1.1.1.1.3.2.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.3.2">ğ±</ci><ci id="S4.p2.2.m2.1.1.1.1.1.1.3.3.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1.3.3">ğ‘˜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\mathbf{y}_{k}\sim p(\mathbf{y}_{k}\mid\mathbf{x}_{k})</annotation></semantics></math> using rendering.
Finally, we compute the visible object masks <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{M}_{j}" display="inline"><semantics id="S4.p2.3.m3.1a"><msub id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">â„³</mi><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">â„³</ci><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\mathcal{M}_{j}</annotation></semantics></math> determining the set of pixels in the depth image with a corresponding 3D point on the surface of object <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{O}_{j}" display="inline"><semantics id="S4.p2.4.m4.1a"><msub id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">ğ’ª</mi><mi id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1">subscript</csymbol><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">ğ’ª</ci><ci id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\mathcal{O}_{j}</annotation></semantics></math>.
Specifically, we render a depth image of each object in isolation and add a pixel to the mask if it is within a threshold from the corresponding full-state depth image.</p>
</div>
</section>
<section id="S5" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">WISDOM Dataset</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To test the effectiveness of this method, we generate the Warehouse Instance Segmentation Dataset for Object Manipulation (WISDOM), a hybrid sim/real dataset designed to train and test category-agnostic instance segmentation networks in a robotic bin-picking environment.
WISDOM includes WISDOM-Sim, a large synthetic dataset of depth images generated using the simulation pipeline, and WISDOM-Real, a set of hand-labeled real RGB-D images for evaluating performance in the real world.</p>
</div>
<section id="S5.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">WISDOM-Sim</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">For WISDOM-Sim, we consider an environment for robotic bin picking consisting of a table and a bin full of objects imaged with an overhead depth camera.
In general, <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="p(\mathbf{x})" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.2" xref="S5.SS1.p1.1.m1.1.2.cmml"><mi id="S5.SS1.p1.1.m1.1.2.2" xref="S5.SS1.p1.1.m1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.2.1" xref="S5.SS1.p1.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S5.SS1.p1.1.m1.1.2.3.2" xref="S5.SS1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S5.SS1.p1.1.m1.1.2.3.2.1" xref="S5.SS1.p1.1.m1.1.2.cmml">(</mo><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">ğ±</mi><mo stretchy="false" id="S5.SS1.p1.1.m1.1.2.3.2.2" xref="S5.SS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.2"><times id="S5.SS1.p1.1.m1.1.2.1.cmml" xref="S5.SS1.p1.1.m1.1.2.1"></times><ci id="S5.SS1.p1.1.m1.1.2.2.cmml" xref="S5.SS1.p1.1.m1.1.2.2">ğ‘</ci><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">ğ±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">p(\mathbf{x})</annotation></semantics></math> can be represented as a product over distributions on:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.5" class="ltx_p"><span id="S5.I1.i1.p1.2.2" class="ltx_text ltx_font_italic">Foreground and background object counts (<math id="S5.I1.i1.p1.1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.I1.i1.p1.1.1.m1.1a"><mi id="S5.I1.i1.p1.1.1.m1.1.1" xref="S5.I1.i1.p1.1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.1.m1.1b"><ci id="S5.I1.i1.p1.1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.1.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.1.m1.1c">m</annotation></semantics></math> and <math id="S5.I1.i1.p1.2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.I1.i1.p1.2.2.m2.1a"><mi id="S5.I1.i1.p1.2.2.m2.1.1" xref="S5.I1.i1.p1.2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.2.2.m2.1b"><ci id="S5.I1.i1.p1.2.2.m2.1.1.cmml" xref="S5.I1.i1.p1.2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.2.2.m2.1c">n</annotation></semantics></math>)</span>: We draw <math id="S5.I1.i1.p1.3.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.I1.i1.p1.3.m1.1a"><mi id="S5.I1.i1.p1.3.m1.1.1" xref="S5.I1.i1.p1.3.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.3.m1.1b"><ci id="S5.I1.i1.p1.3.m1.1.1.cmml" xref="S5.I1.i1.p1.3.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.3.m1.1c">m</annotation></semantics></math> from a Poisson distribution with mean <math id="S5.I1.i1.p1.4.m2.1" class="ltx_Math" alttext="\lambda=7.5" display="inline"><semantics id="S5.I1.i1.p1.4.m2.1a"><mrow id="S5.I1.i1.p1.4.m2.1.1" xref="S5.I1.i1.p1.4.m2.1.1.cmml"><mi id="S5.I1.i1.p1.4.m2.1.1.2" xref="S5.I1.i1.p1.4.m2.1.1.2.cmml">Î»</mi><mo id="S5.I1.i1.p1.4.m2.1.1.1" xref="S5.I1.i1.p1.4.m2.1.1.1.cmml">=</mo><mn id="S5.I1.i1.p1.4.m2.1.1.3" xref="S5.I1.i1.p1.4.m2.1.1.3.cmml">7.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.4.m2.1b"><apply id="S5.I1.i1.p1.4.m2.1.1.cmml" xref="S5.I1.i1.p1.4.m2.1.1"><eq id="S5.I1.i1.p1.4.m2.1.1.1.cmml" xref="S5.I1.i1.p1.4.m2.1.1.1"></eq><ci id="S5.I1.i1.p1.4.m2.1.1.2.cmml" xref="S5.I1.i1.p1.4.m2.1.1.2">ğœ†</ci><cn type="float" id="S5.I1.i1.p1.4.m2.1.1.3.cmml" xref="S5.I1.i1.p1.4.m2.1.1.3">7.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.4.m2.1c">\lambda=7.5</annotation></semantics></math>, truncated to a maximum of 10. We set <math id="S5.I1.i1.p1.5.m3.1" class="ltx_Math" alttext="n=2" display="inline"><semantics id="S5.I1.i1.p1.5.m3.1a"><mrow id="S5.I1.i1.p1.5.m3.1.1" xref="S5.I1.i1.p1.5.m3.1.1.cmml"><mi id="S5.I1.i1.p1.5.m3.1.1.2" xref="S5.I1.i1.p1.5.m3.1.1.2.cmml">n</mi><mo id="S5.I1.i1.p1.5.m3.1.1.1" xref="S5.I1.i1.p1.5.m3.1.1.1.cmml">=</mo><mn id="S5.I1.i1.p1.5.m3.1.1.3" xref="S5.I1.i1.p1.5.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.5.m3.1b"><apply id="S5.I1.i1.p1.5.m3.1.1.cmml" xref="S5.I1.i1.p1.5.m3.1.1"><eq id="S5.I1.i1.p1.5.m3.1.1.1.cmml" xref="S5.I1.i1.p1.5.m3.1.1.1"></eq><ci id="S5.I1.i1.p1.5.m3.1.1.2.cmml" xref="S5.I1.i1.p1.5.m3.1.1.2">ğ‘›</ci><cn type="integer" id="S5.I1.i1.p1.5.m3.1.1.3.cmml" xref="S5.I1.i1.p1.5.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.5.m3.1c">n=2</annotation></semantics></math> since we use two fixed background objects: a table and a bin.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Background object states (<math id="S5.I1.i2.p1.1.1.m1.1" class="ltx_Math" alttext="\{\mathcal{B}_{j}\}_{1}^{n}" display="inline"><semantics id="S5.I1.i2.p1.1.1.m1.1a"><msubsup id="S5.I1.i2.p1.1.1.m1.1.1" xref="S5.I1.i2.p1.1.1.m1.1.1.cmml"><mrow id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.2" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.2" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.2.cmml">â„¬</mi><mi id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.3" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.3" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mn id="S5.I1.i2.p1.1.1.m1.1.1.1.3" xref="S5.I1.i2.p1.1.1.m1.1.1.1.3.cmml">1</mn><mi id="S5.I1.i2.p1.1.1.m1.1.1.3" xref="S5.I1.i2.p1.1.1.m1.1.1.3.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.1.m1.1b"><apply id="S5.I1.i2.p1.1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I1.i2.p1.1.1.m1.1.1.2.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1">superscript</csymbol><apply id="S5.I1.i2.p1.1.1.m1.1.1.1.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I1.i2.p1.1.1.m1.1.1.1.2.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1">subscript</csymbol><set id="S5.I1.i2.p1.1.1.m1.1.1.1.1.2.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1"><apply id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.2">â„¬</ci><ci id="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.1.1.1.1.3">ğ‘—</ci></apply></set><cn type="integer" id="S5.I1.i2.p1.1.1.m1.1.1.1.3.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.1.3">1</cn></apply><ci id="S5.I1.i2.p1.1.1.m1.1.1.3.cmml" xref="S5.I1.i2.p1.1.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.1.m1.1c">\{\mathcal{B}_{j}\}_{1}^{n}</annotation></semantics></math>)</span>: We set the geometry and pose of the background objects to fixed values.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.2" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Foreground object states (<math id="S5.I1.i3.p1.1.1.m1.1" class="ltx_Math" alttext="\{\mathcal{O}_{j}\}_{1}^{m}" display="inline"><semantics id="S5.I1.i3.p1.1.1.m1.1a"><msubsup id="S5.I1.i3.p1.1.1.m1.1.1" xref="S5.I1.i3.p1.1.1.m1.1.1.cmml"><mrow id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.2" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.2" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.2.cmml">ğ’ª</mi><mi id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.3" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.3" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mn id="S5.I1.i3.p1.1.1.m1.1.1.1.3" xref="S5.I1.i3.p1.1.1.m1.1.1.1.3.cmml">1</mn><mi id="S5.I1.i3.p1.1.1.m1.1.1.3" xref="S5.I1.i3.p1.1.1.m1.1.1.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.1.m1.1b"><apply id="S5.I1.i3.p1.1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I1.i3.p1.1.1.m1.1.1.2.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1">superscript</csymbol><apply id="S5.I1.i3.p1.1.1.m1.1.1.1.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I1.i3.p1.1.1.m1.1.1.1.2.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1">subscript</csymbol><set id="S5.I1.i3.p1.1.1.m1.1.1.1.1.2.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1"><apply id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.2">ğ’ª</ci><ci id="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.1.1.1.1.3">ğ‘—</ci></apply></set><cn type="integer" id="S5.I1.i3.p1.1.1.m1.1.1.1.3.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.1.3">1</cn></apply><ci id="S5.I1.i3.p1.1.1.m1.1.1.3.cmml" xref="S5.I1.i3.p1.1.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.1.m1.1c">\{\mathcal{O}_{j}\}_{1}^{m}</annotation></semantics></math>)</span>: We sample the <math id="S5.I1.i3.p1.2.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.I1.i3.p1.2.m1.1a"><mi id="S5.I1.i3.p1.2.m1.1.1" xref="S5.I1.i3.p1.2.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.2.m1.1b"><ci id="S5.I1.i3.p1.2.m1.1.1.cmml" xref="S5.I1.i3.p1.2.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.2.m1.1c">m</annotation></semantics></math> foreground objects uniformly from a dataset of 1,664 3D triangular mesh models from Thingiverse, including objects augmented with artificial cardboard backing to mimic common packages. Object poses are sampled by selecting a random pose above the bin from a uniform distribution, dropping each object into the bin one-by-one in pybullet dynamic simulation, and simulating until all objects come to restÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Camera state (<math id="S5.I1.i4.p1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S5.I1.i4.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.I1.i4.p1.1.1.m1.1.1" xref="S5.I1.i4.p1.1.1.m1.1.1.cmml">ğ’</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.1.1.m1.1b"><ci id="S5.I1.i4.p1.1.1.m1.1.1.cmml" xref="S5.I1.i4.p1.1.1.m1.1.1">ğ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.1.1.m1.1c">\mathcal{C}</annotation></semantics></math>)</span>: We sample camera poses uniformly at random from a bounded set of spherical coordinates above the bin. We sample intrinsic parameters uniformly at random from intervals centered on the parameters of a Photoneo PhoXi S industrial depth camera.</p>
</div>
</li>
</ol>
<p id="S5.SS1.p1.2" class="ltx_p">Because the high-resolution depth sensor we use has very little white noise, we fix <math id="S5.SS1.p1.2.m1.1" class="ltx_Math" alttext="p(\mathbf{y}|\mathbf{x})" display="inline"><semantics id="S5.SS1.p1.2.m1.1a"><mrow id="S5.SS1.p1.2.m1.1.1" xref="S5.SS1.p1.2.m1.1.1.cmml"><mi id="S5.SS1.p1.2.m1.1.1.3" xref="S5.SS1.p1.2.m1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.2.m1.1.1.2" xref="S5.SS1.p1.2.m1.1.1.2.cmml">â€‹</mo><mrow id="S5.SS1.p1.2.m1.1.1.1.1" xref="S5.SS1.p1.2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS1.p1.2.m1.1.1.1.1.2" xref="S5.SS1.p1.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS1.p1.2.m1.1.1.1.1.1" xref="S5.SS1.p1.2.m1.1.1.1.1.1.cmml"><mi id="S5.SS1.p1.2.m1.1.1.1.1.1.2" xref="S5.SS1.p1.2.m1.1.1.1.1.1.2.cmml">ğ²</mi><mo fence="false" id="S5.SS1.p1.2.m1.1.1.1.1.1.1" xref="S5.SS1.p1.2.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S5.SS1.p1.2.m1.1.1.1.1.1.3" xref="S5.SS1.p1.2.m1.1.1.1.1.1.3.cmml">ğ±</mi></mrow><mo stretchy="false" id="S5.SS1.p1.2.m1.1.1.1.1.3" xref="S5.SS1.p1.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m1.1b"><apply id="S5.SS1.p1.2.m1.1.1.cmml" xref="S5.SS1.p1.2.m1.1.1"><times id="S5.SS1.p1.2.m1.1.1.2.cmml" xref="S5.SS1.p1.2.m1.1.1.2"></times><ci id="S5.SS1.p1.2.m1.1.1.3.cmml" xref="S5.SS1.p1.2.m1.1.1.3">ğ‘</ci><apply id="S5.SS1.p1.2.m1.1.1.1.1.1.cmml" xref="S5.SS1.p1.2.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS1.p1.2.m1.1.1.1.1.1.1.cmml" xref="S5.SS1.p1.2.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S5.SS1.p1.2.m1.1.1.1.1.1.2.cmml" xref="S5.SS1.p1.2.m1.1.1.1.1.1.2">ğ²</ci><ci id="S5.SS1.p1.2.m1.1.1.1.1.1.3.cmml" xref="S5.SS1.p1.2.m1.1.1.1.1.1.3">ğ±</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m1.1c">p(\mathbf{y}|\mathbf{x})</annotation></semantics></math> to simply perform perspective depth rendering using an OpenGL z-buffer.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We used these distributions to sample a dataset of 50,000 synthetic depth images containing 320,000 individual ground-truth segmasks.
Generating 50k datapoints took approximately 26 hours on a desktop with an Intel i7-6700 3.4 GHz CPU.
The synthetic images are broken into training and validation sets with an 80/20 split, where the split is both on images as well as objects (i.e. no objects appear in both the training and validation sets). The training set has 40,000 images of 1,280 unique objects, while the validation set contains 10,000 images of 320 unique objects.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">WISDOM-Real</span>
</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/1809.05825/assets/images/wisdom-real.jpg" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="374" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Objects included in the WISDOM-Real dataset. 25 objects were used for fine-tuning, while a separate set of 25 were held out for evaluation.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/1809.05825/assets/images/wisdom-real-bins.jpg" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="481" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example bins from the WISDOM-Real test set. The number of objects in each bin was chosen from a Poisson distribution with mean 5, with a minimum of two objects per bin. The highly-varied geometry and occlusions make these bins challenging to segment.</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To evaluate the real-world performance of category-agnostic instance segmentation methods and their ability to generalize to novel objects across different types of depth sensors, we collected a hand-labeled dataset of real RGB-D images.
WISDOM-Real contains a total of 800 hand-labeled RGB-D images of cluttered bins, with 400 from both a high-resolution Photoneo PhoXi industrial sensor (1032x772 with 0.05 mm depth precision) and a low-resolution Primesense Carmine (640x480 with 1 mm depth precision).
Missing depth values were filled in using fast inpaintingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> with an averaging kernel.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The objects in these bins were sampled from a diverse set of 50 novel objects with highly-varied geometry, all of which are commonly found around the home (see Figure <a href="#S5.F3" title="Figure 3 â€£ V-B WISDOM-Real â€£ V WISDOM Dataset â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), and have no corresponding CAD model in WISDOM-Sim.
This set of 50 objects was split randomly into even training and test sets.
The training set is reserved for learning methods that require real training data, while the test set is used to test generalization to novel objects.
We generated 100 bins containing objects from the training set and an additional 300 bins containing objects from the test set.
For each bin, a truncated Poisson distribution (<math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\lambda=5" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">Î»</mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><eq id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></eq><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">ğœ†</ci><cn type="integer" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\lambda=5</annotation></semantics></math>) was used to determine the number of objects to be placed in the bin, and the objects were sampled uniformly at random from the appropriate subset.
The sampled objects were shaken together in a plastic box to randomize their poses and dumped into a large black bin with identical geometry to the bin used in WISDOM-Sim, and once the objects settled, each camera took an RGB-D image from above.
Sample bins are shown in Figure <a href="#S5.F4" title="Figure 4 â€£ V-B WISDOM-Real â€£ V WISDOM Dataset â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">After dataset collection, all images were hand-labeled to identify unique object masks using the same tools used to label the COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
We estimate that labeling the 800 real images took over 35 hours of effort due to time spent collecting on images, labeling object masks, and data cleaning.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Synthetic Depth Mask R-CNN</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">To adapt Mask R-CNN to perform category-agnostic instance segmentation on depth images, we made several modifications:</p>
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">We treat depth images as grayscale images and triplicate the depth values across three channels to match the input size of the original network.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">We reduce the number of classes to two. Each proposed instance mask is classified as background or as a foreground object. Of these, only foreground masks are visualized.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">We modify the network input to zero-pad the 512x384 pixel images in WISDOM-Sim to 512x512 images and set the region proposal network anchor scales and ratios to correspond to the 512x512 image size.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">For efficiency, we swapped out the ResNet 101 backbone with a smaller ResNet 35 backbone.</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p">We set the mean pixel value to be the average pixel value of the simulated dataset.</p>
</div>
</li>
</ol>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Training was based on Matterportâ€™s open-source Keras and TensorFlow implementation of Mask R-CNN from GitHub, which uses a ResNet 101 and FPN backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. This implementation closely follows the original Mask R-CNN paper inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We made the modifications listed above and trained the network on WISDOM-Sim with an 80-20 train-val split for 60 epochs with a learning rate of 0.01, momentum of 0.9, and weight decay of 0.0001 on a Titan X GPU. On our setup, training took approximately 24 hours and a single forward pass took 105 ms (average of 600 trials). We call the final trained network a Synthetic Depth Mask R-CNN (SD Mask R-CNN).</p>
</div>
<figure id="S6.T1" class="ltx_table">
<table id="S6.T1.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T1.4.5.1" class="ltx_tr">
<td id="S6.T1.4.5.1.1" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S6.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">High-Res</td>
<td id="S6.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Low-Res</td>
</tr>
<tr id="S6.T1.4.4" class="ltx_tr">
<th id="S6.T1.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.4.5.1" class="ltx_text"></span> <span id="S6.T1.4.4.5.2" class="ltx_text">
<span id="S6.T1.4.4.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T1.4.4.5.2.1.1" class="ltx_tr">
<span id="S6.T1.4.4.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.4.4.5.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span><span id="S6.T1.4.4.5.3" class="ltx_text"></span></th>
<th id="S6.T1.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S6.T1.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="S6.T1.1.1.1.1.p1.1" class="ltx_p"><span id="S6.T1.1.1.1.1.p1.1.1" class="ltx_text"></span> <span id="S6.T1.1.1.1.1.p1.1.2" class="ltx_text">
<span id="S6.T1.1.1.1.1.p1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T1.1.1.1.1.p1.1.2.1.1" class="ltx_tr">
<span id="S6.T1.1.1.1.1.p1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.1.1.1.1.p1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">AP</span></span></span>
</span></span><span id="S6.T1.1.1.1.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
<th id="S6.T1.2.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.2.2.2.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S6.T1.2.2.2.1.p1" class="ltx_para ltx_noindent">
<span id="S6.T1.2.2.2.1.p1.1" class="ltx_p"><span id="S6.T1.2.2.2.1.p1.1.1" class="ltx_text"></span> <span id="S6.T1.2.2.2.1.p1.1.2" class="ltx_text">
<span id="S6.T1.2.2.2.1.p1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T1.2.2.2.1.p1.1.2.1.1" class="ltx_tr">
<span id="S6.T1.2.2.2.1.p1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.2.2.2.1.p1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">AR</span></span></span>
</span></span><span id="S6.T1.2.2.2.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
<th id="S6.T1.3.3.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.3.3.3.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S6.T1.3.3.3.1.p1" class="ltx_para ltx_noindent">
<span id="S6.T1.3.3.3.1.p1.1" class="ltx_p"><span id="S6.T1.3.3.3.1.p1.1.1" class="ltx_text"></span> <span id="S6.T1.3.3.3.1.p1.1.2" class="ltx_text">
<span id="S6.T1.3.3.3.1.p1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T1.3.3.3.1.p1.1.2.1.1" class="ltx_tr">
<span id="S6.T1.3.3.3.1.p1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.3.3.3.1.p1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">AP</span></span></span>
</span></span><span id="S6.T1.3.3.3.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
<th id="S6.T1.4.4.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.4.4.4.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S6.T1.4.4.4.1.p1" class="ltx_para ltx_noindent">
<span id="S6.T1.4.4.4.1.p1.1" class="ltx_p"><span id="S6.T1.4.4.4.1.p1.1.1" class="ltx_text"></span> <span id="S6.T1.4.4.4.1.p1.1.2" class="ltx_text">
<span id="S6.T1.4.4.4.1.p1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T1.4.4.4.1.p1.1.2.1.1" class="ltx_tr">
<span id="S6.T1.4.4.4.1.p1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T1.4.4.4.1.p1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">AR</span></span></span>
</span></span><span id="S6.T1.4.4.4.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
</tr>
<tr id="S6.T1.4.6.2" class="ltx_tr">
<td id="S6.T1.4.6.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Euclidean Clustering</td>
<td id="S6.T1.4.6.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.6.2.2.1.1" class="ltx_p">0.324</span>
</span>
</td>
<td id="S6.T1.4.6.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.6.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.6.2.3.1.1" class="ltx_p">0.467</span>
</span>
</td>
<td id="S6.T1.4.6.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.6.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.6.2.4.1.1" class="ltx_p">0.183</span>
</span>
</td>
<td id="S6.T1.4.6.2.5" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.6.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.6.2.5.1.1" class="ltx_p">0.317</span>
</span>
</td>
</tr>
<tr id="S6.T1.4.7.3" class="ltx_tr">
<td id="S6.T1.4.7.3.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Region Growing</td>
<td id="S6.T1.4.7.3.2" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.7.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.7.3.2.1.1" class="ltx_p">0.349</span>
</span>
</td>
<td id="S6.T1.4.7.3.3" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.7.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.7.3.3.1.1" class="ltx_p">0.574</span>
</span>
</td>
<td id="S6.T1.4.7.3.4" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.7.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.7.3.4.1.1" class="ltx_p">0.180</span>
</span>
</td>
<td id="S6.T1.4.7.3.5" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.7.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.7.3.5.1.1" class="ltx_p">0.346</span>
</span>
</td>
</tr>
<tr id="S6.T1.4.8.4" class="ltx_tr">
<td id="S6.T1.4.8.4.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">FT Mask R-CNN (Depth)</td>
<td id="S6.T1.4.8.4.2" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.8.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.8.4.2.1.1" class="ltx_p">0.370</span>
</span>
</td>
<td id="S6.T1.4.8.4.3" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.8.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.8.4.3.1.1" class="ltx_p">0.616</span>
</span>
</td>
<td id="S6.T1.4.8.4.4" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.8.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.8.4.4.1.1" class="ltx_p">0.331</span>
</span>
</td>
<td id="S6.T1.4.8.4.5" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.8.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.8.4.5.1.1" class="ltx_p">0.546</span>
</span>
</td>
</tr>
<tr id="S6.T1.4.9.5" class="ltx_tr">
<td id="S6.T1.4.9.5.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">FT Mask R-CNN (Color)</td>
<td id="S6.T1.4.9.5.2" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.9.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.9.5.2.1.1" class="ltx_p">0.384</span>
</span>
</td>
<td id="S6.T1.4.9.5.3" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.9.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.9.5.3.1.1" class="ltx_p">0.608</span>
</span>
</td>
<td id="S6.T1.4.9.5.4" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.9.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.9.5.4.1.1" class="ltx_p"><span id="S6.T1.4.9.5.4.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">0.385</span></span>
</span>
</td>
<td id="S6.T1.4.9.5.5" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.9.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.9.5.5.1.1" class="ltx_p"><span id="S6.T1.4.9.5.5.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">0.613</span></span>
</span>
</td>
</tr>
<tr id="S6.T1.4.10.6" class="ltx_tr">
<td id="S6.T1.4.10.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">SD Mask R-CNN</td>
<td id="S6.T1.4.10.6.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.10.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.10.6.2.1.1" class="ltx_p"><span id="S6.T1.4.10.6.2.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">0.516</span></span>
</span>
</td>
<td id="S6.T1.4.10.6.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.10.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.10.6.3.1.1" class="ltx_p"><span id="S6.T1.4.10.6.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">0.647</span></span>
</span>
</td>
<td id="S6.T1.4.10.6.4" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.10.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.10.6.4.1.1" class="ltx_p">0.356</span>
</span>
</td>
<td id="S6.T1.4.10.6.5" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S6.T1.4.10.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T1.4.10.6.5.1.1" class="ltx_p">0.465</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Average precision and average recall (as defined by COCO benchmarks) on each dataset for each of the methods considered. SD Mask R-CNN is the highest performing method, even against Mask R-CNN pretrained on the COCO dataset and fine-tuned on real color and depth images from WISDOM-Real.</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We compare performance of SD-Mask-R-CNN with several baseline methods for category-agnostic instance segmentation on RGB-D images.</p>
</div>
<section id="S7.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.4.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.5.2" class="ltx_text ltx_font_italic">Baselines</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">We use four baselines: two Point Cloud Library methods and two color-based Mask R-CNNs pre-trained on COCO and fine-tuned on WISDOM-Real images. For fine-tuning, the image shape and dataset-specific parameters such as mean pixel were set based on the dataset being trained on (e.g., either color or depth images).</p>
</div>
<section id="S7.SS1.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S7.SS1.SSS1.4.1.1" class="ltx_text">VII-A</span>1 </span>Point Cloud Library Baselines</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">The Point Cloud Library, an open-source library for processing 3D data, provides several methods for segmenting point cloudsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. We used two of these methods: Euclidean clustering and region-growing segmentation. Euclidean clustering adds points to clusters based on the Euclidean distance between neighboring points. If a point is within a sphere of a set radius from its neighbor, then it is added to the clusterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Region-growing segmentation operates in a similar way to Euclidean clustering, but instead of considering Euclidean distance between neighboring points, it discriminates clusters based on the difference of angle between normal vectors and curvatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We tuned the parameters of each method on the first ten images of the high-res and low-res WISDOM-Real training sets.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S7.SS1.SSS2.4.1.1" class="ltx_text">VII-A</span>2 </span>Fine-Tuned Mask R-CNN Baselines</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">As deep learning baselines, we used two variants of Mask R-CNN, one trained on color images and one trained on depth images triplicated across the color channels. Both these variants were pre-trained on RGB images from the COCO dataset and then fine-tuned using the 100 color or depth images from the WISDOM-Real high-res training set. All images were rescaled and padded to be 512 by 512 pixels, and the depth images were treated as grayscale images. Both implementations were fine-tuned on the 100 images for 10 epochs with a learning rate of 0.001.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.4.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.5.2" class="ltx_text ltx_font_italic">Benchmarks</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">We compare the category-agnostic instance segmentation performance of all methods using the widely-used COCO instance segmentation benchmarksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Of the metrics in the benchmark, we report average precision (AP) over ten IoU thresholds over a range from 0.50 to 0.95 with a step size of 0.05, and we report average recall (AR) given a maximum of 100 detections.
Averaging over several IoU thresholds rewards better localization from detectors, so we report this score as our main benchmark as opposed to simply the average precision for an IoU threshold of 0.50. All scores are for the segmentation mask IoU calculation.</p>
</div>
<figure id="S7.F5" class="ltx_figure"><img src="/html/1809.05825/assets/images/comparison_phoxi_new.jpg" id="S7.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Images from both sensors with ground truth and object masks generated by each method. For the baseline methods, the better performing method was chosen for each scenario. While the baseline methods tend to undersegment (PCL) or oversegment (Fine-Tuned Mask R-CNN), SD Mask R-CNN segments the objects correctly.</figcaption>
</figure>
</section>
<section id="S7.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS3.4.1.1" class="ltx_text">VII-C</span> </span><span id="S7.SS3.5.2" class="ltx_text ltx_font_italic">Performance</span>
</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">We ran each of the methods on three test datasets: 2000 images from the WISDOM-Sim validation set and 300 real test images each from the Primesense and Phoxi cameras. All real test images were rescaled and padded to be 512 by 512 pixels.
The results are shown in TableÂ <a href="#S6.T1" title="TABLE I â€£ VI Synthetic Depth Mask R-CNN â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, and full precision-recall curves for each dataset can be found in the supplemental file.
The SD Mask R-CNN network shows significant improvement over both the PCL baselines and the fine-tuned Mask R-CNN baselines, and is also robust to sensor noise.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">An example of each methodâ€™s performance on each of the real datasets can be seen in FigureÂ <a href="#S7.F5" title="Figure 5 â€£ VII-B Benchmarks â€£ VII Experiments â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
The visualizations suggest that the PCL baselines tend to undersegment the scene and cluster nearby objects as a single object.
The fine-tuned Mask R-CNN implementations separate objects more effectively, but the color implementation may incorrectly predict multiple object segments on different colored pieces of the same object.
In contrast, the SD Mask R-CNN network can group parts of objects that may be slightly discontinuous in depth space, and is agnostic to color.
It is able to segment the scenes with high accuracy despite significant occlusion and variation in shape.
TableÂ <a href="#S6.T1" title="TABLE I â€£ VI Synthetic Depth Mask R-CNN â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> also shows SD Mask R-CNN can perform similarly on low-res Primesense images, suggesting that the network can generalize to other camera intrinsics and poses.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS4.4.1.1" class="ltx_text">VII-D</span> </span><span id="S7.SS4.5.2" class="ltx_text ltx_font_italic">Robotics Application: Instance-Specific Grasping</span>
</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">To demonstrate the usefulness of SD Mask R-CNN in a robotics task, we ran experiments utilizing category-agnostic instance segmentation as the first phase of an instance-specific grasping pipeline.
In this task, the goal is to identify and grasp a particular target object from a bin filled with other distractor objects.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">We randomly selected a subset of ten objects from WISDOM-Realâ€™s test set and trained an instance classification network on ten RGB images of each object from a variety of poses by fine-tuning a VGG-16Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> classifier.
During each iteration, one object at random was chosen as the target and all of the objects were shaken, dumped into the black bin, and imaged with the Photoneo PhoXi depth sensor.
We then segmented the depth image using either SD Mask R-CNN or one of the baseline methods, colorized the mask using the corresponding RGB sensor image, and then labeled each mask with the pre-trained classifier.
The mask with the highest predicted probability of being the target was fed to a Dex-Net 3.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> policy for planning suction cup grasps, and the planned grasp was executed by an ABB YuMi equipped with a suction gripper.
Each iteration was considered a success if the target object was successfully grasped, lifted, and removed from the bin.</p>
</div>
<div id="S7.SS4.p3" class="ltx_para">
<p id="S7.SS4.p3.3" class="ltx_p">We measured performance on 50 iterations of this task each for three instance segmentation methods.
SD Mask R-CNN achieved a success rate of <math id="S7.SS4.p3.1.m1.1" class="ltx_Math" alttext="74\%" display="inline"><semantics id="S7.SS4.p3.1.m1.1a"><mrow id="S7.SS4.p3.1.m1.1.1" xref="S7.SS4.p3.1.m1.1.1.cmml"><mn id="S7.SS4.p3.1.m1.1.1.2" xref="S7.SS4.p3.1.m1.1.1.2.cmml">74</mn><mo id="S7.SS4.p3.1.m1.1.1.1" xref="S7.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS4.p3.1.m1.1b"><apply id="S7.SS4.p3.1.m1.1.1.cmml" xref="S7.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S7.SS4.p3.1.m1.1.1.1.cmml" xref="S7.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S7.SS4.p3.1.m1.1.1.2.cmml" xref="S7.SS4.p3.1.m1.1.1.2">74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p3.1.m1.1c">74\%</annotation></semantics></math>, significantly higher than the PCL Euclidean clustering baseline (<math id="S7.SS4.p3.2.m2.1" class="ltx_Math" alttext="56\%" display="inline"><semantics id="S7.SS4.p3.2.m2.1a"><mrow id="S7.SS4.p3.2.m2.1.1" xref="S7.SS4.p3.2.m2.1.1.cmml"><mn id="S7.SS4.p3.2.m2.1.1.2" xref="S7.SS4.p3.2.m2.1.1.2.cmml">56</mn><mo id="S7.SS4.p3.2.m2.1.1.1" xref="S7.SS4.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS4.p3.2.m2.1b"><apply id="S7.SS4.p3.2.m2.1.1.cmml" xref="S7.SS4.p3.2.m2.1.1"><csymbol cd="latexml" id="S7.SS4.p3.2.m2.1.1.1.cmml" xref="S7.SS4.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S7.SS4.p3.2.m2.1.1.2.cmml" xref="S7.SS4.p3.2.m2.1.1.2">56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p3.2.m2.1c">56\%</annotation></semantics></math>).
Furthermore, SD Mask R-CNN had performance on par with a Mask R-CNN that was fine-tuned on 100 real color images (<math id="S7.SS4.p3.3.m3.1" class="ltx_Math" alttext="78\%" display="inline"><semantics id="S7.SS4.p3.3.m3.1a"><mrow id="S7.SS4.p3.3.m3.1.1" xref="S7.SS4.p3.3.m3.1.1.cmml"><mn id="S7.SS4.p3.3.m3.1.1.2" xref="S7.SS4.p3.3.m3.1.1.2.cmml">78</mn><mo id="S7.SS4.p3.3.m3.1.1.1" xref="S7.SS4.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS4.p3.3.m3.1b"><apply id="S7.SS4.p3.3.m3.1.1.cmml" xref="S7.SS4.p3.3.m3.1.1"><csymbol cd="latexml" id="S7.SS4.p3.3.m3.1.1.1.cmml" xref="S7.SS4.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S7.SS4.p3.3.m3.1.1.2.cmml" xref="S7.SS4.p3.3.m3.1.1.2">78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p3.3.m3.1c">78\%</annotation></semantics></math>).</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Discussion and Future Work</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We presented WISDOM, a dataset of images and object segmentation masks for the warehouse object manipulation environment, images that are currently unavailable in other major segmentation datasets. Training SD Mask R-CNN, an adaptation of Mask R-CNN, on synthetic depth images from WISDOM-Sim enables transfer to real images without expensive hand-labeling, suggesting that depth alone can encode segmentation cues. SD Mask R-CNN outperforms PCL segmentation methods and Mask R-CNN fine-tuned on real color and depth images for the object instance segmentation task, and can be used as part of a successful instance-specific grasping pipeline.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.2" class="ltx_p">FigureÂ <a href="#S8.F6" title="Figure 6 â€£ VIII Discussion and Future Work â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows preliminary results of the effects of training image dataset size and training object dataset size on the performance of SD Mask R-CNN. We trained SD Mask R-CNN on random subsets of the training dataset with sizes <math id="S8.p2.1.m1.4" class="ltx_Math" alttext="\{4k,8k,20k,40k\}" display="inline"><semantics id="S8.p2.1.m1.4a"><mrow id="S8.p2.1.m1.4.4.4" xref="S8.p2.1.m1.4.4.5.cmml"><mo stretchy="false" id="S8.p2.1.m1.4.4.4.5" xref="S8.p2.1.m1.4.4.5.cmml">{</mo><mrow id="S8.p2.1.m1.1.1.1.1" xref="S8.p2.1.m1.1.1.1.1.cmml"><mn id="S8.p2.1.m1.1.1.1.1.2" xref="S8.p2.1.m1.1.1.1.1.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S8.p2.1.m1.1.1.1.1.1" xref="S8.p2.1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="S8.p2.1.m1.1.1.1.1.3" xref="S8.p2.1.m1.1.1.1.1.3.cmml">k</mi></mrow><mo id="S8.p2.1.m1.4.4.4.6" xref="S8.p2.1.m1.4.4.5.cmml">,</mo><mrow id="S8.p2.1.m1.2.2.2.2" xref="S8.p2.1.m1.2.2.2.2.cmml"><mn id="S8.p2.1.m1.2.2.2.2.2" xref="S8.p2.1.m1.2.2.2.2.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S8.p2.1.m1.2.2.2.2.1" xref="S8.p2.1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="S8.p2.1.m1.2.2.2.2.3" xref="S8.p2.1.m1.2.2.2.2.3.cmml">k</mi></mrow><mo id="S8.p2.1.m1.4.4.4.7" xref="S8.p2.1.m1.4.4.5.cmml">,</mo><mrow id="S8.p2.1.m1.3.3.3.3" xref="S8.p2.1.m1.3.3.3.3.cmml"><mn id="S8.p2.1.m1.3.3.3.3.2" xref="S8.p2.1.m1.3.3.3.3.2.cmml">20</mn><mo lspace="0em" rspace="0em" id="S8.p2.1.m1.3.3.3.3.1" xref="S8.p2.1.m1.3.3.3.3.1.cmml">â€‹</mo><mi id="S8.p2.1.m1.3.3.3.3.3" xref="S8.p2.1.m1.3.3.3.3.3.cmml">k</mi></mrow><mo id="S8.p2.1.m1.4.4.4.8" xref="S8.p2.1.m1.4.4.5.cmml">,</mo><mrow id="S8.p2.1.m1.4.4.4.4" xref="S8.p2.1.m1.4.4.4.4.cmml"><mn id="S8.p2.1.m1.4.4.4.4.2" xref="S8.p2.1.m1.4.4.4.4.2.cmml">40</mn><mo lspace="0em" rspace="0em" id="S8.p2.1.m1.4.4.4.4.1" xref="S8.p2.1.m1.4.4.4.4.1.cmml">â€‹</mo><mi id="S8.p2.1.m1.4.4.4.4.3" xref="S8.p2.1.m1.4.4.4.4.3.cmml">k</mi></mrow><mo stretchy="false" id="S8.p2.1.m1.4.4.4.9" xref="S8.p2.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.p2.1.m1.4b"><set id="S8.p2.1.m1.4.4.5.cmml" xref="S8.p2.1.m1.4.4.4"><apply id="S8.p2.1.m1.1.1.1.1.cmml" xref="S8.p2.1.m1.1.1.1.1"><times id="S8.p2.1.m1.1.1.1.1.1.cmml" xref="S8.p2.1.m1.1.1.1.1.1"></times><cn type="integer" id="S8.p2.1.m1.1.1.1.1.2.cmml" xref="S8.p2.1.m1.1.1.1.1.2">4</cn><ci id="S8.p2.1.m1.1.1.1.1.3.cmml" xref="S8.p2.1.m1.1.1.1.1.3">ğ‘˜</ci></apply><apply id="S8.p2.1.m1.2.2.2.2.cmml" xref="S8.p2.1.m1.2.2.2.2"><times id="S8.p2.1.m1.2.2.2.2.1.cmml" xref="S8.p2.1.m1.2.2.2.2.1"></times><cn type="integer" id="S8.p2.1.m1.2.2.2.2.2.cmml" xref="S8.p2.1.m1.2.2.2.2.2">8</cn><ci id="S8.p2.1.m1.2.2.2.2.3.cmml" xref="S8.p2.1.m1.2.2.2.2.3">ğ‘˜</ci></apply><apply id="S8.p2.1.m1.3.3.3.3.cmml" xref="S8.p2.1.m1.3.3.3.3"><times id="S8.p2.1.m1.3.3.3.3.1.cmml" xref="S8.p2.1.m1.3.3.3.3.1"></times><cn type="integer" id="S8.p2.1.m1.3.3.3.3.2.cmml" xref="S8.p2.1.m1.3.3.3.3.2">20</cn><ci id="S8.p2.1.m1.3.3.3.3.3.cmml" xref="S8.p2.1.m1.3.3.3.3.3">ğ‘˜</ci></apply><apply id="S8.p2.1.m1.4.4.4.4.cmml" xref="S8.p2.1.m1.4.4.4.4"><times id="S8.p2.1.m1.4.4.4.4.1.cmml" xref="S8.p2.1.m1.4.4.4.4.1"></times><cn type="integer" id="S8.p2.1.m1.4.4.4.4.2.cmml" xref="S8.p2.1.m1.4.4.4.4.2">40</cn><ci id="S8.p2.1.m1.4.4.4.4.3.cmml" xref="S8.p2.1.m1.4.4.4.4.3">ğ‘˜</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.1.m1.4c">\{4k,8k,20k,40k\}</annotation></semantics></math> and on four <math id="S8.p2.2.m2.1" class="ltx_Math" alttext="40k" display="inline"><semantics id="S8.p2.2.m2.1a"><mrow id="S8.p2.2.m2.1.1" xref="S8.p2.2.m2.1.1.cmml"><mn id="S8.p2.2.m2.1.1.2" xref="S8.p2.2.m2.1.1.2.cmml">40</mn><mo lspace="0em" rspace="0em" id="S8.p2.2.m2.1.1.1" xref="S8.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S8.p2.2.m2.1.1.3" xref="S8.p2.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.p2.2.m2.1b"><apply id="S8.p2.2.m2.1.1.cmml" xref="S8.p2.2.m2.1.1"><times id="S8.p2.2.m2.1.1.1.cmml" xref="S8.p2.2.m2.1.1.1"></times><cn type="integer" id="S8.p2.2.m2.1.1.2.cmml" xref="S8.p2.2.m2.1.1.2">40</cn><ci id="S8.p2.2.m2.1.1.3.cmml" xref="S8.p2.2.m2.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.2.m2.1c">40k</annotation></semantics></math> image datasets containing 100, 400, 800, and 1600 unique objects. Both AP and AR increase with image dataset size and number of unique objects used in training, although the image dataset size appears to have a stronger correlation with performance. These results suggest that performance might continue to improve with orders of magnitude more training data.</p>
</div>
<figure id="S8.F6" class="ltx_figure"><img src="/html/1809.05825/assets/x1.png" id="S8.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Effect of increasing synthetic training dataset size (left) and increasing the number of unique objects (right) on the performance of SD Mask R-CNN on the WISDOM-Real high-res test set. These results suggest that more data could continue to increase performance.</figcaption>
</figure>
</section>
<section id="S9" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Acknowledgments</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p"><span id="S9.p1.1.1" class="ltx_text" style="font-size:80%;">This work was supported in part by a Google Cloud Focused Research Award for the Mechanical Search Project jointly to UC Berkeleyâ€™s AUTOLAB and the Stanford Vision &amp; Learning Lab, in affiliation with the Berkeley AI Research (BAIR) Lab, Berkeley Deep Drive (BDD), the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS â€œPeople and Robotsâ€ (CPAR) Initiative. The Authors were also supported by the Department of Defense (DoD) through the National Defense Science &amp; Engineering Graduate Fellowship (NDSEG) Program, the SAIL-Toyota Research initiative, the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, Scalable Collaborative Human-Robot Learning (SCHooL) Project, the NSF National Robotics Initiative Award 1734633, and in part by donations from Siemens, Google, Amazon Robotics, Toyota Research Institute, Autodesk, ABB, Knapp, Loccioni, Honda, Intel, Comcast, Cisco, Hewlett-Packard and by equipment grants from PhotoNeo, and NVidia. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors. We thank our colleagues who provided helpful feedback, code, and suggestions, in particular Michael Laskey, Vishal Satish, Daniel Seita, and Ajay Tanwani.</span></p>
</div>
</section>
<section id="A1" class="ltx_appendix ltx_indent_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="A1.SS1.4.1.1" class="ltx_text">A-A</span> </span><span id="A1.SS1.5.2" class="ltx_text ltx_font_italic">WISDOM Dataset Statistics</span>
</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">The real dataset has 3849 total instances with an average of 4.8 object instances per image, fewer than the 7.7 instances per image in the Common Objects in Context (COCO) dataset and the 6.5 instances per image in WISDOM-Sim, but more instances per image than both ImageNet and PASCAL VOC (3.0 and 2.3, respectively). Additionally, it has many more instances that are close to, overlapping with, or occluding other instances, thus making it a more representative dataset for tasks such as bin picking in cluttered environments. Since it is designed for manipulation tasks, most of the objects are much smaller in area than in the COCO dataset, which aims to more evenly distribute instance areas. In the WISDOM dataset, instances take up only 2.28% of total image area in the simulated images, 1.60% of the total image area on average for the high-res images, and 0.94% of the total image area for the low-res images. FigureÂ <a href="#A1.F7" title="Figure 7 â€£ A-A WISDOM Dataset Statistics â€£ Appendix A Appendix â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> compares the distributions of these metrics to the COCO dataset.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/1809.05825/assets/images/WISDOM_stats.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Distributions of the instances per image and instance size for the WISDOM dataset, with comparisons to the COCO dataset. Average number of instances are listed in parentheses next to each dataset. The number of instances and relative object size make this dataset more applicable to manipulation tasks.</figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="A1.SS2.4.1.1" class="ltx_text">A-B</span> </span><span id="A1.SS2.5.2" class="ltx_text ltx_font_italic">Precision-Recall Evaluation</span>
</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">We performed additional experiments to analyze the precision-recall performance of SD Mask R-CNN along with the baseline methods for category-agnostic instance segmentation on RGB-D images: RGB object proposalsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, cluster-based geometric segmentation methods from PCLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and Mask R-CNN fine-tuned for instance segmentation from the WISDOM-Real dataset.
We also include a variant of SD Mask R-CNN fine-tuned on real depth images from WISDOM-Real.
We evaluate performance using the widely-used COCO instance segmentation benchmarksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">The RGB object proposal baselines were based on two algorithms: Geodesic Object Proposals (GOP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and Multi-scale Combinatorial Grouping (MCG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. GOP identifies critical level sets in signed geodesic distance transforms of the original color images and generates object proposal masks based on theseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. MCG employs combinatorial grouping of multi-scale segmentation masks and ranks object proposals in the image. For each of these methods, we take the top 100 detections. We then remove masks where less than half of the area of the mask overlaps with the foreground segmask of the image and apply non-max suppression with a threshold of <math id="A1.SS2.p2.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="A1.SS2.p2.1.m1.1a"><mn id="A1.SS2.p2.1.m1.1.1" xref="A1.SS2.p2.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><cn type="float" id="A1.SS2.p2.1.m1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">0.5</annotation></semantics></math> Intersection-over-Union (IoU).</p>
</div>
<div id="A1.SS2.p3" class="ltx_para">
<p id="A1.SS2.p3.1" class="ltx_p">FigureÂ <a href="#A1.F8" title="Figure 8 â€£ A-B Precision-Recall Evaluation â€£ Appendix A Appendix â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows precision-recall curves on three test datasets: 2000 images from the WISDOM-Sim validation set and 300 test images each from the Primesense and Phoxi cameras. Our learning-based method produces a ranked list of regions, that can be operated at different operating points.
Not only does SD Mask-RCNN achieve higher precision at the same operating point than PCL, it is able to achieve a much higher overall recall without any compromise in precision at the cost of only a handful of extra regions.</p>
</div>
<figure id="A1.F8" class="ltx_figure"><img src="/html/1809.05825/assets/x2.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Average Jaccard Index, Average Recall, and Precision-Recall (at IoU = 0.5) curves for each method and the high-res (top row) and low-res (bottom row) dataset, using segmentation metrics. The fine-tuned SD Mask R-CNN implementation outperforms all baselines on both sensors in the WISDOM-real dataset. The precision-recall curves suggest that the dataset contains some hard instances that are unable to be recalled by any method. These instances are likely heavily occluded objects whose masks get merged with the adjacent mask or flat objects that cannot be distinguished from the bottom of the bin.</figcaption>
</figure>
<div id="A1.SS2.p4" class="ltx_para">
<p id="A1.SS2.p4.1" class="ltx_p">We also evaluate the performance of SD Mask R-CNN and several baseline on the WISDOM-Sim test set.</p>
</div>
<figure id="A1.T2" class="ltx_table">
<table id="A1.T2.2" class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T2.2.2" class="ltx_tr">
<th id="A1.T2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.2.3.1" class="ltx_text"></span> <span id="A1.T2.2.2.3.2" class="ltx_text">
<span id="A1.T2.2.2.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T2.2.2.3.2.1.1" class="ltx_tr">
<span id="A1.T2.2.2.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T2.2.2.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span><span id="A1.T2.2.2.3.3" class="ltx_text"></span></th>
<th id="A1.T2.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T2.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="A1.T2.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="A1.T2.1.1.1.1.p1.1" class="ltx_p"><span id="A1.T2.1.1.1.1.p1.1.1" class="ltx_text"></span> <span id="A1.T2.1.1.1.1.p1.1.2" class="ltx_text">
<span id="A1.T2.1.1.1.1.p1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T2.1.1.1.1.p1.1.2.1.1" class="ltx_tr">
<span id="A1.T2.1.1.1.1.p1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T2.1.1.1.1.p1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">AP</span></span></span>
</span></span><span id="A1.T2.1.1.1.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
<th id="A1.T2.2.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T2.2.2.2.1" class="ltx_inline-logical-block ltx_align_top">
<span id="A1.T2.2.2.2.1.p1" class="ltx_para ltx_noindent">
<span id="A1.T2.2.2.2.1.p1.1" class="ltx_p"><span id="A1.T2.2.2.2.1.p1.1.1" class="ltx_text"></span> <span id="A1.T2.2.2.2.1.p1.1.2" class="ltx_text">
<span id="A1.T2.2.2.2.1.p1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T2.2.2.2.1.p1.1.2.1.1" class="ltx_tr">
<span id="A1.T2.2.2.2.1.p1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T2.2.2.2.1.p1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">AR</span></span></span>
</span></span><span id="A1.T2.2.2.2.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T2.2.3.1" class="ltx_tr">
<td id="A1.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Euclidean Clustering</td>
<td id="A1.T2.2.3.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.2.3.1.2.1.1" class="ltx_p">0.161</span>
</span>
</td>
<td id="A1.T2.2.3.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.2.3.1.3.1.1" class="ltx_p">0.252</span>
</span>
</td>
</tr>
<tr id="A1.T2.2.4.2" class="ltx_tr">
<td id="A1.T2.2.4.2.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Region Growing</td>
<td id="A1.T2.2.4.2.2" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.2.4.2.2.1.1" class="ltx_p">0.172</span>
</span>
</td>
<td id="A1.T2.2.4.2.3" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.2.4.2.3.1.1" class="ltx_p">0.274</span>
</span>
</td>
</tr>
<tr id="A1.T2.2.5.3" class="ltx_tr">
<td id="A1.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">SD Mask R-CNN</td>
<td id="A1.T2.2.5.3.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.2.5.3.2.1.1" class="ltx_p"><span id="A1.T2.2.5.3.2.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">0.664</span></span>
</span>
</td>
<td id="A1.T2.2.5.3.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T2.2.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.2.5.3.3.1.1" class="ltx_p"><span id="A1.T2.2.5.3.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">0.748</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Average precision and average recall (as defined by COCO benchmarks) on the WISDOM-Sim dataset for the PCL baselines SD Mask R-CNN.</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix ltx_indent_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details of Instance-Specific Grasping Experiment</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">To evaluate the effectiveness of SD Mask R-CNN in a robotics task, we performed experiments in which we used segmentation as the first phase of an instance-specific grasping pipeline.
In the experiment, an ABB YuMi robot was presented a pile of ten known objects in a bin and instructed to grasp one specific target object from the bin using a suction gripper.
An attempt was considered successful if the robot lifted the target object out of the bin and successfully transported the object to a receptacle.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">One approach to this problem is to collect real images of the items piled in the bin, labeling object masks in each image, and using that data to train or fine-tune a deep neural network for object classification and segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
However, that data collection process is time consuming and must be re-performed for new object sets, and training and fine-tuning a Mask R-CNN can take some time.
Instead, our experimental pipeline uses a class-agnostic instance segmentation method followed by a standard CNN classifier, which is easier to generate training data for and faster to train.</p>
</div>
<figure id="A2.T3" class="ltx_table">
<table id="A2.T3.12" class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T3.3.3" class="ltx_tr">
<th id="A2.T3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.3.3.4.1" class="ltx_text"></span> <span id="A2.T3.3.3.4.2" class="ltx_text">
<span id="A2.T3.3.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.3.3.4.2.1.1" class="ltx_tr">
<span id="A2.T3.3.3.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A2.T3.3.3.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span><span id="A2.T3.3.3.4.3" class="ltx_text"></span></th>
<th id="A2.T3.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A2.T3.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="A2.T3.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="A2.T3.1.1.1.1.p1.1" class="ltx_p"><span id="A2.T3.1.1.1.1.p1.1.2" class="ltx_text"></span> <span id="A2.T3.1.1.1.1.p1.1.1" class="ltx_text">
<span id="A2.T3.1.1.1.1.p1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.1.1.1.1.p1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Success Rate (</span><math id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1a"><mo id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1.1" xref="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.m1.1c">\%</annotation></semantics></math><span id="A2.T3.1.1.1.1.p1.1.1.1.1.1.1.2" class="ltx_text ltx_font_bold">)</span></span></span>
</span></span><span id="A2.T3.1.1.1.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
<th id="A2.T3.2.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A2.T3.2.2.2.1" class="ltx_inline-logical-block ltx_align_top">
<span id="A2.T3.2.2.2.1.p1" class="ltx_para ltx_noindent">
<span id="A2.T3.2.2.2.1.p1.1" class="ltx_p"><span id="A2.T3.2.2.2.1.p1.1.2" class="ltx_text"></span> <span id="A2.T3.2.2.2.1.p1.1.1" class="ltx_text">
<span id="A2.T3.2.2.2.1.p1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.2.2.2.1.p1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Prec. @ 0.5 (</span><math id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1a"><mo id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1.1" xref="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.m1.1c">\%</annotation></semantics></math><span id="A2.T3.2.2.2.1.p1.1.1.1.1.1.1.2" class="ltx_text ltx_font_bold">)</span></span></span>
</span></span><span id="A2.T3.2.2.2.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
<th id="A2.T3.3.3.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A2.T3.3.3.3.1" class="ltx_inline-logical-block ltx_align_top">
<span id="A2.T3.3.3.3.1.p1" class="ltx_para ltx_noindent">
<span id="A2.T3.3.3.3.1.p1.1" class="ltx_p"><span id="A2.T3.3.3.3.1.p1.1.2" class="ltx_text"></span> <span id="A2.T3.3.3.3.1.p1.1.1" class="ltx_text">
<span id="A2.T3.3.3.3.1.p1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.3.3.3.1.p1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1.1" xref="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1b"><ci id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.m1.1c">\#</annotation></semantics></math><span id="A2.T3.3.3.3.1.p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold"> Corr. Targets</span></span></span>
</span></span><span id="A2.T3.3.3.3.1.p1.1.3" class="ltx_text"></span></span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T3.6.6" class="ltx_tr">
<td id="A2.T3.6.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Euclidean Clustering</td>
<td id="A2.T3.4.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.4.4.1.1.1" class="ltx_p"><math id="A2.T3.4.4.1.1.1.m1.1" class="ltx_Math" alttext="56\pm 14" display="inline"><semantics id="A2.T3.4.4.1.1.1.m1.1a"><mrow id="A2.T3.4.4.1.1.1.m1.1.1" xref="A2.T3.4.4.1.1.1.m1.1.1.cmml"><mn id="A2.T3.4.4.1.1.1.m1.1.1.2" xref="A2.T3.4.4.1.1.1.m1.1.1.2.cmml">56</mn><mo id="A2.T3.4.4.1.1.1.m1.1.1.1" xref="A2.T3.4.4.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="A2.T3.4.4.1.1.1.m1.1.1.3" xref="A2.T3.4.4.1.1.1.m1.1.1.3.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.1.1.1.m1.1b"><apply id="A2.T3.4.4.1.1.1.m1.1.1.cmml" xref="A2.T3.4.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.4.4.1.1.1.m1.1.1.1.cmml" xref="A2.T3.4.4.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="A2.T3.4.4.1.1.1.m1.1.1.2.cmml" xref="A2.T3.4.4.1.1.1.m1.1.1.2">56</cn><cn type="integer" id="A2.T3.4.4.1.1.1.m1.1.1.3.cmml" xref="A2.T3.4.4.1.1.1.m1.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.1.1.1.m1.1c">56\pm 14</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T3.5.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.5.5.2.1.1" class="ltx_p"><math id="A2.T3.5.5.2.1.1.m1.1" class="ltx_Math" alttext="63\pm 19" display="inline"><semantics id="A2.T3.5.5.2.1.1.m1.1a"><mrow id="A2.T3.5.5.2.1.1.m1.1.1" xref="A2.T3.5.5.2.1.1.m1.1.1.cmml"><mn id="A2.T3.5.5.2.1.1.m1.1.1.2" xref="A2.T3.5.5.2.1.1.m1.1.1.2.cmml">63</mn><mo id="A2.T3.5.5.2.1.1.m1.1.1.1" xref="A2.T3.5.5.2.1.1.m1.1.1.1.cmml">Â±</mo><mn id="A2.T3.5.5.2.1.1.m1.1.1.3" xref="A2.T3.5.5.2.1.1.m1.1.1.3.cmml">19</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.5.5.2.1.1.m1.1b"><apply id="A2.T3.5.5.2.1.1.m1.1.1.cmml" xref="A2.T3.5.5.2.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.5.5.2.1.1.m1.1.1.1.cmml" xref="A2.T3.5.5.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="A2.T3.5.5.2.1.1.m1.1.1.2.cmml" xref="A2.T3.5.5.2.1.1.m1.1.1.2">63</cn><cn type="integer" id="A2.T3.5.5.2.1.1.m1.1.1.3.cmml" xref="A2.T3.5.5.2.1.1.m1.1.1.3">19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.5.5.2.1.1.m1.1c">63\pm 19</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T3.6.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.6.6.3.1.1" class="ltx_p"><math id="A2.T3.6.6.3.1.1.m1.1" class="ltx_Math" alttext="35" display="inline"><semantics id="A2.T3.6.6.3.1.1.m1.1a"><mn id="A2.T3.6.6.3.1.1.m1.1.1" xref="A2.T3.6.6.3.1.1.m1.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="A2.T3.6.6.3.1.1.m1.1b"><cn type="integer" id="A2.T3.6.6.3.1.1.m1.1.1.cmml" xref="A2.T3.6.6.3.1.1.m1.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.6.3.1.1.m1.1c">35</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="A2.T3.9.9" class="ltx_tr">
<td id="A2.T3.9.9.4" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Fine-Tuned Mask R-CNN (Color)</td>
<td id="A2.T3.7.7.1" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.7.7.1.1.1" class="ltx_p"><math id="A2.T3.7.7.1.1.1.m1.1" class="ltx_Math" alttext="78\pm 11" display="inline"><semantics id="A2.T3.7.7.1.1.1.m1.1a"><mrow id="A2.T3.7.7.1.1.1.m1.1.1" xref="A2.T3.7.7.1.1.1.m1.1.1.cmml"><mn id="A2.T3.7.7.1.1.1.m1.1.1.2" xref="A2.T3.7.7.1.1.1.m1.1.1.2.cmml">78</mn><mo id="A2.T3.7.7.1.1.1.m1.1.1.1" xref="A2.T3.7.7.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="A2.T3.7.7.1.1.1.m1.1.1.3" xref="A2.T3.7.7.1.1.1.m1.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.7.7.1.1.1.m1.1b"><apply id="A2.T3.7.7.1.1.1.m1.1.1.cmml" xref="A2.T3.7.7.1.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.7.7.1.1.1.m1.1.1.1.cmml" xref="A2.T3.7.7.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="A2.T3.7.7.1.1.1.m1.1.1.2.cmml" xref="A2.T3.7.7.1.1.1.m1.1.1.2">78</cn><cn type="integer" id="A2.T3.7.7.1.1.1.m1.1.1.3.cmml" xref="A2.T3.7.7.1.1.1.m1.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.7.7.1.1.1.m1.1c">78\pm 11</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T3.8.8.2" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.8.8.2.1.1" class="ltx_p"><math id="A2.T3.8.8.2.1.1.m1.1" class="ltx_Math" alttext="85\pm 12" display="inline"><semantics id="A2.T3.8.8.2.1.1.m1.1a"><mrow id="A2.T3.8.8.2.1.1.m1.1.1" xref="A2.T3.8.8.2.1.1.m1.1.1.cmml"><mn id="A2.T3.8.8.2.1.1.m1.1.1.2" xref="A2.T3.8.8.2.1.1.m1.1.1.2.cmml">85</mn><mo id="A2.T3.8.8.2.1.1.m1.1.1.1" xref="A2.T3.8.8.2.1.1.m1.1.1.1.cmml">Â±</mo><mn id="A2.T3.8.8.2.1.1.m1.1.1.3" xref="A2.T3.8.8.2.1.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.8.8.2.1.1.m1.1b"><apply id="A2.T3.8.8.2.1.1.m1.1.1.cmml" xref="A2.T3.8.8.2.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.8.8.2.1.1.m1.1.1.1.cmml" xref="A2.T3.8.8.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="A2.T3.8.8.2.1.1.m1.1.1.2.cmml" xref="A2.T3.8.8.2.1.1.m1.1.1.2">85</cn><cn type="integer" id="A2.T3.8.8.2.1.1.m1.1.1.3.cmml" xref="A2.T3.8.8.2.1.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.8.8.2.1.1.m1.1c">85\pm 12</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T3.9.9.3" class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.9.9.3.1.1" class="ltx_p"><math id="A2.T3.9.9.3.1.1.m1.1" class="ltx_Math" alttext="44" display="inline"><semantics id="A2.T3.9.9.3.1.1.m1.1a"><mn id="A2.T3.9.9.3.1.1.m1.1.1" xref="A2.T3.9.9.3.1.1.m1.1.1.cmml">44</mn><annotation-xml encoding="MathML-Content" id="A2.T3.9.9.3.1.1.m1.1b"><cn type="integer" id="A2.T3.9.9.3.1.1.m1.1.1.cmml" xref="A2.T3.9.9.3.1.1.m1.1.1">44</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.9.9.3.1.1.m1.1c">44</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="A2.T3.12.12" class="ltx_tr">
<td id="A2.T3.12.12.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">SD Mask R-CNN</td>
<td id="A2.T3.10.10.1" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.10.10.1.1.1" class="ltx_p"><math id="A2.T3.10.10.1.1.1.m1.1" class="ltx_Math" alttext="74\pm 12" display="inline"><semantics id="A2.T3.10.10.1.1.1.m1.1a"><mrow id="A2.T3.10.10.1.1.1.m1.1.1" xref="A2.T3.10.10.1.1.1.m1.1.1.cmml"><mn id="A2.T3.10.10.1.1.1.m1.1.1.2" xref="A2.T3.10.10.1.1.1.m1.1.1.2.cmml">74</mn><mo id="A2.T3.10.10.1.1.1.m1.1.1.1" xref="A2.T3.10.10.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="A2.T3.10.10.1.1.1.m1.1.1.3" xref="A2.T3.10.10.1.1.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.10.10.1.1.1.m1.1b"><apply id="A2.T3.10.10.1.1.1.m1.1.1.cmml" xref="A2.T3.10.10.1.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.10.10.1.1.1.m1.1.1.1.cmml" xref="A2.T3.10.10.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="A2.T3.10.10.1.1.1.m1.1.1.2.cmml" xref="A2.T3.10.10.1.1.1.m1.1.1.2">74</cn><cn type="integer" id="A2.T3.10.10.1.1.1.m1.1.1.3.cmml" xref="A2.T3.10.10.1.1.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.10.10.1.1.1.m1.1c">74\pm 12</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T3.11.11.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.11.11.2.1.1" class="ltx_p"><math id="A2.T3.11.11.2.1.1.m1.1" class="ltx_Math" alttext="87\pm 11" display="inline"><semantics id="A2.T3.11.11.2.1.1.m1.1a"><mrow id="A2.T3.11.11.2.1.1.m1.1.1" xref="A2.T3.11.11.2.1.1.m1.1.1.cmml"><mn id="A2.T3.11.11.2.1.1.m1.1.1.2" xref="A2.T3.11.11.2.1.1.m1.1.1.2.cmml">87</mn><mo id="A2.T3.11.11.2.1.1.m1.1.1.1" xref="A2.T3.11.11.2.1.1.m1.1.1.1.cmml">Â±</mo><mn id="A2.T3.11.11.2.1.1.m1.1.1.3" xref="A2.T3.11.11.2.1.1.m1.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.11.11.2.1.1.m1.1b"><apply id="A2.T3.11.11.2.1.1.m1.1.1.cmml" xref="A2.T3.11.11.2.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.11.11.2.1.1.m1.1.1.1.cmml" xref="A2.T3.11.11.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="A2.T3.11.11.2.1.1.m1.1.1.2.cmml" xref="A2.T3.11.11.2.1.1.m1.1.1.2">87</cn><cn type="integer" id="A2.T3.11.11.2.1.1.m1.1.1.3.cmml" xref="A2.T3.11.11.2.1.1.m1.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.11.11.2.1.1.m1.1c">87\pm 11</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T3.12.12.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A2.T3.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T3.12.12.3.1.1" class="ltx_p"><math id="A2.T3.12.12.3.1.1.m1.1" class="ltx_Math" alttext="39" display="inline"><semantics id="A2.T3.12.12.3.1.1.m1.1a"><mn id="A2.T3.12.12.3.1.1.m1.1.1" xref="A2.T3.12.12.3.1.1.m1.1.1.cmml">39</mn><annotation-xml encoding="MathML-Content" id="A2.T3.12.12.3.1.1.m1.1b"><cn type="integer" id="A2.T3.12.12.3.1.1.m1.1.1.cmml" xref="A2.T3.12.12.3.1.1.m1.1.1">39</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.12.12.3.1.1.m1.1c">39</annotation></semantics></math></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results of semantic segmentation experiments, where success is defined as grasping and lifting the correct object. <span id="A2.T3.18.1" class="ltx_text ltx_font_bold">(Success Rate)</span> Number of successful grasps of the correct object over 50 trials. <span id="A2.T3.19.2" class="ltx_text ltx_font_bold">(Prec. @ 0.5)</span> Success rate when the classifier was <math id="A2.T3.14.m1.1" class="ltx_Math" alttext="&gt;50\%" display="inline"><semantics id="A2.T3.14.m1.1b"><mrow id="A2.T3.14.m1.1.1" xref="A2.T3.14.m1.1.1.cmml"><mi id="A2.T3.14.m1.1.1.2" xref="A2.T3.14.m1.1.1.2.cmml"></mi><mo id="A2.T3.14.m1.1.1.1" xref="A2.T3.14.m1.1.1.1.cmml">&gt;</mo><mrow id="A2.T3.14.m1.1.1.3" xref="A2.T3.14.m1.1.1.3.cmml"><mn id="A2.T3.14.m1.1.1.3.2" xref="A2.T3.14.m1.1.1.3.2.cmml">50</mn><mo id="A2.T3.14.m1.1.1.3.1" xref="A2.T3.14.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.14.m1.1c"><apply id="A2.T3.14.m1.1.1.cmml" xref="A2.T3.14.m1.1.1"><gt id="A2.T3.14.m1.1.1.1.cmml" xref="A2.T3.14.m1.1.1.1"></gt><csymbol cd="latexml" id="A2.T3.14.m1.1.1.2.cmml" xref="A2.T3.14.m1.1.1.2">absent</csymbol><apply id="A2.T3.14.m1.1.1.3.cmml" xref="A2.T3.14.m1.1.1.3"><csymbol cd="latexml" id="A2.T3.14.m1.1.1.3.1.cmml" xref="A2.T3.14.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A2.T3.14.m1.1.1.3.2.cmml" xref="A2.T3.14.m1.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.14.m1.1d">&gt;50\%</annotation></semantics></math> certain that the selected segment was the target object. <span id="A2.T3.20.3" class="ltx_text ltx_font_bold">(# Corr. Targets)</span> Number of times the robot targeted the correct object out of 50 trials.</figcaption>
</figure>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">To train the classifier, we collected ten RGB images of each target object in isolation.
Each image was masked and cropped automatically using depth data, and then each crop was augmented by randomly masking the crop with overlaid planes to simulate occlusions.
From the initial set of 100 images, we produced a dataset of 1,000 images with an 80-20 train-validation split.
We then used this dataset to fine-tune the last four layers of a VGG-16 networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> pre-trained on Imagenet.
Fine-tuning the network for 20 epochs took less than two minutes on a Titan X GPU, and the only human intervention required
was capturing the initial object images.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">Given a pre-trained classifier, the procedure used to execute instance-specific suction grasps was composed of three phases.
First, an RGB-D image was taken of the bin with the PhoXi and a class-agnostic instance segmentation method is used to detect object masks.
Then, the classifier was used to choose the mask that is most likely to belong to the target object.
Finally, a suction grasp was planned and executed by constraining grasps planned by Dex-Net 3.0 to the target object maskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="A2.p5" class="ltx_para">
<p id="A2.p5.1" class="ltx_p">We benchmarked SD Mask R-CNN on this pipeline against two segmentation methods.
First, we compared against the PCL Euclidean Clustering method to evaluate baseline performance.
Second, we compared with Mask R-CNN fine-tuned on the WISDOM-Real training dataset to evaluate whether SD Mask R-CNN is competitive with methods trained on real data.</p>
</div>
<div id="A2.p6" class="ltx_para">
<p id="A2.p6.1" class="ltx_p">Each segmentation method was tested in 50 independent trials.
Each trial involved shaking the ten objects in a box, pouring them into the bin, and allowing the system to select a target object uniformly at random to attempt to locate and grasp.
The results of these instance-specific grasping experiments are shown in Table <a href="#A2.T3" title="TABLE III â€£ Appendix B Details of Instance-Specific Grasping Experiment â€£ Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="A2.p7" class="ltx_para">
<p id="A2.p7.1" class="ltx_p">SD Mask R-CNN outperforms the PCL baseline and achieves performance on par with Mask R-CNN fine-tuned on real data, despite the fact that SD Mask R-CNN was training on only synthetic data.
This suggests that high-quality instance segmentation can be achieved without expensive data collection from humans or self-supervision.
This could significantly reduce the effort needed to take advantage of object segmentation for new robotic tasks.

</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahler etÂ al. [2017a]</span>
<span class="ltx_bibblock">
J.Â Mahler, J.Â Liang, S.Â Niyaz, M.Â Laskey, R.Â Doan, X.Â Liu, J.Â A. Ojea, and
K.Â Goldberg, â€œDex-net 2.0: Deep learning to plan robust grasps with
synthetic point clouds and analytic grasp metrics,â€ <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. Robotics:
Science and Systems (RSS)</em>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ten Pas and Platt [2018]</span>
<span class="ltx_bibblock">
A.Â ten Pas and R.Â Platt, â€œUsing geometry to detect grasp poses in 3d point
clouds,â€ in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Robotics Research</em>.Â Â Â Springer, 2018, pp. 307â€“324.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ArbelÃ¡ez etÂ al. [2014]</span>
<span class="ltx_bibblock">
P.Â ArbelÃ¡ez, J.Â Pont-Tuset, J.Â T. Barron, F.Â Marques, and J.Â Malik,
â€œMultiscale combinatorial grouping,â€ in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR)</em>, 2014, pp. 328â€“335.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KrÃ¤henbÃ¼hl and Koltun [2014]</span>
<span class="ltx_bibblock">
P.Â KrÃ¤henbÃ¼hl and V.Â Koltun, â€œGeodesic object proposals,â€ in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.Â Â Â Springer, 2014, pp. 725â€“739.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long etÂ al. [2015]</span>
<span class="ltx_bibblock">
J.Â Long, E.Â Shelhamer, and T.Â Darrell, â€œFully convolutional networks for
semantic segmentation,â€ in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR)</em>, 2015, pp. 3431â€“3440.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. [2017]</span>
<span class="ltx_bibblock">
K.Â He, G.Â Gkioxari, P.Â DollÃ¡r, and R.Â Girshick, â€œMask r-cnn. arxiv
preprint arxiv: 170306870,â€ 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. [2014]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.Â Maire, S.Â Belongie, J.Â Hays, P.Â Perona, D.Â Ramanan,
P.Â DollÃ¡r, and C.Â L. Zitnick, â€œMicrosoft coco: Common objects in
context,â€ in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.Â Â Â Springer, 2014, pp. 740â€“755.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang etÂ al. [2017]</span>
<span class="ltx_bibblock">
E.Â Jang, S.Â Vijaynarasimhan, P.Â Pastor, J.Â Ibarz, and S.Â Levine, â€œEnd-to-end
learning of semantic grasping,â€ <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Conf. on Robot Learning (CoRL)</em>, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. [2017]</span>
<span class="ltx_bibblock">
A.Â Zeng, K.-T. Yu, S.Â Song, D.Â Suo, E.Â Walker, A.Â Rodriguez, and J.Â Xiao,
â€œMulti-view self-supervised deep learning for 6d pose estimation in the
amazon picking challenge,â€ in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Robotics and
Automation (ICRA)</em>.Â Â Â IEEE, 2017, pp.
1386â€“1383.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morrison etÂ al. [2017]</span>
<span class="ltx_bibblock">
D.Â Morrison, A.Â Tow, M.Â McTaggart, R.Â Smith, N.Â Kelly-Boxall, S.Â Wade-McCue,
J.Â Erskine, R.Â Grinover, A.Â Gurman, T.Â Hunn <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œCartman: The
low-cost cartesian manipulator that won the amazon robotics challenge,â€
<em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1709.06283</em>, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwarz etÂ al. [2017]</span>
<span class="ltx_bibblock">
M.Â Schwarz, A.Â Milan, C.Â Lenz, A.Â Munoz, A.Â S. Periyasamy, M.Â Schreiber,
S.Â SchÃ¼ller, and S.Â Behnke, â€œNimbro picking: Versatile part handling for
warehouse automation,â€ in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Robotics and
Automation (ICRA)</em>.Â Â Â IEEE, 2017, pp.
3032â€“3039.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jonschkowski etÂ al. [2016]</span>
<span class="ltx_bibblock">
R.Â Jonschkowski, C.Â Eppner, S.Â HÃ¶fer, R.Â MartÃ­n-MartÃ­n, and
O.Â Brock, â€œProbabilistic multi-class segmentation for the amazon picking
challenge,â€ in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE/RSJ Int. Conf. on Intelligent Robots and
Systems (IROS)</em>.Â Â Â IEEE, 2016, pp. 1â€“7.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson-Roberson etÂ al. [2017]</span>
<span class="ltx_bibblock">
M.Â Johnson-Roberson, C.Â Barto, R.Â Mehta, S.Â N. Sridhar, K.Â Rosaen, and
R.Â Vasudevan, â€œDriving in the matrix: Can virtual worlds replace
human-generated annotations for real world tasks?â€ in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE
Int. Conf. Robotics and Automation (ICRA)</em>.Â Â Â IEEE, 2017, pp. 746â€“753.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahler etÂ al. [2017b]</span>
<span class="ltx_bibblock">
J.Â Mahler, M.Â Matl, X.Â Liu, A.Â Li, D.Â Gealy, and K.Â Goldberg, â€œDex-net 3.0:
Computing robust robot suction grasp targets in point clouds using a new
analytic model and deep learning,â€ <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Robotics
and Automation (ICRA)</em>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ros etÂ al. [2016]</span>
<span class="ltx_bibblock">
G.Â Ros, L.Â Sellart, J.Â Materzynska, D.Â Vazquez, and A.Â M. Lopez, â€œThe synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes,â€ in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR)</em>, 2016, pp. 3234â€“3243.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao etÂ al. [2018]</span>
<span class="ltx_bibblock">
L.Â Shao, Y.Â Tian, and J.Â Bohg, â€œClusternet: Instance segmentation in rgb-d
images,â€ <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.08894</em>, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobin etÂ al. [2017]</span>
<span class="ltx_bibblock">
J.Â Tobin, R.Â Fong, A.Â Ray, J.Â Schneider, W.Â Zaremba, and P.Â Abbeel, â€œDomain
randomization for transferring deep neural networks from simulation to the
real world,â€ in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE/RSJ Int. Conf. on Intelligent Robots and
Systems (IROS)</em>.Â Â Â IEEE, 2017, pp.
23â€“30.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexe etÂ al. [2012]</span>
<span class="ltx_bibblock">
B.Â Alexe, T.Â Deselaers, and V.Â Ferrari, â€œMeasuring the objectness of image
windows,â€ <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, vol.Â 34, no.Â 11, pp. 2189â€“2202, 2012.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Endres and Hoiem [2010]</span>
<span class="ltx_bibblock">
I.Â Endres and D.Â Hoiem, â€œCategory independent object proposals,â€ in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.Â Â Â Springer, 2010, pp. 575â€“588.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ de Sande etÂ al. [2011]</span>
<span class="ltx_bibblock">
K.Â E. VanÂ de Sande, J.Â R. Uijlings, T.Â Gevers, and A.Â W. Smeulders,
â€œSegmentation as selective search for object recognition,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc.
IEEE Int. Conf. on Computer Vision (ICCV)</em>.Â Â Â IEEE, 2011, pp. 1879â€“1886.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira etÂ al. [2012]</span>
<span class="ltx_bibblock">
J.Â Carreira, R.Â Caseiro, J.Â Batista, and C.Â Sminchisescu, â€œSemantic
segmentation with second-order pooling,â€ in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">European Conference on
Computer Vision</em>.Â Â Â Springer, 2012, pp.
430â€“443.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuo etÂ al. [2015]</span>
<span class="ltx_bibblock">
W.Â Kuo, B.Â Hariharan, and J.Â Malik, â€œDeepbox: Learning objectness with
convolutional networks,â€ in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Computer Vision
(ICCV)</em>, 2015, pp. 2479â€“2487.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. [2015]</span>
<span class="ltx_bibblock">
S.Â Ren, K.Â He, R.Â Girshick, and J.Â Sun, â€œFaster r-cnn: Towards real-time
object detection with region proposal networks,â€ in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, 2015, pp. 91â€“99.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinheiro etÂ al. [2015]</span>
<span class="ltx_bibblock">
P.Â O. Pinheiro, R.Â Collobert, and P.Â DollÃ¡r, â€œLearning to segment object
candidates,â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
2015, pp. 1990â€“1998.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinheiro etÂ al. [2016]</span>
<span class="ltx_bibblock">
P.Â O. Pinheiro, T.-Y. Lin, R.Â Collobert, and P.Â DollÃ¡r, â€œLearning to
refine object segments,â€ in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.Â Â Â Springer, 2016, pp. 75â€“91.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hariharan etÂ al. [2015]</span>
<span class="ltx_bibblock">
B.Â Hariharan, P.Â ArbelÃ¡ez, R.Â Girshick, and J.Â Malik, â€œHypercolumns for
object segmentation and fine-grained localization,â€ in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, 2015, pp. 447â€“456.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu [2010]</span>
<span class="ltx_bibblock">
R.Â B. Rusu, â€œSemantic 3d object maps for everyday manipulation in human living
environments,â€ <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">KI-KÃ¼nstliche Intelligenz</em>, vol.Â 24, no.Â 4, pp.
345â€“348, 2010.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu and Cousins [2011]</span>
<span class="ltx_bibblock">
R.Â B. Rusu and S.Â Cousins, â€œ3d is here: Point cloud library (pcl),â€ in
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</em>.Â Â Â IEEE, 2011, pp. 1â€“4.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vo etÂ al. [2015]</span>
<span class="ltx_bibblock">
A.-V. Vo, L.Â Truong-Hong, D.Â F. Laefer, and M.Â Bertolotto, â€œOctree-based
region growing for point cloud segmentation,â€ <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ISPRS Journal of
Photogrammetry and Remote Sensing</em>, vol. 104, pp. 88â€“100, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabbani etÂ al. [2006]</span>
<span class="ltx_bibblock">
T.Â Rabbani, F.Â Van DenÂ Heuvel, and G.Â Vosselmann, â€œSegmentation of point
clouds using smoothness constraint,â€ <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">International archives of
photogrammetry, remote sensing and spatial information sciences</em>, vol.Â 36,
no.Â 5, pp. 248â€“253, 2006.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. [2013]</span>
<span class="ltx_bibblock">
S.Â Gupta, P.Â Arbelaez, and J.Â Malik, â€œPerceptual organization and recognition
of indoor scenes from rgb-d images,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR)</em>.Â Â Â IEEE, 2013, pp. 564â€“571.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. [2014]</span>
<span class="ltx_bibblock">
S.Â Gupta, R.Â Girshick, P.Â ArbelÃ¡ez, and J.Â Malik, â€œLearning rich features
from rgb-d images for object detection and segmentation,â€ in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">European
Conference on Computer Vision</em>.Â Â Â Springer, 2014, pp. 345â€“360.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2018]</span>
<span class="ltx_bibblock">
X.Â Chen, K.Â Kundu, Y.Â Zhu, H.Â Ma, S.Â Fidler, and R.Â Urtasun, â€œ3d object
proposals using stereo imagery for accurate object class detection,â€
<em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>,
vol.Â 40, no.Â 5, pp. 1259â€“1272, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia-Garcia etÂ al. [2017]</span>
<span class="ltx_bibblock">
A.Â Garcia-Garcia, S.Â Orts-Escolano, S.Â Oprea, V.Â Villena-Martinez, and
J.Â Garcia-Rodriguez, â€œA review on deep learning techniques applied to
semantic segmentation,â€ <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.06857</em>, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. [2017]</span>
<span class="ltx_bibblock">
G.Â Lin, A.Â Milan, C.Â Shen, and I.Â Reid, â€œRefinenet: Multi-path refinement
networks for high-resolution semantic segmentation,â€ in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hariharan etÂ al. [2014]</span>
<span class="ltx_bibblock">
B.Â Hariharan, P.Â ArbelÃ¡ez, R.Â Girshick, and J.Â Malik, â€œSimultaneous
detection and segmentation,â€ in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.Â Â Â Springer, 2014, pp. 297â€“312.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi etÂ al. [2017]</span>
<span class="ltx_bibblock">
C.Â R. Qi, H.Â Su, K.Â Mo, and L.Â J. Guibas, â€œPointnet: Deep learning on point
sets for 3d classification and segmentation,â€ <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR)</em>, vol.Â 1, no.Â 2, p.Â 4, 2017.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. [2017]</span>
<span class="ltx_bibblock">
L.Â Ye, Z.Â Liu, and Y.Â Wang, â€œDepth-aware object instance segmentation,â€ in
<em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Image Processing (ICIP), 2017 IEEE International Conference on</em>.Â Â Â IEEE, 2017, pp. 325â€“329.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2015]</span>
<span class="ltx_bibblock">
Y.-T. Chen, X.Â Liu, and M.-H. Yang, â€œMulti-instance object segmentation with
occlusion handling,â€ in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR)</em>, 2015, pp. 3470â€“3478.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. [2018]</span>
<span class="ltx_bibblock">
W.Â Wang, R.Â Yu, Q.Â Huang, and U.Â Neumann, â€œSgpn: Similarity group proposal
network for 3d point cloud instance segmentation,â€ in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shotton etÂ al. [2011]</span>
<span class="ltx_bibblock">
J.Â Shotton, A.Â Fitzgibbon, M.Â Cook, T.Â Sharp, M.Â Finocchio, R.Â Moore,
A.Â Kipman, and A.Â Blake, â€œReal-time human pose recognition in parts from
single depth images,â€ in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR)</em>.Â Â Â Ieee,
2011, pp. 1297â€“1304.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. [2015]</span>
<span class="ltx_bibblock">
H.Â Su, C.Â R. Qi, Y.Â Li, and L.Â J. Guibas, â€œRender for cnn: Viewpoint
estimation in images using cnns trained with rendered 3d model views,â€ in
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</em>, 2015, pp.
2686â€“2694.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marion etÂ al. [2018]</span>
<span class="ltx_bibblock">
P.Â Marion, P.Â R. Florence, L.Â Manuelli, and R.Â Tedrake, â€œLabelfusion: A
pipeline for generating ground truth labels for real rgbd data of cluttered
scenes,â€ in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</em>,
2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ciocarlie etÂ al. [2014]</span>
<span class="ltx_bibblock">
M.Â Ciocarlie, K.Â Hsiao, E.Â G. Jones, S.Â Chitta, R.Â B. Rusu, and I.Â A.
Åucan, â€œTowards reliable grasping and manipulation in household
environments,â€ in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Experimental Robotics</em>.Â Â Â Springer, 2014, pp. 241â€“252.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eppner etÂ al. [2016]</span>
<span class="ltx_bibblock">
C.Â Eppner, S.Â HÃ¶fer, R.Â Jonschkowski, R.Â M. Martin, A.Â Sieverling, V.Â Wall,
and O.Â Brock, â€œLessons from the amazon picking challenge: Four aspects of
building robotic systems.â€ in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Robotics: Science and Systems</em>, 2016.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwarz etÂ al. [2016]</span>
<span class="ltx_bibblock">
M.Â Schwarz, A.Â Milan, A.Â S. Periyasamy, and S.Â Behnke, â€œRgb-d object detection
and semantic segmentation for autonomous manipulation in clutter,â€ <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">The
International Journal of Robotics Research</em>, p. 0278364917713117, 2016.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milan etÂ al. [2017]</span>
<span class="ltx_bibblock">
A.Â Milan, T.Â Pham, K.Â Vijay, D.Â Morrison, A.Â Tow, L.Â Liu, J.Â Erskine,
R.Â Grinover, A.Â Gurman, T.Â Hunn <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œSemantic segmentation from
limited training data,â€ <em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1709.07665</em>, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahler and Goldberg [2017]</span>
<span class="ltx_bibblock">
J.Â Mahler and K.Â Goldberg, â€œLearning deep policies for robot bin picking by
simulating robust grasping sequences,â€ in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Conf. on Robot Learning
(CoRL)</em>, 2017, pp. 515â€“524.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oliveira and Chang [2001]</span>
<span class="ltx_bibblock">
B.Â B. M.Â R. Oliveira, ManuelÂ M and Y.-S. Chang, â€œFast digital image
inpainting,â€ in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Appeared in the Proceedings of the International
Conference on Visualization, Imaging and Image Processing (VIIP 2001),
Marbella, Spain</em>, 2001, pp. 106â€“107.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdulla [2017â€“2018]</span>
<span class="ltx_bibblock">
W.Â Abdulla, â€œMask r-cnn for object detection and instance segmentation on
keras and tensorflow,â€ <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/matterport/Mask_RCNN/</span>,
2017â€“2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman [2014]</span>
<span class="ltx_bibblock">
K.Â Simonyan and A.Â Zisserman, â€œVery deep convolutional networks for
large-scale image recognition,â€ <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1809.05824" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1809.05825" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1809.05825">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1809.05825" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1809.05826" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 19:57:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
