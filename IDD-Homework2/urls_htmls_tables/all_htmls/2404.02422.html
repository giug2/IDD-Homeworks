<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.02422] Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data</title><meta property="og:description" content="Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.02422">

<!--Generated on Sun May  5 19:38:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Enhancing Low-Resource LLMs Classification 
<br class="ltx_break">with PEFT and Synthetic Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.

<br class="ltx_break">
<br class="ltx_break">
<span id="id14.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>LoRA, PEFT, LLMs, Few-Shot Learning, Text Classification</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id13" class="ltx_logical-block">
<div id="id13.p1" class="ltx_para">
<p id="id13.p1.1" class="ltx_p ltx_align_center"><span id="id13.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Enhancing Low-Resource LLMs Classification 
<br class="ltx_break">with PEFT and Synthetic Data</span></p>
<br class="ltx_break ltx_centering">
<table id="id12.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_top">
<thead class="ltx_thead">
<tr id="id5.5.5" class="ltx_tr">
<th id="id5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="id5.5.5.5.5" class="ltx_text ltx_font_bold" style="font-size:120%;">Parth Patwa<sup id="id5.5.5.5.5.2" class="ltx_sup"><span id="id5.5.5.5.5.2.1" class="ltx_text ltx_font_medium">1</span></sup>, Simone Filice<sup id="id5.5.5.5.5.3" class="ltx_sup"><span id="id5.5.5.5.5.3.1" class="ltx_text ltx_font_medium">2</span></sup><sup id="id5.5.5.5.5.4" class="ltx_sup"><span id="id5.5.5.5.5.4.1" class="ltx_text ltx_font_medium">∗</span></sup><span id="id4.4.4.4.4.1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><sup id="id4.4.4.4.4.1.1" class="ltx_sup"><span id="id4.4.4.4.4.1.1.1" class="ltx_text ltx_font_medium">∗</span></sup>Work done while at Amazon.</span></span></span>, Zhiyu Chen<sup id="id5.5.5.5.5.5" class="ltx_sup"><span id="id5.5.5.5.5.5.1" class="ltx_text ltx_font_medium">1</span></sup>,</span></th>
</tr>
<tr id="id8.8.8" class="ltx_tr">
<th id="id8.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="id8.8.8.3.3" class="ltx_text ltx_font_bold" style="font-size:120%;">Giuseppe Castellucci<sup id="id8.8.8.3.3.1" class="ltx_sup"><span id="id8.8.8.3.3.1.1" class="ltx_text ltx_font_medium">1</span></sup>, Oleg Rokhlenko<sup id="id8.8.8.3.3.2" class="ltx_sup"><span id="id8.8.8.3.3.2.1" class="ltx_text ltx_font_medium">1</span></sup>, Shervin Malmasi<sup id="id8.8.8.3.3.3" class="ltx_sup"><span id="id8.8.8.3.3.3.1" class="ltx_text ltx_font_medium">1</span></sup></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="id9.9.9" class="ltx_tr">
<td id="id9.9.9.1" class="ltx_td ltx_align_center">
<sup id="id9.9.9.1.1" class="ltx_sup">1</sup>Amazon, USA</td>
</tr>
<tr id="id10.10.10" class="ltx_tr">
<td id="id10.10.10.1" class="ltx_td ltx_align_center">
<sup id="id10.10.10.1.1" class="ltx_sup">2</sup>Technology Innovation Institute, Israel</td>
</tr>
<tr id="id12.12.12" class="ltx_tr">
<td id="id12.12.12.2" class="ltx_td ltx_align_center">
<sup id="id12.12.12.2.1" class="ltx_sup">1</sup>{parthptw, zhiyuche, giusecas, olegro, malmasi}@amazon.com, <sup id="id12.12.12.2.2" class="ltx_sup">2</sup>simone.filice@tii.ae</td>
</tr>
</tbody>
</table>
<p id="id13.p1.2" class="ltx_p ltx_align_center"><span id="id13.p1.2.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent years have been characterized by a paradigm shift in text classification. Large Language Models (LLMs) offer valid alternatives to the traditional approach of fine-tuning pre-trained models (e.g., BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>) on annotated datasets. In-Context Learning (ICL) is a first option, where a LLM learns how to solve a task by simply observing a few examples provided in its prompt (i.e., without any fine-tuning stage) <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>. Another alternative is the 0-shot setting, where the LLM directly solves a task by simply following the provided instructions (i.e., without any example). Instruction-fine tuned models like Flan-T5 <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>, Instruct-GPT <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, or ChatGPT excel in this setting. The advantage of these two alternatives with respect to the traditional approach is that they can be used to bootstrap a model when data is scarce or totally absent.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The 0-shot setting is generally more appealing since it does not require any data, however, the few-shot ICL setting typically leads to better results by only leveraging a small number of samples (e.g., less than 10 examples). Obtaining a few annotated data might not be a significant drawback, as a practitioner with moderate domain knowledge can readily create a small number of examples manually.
However, a major disadvantage is the higher computational cost, latency, and memory requirements associated with the longer prompt, which needs to contain illustrative examples.
A possible solution to leverage the few available examples without incurring the ICL inference costs would be to use them for fine-tuning the LLM, which is possible by using some Parameter-Efficient Fine Tuning (PEFT) techniques <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022c</a>); Lester et al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>); Hu et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>. Unfortunately, as we will demonstrate in the experimental section, PEFT is not effective with very few examples, due to under-fitting or over-fitting phenomena.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a solution to the low-resource PEFT for text classification with LLMs by defining a framework that enables faster, cheaper and more accurate inference than ICL in such a scenario. We hypothesize that LLMs already have some knowledge of how to solve a classification task, but the sub-optimal usage of the available resources (i.e., the few-shot examples) results in low PEFT performance under the low-resource setting. On the contrary, LLMs typically excel in generation tasks, hence we frame an auxiliary data augmentation task that we use to unlock the LLM classification capabilities. Our method consists of three steps. First, we use the LLM to generate synthetic examples for each class of the text classification task we target. Then, we use the same LLM in the ICL setting to classify the examples and clean the data by removing label-inconsistent generated examples. Finally, we fine-tune the LLM with PEFT using the generated and cleaned data. Our experiments show that the resulting classifier reaches accuracy levels comparable to or better than the ICL setting in three different text classification tasks while being a lot more efficient (<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim</annotation></semantics></math>2x to 5x speed boost).
In these generate-filter-train stages we always use the same LLM to demonstrate that what leads to a good accuracy is just a better usage of the few available examples and not the employment of any other resource (e.g., another LLM) which might bring additional knowledge to solve the task.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The rest of the paper is organized as it follows: in Section <a href="#S2" title="2. Related Work ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we discuss the related works. In Section <a href="#S3" title="3. Methodology ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we present our method, while in Sections <a href="#S4" title="4. Experiments ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5" title="5. Results and Analysis ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we discuss the experimental setting and the results, respectively. Finally, Section <a href="#S6" title="6. Conclusion and Future work ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> derives the conclusions.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overview of our method. First, very few real data points are used to generate synthetic data using ICL. Then, the synthetic data is filtered using ICL by LLM again. Finally, the filtered data and the real data are combined to train the LLM using LoRA.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.    Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">LLMs demonstrate impressive capabilities to solve Natural Language Understanding tasks. For instance, classification tasks can be approached in a generative way, i.e., by asking the LLM to generate the class name associated with an input example. 0-shot and ICL are two variants of this paradigm.
Recent models (e.g., GPT-3, <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, Flan-T5 <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>, ChatGPT <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, Falcon <cite class="ltx_cite ltx_citemacro_cite">Penedo et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> or Vicuna <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>) reach impressive results in both settings, and researchers started considering using LLMs as annotators <cite class="ltx_cite ltx_citemacro_cite">Rosenbaum et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Zhu et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>); He et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. For instance, <cite class="ltx_cite ltx_citemacro_citet">Rosenbaum et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> propose a method that uses LLMs to generate and annotate data for Intent Classification and Slot Filling. <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> propose a two-step approach where they first use ChatGPT to generate a few-shot Chain-of-Thought prompt, which they then use to annotate unlabeled data. Results are competitive with human annotators, but their classification procedure is relatively slow since the LLM is invoked twice with prompts that need to contain both examples and explanations. Conversely, we propose a solution whose computational complexity at inference time corresponds to the 0-shot setting case.
Even if these results are impressive, they still might not reach the state-of-the-art performance achievable when LLMs are fully fine-tuned on large data.
Fine-tuning LLMs is extremely expensive, but a viable solution is offered by Parameter Efficient Fine-Tuning (PEFT) techniques <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022c</a>); Lester et al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>); Hu et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, where a pre-trained model is fine-tuned by only updating a small number (e.g., 0.01%) of added or selected parameters. These methods report results that match the performance of full fine-tuning when large training datasets are available. On the contrary, there has been relatively little focus on (parameter-efficient) fine-tuning in low-resource settings. Our paper targets this scenario, as we assume we can access only a few annotated examples (e.g., four per class) and no unlabeled data. A work operating in a similar setting is <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022b</a>)</cite>, where the authors propose a novel PEFT technique that is demonstrated to work well in low resource settings when the PEFT weights are pre-trained and multiple tasks are trained in parallel. We differ from their work as we do not pre-train the PEFT weights and we target a single task at a time, without assuming (possibly related) data from other tasks is available. Relaxing this assumption is especially useful when dealing with very peculiar tasks not sharing similarities with other available datasets. Hence, to the best of our knowledge, we are the first to improve few-shot PEFT without additional resources (external datasets or models).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Methodology </h2>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:867.2pt;height:451.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(227.6pt,-118.5pt) scale(2.10480565981878,2.10480565981878) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">#real</span></td>
<td id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">#syn</span></td>
<td id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">SST2</span></td>
<td id="S3.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">TREC</span></td>
<td id="S3.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">AG News</span></td>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.2.2.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">acc</span></td>
<td id="S3.T1.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">inf. time</span></td>
<td id="S3.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">acc</span></td>
<td id="S3.T1.1.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.2.2.8.1" class="ltx_text ltx_font_bold">inf. time</span></td>
<td id="S3.T1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.2.2.9.1" class="ltx_text ltx_font_bold">acc</span></td>
<td id="S3.T1.1.1.2.2.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.2.2.10.1" class="ltx_text ltx_font_bold">inf. time</span></td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Vicuna7b</td>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0-shot</td>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
<td id="S3.T1.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
<td id="S3.T1.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.55</td>
<td id="S3.T1.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.27</td>
<td id="S3.T1.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.16</td>
<td id="S3.T1.1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.28</td>
<td id="S3.T1.1.1.3.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.36</td>
<td id="S3.T1.1.1.3.3.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna7b</td>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r">ICL</td>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
<td id="S3.T1.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="S3.T1.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.1.1.4.4.5.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S3.T1.1.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r">0.6</td>
<td id="S3.T1.1.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r">0.60</td>
<td id="S3.T1.1.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r">0.9</td>
<td id="S3.T1.1.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r">0.75</td>
<td id="S3.T1.1.1.4.4.10" class="ltx_td ltx_align_left ltx_border_r">2.5</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna7b</td>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r">LoRA</td>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
<td id="S3.T1.1.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="S3.T1.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r">0.51</td>
<td id="S3.T1.1.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r">0.27</td>
<td id="S3.T1.1.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r">0.49</td>
<td id="S3.T1.1.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r">0.28</td>
<td id="S3.T1.1.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r">0.35</td>
<td id="S3.T1.1.1.5.5.10" class="ltx_td ltx_align_left ltx_border_r">0.5</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna7b</td>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r">LoRA</td>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r">25</td>
<td id="S3.T1.1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="S3.T1.1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r">0.89</td>
<td id="S3.T1.1.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r">0.27</td>
<td id="S3.T1.1.1.6.6.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.1.1.6.6.7.1" class="ltx_text ltx_font_bold">0.84</span></td>
<td id="S3.T1.1.1.6.6.8" class="ltx_td ltx_align_left ltx_border_r">0.28</td>
<td id="S3.T1.1.1.6.6.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.1.1.6.6.9.1" class="ltx_text ltx_font_bold">0.86</span></td>
<td id="S3.T1.1.1.6.6.10" class="ltx_td ltx_align_left ltx_border_r">0.5</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna7b</td>
<td id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r">ours</td>
<td id="S3.T1.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
<td id="S3.T1.1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r">21</td>
<td id="S3.T1.1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r">0.90</td>
<td id="S3.T1.1.1.7.7.6" class="ltx_td ltx_align_left ltx_border_r">0.27</td>
<td id="S3.T1.1.1.7.7.7" class="ltx_td ltx_align_left ltx_border_r">0.79</td>
<td id="S3.T1.1.1.7.7.8" class="ltx_td ltx_align_left ltx_border_r">0.28</td>
<td id="S3.T1.1.1.7.7.9" class="ltx_td ltx_align_left ltx_border_r">0.82</td>
<td id="S3.T1.1.1.7.7.10" class="ltx_td ltx_align_left ltx_border_r">0.5</td>
</tr>
<tr id="S3.T1.1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Vicuna13b</td>
<td id="S3.T1.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0-shot</td>
<td id="S3.T1.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
<td id="S3.T1.1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
<td id="S3.T1.1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.85</td>
<td id="S3.T1.1.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.37</td>
<td id="S3.T1.1.1.8.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.36</td>
<td id="S3.T1.1.1.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.38</td>
<td id="S3.T1.1.1.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.31</td>
<td id="S3.T1.1.1.8.8.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1.83</td>
</tr>
<tr id="S3.T1.1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna13b</td>
<td id="S3.T1.1.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r">ICL</td>
<td id="S3.T1.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
<td id="S3.T1.1.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="S3.T1.1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r">0.93</td>
<td id="S3.T1.1.1.9.9.6" class="ltx_td ltx_align_left ltx_border_r">1.2</td>
<td id="S3.T1.1.1.9.9.7" class="ltx_td ltx_align_left ltx_border_r">0.75</td>
<td id="S3.T1.1.1.9.9.8" class="ltx_td ltx_align_left ltx_border_r">1.7</td>
<td id="S3.T1.1.1.9.9.9" class="ltx_td ltx_align_left ltx_border_r">0.80</td>
<td id="S3.T1.1.1.9.9.10" class="ltx_td ltx_align_left ltx_border_r">4.36</td>
</tr>
<tr id="S3.T1.1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna13b</td>
<td id="S3.T1.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r">LoRA</td>
<td id="S3.T1.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
<td id="S3.T1.1.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="S3.T1.1.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r">0.84</td>
<td id="S3.T1.1.1.10.10.6" class="ltx_td ltx_align_left ltx_border_r">0.37</td>
<td id="S3.T1.1.1.10.10.7" class="ltx_td ltx_align_left ltx_border_r">0.62</td>
<td id="S3.T1.1.1.10.10.8" class="ltx_td ltx_align_left ltx_border_r">0.38</td>
<td id="S3.T1.1.1.10.10.9" class="ltx_td ltx_align_left ltx_border_r">0.64</td>
<td id="S3.T1.1.1.10.10.10" class="ltx_td ltx_align_left ltx_border_r">1.83</td>
</tr>
<tr id="S3.T1.1.1.11.11" class="ltx_tr">
<td id="S3.T1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna13b</td>
<td id="S3.T1.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r">LoRA</td>
<td id="S3.T1.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r">25</td>
<td id="S3.T1.1.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="S3.T1.1.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r">0.93</td>
<td id="S3.T1.1.1.11.11.6" class="ltx_td ltx_align_left ltx_border_r">0.37</td>
<td id="S3.T1.1.1.11.11.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.1.1.11.11.7.1" class="ltx_text ltx_font_bold">0.93</span></td>
<td id="S3.T1.1.1.11.11.8" class="ltx_td ltx_align_left ltx_border_r">0.38</td>
<td id="S3.T1.1.1.11.11.9" class="ltx_td ltx_align_left ltx_border_r">0.84</td>
<td id="S3.T1.1.1.11.11.10" class="ltx_td ltx_align_left ltx_border_r">1.83</td>
</tr>
<tr id="S3.T1.1.1.12.12" class="ltx_tr">
<td id="S3.T1.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vicuna13b</td>
<td id="S3.T1.1.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r">ours</td>
<td id="S3.T1.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
<td id="S3.T1.1.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r">21</td>
<td id="S3.T1.1.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.1.1.12.12.5.1" class="ltx_text ltx_font_bold">0.93</span></td>
<td id="S3.T1.1.1.12.12.6" class="ltx_td ltx_align_left ltx_border_r">0.37</td>
<td id="S3.T1.1.1.12.12.7" class="ltx_td ltx_align_left ltx_border_r">0.81</td>
<td id="S3.T1.1.1.12.12.8" class="ltx_td ltx_align_left ltx_border_r">0.38</td>
<td id="S3.T1.1.1.12.12.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.1.1.12.12.9.1" class="ltx_text ltx_font_bold">0.86</span></td>
<td id="S3.T1.1.1.12.12.10" class="ltx_td ltx_align_left ltx_border_r">1.83</td>
</tr>
<tr id="S3.T1.1.1.13.13" class="ltx_tr">
<td id="S3.T1.1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r ltx_border_tt">Vicuna7b</td>
<td id="S3.T1.1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">LoRA</td>
<td id="S3.T1.1.1.13.13.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">Full</td>
<td id="S3.T1.1.1.13.13.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0</td>
<td id="S3.T1.1.1.13.13.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0.97</td>
<td id="S3.T1.1.1.13.13.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0.27</td>
<td id="S3.T1.1.1.13.13.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0.98</td>
<td id="S3.T1.1.1.13.13.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0.28</td>
<td id="S3.T1.1.1.13.13.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0.95</td>
<td id="S3.T1.1.1.13.13.10" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_tt">0.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy results and inference times (in seconds) on three classification tasks. In bold the best performing method for the model in a data-scarce setting. "Full" data refers to the use of the entire available training data. #real and #syn refer to the number of real and synthetic examples per class used for training.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We explore a low-resource setting where we have few training examples per class and no unlabeled data. ICL methods could achieve reasonable performance with few-shot samples but inference cost is high due to long prompts. PEFT methods like LoRA are known to be more efficient than ICL at inference. However, we find that LoRA performs worse than ICL in data-scarce settings (see Tab. <a href="#S3.T1" title="Table 1 ‣ 3. Methodology ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). In this paper, we aim to explore the potential of combining the strengths of PEFT and ICL methods for achieving efficient and effective text classification.
Hence we propose to augment the training data with synthetic data to better align the generation and classification capability of LLMs and to ensure that PEFT is performed on a decent amount of data. Our method has 3 steps: generate data, filter data, and train. An overview of our method is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><svg id="S3.F2.pic1" class="ltx_picture ltx_centering" height="247.61" overflow="visible" version="1.1" width="464.39"><g transform="translate(0,247.61) matrix(1 0 0 -1 0 0) translate(232.19,0) translate(0,123.8)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 226.38 123.53 L -226.38 123.53 C -229.44 123.53 -231.92 121.05 -231.92 117.99 L -231.92 -117.99 C -231.92 -121.05 -229.44 -123.53 -226.38 -123.53 L 226.38 -123.53 C 229.44 -123.53 231.92 -121.05 231.92 -117.99 L 231.92 117.99 C 231.92 121.05 229.44 123.53 226.38 123.53 Z M -231.92 -123.53" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -227.31 -8.49)" fill="#000000" stroke="#000000"><foreignObject width="454.61" height="237.83" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible">
<div id="S3.F2.pic1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:325.2pt;height:171.9pt;vertical-align:-79.8pt;"><span class="ltx_transformed_inner" style="transform:translate(69.9pt,-19.8pt) scale(1.75385964425762,1.75385964425762) ;">
<div id="S3.F2.pic1.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:182.1pt;height:98pt;vertical-align:-45.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<div id="S3.F2.pic1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:127.5pt;height:68.6pt;vertical-align:-31.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.3pt,7.9pt) scale(0.7,0.7) ;">
<p id="S3.F2.pic1.1.1.1.1.1.1.1.1" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1" class="ltx_text">
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:182.1pt;">
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Few examples of movie reviews having positive sentiment are given. Generate more positive reviews
<br class="ltx_break"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="color:#0000FF;">Text:</span></span> [Positive review 1]</span>
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Label:</span> Positive</span>
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.3" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">…</span></span>
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.4" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Text:</span> [Positive review 4]</span>
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.5" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Label:</span> Positive</span>
<span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.6" class="ltx_p"><span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Text:</span> <span id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.6.2" class="ltx_text" style="color:#FF8000;">[the model generates this]
<br class="ltx_break"></span></span>
</span></span></p>
</span></div>
</span></div>
</span></div>
<span id="S3.F2.pic1.2.2.2.2.2" class="ltx_text">
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of a prompt used for generating positive reviews for SST2 data. Four examples of the positive class are provided in the prompt. </figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><svg id="S3.F3.pic1" class="ltx_picture ltx_centering" height="247.61" overflow="visible" version="1.1" width="464.39"><g transform="translate(0,247.61) matrix(1 0 0 -1 0 0) translate(232.19,0) translate(0,123.8)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 226.38 123.53 L -226.38 123.53 C -229.44 123.53 -231.92 121.05 -231.92 117.99 L -231.92 -117.99 C -231.92 -121.05 -229.44 -123.53 -226.38 -123.53 L 226.38 -123.53 C 229.44 -123.53 231.92 -121.05 231.92 -117.99 L 231.92 117.99 C 231.92 121.05 229.44 123.53 226.38 123.53 Z M -231.92 -123.53" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -227.31 -8.49)" fill="#000000" stroke="#000000"><foreignObject width="454.61" height="237.83" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible">
<div id="S3.F3.pic1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:325.2pt;height:171.9pt;vertical-align:-79.8pt;"><span class="ltx_transformed_inner" style="transform:translate(69.9pt,-19.8pt) scale(1.75385964425762,1.75385964425762) ;">
<div id="S3.F3.pic1.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:182.1pt;height:98pt;vertical-align:-45.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<div id="S3.F3.pic1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:127.5pt;height:68.6pt;vertical-align:-31.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.3pt,7.9pt) scale(0.7,0.7) ;">
<p id="S3.F3.pic1.1.1.1.1.1.1.1.1" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1" class="ltx_text">
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:182.1pt;">
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Classify the sentiment of the given movie review into Positive or Negative
<br class="ltx_break"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="color:#0000FF;">Text:</span></span> [review 1]</span>
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Label:</span> [Label 1]</span>
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.3" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">…</span></span>
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.4" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Text:</span> [review 8]</span>
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.5" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Label:</span> [Label 8]</span>
<span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.6" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Text:</span> <span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.6.2" class="ltx_text" style="color:#800080;">[generated review]
<br class="ltx_break"><span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.6.2.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Label:</span></span> <span id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.6.3" class="ltx_text" style="color:#FF8000;">[Predicted Label]</span></span>
</span></span></p>
</span></div>
</span></div>
</span></div>
<span id="S3.F3.pic1.2.2.2.2.2" class="ltx_text">
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example of a prompt used for classifying the sentiment of a movie review. Four examples per class are given in the prompt in a random order.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Generation step:</span> <cite class="ltx_cite ltx_citemacro_citet">Chavan et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> show that in a few-shot setting, the performance of PEFT significantly improves as the number of training samples per class increases. We also observe similar results in our initial experiments (presented in section <a href="#S5" title="5. Results and Analysis ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Further, since ICL performs well, we hypothesize that the model has the inherent knowledge to solve the classification task and that the low PEFT results are due to sub-optimal usage of the available resources (the few shot examples). To fill this gap, we first use the LLM <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mathcal{L}</annotation></semantics></math> in the ICL setting to generate synthetic data which we can use to augment the few shot examples at our disposal. We generate examples for each class in the targeted classification task. An example of the prompt we used in this step is shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Methodology ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Filtering step:</span> We first apply a basic filtering step to discard duplicates and malformed generations (i.e., too short or too long texts). On manual inspection of the generated data, we found some label-inconsistent generations (i.e., data that are not valid examples of the class they should represent). We hypothesize this is due to hallucination. To identify and remove these cases, we classify the generated data using ICL with <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\mathcal{L}</annotation></semantics></math>. The prompt used for this stage is similar to the one shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3. Methodology ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. If the predicted label does not match the intended label from the generation step, we discard the generated example. We repeat these generation-filtering steps until we produce <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">N</span> new data samples for each class in the targeted classification task.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.3" class="ltx_p"><span id="S3.p4.3.1" class="ltx_text ltx_font_bold">Training step:</span> Finally, we use the filtered data along with the few (4 per class) real examples for the PEFT of the LLM <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\mathcal{L}</annotation></semantics></math> with LoRA. Note that <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.p4.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><ci id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\mathcal{L}</annotation></semantics></math> is used for all 3 steps, as we want to validate our hypothesis that <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.p4.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">\mathcal{L}</annotation></semantics></math> does not need additional knowledge to work in the PEFT setting, but only a more stable training process which can be guaranteed by the self-generated synthetic examples.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Inference</span> Conversely to the ICL setting, the LLM does not use any example at inference time. Note that the three steps (generate, filter, train) are used only at training time, i.e., there is no impact on the inference latency.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.02422/assets/images/pos_real_wc.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Real data</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.02422/assets/images/pos_syn_wc.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Synthetic data</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Word clouds of the real and synthetic data belonging to the positive class in SST2. </figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We use the Vicuna LLM <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, which is based on LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>. We selected Vicuna as it is the best-performing model among those whose weights were publicly available at the time of our experiments, according to the Chatbot Arena Leaderboard<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://lmsys.org/blog/2023-05-25-leaderboard/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-05-25-leaderboard/</a></span></span></span>. We experiment with 2 model sizes - Vicuna-7b and Vicuna-13b which have 7 billion and 13 billion parameters, respectively. We show results on the official test sets of 3 datasets - SST2 (sentiment analysis, 2 classes) <cite class="ltx_cite ltx_citemacro_cite">Pang and Lee (<a href="#bib.bib18" title="" class="ltx_ref">2005</a>)</cite>, AG News (news topic classification, 4 classes) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite>, and TREC (question classification, 6 classes) <cite class="ltx_cite ltx_citemacro_cite">Li and Roth (<a href="#bib.bib10" title="" class="ltx_ref">2002</a>)</cite>.
To generate synthetic data, we use random sample decoding with temperature = 1.0, top_k = 50 and num_beams = 1 or 2.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">For a fair comparison across models and methods, throughout our experiments, we do not tune the LoRA hyper-parameters (we set rank=8, alpha=32, and dropout=0.1). Similarly, we make minimal changes to the prompt for all ICL experiments and do not perform any prompt engineering.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The code is implemented using hugging face and PEFT library <cite class="ltx_cite ltx_citemacro_cite">Mangrulkar et al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> with torch backend. All models are trained on 4 v100 GPUs of 16GB each. All LoRA models are trained for 100 epochs. The language modeling loss (next token prediction) is optimized using the Adam optimizer. Batch size is set to 2 for Vicuna-13b and 8 for Vicuna-7b. We run each experiment multiple times with different seeds and different few-shot examples. We observe negligible deviation across runs and report the average accuracy. We also report the results of several baselines, including 0-shot, ICL, and vanilla LoRA trained with different numbers of real examples. We also report LoRA trained with the full training set. This could be considered a potential upper-bound reachable in high resource settings and gives results comparable to the respective SoTA methods on the 3 datasets <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Cer et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>); Yang et al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Results and Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3. Methodology ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports the accuracy results and inference times on three tasks. As expected, the few-shot ICL is more accurate than the 0-shot method but significantly slower. Across model sizes and datasets, vanilla LoRA performs a lot worse than ICL in a few shot setting (4 real data points per class). Furthermore, it is even worse than or comparable to the 0-shot method in some cases. Conversely, our generate-filter-train approach is comparable to ICL on the SST2 data and considerably outperforms the ICL baseline on the other datasets. For example, when using Vicuna-7b model on the TREC data, our method gives 0.84 accuracy whereas ICL gives only 0.6 accuracy. Our method is also comparable to or slightly worse than training LoRA with 25 real samples per class. Note that the inference time of LoRA is lower and independent of training data size, whereas the inference time of few-shot ICL is higher and increases with an increase in training data <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022a</a>)</cite>. As expected, the overall performance is generally better with Vicuna-13b than with Vicuna-7b.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Ablation studies:</span> We train LoRA on the unfiltered synthetic data (by skipping the filtering step) and find that it achieved an accuracy of 0.68 on the TREC dataset. Using the filtered data the accuracy is 0.79, which shows the usefulness of the filtering step. We manually checked 125 unfiltered synthetic TREC samples and we found that 33 were incorrect (cases of hallucination). For example <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">"Who is called the Father of Geometry”</span> was incorrectly generated for the <span id="S5.p2.1.3" class="ltx_text ltx_font_typewriter">location</span> class. However, only 5 samples were incorrect out of 125 examples after filtering. For the SST2 dataset, no sample was identified as incorrectly labeled, hence the filtering step did not affect the performance.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Effect of Data Size:</span> We experiment with various data sizes. The results of Vicuna-7b on SST2 and TREC are shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5. Results and Analysis ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S5.F6" title="Figure 6 ‣ 5. Results and Analysis ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, respectively. Increasing the size of real data is always beneficial; on the opposite, adding more synthetic data does always not provide a clear benefit. The main reason is the lack of diversity in the synthetic data.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2404.02422/assets/images/bar-graph_updated.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Data size vs performance of Vicuna-7b on SST2.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2404.02422/assets/images/data_vs_acc_trec.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Data size vs performance of Vicuna-7b on TREC. Please note that in the real dataset, some classes have a limited number of instances (possibly lower than the values reported on x-axis). In such case, all instances of these classes are used.</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2404.02422/assets/x2.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Data size vs unique trigrams in SST2. </figcaption>
</figure>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Data diversity:</span> Figure <a href="#S5.F7" title="Figure 7 ‣ 5. Results and Analysis ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the number of unique tri-grams vs. data size for real and synthetic SST2 data. We can see that for smaller data sizes, the diversity of the real data is comparable to that of synthetic data. However, as the data size increases, the real data diversity increases faster. This shows the difficulty of generating a large amount of diverse synthetic data with only 4 seed examples.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Qualitative Data Analysis:</span> Figure <a href="#S3.F4" title="Figure 4 ‣ 3. Methodology ‣ Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the word clouds of the real and synthetic examples belonging to the <span id="S5.p5.1.2" class="ltx_text ltx_font_italic">positive</span> class of SST2 data. SST2 is a dataset of sentiment analysis of movie reviews. Hence, we can see that words like <span id="S5.p5.1.3" class="ltx_text ltx_font_italic">film</span>, <span id="S5.p5.1.4" class="ltx_text ltx_font_italic">movie</span> appear in both word clouds. Words that show positive sentiment like <span id="S5.p5.1.5" class="ltx_text ltx_font_italic">entertaining</span>, <span id="S5.p5.1.6" class="ltx_text ltx_font_italic">beautiful</span>, <span id="S5.p5.1.7" class="ltx_text ltx_font_italic">funny</span> also appear in both the word clouds. Further, we see other positive words like <span id="S5.p5.1.8" class="ltx_text ltx_font_italic">stunning</span>, <span id="S5.p5.1.9" class="ltx_text ltx_font_italic">delightful</span> only in the synthetic data word cloud whereas subtle positive words like <span id="S5.p5.1.10" class="ltx_text ltx_font_italic">compelling</span>, <span id="S5.p5.1.11" class="ltx_text ltx_font_italic">solid</span> are seen only in the real data word cloud. From this, we can conclude that the synthetic data has a slightly different distribution and can capture the meaning of the positive class.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusion and Future work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduced a framework to make LLMs more efficient and effective text classifiers in very low-resource settings. The procedure we proposed consists of three steps. In the first step, the LLM is used to augment a very small training set with synthetic data; then, we adopt the LLM to classify the generated data and remove label-inconsistent examples; finally, we use the resulting data to fine-tune the LLM using LoRA. By running experiments on three different classification datasets we demonstrated how training LoRA using the self-generated synthetic data allowed our model to be comparable to or surpass several baselines operating in low resource settings, including 0-shot, ICL, and vanilla LoRA. In future work, we plan to improve the quality of the generated examples by promoting data diversity. Some strategies to improve data diversity include increasing attribute diversity <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>, logit suppression <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> etc.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Limitations</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Our method might not work on tasks that are particularly challenging and hard to catch with only a few examples. In this case, ICL is expected to fail, and similarly, our first two steps are expected to produce low-quality examples making the entire procedure ineffective. Another limitation is that our approach is fully based on LLMs and cannot be applied to low-resource languages where there is no existing LLM working well.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Ethics</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Generating data using LLMs for text classification exposes the resulting classifier to the biases acquired during the LLM pre-training. In our framework, this phenomenon is potentially even amplified, as using the same LLM to generate and filter the data might reinforce such biases. Unfortunately, there is no one-size-fits-all solution for this problem. The biases are dependent on the application domain and on the data distribution to be generated. However, we encourage the readers to be very cautious about using this framework and to take the appropriate actions - for example, compiling a list of the potential biases specific for the target application domain and checking for those in the generated data - to mitigate the potential biases that may get reinforced when using a methodology similar to the one here presented.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.   References</h2>

<div id="S9.p1" class="ltx_para">
<span id="S9.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cer et al. (2018)</span>
<span class="ltx_bibblock">
Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1803.11175" title="" class="ltx_ref ltx_href">Universal sentence encoder</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chavan et al. (2023)</span>
<span class="ltx_bibblock">
Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.07967" title="" class="ltx_ref ltx_href">One-for-all: Generalized lora for parameter-efficient fine-tuning</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2210.11416" title="" class="ltx_ref ltx_href">Scaling instruction-finetuned language models</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2023)</span>
<span class="ltx_bibblock">
John Chung, Ece Kamar, and Saleema Amershi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.34" title="" class="ltx_ref ltx_href">Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 575–593, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.16854" title="" class="ltx_ref ltx_href">Annollm: Making large language models to be better crowdsourced annotators</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2106.09685" title="" class="ltx_ref ltx_href">Lora: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2106.09685.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.243" title="" class="ltx_ref ltx_href">The power of scale for parameter-efficient prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Roth (2002)</span>
<span class="ltx_bibblock">
Xin Li and Dan Roth. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/C02-1150" title="" class="ltx_ref ltx_href">Learning question classifiers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">COLING</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022a)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2205.05638" title="" class="ltx_ref ltx_href">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022b)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 35, pages 1950–1965. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2110.07602" title="" class="ltx_ref ltx_href">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.07602.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022c)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-short.8" title="" class="ltx_ref ltx_href">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 61–68, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar et al. (2022)</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. 2022.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/peft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/peft</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2203.02155" title="" class="ltx_ref ltx_href">Training language models to follow instructions with human feedback</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang and Lee (2005)</span>
<span class="ltx_bibblock">
Bo Pang and Lillian Lee. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1219840.1219855" title="" class="ltx_ref ltx_href">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ACL</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al. (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.01116" title="" class="ltx_ref ltx_href">The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2023)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.10683" title="" class="ltx_ref ltx_href">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenbaum et al. (2022)</span>
<span class="ltx_bibblock">
Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.coling-1.18" title="" class="ltx_ref ltx_href">LINGUIST: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 218–241, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2302.13971" title="" class="ltx_ref ltx_href">Llama: Open and efficient foundation language models</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf" title="" class="ltx_ref ltx_href">Xlnet: Generalized autoregressive pretraining for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 32. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/ae9500c4f5607caf2eff033c67daa9d7-Paper-Datasets_and_Benchmarks.pdf" title="" class="ltx_ref ltx_href">Large language model as attributed training data generator: A tale of diversity and bias</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 36, pages 55734–55784. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2016)</span>
<span class="ltx_bibblock">
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1509.01626" title="" class="ltx_ref ltx_href">Character-level convolutional networks for text classification</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.05685" title="" class="ltx_ref ltx_href">Judging llm-as-a-judge with mt-bench and chatbot arena</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Y Zhu, P Zhang, EU Haq, P Hui, and G Tyson. 2023.

</span>
<span class="ltx_bibblock">Can chatgpt reproduce human-generated labels.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">A Study of Social Computing Tasks</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.02421" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.02422" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.02422">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.02422" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.02423" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:38:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
