<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation</title>
<!--Generated on Sat Jun  1 21:50:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.00787v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S1" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S2" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S2.SS0.SSS0.Px1" title="In 2 Background ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Intrinsic debiasing methods.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S2.SS0.SSS0.Px2" title="In 2 Background ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">The effect of debiasing on NMT.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S3" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Integrating Intrinsic Debiasing in MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S3.SS0.SSS0.Px1" title="In 3 Integrating Intrinsic Debiasing in MT ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Which <em class="ltx_emph ltx_font_italic">embedding</em> to debias?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S3.SS0.SSS0.Px2" title="In 3 Integrating Intrinsic Debiasing in MT ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Which <em class="ltx_emph ltx_font_italic">words</em> to debias?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S3.SS0.SSS0.Px3" title="In 3 Integrating Intrinsic Debiasing in MT ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">How does debiasing affect different <em class="ltx_emph ltx_font_italic">languages</em>?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS1" title="In 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Setup ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">MT model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Setup ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Metrics and datasets.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS2" title="In 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS2.SSS0.Px1" title="In 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Debiasing <em class="ltx_emph ltx_font_italic">1-token-profession</em> professions outperforms other approaches.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS2.SSS0.Px2" title="In 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">The optimal embedding table to debias depends on the debiasing method.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS2.SSS0.Px3" title="In 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Results vary between languages.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS2.SSS0.Px4" title="In 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">LEACE and Hard-Debiasing do not significantly harm BLEU scores.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.SS2.SSS0.Px5" title="In 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title">Human evaluation shows that gender prediction is indeed improved with Hard-Debiasing.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S5" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#A1" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#A2" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Human Annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#A3" title="In The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Statistical Significance</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Impact of Intrinsic Debiasing on Downstream Tasks:
<br class="ltx_break"/>A Case Study on Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bar Iluz <sup class="ltx_sup" id="id1.1.1"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mi id="id1.1.1.m1.1.1" mathvariant="normal" xref="id1.1.1.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">♢</annotation></semantics></math></sup>
, Yanai Elazar <sup class="ltx_sup" id="id3.3.3"><math alttext="\spadesuit" class="ltx_Math" display="inline" id="id2.2.2.m1.1"><semantics id="id2.2.2.m1.1a"><mi id="id2.2.2.m1.1.1" mathvariant="normal" xref="id2.2.2.m1.1.1.cmml">♠</mi><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">♠</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\spadesuit</annotation><annotation encoding="application/x-llamapun" id="id2.2.2.m1.1d">♠</annotation></semantics></math>,<math alttext="\clubsuit" class="ltx_Math" display="inline" id="id3.3.3.m2.1"><semantics id="id3.3.3.m2.1a"><mi id="id3.3.3.m2.1.1" mathvariant="normal" xref="id3.3.3.m2.1.1.cmml">♣</mi><annotation-xml encoding="MathML-Content" id="id3.3.3.m2.1b"><ci id="id3.3.3.m2.1.1.cmml" xref="id3.3.3.m2.1.1">♣</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m2.1c">\clubsuit</annotation><annotation encoding="application/x-llamapun" id="id3.3.3.m2.1d">♣</annotation></semantics></math></sup>, Asaf Yehudai <sup class="ltx_sup" id="id4.4.4"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id4.4.4.m1.1"><semantics id="id4.4.4.m1.1a"><mi id="id4.4.4.m1.1.1" mathvariant="normal" xref="id4.4.4.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id4.4.4.m1.1b"><ci id="id4.4.4.m1.1.1.cmml" xref="id4.4.4.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id4.4.4.m1.1d">♢</annotation></semantics></math></sup>, Gabriel Stanovsky <sup class="ltx_sup" id="id5.5.5"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id5.5.5.m1.1"><semantics id="id5.5.5.m1.1a"><mi id="id5.5.5.m1.1.1" mathvariant="normal" xref="id5.5.5.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id5.5.5.m1.1b"><ci id="id5.5.5.m1.1.1.cmml" xref="id5.5.5.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id5.5.5.m1.1d">♢</annotation></semantics></math></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.6"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id6.6.6.m1.1"><semantics id="id6.6.6.m1.1a"><mi id="id6.6.6.m1.1.1" mathvariant="normal" xref="id6.6.6.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id6.6.6.m1.1b"><ci id="id6.6.6.m1.1.1.cmml" xref="id6.6.6.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id6.6.6.m1.1d">♢</annotation></semantics></math></sup>Hebrew University of Jerusalem,
<sup class="ltx_sup" id="id7.7.7"><math alttext="\spadesuit" class="ltx_Math" display="inline" id="id7.7.7.m1.1"><semantics id="id7.7.7.m1.1a"><mi id="id7.7.7.m1.1.1" mathvariant="normal" xref="id7.7.7.m1.1.1.cmml">♠</mi><annotation-xml encoding="MathML-Content" id="id7.7.7.m1.1b"><ci id="id7.7.7.m1.1.1.cmml" xref="id7.7.7.m1.1.1">♠</ci></annotation-xml><annotation encoding="application/x-tex" id="id7.7.7.m1.1c">\spadesuit</annotation><annotation encoding="application/x-llamapun" id="id7.7.7.m1.1d">♠</annotation></semantics></math></sup>Allen Institute for AI,
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.8"><math alttext="\clubsuit" class="ltx_Math" display="inline" id="id8.8.8.m1.1"><semantics id="id8.8.8.m1.1a"><mi id="id8.8.8.m1.1.1" mathvariant="normal" xref="id8.8.8.m1.1.1.cmml">♣</mi><annotation-xml encoding="MathML-Content" id="id8.8.8.m1.1b"><ci id="id8.8.8.m1.1.1.cmml" xref="id8.8.8.m1.1.1">♣</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.8.8.m1.1c">\clubsuit</annotation><annotation encoding="application/x-llamapun" id="id8.8.8.m1.1d">♣</annotation></semantics></math></sup>University of Washington
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id1">{bar.iluz,gabriel.stanovsky,asaf.Yehudai}@mail.huji.ac.il</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id2">yanaiela@gmail.com
<br class="ltx_break"/></span>
</span></span>
</div>
<h1 class="ltx_title ltx_title_document">Applying Intrinsic Debiasing on Downstream Tasks:
<br class="ltx_break"/>Challenges and Considerations for Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bar Iluz <sup class="ltx_sup" id="id1.1.1"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mi id="id1.1.1.m1.1.1" mathvariant="normal" xref="id1.1.1.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">♢</annotation></semantics></math></sup>
, Yanai Elazar <sup class="ltx_sup" id="id3.3.3"><math alttext="\spadesuit" class="ltx_Math" display="inline" id="id2.2.2.m1.1"><semantics id="id2.2.2.m1.1a"><mi id="id2.2.2.m1.1.1" mathvariant="normal" xref="id2.2.2.m1.1.1.cmml">♠</mi><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">♠</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\spadesuit</annotation><annotation encoding="application/x-llamapun" id="id2.2.2.m1.1d">♠</annotation></semantics></math>,<math alttext="\clubsuit" class="ltx_Math" display="inline" id="id3.3.3.m2.1"><semantics id="id3.3.3.m2.1a"><mi id="id3.3.3.m2.1.1" mathvariant="normal" xref="id3.3.3.m2.1.1.cmml">♣</mi><annotation-xml encoding="MathML-Content" id="id3.3.3.m2.1b"><ci id="id3.3.3.m2.1.1.cmml" xref="id3.3.3.m2.1.1">♣</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m2.1c">\clubsuit</annotation><annotation encoding="application/x-llamapun" id="id3.3.3.m2.1d">♣</annotation></semantics></math></sup>, Asaf Yehudai <sup class="ltx_sup" id="id4.4.4"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id4.4.4.m1.1"><semantics id="id4.4.4.m1.1a"><mi id="id4.4.4.m1.1.1" mathvariant="normal" xref="id4.4.4.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id4.4.4.m1.1b"><ci id="id4.4.4.m1.1.1.cmml" xref="id4.4.4.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id4.4.4.m1.1d">♢</annotation></semantics></math></sup>, Gabriel Stanovsky <sup class="ltx_sup" id="id5.5.5"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id5.5.5.m1.1"><semantics id="id5.5.5.m1.1a"><mi id="id5.5.5.m1.1.1" mathvariant="normal" xref="id5.5.5.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id5.5.5.m1.1b"><ci id="id5.5.5.m1.1.1.cmml" xref="id5.5.5.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id5.5.5.m1.1d">♢</annotation></semantics></math></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.6"><math alttext="\diamondsuit" class="ltx_Math" display="inline" id="id6.6.6.m1.1"><semantics id="id6.6.6.m1.1a"><mi id="id6.6.6.m1.1.1" mathvariant="normal" xref="id6.6.6.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="id6.6.6.m1.1b"><ci id="id6.6.6.m1.1.1.cmml" xref="id6.6.6.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.m1.1c">\diamondsuit</annotation><annotation encoding="application/x-llamapun" id="id6.6.6.m1.1d">♢</annotation></semantics></math></sup>Hebrew University of Jerusalem,
<sup class="ltx_sup" id="id7.7.7"><math alttext="\spadesuit" class="ltx_Math" display="inline" id="id7.7.7.m1.1"><semantics id="id7.7.7.m1.1a"><mi id="id7.7.7.m1.1.1" mathvariant="normal" xref="id7.7.7.m1.1.1.cmml">♠</mi><annotation-xml encoding="MathML-Content" id="id7.7.7.m1.1b"><ci id="id7.7.7.m1.1.1.cmml" xref="id7.7.7.m1.1.1">♠</ci></annotation-xml><annotation encoding="application/x-tex" id="id7.7.7.m1.1c">\spadesuit</annotation><annotation encoding="application/x-llamapun" id="id7.7.7.m1.1d">♠</annotation></semantics></math></sup>Allen Institute for AI,
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.8"><math alttext="\clubsuit" class="ltx_Math" display="inline" id="id8.8.8.m1.1"><semantics id="id8.8.8.m1.1a"><mi id="id8.8.8.m1.1.1" mathvariant="normal" xref="id8.8.8.m1.1.1.cmml">♣</mi><annotation-xml encoding="MathML-Content" id="id8.8.8.m1.1b"><ci id="id8.8.8.m1.1.1.cmml" xref="id8.8.8.m1.1.1">♣</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.8.8.m1.1c">\clubsuit</annotation><annotation encoding="application/x-llamapun" id="id8.8.8.m1.1d">♣</annotation></semantics></math></sup>University of Washington
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id1">{bar.iluz,gabriel.stanovsky,asaf.Yehudai}@mail.huji.ac.il</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id2">yanaiela@gmail.com
<br class="ltx_break"/></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">Most works on gender bias focus on intrinsic bias — removing traces of information about a protected group from the model’s internal representation. However, these works are often disconnected from the impact of such debiasing on downstream applications, which is the main motivation for debiasing in the first place.
In this work, we systematically test how methods for intrinsic debiasing affect neural machine translation models, by measuring the extrinsic bias of such systems under different design choices.
We highlight three challenges and mismatches between the debiasing techniques and their end-goal usage, including the choice of embeddings to debias, the mismatch between words and sub-word tokens debiasing, and the effect on different target languages. We find that these considerations have a significant impact on downstream performance and the success of debiasing.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Natural language processing models were shown to over-rely and over-represent gender stereotypes.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Throughout this work we refer to morphological gender, and specifically to masculine and feminine pronouns as captured in earlier work. We note that future important work can extend our work beyond these pronouns to e.g., neo-pronouns <cite class="ltx_cite ltx_citemacro_citep">(Lauscher et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib18" title="">2022</a>)</cite>.</span></span></span> These can typically be found in their internal representation or predictions. For example, consider the following sentence:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<span class="ltx_ERROR undefined" id="S1.p2.1">{dependency}</span><span class="ltx_ERROR undefined" id="S1.p2.2">{deptext}</span>
<p class="ltx_p" id="S1.p2.3">(1) &amp; The &amp; <span class="ltx_text ltx_font_bold" id="S1.p2.3.1">doctor</span> &amp;asked &amp; the &amp; <span class="ltx_text ltx_font_bold" id="S1.p2.3.2">nurse</span> &amp; to &amp; help &amp; <span class="ltx_text ltx_font_italic" id="S1.p2.3.3">her</span>
<br class="ltx_break"/>in &amp; the &amp; procedure.
<br class="ltx_break"/>
<span class="ltx_ERROR undefined" id="S1.p2.3.4">\depedge</span>[edge height=0.5em]93coref


Inferring that <span class="ltx_text ltx_font_italic" id="S1.p2.3.5">her</span> refers to the nurse rather than the doctor may indicate that the model is biased.
A useful distinction of model’s biases was proposed by <cite class="ltx_cite ltx_citemacro_citep">(Goldfarb-Tarrant et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib12" title="">2021a</a>; Cao et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib3" title="">2022a</a>)</cite>:
<em class="ltx_emph ltx_font_italic" id="S1.p2.3.6">Intrinsic bias</em> typically manifests in the geometry of the model’s embeddings.
For example, finding that stereotypically female occupations (e.g. “nurse”, “receptionist”) are grouped together in the embedding space, while stereotypically male occupations (e.g. “doctor”, “CEO”) are closer to each other <cite class="ltx_cite ltx_citemacro_cite">Gonen and Goldberg (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib14" title="">2019</a>)</cite>.
<em class="ltx_emph ltx_font_italic" id="S1.p2.3.7">Extrinsic bias</em> on the other hand is measured in downstream tasks.
For instance, in machine translation (MT), which is the focus of this work, a biased model may translate Example (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S1" title="1 Introduction ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>) to Spanish using a masculine inflection for the word “doctor”, even though a human translator is likely to use a feminine inflection <cite class="ltx_cite ltx_citemacro_citep">(Stanovsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib26" title="">2019</a>)</cite>.
Intrinsic and extrinsic bias do not necessarily correlate <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib4" title="">2022b</a>; Goldfarb-Tarrant et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib13" title="">2021b</a>)</cite>, and biases might reoccur when applying debiased models on other tasks <cite class="ltx_cite ltx_citemacro_citep">(Orgad et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib19" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="667" id="S1.F1.g1" src="x1.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A schematic view of a neural machine translation system, highlighting different possibilities for applying intrinsic debiasing techniques. We examine three considerations: (1) where to apply the debiasing;
(2) which tokens to apply the debiasing to (e.g. only gender-indicative words or the entire vocabulary); and (3) the effect of different target languages.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we identify a gap in the literature between intrinsic bias mitigation and its influence on downstream tasks.
Namely, while <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">extrinsic bias</em> may affect human users in a variety of applications, debiasing techniques often focus only on <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">intrinsic measures</em>, aiming to obfuscate gender from pretrained embeddings <cite class="ltx_cite ltx_citemacro_citep">(Bolukbasi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib2" title="">2016</a>; Elazar and Goldberg, <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib10" title="">2018</a>; Ravfogel et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib21" title="">2020</a>)</cite>.
These approaches
leave many unanswered questions when deploying them within a complex downstream model for specific tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, we systemically explore three fundamental challenges when integrating intrinsic debiasing techniques within complex open-source neural MT architectures. We find that different design choices lead to a wide difference in extrinsic bias as well as task performance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">First, we explore different approaches to cope with discrepancies between different tokenization strategies.
While intrinsic debiasing is largely performed over complete words from a fixed dictionary, modern MT requires mapping those onto sub-word elements determined via a data-dependant tokenizer. We find that debiasing only complete words outperforms a more naive debiasing of all sub-word tokens.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Second, several word embedding tables could be debiased within an MT system.
Therefore, a preliminary architectural question is which of them to debias. We explore various combinations, finding the optimal configuration depends on the intrinsic debiasing technique.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Third, We explore the effects of debiasing a translation model over three target languages (Hebrew, German, and Russian). While all three encode morphological noun gender, they differ in script, typology, and morphology. We find that an important factor for debiasing efficiency is the number of words represented as single tokens, a property determined both by the language’s morphological properties as well as its sampled distribution in the tokenizer training data.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Taken together, our results suggest that extrinsic debiasing involves many interdependent challenges which cannot be inferred from an intrinsic outlook. We hope our work will promote more research on combining intrinsic debiasing methods to downstream tasks to produce extrinsically fairer MT models.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">There is an abundance of debiasing methods in the field <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib28" title="">2021</a>); Schick et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib23" title="">2021</a>); Shen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib25" title="">2021</a>); Dev and Phillips (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib7" title="">2019</a>); Dev et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib6" title="">2021</a>); Kaneko and Bollegala (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib16" title="">2021</a>); Shao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib24" title="">2023</a>)</cite>. Most of them focus on intrinsic debiasing. We focus on three prominent methods, outlined below.
Importantly, all of these methods learn a transformation that can be applied to arbitrary vectors, once the model has finished training, and all were tested mostly intrinsically.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Intrinsic debiasing methods.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">We experiment with three methods: (1) Hard-Debiasing  <cite class="ltx_cite ltx_citemacro_citep">(Bolukbasi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib2" title="">2016</a>)</cite> removes a gender subspace via a Principal Component Analysis (PCA) of predetermined word pairs which are considered as indicative of gender;
(2) INLP <cite class="ltx_cite ltx_citemacro_citep">(Ravfogel et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib21" title="">2020</a>)</cite> learns the direction of the gender subspace rather than using a predefined list of words; and
(3) LEACE <cite class="ltx_cite ltx_citemacro_citep">(Belrose et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib1" title="">2023</a>)</cite> which prevents all linear classifiers from detecting a guarded concept.
A key difference between the methods is that Hard-Debiasing is non-linear and non-exhaustive, leaving stereotypical information after its’ application <cite class="ltx_cite ltx_citemacro_cite">Gonen and Goldberg (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib14" title="">2019</a>)</cite>. In contrast, INLP and LEACE are linear and exhaustive; after applying INLP, stereotypical information can’t be extracted with a specific linear classifier, and after applying LEACE, it can’t be extracted with any linear classifiers.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">The effect of debiasing on NMT.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Most related to our work, <cite class="ltx_cite ltx_citemacro_citet">Escudé Font and Costa-jussà (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib11" title="">2019</a>)</cite> explored the impact of debiasing methods on an English-to-Spanish MT task. However, they tested the MT models only on simple synthetic data, while here we focus on complex data reflecting real biases, and explore various design choices.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Integrating Intrinsic Debiasing in MT</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We examine debiasing methods within the popular encoder-decoder approach to MT, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.
Next, we describe the different research questions addressed in our setup.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Which <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.1.1">embedding</em> to debias?</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">An encoder-decoder model has multiple embedding tables that can be intrinsically debiased: (1) the input matrix of the encoder; (2) the input matrix of the decoder; and (3) the output of the decoder, usually before the softmax layer.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In a complex system, such as the transformer encoder-decoder architecture, the representations after each transformer layer and within each layer can be debiased as well. We leave the investigation of such debiasing to future work.</span></span></span>
We employ different intrinsic debiasing techniques to each of these tables and evaluate their effect on downstream performance.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Which <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.1.1">words</em> to debias?</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">Tokenization poses a challenge for extrinsic debiasing as it may introduce discrepancies between the intrinsically debiased elements (complete words) and the MT input model (sub-word tokens) <cite class="ltx_cite ltx_citemacro_citep">(Iluz et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib15" title="">2023</a>)</cite>. We experiment with three different configurations: (1) <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.1">all-tokens</em>: debiases embeddings of all tokens in the model’s vocabulary;
(2) <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.2">n-token-profession</em>: debiases all embeddings of words that appear in a predefined set of professions, even if they are split across multiple tokens, and (3) <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.3">1-token-profession</em>: debiases only the embeddings of a predefined set of professions that align with the vocabulary of the debiasing technique, e.g., “nurse” is debiased only if it appears as a single token.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">How does debiasing affect different <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px3.1.1">languages</em>?</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">We experiment with three target languages that encode morphological gender for nouns, representing different typological features: (1) Hebrew, a Semitic language with abjad script, (2) Russian, a Slavic language with a Cyrillic script, and (3) German, a Germanic language with Latin alphabet.

</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1">Language</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">Dataset Name</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">Dataset Size</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.1">Russian</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.2">newstest2019</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.3">1997</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.1">German</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.2">newstest2012</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">3003</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.4.3.1">Hebrew</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.4.3.2">TED dev</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.4.3.3">1000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Datasets used for evaluating different target languages. The Dataset Size describes the number of sentences in the dataset.
Russian and German datasets are described in <cite class="ltx_cite ltx_citemacro_citet">Choshen and Abend (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib5" title="">2021</a>)</cite>’s paper. The Hebrew dataset is based on the Opus TED talks dataset <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib22" title="">2020</a>)</cite>.</figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">MT model.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We make use of
OPUS-MT <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann and Thottingal, <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib27" title="">2020</a>)</cite>,<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Helsinki-NLP/Opus-MT" title="">https://github.com/Helsinki-NLP/Opus-MT</a></span></span></span> a transformer-based MT model built of 6 self-attention layers and 8 attention heads in the encoder and the decoder. The model was trained on Opus,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opus.nlpl.eu" title="">https://opus.nlpl.eu</a></span></span></span> an open-source webtext dataset, which uses SentencePiece tokenization <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib17" title="">2018</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Metrics and datasets.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">For extrinsic debiasing measurement, we employ the automatic accuracy metric from <cite class="ltx_cite ltx_citemacro_citet">Stanovsky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib26" title="">2019</a>)</cite>, assessing the percentage of instances where the target entity retains its original gender from the English sentence, using morphological markers in the target language.
We focus on the performance on the <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.1">anti-stereotypical</em> set of 1584 sentences from WinoMT <cite class="ltx_cite ltx_citemacro_citep">(Stanovsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib26" title="">2019</a>)</cite>. These consist of anti-stereotypical gender role assignments, such as the female doctor in Example <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S1" title="1 Introduction ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.
In addition, we approximate the translation quality before and after debiasing using BLEU  <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib20" title="">2002</a>)</cite> on several parallel corpora described in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.T1" title="Table 1 ‣ 4.1 Experimental Setup ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, and manually evaluate the translations to corroborate our findings.
Finally, all results are statistically significant with p-value <math alttext="&lt;0.05" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1"><lt id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2">absent</csymbol><cn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p1.1.m1.1d">&lt; 0.05</annotation></semantics></math>, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#A3" title="Appendix C Statistical Significance ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">C</span></a> for details.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:136pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(102.0pt,-32.0pt) scale(1.88838057709208,1.88838057709208) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.1">Target Language</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.2">German</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.3">Hebrew</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.4">Russian</th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.2.2.1">no-debiasing</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.2">57.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.3">45.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.4">41.0</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.3.1.1">n-token-profession</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.2">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.3">48.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.4">41.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.1.4.2.1">1-token-profession</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.2.2.1">61.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.2.3.1">48.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.2.4.1">41.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy on different target languages when varying the tokens debiasing strategy. Presenting results for applying (1) the baseline (no-debiasing), (2) <span class="ltx_text ltx_font_italic" id="S4.T2.4.1">n-token-profession</span>, debiasing tokens corresponding to professions that are tokenized into one or more tokens, and (3) <span class="ltx_text ltx_font_italic" id="S4.T2.5.2">1-token-profession</span>, debiasing only professions that are tokenized into a single token. For brevity, each cell presents the the best performing choice of embedding table and debiasing method.
</figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Debiasing <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS0.Px1.1.1">1-token-profession</em> professions outperforms other approaches.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.T2" title="Table 2 ‣ 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the gender translation accuracy when applying debiasing methods on different tokens. <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Excluding results for debiasing all tokens, as it led to garbled translations where automatic debiasing measures are irrelevant.</span></span></span>
For the three tested languages, debiasing only professions that are tokenized into single tokens improved the gender prediction the most.
This hints that the sub-word tokens that compose a profession word do not hold the same gender information as the whole word.
</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:104.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(67.0pt,-16.1pt) scale(1.44740794126373,1.44740794126373) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1">Embedding Table</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.3.1">Hard-Debiasing</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.4.1">INLP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.5.1">LEACE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.2.1.1">Encoder Input</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.2">48.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1.3.1">49.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.4">43.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.5">43.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.1.3.2.1">Decoder Input</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.2">48.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.3">48.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.4">50.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.2.5.1">53.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.1.4.3.1">Decoder Output</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.4.3.2">48.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.4.3.3">48.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.3.4.1">50.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.3.5.1">53.8</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Opus MT’s gender prediction accuracy with intrinsic debiasing methods applied on different embedding tables. Each cell is averaged across our target languages (<em class="ltx_emph ltx_font_italic" id="S4.T3.3.1">de, he, ru</em>). Bold numbers represent best per debiasing method. The accuracy is measured by <cite class="ltx_cite ltx_citemacro_citet">Stanovsky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib26" title="">2019</a>)</cite>’s method on their WinoMT dataset</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">The optimal embedding table to debias depends on the debiasing method.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.T3" title="Table 3 ‣ Debiasing 1-token-profession professions outperforms other approaches. ‣ 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the improvement in gender prediction averaged across languages when applied on different embedding tables.
Hard-Debiasing improves gender prediction only when debiasing the encoder’s inputs, while INLP and LEACE improves gender prediction accuracy the most when applied to the decoder output.
This may be explained by INLP’s and LEACE’s linearity, which therefore works best at the end of the decoder, after all nonlinear layers, while Hard-Debiasing employs a non-linear PCA component.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We tested debiasing all 8 combinations of the three embedding tables, but this did not change our findings.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Results vary between languages.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">Debiasing has a positive impact on the accuracy of gender translation in both German and Hebrew, with German improving by 3.7 points and Hebrew by 2.8 points. In contrast, Russian did not see as much improvement (Table <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.T2" title="Table 2 ‣ 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>).
The difference may be due to Russian’s relatively rich morphology (e.g., it has 7 cases compared to 4 in German <cite class="ltx_cite ltx_citemacro_citep">(Dryer and Haspelmath, <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib9" title="">2013</a>)</cite>), resulting in much fewer single-token professions (59% in Russian compared to 65% in Hebrew, and 83% in German).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">LEACE and Hard-Debiasing do not significantly harm BLEU scores.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S4.F2" title="Figure 2 ‣ Human evaluation shows that gender prediction is indeed improved with Hard-Debiasing. ‣ 4.2 Results ‣ 4 Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the relationship between the difference in the gender prediction and the difference in BLEU.
Hard-Debiasing and LEACE both have a small negative effect to the BLEU scores, while
in comparison, INLP presents a trade-off between the improvement in gender prediction and the translation quality according to BLEU scores. This shows that INLP removes information which is important for the translation model, while LEACE (which was proved to be the minimal transformation needed to remove gender information) and Hard-Debiasing indeed preserve more of the information. In terms of the gender prediction accuracy, the best setting of Hard-Debiasing is when applied to the encoder, while INLP and LEACE improve the gender prediction the most when applied to the decoder outputs. LEACE performs better than INLP when applied on the decoder as it was designed to prevent all linear classifiers from detecting the guarded concept, while INLP learns to obfuscate only one linear classifier.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Human evaluation shows that gender prediction is indeed improved with Hard-Debiasing.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px5.p1.1">We manually annotate a portion of the translation to assess how well the automatic gender prediction metrics estimate real bias.
We annotate the configuration of Hard-Debiasing which changes the translations the most compared to other methods: applying Hard-Debiasing to the encoder’s input with the <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS0.Px5.p1.1.1">1-token-profession</em> paradigm.
Out of the 1584 sentences in the dataset, 184 (11%) changed after the debiasing. 32% out of all the sentences that changed after the debiasing corrected the profession’s gender prediction. These numbers are somewhat higher than what the automatic metrics suggest (26% improvement on the same setup). See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#A1" title="Appendix A Human Evaluation ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a> for additional details.
</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S4.F2.g1" src="extracted/5630650/figures/fig_2_with_leace_new.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The relation between gender prediction accuracy difference (orange) and the BLEU difference (blue) between the original model (without any intervention) and the debiased model. The left part presents the results with Hard-Debiasing, INLP in the middle, and LEACE on the right. For each method, we present the results per each location (Encoder, Decoder-input, and Decoder-output), as well as each language).</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We systematically explore different challenges and design choices when integrating intrinsic debiasing methods within complex machine translation systems. We find that it is better to debias only words representative of gender and correspond to single tokens,
that it is important to couple the debiasing method with the specific embedding table (e.g., encoder versus decoder), and that different target languages lead to vastly different results. Future work can evaluate additional debiasing methods on additional tasks, that may require other considerations when applying such methods.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our work explores the integration of debiasing within a complex machine translation system.
As such, the space of possible combinations to explore is very large, including the embedding table to debias, the choice of
target languages, their corresponding test corpora, the debiasing method to explore and their hyperparameter settings, and more.
We systematically explore a subset of these options, which may hinder the generalizability of our specific results, e.g., which tokenization scheme works best. We encourage future work to re-examine our findings in other settings and possibly refine or amend them, while our main takeaway is the broader set of considerations which should be taken into account when debiasing complex, real-world systems.
Additionally, to solve this task, machine translation systems need to also improve their coreference resolution abilities, which we did not examine here <cite class="ltx_cite ltx_citemacro_cite">Yehudai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib29" title="">2023</a>)</cite>.
Additionally, our work focuses on gender bias, but certain debiasing techniques are broad and can be used for other protected attributes, thus we aspire that our work will pave the way for exploring other attributes in future works.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belrose et al. (2023)</span>
<span class="ltx_bibblock">
Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.03819" title="">Leace: Perfect linear concept erasure in closed form</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">ArXiv preprint</em>, abs/2306.03819.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al. (2016)</span>
<span class="ltx_bibblock">
Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html" title="">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain</em>, pages 4349–4357.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2022a)</span>
<span class="ltx_bibblock">
Yang Trista Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Varun Kumar, Jwala Dhamala, and Aram Galstyan. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-short.62" title="">On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 561–570, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2022b)</span>
<span class="ltx_bibblock">
Yang Trista Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Varun Kumar, Jwala Dhamala, and Aram Galstyan. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-short.62" title="">On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 561–570, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choshen and Abend (2021)</span>
<span class="ltx_bibblock">
Leshem Choshen and Omri Abend. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2101.12640" title="">Transition based graph decoder for neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ArXiv preprint</em>, abs/2101.12640.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dev et al. (2021)</span>
<span class="ltx_bibblock">
Sunipa Dev, Tao Li, Jeff M Phillips, and Vivek Srikumar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.411" title="">OSCaR: Orthogonal subspace correction and rectification of biases in word embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 5034–5050, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dev and Phillips (2019)</span>
<span class="ltx_bibblock">
Sunipa Dev and Jeff M. Phillips. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://proceedings.mlr.press/v89/dev19a.html" title="">Attenuating bias in word vectors</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan</em>, volume 89 of <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">Proceedings of Machine Learning Research</em>, pages 879–887. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dror et al. (2018)</span>
<span class="ltx_bibblock">
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1128" title="">The hitchhiker’s guide to testing statistical significance in natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1383–1392, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dryer and Haspelmath (2013)</span>
<span class="ltx_bibblock">
Matthew S. Dryer and Martin Haspelmath, editors. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://wals.info/" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1.1">WALS Online</em></a>.

</span>
<span class="ltx_bibblock">Max Planck Institute for Evolutionary Anthropology, Leipzig.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elazar and Goldberg (2018)</span>
<span class="ltx_bibblock">
Yanai Elazar and Yoav Goldberg. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1002" title="">Adversarial removal of demographic attributes from text data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 11–21, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Escudé Font and Costa-jussà (2019)</span>
<span class="ltx_bibblock">
Joel Escudé Font and Marta R. Costa-jussà. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-3821" title="">Equalizing gender bias in neural machine translation with word embeddings techniques</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</em>, pages 147–154, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldfarb-Tarrant et al. (2021a)</span>
<span class="ltx_bibblock">
Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, and Adam Lopez. 2021a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.150" title="">Intrinsic bias metrics do not correlate with application bias</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 1926–1940, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldfarb-Tarrant et al. (2021b)</span>
<span class="ltx_bibblock">
Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, and Adam Lopez. 2021b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.150" title="">Intrinsic bias metrics do not correlate with application bias</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 1926–1940, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonen and Goldberg (2019)</span>
<span class="ltx_bibblock">
Hila Gonen and Yoav Goldberg. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1061" title="">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 609–614, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iluz et al. (2023)</span>
<span class="ltx_bibblock">
Bar Iluz, Tomasz Limisiewicz, Gabriel Stanovsky, and David Mareček. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.12491" title="">Exploring the impact of training data distribution and subword tokenization on gender bias in machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ArXiv preprint</em>, abs/2309.12491.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaneko and Bollegala (2021)</span>
<span class="ltx_bibblock">
Masahiro Kaneko and Danushka Bollegala. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.107" title="">Debiasing pre-trained contextualised embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 1256–1266, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lauscher et al. (2022)</span>
<span class="ltx_bibblock">
Anne Lauscher, Archie Crowley, and Dirk Hovy. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.105" title="">Welcome to the modern world of pronouns: Identity-inclusive natural language processing beyond gender</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 1221–1232, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orgad et al. (2022)</span>
<span class="ltx_bibblock">
H Orgad, S Goldfarb-Tarrant, and Y Belinkov. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2204.06827" title="">How gender debiasing affects internal model representations, and why it matters. corr, abs/2204.06827, 2022. doi: 10.48550</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ArXiv preprint</em>, abs/2204.06827.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravfogel et al. (2020)</span>
<span class="ltx_bibblock">
Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.647" title="">Null it out: Guarding protected attributes by iterative nullspace projection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 7237–7256, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2020)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.365" title="">Making monolingual sentence embeddings multilingual using knowledge distillation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 4512–4525, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2021)</span>
<span class="ltx_bibblock">
Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00434" title="">Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Transactions of the Association for Computational Linguistics</em>, 9:1408–1424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2023)</span>
<span class="ltx_bibblock">
Shun Shao, Yftah Ziser, and Shay B. Cohen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eacl-main.118" title="">Gold doesn’t always glitter: Spectral removal of linear and nonlinear guarded attribute information</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 1611–1622, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Aili Shen, Xudong Han, Trevor Cohn, Timothy Baldwin, and Lea Frermann. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2109.10645" title="">Contrastive learning for fair representations</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ArXiv preprint</em>, abs/2109.10645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanovsky et al. (2019)</span>
<span class="ltx_bibblock">
Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1164" title="">Evaluating gender bias in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 1679–1684, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal (2020)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Santhosh Thottingal. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.61" title="">OPUS-MT – building open translation services for the world</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 479–480, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Liwen Wang, Yuanmeng Yan, Keqing He, Yanan Wu, and Weiran Xu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.293" title="">Dynamically disentangling social bias from task-oriented representations with adversarial attack</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3740–3750, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yehudai et al. (2023)</span>
<span class="ltx_bibblock">
Asaf Yehudai, Arie Cattan, Omri Abend, and Gabriel Stanovsky. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.eacl-main.69" title="">Evaluating and improving the coreference capabilities of machine translation models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 980–992, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Human Evaluation</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The evaluation proposed by <cite class="ltx_cite ltx_citemacro_citet">Stanovsky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib26" title="">2019</a>)</cite> is automatic and thus bound to introduce some noise. To mitigate this concern, we perform a human evaluation which complements the automatic metric and provides a more thorough analysis of the results.
The evaluation was applied to translations into Hebrew which were debiased according to the <em class="ltx_emph ltx_font_italic" id="A1.p1.1.1">1-token-profession</em> words (see Section <a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#S3" title="3 Integrating Intrinsic Debiasing in MT ‣ The Impact of Intrinsic Debiasing on Downstream Tasks: A Case Study on Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>), when specifically applied on encoder’s input.
We only analyze the sentences that changed in any way compared to the original (non debiased) translations.
There were 184 out of 1584 sentences which changed (11%). 32% of the changed sentences improved the gender prediction (i.e the gender of a profession was incorrect in the original translation and correct in the debiased translation), 3% of the changed sentences were detracted (i.e the gender of a profession was correct in the original translation and incorrect in the debiased translation), and in the rest 65% the change did not affect the gender of the profession.
In 23% out of the 65% of unaffected sentences, the gender of the other profession in the sentence was changed, for example in the sentence “The salesperson talked to the auditor because she was worried about the audit” the gender of salesperson did not change in the debiased translations, but the gender of the auditor was changed from male to female form. Note that this is not a mistake since the gender of auditor is unknown in the source sentence. This shows that the debiasing method affects a larger amount of professions which are not counted in the improvement of the model.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Human Annotations</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">To define the gender direction in the target language for both debias methods, we needed the translations of the 10 representative gender word pairs for each language.
To get those pairs, we asked a native speaker of each of these languages to translate them into their language. In the case of a pair that is irrelevant to the target language (like Mary and John which are common male and female names in English but not in other languages), we asked them to adapt the pair to represent gender pairs in their language.
The set of professions that we debias was also translated into the target languages by three native speakers in each language. The professions annotations were taken from <cite class="ltx_cite ltx_citemacro_citet">Iluz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib15" title="">2023</a>)</cite>.
The translations of the 10 pairs were collected for four languages, German, Hebrew, Russian, and Spanish. <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>link for the 10 pairs datasets will be released upon publication.</span></span></span></p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Statistical Significance</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In order to determine the statistical significance of our findings, we employed McNemar’s test, as recommended by <cite class="ltx_cite ltx_citemacro_citet">Dror et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00787v1#bib.bib8" title="">2018</a>)</cite>.
McNemar’s test is designed for models with binary labels, therefore it is suitable to test the gender bias scores where each sentence is classified as correct if the gender is accurately identified in the translation and incorrect otherwise.
The null hypothesis for this test states that the marginal probability for each outcome is equal between the two algorithms being compared, indicating that the models are identical. In our case, the two models being compared are the original translation model and the debiased version.
When concatenating results per debias method, we get that the results of Hard Debias are significant with p-value of 3.01E-07, and the results of INLP are significant with p-value of 9.65E-06.
When comparing the results per embedding table to debias, we get that debiasing the encoder inputs is significant with p-value of 5.42E-10, debiasing the decoder inputs is significant with p-value of 0.016 and debiasing the decoder outputs is significant with p-value of 0.014.
finally when concatenating all the results, we get that comparing the outputs of a debiased model to a the original model, the results are significant with p-value of 0.01.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Jun  1 21:50:06 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
