<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>vitaLITy 2: Reviewing Academic Literature Using Large Language Models</title>
<!--Generated on Sat Aug 24 03:14:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.13450v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S1" title="In vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2" title="In vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Architecture and Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2.SS1" title="In 2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Similarity Search using LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2.SS2" title="In 2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Chat with Papers using RAG and Prompt Chaining</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S3" title="In vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">vitaLITy 2</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S3.SS1" title="In 3 vitaLITy 2 ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset of Academic Articles</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S3.SS2" title="In 3 vitaLITy 2 ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>User Interface</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S4" title="In vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Usage Scenarios</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S4.SS1" title="In 4 Usage Scenarios ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>LLM Summarization of Literature Review</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S4.SS2" title="In 4 Usage Scenarios ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Contextual Conversations about Papers with LLM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S5" title="In vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion, Limitations, &amp; Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S6" title="In vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\onlineid</span>
<p class="ltx_p" id="p1.2">1182
<span class="ltx_ERROR undefined" id="p1.2.1">\vgtccategory</span>Research
<span class="ltx_ERROR undefined" id="p1.2.2">\vgtcinsertpkg</span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id1.id1">vitaLITy 2</span>: Reviewing Academic Literature
<br class="ltx_break"/>Using Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongye An
<br class="ltx_break"/><span class="ltx_text" id="id2.1.id1" style="font-size:70%;">University of Nottingham </span>
</span><span class="ltx_author_notes">e-mail: psxah15@exmail.nottingham.ac.uk</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arpit Narechania
<br class="ltx_break"/><span class="ltx_text" id="id3.1.id1" style="font-size:70%;">Georgia Institute of Technology
</span>
</span><span class="ltx_author_notes">e-mail: arpitnarechania@gatech.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Emily Wall
<br class="ltx_break"/><span class="ltx_text" id="id4.1.id1" style="font-size:70%;">Emory University
</span>
</span><span class="ltx_author_notes">e-mail: emily.wall@emory.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kai Xu
<br class="ltx_break"/><span class="ltx_text" id="id5.1.id1" style="font-size:70%;">University of Nottingham </span>
</span><span class="ltx_author_notes">e-mail: kai.xu@nottingham.ac.uk</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Academic literature reviews have traditionally relied on techniques such as keyword searches and accumulation of relevant back-references, using databases like Google Scholar or IEEEXplore.
However, both the precision and accuracy of these search techniques is limited by the presence or absence of specific keywords, making literature review akin to searching for needles in a haystack.
We present <span class="ltx_text ltx_font_smallcaps" id="id6.id1.1">vitaLITy 2</span>, a solution that uses a Large Language Model or LLM-based approach to identify semantically relevant literature in a textual embedding space.
We include a corpus of 66,692 papers from 1970-2023 which are searchable through text embeddings created by three language models.
<span class="ltx_text ltx_font_smallcaps" id="id6.id1.2">vitaLITy 2</span> contributes a novel Retrieval Augmented Generation (RAG) architecture and can be interacted with through an LLM with augmented prompts, including summarization of a collection of papers.
<span class="ltx_text ltx_font_smallcaps" id="id6.id1.3">vitaLITy 2</span> also provides a chat interface that allow users to perform complex queries without learning any new programming language. This also enables users to take advantage of the knowledge captured in the LLM from its enormous training corpus.
Finally, we demonstrate the applicability of <span class="ltx_text ltx_font_smallcaps" id="id6.id1.4">vitaLITy 2</span> through two usage scenarios.
<span class="ltx_text ltx_font_smallcaps" id="id6.id1.5">vitaLITy 2</span> is available as open-source software at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://vitality-vis.github.io" title="">https://vitality-vis.github.io</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>large language model, retrieval augmented generation, text embedding, vector database, literature review, data visualization
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">Introduction</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">In recent years, the proliferation of vast digital repositories of academic papers has posed numerous challenges for researchers and scholars alike, including (i) efficiently retrieving pertinent information from this expansive pool, (ii) comprehensively visualizing the relationships within scholarly literature, and (iii) summarizing extensive bodies of literature.
Traditional information retrieval methods for literature reviews often fall short in capturing the nuanced connections between academic papers, thereby hindering the seamless extraction of relevant insights; e.g., existing literature review approaches are often ad hoc rather than systematic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib35" title="">35</a>]</cite> and even systematic approaches using techniques like keyword search can lead to inadequate coverage of evidence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1"><span class="ltx_text ltx_font_smallcaps" id="p4.1.1">vitaLITy 1</span> began to address these challenges by introducing a transformer-based approach to exploring academic literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib26" title="">26</a>]</cite>. This system included a web-based visualization interface that used text embeddings to identify semantically similar literature based on input papers or even unpublished paper abstracts using SPECTER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib8" title="">8</a>]</cite> and GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib31" title="">31</a>]</cite>. It also introduced a corpus of more than 59,000 papers across 38 prominent visualization venues, with summative user feedback demonstrating the utility of the tool.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">While prior approaches to visualize academic articles have made significant strides towards finding semantically related articles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib26" title="">26</a>]</cite> and exploring citation networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib43" title="">43</a>]</cite>, these efforts nonetheless are limited in their ability to support intuitive interaction with a corpus of academic literature or assist individuals in summarizing large bodies of literature. We recognize the past two years have seen an explosive growth and innovation in the capability and range of applications of Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib51" title="">51</a>]</cite>, including use within the visualization community for generating visualizations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib32" title="">32</a>]</cite> and authoring data-driven articles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib37" title="">37</a>]</cite>. We observe an opportunity to build upon prior literature search visualization approaches and recent developments with LLMs to introduce literature search methods to address these gaps.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">We present <span class="ltx_text ltx_font_smallcaps" id="p6.1.1">vitaLITy 2</span>, an open-source visualization system for conducting literature searches, that incorporates a novel Retrieval Augmented Generation (RAG) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib21" title="">21</a>]</cite>. <span class="ltx_text ltx_font_smallcaps" id="p6.1.2">vitaLITy 2</span> enables users to search for relevant literature using (1) a paper(s) as the seed to find similar work, (2) the abstract of an existing paper or paper to-be-written, (3) traditional keyword-based searching, or (4) a natural language query to an LLM-powered chat interface. Results are queried from a vector database and shown in a rank-ordered table with a similarity score including title, abstract, authors, citation counts, and links to the original sources. Results are also visualized in a projection where papers that appear closer together spatially are more similar in the vector embedding space. Users can also interact with the LLM interface to summarize a corpus of papers and ask contextually relevant questions, e.g., <span class="ltx_text ltx_font_italic" id="p6.1.3">“what is the grounded theory method?”</span></p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p" id="p7.1">Over the <span class="ltx_text ltx_font_smallcaps" id="p7.1.1">vitaLITy 1</span> predecessor system, <span class="ltx_text ltx_font_smallcaps" id="p7.1.2">vitaLITy 2</span> introduces a number of novel features, including its RAG architecture and integration of prompt chaining (described in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2" title="2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>). The system supports natural context-aware conversations through LLMs to summarize and understand collections of papers. The system also augments the dataset of 59,000 papers from <span class="ltx_text ltx_font_smallcaps" id="p7.1.3">vitaLITy 1</span> to cover publications across 38 visualization venues over the past 3 years for a total of 66,692 papers. <span class="ltx_text ltx_font_smallcaps" id="p7.1.4">vitaLITy 2</span> is available as open-source software at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://vitality-vis.github.io" title="">https://vitality-vis.github.io</a>.</p>
</div>
<div class="ltx_para" id="p8">
<p class="ltx_p" id="p8.1">The remainder of this paper is organized as follows: we review relevant background information in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S1" title="1 Related Work ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>. Next, we discuss the architecture of <span class="ltx_text ltx_font_smallcaps" id="p8.1.1">vitaLITy 2</span> in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2" title="2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> followed by a description of the front-end interface and system capabilities in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S3" title="3 vitaLITy 2 ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>. We provide exemplary usage scenarios in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S4" title="4 Usage Scenarios ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>.
Finally, we discuss the implications and applications of this work in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S5" title="5 Discussion, Limitations, &amp; Future Work ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Related Work</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the ever growing body of scientific literature, there is a constant need for support to conduct more effective and efficient literature research. This challenge has attracted increasing research attention as more publications become digitally available more widely. Early work comes from both within the visualization community (such as PaperVis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib6" title="">6</a>]</cite>) and beyond (such as the work by El-Arini and Guestrin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib10" title="">10</a>]</cite>). For this paper, we will focus on the results from visualization and related communities.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Broadly speaking, the visualization research for literature analysis spans a spectrum, from providing an overview of a field to helping find and understand papers related to a specific topic. For the former, a well-known example is the work by Isenberg et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib16" title="">16</a>]</cite> that aims to provide an overview of the entire visualization research landscape, using papers spanning two and a half decades. While there are other efforts that utilize a similar paper collection, i.e., all the papers from IEEE VIS, few solely focus on the large picture.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The other end of the spectrum targets literature for a specific topic. This is closer to the goal of <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">vitaLITy 2</span>, so we would focus our discussion here. Common approaches employed by these visualization efforts include topic or semantic analysis, network analysis, and supporting the literature review workflow. From early on it has been recognized that keyword matching alone is not an effective way to find relevant papers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib44" title="">44</a>]</cite>, as a concept or technique can often be expressed in several different ways. Most of the proposed approaches use some form of text analytics or natural language processing methods. Topic modeling is a popular approach among the early work that can help identify relevant papers by grouping similar ones together <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib44" title="">44</a>]</cite>. This includes techniques that are designed for text analysis in general and can be easily applied to literature analysis such as Serendip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib1" title="">1</a>]</cite>. As NLP research progresses, new techniques are being used for literature analysis, such as text embeddings generated by transformer-based models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib4" title="">4</a>]</cite>, including GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib30" title="">30</a>]</cite> and SPECTER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib8" title="">8</a>]</cite> used in <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.2">vitaLITy 1</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib26" title="">26</a>]</cite>. <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.3">vitaLITy 2</span> takes advantage of text embeddings created by the latest LLMs, which represent a breakthrough for many NLP tasks, achieving performance levels close to humans.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Network analysis is another common approach used by literature visualization, creating and visualizing co-author and affiliation network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib42" title="">42</a>]</cite>, citation network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib50" title="">50</a>]</cite>, and similarity network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib26" title="">26</a>]</cite>, among others. <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">vitaLITy 2</span> focuses on paper similarity, using the latest LLM-based embeddings to create a more accurate network topology. Finally, there are a series of work targeting the literature review workflow, such as LitSense <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib36" title="">36</a>]</cite> and Relatedly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib29" title="">29</a>]</cite>. While not a focus, the interface and interactions of <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.2">vitaLITy 2</span> are designed to match and support the workflow of common literature analysis tasks.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Dimensionality reduction is essential for visualizing high-dimensional data, such as the embeddings derived from LLMs. <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">vitaLITy 2</span> leverages state-of-the-art techniques to reduce the dimensionality of embeddings while retaining their semantic relationships. UMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib3" title="">3</a>]</cite> has been shown to outperform traditional methods like t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib40" title="">40</a>]</cite> and PCA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib22" title="">22</a>]</cite> in terms of preserving both local and global structures in the data. This capability makes UMAP particularly suitable for visualizing the complex relationships within the literature embeddings generated by LLMs. Once the embeddings are reduced to a lower-dimensional space, they are visualized using interactive plots. These plots allow users to explore the relationships between different papers intuitively.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Using LLMs for literature visualization is still in its infancy. The closest example we found is SciDaSynth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib41" title="">41</a>]</cite>, which has not been peer reviewed. While it also uses LLMs to create publication embedding and provides a visual interface for exploration and analysis, it does not come with a collection of papers, and the focus of analysis is on paper comparison. Similar to SciDaSynth, <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.1">vitaLITy 2</span> introduces LLMs while retaining the original paper visualization and retrieval capabilities of <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.2">vitaLITy 1</span>. <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.3">vitaLITy 2</span> offers users a novel natural language-based interaction for paper retrieval. The search results are integrated into the existing exploratory and analytical visualization panels of <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.4">vitaLITy 1</span>, thereby combining LLMs technology with visualization.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Architecture and Implementation</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S2.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of <span class="ltx_text ltx_font_smallcaps" id="S2.F1.7.1">vitaLITy 2</span>: <span class="ltx_text ltx_font_bold" id="S2.F1.8.2">(Step 1)</span> User input to the system. <span class="ltx_text ltx_font_bold" id="S2.F1.9.3">(Step 2 &amp; 3)</span> Retrieve data from the vector database. <span class="ltx_text ltx_font_bold" id="S2.F1.10.4">(Step 4)</span> Combine the result with user input in the prompt. <span class="ltx_text ltx_font_bold" id="S2.F1.11.5">(Step 5)</span> Recall result from LLM. <span class="ltx_text ltx_font_bold" id="S2.F1.12.6">(Step 6)</span> Return the final result to the user.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2.F1" title="Figure 1 ‣ 2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the architecture, flow of data, and core links of <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">vitaLITy 2</span>.
All the papers in the corpus are first pre-processed using an LLM to create their embeddings (for similarity search), with the results stored in a vector database.
This is shown as the orange part on the top. Similarity search is then performed within the vector database without further LLM access (the bottom line). User input from the chat interface (Step 1) is broken down into a series of smaller tasks using “Prompt chaining.” For each sub-task, relevant information is retrieved from the vector database (Step 2 &amp; 3) and then combined with user input in the prompt before sending it to the LLM (Step 4). “Prompt history” provides context from previous conversations. Once all the sub-tasks are completed (Step 5), the final results are returned to the user (Step 6).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Similarity Search using LLM</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">All the papers in the corpus are pre-processed using an LLM to create their embeddings based on paper metadata, which includes the title, authors, the conference or journal in which the paper is published, publication date, keywords, and abstract.
These text embeddings convert textual data into high-dimensional vectors that capture semantic relationships among paper metadata <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib38" title="">38</a>]</cite>, ensuring thorough comprehension of each paper and enabling more effective similarity searches within the vector database.
For clarity, the “text embedding” mentioned refers to embeddings created from paper metadata rather than the full paper content. This metadata-focused approach balances detail with computational efficiency and allows for robust semantic matching.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">To find similar papers from an academic literature corpus, while <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.1">vitaLITy 1</span> uses GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib30" title="">30</a>]</cite> and SPECTER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib8" title="">8</a>]</cite> embeddings,
<span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.2">vitaLITy 2</span> uses ADA, the embeddings generated by OpenAI’s <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">text-embedding-ada-020</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib28" title="">28</a>]</cite>, which is a breakthrough model that generates contextually rich embeddings by leveraging advanced attention mechanisms and transformer architectures.
Thus, <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.4">vitaLITy 2</span> now includes ADA, GloVe, and SPECTER embeddings.
<span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.5">vitaLITy 2</span> also leverages two <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.6">vector databases</span> (Faiss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib17" title="">17</a>]</cite> and ChromaDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib7" title="">7</a>]</cite>) to store and manage these embeddings and also to locate similar papers via approximate nearest neighbor search.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Chat with Papers using RAG and Prompt Chaining</h3>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="464" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The <span class="ltx_text ltx_font_smallcaps" id="S2.F2.12.1">vitaLITy 2</span> User Interface. (A) <span class="ltx_text ltx_font_bold" id="S2.F2.13.2">Paper Collection View</span> shows the entire corpus of publications, (B) <span class="ltx_text ltx_font_bold" id="S2.F2.14.3">Similarity Search View</span> shows options to look-up publications that are similar to another list of publications or by a work-in-progress title and abstract, (C) <span class="ltx_text ltx_font_bold" id="S2.F2.15.4">Visualization Canvas</span> shows an interactive 2-D UMAP projection of the embedding space of the entire paper collection, (D) <span class="ltx_text ltx_font_bold" id="S2.F2.16.5">Meta View</span> shows summaries of certain attributes with respect to the Paper Collection View (A), (E) Opens a <span class="ltx_text ltx_font_bold" id="S2.F2.17.6">Saved Papers View</span> from which the saved papers can be exported as JSON.
Extending <span class="ltx_text ltx_font_smallcaps" id="S2.F2.18.7">vitaLITy 1</span>, we added (F) <span class="ltx_text ltx_font_bold" id="S2.F2.19.8">Chat with your Data</span> view to allow users to ask natural-language based questions based on the paper corpus.
We also added ADA embeddings (in addition to GloVe and SPECTER embeddings) and enable users to <em class="ltx_emph ltx_font_italic" id="S2.F2.20.9">Summarize</em> or write a <em class="ltx_emph ltx_font_italic" id="S2.F2.21.10">Literature Review</em> on the <span class="ltx_text ltx_font_bold" id="S2.F2.22.11">Saved Papers</span> using LLMs, including the ability to customize the prompts.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">While text embeddings and vector databases allow finding similar papers, there are several other kinds of analyses a user may want to perform, such as understanding a technical concept, summarizing a single paper, or writing a literature review based on multiple papers. To perform such analyses, users may need to build customized tools and/or learn a new query language, neither of which are readily accessible to nor understandable by many users.
<span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.1">vitaLITy 2</span> addresses this issue by leveraging the powerful natural language understanding and generation capabilities of LLMs, allowing users to directly engage with the LLM using natural language.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">However, there are a few challenges when applying LLMs for such analyses:
(1) <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">Prompt Size</em> – This refers to the largest prompt that an LLM can accept and depends on the LLM type and version. Earlier versions of LLMs accepted a few thousand tokens, wherein each word in a prompt accounted for a few tokens. More recent versions have a larger limit, e.g., up to 16K tokens for GPT 3.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib11" title="">11</a>]</cite>. However, this can still limit analyses involving a large corpus of papers, which often have thousands of words each.
(2) <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">Hallucination</em> – it is well known that LLMs can create seemingly plausible information about something that does not exist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib47" title="">47</a>]</cite>, e.g., suggest papers that never existed, that can be detrimental.
(3) <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.3">Limited Comprehension Capabilities</em> – Despite the ability to generate seemingly logical and coherent text, LLMs lack genuine comprehension capabilities. This implies that an LLM may encounter difficulties when handling complex problems or tasks, particularly those requiring a deeper understanding of context or concepts.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Noori’s process of doing literature review using <span class="ltx_text ltx_font_smallcaps" id="S2.F3.14.1">vitaLITy 2</span>. <span class="ltx_text ltx_font_bold" id="S2.F3.15.2">(Step 1)</span> Noori searches the <span class="ltx_text ltx_font_smallcaps" id="S2.F3.16.3">vitaLITy 2</span> database for articles recommended by supervisor. <span class="ltx_text ltx_font_bold" id="S2.F3.17.4">(Step 2)</span> Noori selects Ada Embedding as the embedding option used by <span class="ltx_text ltx_font_smallcaps" id="S2.F3.18.5">vitaLITy 2</span> in “Similarity search”. <span class="ltx_text ltx_font_bold" id="S2.F3.19.6">(Step 3)</span> Noori adds the paper she just searched as a seed for “Similarity Search”. <span class="ltx_text ltx_font_bold" id="S2.F3.20.7">(Step 4)</span> Noori uses “Similarity Search” to find some related papers. <span class="ltx_text ltx_font_bold" id="S2.F3.21.8">(Step 5)</span> Noori saves papers with similarity score of <math alttext="&gt;0.1" class="ltx_Math" display="inline" id="S2.F3.2.m1.1"><semantics id="S2.F3.2.m1.1b"><mrow id="S2.F3.2.m1.1.1" xref="S2.F3.2.m1.1.1.cmml"><mi id="S2.F3.2.m1.1.1.2" xref="S2.F3.2.m1.1.1.2.cmml"></mi><mo id="S2.F3.2.m1.1.1.1" xref="S2.F3.2.m1.1.1.1.cmml">&gt;</mo><mn id="S2.F3.2.m1.1.1.3" xref="S2.F3.2.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F3.2.m1.1c"><apply id="S2.F3.2.m1.1.1.cmml" xref="S2.F3.2.m1.1.1"><gt id="S2.F3.2.m1.1.1.1.cmml" xref="S2.F3.2.m1.1.1.1"></gt><csymbol cd="latexml" id="S2.F3.2.m1.1.1.2.cmml" xref="S2.F3.2.m1.1.1.2">absent</csymbol><cn id="S2.F3.2.m1.1.1.3.cmml" type="float" xref="S2.F3.2.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.2.m1.1d">&gt;0.1</annotation><annotation encoding="application/x-llamapun" id="S2.F3.2.m1.1e">&gt; 0.1</annotation></semantics></math> and highlights them in the UMAP Visualization MAP. <span class="ltx_text ltx_font_bold" id="S2.F3.22.9">(Step 6)</span> Noori selects an additional paper that interested her in the UMAP visualization map. <span class="ltx_text ltx_font_bold" id="S2.F3.23.10">(Step 7)</span> Noori tries to modify and use different prompts and does “Summarize” and “Literature Review”. <span class="ltx_text ltx_font_bold" id="S2.F3.24.11">(Step 8)</span> Noori exports the saved papers to a bib file.
</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="409" id="S2.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Aaron’s process for literature search using <span class="ltx_text ltx_font_smallcaps" id="S2.F4.8.1">vitaLITy 2</span>. <span class="ltx_text ltx_font_bold" id="S2.F4.9.2">(Step 1)</span> Aaron utilizes the “Chat with your data” feature to quickly explore the domain of “grounded theory”, an area unfamiliar to him. <span class="ltx_text ltx_font_bold" id="S2.F4.10.3">(Step 2)</span> Aaron plots an paper cited in the LLM feedback onto the UMAP visualization interface. <span class="ltx_text ltx_font_bold" id="S2.F4.11.4">(Step 3)</span> Aaron selects a set of closely related papers from the UMAP visualization and Aaron adds these papers to the “Similarity Search”.<span class="ltx_text ltx_font_bold" id="S2.F4.12.5">(Step 4)</span> Aaron uses “Similarity Search” feature to find some semantically similar papers. <span class="ltx_text ltx_font_bold" id="S2.F4.13.6">(Step 5)</span> Aaron saves a subset of papers of particular interest to his “saved papers” list. <span class="ltx_text ltx_font_bold" id="S2.F4.14.7">(Step 6)</span> Aaron employs the “Summarize” and “Literature Review” feature to review the saved papers.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">To overcome these issues, we employ two popular approaches: Retrieval Augmented Generation (RAG) and Prompt Chaining.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">RAG</span>. RAG semantically processes the user’s input query to only retrieve relevant information from within the data corpus, thereby reducing the subsequent prompt size and also minimizing the risk of hallucination.
This retrieval process is not about finding exact answers but rather fetching documents that contain potentially useful context or information related to the query.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p5.1.1">Prompt Chaining</span>.
Prompt chaining breaks down complex tasks to a series of smaller steps and provides specific prompts that are known to be effective for each of these steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib45" title="">45</a>]</cite>.
<span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p5.1.2">vitaLITy 2</span> borrows this idea and implements a conversation framework to manage the content and context of user queries and LLM responses. <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p5.1.3">vitaLITy 2</span> uses LangChain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib20" title="">20</a>]</cite> a popular open-source library for this purpose. As LLM API calls operate independently of each other without inherent memorization or management of the context, there is no built-in functionality for retaining conversation history.
Sending the entire dialogue history with each LLM API call risks exceeding the maximum token limit.
Therefore, we adopt a two-step approach in <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p5.1.4">vitaLITy 2</span>: first, a summary of recent conversations is generated by calling the LLM API once to obtain a “condensed conversation history.”
Then, in the second API call, the user’s query, conversation history, and retrieved information are concatenated into a prompt and sent to the LLM API.
This an example of prompt chaining, that efficiently combines the results of two or more LLM API calls.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">vitaLITy 2</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We present <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">vitaLITy 2</span>, an LLM-powered visual analytics tool to help users write academic literature reviews.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset of Academic Articles</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">vitaLITy 1</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib26" title="">26</a>]</cite> provided a dataset of 59,232 academic papers from visualization and HCI literature, along with their metadata such as their title, abstract, author(s), keyword(s), publisher, publication year, citation counts, and n-dimensional and 2-dimensional vector embeddings (GloVe and SPECTER).
<span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">vitaLITy 1</span> also open-sourced a web-scraping framework to extract the above information from digital repositories such as IEEE Xplore and ACM Digital Library for continued development of the corpus.
<span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.3">vitaLITy 2</span> used this scraper to extract more recent papers between 2021-2023, resulting in an augmented dataset of 66,692 papers.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>User Interface</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2.F2" title="Figure 2 ‣ 2.2 Chat with Papers using RAG and Prompt Chaining ‣ 2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows the <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.1">vitaLITy 2</span> user interface, illustrating the new features built on top of <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.2">vitaLITy 1</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.1.1">vitaLITy 1</span>, the <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.2">Paper Collection View</span> (A) shows the entire corpus of publications, the <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.3">Similarity Search View</span> (B) shows options to look-up publications that are similar to another list of publications or by a work-in-progress title and abstract, the <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.4">Visualization Canvas</span> (C) shows an interactive 2-D UMAP projection of the embedding space of the entire paper collection, the <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.5">Meta View</span> (D) shows summaries of certain attributes with respect to the Paper Collection View (A), and (E) opens a <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.6">Saved Papers View</span> from which the saved papers can be exported in a JSON and .bibtex format for later use. We added three new capabilities in <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.1.7">vitaLITy 2</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">First, we expanded available embedding options (from GloVe and SPECTER) to add OpenAI’s ADA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib27" title="">27</a>]</cite>.
Users can view the 2-dimensional ADA embeddings in the UMAP and search for similar papers (by title or abstract).
Based on our own testing and consistent with prior benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib25" title="">25</a>]</cite>, we found ADA embeddings to perform better than GloVe and SPECTER on <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p3.1.1">vitaLITy 2</span> features.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Second, we added a new <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Chat with your Data View</span> (F) to allow users to ask natural-language questions based on the entire corpus, e.g., <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.1.2">“Help me find some papers related to geographic science visualization.”</em>
For papers mentioned in the LLM’s output, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p4.1.3">vitaLITy 2</span> allows the user to map (view in the UMAP), select (look up other similar papers), or save them (include it in a literature review or export).
This capability holds immense potential for revolutionizing scholarly information access, enabling users to effectively harness the capabilities of RAG and pose a wide spectrum of inquiries.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Third, we also added novel capabilities to summarize papers and conduct literature reviews (in the <span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Saved Papers</span> view), building upon the robust semantic understanding capabilities of LLM.
Clicking the “Summarize” button outputs a short summary of each of the saved papers, one below the other.
Clicking the “Literature Review” button goes a step further, and outputs a comprehensive literature review based on the saved papers, including descriptions, comparisons, and a bibliography.
These enhancements enable users to swiftly grasp the key information contained within their saved papers or draft an early version of their related work section in their ongoing manuscript.
Note that <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p5.1.2">vitaLITy 2</span> allows users to customize the base LLM prompts to control the verbosity, writing style, and format of the LLM’s response.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Usage Scenarios</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>LLM Summarization of Literature Review</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Noori is an undergraduate student who has joined a visualization research lab, working under the supervision of a faculty member and Ph.D. student on an ongoing project conducting controlled experiments on multiverse analyses. Per her supervisor’s suggestion, she uses <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p1.1.1">vitaLITy 2</span> to kickstart her literature review.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">She begins using the “Saved Papers” feature in <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p2.1.1">vitaLITy 2</span> to collect papers through various search methods. She starts with a recent paper her supervisor suggested from the CHI conference by Sarma and colleagues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib33" title="">33</a>]</cite> and both “Selects” it and “Saves” it. From this selection, she searches for related papers using the ADA embedding. Of the 25 output similar papers, she decides to save any that have a similarity score of <math alttext="&gt;0.1" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><gt id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">absent</csymbol><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.p2.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&gt;0.1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">&gt; 0.1</annotation></semantics></math>, which results in an additional 5 papers (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib19" title="">19</a>]</cite>). She highlights the papers in the UMAP visualization and hovers on nearby papers, selecting an additional relevant paper on parallel program performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Noori visits the set of saved papers in “Saved Papers” and selects “Literature Review.” <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.1">vitaLITy 2</span> shares (1) a paper-by-paper summarization, then (2) a comparison and contrast of the set of 7 total papers.
Noori adjusts the default prompt to describe a higher-level summary to suit her experience, then reads through the result. The literature review points out the motivation of most of the papers on transparency of data analysis, and reveals a divide where some papers represented earlier relevant ideas (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib34" title="">34</a>]</cite>), while the others were more recent (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib24" title="">24</a>]</cite>). She accordingly uses these observations to help her write a first draft of the related work section on multiverse visualization to her team’s Overleaf document. She uses the “Export” feature of <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.2">vitaLITy 2</span> to add the related papers to the .bib file, and she adds a short paragraph detailing her literature review methodology – including use of <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.3">vitaLITy 2</span> and LLM summarization in the initial draft.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">She reads the papers in more detail, iterates on the writing accordingly, then shares it with her supervisor for feedback. She begins her next literature review task: identifying and summarizing literature on general data analysis workflows with <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p4.1.1">vitaLITy 2</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Contextual Conversations about Papers with LLM</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Aaron is a new faculty member at a university, having just defended his Ph.D. Having primarily used quantitative methods for his dissertation research, he wants to explore qualitative methods in his next project: conducting in-depth interviews to understand the potential of visualization to address user concerns around misinformation.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Aaron begins using <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.1.1">vitaLITy 2</span> to search for a CSCW paper he knows on misinformation in times of crisis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib15" title="">15</a>]</cite>. He selects the “info” icon to read the abstract and additional paper metadata. The abstract mentions the use of <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">“constructivist grounded theory to guide [their] inquiry.”</span> Aaron has heard of this method before, but is not familiar with how it works. Using the LLM interface in <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.1.3">vitaLITy 2</span>, he asks <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.4">“can you explain how the method of grounded theory works?”</span> <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.1.5">vitaLITy 2</span> responds with an in-depth description (Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2.F4" title="Figure 4 ‣ 2.2 Chat with Papers using RAG and Prompt Chaining ‣ 2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>).
He then asks <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.6">“can you find me some relevant papers on the topic of grounded theory?”</span> The system responds with a description of some relevant papers, which use the grounded theory method. The result is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S2.F4" title="Figure 4 ‣ 2.2 Chat with Papers using RAG and Prompt Chaining ‣ 2 Architecture and Implementation ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Aaron continues his literature review accordingly, having a better baseline understanding of the method. Aaron notices that <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.1">vitaLITy 2</span> highlights the titles of the papers cited in the answer in bold blue text. Aaron clicks on these titles, and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.2">vitaLITy 2</span> pops up a function box, including (1) Highlight in the UMAP visualization, (2) Add the paper to the similarity search, and (3) Save the paper to the “Saved Papers” list. Aaron selects one of the papers he is interested in and uses the similarity search function to find other papers similar to it. Next, Aaron saves all of these articles in the “Saved Paper” list.
Finally, Aaron uses the <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">Summarize</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.4">Literature Review</span> features to further review on these saved papers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion, Limitations, &amp; Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We observe a few limitations to our current approach. First, while using LLMs to summarize a set of papers can be a useful starting point for a literature review, the quality of the output is far from sufficient for being included in a paper as-is. The summarization is based on the meta-data that is contained in the <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">vitaLITy 2</span> paper corpus, which does not include full-text of the articles. A potential solution is to optimize the web crawler to also retrieve the full text of papers, segment these texts into appropriately sized chunks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib49" title="">49</a>]</cite> and generate embeddings for these chunks. These embeddings, along with the associated meta-data (title, authors, keywords, abstract, etc.), should then be stored in the vector database. This will ensure more comprehensive and accurate summaries.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Furthermore, some studies also indicate that there is concern within the academic community regarding the development and use of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib18" title="">18</a>]</cite>. The accuracy of response by LLMs depends on the initial training. Potential biases, inaccuracies, or misunderstandings present in the data used for training the models can lead to erroneous outputs. To mitigate this problem, we suggest adding a user prompt in <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">vitaLITy 2</span> to explicitly warn users about the limitations of content generated by <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">vitaLITy 2</span>, thereby reminding and cautioning users to use this tool with care.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Another concern is hallucination from LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib46" title="">46</a>]</cite>. In one instance, when asked to describe the concept of “grounded theory” in Usage Scenario 2 (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#S4" title="4 Usage Scenarios ‣ vitaLITy 2: Reviewing Academic Literature Using Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>), the LLM described a reasonable high-level summary of the approach, but referenced a paper not contained in the <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.1">vitaLITy 2</span> database. Upon searching the title, it referred to a reasonably popular book from Birks and Mills in 2015 with more than 2,000 citations at the time of this writing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib5" title="">5</a>]</cite>. However, when asking for additional relevant papers on the topic of grounded theory (implying that they come from the <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.2">vitaLITy 2</span> corpus), the LLM responded with some paper titles that were neither contained in the <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.3">vitaLITy 2</span> database, nor obvious paper titles outside the database according to a Google Scholar search.
Although completely eliminating hallucinations in LLMs remains challenging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib48" title="">48</a>]</cite>, various strategies exist to reduce their occurrence. As discussed in this paper, actually the RAG architecture is one such approach. RAG mitigates LLMs hallucinations by providing additional contextual information. In future work, we can further reduce hallucinations in RAG by incorporating external knowledge bases from reliable sources, such as Google Scholar search. Additionally, as research on LLMs advances, several potential methods to address the issue of hallucinations in LLMs have emerged.
Recent benchmarks suggest that the incidence of AI hallucination is relatively small for GPT-4 by OpenAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13450v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced a system, <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">vitaLITy 2</span>, for conducting literature review using a RAG architecture, a corpus of more than 66,000 papers from visualization-related venues, and novel features for interacting with the paper corpus through Large Language Models (LLMs). We provide the system as an open-sourced code contribution at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://vitality-vis.github.io" title="">https://vitality-vis.github.io</a>, alongside the paper corpus, and hope to stimulate future work in optimizing literature review methods.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
E. Alexander, J. Kohlmann, R. Valenza, M. Witmore, and M. Gleicher.

</span>
<span class="ltx_bibblock">Serendip: Topic model-driven visual exploration of text corpora.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">2014 IEEE Conference on Visual Analytics Science and Technology (VAST)</span>, pp. 173–182. IEEE, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R. F. Baumeister and M. R. Leary.

</span>
<span class="ltx_bibblock">Writing narrative literature reviews.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Review of general psychology</span>, 1(3):311–320, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E. Becht, L. McInnes, J. Healy, C.-A. Dutertre, I. W. Kwok, L. G. Ng, F. Ginhoux, and E. W. Newell.

</span>
<span class="ltx_bibblock">Dimensionality reduction for visualizing single-cell data using umap.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Nature biotechnology</span>, 37(1):38–44, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Benito-Santos and R. Therón.

</span>
<span class="ltx_bibblock">GlassViz: Visualizing Automatically-Extracted Entry Points for Exploring Scientific Corpora in Problem-Driven Visualization Research.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">2020 IEEE Visualization Conference (VIS)</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Birks and J. Mills.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Grounded theory: A practical guide</span>.

</span>
<span class="ltx_bibblock">Sage, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.-K. Chou and C.-K. Yang.

</span>
<span class="ltx_bibblock">Papervis: Literature review made easy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Computer Graphics Forum</span>, vol. 30, pp. 721–730. Wiley Online Library, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
ChromaDB.

</span>
<span class="ltx_bibblock">The AI-native open-source embedding database.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.trychroma.com" title="">https://www.trychroma.com</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Cohan, S. Feldman, I. Beltagy, D. Downey, and D. S. Weld.

</span>
<span class="ltx_bibblock">Specter: Document-level representation learning using citation-informed transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">ACL</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Dattolo and M. Corbatto.

</span>
<span class="ltx_bibblock">Visualbib: narrative views for customized bibliographies.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">2018 22nd International Conference Information Visualisation (IV)</span>, pp. 133–138. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. El-Arini and C. Guestrin.

</span>
<span class="ltx_bibblock">Beyond keyword search: discovering relevant scientific literature.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">ACM SIGKDD</span>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Models - OpenAI API.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" title="">https://platform.openai.com/docs/models/gpt-3-5-turbo</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. Gu, E. Jun, and T. Althoff.

</span>
<span class="ltx_bibblock">Understanding and supporting debugging workflows in multiverse analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">CHI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. T. Hackstadt and A. D. Malony.

</span>
<span class="ltx_bibblock">Visualizing parallel programs and performance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE CGA</span>, 15(4):12–14, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F. Heimerl, Q. Han, S. Koch, and T. Ertl.

</span>
<span class="ltx_bibblock">Citerivers: Visual analytics of citation patterns.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE TVCG</span>, 22(1):190–199, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. L. Huang, K. Starbird, M. Orand, S. A. Stanek, and H. T. Pedersen.

</span>
<span class="ltx_bibblock">Connected through crisis: Emotional proximity and the spread of misinformation online.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">ACM CSCW</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P. Isenberg, T. Isenberg, M. Sedlmair, J. Chen, and T. Möller.

</span>
<span class="ltx_bibblock">Visualization as seen through its research paper keywords.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE TVCG</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Johnson, M. Douze, and H. Jégou.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. K. Kim, M. Chua, M. Rickard, and A. Lorenzo.

</span>
<span class="ltx_bibblock">Chatgpt and large language model (llm) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Journal of Pediatric Urology</span>, 19(5):598–604, 2023. doi: 10 . 1016/j . jpurol . 2023 . 05 . 018

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Ko, S. Afzal, S. Walton, Y. Yang, J. Chae, A. Malik, Y. Jang, M. Chen, and D. Ebert.

</span>
<span class="ltx_bibblock">Analyzing high-dimensional multivariate network links with integrated anomaly detection, highlighting and exploration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">2014 IEEE conference on visual analytics science and technology (VAST)</span>, pp. 83–92. IEEE, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
LangChain.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.langchain.com/" title="">https://www.langchain.com/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing Systems</span>, vol. 33, pp. 9459–9474. Curran Associates, Inc., 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Maćkiewicz and W. Ratajczak.

</span>
<span class="ltx_bibblock">Principal components analysis (pca).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Computers &amp; Geosciences</span>, 19(3):303–342, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T. R. McIntosh, T. Liu, T. Susnjak, P. Watters, A. Ng, and M. N. Halgamuge.

</span>
<span class="ltx_bibblock">A Culturally Sensitive Test to Evaluate Nuanced GPT Hallucination.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">IEEE Transactions on Artificial Intelligence</span>, pp. 1–13, 2023.

</span>
<span class="ltx_bibblock">Conference Name: IEEE Transactions on Artificial Intelligence. doi: 10 . 1109/TAI . 2023 . 3332837

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. A. Merrill, G. Zhang, and T. Althoff.

</span>
<span class="ltx_bibblock">Multiverse: mining collective data science knowledge from code on the web to suggest alternative analysis approaches.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">ACM SIGKDD</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
N. Muennighoff, N. Tazi, L. Magne, and N. Reimers.

</span>
<span class="ltx_bibblock">Mteb: Massive text embedding benchmark.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2210.07316</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Narechania, A. Karduni, R. Wesslen, and E. Wall.

</span>
<span class="ltx_bibblock">VitaLITy: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">IEEE TVCG</span>, Jan. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al.

</span>
<span class="ltx_bibblock">Text and code embeddings by contrastive pre-training, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">New and improved embedding model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/new-and-improved-embedding-model" title="">https://openai.com/blog/new-and-improved-embedding-model</a>, 2022.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Palani, A. Naik, D. Downey, A. X. Zhang, J. Bragg, and J. C. Chang.

</span>
<span class="ltx_bibblock">Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">ACM CHI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning.

</span>
<span class="ltx_bibblock">GloVe: Global Vectors for Word Representation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">ACM EMNLP</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. D. Manning.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">ACM EMNLP</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Sah, R. Mitra, A. Narechania, A. Endert, J. Stasko, and W. Dou.

</span>
<span class="ltx_bibblock">Generating Analytic Specifications for Data Visualization from Natural Language Queries using Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">NLVIZ Workshop (IEEE VIS)</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Sarma, A. Kale, M. J. Moon, N. Taback, F. Chevalier, J. Hullman, and M. Kay.

</span>
<span class="ltx_bibblock">multiverse: Multiplexing alternative data analyses in r notebooks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">ACM CHI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
B. Schindler, J. Waser, H. Ribičić, R. Fuchs, and R. Peikert.

</span>
<span class="ltx_bibblock">Multiverse data-flow control.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">IEEE TVCG</span>, 19(6):1005–1019, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. Snyder.

</span>
<span class="ltx_bibblock">Literature review as a research methodology: An overview and guidelines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Journal of Business Research</span>, 104:333–339, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
N. Sultanum, C. Murad, and D. Wigdor.

</span>
<span class="ltx_bibblock">Understanding and Supporting Academic Literature Review Workflows with LitSense.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the International Conference on Advanced Visual Interfaces</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
N. Sultanum and A. Srinivasan.

</span>
<span class="ltx_bibblock">Datatales: Investigating the use of large language models for authoring data-driven articles.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">2023 IEEE Visualization and Visual Analytics (VIS)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Tang, M. Qu, and Q. Mei.

</span>
<span class="ltx_bibblock">PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">ACM SIGKDD</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Tian, W. Cui, D. Deng, X. Yi, Y. Yang, H. Zhang, and Y. Wu.

</span>
<span class="ltx_bibblock">Chartgpt: Leveraging LLMs to generate charts from abstract natural language.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">IEEE TVCG</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
L. Van der Maaten and G. Hinton.

</span>
<span class="ltx_bibblock">Visualizing data using t-sne.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Journal of machine learning research</span>, 9(11), 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
X. Wang, S. L. Huey, R. Sheng, S. Mehta, and F. Wang.

</span>
<span class="ltx_bibblock">SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2404.13765" title="">https://doi.org/10.48550/arXiv.2404.13765</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y. Wang, M. Yu, G. Shan, H.-W. Shen, and Z. Lu.

</span>
<span class="ltx_bibblock">Vispubcompas: a comparative analytical system for visualization publication data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Journal of Visualization</span>, 22(5):941–953, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. Wilkins, J. Järvi, A. Jain, G. Kejriwal, A. Kerne, and V. Gumudavelly.

</span>
<span class="ltx_bibblock">Evolutionworks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">IFIP Conference on Human-Computer Interaction</span>, pp. 213–230. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
S. Wu, Y. Zhao, F. Parvinzamir, N. Ersotelos, S. Wei, and F. Dong.

</span>
<span class="ltx_bibblock">Literature Explorer: effective retrieval of scientific documents through nonparametric thematic topic detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">The Visual Computer</span>, 36, July 2020. doi: 10 . 1007/s00371-019-01721-7

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. Wu, M. Terry, and C. J. Cai.

</span>
<span class="ltx_bibblock">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">ACM CHI</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Z. Xu, S. Jain, and M. Kankanhalli.

</span>
<span class="ltx_bibblock">Hallucination is inevitable: An innate limitation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2401.11817</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J.-Y. Yao, K.-P. Ning, Z.-H. Liu, M.-N. Ning, and L. Yuan.

</span>
<span class="ltx_bibblock">LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples, Oct. 2023. doi: 10 . 48550/arXiv . 2310 . 01469

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J.-Y. Yao, K.-P. Ning, Z.-H. Liu, M.-N. Ning, and L. Yuan.

</span>
<span class="ltx_bibblock">Llm lies: Hallucinations are not bugs, but features as adversarial examples.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2310.01469</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A. J. Yepes, Y. You, J. Milczek, S. Laverde, and L. Li.

</span>
<span class="ltx_bibblock">Financial report chunking for effective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2402.05131</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
T. Yoon, H. Han, H. Ha, J. Hong, and K. Lee.

</span>
<span class="ltx_bibblock">A Conference Paper Exploring System Based on Citing Motivation and Topic.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">IEEE PacificVis</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2303.18223</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 24 03:14:24 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
