<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2103.02937] Visual Question Answering: which investigated applications?</title><meta property="og:description" content="Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering: which investigated applications?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering: which investigated applications?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2103.02937">

<!--Generated on Sat Mar  2 06:47:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Visual Question Answering: which investigated applications?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Silvio <span id="id1.1.id1" class="ltx_text" style="color:#FF0000;">Barra</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:silvio.barra@unina.it">silvio.barra@unina.it</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Carmen <span id="id2.1.id1" class="ltx_text" style="color:#FF0000;">Bisogni</span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria <span id="id3.1.id1" class="ltx_text" style="color:#FF0000;">De Marsico</span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefano <span id="id4.1.id1" class="ltx_text" style="color:#FF0000;">Ricciardi</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Naples Federico II, Naples, Italy
</span>
<span class="ltx_contact ltx_role_address">University of Salerno, Salerno, Italy
</span>
<span class="ltx_contact ltx_role_address">Sapienza University of Rome, Rome, Italy
</span>
<span class="ltx_contact ltx_role_address">University of Molise, Campobasso, Italy
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>

</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">MSC: </h6>41A05, 41A10, 65D05, 65D17
<span id="id6.id1" class="ltx_ERROR undefined">\KWD</span>Visual Question Answering, Real-world VQA, VQA for medical applicatons, VQA for assistive applications, VQA for context awareness, VQA in cultural heritage and education

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Pattern Recognition Letters</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is at present one of the most interesting joint applications of Artificial Intelligence (AI) to Computer Vision (CV) and Natural Language Processing (NLP). Its purpose is to achieve systems capable of answering different types of questions expressed in natural language and regarding any image. To this aim, a VQA system relies on algorithms of different nature that jointly take as input an image and a natural language question about it and generate a natural language answer as output. Humans naturally succeed in this, except for special conditions, and AI aims at reproducing this ability. The role of NLP in solving this multi-disciplinary problem is to understand the question, and of course to generate the answer according to the results obtained by CV. Text-based Q&amp;A is a longer studied problem in NLP. The difference with VQA is that both search and reasoning regard the content of an image. A classification of CV tasks entailed by VQA can be found in the recent survey in <cite class="ltx_cite ltx_citemacro_cite">Manmadhan and Kovoor (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> and summarized in the right part of figure <a href="#S1.F1" title="Fig. 1 ‣ 1 Introduction ‣ Visual Question Answering: which investigated applications?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where we also shown the NLP tasks involved in this case. On the left part of the figure, the overall workflow of a classical VQA framework can be summarized with the proposed example.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2103.02937/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="531" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 1: </span>An example of a workflow of VQA method on the left, some of the CV tasks and NLP tasks involved in VQA are on the right.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA research crosses several AI areas, including CV, NLP and also Knowledge Representation &amp; Reasoning (KR), the latter being able to reason and extract semantics from processed media. Several surveys discuss VQA approaches from different points of view. Among the most recent ones, <cite class="ltx_cite ltx_citemacro_cite">Manmadhan and Kovoor (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> propose an extensive analysis and comparison, among other aspects, of different methodologies underlying the different steps of VQA including featurization, for both image and question (Phase I), and joint comprehension of image and question features to generate a correct answer (Phase II). <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib58" title="" class="ltx_ref">2019</a>)</cite> devote special attention to fusion techniques adopted in Phase II, distinguishing between fusion techniques for image QA and for video QA. Similarly, <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib56" title="" class="ltx_ref">2017</a>)</cite> classify methods by their strategy to connect the visual and textual modalities and, in particular, it examines the approach of combining convolutional and recurrent neural networks to map images and questions onto a common feature space. Looking at these surveys, it is possible to observe that their attention is focused on methodological proposals, generally neglecting the possible application domains. The latter are only shortly listed as those where the VQA can be useful. The set mentioned by <cite class="ltx_cite ltx_citemacro_cite">Manmadhan and Kovoor (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> includes: to help blind users to communicate through pictures, to attract customers of online shopping sites by giving ”semantically” satisfying results for their search queries, to allow learners engaged in educational services to interact with images, to help the analysts in surveillance data analysis to summarize the available visual data. The authors also hypothesize as Visual Dialogue, envisaged as a successor of VQA, can even be used to give natural language instructions to robots. A similar list is presented in <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib58" title="" class="ltx_ref">2019</a>)</cite>: blind person assistance (the most popular according to the citations achieved), autonomous driving, smart camera processing on food images, implementation of robot tutors with the function of automatic math problem solvers, execution of trivial tasks such as ”<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">spotting an empty picnic table at the park, or locating the restroom at the other end of the room without having to ask</span>.” A more general use is mentioned by <cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite> for advanced image retrieval. Without using image meta-data or tags, it could be possible, e.g., to find all images taken in a certain setting: one might simply ask ‘Is it raining?’ for all images in the dataset without using image annotations. The general-purpose nature of most VQA-related literature is also reflected by the datasets exploited as benchmarks: even in the review papers, the surveyed datasets are mostly general-purpose ones, where images are possibly classified in natural, clip art or synthetic as in <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib56" title="" class="ltx_ref">2017</a>)</cite>, with no reference to a specific source domain. Only the work in <cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite> focuses, among the other topics, on criticizing some current popular datasets with regard to their ability to properly train and assess VQA algorithms, and on proposing new features for future VQA benchmarks. Among the others, the authors mention larger size and also lower amount of bias, since current VQA systems are considered more dependent on the questions and how they are phrased than the image content. The additional requirement explored here is that the dataset used for performance evaluation should also be related to the VQA domain, if this presents specific conditions. As a matter of fact, most works (relatively fewer than general-purpose ones) tackling a specific application domain also propose suitable related datasets.
<br class="ltx_break"></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The aim of the present paper is to survey VQA proposals from a novel point of view, and to investigate at which extent different application domains inspire different kinds of questions and call for different benchmarks and/or approaches. An extensive literature search reveals that relatively few papers have tackled specific domains. The following sections will focus on these. The most popular special application of VQA in literature is the support for automatic intelligent medical diagnosis, that deserves a large section. It encompasses different kinds of problems, characterized by different kinds of image capture technologies and image content types. The aid to blind and visually impaired individuals follows, enabling them to get information about images both on the web and in the real world, e.g., in advanced domotics. A kind of anticipation though without implementation is already envisaged in <cite class="ltx_cite ltx_citemacro_cite">Muñoz et al. (<a href="#bib.bib38" title="" class="ltx_ref">2006</a>)</cite>. It is worth noticing that these two domains have inspired non only the collection of specific ad-hoc datasets, but also their use as benchmarks in domain-related international challenges. A much lower number of devoted works deal with unattended surveillance, with proposals for systems able to relief a human operator from the burden of continuous attention and to raise an alarm in anomalous situations. Social and cultural purposes inspire systems addressing advanced education and personalized fruition of cultural heritage, and smart and customer-tailored advertisement. The paper will finally report about some very recent works focusing on the novelty of either the kind of data taken into account or of the new approaches to questioning/answering.
<br class="ltx_break"></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The paper proceeds as follows. Sections from <a href="#S2" title="2 Medical VQA ‣ Visual Question Answering: which investigated applications?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to <a href="#S6" title="6 VQA and Advertising ‣ Visual Question Answering: which investigated applications?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> present recent works in the domains of medical VQA, support for blind people, video surveillance, education and cultural heritage, and advertising; Section <a href="#S7" title="7 Emerging challenges/Misc ‣ Visual Question Answering: which investigated applications?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents emerging approaches for new kinds of data and new questioning/answering strategies. Section <a href="#S8" title="8 Discussion and conclusions ‣ Visual Question Answering: which investigated applications?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> briefly points out some concluding remarks.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2103.02937/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="332" height="383" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 2: </span>Examples of various VQA applications: (1) Medical image from ImageCLEF 2018 VQA-Med (<cite class="ltx_cite ltx_citemacro_cite">Abacha et al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>); (2) For Visually impaired people, from VizWiz Dataset (<cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>); (3) Video Surveillance image from BOAR Dataset (<cite class="ltx_cite ltx_citemacro_cite">Toor et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019b</a>)</cite>); (4) Image of a Painting from Artpedia (<cite class="ltx_cite ltx_citemacro_cite">Stefanini et al. (<a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>); (5) Advertising image from <cite class="ltx_cite ltx_citemacro_cite">Hussain et al. (<a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Medical VQA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">AI-based medical image understanding and related medical questions-answering (from here on, med-VQA) is recently attracting increasing interest by researchers. In fact, this topic is opening new scenarios for supporting medical staff in taking clinical decision, as well as for enhanced diagnosis through computer-based ”second opinion”. However, the experimentation of any approach is conditioned by the availability of a dedicated database including medical images, of possibly specific type, and related QA pairs. These requirements have been first addressed by the ImageCLEF 2018 evaluation campaign for the Medical Domain Visual Question Answering pilot task, as described in <cite class="ltx_cite ltx_citemacro_cite">Hasan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>. The related first med-VQA public dataset included a total of 2,866 medical images, 2,278 of which used for training, 264 for testing and 324 for validation, along with 6,413 QA pairs. In The following ImageCLEF 2019 edition (<cite class="ltx_cite ltx_citemacro_cite">Abacha et al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>), a larger dataset containing 4,200 radiology images as well as 15,992 QA pairs was released, with a wide variety of imaging modalities, type of organs and pathologies. All the works resumed in the following have based the reported experiments on one of the aforementioned datasets. More recently, the introduction of two new datasets, namely VQA-RAD presented in <cite class="ltx_cite ltx_citemacro_cite">Lau et al. (<a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite>, and PathVQA described in <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>, promises to further improve the variety and specificity of training and test samples for this challenging declination of VQA. A broad range of deep frameworks has been proposed to address the requirements of med-VQA. The authors of <cite class="ltx_cite ltx_citemacro_cite">Vu et al. (<a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite> propose a med-VQA deep learning approach exploiting a multimodal question-centric strategy fusing together the image and the written question in the query, assigning a greater fusion weight to the latter. The fusion mechanism combines question and image features to achieve maximum adherence to the query sentence. The answer to the query can be of different types, ranging from binary and numbers to short sentences. The achieved accuracy exceed state-of-the-art. In <cite class="ltx_cite ltx_citemacro_cite">Ren and Zhou (<a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>, a novel method is presented to break the complex med-VQA problem down into multiple simpler problems through a classification and generative model. To this aim, the proposed model uses data augmentation as well as text tokenization, switching between classification and generative models by changing both output layer and loss function while retaining the core component. The generative model is built masking position by position instead of using an encoder-decoder framework. Transfer learning and multi-task learning within a modular pipeline architecture are used in <cite class="ltx_cite ltx_citemacro_cite">Kornuta et al. (<a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite> to cope with the wide variety of images in the ImageCLEF-2019 dataset by extracting its inherent domain knowledge. The proposed Cross Facts Network basically exploits upstream tasks to cross-utilize information useful to increase the precision on more complex downstream tasks. This results in a clear score improvement on the validation set. On a similar line of research the authors of <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite> propose ETM-Trans, a deep transfer learning approach based on embedded topic modelling applied to textual questions, through which topic labels are associated to medical images for fine tuning the pre-trained ImageNet model. A co-attention mechanism is also exploited, where residual networks is used to provide fine-grained contextual information for answer derivation. In <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite> a CNN based Inception-Resnet-v2 model is used to extract image features along with a RNN based Bi-LSTM model to encode questions. The concatenation of image features and coded questions is therefore used to generate the answers. A normalization step, including both image enhancement techniques and questions lemmatization is performed beforehand. The shortage of large labeled datasets to effectively train deep learning models for med-VQA is the focus of <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>. The authors explore the use of an unsupervised denoising auto-encoder to leverage the availability of large quantities of unlabelled medical images to achieve trained weights that can be more easily adapted to the med-VQA domain. Moreover, they also exploit supervised meta-learning to learn meta-weights which can adapt to the domain of interest requiring only a small labeled training set. The authors of <cite class="ltx_cite ltx_citemacro_cite">Lubna et al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite> present a convolutional neural-network-based med-VQA system, aimed at providing answers according to input image modalities such as X-ray, computer-tomography, magnetic resonance, ultrasound, positron emission tomography etc. where the image modality can also be identified by the system. On a similar line of research, in <cite class="ltx_cite ltx_citemacro_cite">Bghiel et al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite> a CNN is used to process medical image queries along with a RNN encoder-decoder model to encode image and question input vectors and to decode the states required to predict target answers. These are generated in natural language as output by means of greedy search algorithm. An encoder-decoder model is also at the core of <cite class="ltx_cite ltx_citemacro_cite">Allaouzi et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. Here a pre-trained CNN model is used in the encoding step along with LSTM model to embed textual data. Another deep learning inspired approach is the one proposed in <cite class="ltx_cite ltx_citemacro_cite">Allaouzi and Ahmed (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> where a combination of CNN and bi-directional Long Short Term Memory (LSTM) coupled with a decision tree classifier is used to address the med-VQA problem in terms of a multi-label classification task. According to this method, each label is associated to a unique word among those included in the answer dictionary previously built upon the training set. Similarly, the authors of <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite> exploit residual networks of a deep learning framework to extract visual features from the input image as a result of its interaction with LSTM representation of the question. The goal is to achieve small granularity context data useful to derive the answer. Efficient visual-textual feature integration is achieved through Multi-modal Factorized High-order as well as Multi-modal Factorized Bilinear pooling. LSTM and Support Vector Machine (SVM) are explored in <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>. LSTM is used for extracting textual features from questions along with image features thanks to transfer learning and co-attention mechanism. An SVM based model is trained to predict what category a question belongs to, providing an additional feature. All the resulting features are then efficiently integrated by means of a multi-modal factorized high-order pooling technique. <cite class="ltx_cite ltx_citemacro_cite">Al-Sadi et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> propose a med-VQA model based on differently specialized sub-models, each optimized to answer to a specific class of questions. Considered image classification sub-models include ”modality”, ”abnormality”, ”organ systems” and ”plane” that are defined through pre-trained VGG16 network. Since questions related to each type are repetitive, the approach is not based on them to predict the answers, yet they are used to choose the best suited model to produce the answers and their format. A CNN based on VGG16 network is also exploited in <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite> along with global average pooling strategy to extract medical image features by means of a small-size training set. A BERT model is used to encode the semantics behind the input question and then a co-attention mechanism enhanced by jointly learned attention is used for feature fusion. A bilinear model aimed at grouping and synthesizing extracted image and question features for med-VQA is proposed by <cite class="ltx_cite ltx_citemacro_cite">Vu et al. (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>. This model exploits an attention-based scheme to restrict on relevant input context, instead of relying on additional training data. Additionally, the method is also boosted by an ensemble of trained models. On a different line of research, image captioning and machine translation are explored by the authors of <cite class="ltx_cite ltx_citemacro_cite">Ambati and Dudyala (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, aiming at generating an answer to the image-question pair in terms of a sequence of words. Image captioning requires an accurate image understanding, and similarly machine translation requires an accurate comprehension of the input sequence to effectively translate it. This approach provided the highest accuracy scores for the ImageCLEF 2018 challenge. Stacked attention Network (SAN) along with Multimodal Compact Bilinear Pooling (MCB) VQA models are used in <cite class="ltx_cite ltx_citemacro_cite">Abacha et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>. In this approach, both models rely on CNNs for image processing, respectively VGG-16 for SAN and ResNet-152 for MCB, while LSTMs are exploited for question processing. Their final hidden layer provides question vector extraction. In <cite class="ltx_cite ltx_citemacro_cite">Bansal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, the proposed med-VQA pipeline partitions questions into two classes. The first one requires answers to come from a fixed pool of categories, while the second one requires to generate answers based on abnormal visual features in the input image. The first class is defined by using Universal Sentence Encoder question embeddings and ResNet image embeddings, feeding an attention-based mechanism to generate answers. The second class uses the same ResNet image embedding along with word embeddings from a Word2Vec model. This is pre-trained on PubMed data which is used as an input to a sequence to sequence model which generates descriptions of abnormalities.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VQA for visually impaired people</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Assistance to blind people is among the objectives of several VQA applications proposed in the recent years. This is mainly due to the ability of automatic VQA to answer daily questions which may help visually impaired people to live without visual barriers. During the last ten years there has been a quite fast evolution which led from needing the aid from volunteers or workers paid for answering blind’s questions <cite class="ltx_cite ltx_citemacro_citep">(Bigham et al., <a href="#bib.bib14" title="" class="ltx_ref">2010a</a>; Lasecki et al., <a href="#bib.bib30" title="" class="ltx_ref">2013</a>)</cite>, to the automatic analysis of images and related questions, to extract and generate the proper answers. To this aim, the dataset proposed and described in <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> contains 31000 visual questions originated by blind people who took a photo with their mobile phone and recorded a spoken question about it. Each image is labelled with 10 crowdsourced answers. A ”privacy preserving” version of the dataset is released <cite class="ltx_cite ltx_citemacro_citep">(Gurari et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, where image private regions are removed (credit card numbers, subject information on medical prescription, etc.). An iPhone application, named VizWiz <cite class="ltx_cite ltx_citemacro_cite">Bigham et al. (<a href="#bib.bib15" title="" class="ltx_ref">2010b</a>)</cite> allows asking a visual question and obtaining an answer in nearly real time. The authors of <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> propose a combined bottom-up and top-down attention mechanisms in which the question is analyzed by means of a GRU and the image is processed by a CNN. The vectors are then combined to produce a set of scores over the candidate answers. The bottom-up mechanism is based on a Faster-RCNN, which submits to the model the image regions together with the labels, while the top-down mechanism weighs the image features by applying a weighted sum with the GRU output. Another interesting application aimed at helping blind people is described in <cite class="ltx_cite ltx_citemacro_cite">Weiss et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>)</cite>, in which the authors exploit a reinforcement learning model in order to help a blind person to navigate the street.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>VQA in Video Surveillance scenarios</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The adoption of a VQA approach in video surveillance scenarios may help operators to enhance the understanding of a scene, thus helping them to take fair and faster decisions. The authors of <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> propose a complete platform called ISEE for parsing large video surveillance data. The platform is organized in three modules, which are distributed on both CPU and GPU cluster: (i) detection and tracking module, (ii) attribute recognition module, and (iii) re-identification module. The first module exploits a Gaussian Mixture Model for the analysis on the CPU and a Single Shot multibox detector with a Faster R-CNN for the detection on the GPU. Both use the Nearest Neighbor-based tracker. The second module exploits DeepMAR and LSPR_attr for the attribute recognition; the third exploits LSPR_ReId and MSCAN for re-identification. The system has been tested over the RAP dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>. The authors of <cite class="ltx_cite ltx_citemacro_cite">Toor et al. (<a href="#bib.bib49" title="" class="ltx_ref">2019a</a>)</cite> mostly focus on the soft biometric aspects of the Q&amp;A in video surveillance, thus proposing C2VQA-BOARS (Context and Collaborative Visual Question Ansqering for Biometric Object-Attribute Relevance and Surveillance). The system answers a question by fusing information from the question itself with the caption obtained by an analysis of the image. Three models are proposed: (i) C2VQA-All uses a set of BiLSTMS to encode question and caption; four equally weighted training objectives are used to train the model: question relevance, type of object in the question, the attributes of the object and the final classification for the relevance of the object; (ii) C2VQA-Image takes a set of GloVe word-embeddings and uses a 2-layer LSTM for question encoding; the question is combined with the dense vector obtained by feeding a pre-trained ResNet-50 model with the considered image; (iii) C2VQA-Rel is similar to C2VQA-All, but only takes the binary relevance of the question and the final classification of the object attribute relevance. More conceptual approaches can be found in <cite class="ltx_cite ltx_citemacro_cite">Katz et al. (<a href="#bib.bib27" title="" class="ltx_ref">2003</a>)</cite> and in <cite class="ltx_cite ltx_citemacro_cite">Tu et al. (<a href="#bib.bib51" title="" class="ltx_ref">2014</a>)</cite>. In the former, the authors propose a system for supporting the question answering operation about moving objects in videos, by filtering the trajectory information of the objects (people and vehicles), and representing the movements by means of a structured annotation. These annotations can be easily navigated for obtaining answers over movement-related questions (”<span id="S4.p1.1.1" class="ltx_text ltx_font_italic">Which direction is the red car going?</span>”, ”<span id="S4.p1.1.2" class="ltx_text ltx_font_italic">Did any cars leave the garage?</span>”…). The latter work, instead, proposes an ontology/graph based taxonomy schema for describing events in the video and associated captions. A probabilistic generative model is then used for capturing the relations between input video and input question.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>VQA Education and cultural heritage</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">One of the main aspects of VQA is its high correlation to human perception.
Even if a VQA system can focus the attention on different parts of an image compared to humans, it is proven that devising a VQA architecture ”interested” in the same image parts is possible <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>.
The inverse process can be also be carried out.
<cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> developed and tested an educational robot using VQA to formulate questions and start an educational dialog taking inspiration from the surrounding environment, using a faster R-CNN. This system shows a great ability to improve the children’s desire to explore. VQA can improve such desire for adults too, in particular for cultural heritage. <cite class="ltx_cite ltx_citemacro_cite">Bongini et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> propose to explore museums and art galleries using VQA to interact with an audio-guide. The authors use many classical VQA datasets and a cultural dataset named Artpedia <cite class="ltx_cite ltx_citemacro_citep">(Stefanini et al., <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite> to feed two BERT modules, for question classification and answering, and a faster R-CNN for the VQA module. They annotated 30 images of Artpedia with 3 or 5 Q&amp;A to perform tests. As a result, the user can directly ask questions he/she is interested in, avoiding long descriptions and freely navigating through the elements of the painting or the sculpture. This way to explore art can replace static audio-guides and the growing interest in this goal has inspired to the construction of a dedicated dataset <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al., <a href="#bib.bib45" title="" class="ltx_ref">2016</a>)</cite>. This dataset is focused on the old-Egiptyan Amarna period and contains 16 artworks, 805 questions, 204 documents, 101 related documents and 139 related paragraphs in English. This dataset is very specific and quite limited, but it is an interesting starting point to apply VQA in the service of art.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>VQA and Advertising</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Advertising is something strongly related to image understanding. A user looking at an advertise not only sees the objects inside the scene, but also the related text and the relations among the objects, and interprets all such information within a precise cultural context. An advertise must be quite simple to be understandable for the greatest number of people and at the same time interesting and eye-catching.
No surprise than that VQA can find a challenging field of application in advertising.
The first task to complete is using VQA to understand the advertise and, in particular, the underlying communicative strategy. <cite class="ltx_cite ltx_citemacro_cite">Hussain et al. (<a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> present two datasets for this purpose, one with images and one with videos. The image dataset contain 64832 ads. For each advertise there is a set of Q&amp;A about what the client is led to do by it, for a total of 202090 elements with 38 topics, 30 sentiments and 221 symbols. The video dataset has 3477 elements, with 3 or 5 questions per video and the same symbols, sentiments and topics of images. The authors use a two-layer LSTM and VGGNet to decode images ads and 152-layer ResNets to decode video ads.
Once an automatic system is able to understand the meaning of an advertise, it is natural to ask whether it is possible to automatically choose which ads to show.
The authors of <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> focus their research on predicting the users’ preferences by understanding what impress them most. They built Real-ad, a dataset of 3747 images with 40 attributes and collected about 500 millions of impressions from the users. They include VQA in a low level fusion using LSTM, followed by an attention mechanism and a high level fusion to emphasize the relations between visual and auxiliary information. The result is an attention heatmap that shows the parts of the images from which the potential client is attracted.
Finally, the most challenging step will be to automatically find the most successful ads to help advertising designers.
<cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite> propose a way to use VQA to extract relevant information about past campaigns in multi-source data as texts and images. The authors use 64000 images from the dataset in <cite class="ltx_cite ltx_citemacro_cite">Hussain et al. (<a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> and extract information, also searching online, analyse keyphrases and generate new possible advertises. To this aim, they use a cross-modality encoder architecture followed by a feed forward network. Even if not exploded yet, this task shows an interesting research directions.</p>
</div>
<figure id="S6.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>A summary of application domains tackled by VQA literature with a summary of the features of the datasets used as benchmarks. Also, the approaches are reported, with the best results for each dataset. Following a legend for interpret the table:
Dataset: C:Classes, L:Labels.
Dataset Size: AS: Audio Scenes, C:Classes, T:Text, HoV:Hours of Video, I:images, Ic:Icons, V:Videos.
Notes: A:Answers, At:Attributes, C:Classes, Cap:Caption, I:Images, Imp:Impression, Q:Questions.
Approaches: AM:Attention Mechanism, BM:Bert Model, CM:Classifier Model, DN:DenseNet, ED:Encoder-Decoder, F:Fusion, FRCNN: Faster-RCNN, GM:Generative Model, IA: Image Attention, IN:ImageNet, IV2: InceptionV2, MLA:MultiLevel Attention, MMFN:Multi-Step Modality Fusion Network, QCM:Question Classifier Model, RM:Reasoning Module, RN:ResNet, TE:Text Embedding, WE:Word Embedding,W2V:Word2Vec.
Best Results: ACC:Accuracy, BLEU:Bilingual Evaluation Understudy Score: TDA:Task Dendendant Accuracy, WBSS:Word-Based Semantic Similarity</figcaption>
<table id="S6.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T1.8.9.1" class="ltx_tr">
<th id="S6.T1.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.1.1" class="ltx_text" style="font-size:70%;">Domain</span></th>
<th id="S6.T1.8.9.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.2.1" class="ltx_text" style="font-size:70%;">Reference(s)</span></th>
<td id="S6.T1.8.9.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.3.1" class="ltx_text" style="font-size:70%;">Dataset</span></td>
<td id="S6.T1.8.9.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.4.1" class="ltx_text" style="font-size:70%;">Dataset Size</span></td>
<td id="S6.T1.8.9.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.5.1" class="ltx_text" style="font-size:70%;">Notes</span></td>
<td id="S6.T1.8.9.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.6.1" class="ltx_text" style="font-size:70%;">Approach</span></td>
<td id="S6.T1.8.9.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.9.1.7.1" class="ltx_text" style="font-size:70%;">Best Result</span></td>
</tr>
<tr id="S6.T1.8.10.2" class="ltx_tr">
<th id="S6.T1.8.10.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S6.T1.8.10.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Vu et al. <span id="S6.T1.8.10.2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib53" title="" class="ltx_ref">2020</a><span id="S6.T1.8.10.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.10.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.10.2.3.1" class="ltx_text" style="font-size:70%;">ImageCLEF 2019</span></td>
<td id="S6.T1.8.10.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.10.2.4.1" class="ltx_text" style="font-size:70%;">3200 I</span></td>
<td id="S6.T1.8.10.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.10.2.5.1" class="ltx_text" style="font-size:70%;">12,792 Q&amp;A</span></td>
<td id="S6.T1.8.10.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.10.2.6.1" class="ltx_text" style="font-size:70%;">RN152+BM (AM+F)</span></td>
<td id="S6.T1.8.10.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Ren and Zhou <span id="S6.T1.8.10.2.7.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">(</span><a href="#bib.bib42" title="" class="ltx_ref">2020</a><span id="S6.T1.8.10.2.7.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">)</span></cite></td>
</tr>
<tr id="S6.T1.8.11.3" class="ltx_tr">
<th id="S6.T1.8.11.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.11.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Ren and Zhou <span id="S6.T1.8.11.3.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib42" title="" class="ltx_ref">2020</a><span id="S6.T1.8.11.3.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.11.3.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.11.3.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.11.3.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.11.3.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.11.3.6.1" class="ltx_text" style="font-size:70%;">RN152+TE (GM)</span></td>
<td id="S6.T1.8.11.3.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.11.3.7.1" class="ltx_text" style="font-size:70%;">ACC: 64%</span></td>
</tr>
<tr id="S6.T1.8.12.4" class="ltx_tr">
<th id="S6.T1.8.12.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.12.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Allaouzi et al. <span id="S6.T1.8.12.4.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib7" title="" class="ltx_ref">2019</a><span id="S6.T1.8.12.4.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.12.4.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.12.4.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.12.4.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.12.4.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.12.4.6.1" class="ltx_text" style="font-size:70%;">DN121+LSTM (ED)</span></td>
<td id="S6.T1.8.12.4.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.12.4.7.1" class="ltx_text" style="font-size:70%;">BLEU: 65,9%</span></td>
</tr>
<tr id="S6.T1.8.13.5" class="ltx_tr">
<th id="S6.T1.8.13.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.13.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Bansal et al. <span id="S6.T1.8.13.5.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib11" title="" class="ltx_ref">2019</a><span id="S6.T1.8.13.5.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.13.5.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.13.5.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.13.5.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.13.5.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.13.5.6.1" class="ltx_text" style="font-size:70%;">VQACM (Seq-to-Seq)</span></td>
<td id="S6.T1.8.13.5.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.14.6" class="ltx_tr">
<th id="S6.T1.8.14.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.14.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Al-Sadi et al. <span id="S6.T1.8.14.6.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib5" title="" class="ltx_ref">2019</a><span id="S6.T1.8.14.6.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.14.6.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.14.6.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.14.6.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.14.6.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.14.6.6.1" class="ltx_text" style="font-size:70%;">QCM+VGG</span></td>
<td id="S6.T1.8.14.6.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.15.7" class="ltx_tr">
<th id="S6.T1.8.15.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.15.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Bghiel et al. <span id="S6.T1.8.15.7.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib12" title="" class="ltx_ref">2019</a><span id="S6.T1.8.15.7.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.15.7.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.15.7.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.15.7.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.15.7.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.15.7.6.1" class="ltx_text" style="font-size:70%;">RN50+W2V (ED)</span></td>
<td id="S6.T1.8.15.7.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.16.8" class="ltx_tr">
<th id="S6.T1.8.16.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.16.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Lubna et al. <span id="S6.T1.8.16.8.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib36" title="" class="ltx_ref">2019</a><span id="S6.T1.8.16.8.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.16.8.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.16.8.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.16.8.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.16.8.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.16.8.6.1" class="ltx_text" style="font-size:70%;">IN+NLP</span></td>
<td id="S6.T1.8.16.8.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.17.9" class="ltx_tr">
<th id="S6.T1.8.17.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.17.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Vu et al. <span id="S6.T1.8.17.9.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib52" title="" class="ltx_ref">2019</a><span id="S6.T1.8.17.9.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.17.9.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.17.9.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.17.9.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.17.9.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.17.9.6.1" class="ltx_text" style="font-size:70%;">RN152+BM (AM)</span></td>
<td id="S6.T1.8.17.9.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.18.10" class="ltx_tr">
<th id="S6.T1.8.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.8.18.10.1.1" class="ltx_text" style="font-size:70%;">Medical</span></th>
<th id="S6.T1.8.18.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Yan et al. <span id="S6.T1.8.18.10.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib57" title="" class="ltx_ref">2019</a><span id="S6.T1.8.18.10.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.18.10.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.18.10.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.18.10.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.18.10.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.18.10.6.1" class="ltx_text" style="font-size:70%;">VGG16+Enc (Co-AM)</span></td>
<td id="S6.T1.8.18.10.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.19.11" class="ltx_tr">
<th id="S6.T1.8.19.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.19.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Shi et al. <span id="S6.T1.8.19.11.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib46" title="" class="ltx_ref">2019</a><span id="S6.T1.8.19.11.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.19.11.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.19.11.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.19.11.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.19.11.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.19.11.6.1" class="ltx_text" style="font-size:70%;">RN152+LSTM (Co-AM+MFH)</span></td>
<td id="S6.T1.8.19.11.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.20.12" class="ltx_tr">
<th id="S6.T1.8.20.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.20.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Kornuta et al. <span id="S6.T1.8.20.12.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib28" title="" class="ltx_ref">2019</a><span id="S6.T1.8.20.12.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.20.12.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.20.12.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.20.12.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.20.12.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.20.12.6.1" class="ltx_text" style="font-size:70%;">ED+RM</span></td>
<td id="S6.T1.8.20.12.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.21.13" class="ltx_tr">
<th id="S6.T1.8.21.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.21.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite">Zhou et al. <span id="S6.T1.8.21.13.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib59" title="" class="ltx_ref">2018</a><span id="S6.T1.8.21.13.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite><span id="S6.T1.8.21.13.2.3" class="ltx_text" style="font-size:70%;">,</span>
</th>
<td id="S6.T1.8.21.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.21.13.3.1" class="ltx_text" style="font-size:70%;">ImageCLEF 2018</span></td>
<td id="S6.T1.8.21.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.21.13.4.1" class="ltx_text" style="font-size:70%;">2866 I</span></td>
<td id="S6.T1.8.21.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.21.13.5.1" class="ltx_text" style="font-size:70%;">6,413 Q&amp;A</span></td>
<td id="S6.T1.8.21.13.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.21.13.6.1" class="ltx_text" style="font-size:70%;">IV2+BiLSTM (AM)</span></td>
<td id="S6.T1.8.21.13.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Ambati and Dudyala <span id="S6.T1.8.21.13.7.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib8" title="" class="ltx_ref">2018</a><span id="S6.T1.8.21.13.7.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
</tr>
<tr id="S6.T1.8.22.14" class="ltx_tr">
<th id="S6.T1.8.22.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.22.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Allaouzi and Ahmed <span id="S6.T1.8.22.14.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib6" title="" class="ltx_ref">2018</a><span id="S6.T1.8.22.14.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.22.14.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.22.14.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.22.14.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.22.14.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.22.14.6.1" class="ltx_text" style="font-size:70%;">VGG16+BDLSTM</span></td>
<td id="S6.T1.8.22.14.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.22.14.7.1" class="ltx_text" style="font-size:70%;">BLEU: 0,188</span></td>
</tr>
<tr id="S6.T1.8.23.15" class="ltx_tr">
<th id="S6.T1.8.23.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.23.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Ambati and Dudyala <span id="S6.T1.8.23.15.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib8" title="" class="ltx_ref">2018</a><span id="S6.T1.8.23.15.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.23.15.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.23.15.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.23.15.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.23.15.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.23.15.6.1" class="ltx_text" style="font-size:70%;">VGG16+GRU (ED)</span></td>
<td id="S6.T1.8.23.15.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.23.15.7.1" class="ltx_text" style="font-size:70%;">WBSS: 0,209</span></td>
</tr>
<tr id="S6.T1.8.24.16" class="ltx_tr">
<th id="S6.T1.8.24.16.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.24.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Abacha et al. <span id="S6.T1.8.24.16.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib1" title="" class="ltx_ref">2018</a><span id="S6.T1.8.24.16.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.24.16.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.24.16.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.24.16.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.24.16.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.24.16.6.1" class="ltx_text" style="font-size:70%;">VGG16+LSTM (SANet</span></td>
<td id="S6.T1.8.24.16.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.25.17" class="ltx_tr">
<th id="S6.T1.8.25.17.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.25.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Peng et al. <span id="S6.T1.8.25.17.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib41" title="" class="ltx_ref">2018</a><span id="S6.T1.8.25.17.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.25.17.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.25.17.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.25.17.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.25.17.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.25.17.6.1" class="ltx_text" style="font-size:70%;">RN+LSTM (Co-AM+MFP F)</span></td>
<td id="S6.T1.8.25.17.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.26.18" class="ltx_tr">
<th id="S6.T1.8.26.18.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.26.18.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Liu et al. <span id="S6.T1.8.26.18.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib34" title="" class="ltx_ref">2019</a><span id="S6.T1.8.26.18.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.26.18.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.26.18.4" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.26.18.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.26.18.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.26.18.6.1" class="ltx_text" style="font-size:70%;">IN+LSTM (Co-AM)</span></td>
<td id="S6.T1.8.26.18.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.27.19" class="ltx_tr">
<th id="S6.T1.8.27.19.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.27.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. <span id="S6.T1.8.27.19.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib39" title="" class="ltx_ref">2019</a><span id="S6.T1.8.27.19.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.27.19.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.27.19.3.1" class="ltx_text" style="font-size:70%;">VQA-RAD</span></td>
<td id="S6.T1.8.27.19.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.27.19.4.1" class="ltx_text" style="font-size:70%;">315 I</span></td>
<td id="S6.T1.8.27.19.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.27.19.5.1" class="ltx_text" style="font-size:70%;">3515 Q&amp;A</span></td>
<td id="S6.T1.8.27.19.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.27.19.6.1" class="ltx_text" style="font-size:70%;">CNNEnc.+WE (Co-AM)</span></td>
<td id="S6.T1.8.27.19.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.27.19.7.1" class="ltx_text" style="font-size:70%;">ACC: 74,1%</span></td>
</tr>
<tr id="S6.T1.8.28.20" class="ltx_tr">
<th id="S6.T1.8.28.20.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S6.T1.8.28.20.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Gurari et al. <span id="S6.T1.8.28.20.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib19" title="" class="ltx_ref">2019</a><span id="S6.T1.8.28.20.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.28.20.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.28.20.3.1" class="ltx_text" style="font-size:70%;">VizWiz-Priv</span></td>
<td id="S6.T1.8.28.20.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.28.20.4.1" class="ltx_text" style="font-size:70%;">5537 I</span></td>
<td id="S6.T1.8.28.20.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.28.20.5.1" class="ltx_text" style="font-size:70%;">1403 Q&amp;A</span></td>
<td id="S6.T1.8.28.20.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.28.20.6.1" class="ltx_text" style="font-size:70%;">Several Approaches</span></td>
<td id="S6.T1.8.28.20.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.28.20.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S6.T1.8.29.21" class="ltx_tr">
<th id="S6.T1.8.29.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.8.29.21.1.1" class="ltx_text" style="font-size:70%;">Visually</span></th>
<th id="S6.T1.8.29.21.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Gurari et al. <span id="S6.T1.8.29.21.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2018</a><span id="S6.T1.8.29.21.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.29.21.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.29.21.3.1" class="ltx_text" style="font-size:70%;">VizWiz</span></td>
<td id="S6.T1.8.29.21.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.29.21.4.1" class="ltx_text" style="font-size:70%;">31K I</span></td>
<td id="S6.T1.8.29.21.5" class="ltx_td ltx_align_left ltx_border_r">
<table id="S6.T1.8.29.21.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.8.29.21.5.1.1" class="ltx_tr">
<td id="S6.T1.8.29.21.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.29.21.5.1.1.1.1" class="ltx_text" style="font-size:70%;">31000 Q</span></td>
</tr>
<tr id="S6.T1.8.29.21.5.1.2" class="ltx_tr">
<td id="S6.T1.8.29.21.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.29.21.5.1.2.1.1" class="ltx_text" style="font-size:70%;">10 A each</span></td>
</tr>
</table>
</td>
<td id="S6.T1.8.29.21.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.29.21.6.1" class="ltx_text" style="font-size:70%;">Several Approaches</span></td>
<td id="S6.T1.8.29.21.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.29.21.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S6.T1.8.30.22" class="ltx_tr">
<th id="S6.T1.8.30.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.8.30.22.1.1" class="ltx_text" style="font-size:70%;">impaired</span></th>
<th id="S6.T1.8.30.22.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Anderson et al. <span id="S6.T1.8.30.22.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib9" title="" class="ltx_ref">2018</a><span id="S6.T1.8.30.22.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.30.22.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.30.22.3.1" class="ltx_text" style="font-size:70%;">VQAv2.0</span></td>
<td id="S6.T1.8.30.22.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.30.22.4.1" class="ltx_text" style="font-size:70%;">1.1M I</span></td>
<td id="S6.T1.8.30.22.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.30.22.5.1" class="ltx_text" style="font-size:70%;">1.1M Q&amp;A</span></td>
<td id="S6.T1.8.30.22.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.30.22.6.1" class="ltx_text" style="font-size:70%;">FRCNN+RN101+LSTM</span></td>
<td id="S6.T1.8.30.22.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.30.22.7.1" class="ltx_text" style="font-size:70%;">ACC: 70,34%</span></td>
</tr>
<tr id="S6.T1.8.31.23" class="ltx_tr">
<th id="S6.T1.8.31.23.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.31.23.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Weiss et al. <span id="S6.T1.8.31.23.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib55" title="" class="ltx_ref">2019</a><span id="S6.T1.8.31.23.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.31.23.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.31.23.3.1" class="ltx_text" style="font-size:70%;">SEVN Simulator</span></td>
<td id="S6.T1.8.31.23.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.31.23.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S6.T1.8.31.23.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.31.23.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S6.T1.8.31.23.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.31.23.6.1" class="ltx_text" style="font-size:70%;">RL-based Approach</span></td>
<td id="S6.T1.8.31.23.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.31.23.7.1" class="ltx_text" style="font-size:70%;">ACC: 74,8%</span></td>
</tr>
<tr id="S6.T1.8.32.24" class="ltx_tr">
<th id="S6.T1.8.32.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T1.8.32.24.1.1" class="ltx_text" style="font-size:70%;">Video</span></th>
<th id="S6.T1.8.32.24.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Li et al. <span id="S6.T1.8.32.24.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib33" title="" class="ltx_ref">2019</a><span id="S6.T1.8.32.24.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.32.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.32.24.3.1" class="ltx_text" style="font-size:70%;">RAP</span></td>
<td id="S6.T1.8.32.24.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.32.24.4.1" class="ltx_text" style="font-size:70%;">587 HoV</span></td>
<td id="S6.T1.8.32.24.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S6.T1.8.32.24.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.8.32.24.5.1.1" class="ltx_tr">
<td id="S6.T1.8.32.24.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.32.24.5.1.1.1.1" class="ltx_text" style="font-size:70%;">84928 I</span></td>
</tr>
<tr id="S6.T1.8.32.24.5.1.2" class="ltx_tr">
<td id="S6.T1.8.32.24.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.32.24.5.1.2.1.1" class="ltx_text" style="font-size:70%;">72 At</span></td>
</tr>
</table>
</td>
<td id="S6.T1.8.32.24.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.32.24.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S6.T1.8.32.24.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.8.32.24.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S6.T1.2.2" class="ltx_tr">
<th id="S6.T1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.2.2.3.1" class="ltx_text" style="font-size:70%;">Surveillance</span></th>
<th id="S6.T1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Toor et al. <span id="S6.T1.2.2.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib49" title="" class="ltx_ref">2019a</a><span id="S6.T1.2.2.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.2.2.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.2.2.5.1" class="ltx_text" style="font-size:70%;">BOAR + BTV</span></td>
<td id="S6.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">
<table id="S6.T1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.1.1.1.1.1" class="ltx_tr">
<td id="S6.T1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S6.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.1.1.1.1.1.1.m1.1a"><mo mathsize="70%" id="S6.T1.1.1.1.1.1.1.m1.1.1" xref="S6.T1.1.1.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T1.1.1.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.1.1.1.1.1.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">45K I</span>
</td>
</tr>
<tr id="S6.T1.1.1.1.1.2" class="ltx_tr">
<td id="S6.T1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.1.1.1.1.2.1.1" class="ltx_text" style="font-size:70%;">5 V</span></td>
</tr>
</table>
</td>
<td id="S6.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_r">
<table id="S6.T1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.2.2.2.1.1" class="ltx_tr">
<td id="S6.T1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S6.T1.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.2.2.2.1.1.1.m1.1a"><mo mathsize="70%" id="S6.T1.2.2.2.1.1.1.m1.1.1" xref="S6.T1.2.2.2.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.2.2.2.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T1.2.2.2.1.1.1.m1.1.1.cmml" xref="S6.T1.2.2.2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.2.2.2.1.1.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.2.2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">23K Q</span>
</td>
</tr>
<tr id="S6.T1.2.2.2.1.2" class="ltx_tr">
<td id="S6.T1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.2.2.2.1.2.1.1" class="ltx_text" style="font-size:70%;">101 C</span></td>
</tr>
</table>
</td>
<td id="S6.T1.2.2.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.2.2.6.1" class="ltx_text" style="font-size:70%;">RN50+BiLSTM</span></td>
<td id="S6.T1.2.2.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.2.2.7.1" class="ltx_text" style="font-size:70%;">ACC: 61,36%</span></td>
</tr>
<tr id="S6.T1.8.33.25" class="ltx_tr">
<th id="S6.T1.8.33.25.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.33.25.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Tu et al. <span id="S6.T1.8.33.25.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib51" title="" class="ltx_ref">2014</a><span id="S6.T1.8.33.25.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.33.25.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.33.25.3.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.8.33.25.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.33.25.4.1" class="ltx_text" style="font-size:70%;">2 V</span></td>
<td id="S6.T1.8.33.25.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.33.25.5.1" class="ltx_text" style="font-size:70%;">5 Cap each</span></td>
<td id="S6.T1.8.33.25.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.33.25.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S6.T1.8.33.25.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.33.25.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S6.T1.3.3" class="ltx_tr">
<th id="S6.T1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T1.3.3.2.1" class="ltx_text" style="font-size:70%;">Cultural</span></th>
<th id="S6.T1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Bongini et al. <span id="S6.T1.3.3.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib16" title="" class="ltx_ref">2020</a><span id="S6.T1.3.3.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.3.3.4.1" class="ltx_text" style="font-size:70%;">Annotated Artpedia</span></td>
<td id="S6.T1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.3.3.5.1" class="ltx_text" style="font-size:70%;">30 I</span></td>
<td id="S6.T1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S6.T1.3.3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.3.3.1.m1.1a"><mo mathsize="70%" id="S6.T1.3.3.1.m1.1.1" xref="S6.T1.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.3.3.1.m1.1b"><csymbol cd="latexml" id="S6.T1.3.3.1.m1.1.1.cmml" xref="S6.T1.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.3.3.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.3.3.1.1" class="ltx_text" style="font-size:70%;">120 Q&amp;A</span>
</td>
<td id="S6.T1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.3.3.6.1" class="ltx_text" style="font-size:70%;">FRCNN+BM</span></td>
<td id="S6.T1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.3.3.7.1" class="ltx_text" style="font-size:70%;">ACC:25,1%</span></td>
</tr>
<tr id="S6.T1.8.34.26" class="ltx_tr">
<th id="S6.T1.8.34.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.8.34.26.1.1" class="ltx_text" style="font-size:70%;">Heritage</span></th>
<th id="S6.T1.8.34.26.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Sheng et al. <span id="S6.T1.8.34.26.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib45" title="" class="ltx_ref">2016</a><span id="S6.T1.8.34.26.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.34.26.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.34.26.3.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.8.34.26.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.34.26.4.1" class="ltx_text" style="font-size:70%;">16 I, 444 T</span></td>
<td id="S6.T1.8.34.26.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.34.26.5.1" class="ltx_text" style="font-size:70%;">805 Q&amp;A</span></td>
<td id="S6.T1.8.34.26.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.34.26.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S6.T1.8.34.26.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.34.26.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S6.T1.4.4" class="ltx_tr">
<th id="S6.T1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T1.4.4.2.1" class="ltx_text" style="font-size:70%;">Advertising</span></th>
<th id="S6.T1.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Hussain et al. <span id="S6.T1.4.4.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib25" title="" class="ltx_ref">2017</a><span id="S6.T1.4.4.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.4.4.4.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S6.T1.4.4.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.4.4.5.1.1" class="ltx_tr">
<td id="S6.T1.4.4.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.4.4.5.1.1.1.1" class="ltx_text" style="font-size:70%;">64832 I,</span></td>
</tr>
<tr id="S6.T1.4.4.5.1.2" class="ltx_tr">
<td id="S6.T1.4.4.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.4.4.5.1.2.1.1" class="ltx_text" style="font-size:70%;">3477 V</span></td>
</tr>
</table>
</td>
<td id="S6.T1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S6.T1.4.4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.4.4.1.m1.1a"><mo mathsize="70%" id="S6.T1.4.4.1.m1.1.1" xref="S6.T1.4.4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S6.T1.4.4.1.m1.1.1.cmml" xref="S6.T1.4.4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.4.4.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.4.4.1.1" class="ltx_text" style="font-size:70%;">273k Q&amp;A</span>
</td>
<td id="S6.T1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.4.4.6.1" class="ltx_text" style="font-size:70%;">RN+LSTM</span></td>
<td id="S6.T1.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.4.4.7.1" class="ltx_text" style="font-size:70%;">TDA</span></td>
</tr>
<tr id="S6.T1.8.35.27" class="ltx_tr">
<th id="S6.T1.8.35.27.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.35.27.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Park et al. <span id="S6.T1.8.35.27.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib40" title="" class="ltx_ref">2019</a><span id="S6.T1.8.35.27.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.35.27.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.35.27.3.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.8.35.27.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.35.27.4.1" class="ltx_text" style="font-size:70%;">3747 I</span></td>
<td id="S6.T1.8.35.27.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.35.27.5.1" class="ltx_text" style="font-size:70%;">500k Imp</span></td>
<td id="S6.T1.8.35.27.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.35.27.6.1" class="ltx_text" style="font-size:70%;">MMFN</span></td>
<td id="S6.T1.8.35.27.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.35.27.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S6.T1.5.5" class="ltx_tr">
<th id="S6.T1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S6.T1.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Chou et al. <span id="S6.T1.5.5.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib17" title="" class="ltx_ref">2020</a><span id="S6.T1.5.5.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.5.5.4.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S6.T1.5.5.1.1" class="ltx_text" style="font-size:70%;">1490 I in </span><math id="S6.T1.5.5.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S6.T1.5.5.1.m1.1a"><msup id="S6.T1.5.5.1.m1.1.1" xref="S6.T1.5.5.1.m1.1.1.cmml"><mn mathsize="70%" id="S6.T1.5.5.1.m1.1.1.2" xref="S6.T1.5.5.1.m1.1.1.2.cmml">360</mn><mo mathsize="70%" id="S6.T1.5.5.1.m1.1.1.3" xref="S6.T1.5.5.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T1.5.5.1.m1.1b"><apply id="S6.T1.5.5.1.m1.1.1.cmml" xref="S6.T1.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T1.5.5.1.m1.1.1.1.cmml" xref="S6.T1.5.5.1.m1.1.1">superscript</csymbol><cn type="integer" id="S6.T1.5.5.1.m1.1.1.2.cmml" xref="S6.T1.5.5.1.m1.1.1.2">360</cn><compose id="S6.T1.5.5.1.m1.1.1.3.cmml" xref="S6.T1.5.5.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.5.5.1.m1.1c">360^{\circ}</annotation></semantics></math>
</td>
<td id="S6.T1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.5.5.5.1" class="ltx_text" style="font-size:70%;">16945 Q&amp;A</span></td>
<td id="S6.T1.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.5.5.6.1" class="ltx_text" style="font-size:70%;">Tucker&amp;Diffusion (MLA)</span></td>
<td id="S6.T1.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T1.5.5.7.1" class="ltx_text" style="font-size:70%;">ACC: 58,66%</span></td>
</tr>
<tr id="S6.T1.8.36.28" class="ltx_tr">
<th id="S6.T1.8.36.28.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.36.28.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Sampat et al. <span id="S6.T1.8.36.28.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib44" title="" class="ltx_ref">2020</a><span id="S6.T1.8.36.28.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.36.28.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.36.28.3.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.8.36.28.4" class="ltx_td ltx_align_left ltx_border_r">
<table id="S6.T1.8.36.28.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.8.36.28.4.1.1" class="ltx_tr">
<td id="S6.T1.8.36.28.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.36.28.4.1.1.1.1" class="ltx_text" style="font-size:70%;">10209 I</span></td>
</tr>
<tr id="S6.T1.8.36.28.4.1.2" class="ltx_tr">
<td id="S6.T1.8.36.28.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.36.28.4.1.2.1.1" class="ltx_text" style="font-size:70%;">9156 T</span></td>
</tr>
</table>
</td>
<td id="S6.T1.8.36.28.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.36.28.5.1" class="ltx_text" style="font-size:70%;">9267 Q&amp;A</span></td>
<td id="S6.T1.8.36.28.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.36.28.6.1" class="ltx_text" style="font-size:70%;">CNN enc.(I&amp;Q)</span></td>
<td id="S6.T1.8.36.28.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.36.28.7.1" class="ltx_text" style="font-size:70%;">ACC: 39,63%</span></td>
</tr>
<tr id="S6.T1.8.37.29" class="ltx_tr">
<th id="S6.T1.8.37.29.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.37.29.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Wang et al. <span id="S6.T1.8.37.29.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib54" title="" class="ltx_ref">2020</a><span id="S6.T1.8.37.29.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.37.29.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.37.29.3.1" class="ltx_text" style="font-size:70%;">TACoS-QA</span></td>
<td id="S6.T1.8.37.29.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.37.29.4.1" class="ltx_text" style="font-size:70%;">185 V</span></td>
<td id="S6.T1.8.37.29.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.37.29.5.1" class="ltx_text" style="font-size:70%;">21310 Q&amp;A</span></td>
<td id="S6.T1.8.37.29.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.37.29.6.1" class="ltx_text" style="font-size:70%;">3DCNN+LSTM</span></td>
<td id="S6.T1.8.37.29.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.37.29.7.1" class="ltx_text" style="font-size:70%;">ACC: 24,82%</span></td>
</tr>
<tr id="S6.T1.8.38.30" class="ltx_tr">
<th id="S6.T1.8.38.30.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.38.30.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Wang et al. <span id="S6.T1.8.38.30.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib54" title="" class="ltx_ref">2020</a><span id="S6.T1.8.38.30.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.38.30.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.38.30.3.1" class="ltx_text" style="font-size:70%;">MSR-VTT-QA</span></td>
<td id="S6.T1.8.38.30.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.38.30.4.1" class="ltx_text" style="font-size:70%;">3852 V</span></td>
<td id="S6.T1.8.38.30.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.38.30.5.1" class="ltx_text" style="font-size:70%;">19748 Q&amp;A</span></td>
<td id="S6.T1.8.38.30.6" class="ltx_td ltx_border_r"></td>
<td id="S6.T1.8.38.30.7" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S6.T1.8.39.31" class="ltx_tr">
<th id="S6.T1.8.39.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.8.39.31.1.1" class="ltx_text" style="font-size:70%;">Misc</span></th>
<th id="S6.T1.8.39.31.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Abdelnour et al. <span id="S6.T1.8.39.31.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib3" title="" class="ltx_ref">2019</a><span id="S6.T1.8.39.31.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.39.31.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.39.31.3.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.8.39.31.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.39.31.4.1" class="ltx_text" style="font-size:70%;">7500 AS</span></td>
<td id="S6.T1.8.39.31.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.39.31.5.1" class="ltx_text" style="font-size:70%;">300000 Q&amp;A</span></td>
<td id="S6.T1.8.39.31.6" class="ltx_td ltx_align_left ltx_border_r">
<table id="S6.T1.8.39.31.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.8.39.31.6.1.1" class="ltx_tr">
<td id="S6.T1.8.39.31.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.39.31.6.1.1.1.1" class="ltx_text" style="font-size:70%;">FiLM Network</span></td>
</tr>
<tr id="S6.T1.8.39.31.6.1.2" class="ltx_tr">
<td id="S6.T1.8.39.31.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.39.31.6.1.2.1.1" class="ltx_text" style="font-size:70%;">MAC Network</span></td>
</tr>
</table>
</td>
<td id="S6.T1.8.39.31.7" class="ltx_td ltx_align_left ltx_border_r">
<table id="S6.T1.8.39.31.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.8.39.31.7.1.1" class="ltx_tr">
<td id="S6.T1.8.39.31.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.39.31.7.1.1.1.1" class="ltx_text" style="font-size:70%;">ACC: 90,3%</span></td>
</tr>
<tr id="S6.T1.8.39.31.7.1.2" class="ltx_tr">
<td id="S6.T1.8.39.31.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S6.T1.8.39.31.7.1.2.1.1" class="ltx_text" style="font-size:70%;">ACC: 44,8%</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S6.T1.8.40.32" class="ltx_tr">
<th id="S6.T1.8.40.32.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.40.32.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Hosseinabad et al. <span id="S6.T1.8.40.32.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib24" title="" class="ltx_ref">2020</a><span id="S6.T1.8.40.32.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.40.32.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.40.32.3.1" class="ltx_text" style="font-size:70%;">Homemade</span></td>
<td id="S6.T1.8.40.32.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.40.32.4.1" class="ltx_text" style="font-size:70%;">85321 Ic</span></td>
<td id="S6.T1.8.40.32.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.40.32.5.1" class="ltx_text" style="font-size:70%;">429654 Q&amp;A</span></td>
<td id="S6.T1.8.40.32.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.40.32.6.1" class="ltx_text" style="font-size:70%;">CNN+LSTM</span></td>
<td id="S6.T1.8.40.32.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.40.32.7.1" class="ltx_text" style="font-size:70%;">ACC: 25,78%</span></td>
</tr>
<tr id="S6.T1.8.41.33" class="ltx_tr">
<th id="S6.T1.8.41.33.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.41.33.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Bhattacharya et al. <span id="S6.T1.8.41.33.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib13" title="" class="ltx_ref">2019</a><span id="S6.T1.8.41.33.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.41.33.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.41.33.3.1" class="ltx_text" style="font-size:70%;">VQA 2.0+VizWIz+L</span></td>
<td id="S6.T1.8.41.33.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.41.33.4.1" class="ltx_text" style="font-size:70%;">44955 I</span></td>
<td id="S6.T1.8.41.33.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.41.33.5.1" class="ltx_text" style="font-size:70%;">224775 Q&amp;A</span></td>
<td id="S6.T1.8.41.33.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.41.33.6.1" class="ltx_text" style="font-size:70%;">CNN+GRU</span></td>
<td id="S6.T1.8.41.33.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.8.41.33.7.1" class="ltx_text" style="font-size:70%;">ACC: 44,55%</span></td>
</tr>
<tr id="S6.T1.6.6" class="ltx_tr">
<th id="S6.T1.6.6.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S6.T1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Toor et al. <span id="S6.T1.6.6.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib50" title="" class="ltx_ref">2019b</a><span id="S6.T1.6.6.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.6.6.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.6.6.4.1" class="ltx_text" style="font-size:70%;">COCO-A+VG+C</span></td>
<td id="S6.T1.6.6.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.6.6.5.1" class="ltx_text" style="font-size:70%;">19431 I, 342 C</span></td>
<td id="S6.T1.6.6.1" class="ltx_td ltx_align_left ltx_border_r">
<math id="S6.T1.6.6.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.6.6.1.m1.1a"><mo mathsize="70%" id="S6.T1.6.6.1.m1.1.1" xref="S6.T1.6.6.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.6.6.1.m1.1b"><csymbol cd="latexml" id="S6.T1.6.6.1.m1.1.1.cmml" xref="S6.T1.6.6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.6.6.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.6.6.1.1" class="ltx_text" style="font-size:70%;">80K Q&amp;A</span>
</td>
<td id="S6.T1.6.6.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.6.6.6.1" class="ltx_text" style="font-size:70%;">GloVe+BiLSTM</span></td>
<td id="S6.T1.6.6.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S6.T1.6.6.7.1" class="ltx_text" style="font-size:70%;">ACC: 79,32%</span></td>
</tr>
<tr id="S6.T1.8.8" class="ltx_tr">
<th id="S6.T1.8.8.3" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="S6.T1.8.8.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Teney and Hengel <span id="S6.T1.8.8.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib48" title="" class="ltx_ref">2019</a><span id="S6.T1.8.8.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></th>
<td id="S6.T1.8.8.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S6.T1.8.8.5.1" class="ltx_text" style="font-size:70%;">VQA-CPv2+COCO</span></td>
<td id="S6.T1.8.8.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S6.T1.8.8.6.1" class="ltx_text" style="font-size:70%;">325721 I</span></td>
<td id="S6.T1.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">
<table id="S6.T1.8.8.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S6.T1.7.7.1.1.1" class="ltx_tr">
<td id="S6.T1.7.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S6.T1.7.7.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.7.7.1.1.1.1.m1.1a"><mo mathsize="70%" id="S6.T1.7.7.1.1.1.1.m1.1.1" xref="S6.T1.7.7.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.7.7.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T1.7.7.1.1.1.1.m1.1.1.cmml" xref="S6.T1.7.7.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.7.7.1.1.1.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.7.7.1.1.1.1.1" class="ltx_text" style="font-size:70%;">1.5M Q</span>
</td>
</tr>
<tr id="S6.T1.8.8.2.2.2" class="ltx_tr">
<td id="S6.T1.8.8.2.2.2.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S6.T1.8.8.2.2.2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.T1.8.8.2.2.2.1.m1.1a"><mo mathsize="70%" id="S6.T1.8.8.2.2.2.1.m1.1.1" xref="S6.T1.8.8.2.2.2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T1.8.8.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S6.T1.8.8.2.2.2.1.m1.1.1.cmml" xref="S6.T1.8.8.2.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.8.8.2.2.2.1.m1.1c">\sim</annotation></semantics></math><span id="S6.T1.8.8.2.2.2.1.1" class="ltx_text" style="font-size:70%;">15M A</span>
</td>
</tr>
</table>
</td>
<td id="S6.T1.8.8.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S6.T1.8.8.7.1" class="ltx_text" style="font-size:70%;">IA+WE</span></td>
<td id="S6.T1.8.8.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S6.T1.8.8.8.1" class="ltx_text" style="font-size:70%;">ACC: 34,25%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Emerging challenges/Misc</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Emerging lines in VQA mainly focus on new input or Q&amp;A. 
<br class="ltx_break">Concerning the data, <cite class="ltx_cite ltx_citemacro_cite">Chou et al. (<a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> explore the use of <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S7.p1.1.m1.1a"><msup id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml"><mn id="S7.p1.1.m1.1.1.2" xref="S7.p1.1.m1.1.1.2.cmml">360</mn><mo id="S7.p1.1.m1.1.1.3" xref="S7.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><apply id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.p1.1.m1.1.1.1.cmml" xref="S7.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S7.p1.1.m1.1.1.2.cmml" xref="S7.p1.1.m1.1.1.2">360</cn><compose id="S7.p1.1.m1.1.1.3.cmml" xref="S7.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">360^{\circ}</annotation></semantics></math> images on which the information can be located in all the field of view. They used 1490 indoor images from two 3D datasets and generated 16945 Q&amp;A. A cubemap-based architecture extracts visual features and, then, a multi-level attention network aggregates features. Other than image, a VQA system may be required to extract information also from text. <cite class="ltx_cite ltx_citemacro_cite">Sampat et al. (<a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite> built a dataset to test modalities in which an image-text joint inference is required. The dataset is composed by educational, web and other VQA datasets resources, for a total of 10209 images, 9156 different texts and 9267 questions. They also tested this new dataset using existing VQA models discovering that the latter do not fit well those new data.
The video-question-answering is also a well-known field, however the long-video QA is unexplored. <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> manage this kind of data in their work building two datasets of long video, TACoS-QA and MSR-VTT-QA, containing 187 and 3852 videos, respectively. The number of Q&amp;A is about 20000 per dataset. Based on that, they develop a matching-guided attention model for video and question embedding, question-related video content localization
and answer prediction.
The VQA know-how is extending to Acustic Question Answering (AQA). <cite class="ltx_cite ltx_citemacro_cite">Abdelnour et al. (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> show how to create auditory scenes and related Q&amp;A. Their 7500 scenes and 300000 Q&amp;A were tested using two neural networks: FiLM, based on Conditional Batch Normalization, and MAC, based on LSTM models. 
<br class="ltx_break">Concerning the new challenges that can emerge for Q&amp;As, the more intuitive is the possibility of multiple answers. Both <cite class="ltx_cite ltx_citemacro_cite">Hosseinabad et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Bhattacharya et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> explore this possibility. The former start this research building a dataset of 100 simple icons randomly located in 85321 images for a total of 429654 Q&amp;A. They tested this new dataset building a LSTM-based neural network.
The latter focus on the reason why a question can have more than one answer. They built a dataset, starting from two popular datasets, VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib10" title="" class="ltx_ref">2015</a>)</cite> and VizWiz <cite class="ltx_cite ltx_citemacro_citep">(Gurari et al., <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>, for a total of 44955 images and 224775 annotations, labelling 9 reason for different answers. They built a prediction answer network based on CNN and attention model in which not only the answer is predicted but also a probable reason for difference.
Studying the properties of the questions is gaining popularity. <cite class="ltx_cite ltx_citemacro_cite">Toor et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019b</a>)</cite> not only built a system to detect when a question is not related to the image but also a method to edit this question. Starting from the COCO-A <cite class="ltx_cite ltx_citemacro_citep">(Ronchi and Perona, <a href="#bib.bib43" title="" class="ltx_ref">2015</a>)</cite> and the VG <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib29" title="" class="ltx_ref">2016</a>)</cite> datasets, they obtained 19431 images and 55738 Q&amp;A for the question relevance and 22172 questions about 342 classes for the question editing. Their method is then based on BiLSTM embedded in a neural network architecture.
From all those works it is clear that a huge amount of Q&amp;A is required to built a well generalising system, however collecting them is time-consuming. For this reason, <cite class="ltx_cite ltx_citemacro_cite">Teney and Hengel (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite> propose a new VQA system in which the ability to answer an unknown question is obtained fusing known questions and external data. For this purpose they focus on weight adaptation on a basic VQA model, using VQA-CP v2 dataset <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> and COCO captioning dataset. 
<br class="ltx_break">Finally, to evaluate the effectiveness of a VQA system, <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite> propose an Inverse VQA. They use a question encoder that encodes the image and question, and a decoder that, from the features of the image, generates a visual question. The question encoder is based on an LSTM architecture and the authors show how to use this method to perform VQA diagnosis.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Discussion and conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Two elements are interesting to underline. First, in most cases, even though specific datasets are collected, methods are mostly inherited though sometimes adapted. In general, except for med-VQA, there is no attempt for boost optimization following a stronger domain characterization, except for new kinds of data. It can be hypothesized that when the image features are very different than usual, as in medical imaging, this could take to a more effective design of the entailed processing. A second consideration concerns the size of the datasets. While this problems is underlined in the general case in <cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>, ad-hoc datasets seldom reach the huge amount of information requested for a robust generalizability of the obtained performance. These two aspects definitely deserve attention from the VQA community in order to finally reach performance able to boost real world domain-specific applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abacha et al. (2018)</span>
<span class="ltx_bibblock">
Abacha, A.B., Gayen, S.,
Lau, J.J., Rajaraman, S.,
Demner-Fushman, D., 2018.

</span>
<span class="ltx_bibblock">Nlm at imageclef 2018 visual question answering in
the medical domain., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abacha et al. (2019)</span>
<span class="ltx_bibblock">
Abacha, A.B., Hasan, S.A.,
Datla, V.V., Liu, J.,
Demner-Fushman, D., Müller, H.,
2019.

</span>
<span class="ltx_bibblock">Vqa-med: Overview of the medical visual question
answering task at imageclef 2019, in: CLEF2019 Working
Notes. CEUR Workshop Proceedings, pp. 09–12.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelnour et al. (2019)</span>
<span class="ltx_bibblock">
Abdelnour, J., Salvi, G.,
Rouat, J., 2019.

</span>
<span class="ltx_bibblock">From visual to acoustic question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1902.11280 .

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Agrawal, A., Batra, D.,
Parikh, D., Kembhavi, A.,
2018.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming priors
for visual question answering.

</span>
<span class="ltx_bibblock">2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 4971–4980.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Sadi et al. (2019)</span>
<span class="ltx_bibblock">
Al-Sadi, A., Talafha, B.,
Al-Ayyoub, M., Jararweh, Y.,
Costen, F., 2019.

</span>
<span class="ltx_bibblock">Just at imageclef 2019 visual question answering in
the medical domain.

</span>
<span class="ltx_bibblock">Working Notes of CLEF .

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allaouzi and Ahmed (2018)</span>
<span class="ltx_bibblock">
Allaouzi, I., Ahmed, M.B.,
2018.

</span>
<span class="ltx_bibblock">Deep neural networks and decision tree classifier for
visual question answering in the medical domain., in:
CLEF (Working Notes).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allaouzi et al. (2019)</span>
<span class="ltx_bibblock">
Allaouzi, I., Benamrou, B.,
Ahmed, M., 2019.

</span>
<span class="ltx_bibblock">An encoder-decoder model for visual question
answering in the medical domain.

</span>
<span class="ltx_bibblock">Working Notes of CLEF .

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ambati and Dudyala (2018)</span>
<span class="ltx_bibblock">
Ambati, R., Dudyala, C.R.,
2018.

</span>
<span class="ltx_bibblock">A sequence-to-sequence model approach for imageclef
2018 medical domain visual question answering, in: 2018
15th IEEE India Council International Conference (INDICON),
IEEE. pp. 1–6.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Anderson, P., He, X.,
Buehler, C., Teney, D.,
Johnson, M., Gould, S.,
Zhang, L., 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning
and visual question answering, in: The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A.,
Lu, J., Mitchell, M.,
Batra, D., Zitnick, C.L.,
Parikh, D., 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering, in:
Proceedings of the 2015 IEEE International Conference on
Computer Vision (ICCV), IEEE Computer Society,
USA. p. 2425–2433.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV.2015.279</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICCV.2015.279</span></a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al. (2019)</span>
<span class="ltx_bibblock">
Bansal, M., Gadgil, T.,
Shah, R., Verma, P.,
2019.

</span>
<span class="ltx_bibblock">Medical visual question answering at image clef
2019-vqa med., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bghiel et al. (2019)</span>
<span class="ltx_bibblock">
Bghiel, A., Dahdouh, Y.,
Allaouzi, I., Ahmed, M.B.,
Boudhir, A.A., 2019.

</span>
<span class="ltx_bibblock">Visual question answering system for identifying
medical images attributes, in: The Proceedings of the
Third International Conference on Smart City Applications,
Springer. pp. 483–492.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattacharya et al. (2019)</span>
<span class="ltx_bibblock">
Bhattacharya, N., Li, Q.,
Gurari, D., 2019.

</span>
<span class="ltx_bibblock">Why does a visual question have different answers?,
in: Proceedings of the IEEE International Conference on
Computer Vision, pp. 4271–4280.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bigham et al. (2010a)</span>
<span class="ltx_bibblock">
Bigham, J.P., Jayant, C.,
Ji, H., Little, G.,
Miller, A., Miller, R.C.,
Miller, R., Tatarowicz, A.,
White, B., White, S.,
Yeh, T., 2010a.

</span>
<span class="ltx_bibblock">Vizwiz: Nearly real-time answers to visual
questions, in: Proceedings of the 23nd Annual ACM
Symposium on User Interface Software and Technology,
Association for Computing Machinery,
New York, NY, USA. p. 333–342.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bigham et al. (2010b)</span>
<span class="ltx_bibblock">
Bigham, J.P., Jayant, C.,
Ji, H., Little, G.,
Miller, A., Miller, R.C.,
Miller, R., Tatarowicz, A.,
White, B., White, S., et al.,
2010b.

</span>
<span class="ltx_bibblock">Vizwiz: nearly real-time answers to visual
questions, in: Proceedings of the 23nd annual ACM
symposium on User interface software and technology, pp.
333–342.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bongini et al. (2020)</span>
<span class="ltx_bibblock">
Bongini, P., Becattini, F.,
Bagdanov, A.D., Bimbo, A.D.,
2020.

</span>
<span class="ltx_bibblock">Visual question answering for cultural heritage.

</span>
<span class="ltx_bibblock">ArXiv abs/2003.09853.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chou et al. (2020)</span>
<span class="ltx_bibblock">
Chou, S.H., Chao, W.L.,
Lai, W.S., Sun, M.,
Yang, M.H., 2020.

</span>
<span class="ltx_bibblock">Visual question answering on <math id="bib.bib17.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="bib.bib17.1.m1.1a"><msup id="bib.bib17.1.m1.1.1" xref="bib.bib17.1.m1.1.1.cmml"><mn id="bib.bib17.1.m1.1.1.2" xref="bib.bib17.1.m1.1.1.2.cmml">360</mn><mo id="bib.bib17.1.m1.1.1.3" xref="bib.bib17.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib17.1.m1.1b"><apply id="bib.bib17.1.m1.1.1.cmml" xref="bib.bib17.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib17.1.m1.1.1.1.cmml" xref="bib.bib17.1.m1.1.1">superscript</csymbol><cn type="integer" id="bib.bib17.1.m1.1.1.2.cmml" xref="bib.bib17.1.m1.1.1.2">360</cn><compose id="bib.bib17.1.m1.1.1.3.cmml" xref="bib.bib17.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.1.m1.1c">360^{\circ}</annotation></semantics></math> images.

</span>
<span class="ltx_bibblock">ArXiv abs/2001.03339.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2017)</span>
<span class="ltx_bibblock">
Das, A., Agrawal, H.,
Zitnick, L., Parikh, D.,
Batra, D., 2017.

</span>
<span class="ltx_bibblock">Human attention in visual question answering: Do
humans and deep networks look at the same regions?

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
163, 90 – 100.

</span>
<span class="ltx_bibblock">Language in Vision.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2019)</span>
<span class="ltx_bibblock">
Gurari, D., Li, Q., Lin,
C., Zhao, Y., Guo, A.,
Stangl, A., Bigham, J.P.,
2019.

</span>
<span class="ltx_bibblock">Vizwiz-priv: A dataset for recognizing the presence
and purpose of private visual information in images taken by blind people,
in: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 939–948.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2018)</span>
<span class="ltx_bibblock">
Gurari, D., Li, Q.,
Stangl, A., Guo, A.,
Lin, C., Grauman, K.,
Luo, J., Bigham, J.,
2018.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions
from blind people, in: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp.
3608–3617.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan et al. (2018)</span>
<span class="ltx_bibblock">
Hasan, S.A., Ling, Y.,
Farri, O., Liu, J.,
Müller, H., Lungren, M.,
2018.

</span>
<span class="ltx_bibblock">Overview of imageclef 2018 medical domain visual
question answering task., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017)</span>
<span class="ltx_bibblock">
He, B., Xia, M., Yu,
X., Jian, P., Meng, H.,
Chen, Z., 2017.

</span>
<span class="ltx_bibblock">An educational robot system of visual question
answering for preschoolers, in: 2017 2nd International
Conference on Robotics and Automation Engineering (ICRAE), pp.
441–445.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
He, X., Zhang, Y., Mou,
L., Xing, E., Xie, P.,
2020.

</span>
<span class="ltx_bibblock">Pathvqa: 30000+ questions for medical visual question
answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2003.10286 .

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseinabad et al. (2020)</span>
<span class="ltx_bibblock">
Hosseinabad, S.H., Safayani, M.,
Mirzaei, A., 2020.

</span>
<span class="ltx_bibblock">Multiple answers to a question: a new approach for
visual question answering.

</span>
<span class="ltx_bibblock">The Visual Computer , 1–13.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hussain et al. (2017)</span>
<span class="ltx_bibblock">
Hussain, Z., Zhang, M.,
Zhang, X., Ye, K.,
Thomas, C., Agha, Z.,
Ong, N., Kovashka, A.,
2017.

</span>
<span class="ltx_bibblock">Automatic understanding of image and video
advertisements, in: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017)</span>
<span class="ltx_bibblock">
Kafle, K., Kanan, C., 2017.

</span>
<span class="ltx_bibblock">Visual question answering: Datasets, algorithms, and
future challenges.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
163, 3–20.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katz et al. (2003)</span>
<span class="ltx_bibblock">
Katz, B., Lin, J.J.,
Stauffer, C., Grimson, W.E.L.,
2003.

</span>
<span class="ltx_bibblock">Answering questions about moving objects in
surveillance videos, in: New Directions in Question
Answering.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornuta et al. (2019)</span>
<span class="ltx_bibblock">
Kornuta, T., Rajan, D.,
Shivade, C., Asseman, A.,
Ozcan, A.S., 2019.

</span>
<span class="ltx_bibblock">Leveraging medical visual question answering with
supporting facts.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1905.12008 .

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2016)</span>
<span class="ltx_bibblock">
Krishna, R., Zhu, Y.,
Groth, O., Johnson, J.,
Hata, K., Kravitz, J.,
Chen, S., Kalantidis, Y.,
Li, L.J., Shamma, D.,
Bernstein, M., Li, F.F.,
2016.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using
crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision
123.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lasecki et al. (2013)</span>
<span class="ltx_bibblock">
Lasecki, W.S., Thiha, P.,
Zhong, Y., Brady, E.,
Bigham, J.P., 2013.

</span>
<span class="ltx_bibblock">Answering visual questions with conversational crowd
assistants, in: Proceedings of the 15th International
ACM SIGACCESS Conference on Computers and Accessibility,
Association for Computing Machinery,
New York, NY, USA.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lau et al. (2018)</span>
<span class="ltx_bibblock">
Lau, J.J., Gayen, S.,
Abacha, A.B., Demner-Fushman, D.,
2018.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions
and answers about radiology images.

</span>
<span class="ltx_bibblock">Scientific data 5,
1–10.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Li, D., Zhang, Z.,
Chen, X., Huang, K.,
2019.

</span>
<span class="ltx_bibblock">A richly annotated pedestrian dataset for person
retrieval in real surveillance scenarios.

</span>
<span class="ltx_bibblock">IEEE Transactions on Image Processing
28, 1575–1590.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Li, D., Zhang, Z., Yu,
K., Huang, K., Tan, T.,
2019.

</span>
<span class="ltx_bibblock">Isee: An intelligent scene exploration and evaluation
platform for large-scale visual surveillance.

</span>
<span class="ltx_bibblock">IEEE Transactions on Parallel and Distributed
Systems 30, 2743–2758.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Liu, F., Peng, Y., Rosen,
M.P., 2019.

</span>
<span class="ltx_bibblock">An effective deep transfer learning and information
fusion framework for medical visual question answering, in:
International Conference of the Cross-Language Evaluation
Forum for European Languages, Springer. pp.
238–247.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Liu, F., Xiang, T.,
Hospedales, T.M., Yang, W.,
Sun, C., 2020.

</span>
<span class="ltx_bibblock">Inverse visual question answering: A new benchmark
and vqa diagnosis tool.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine
Intelligence 42, 460–474.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lubna et al. (2019)</span>
<span class="ltx_bibblock">
Lubna, A., Kalady, S.,
Lijiya, A., 2019.

</span>
<span class="ltx_bibblock">Mobvqa: A modality based medical image visual
question answering system, in: TENCON 2019-2019 IEEE
Region 10 Conference (TENCON), IEEE. pp.
727–732.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manmadhan and Kovoor (2020)</span>
<span class="ltx_bibblock">
Manmadhan, S., Kovoor, B.C.,
2020.

</span>
<span class="ltx_bibblock">Visual question answering: a state-of-the-art
review.

</span>
<span class="ltx_bibblock">Artificial Intelligence Review ,
1–41.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muñoz et al. (2006)</span>
<span class="ltx_bibblock">
Muñoz, C., Arellano, D.,
Perales, F.J., Fontanet, G.,
2006.

</span>
<span class="ltx_bibblock">Perceptual and intelligent domotic system for
disabled people, in: Proceedings of the 6th IASTED
International Conference on Visualization, Imaging and Image Processing, pp.
70–75.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2019)</span>
<span class="ltx_bibblock">
Nguyen, B.D., Do, T.T.,
Nguyen, B.X., Do, T.,
Tjiputra, E., Tran, Q.D.,
2019.

</span>
<span class="ltx_bibblock">Overcoming data limitation in medical visual question
answering, in: International Conference on Medical Image
Computing and Computer-Assisted Intervention,
Springer. pp. 522–530.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2019)</span>
<span class="ltx_bibblock">
Park, K.W., Lee, J., Kwon,
S., Ha, J.W., Kim, K.M.,
Zhang, B.T., 2019.

</span>
<span class="ltx_bibblock">Which ads to show? advertisement image assessment
with auxiliary information via multi-step modality fusion.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1910.02358 .

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2018)</span>
<span class="ltx_bibblock">
Peng, Y., Liu, F., Rosen,
M.P., 2018.

</span>
<span class="ltx_bibblock">Umass at imageclef medical visual question answering
(med-vqa) 2018 task., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren and Zhou (2020)</span>
<span class="ltx_bibblock">
Ren, F., Zhou, Y., 2020.

</span>
<span class="ltx_bibblock">Cgmvqa: A new classification and generative model for
medical visual question answering.

</span>
<span class="ltx_bibblock">IEEE Access 8,
50626–50636.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronchi and Perona (2015)</span>
<span class="ltx_bibblock">
Ronchi, M.R., Perona, P.,
2015.

</span>
<span class="ltx_bibblock">Describing common human visual actions in images,
in: Xianghua Xie, M.W.J., Tam, G.K.L.
(Eds.), Proceedings of the British Machine Vision
Conference (BMVC), BMVA Press. pp.
52.1–52.12.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sampat et al. (2020)</span>
<span class="ltx_bibblock">
Sampat, S., Yang, Y.,
Baral, C., 2020.

</span>
<span class="ltx_bibblock">Diverse visuo-lingustic question answering (dvlqa)
challenge.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2005.00330 .

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. (2016)</span>
<span class="ltx_bibblock">
Sheng, S., Gool, L.V.,
Moens, M.F., 2016.

</span>
<span class="ltx_bibblock">A dataset for multimodal question answering in the
cultural heritage domain, in: LT4DH@COLING.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2019)</span>
<span class="ltx_bibblock">
Shi, L., Liu, F., Rosen,
M.P., 2019.

</span>
<span class="ltx_bibblock">Deep multimodal learning for medical visual question
answering.

</span>
<span class="ltx_bibblock">Working Notes of CLEF .

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stefanini et al. (2019)</span>
<span class="ltx_bibblock">
Stefanini, M., Cornia, M.,
Baraldi, L., Corsini, M.,
Cucchiara, R., 2019.

</span>
<span class="ltx_bibblock">Artpedia: A new visual-semantic dataset with visual
and contextual sentences in the artistic domain, in: Ricci,
E., Rota Bulò, S., Snoek, C.,
Lanz, O., Messelodi, S.,
Sebe, N. (Eds.), ICIAP 2019,
Springer International Publishing,
Cham. pp. 729–740.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney and Hengel (2019)</span>
<span class="ltx_bibblock">
Teney, D., Hengel, A.v.d.,
2019.

</span>
<span class="ltx_bibblock">Actively seeking and learning from live data, in:
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1940–1949.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toor et al. (2019a)</span>
<span class="ltx_bibblock">
Toor, A.S., Wechsler, H.,
Nappi, M., 2019a.

</span>
<span class="ltx_bibblock">Biometric surveillance using visual question
answering.

</span>
<span class="ltx_bibblock">Pattern Recognition Letters 126,
111 – 118.

</span>
<span class="ltx_bibblock">Robustness, Security and Regulation Aspects in Current
Biometric Systems.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toor et al. (2019b)</span>
<span class="ltx_bibblock">
Toor, A.S., Wechsler, H.,
Nappi, M., 2019b.

</span>
<span class="ltx_bibblock">Question action relevance and editing for visual
question answering.

</span>
<span class="ltx_bibblock">Multimedia Tools and Applications
78, 2921–2935.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. (2014)</span>
<span class="ltx_bibblock">
Tu, K., Meng, M.,
Lee, M.W., Choe, T.E.,
Zhu, S., 2014.

</span>
<span class="ltx_bibblock">Joint video and text parsing for understanding events
and answering queries.

</span>
<span class="ltx_bibblock">IEEE MultiMedia 21,
42–70.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2019)</span>
<span class="ltx_bibblock">
Vu, M., Sznitman, R.,
Nyholm, T., Löfstedt, T.,
2019.

</span>
<span class="ltx_bibblock">Ensemble of streamlined bilinear visual question
answering models for the imageclef 2019 challenge in the medical domain, in:
CLEF 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2020)</span>
<span class="ltx_bibblock">
Vu, M.H., Löfstedt, T.,
Nyholm, T., Sznitman, R.,
2020.

</span>
<span class="ltx_bibblock">A question-centric model for visual question
answering in medical imaging.

</span>
<span class="ltx_bibblock">IEEE Transactions on Medical Imaging .

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Wang, W., Huang, Y., Wang,
L., 2020.

</span>
<span class="ltx_bibblock">Long video question answering: A matching-guided
attention model.

</span>
<span class="ltx_bibblock">Pattern Recognition 102,
107248.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiss et al. (2019)</span>
<span class="ltx_bibblock">
Weiss, M., Chamorro, S.,
Girgis, R., Luck, M.,
Kahou, S.E., Cohen, J.P.,
Nowrouzezahrai, D., Precup, D.,
Golemo, F., Pal, C.,
2019.

</span>
<span class="ltx_bibblock">Navigation agents for the visually impaired: A
sidewalk simulator and experiments.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1910.13249 .

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2017)</span>
<span class="ltx_bibblock">
Wu, Q., Teney, D., Wang,
P., Shen, C., Dick, A.,
van den Hengel, A., 2017.

</span>
<span class="ltx_bibblock">Visual question answering: A survey of methods and
datasets.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
163, 21–40.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2019)</span>
<span class="ltx_bibblock">
Yan, X., Li, L., Xie, C.,
Xiao, J., Gu, L., 2019.

</span>
<span class="ltx_bibblock">Zhejiang university at imageclef 2019 visual question
answering in the medical domain.

</span>
<span class="ltx_bibblock">Working Notes of CLEF .

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Zhang, D., Cao, R., Wu,
S., 2019.

</span>
<span class="ltx_bibblock">Information fusion in visual question answering: A
survey.

</span>
<span class="ltx_bibblock">Information Fusion 52,
268–280.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Zhou, Y., Kang, X., Ren,
F., 2018.

</span>
<span class="ltx_bibblock">Employing inception-resnet-v2 and bi-lstm for medical
domain visual question answering., in: CLEF (Working
Notes).

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2020)</span>
<span class="ltx_bibblock">
Zhou, Y., Mishra, S.,
Verma, M., Bhamidipati, N.,
Wang, W., 2020.

</span>
<span class="ltx_bibblock">Recommending themes for ad creative design via
visual-linguistic representations, in: Proceedings of
The Web Conference 2020, pp. 2521–2527.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2103.02936" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2103.02937" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2103.02937">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2103.02937" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2103.02938" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 06:47:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
