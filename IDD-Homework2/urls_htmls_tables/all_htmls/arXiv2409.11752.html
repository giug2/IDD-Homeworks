<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models</title>
<!--Generated on Sun Sep 29 05:25:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Rein Vision Foundation Model Domain generalization Adenocarcinoma Segmentation." lang="en" name="keywords"/>
<base href="/html/2409.11752v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S1" title="In Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S2" title="In Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S3" title="In Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S3.SS1" title="In 3 Experiment ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S3.SS2" title="In 3 Experiment ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Implementation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S4" title="In Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S5" title="In Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Chongqing University of Technology, Chongqing 400054, China 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>lanlbn@cqut.edu.cn</span></span></span>
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Chongqing Zhijian Life Technology Co. LTD, Chongqing 400039, China 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{caipzh, zhangxy}@zhijianlife.cn</span></span></span>
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>zhaoze@ict.ac.cn</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pengzhou Cai 
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xueyuan Zhang
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Libin Lan<sup class="ltx_sup" id="id1.1.id1">(🖂)</sup>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ze Zhao<sup class="ltx_sup" id="id2.1.id1">(🖂)</sup>
</span><span class="ltx_author_notes">33</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">In recent years, significant progress has been made in tumor segmentation within the field of digital pathology. However, variations in organs, tissue preparation methods, and image acquisition processes can lead to domain discrepancies among digital pathology images. To address this problem, in this paper, we use Rein, a fine-tuning method, to parametrically and efficiently fine-tune various vision foundation models (VFMs) for MICCAI 2024 Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS2024). The core of Rein consists of a set of learnable tokens, which are directly linked to instances, improving functionality at the instance level in each layer. In the data environment of the COSAS2024 Challenge, extensive experiments demonstrate that Rein fine-tuned the VFMs to achieve satisfactory results. Specifically, we used Rein to fine-tune ConvNeXt and DINOv2. Our team used the former to achieve scores of 0.7719 and 0.7557 on the preliminary test phase and final test phase in task1, respectively, while the latter achieved scores of 0.8848 and 0.8192 on the preliminary test phase and final test phase in task2. Code is available at <a class="ltx_ref ltx_href" href="https://github.com/ZhiJianLife/cosas2024_Zhijian-Life" title="">GitHub</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Rein Vision Foundation Model Domain generalization Adenocarcinoma Segmentation.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Adenocarcinomas are prevalent in tissues such as the breast, stomach, colon, and lungs, making glandular segmentation, particularly of adenocarcinoma regions, a primary focus. In recent years, key challenges in adenocarcinoma segmentation, such as (GlaS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib1" title="">1</a>]</cite>, DigestPath <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib2" title="">2</a>]</cite>), have driven advances in digital pathology, significantly improving tumor diagnosis and localization. However, the inherent variability in digital pathology images and tissue types has posed substantial challenges for existing algorithms. Differences in organs, tissue preparation methods, and image acquisition processes have led to what is known as domain shifting. To address this issue, MIDOG 21/22 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib4" title="">4</a>]</cite> have developed. In order to further improve the generalization ability of segmentation model in the field of pathological images, we utilize Rein <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib5" title="">5</a>]</cite> to fine-tune the VFMs (e.g., Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib6" title="">6</a>]</cite>, ConvNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib7" title="">7</a>]</cite>, DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib8" title="">8</a>]</cite>) for MICCAI 2024 Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://cosas.grand-challenge.org/ </span></span></span>. Our contributions are as follows:</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mo id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">∙</annotation></semantics></math> We utilize Rein to fine-tune the SAM, ConvNeXt, DINOv2 for MICCAI 2024 COSAS2024 Challenge, in which Rein is a fine-tuning method. The core of Rein consists of a set of learnable tokens, which are directly linked to instances, improving functionality at the instance level in each layer.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">∙</annotation></semantics></math>For task1, our team achieved scores of 0.7719 and 0.7557 on the preliminary test phase and final test phase in task1, respectively. For task2, our team achieved scores of 0.8848 and 0.8192 on the preliminary test phase and final test phase in task2, respectively.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="378" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The network architecture of the method. We freeze the weights for backbone and fine-tune the weights for the rein module and Mask2Former. <math alttext="f_{i}" class="ltx_Math" display="inline" id="S1.F1.4.m1.1"><semantics id="S1.F1.4.m1.1b"><msub id="S1.F1.4.m1.1.1" xref="S1.F1.4.m1.1.1.cmml"><mi id="S1.F1.4.m1.1.1.2" xref="S1.F1.4.m1.1.1.2.cmml">f</mi><mi id="S1.F1.4.m1.1.1.3" xref="S1.F1.4.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.4.m1.1c"><apply id="S1.F1.4.m1.1.1.cmml" xref="S1.F1.4.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.4.m1.1.1.1.cmml" xref="S1.F1.4.m1.1.1">subscript</csymbol><ci id="S1.F1.4.m1.1.1.2.cmml" xref="S1.F1.4.m1.1.1.2">𝑓</ci><ci id="S1.F1.4.m1.1.1.3.cmml" xref="S1.F1.4.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m1.1d">f_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m1.1e">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{o}" class="ltx_Math" display="inline" id="S1.F1.5.m2.1"><semantics id="S1.F1.5.m2.1b"><msub id="S1.F1.5.m2.1.1" xref="S1.F1.5.m2.1.1.cmml"><mi id="S1.F1.5.m2.1.1.2" xref="S1.F1.5.m2.1.1.2.cmml">f</mi><mi id="S1.F1.5.m2.1.1.3" xref="S1.F1.5.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.5.m2.1c"><apply id="S1.F1.5.m2.1.1.cmml" xref="S1.F1.5.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.5.m2.1.1.1.cmml" xref="S1.F1.5.m2.1.1">subscript</csymbol><ci id="S1.F1.5.m2.1.1.2.cmml" xref="S1.F1.5.m2.1.1.2">𝑓</ci><ci id="S1.F1.5.m2.1.1.3.cmml" xref="S1.F1.5.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m2.1d">f_{o}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.5.m2.1e">italic_f start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> represent the output of the previous layer and the input of the next layer respectively. <math alttext="Q" class="ltx_Math" display="inline" id="S1.F1.6.m3.1"><semantics id="S1.F1.6.m3.1b"><mi id="S1.F1.6.m3.1.1" xref="S1.F1.6.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S1.F1.6.m3.1c"><ci id="S1.F1.6.m3.1.1.cmml" xref="S1.F1.6.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m3.1d">Q</annotation><annotation encoding="application/x-llamapun" id="S1.F1.6.m3.1e">italic_Q</annotation></semantics></math> is the object of the token map, and Q is then linked to the instance to achieve enhanced performance.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In the field of Natural Language Processing (NLP), parameter-efficient fine-tuning (PEFT) has proven highly effective by keeping most VFMs parameters frozen and fine-tuning only a small subset. The <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib5" title="">5</a>]</cite> proposed embedding a mechanism, named ’Rein’, between the layers of the backbone. Rein actively refines and passes feature maps from each layer to the next, allowing for more effective utilization of VFMs’ powerful capabilities. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib5" title="">5</a>]</cite>, we utilize Rein to fine-tune the SAM, ConvNeXt, DINOv2 for MICCAI 2024 COSAS2024 Challenge. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a> shows the network architecture of the method. Specifically, the network contains encoder and decoder. For encoder, freezing backbone and fine-tuning Rein is a key step in incorporating medical knowledge into the vision foundation model. For decoder, we adopt Mask2Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib9" title="">9</a>]</cite> as our segmentation head because it integrates various VFM as the backbone. The core of Rein consists of a set of learnable tokens, which are directly linked to instances, improving functionality at the instance level in each layer. To significantly reduce the number of parameters, similar to LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib10" title="">10</a>]</cite>, it implemented low-rank token sequence. Moreover, to address the redundancy of parameters in a layer-specific multilayer perceptron (MLP) weight, a shared MLP weight is used between the layers.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Details of the dataset of the COSAS2024 Challenge.
(I/C) represents the number of images and the number of categories (organs or scanners). </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.2" style="width:433.6pt;height:54.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.4pt,0.0pt) scale(0.998164277091419,0.998164277091419) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.2.2.3.1.1">COSAS2024</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.2.3.1.2">Image Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.2.3.1.3">Total(I/C)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.2.3.1.4">Train(I/C)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.2.3.1.5">Preliminary test (I/C)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.2.3.1.6">Final test(I/C)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2">Task1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.1">1500<math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><times id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">×</annotation></semantics></math>1500</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.3">290/6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.4">180/3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.5">20/4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1.6">90/6</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.2.2.2.2">Task2</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.2.1">1500<math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.2.2.2.1.m1.1"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><times id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.1.m1.1d">×</annotation></semantics></math>1500</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.2.3">290/6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.2.4">180/3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.2.5">20/4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.2.6">90/6</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The dataset of COSAS2024 <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://cosas.grand-challenge.org/datasets/ </span></span></span> is the first and largest domain generalization dataset for the digital pathology segmentation task. There are two factors causing the domain shift: different organs in task1 and different scanners in task2.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For task1, the dataset consisted of 290 pathological images of six different adenocarcinomas extracted from the WSI digitized by the TEKSQRAY SQS-600P scanner, with an average size of 1500 x 1500 pixels. The training set consisted of 180 images from 3 organs (gastric adenocarcinoma, colorectal adenocarcinoma and pancreatic ductal adenocarcinoma), the preliminary test set consisted of 20 images from 4 organs, and the final test set consisted of 90 images from 6 organs.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">For task2, the data included 290 images of invasive breast cancer obtained from six different WSI scanners, each approximately 1500 x 1500 pixels in size. The training set consisted of 180 images from 3 scanners, the preliminary test set consisted of 20 images from 4 scanners, and the final test set consisted of 90 images from 6 scanners. Please refer to Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S2.T1" title="Table 1 ‣ 2 Methods ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a> for detailed data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation details</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The method is implemented based on MMsegmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#bib.bib11" title="">11</a>]</cite> codebase. All our experiments are conducted on a single NVIDIA GeForce RTX 4090 with 24GB. We chose three VFMs as fine-tuning objects, including SAM-h, ConvNeXt, DINOv2. For the training pharse, we employed the AdamW optimizer to optimize the model during the back propagation. We configured the learning rate to 1e-5 for the backbone and 1e-4 for both the decode head and the Rein. In addition, we use a setup of 60,000 iterations with a batch size of 4, cropping images to a resolution of 512 × 512.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">First of all, since the test set is not publicly available at present, in the training stage, we randomly divide the training set in a ratio of 8:2 for training and testing to evaluate the methods. To evaluate the segmentation performance of the different methods, we utilize two common evaluation metrics: average Dice-Similarity Coefficient (DSC), the mean Intersection over Union (mIoU). Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S4.T2" title="Table 2 ‣ 4 Results ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows the segmentation results of different methods for task1 and task2. When combined with Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S4.T2" title="Table 2 ‣ 4 Results ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S4.F2" title="Figure 2 ‣ 4 Results ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">2</span></a> , it is clear that fine-tuning ConvNeXt and DINOv2 using rein for task1 and task2, respectively, achieves the best segmentation results and demonstrates strong generalization capabilities.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">By detailing the features of each layer of backbone and connecting it to instances, rein can greatly narrow the gap between different organs and scanner domains. Otherwise, due to the differences between the pre-training weights of different models, we found that ConvNext’s pre-trained model was more suitable for cross-organ adenocarcinoma segmentation, while DINOv2’s pre-trained model was more suitable for cross-scanner adenocarcinoma segmentation.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The quantitative results of the different methods for the dataset of task1 and task2. The symbol <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.m1.1"><semantics id="S4.T2.2.m1.1b"><mo id="S4.T2.2.m1.1.1" stretchy="false" xref="S4.T2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><ci id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.m1.1e">↑</annotation></semantics></math> indicates the larger the better. The best result is in <span class="ltx_text ltx_font_bold" id="S4.T2.8.1">Blod</span>. </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.6.4.5">Task1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.1.1">DSC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.3.1.1.m1.1"><semantics id="S4.T2.3.1.1.m1.1a"><mo id="S4.T2.3.1.1.m1.1.1" stretchy="false" xref="S4.T2.3.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><ci id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.4.2.2">mIoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.2.2.m1.1"><semantics id="S4.T2.4.2.2.m1.1a"><mo id="S4.T2.4.2.2.m1.1.1" stretchy="false" xref="S4.T2.4.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><ci id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.6.4.6">Task2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.5.3.3">DSC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.5.3.3.m1.1"><semantics id="S4.T2.5.3.3.m1.1a"><mo id="S4.T2.5.3.3.m1.1.1" stretchy="false" xref="S4.T2.5.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.3.m1.1b"><ci id="S4.T2.5.3.3.m1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.4.4">mIoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.6.4.4.m1.1"><semantics id="S4.T2.6.4.4.m1.1a"><mo id="S4.T2.6.4.4.m1.1.1" stretchy="false" xref="S4.T2.6.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.4.4.m1.1b"><ci id="S4.T2.6.4.4.m1.1.1.cmml" xref="S4.T2.6.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.4.4.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.6.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.6.5.1.1">ConvNeXt</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.6.5.1.2.1">0.8568</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.5.1.3">0.7433</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.5.1.4">ConvNeXt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.1.5">0.8959</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.1.6">0.7497</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.6.6.2.1">SAM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.6.2.2">0.8489</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.6.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.6.6.2.3.1">0.7502</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.6.2.4">SAM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.6.2.5">0.8914</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.6.2.6">0.7665</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.6.7.3.1">DINOv2</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.6.7.3.2">0.8477</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.6.7.3.3">0.7426</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.6.7.3.4">DINOv2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.6.7.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.3.5.1">0.9054</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.6.7.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.3.6.1">0.7721</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="S4.F2.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The segmentation results of different methods for the dataset of both tasks. </figcaption>
</figure>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">In addition, we present the evaluation results on the preliminary test set and the final test set in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S4.T3" title="Table 3 ‣ 4 Results ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">3</span></a> and in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11752v3#S4.T4" title="Table 4 ‣ 4 Results ‣ Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models"><span class="ltx_text ltx_ref_tag">4</span></a>. The final score is expressed as: scores = 0.5<math alttext="\times" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mo id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><times id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">×</annotation></semantics></math> DSC + 0.5 <math alttext="\times" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mo id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><times id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">×</annotation></semantics></math> JSC, where JSC shows Jaccard Similarity Coefficient. For task1, our team achieved a score of 0.7719 and 0.7557 on
the preliminary test phase and final test phase using Rein to fine-tune
ConvNeXt, respectively. For task2, our team achieved a score of 0.8848
and 0.8192 on the preliminary test phase and final test phase using Rein
to fine-tune DINOv2, respectively.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The quantitative results of the different teams on the preliminary test set for the task1 and task2. We have only listed the results of the top ten teams and the pink areas in the table represent our team’s results.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.2.2.3">Teams</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1">Task1 score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.2.2.4">Teams</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2">Task2 score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.3.1.1">agalaran</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.3.1.2">0.7865</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.3.1.3">deepmicroscopy</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.1.4">0.8858</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.4.2.1">deepmicroscopy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.4.2.2">0.7776</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.4.2.3" style="background-color:#FFE6E6;">Zhijian Life</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.4.2.4" style="background-color:#FFE6E6;">0.8848</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.5.3.1" style="background-color:#FFE6E6;">Zhijian Life</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.5.3.2" style="background-color:#FFE6E6;">0.7719</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.5.3.3">ICT_team</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.5.3.4">0.8833</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.6.4.1">ICT_team</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.6.4.2">0.7714</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.6.4.3">SMF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.4.4">0.8775</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.7.5.1">Sanmed_AI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.7.5.2">0.7633</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.7.5.3">Biototem</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.5.4">0.8728</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.8.6.1">Biototem</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.8.6.2">0.7625</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.8.6.3">DeepLearnAI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.8.6.4">0.8646</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.9.7.1">Amaranth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.9.7.2">0.7534</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.9.7.3">Amaranth</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.9.7.4">0.8643</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.10.8.1">baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.10.8.2">0.7351</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.10.8.3">agalaran</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.10.8.4">0.8584</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.11.9.1">DeepLearnAI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.11.9.2">0.7198</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.11.9.3">Sanmed_AI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.11.9.4">0.8457</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.12.10.1">excute(me)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.12.10.2">0.7049</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.12.10.3">Team-Tiger</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.2.12.10.4">0.8408</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The quantitative results of the different teams on the final test set for the task1 and task2. The pink areas in the table represent our team’s results.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.2.2.3">Teams</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.1">Task1 score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.2.2.4">Teams</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.2.2">Task2 score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.1">deepmicroscopy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.2">0.8020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.3">deepmicroscopy</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.4">0.8527</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.4.2.1">ICT_team</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.4.2.2">0.7976</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.4.2.3">Biototem</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.2.4">0.8354</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.5.3.1">Amaranth</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.5.3.2">0.7774</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.5.3.3" style="background-color:#FFE6E6;">Zhijian Life</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.5.3.4" style="background-color:#FFE6E6;">0.8192</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.6.4.1">Sanmed_AI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.6.4.2">0.7753</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.6.4.3">agalaran</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.6.4.4">0.8175</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.7.5.1">Biototem</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.7.5.2">0.7643</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.7.5.3">Amaranth</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.5.4">0.8128</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.8.6.1">agaldran</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.8.6.2">0.7607</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.8.6.3">Team-Tiger</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6.4">0.8093</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.9.7.1">Team-Tiger</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.9.7.2">0.7583</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.9.7.3">ICT_team</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.9.7.4">0.7944</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.10.8.1" style="background-color:#FFE6E6;">Zhijian Life</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.10.8.2" style="background-color:#FFE6E6;">0.7557</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.10.8.3">SMF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.8.4">0.7924</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.11.9.1">DeepLearnAI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.11.9.2">0.7469</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.11.9.3">DeepLearnAI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.11.9.4">0.7597</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.2.12.10.1">Long Xin</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.2.12.10.2">0.6446</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.2.12.10.3">Sanmed_AI</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.2.12.10.4">0.7420</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we use Rein to parameterically and efficiently fine-tune ConvNeXt and DINOv2 for MICCAI 2024 COSAS 2024. Extensive experiments demonstrate that Rein
fine-tuned the vision foundation model to achieve satisfactory results. We believe that Rein fine-tuned VFMs has great potential to generalize in the field of adenocarcinoma, whether adenocarcinoma comes from an organ or a scanner.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J Matuszewski, Elia Bruni, Urko Sanchez, et al.

</span>
<span class="ltx_bibblock">Gland segmentation in colon histology images: The glas challenge contest.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Medical image analysis</span>, 35:489–502, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Qian Da, Xiaodi Huang, Zhongyu Li, Yanfei Zuo, Chenbin Zhang, Jingxin Liu, Wen Chen, Jiahui Li, Dou Xu, Zhiqiang Hu, et al.

</span>
<span class="ltx_bibblock">Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Medical Image Analysis</span>, 80:102485, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Marc Aubreville, Nikolas Stathonikos, Christof A Bertram, Robert Klopfleisch, Natalie Ter Hoeve, Francesco Ciompi, Frauke Wilm, Christian Marzahl, Taryn A Donovan, Andreas Maier, et al.

</span>
<span class="ltx_bibblock">Mitosis domain generalization in histopathology images—the midog challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Medical Image Analysis</span>, 84:102699, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Marc Aubreville, Nikolas Stathonikos, Taryn A Donovan, Robert Klopfleisch, Jonas Ammeling, Jonathan Ganz, Frauke Wilm, Mitko Veta, Samir Jabari, Markus Eckstein, et al.

</span>
<span class="ltx_bibblock">Domain generalization across tumor types, laboratories, and species—insights from the 2022 edition of the mitosis domain generalization challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Medical Image Analysis</span>, 94:103155, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Zhixiang Wei, Lin Chen, Yi Jin, Xiaoxiao Ma, Tianle Liu, Pengyang Ling, Ben Wang, Huaian Chen, and Jinjin Zheng.

</span>
<span class="ltx_bibblock">Stronger fewer &amp; superior: Harnessing vision foundation models for domain generalized semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 28619–28630, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4015–4026, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.

</span>
<span class="ltx_bibblock">A convnet for the 2020s.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 11976–11986, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2304.07193</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar.

</span>
<span class="ltx_bibblock">Masked-attention mask transformer for universal image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 1290–1299, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
MMSegmentation Contributors.

</span>
<span class="ltx_bibblock">Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmsegmentation" title="">https://github.com/open-mmlab/mmsegmentation</a>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 29 05:25:20 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
