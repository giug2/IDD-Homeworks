<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1909.11875] Federated Learning in Mobile Edge Networks: A Comprehensive Survey</title><meta property="og:description" content="In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applicat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning in Mobile Edge Networks: A Comprehensive Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning in Mobile Edge Networks: A Comprehensive Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1909.11875">

<!--Generated on Thu Mar  7 10:45:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  mobile edge networks,  resource allocation,  communication cost,  data privacy,  data security
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning in Mobile Edge Networks: A Comprehensive Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Wei Yang Bryan Lim,
Nguyen Cong Luong,
Dinh Thai Hoang,
Yutao Jiao,
Ying-Chang Liang, <span id="id1.1.id1" class="ltx_text ltx_font_italic">Fellow, IEEE</span>,
Qiang Yang, <span id="id2.2.id2" class="ltx_text ltx_font_italic">Fellow, IEEE</span>,
Dusit Niyato, <span id="id3.3.id3" class="ltx_text ltx_font_italic">Fellow, IEEE</span>,
and Chunyan Miao
</span><span class="ltx_author_notes">W. Y. B. Lim is with Alibaba Group and the Alibaba-NTU Joint Research Institute, Nanyang Technological University, Singapore. Email: limw0201@e.ntu.edu.sg.N. C. Luong is with Faculty of Information Technology, PHENIKAA University, Hanoi 12116, Vietnam, and is with PHENIKAA Research and Technology Institute (PRATI), A&amp;A Green Phoenix
Group JSC, No.167 Hoang Ngan, Trung Hoa, Cau Giay, Hanoi 11313, Vietnam. Email: luong.nguyencong@phenikaa-uni.edu.vn.D. T. Hoang is with the School of Electrical and Data Engineering, University of Technology Sydney, Australia. E-mail: hoang.dinh@uts.edu.au.Y. Jiao, D. Niyato and C. Miao are with School of Computer Science and Engineering, Nanyang Technological University, Singapore. E-mails:
yjiao001@e.ntu.edu.sg, dniyato@ntu.edu.sg, ascymiao@ntu.edu.sg.Y.-C. Liang is with the Center for Intelligent Networking and Communications (CINC), University of Electronic Science and Technology of China (UESTC), Chengdu 611731, China. E-mail: liangyc@ieee.org.Q. Yang is with Hong Kong University of Science and Technology, Hong Kong, China. Email: qyang@cse.ust.hk.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, mobile edge networks, resource allocation, communication cost, data privacy, data security

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.2" class="ltx_p">Currently, there are nearly <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S1.p1.1.m1.1a"><mn id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><cn type="integer" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">7</annotation></semantics></math> billion connected Internet of Things (IoT) devices<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and <math id="S1.p1.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S1.p1.2.m2.1a"><mn id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><cn type="integer" id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">3</annotation></semantics></math> billion smartphones around the world. These devices are equipped with increasingly advanced sensors, computing, and communication capabilities. As such, they can potentially be deployed for various crowdsensing tasks, e.g., for medical purposes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and air quality monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Coupled with the rise of Deep Learning (DL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, the wealth of data collected by end devices opens up countless possibilities for meaningful research and applications.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the traditional cloud-centric approach, data collected by mobile devices is uploaded and processed centrally in a cloud-based server or data center. In particular, data collected by IoT devices and smartphones such as measurements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, photos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and location information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are aggregated at the data center <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Thereafter, the data is used to provide insights or produce effective inference models. However, this approach is no longer sustainable for the following reasons. Firstly, data owners are increasingly privacy sensitive. Following privacy concerns among consumers in the age of big data, policy makers have responded with the implementation of data privacy legislations such as the European Commission’s General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and Consumer Privacy Bill of Rights in the US <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In particular, the consent (GDPR Article 6) and data minimalization principle (GDPR Article 5) limits data collection and storage only to what is consumer-consented and absolutely necessary for processing. Secondly, a cloud-centric approach involves long propagation delays and incurs unacceptable latency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for applications in which real-time decisions have to be made, e.g., in self-driving car systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Thirdly, the transfer of data to the cloud for processing burdens the backbone networks especially in tasks involving unstructured data, e.g., in video analytics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This is exacerbated by the fact that cloud-centric training is relatively reliant on wireless communications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. As a result, this can potentially impede the development of new technologies.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With data sources mainly located outside the cloud today <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, Mobile Edge Computing (MEC) has naturally been proposed as a solution in which the computing and storage capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> of end devices and edge servers are leveraged on to bring model training closer to where data is produced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. As defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, an end-edge-cloud computing network comprises: (i) end devices, (ii) edge nodes, and (iii) cloud server. For model training in conventional MEC approaches, a collaborative paradigm has been proposed in which training data are first sent to the edge servers for model training up to lower level DNN layers, before more computation intensive tasks are offloaded to the cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). However, this arrangement incurs significant communication costs and is unsuitable especially for applications that require persistent training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In addition, computation offloading and data processing at edge servers still involve the transmission of potentially sensitive personal data. This can discourage privacy-sensitive consumers from taking part in model training, or even violate increasingly stringent privacy laws <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Although various privacy preservation methods, e.g.,
differential privacy (DP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, have been proposed, a number of users are still not willing to expose their private
data for fear that their data may be inspected by external servers. In the long run, this discourages the
development of technologies as well as new applications.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/cloudedgefl.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">Edge AI approach brings AI processing closer to where data is produced. In particular, FL allows training on devices where the data is produced.</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To guarantee that training data remains on personal devices and to facilitate collaborative machine learning of complex models among distributed devices, a decentralized ML approach called Federated Learning (FL) is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. In FL, mobile devices use their local data to cooperatively train an ML model required by an FL server. They then send the model updates, i.e., the model’s weights, to the FL server for aggregation. The steps are repeated in multiple rounds until a desirable accuracy is achieved. This implies that FL can be an enabling technology for ML model training at mobile edge networks. As compared to conventional cloud-centric ML model training approaches, the implementation of FL for model training at mobile edge networks features the following advantages.</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Highly efficient use of network bandwidth:</span> Less information is required to be transmitted to the cloud. For example, instead of sending the raw data over for processing, participating devices only send the updated model parameters for aggregation. As a result, this significantly reduces costs of data communication and relieves the burden on backbone networks.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Privacy:</span> Following the above point, the raw data of users need not be sent to the cloud. Under the assumption that FL participants and servers are non-malicious, this enhances user privacy and reduces the probability of eavesdropping to a certain extent. In fact, with enhanced privacy, more users will be willing to take part in collaborative model training and so, better inference models can be built.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Low latency:</span> With FL, ML models can be consistently trained and updated. Meanwhile, in the MEC paradigm, real-time decisions, e.g., event detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, can be made locally at the edge nodes or end devices. Therefore, the latency is much lower than that when decisions are made in the cloud before transmitting them to the end devices. This is vital for time critical applications such as self-driving car systems in which the slightest delays can potentially be life threatening <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Given the aforementioned advantages, FL has seen recent successes in several applications. For example, the Federated Averaging algorithm (<span id="S1.p6.1.1" class="ltx_text ltx_font_italic">FedAvg</span>) proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> has been applied to Google’s Gboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to improve next-word prediction models. In addition, several studies have also explored the use of FL in a number of scenarios in which data is sensitive in nature, e.g., to develop predictive models for diagnosis in health AI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and to foster collaboration across multiple hospitals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and Government agencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Besides being an enabling technology for ML model training <span id="S1.p7.1.1" class="ltx_text ltx_font_italic">at</span> mobile edge networks, FL has also been increasingly applied as an enabling technology <span id="S1.p7.1.2" class="ltx_text ltx_font_italic">for</span> mobile edge network optimization. Given the computation and storage constraints of increasingly complex mobile edge networks, conventional network optimization approaches that are built on static models fare relatively poorly in modelling dynamic networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. As such, a data-driven Deep Learning (DL) based approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for optimizing resource allocation is increasingly popular. For example, DL can be used for representation learning of network conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> whereas Deep Reinforcement Learning (DRL) can optimize decision making through interactions with the dynamic environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. However, the aforementioned approaches require user data as an input and these data may be sensitive or inaccessible in nature due to regulatory constraints. As such, in this survey, we also consider FL’s potential to serve as an enabling technology for optimizing mobile edge networks, e.g., in cell association <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, computation offloading <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and vehicular networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">However, there are several challenges to be solved before FL can be implemented at scale. Firstly, even though raw data no longer needs to be sent to the cloud servers, communication costs remain an issue due to the high dimensonality of model updates and limited communication bandwith of participating mobile devices. In particular, state-of-the art DNN model training can involve the communication of millions of parameters for aggregation. Secondly, in a large and complex mobile edge network, the heterogeneity of participating devices in terms of data quality, computation power, and willingness to participate have to be well managed from the resource allocation perspective. Thirdly, FL does not guarantee privacy in the presence of malicious participants or aggregating servers. In particular, recent research works have clearly shown that a malicious participant may exist in FL and can infer the information of other participants just from the shared parameters alone. As such, privacy and security issues in FL still need to be considered.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Although there are surveys on MEC and FL, the existing studies usually treat the two topics separately. For existing surveys on FL, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> place more emphasis on discussing the architecture and categorization of different FL settings to be used for the varying distributions of training data. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> highlight the applications of FL in wireless communications but do not discuss the issues pertaining to FL implementation. In addition, the focus of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is on cellular network architecture rather than mobile edge networks. In contrast, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> provide a brief tutorial on FL and the challenges related to its implementation, but do not consider the issue of resource allocation in FL, or the potential applications of FL for mobile edge network optimization. On the other hand, for surveys in MEC that focus on implementing ML model training at edge networks, a macroscopic approach is usually adopted in which FL is briefly mentioned as one of the enabling technologies in the MEC paradigm, but without detailed elaboration with regards to its implementation or the related challenges. In particular, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> study the architectures and process of training and inference at edge networks without considering the challenges to FL implementation. In addition, surveys studying the implementation of DL for mobile edge network optimization mostly do not focus on FL as a potential solution to preserve data privacy. For example, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> discuss strategies for optimizing caching and computation offloading for mobile edge networks, but do not consider the use of privacy preserving federated approaches in their studies. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> considers the use of DRL in communications and networking but do not include federated DRL approaches.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">An overview of selected surveys in FL and MEC</span></figcaption>
<table id="S1.T1.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.5.1" class="ltx_tr" style="background-color:#AEAAAA;">
<td id="S1.T1.5.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.5.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#AEAAAA;">Ref.</span></td>
<td id="S1.T1.5.1.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#AEAAAA;">Subject<span id="S1.T1.5.1.2.1.1" class="ltx_text ltx_font_medium"></span></span></td>
<td id="S1.T1.5.1.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.1.3.1" class="ltx_text ltx_font_bold" style="background-color:#AEAAAA;">Contribution<span id="S1.T1.5.1.3.1.1" class="ltx_text ltx_font_medium"></span></span></td>
</tr>
<tr id="S1.T1.5.2" class="ltx_tr">
<td id="S1.T1.5.2.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.2.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></span></td>
<td id="S1.T1.5.2.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">
<span id="S1.T1.5.2.2.1" class="ltx_text">FL</span></td>
<td id="S1.T1.5.2.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Introductory tutorial on categorization of different FL settings, e.g., vertical FL, horizontal FL, and Federated Transfer Learning</td>
</tr>
<tr id="S1.T1.5.3" class="ltx_tr">
<td id="S1.T1.5.3.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.3.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></span></td>
<td id="S1.T1.5.3.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.3.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">FL in optimizing resource allocation for wireless networks while preserving data privacy</td>
</tr>
<tr id="S1.T1.5.4" class="ltx_tr">
<td id="S1.T1.5.4.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.4.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite></span></td>
<td id="S1.T1.5.4.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.4.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Tutorial on FL and discussions of implementation challenges in FL</td>
</tr>
<tr id="S1.T1.5.5" class="ltx_tr">
<td id="S1.T1.5.5.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.5.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span></td>
<td id="S1.T1.5.5.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">
<span id="S1.T1.5.5.2.1" class="ltx_text">MEC</span></td>
<td id="S1.T1.5.5.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Computation offloading strategy to optimize DL performance in edge computing</td>
</tr>
<tr id="S1.T1.5.6" class="ltx_tr">
<td id="S1.T1.5.6.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.6.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></span></td>
<td id="S1.T1.5.6.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.6.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on architectures and frameworks for edge intelligence</td>
</tr>
<tr id="S1.T1.5.7" class="ltx_tr">
<td id="S1.T1.5.7.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.7.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite></span></td>
<td id="S1.T1.5.7.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.7.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">ML for IoT management, e.g., network management and security</td>
</tr>
<tr id="S1.T1.5.8" class="ltx_tr">
<td id="S1.T1.5.8.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.8.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span></td>
<td id="S1.T1.5.8.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.8.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on computation offloading in MEC</td>
</tr>
<tr id="S1.T1.5.9" class="ltx_tr">
<td id="S1.T1.5.9.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.9.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></span></td>
<td id="S1.T1.5.9.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.9.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on DRL approaches to address issues in communications and networking</td>
</tr>
<tr id="S1.T1.5.10" class="ltx_tr">
<td id="S1.T1.5.10.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.10.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite></span></td>
<td id="S1.T1.5.10.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.10.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on techniques for computation offloading</td>
</tr>
<tr id="S1.T1.5.11" class="ltx_tr">
<td id="S1.T1.5.11.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.11.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite></span></td>
<td id="S1.T1.5.11.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.11.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on architectures and applications of MEC</td>
</tr>
<tr id="S1.T1.5.12" class="ltx_tr">
<td id="S1.T1.5.12.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.12.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></span></td>
<td id="S1.T1.5.12.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.12.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on computing, caching, and communication issues at mobile edge networks</td>
</tr>
<tr id="S1.T1.5.13" class="ltx_tr">
<td id="S1.T1.5.13.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.5.13.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></span></td>
<td id="S1.T1.5.13.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_r"></td>
<td id="S1.T1.5.13.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t">Survey on the phases of caching and comparison among the different caching schemes</td>
</tr>
<tr id="S1.T1.5.14" class="ltx_tr">
<td id="S1.T1.5.14.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.5.14.1.1" class="ltx_text" style="color:#000000;">  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span></td>
<td id="S1.T1.5.14.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_b ltx_border_r"></td>
<td id="S1.T1.5.14.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Survey on joint mobile computing and wireless communication resource management in MEC</td>
</tr>
</table>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/flowchart.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text" style="font-size:90%;">Classification of related studies to be discussed in this survey. </span></figcaption>
</figure>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">In summary, most existing surveys on FL do not consider the applications of FL in the context of mobile edge networks, whereas existing surveys on MEC do not consider the challenges to FL implementation, or the potential of FL to be applied in mobile edge network optimization. This motivates us to have a comprehensive survey that has the following contributions:</p>
</div>
<div id="S1.p11" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We motivate the importance of FL as an important paradigm shift towards enabling collaborative ML model training. Then, we provide a concise tutorial on FL implementation and present to the reader a list of useful open-source frameworks that paves the way for future research on FL and its applications.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We discuss the unique features of FL relative to a centralized ML approach and the resulting implementation challenges. For each of this challenge, we present to the reader a comprehensive discussion of existing solutions and approaches explored in the FL literature.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We discuss FL as an enabling technology for mobile edge network optimization. In particular, we discuss the current and potential applications of FL as a privacy-preserving approach for applications in edge computing.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">We discuss the challenges and future research directions of FL.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">For the reader’s convenience, we classify the related studies to be discussed in this survey in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The classification is based on (i) FL at mobile edge network, i.e., studies that focus on solving the challenges and issues related to implementing the collaborative training of ML models on end devices, and (ii) FL for mobile edge network, i.e., studies that specifically explore the application of FL for mobile edge network optimization. While the former group of studies works on addressing the fundamental issues of FL, the latter group uses FL as an application tool to solve issues in edge computing. We also present a list of common abbreviations for reference in Table <a href="#S1.T2" title="TABLE II ‣ I Introduction ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div id="S1.p13" class="ltx_para">
<p id="S1.p13.1" class="ltx_p">The rest of this paper is organized as follows. Section <a href="#S2" title="II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> introduces the background and fundamentals of FL. Section <a href="#S3" title="III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> reviews solutions provided to reduce communication costs. Section <a href="#S4" title="IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> discusses resource allocation approaches in FL. Section <a href="#S5" title="V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> discusses privacy and security issues. Section <a href="#S6" title="VI Applications of Federated Learning for Mobile Edge Computing ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> discusses applications of FL for mobile edge network optimization. Section <a href="#S7" title="VII Challenges and Future Research Directions ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> discusses the challenges and future research directions in FL. Section <a href="#S8" title="VIII Conclusion ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a> concludes the paper.</p>
</div>
<figure id="S1.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T2.3.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S1.T2.4.2" class="ltx_text" style="font-size:90%;">List of common abbreviations. </span></figcaption>
<table id="S1.T2.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T2.5.1" class="ltx_tr" style="background-color:#9B9B9B;">
<td id="S1.T2.5.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T2.5.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#9B9B9B;">Abbreviation</span></td>
<td id="S1.T2.5.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T2.5.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#9B9B9B;">Description</span></td>
</tr>
<tr id="S1.T2.5.2" class="ltx_tr">
<td id="S1.T2.5.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">BAA</td>
<td id="S1.T2.5.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Broadband Analog Aggregation</td>
</tr>
<tr id="S1.T2.5.3" class="ltx_tr">
<td id="S1.T2.5.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">CNN</td>
<td id="S1.T2.5.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Convolutional Neural Network</td>
</tr>
<tr id="S1.T2.5.4" class="ltx_tr">
<td id="S1.T2.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">CV</td>
<td id="S1.T2.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Computer Vision</td>
</tr>
<tr id="S1.T2.5.5" class="ltx_tr">
<td id="S1.T2.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">DDQN</td>
<td id="S1.T2.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Double Deep Q-Network</td>
</tr>
<tr id="S1.T2.5.6" class="ltx_tr">
<td id="S1.T2.5.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">DL</td>
<td id="S1.T2.5.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Learning</td>
</tr>
<tr id="S1.T2.5.7" class="ltx_tr">
<td id="S1.T2.5.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">DNN</td>
<td id="S1.T2.5.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S1.T2.5.8" class="ltx_tr">
<td id="S1.T2.5.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">DP</td>
<td id="S1.T2.5.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Differential Privacy</td>
</tr>
<tr id="S1.T2.5.9" class="ltx_tr">
<td id="S1.T2.5.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">DQL</td>
<td id="S1.T2.5.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Q-Learning</td>
</tr>
<tr id="S1.T2.5.10" class="ltx_tr">
<td id="S1.T2.5.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">DRL</td>
<td id="S1.T2.5.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Reinforcement Learning</td>
</tr>
<tr id="S1.T2.5.11" class="ltx_tr">
<td id="S1.T2.5.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedAvg</td>
<td id="S1.T2.5.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Federated Averaging</td>
</tr>
<tr id="S1.T2.5.12" class="ltx_tr">
<td id="S1.T2.5.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FL</td>
<td id="S1.T2.5.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Federated Learning</td>
</tr>
<tr id="S1.T2.5.13" class="ltx_tr">
<td id="S1.T2.5.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">GAN</td>
<td id="S1.T2.5.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Generative Adversarial Network</td>
</tr>
<tr id="S1.T2.5.14" class="ltx_tr">
<td id="S1.T2.5.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">IID</td>
<td id="S1.T2.5.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S1.T2.5.14.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T2.5.14.2.1.1" class="ltx_tr">
<td id="S1.T2.5.14.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Independent and Identically</td>
</tr>
<tr id="S1.T2.5.14.2.1.2" class="ltx_tr">
<td id="S1.T2.5.14.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Distributed</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T2.5.15" class="ltx_tr">
<td id="S1.T2.5.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">IoT</td>
<td id="S1.T2.5.15.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Internet of Things</td>
</tr>
<tr id="S1.T2.5.16" class="ltx_tr">
<td id="S1.T2.5.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">IoV</td>
<td id="S1.T2.5.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Internet of Vehicles</td>
</tr>
<tr id="S1.T2.5.17" class="ltx_tr">
<td id="S1.T2.5.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">LSTM</td>
<td id="S1.T2.5.17.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Long Short Term Memory</td>
</tr>
<tr id="S1.T2.5.18" class="ltx_tr">
<td id="S1.T2.5.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">MEC</td>
<td id="S1.T2.5.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Mobile Edge Computing</td>
</tr>
<tr id="S1.T2.5.19" class="ltx_tr">
<td id="S1.T2.5.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">ML</td>
<td id="S1.T2.5.19.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Machine Learning</td>
</tr>
<tr id="S1.T2.5.20" class="ltx_tr">
<td id="S1.T2.5.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">MLP</td>
<td id="S1.T2.5.20.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Multilayer Perceptron</td>
</tr>
<tr id="S1.T2.5.21" class="ltx_tr">
<td id="S1.T2.5.21.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">NLP</td>
<td id="S1.T2.5.21.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Natural Language Processing</td>
</tr>
<tr id="S1.T2.5.22" class="ltx_tr">
<td id="S1.T2.5.22.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">OFDMA</td>
<td id="S1.T2.5.22.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S1.T2.5.22.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T2.5.22.2.1.1" class="ltx_tr">
<td id="S1.T2.5.22.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Orthogonal Frequency-division</td>
</tr>
<tr id="S1.T2.5.22.2.1.2" class="ltx_tr">
<td id="S1.T2.5.22.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Multiple Access</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T2.5.23" class="ltx_tr">
<td id="S1.T2.5.23.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">QoE</td>
<td id="S1.T2.5.23.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Quality of Experience</td>
</tr>
<tr id="S1.T2.5.24" class="ltx_tr">
<td id="S1.T2.5.24.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">RNN</td>
<td id="S1.T2.5.24.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Recurrent Neural Network</td>
</tr>
<tr id="S1.T2.5.25" class="ltx_tr">
<td id="S1.T2.5.25.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">SGD</td>
<td id="S1.T2.5.25.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Stochastic Gradient Descent</td>
</tr>
<tr id="S1.T2.5.26" class="ltx_tr">
<td id="S1.T2.5.26.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">SMPC</td>
<td id="S1.T2.5.26.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Secure Multiparty Computation</td>
</tr>
<tr id="S1.T2.5.27" class="ltx_tr">
<td id="S1.T2.5.27.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">SNR</td>
<td id="S1.T2.5.27.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Signal-to-noise ratio</td>
</tr>
<tr id="S1.T2.5.28" class="ltx_tr">
<td id="S1.T2.5.28.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">SVM</td>
<td id="S1.T2.5.28.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Support Vector Machine</td>
</tr>
<tr id="S1.T2.5.29" class="ltx_tr">
<td id="S1.T2.5.29.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">TFF</td>
<td id="S1.T2.5.29.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TensorFlow Federated</td>
</tr>
<tr id="S1.T2.5.30" class="ltx_tr">
<td id="S1.T2.5.30.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">UE</td>
<td id="S1.T2.5.30.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">User Equipment</td>
</tr>
<tr id="S1.T2.5.31" class="ltx_tr">
<td id="S1.T2.5.31.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">URLLC</td>
<td id="S1.T2.5.31.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Ultra reliable low latency communication</td>
</tr>
</table>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background and Fundamentals of Federated Learning</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Artificial Intelligence (AI) has become an essential part of our lives today, following the recent successes and progression of DL in several domains, e.g., Computer Vision (CV) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and Natural Language Processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. In traditional training of Deep Neural Networks (DNNs), a cloud based approach is adopted whereby data is centralized and model training occurs in powerful cloud servers. However, given the ubiquity of mobile devices that are equipped with increasingly advanced sensing and computing capabilities, the trend of migrating intelligence from the cloud to the edge, i.e., in the MEC paradigm, has naturally arisen. In addition, amid growing privacy concerns, the concept of FL has been proposed.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">FL involves the collaborative training of DNN models on end devices. There are, in general, two steps in the FL training process namely (i) local model training on end devices and (ii) global aggregation of updated parameters in the FL server. In this section, we first provide a brief introduction to DNN model training, which generalizes local model training in FL. Note that while FL can be applied to the training of ML models in general, we focus specifically on DNN model training in this section as a majority of the papers that we subsequently review study the federated training of DNN models. In addition, the DNN models are easily aggregated and outperform conventional ML techniques especially when the data is large. The implementation of FL at mobile edge networks can thus naturally leverage on the increasing computing power and wealth of data collected by distributed end devices, both of which are driving forces contributing to the rise of DL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. As such, a brief introduction to general DNN model training will be useful for subsequent sections. Thereafter, we proceed to provide a tutorial of the FL training process that incorporates both global aggregation and local training. In addition, we also highlight the statistical challenges of FL model training and present the protocols and open-source frameworks of FL.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Deep Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Conventional ML algorithms rely on <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">hand-engineered</span> feature extractors to process raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. As such, domain expertise is often a prerequisite for building an effective ML model. In addition, feature selection has to be customized and reinitiated for each new problem. On the other hand, DNNs are representation learning based, i.e., DNNs can automatically discover and learn these features from raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and thus often outperform conventional ML algorithms especially when there is an abundance of data.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">DL lies within the domain of the brain-inspired computing paradigm, of which the neural network is an important part of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. In general, a neural network design emulates that of a neuron <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. It comprises three layers: (i) input layer, (ii) hidden layer, and (iii) output layer. In a conventional feedforward neural network, a weighted and bias-corrected input value is passed through a non-linear activation function to derive an output <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> (Fig. <a href="#S2.F3" title="Figure 3 ‣ II-A Deep Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Some activation functions include the ReLu and softmax functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. A typical DNN comprises multiple hidden layers that map an input to an output. For example, the goal of a DNN trained for image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> is to produce a vector of scores as the output, in which the positional index of the highest score corresponds to the class to which the input image is classified to belong. As such, the objective of training a DNN is to optimize the weights of the network such that the loss function, i.e., difference between the ground truth and model output, is minimized.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/backprop.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="347" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.2.1" class="ltx_text" style="font-size:90%;">In forward pass, an output is derived from the weights and inputs. In backpropagation, the input gradient <math id="S2.F3.2.1.m1.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S2.F3.2.1.m1.1b"><mi id="S2.F3.2.1.m1.1.1" xref="S2.F3.2.1.m1.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S2.F3.2.1.m1.1c"><ci id="S2.F3.2.1.m1.1.1.cmml" xref="S2.F3.2.1.m1.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.2.1.m1.1d">e</annotation></semantics></math> is used to calibrate the weights of the DNN model.</span></figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.3" class="ltx_p">Before training, the dataset is first split into the training and test dataset. Then, the training dataset is used as input data for the optimization of weights in the DNN. The weights are calibrated through stochastic gradient descent (SGD), in which the weights are updated by the product of (i) the learning rate <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="lr" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.1.m1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><times id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1"></times><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">𝑙</ci><ci id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">lr</annotation></semantics></math>, i.e., the step size of gradient descent in each iteration, and (ii) partial derivative of the loss function <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">L</annotation></semantics></math> with respect to the weight <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">w</annotation></semantics></math>. The SGD formula is as follows:</p>
<table id="S8.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\displaystyle W=W-lr\frac{\partial L}{\partial W}" display="inline"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">W</mi><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">W</mi><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">−</mo><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.1" xref="S2.E1.m1.1.1.3.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.1a" xref="S2.E1.m1.1.1.3.3.1.cmml">​</mo><mstyle displaystyle="true" id="S2.E1.m1.1.1.3.3.4" xref="S2.E1.m1.1.1.3.3.4.cmml"><mfrac id="S2.E1.m1.1.1.3.3.4a" xref="S2.E1.m1.1.1.3.3.4.cmml"><mrow id="S2.E1.m1.1.1.3.3.4.2" xref="S2.E1.m1.1.1.3.3.4.2.cmml"><mo rspace="0em" id="S2.E1.m1.1.1.3.3.4.2.1" xref="S2.E1.m1.1.1.3.3.4.2.1.cmml">∂</mo><mi id="S2.E1.m1.1.1.3.3.4.2.2" xref="S2.E1.m1.1.1.3.3.4.2.2.cmml">L</mi></mrow><mrow id="S2.E1.m1.1.1.3.3.4.3" xref="S2.E1.m1.1.1.3.3.4.3.cmml"><mo rspace="0em" id="S2.E1.m1.1.1.3.3.4.3.1" xref="S2.E1.m1.1.1.3.3.4.3.1.cmml">∂</mo><mi id="S2.E1.m1.1.1.3.3.4.3.2" xref="S2.E1.m1.1.1.3.3.4.3.2.cmml">W</mi></mrow></mfrac></mstyle></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑊</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><minus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></minus><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">𝑊</ci><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><times id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1"></times><ci id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2">𝑙</ci><ci id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3">𝑟</ci><apply id="S2.E1.m1.1.1.3.3.4.cmml" xref="S2.E1.m1.1.1.3.3.4"><divide id="S2.E1.m1.1.1.3.3.4.1.cmml" xref="S2.E1.m1.1.1.3.3.4"></divide><apply id="S2.E1.m1.1.1.3.3.4.2.cmml" xref="S2.E1.m1.1.1.3.3.4.2"><partialdiff id="S2.E1.m1.1.1.3.3.4.2.1.cmml" xref="S2.E1.m1.1.1.3.3.4.2.1"></partialdiff><ci id="S2.E1.m1.1.1.3.3.4.2.2.cmml" xref="S2.E1.m1.1.1.3.3.4.2.2">𝐿</ci></apply><apply id="S2.E1.m1.1.1.3.3.4.3.cmml" xref="S2.E1.m1.1.1.3.3.4.3"><partialdiff id="S2.E1.m1.1.1.3.3.4.3.1.cmml" xref="S2.E1.m1.1.1.3.3.4.3.1"></partialdiff><ci id="S2.E1.m1.1.1.3.3.4.3.2.cmml" xref="S2.E1.m1.1.1.3.3.4.3.2">𝑊</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle W=W-lr\frac{\partial L}{\partial W}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\displaystyle\frac{\partial L}{\partial W}\approx\frac{1}{m}\sum_{i\epsilon B}\frac{\partial l^{(i)}}{\partial W}" display="inline"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.2" xref="S2.E2.m1.1.2.cmml"><mstyle displaystyle="true" id="S2.E2.m1.1.2.2" xref="S2.E2.m1.1.2.2.cmml"><mfrac id="S2.E2.m1.1.2.2a" xref="S2.E2.m1.1.2.2.cmml"><mrow id="S2.E2.m1.1.2.2.2" xref="S2.E2.m1.1.2.2.2.cmml"><mo rspace="0em" id="S2.E2.m1.1.2.2.2.1" xref="S2.E2.m1.1.2.2.2.1.cmml">∂</mo><mi id="S2.E2.m1.1.2.2.2.2" xref="S2.E2.m1.1.2.2.2.2.cmml">L</mi></mrow><mrow id="S2.E2.m1.1.2.2.3" xref="S2.E2.m1.1.2.2.3.cmml"><mo rspace="0em" id="S2.E2.m1.1.2.2.3.1" xref="S2.E2.m1.1.2.2.3.1.cmml">∂</mo><mi id="S2.E2.m1.1.2.2.3.2" xref="S2.E2.m1.1.2.2.3.2.cmml">W</mi></mrow></mfrac></mstyle><mo id="S2.E2.m1.1.2.1" xref="S2.E2.m1.1.2.1.cmml">≈</mo><mrow id="S2.E2.m1.1.2.3" xref="S2.E2.m1.1.2.3.cmml"><mstyle displaystyle="true" id="S2.E2.m1.1.2.3.2" xref="S2.E2.m1.1.2.3.2.cmml"><mfrac id="S2.E2.m1.1.2.3.2a" xref="S2.E2.m1.1.2.3.2.cmml"><mn id="S2.E2.m1.1.2.3.2.2" xref="S2.E2.m1.1.2.3.2.2.cmml">1</mn><mi id="S2.E2.m1.1.2.3.2.3" xref="S2.E2.m1.1.2.3.2.3.cmml">m</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.2.3.1" xref="S2.E2.m1.1.2.3.1.cmml">​</mo><mrow id="S2.E2.m1.1.2.3.3" xref="S2.E2.m1.1.2.3.3.cmml"><mstyle displaystyle="true" id="S2.E2.m1.1.2.3.3.1" xref="S2.E2.m1.1.2.3.3.1.cmml"><munder id="S2.E2.m1.1.2.3.3.1a" xref="S2.E2.m1.1.2.3.3.1.cmml"><mo movablelimits="false" id="S2.E2.m1.1.2.3.3.1.2" xref="S2.E2.m1.1.2.3.3.1.2.cmml">∑</mo><mrow id="S2.E2.m1.1.2.3.3.1.3" xref="S2.E2.m1.1.2.3.3.1.3.cmml"><mi id="S2.E2.m1.1.2.3.3.1.3.2" xref="S2.E2.m1.1.2.3.3.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.2.3.3.1.3.1" xref="S2.E2.m1.1.2.3.3.1.3.1.cmml">​</mo><mi id="S2.E2.m1.1.2.3.3.1.3.3" xref="S2.E2.m1.1.2.3.3.1.3.3.cmml">ϵ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.2.3.3.1.3.1a" xref="S2.E2.m1.1.2.3.3.1.3.1.cmml">​</mo><mi id="S2.E2.m1.1.2.3.3.1.3.4" xref="S2.E2.m1.1.2.3.3.1.3.4.cmml">B</mi></mrow></munder></mstyle><mstyle displaystyle="true" id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mfrac id="S2.E2.m1.1.1a" xref="S2.E2.m1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mo rspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">∂</mo><msup id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.3.2.cmml">l</mi><mrow id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.3.cmml">(</mo><mi id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.3.cmml">)</mo></mrow></msup></mrow><mrow id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><mo rspace="0em" id="S2.E2.m1.1.1.3.1" xref="S2.E2.m1.1.1.3.1.cmml">∂</mo><mi id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml">W</mi></mrow></mfrac></mstyle></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.2.cmml" xref="S2.E2.m1.1.2"><approx id="S2.E2.m1.1.2.1.cmml" xref="S2.E2.m1.1.2.1"></approx><apply id="S2.E2.m1.1.2.2.cmml" xref="S2.E2.m1.1.2.2"><divide id="S2.E2.m1.1.2.2.1.cmml" xref="S2.E2.m1.1.2.2"></divide><apply id="S2.E2.m1.1.2.2.2.cmml" xref="S2.E2.m1.1.2.2.2"><partialdiff id="S2.E2.m1.1.2.2.2.1.cmml" xref="S2.E2.m1.1.2.2.2.1"></partialdiff><ci id="S2.E2.m1.1.2.2.2.2.cmml" xref="S2.E2.m1.1.2.2.2.2">𝐿</ci></apply><apply id="S2.E2.m1.1.2.2.3.cmml" xref="S2.E2.m1.1.2.2.3"><partialdiff id="S2.E2.m1.1.2.2.3.1.cmml" xref="S2.E2.m1.1.2.2.3.1"></partialdiff><ci id="S2.E2.m1.1.2.2.3.2.cmml" xref="S2.E2.m1.1.2.2.3.2">𝑊</ci></apply></apply><apply id="S2.E2.m1.1.2.3.cmml" xref="S2.E2.m1.1.2.3"><times id="S2.E2.m1.1.2.3.1.cmml" xref="S2.E2.m1.1.2.3.1"></times><apply id="S2.E2.m1.1.2.3.2.cmml" xref="S2.E2.m1.1.2.3.2"><divide id="S2.E2.m1.1.2.3.2.1.cmml" xref="S2.E2.m1.1.2.3.2"></divide><cn type="integer" id="S2.E2.m1.1.2.3.2.2.cmml" xref="S2.E2.m1.1.2.3.2.2">1</cn><ci id="S2.E2.m1.1.2.3.2.3.cmml" xref="S2.E2.m1.1.2.3.2.3">𝑚</ci></apply><apply id="S2.E2.m1.1.2.3.3.cmml" xref="S2.E2.m1.1.2.3.3"><apply id="S2.E2.m1.1.2.3.3.1.cmml" xref="S2.E2.m1.1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.2.3.3.1.1.cmml" xref="S2.E2.m1.1.2.3.3.1">subscript</csymbol><sum id="S2.E2.m1.1.2.3.3.1.2.cmml" xref="S2.E2.m1.1.2.3.3.1.2"></sum><apply id="S2.E2.m1.1.2.3.3.1.3.cmml" xref="S2.E2.m1.1.2.3.3.1.3"><times id="S2.E2.m1.1.2.3.3.1.3.1.cmml" xref="S2.E2.m1.1.2.3.3.1.3.1"></times><ci id="S2.E2.m1.1.2.3.3.1.3.2.cmml" xref="S2.E2.m1.1.2.3.3.1.3.2">𝑖</ci><ci id="S2.E2.m1.1.2.3.3.1.3.3.cmml" xref="S2.E2.m1.1.2.3.3.1.3.3">italic-ϵ</ci><ci id="S2.E2.m1.1.2.3.3.1.3.4.cmml" xref="S2.E2.m1.1.2.3.3.1.3.4">𝐵</ci></apply></apply><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><divide id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1"></divide><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><partialdiff id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></partialdiff><apply id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.3">superscript</csymbol><ci id="S2.E2.m1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.3.2">𝑙</ci><ci id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1">𝑖</ci></apply></apply><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><partialdiff id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3.1"></partialdiff><ci id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2">𝑊</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle\frac{\partial L}{\partial W}\approx\frac{1}{m}\sum_{i\epsilon B}\frac{\partial l^{(i)}}{\partial W}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.3" class="ltx_p">Note that the SGD formula presented in (<a href="#S2.E1" title="In II-A Deep Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is that of a mini-batch GD. In particular, equation (<a href="#S2.E2" title="In II-A Deep Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is derived as the average gradient matrix over the gradient matrices of <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">B</annotation></semantics></math> batches, in which each batch is a random subset consisting of <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">m</annotation></semantics></math> training samples. This is preferred over the full batch GD, i.e., where the entirety of the training set is included in computing the partial derivative, since the full batch GD can lead to slow training and batch memorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. The gradient matrices are derived through backpropagation from the input gradient <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">e</annotation></semantics></math> (Fig. <a href="#S2.F3" title="Figure 3 ‣ II-A Deep Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">The training iterations are then repeated over many epochs, i.e., full passes over the training set, for loss minimalization. A well-trained DNN generalizes well, i.e., achieve high <span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_italic">inference</span> accuracy when applied to data that it has not seen before, e.g., the test set. There are other alternatives to supervised learning, e.g., semi-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, unsupervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. In addition, there also exists several DNN networks and architectures tailored to process the varying natures of input data, e.g., Multilayer Perceptron (MLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, Convolutional Neural Network (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> typically for CV tasks, and Recurrent Neural Network (RNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> usually for sequential tasks. However, an in-depth discussion is out of the scope of this paper. We refer interested readers to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> for comprehensive discussions of DNN architectures and training strategies. We next focus on FL, an important paradigm shift towards enabling privacy preserving and collaborative DL model training.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Motivated by privacy concerns among data owners, the concept of FL is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. FL allows users to collaboratively train a
shared model while keeping personal data on their devices, thus alleviating
their privacy concerns. As such, FL can serve as an enabling technology for ML model training at mobile edge networks. For an introduction to the categorizations of different FL settings, e.g., vertical and horizontal FL, we refer the interested readers to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.10" class="ltx_p">In general, there are two main entities in the FL
system, i.e., the data owners (viz. <span id="S2.SS2.p2.10.1" class="ltx_text ltx_font_italic">participants</span>) and the model owner (viz. <span id="S2.SS2.p2.10.2" class="ltx_text ltx_font_italic">FL server</span>). Let <math id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="\mathcal{N}=\left\{1,\ldots,N\right\}" display="inline"><semantics id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">𝒩</mi><mo id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">=</mo><mrow id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">{</mo><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">1</mn><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">…</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">N</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml" xref="S2.SS2.p2.1.m1.3.4"><eq id="S2.SS2.p2.1.m1.3.4.1.cmml" xref="S2.SS2.p2.1.m1.3.4.1"></eq><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">𝒩</ci><set id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><cn type="integer" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">1</cn><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">…</ci><ci id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">𝑁</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">\mathcal{N}=\left\{1,\ldots,N\right\}</annotation></semantics></math>
denote the set of <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">N</annotation></semantics></math> data owners, each of which has
a private dataset <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="D_{i\in\mathcal{N}}" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><msub id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mi id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml">D</mi><mrow id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml"><mi id="S2.SS2.p2.3.m3.1.1.3.2" xref="S2.SS2.p2.3.m3.1.1.3.2.cmml">i</mi><mo id="S2.SS2.p2.3.m3.1.1.3.1" xref="S2.SS2.p2.3.m3.1.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.3.m3.1.1.3.3" xref="S2.SS2.p2.3.m3.1.1.3.3.cmml">𝒩</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2">𝐷</ci><apply id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3"><in id="S2.SS2.p2.3.m3.1.1.3.1.cmml" xref="S2.SS2.p2.3.m3.1.1.3.1"></in><ci id="S2.SS2.p2.3.m3.1.1.3.2.cmml" xref="S2.SS2.p2.3.m3.1.1.3.2">𝑖</ci><ci id="S2.SS2.p2.3.m3.1.1.3.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3.3">𝒩</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">D_{i\in\mathcal{N}}</annotation></semantics></math>. Each data owner <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">i</annotation></semantics></math> uses
its dataset <math id="S2.SS2.p2.5.m5.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS2.p2.5.m5.1a"><msub id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml"><mi id="S2.SS2.p2.5.m5.1.1.2" xref="S2.SS2.p2.5.m5.1.1.2.cmml">D</mi><mi id="S2.SS2.p2.5.m5.1.1.3" xref="S2.SS2.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><apply id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.5.m5.1.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p2.5.m5.1.1.2.cmml" xref="S2.SS2.p2.5.m5.1.1.2">𝐷</ci><ci id="S2.SS2.p2.5.m5.1.1.3.cmml" xref="S2.SS2.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">D_{i}</annotation></semantics></math> to train a<em id="S2.SS2.p2.10.3" class="ltx_emph ltx_font_italic"> local model</em> <math id="S2.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{w}_{i}" display="inline"><semantics id="S2.SS2.p2.6.m6.1a"><msub id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml"><mi id="S2.SS2.p2.6.m6.1.1.2" xref="S2.SS2.p2.6.m6.1.1.2.cmml">𝐰</mi><mi id="S2.SS2.p2.6.m6.1.1.3" xref="S2.SS2.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><apply id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.6.m6.1.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p2.6.m6.1.1.2.cmml" xref="S2.SS2.p2.6.m6.1.1.2">𝐰</ci><ci id="S2.SS2.p2.6.m6.1.1.3.cmml" xref="S2.SS2.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">\mathbf{w}_{i}</annotation></semantics></math>
and send only the local model parameters to the FL server. Then,
all collected local models are aggregated <math id="S2.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathbf{\mathbf{w}}=\cup_{i\in\mathcal{N}}\mathbf{w}_{i}" display="inline"><semantics id="S2.SS2.p2.7.m7.1a"><mrow id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml"><mi id="S2.SS2.p2.7.m7.1.1.2" xref="S2.SS2.p2.7.m7.1.1.2.cmml">𝐰</mi><mo rspace="0em" id="S2.SS2.p2.7.m7.1.1.1" xref="S2.SS2.p2.7.m7.1.1.1.cmml">=</mo><mrow id="S2.SS2.p2.7.m7.1.1.3" xref="S2.SS2.p2.7.m7.1.1.3.cmml"><msub id="S2.SS2.p2.7.m7.1.1.3.1" xref="S2.SS2.p2.7.m7.1.1.3.1.cmml"><mo lspace="0em" id="S2.SS2.p2.7.m7.1.1.3.1.2" xref="S2.SS2.p2.7.m7.1.1.3.1.2.cmml">∪</mo><mrow id="S2.SS2.p2.7.m7.1.1.3.1.3" xref="S2.SS2.p2.7.m7.1.1.3.1.3.cmml"><mi id="S2.SS2.p2.7.m7.1.1.3.1.3.2" xref="S2.SS2.p2.7.m7.1.1.3.1.3.2.cmml">i</mi><mo id="S2.SS2.p2.7.m7.1.1.3.1.3.1" xref="S2.SS2.p2.7.m7.1.1.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.7.m7.1.1.3.1.3.3" xref="S2.SS2.p2.7.m7.1.1.3.1.3.3.cmml">𝒩</mi></mrow></msub><msub id="S2.SS2.p2.7.m7.1.1.3.2" xref="S2.SS2.p2.7.m7.1.1.3.2.cmml"><mi id="S2.SS2.p2.7.m7.1.1.3.2.2" xref="S2.SS2.p2.7.m7.1.1.3.2.2.cmml">𝐰</mi><mi id="S2.SS2.p2.7.m7.1.1.3.2.3" xref="S2.SS2.p2.7.m7.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><apply id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1"><eq id="S2.SS2.p2.7.m7.1.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1.1"></eq><ci id="S2.SS2.p2.7.m7.1.1.2.cmml" xref="S2.SS2.p2.7.m7.1.1.2">𝐰</ci><apply id="S2.SS2.p2.7.m7.1.1.3.cmml" xref="S2.SS2.p2.7.m7.1.1.3"><apply id="S2.SS2.p2.7.m7.1.1.3.1.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S2.SS2.p2.7.m7.1.1.3.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1">subscript</csymbol><union id="S2.SS2.p2.7.m7.1.1.3.1.2.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1.2"></union><apply id="S2.SS2.p2.7.m7.1.1.3.1.3.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1.3"><in id="S2.SS2.p2.7.m7.1.1.3.1.3.1.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1.3.1"></in><ci id="S2.SS2.p2.7.m7.1.1.3.1.3.2.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1.3.2">𝑖</ci><ci id="S2.SS2.p2.7.m7.1.1.3.1.3.3.cmml" xref="S2.SS2.p2.7.m7.1.1.3.1.3.3">𝒩</ci></apply></apply><apply id="S2.SS2.p2.7.m7.1.1.3.2.cmml" xref="S2.SS2.p2.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS2.p2.7.m7.1.1.3.2.1.cmml" xref="S2.SS2.p2.7.m7.1.1.3.2">subscript</csymbol><ci id="S2.SS2.p2.7.m7.1.1.3.2.2.cmml" xref="S2.SS2.p2.7.m7.1.1.3.2.2">𝐰</ci><ci id="S2.SS2.p2.7.m7.1.1.3.2.3.cmml" xref="S2.SS2.p2.7.m7.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">\mathbf{\mathbf{w}}=\cup_{i\in\mathcal{N}}\mathbf{w}_{i}</annotation></semantics></math>
to generate a <em id="S2.SS2.p2.10.4" class="ltx_emph ltx_font_italic">global model</em> <math id="S2.SS2.p2.8.m8.1" class="ltx_Math" alttext="\mathbf{w}_{G}" display="inline"><semantics id="S2.SS2.p2.8.m8.1a"><msub id="S2.SS2.p2.8.m8.1.1" xref="S2.SS2.p2.8.m8.1.1.cmml"><mi id="S2.SS2.p2.8.m8.1.1.2" xref="S2.SS2.p2.8.m8.1.1.2.cmml">𝐰</mi><mi id="S2.SS2.p2.8.m8.1.1.3" xref="S2.SS2.p2.8.m8.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m8.1b"><apply id="S2.SS2.p2.8.m8.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.8.m8.1.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.p2.8.m8.1.1.2.cmml" xref="S2.SS2.p2.8.m8.1.1.2">𝐰</ci><ci id="S2.SS2.p2.8.m8.1.1.3.cmml" xref="S2.SS2.p2.8.m8.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m8.1c">\mathbf{w}_{G}</annotation></semantics></math>. This
is different from the traditional centralized training which uses
<math id="S2.SS2.p2.9.m9.1" class="ltx_Math" alttext="\mathbf{D}=\cup_{i\in\mathcal{N}}D_{i}" display="inline"><semantics id="S2.SS2.p2.9.m9.1a"><mrow id="S2.SS2.p2.9.m9.1.1" xref="S2.SS2.p2.9.m9.1.1.cmml"><mi id="S2.SS2.p2.9.m9.1.1.2" xref="S2.SS2.p2.9.m9.1.1.2.cmml">𝐃</mi><mo rspace="0em" id="S2.SS2.p2.9.m9.1.1.1" xref="S2.SS2.p2.9.m9.1.1.1.cmml">=</mo><mrow id="S2.SS2.p2.9.m9.1.1.3" xref="S2.SS2.p2.9.m9.1.1.3.cmml"><msub id="S2.SS2.p2.9.m9.1.1.3.1" xref="S2.SS2.p2.9.m9.1.1.3.1.cmml"><mo lspace="0em" id="S2.SS2.p2.9.m9.1.1.3.1.2" xref="S2.SS2.p2.9.m9.1.1.3.1.2.cmml">∪</mo><mrow id="S2.SS2.p2.9.m9.1.1.3.1.3" xref="S2.SS2.p2.9.m9.1.1.3.1.3.cmml"><mi id="S2.SS2.p2.9.m9.1.1.3.1.3.2" xref="S2.SS2.p2.9.m9.1.1.3.1.3.2.cmml">i</mi><mo id="S2.SS2.p2.9.m9.1.1.3.1.3.1" xref="S2.SS2.p2.9.m9.1.1.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.9.m9.1.1.3.1.3.3" xref="S2.SS2.p2.9.m9.1.1.3.1.3.3.cmml">𝒩</mi></mrow></msub><msub id="S2.SS2.p2.9.m9.1.1.3.2" xref="S2.SS2.p2.9.m9.1.1.3.2.cmml"><mi id="S2.SS2.p2.9.m9.1.1.3.2.2" xref="S2.SS2.p2.9.m9.1.1.3.2.2.cmml">D</mi><mi id="S2.SS2.p2.9.m9.1.1.3.2.3" xref="S2.SS2.p2.9.m9.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m9.1b"><apply id="S2.SS2.p2.9.m9.1.1.cmml" xref="S2.SS2.p2.9.m9.1.1"><eq id="S2.SS2.p2.9.m9.1.1.1.cmml" xref="S2.SS2.p2.9.m9.1.1.1"></eq><ci id="S2.SS2.p2.9.m9.1.1.2.cmml" xref="S2.SS2.p2.9.m9.1.1.2">𝐃</ci><apply id="S2.SS2.p2.9.m9.1.1.3.cmml" xref="S2.SS2.p2.9.m9.1.1.3"><apply id="S2.SS2.p2.9.m9.1.1.3.1.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1"><csymbol cd="ambiguous" id="S2.SS2.p2.9.m9.1.1.3.1.1.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1">subscript</csymbol><union id="S2.SS2.p2.9.m9.1.1.3.1.2.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1.2"></union><apply id="S2.SS2.p2.9.m9.1.1.3.1.3.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1.3"><in id="S2.SS2.p2.9.m9.1.1.3.1.3.1.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1.3.1"></in><ci id="S2.SS2.p2.9.m9.1.1.3.1.3.2.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1.3.2">𝑖</ci><ci id="S2.SS2.p2.9.m9.1.1.3.1.3.3.cmml" xref="S2.SS2.p2.9.m9.1.1.3.1.3.3">𝒩</ci></apply></apply><apply id="S2.SS2.p2.9.m9.1.1.3.2.cmml" xref="S2.SS2.p2.9.m9.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS2.p2.9.m9.1.1.3.2.1.cmml" xref="S2.SS2.p2.9.m9.1.1.3.2">subscript</csymbol><ci id="S2.SS2.p2.9.m9.1.1.3.2.2.cmml" xref="S2.SS2.p2.9.m9.1.1.3.2.2">𝐷</ci><ci id="S2.SS2.p2.9.m9.1.1.3.2.3.cmml" xref="S2.SS2.p2.9.m9.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.9.m9.1c">\mathbf{D}=\cup_{i\in\mathcal{N}}D_{i}</annotation></semantics></math> to train a model <math id="S2.SS2.p2.10.m10.1" class="ltx_Math" alttext="\mathbf{w}_{T}" display="inline"><semantics id="S2.SS2.p2.10.m10.1a"><msub id="S2.SS2.p2.10.m10.1.1" xref="S2.SS2.p2.10.m10.1.1.cmml"><mi id="S2.SS2.p2.10.m10.1.1.2" xref="S2.SS2.p2.10.m10.1.1.2.cmml">𝐰</mi><mi id="S2.SS2.p2.10.m10.1.1.3" xref="S2.SS2.p2.10.m10.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m10.1b"><apply id="S2.SS2.p2.10.m10.1.1.cmml" xref="S2.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.10.m10.1.1.1.cmml" xref="S2.SS2.p2.10.m10.1.1">subscript</csymbol><ci id="S2.SS2.p2.10.m10.1.1.2.cmml" xref="S2.SS2.p2.10.m10.1.1.2">𝐰</ci><ci id="S2.SS2.p2.10.m10.1.1.3.cmml" xref="S2.SS2.p2.10.m10.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.10.m10.1c">\mathbf{w}_{T}</annotation></semantics></math>, i.e., data from each individual source is aggregated first before model training takes place centrally.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/flsteps.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="278" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text" style="font-size:90%;">General FL training process involving <span id="S2.F4.4.2.1" class="ltx_text ltx_font_italic">N</span> participants.</span></figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">A typical architecture and training process of an FL system is shown in
Fig. <a href="#S2.F4" title="Figure 4 ‣ II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In this system, the data
owners serve as the FL participants which collaboratively
train an ML model required by an aggregate server. An underlying assumption is that the data owners are honest, which
means they use their real private data to do the training and submit the
true local models to the FL server. Of course, this assumption may not always be realistic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and we discuss the proposed solutions subsequently in Sections <a href="#S4" title="IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and <a href="#S5" title="V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">In general,
the FL training process includes the following three steps.
Note: the <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_italic">local</span> model refers to
the model trained at each participating device, whereas the <span id="S2.SS2.p4.1.2" class="ltx_text ltx_font_italic">global</span> model refers to
the model aggregated by the FL server.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Step 1 (Task initialization)</span>: The server decides the training task, i.e., the target application, and the corresponding data requirements.
The server also specifies the hyperparameters of the global model
and the training process, e.g., learning rate. Then, the server broadcasts the initialized
global model <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{0}" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><msubsup id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.2.2" xref="S2.I1.i1.p1.1.m1.1.1.2.2.cmml">𝐰</mi><mi id="S2.I1.i1.p1.1.m1.1.1.2.3" xref="S2.I1.i1.p1.1.m1.1.1.2.3.cmml">G</mi><mn id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">superscript</csymbol><apply id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.2">𝐰</ci><ci id="S2.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.3">𝐺</ci></apply><cn type="integer" id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">\mathbf{w}_{G}^{0}</annotation></semantics></math> and task to selected participants.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.7" class="ltx_p"><span id="S2.I1.i2.p1.7.2" class="ltx_text ltx_font_italic">Step 2 (Local model training and update)</span>: Based on the global model
<math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{t}" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><msubsup id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2.2" xref="S2.I1.i2.p1.1.m1.1.1.2.2.cmml">𝐰</mi><mi id="S2.I1.i2.p1.1.m1.1.1.2.3" xref="S2.I1.i2.p1.1.m1.1.1.2.3.cmml">G</mi><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">superscript</csymbol><apply id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.2">𝐰</ci><ci id="S2.I1.i2.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.3">𝐺</ci></apply><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">\mathbf{w}_{G}^{t}</annotation></semantics></math>, where <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mi id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><ci id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">t</annotation></semantics></math> denotes the current iteration index,
each participant respectively uses its local data and device to update
the local model parameters <math id="S2.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{w}_{i}^{t}" display="inline"><semantics id="S2.I1.i2.p1.3.m3.1a"><msubsup id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.2.2" xref="S2.I1.i2.p1.3.m3.1.1.2.2.cmml">𝐰</mi><mi id="S2.I1.i2.p1.3.m3.1.1.2.3" xref="S2.I1.i2.p1.3.m3.1.1.2.3.cmml">i</mi><mi id="S2.I1.i2.p1.3.m3.1.1.3" xref="S2.I1.i2.p1.3.m3.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><apply id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.3.m3.1.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">superscript</csymbol><apply id="S2.I1.i2.p1.3.m3.1.1.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.3.m3.1.1.2.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.3.m3.1.1.2.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2">𝐰</ci><ci id="S2.I1.i2.p1.3.m3.1.1.2.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.3">𝑖</ci></apply><ci id="S2.I1.i2.p1.3.m3.1.1.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">\mathbf{w}_{i}^{t}</annotation></semantics></math>. The goal of participant <math id="S2.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.I1.i2.p1.4.m4.1a"><mi id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.1b"><ci id="S2.I1.i2.p1.4.m4.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.1c">i</annotation></semantics></math> in iteration <em id="S2.I1.i2.p1.5.1" class="ltx_emph ltx_font_italic"><math id="S2.I1.i2.p1.5.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.I1.i2.p1.5.1.m1.1a"><mi id="S2.I1.i2.p1.5.1.m1.1.1" xref="S2.I1.i2.p1.5.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.5.1.m1.1b"><ci id="S2.I1.i2.p1.5.1.m1.1.1.cmml" xref="S2.I1.i2.p1.5.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.5.1.m1.1c">t</annotation></semantics></math></em>
is to find optimal parameters <math id="S2.I1.i2.p1.6.m5.1" class="ltx_Math" alttext="\mathbf{w}_{i}^{t}" display="inline"><semantics id="S2.I1.i2.p1.6.m5.1a"><msubsup id="S2.I1.i2.p1.6.m5.1.1" xref="S2.I1.i2.p1.6.m5.1.1.cmml"><mi id="S2.I1.i2.p1.6.m5.1.1.2.2" xref="S2.I1.i2.p1.6.m5.1.1.2.2.cmml">𝐰</mi><mi id="S2.I1.i2.p1.6.m5.1.1.2.3" xref="S2.I1.i2.p1.6.m5.1.1.2.3.cmml">i</mi><mi id="S2.I1.i2.p1.6.m5.1.1.3" xref="S2.I1.i2.p1.6.m5.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.6.m5.1b"><apply id="S2.I1.i2.p1.6.m5.1.1.cmml" xref="S2.I1.i2.p1.6.m5.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.6.m5.1.1.1.cmml" xref="S2.I1.i2.p1.6.m5.1.1">superscript</csymbol><apply id="S2.I1.i2.p1.6.m5.1.1.2.cmml" xref="S2.I1.i2.p1.6.m5.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.6.m5.1.1.2.1.cmml" xref="S2.I1.i2.p1.6.m5.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.6.m5.1.1.2.2.cmml" xref="S2.I1.i2.p1.6.m5.1.1.2.2">𝐰</ci><ci id="S2.I1.i2.p1.6.m5.1.1.2.3.cmml" xref="S2.I1.i2.p1.6.m5.1.1.2.3">𝑖</ci></apply><ci id="S2.I1.i2.p1.6.m5.1.1.3.cmml" xref="S2.I1.i2.p1.6.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.6.m5.1c">\mathbf{w}_{i}^{t}</annotation></semantics></math> that minimize
the loss function <math id="S2.I1.i2.p1.7.m6.1" class="ltx_Math" alttext="L(\mathbf{w}_{i}^{t})" display="inline"><semantics id="S2.I1.i2.p1.7.m6.1a"><mrow id="S2.I1.i2.p1.7.m6.1.1" xref="S2.I1.i2.p1.7.m6.1.1.cmml"><mi id="S2.I1.i2.p1.7.m6.1.1.3" xref="S2.I1.i2.p1.7.m6.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.I1.i2.p1.7.m6.1.1.2" xref="S2.I1.i2.p1.7.m6.1.1.2.cmml">​</mo><mrow id="S2.I1.i2.p1.7.m6.1.1.1.1" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.I1.i2.p1.7.m6.1.1.1.1.2" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.cmml">(</mo><msubsup id="S2.I1.i2.p1.7.m6.1.1.1.1.1" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.cmml"><mi id="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.2" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.2.cmml">𝐰</mi><mi id="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.3" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.3.cmml">i</mi><mi id="S2.I1.i2.p1.7.m6.1.1.1.1.1.3" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.I1.i2.p1.7.m6.1.1.1.1.3" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.7.m6.1b"><apply id="S2.I1.i2.p1.7.m6.1.1.cmml" xref="S2.I1.i2.p1.7.m6.1.1"><times id="S2.I1.i2.p1.7.m6.1.1.2.cmml" xref="S2.I1.i2.p1.7.m6.1.1.2"></times><ci id="S2.I1.i2.p1.7.m6.1.1.3.cmml" xref="S2.I1.i2.p1.7.m6.1.1.3">𝐿</ci><apply id="S2.I1.i2.p1.7.m6.1.1.1.1.1.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.7.m6.1.1.1.1.1.1.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1">superscript</csymbol><apply id="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.1.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.2.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.2">𝐰</ci><ci id="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.3.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.I1.i2.p1.7.m6.1.1.1.1.1.3.cmml" xref="S2.I1.i2.p1.7.m6.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.7.m6.1c">L(\mathbf{w}_{i}^{t})</annotation></semantics></math>, i.e.,</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\mathbf{w}_{i}^{t^{*}}=\arg\min_{\mathbf{w}_{i}^{t}}L(\mathbf{w}_{i}^{t})." display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><msubsup id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.3.2.2" xref="S2.E3.m1.1.1.1.1.3.2.2.cmml">𝐰</mi><mi id="S2.E3.m1.1.1.1.1.3.2.3" xref="S2.E3.m1.1.1.1.1.3.2.3.cmml">i</mi><msup id="S2.E3.m1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.3.3.2" xref="S2.E3.m1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S2.E3.m1.1.1.1.1.3.3.3" xref="S2.E3.m1.1.1.1.1.3.3.3.cmml">∗</mo></msup></msubsup><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S2.E3.m1.1.1.1.1.1.3a" xref="S2.E3.m1.1.1.1.1.1.3.cmml">⁡</mo><mrow id="S2.E3.m1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.3.2.cmml"><munder id="S2.E3.m1.1.1.1.1.1.3.2.1" xref="S2.E3.m1.1.1.1.1.1.3.2.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.2.1.2" xref="S2.E3.m1.1.1.1.1.1.3.2.1.2.cmml">min</mi><msubsup id="S2.E3.m1.1.1.1.1.1.3.2.1.3" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.2" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.2.cmml">𝐰</mi><mi id="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.3" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.3.cmml">i</mi><mi id="S2.E3.m1.1.1.1.1.1.3.2.1.3.3" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.3.cmml">t</mi></msubsup></munder><mo lspace="0.167em" id="S2.E3.m1.1.1.1.1.1.3.2a" xref="S2.E3.m1.1.1.1.1.1.3.2.cmml">⁡</mo><mi id="S2.E3.m1.1.1.1.1.1.3.2.2" xref="S2.E3.m1.1.1.1.1.1.3.2.2.cmml">L</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐰</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><apply id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2.2">𝐰</ci><ci id="S2.E3.m1.1.1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S2.E3.m1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S2.E3.m1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3.3.2">𝑡</ci><times id="S2.E3.m1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3.3"></times></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><arg id="S2.E3.m1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.1"></arg><apply id="S2.E3.m1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2"><apply id="S2.E3.m1.1.1.1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.3.2.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1">subscript</csymbol><min id="S2.E3.m1.1.1.1.1.1.3.2.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.2"></min><apply id="S2.E3.m1.1.1.1.1.1.3.2.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.3.2.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.2">𝐰</ci><ci id="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.2.3">𝑖</ci></apply><ci id="S2.E3.m1.1.1.1.1.1.3.2.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.1.3.3">𝑡</ci></apply></apply><ci id="S2.E3.m1.1.1.1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2.2">𝐿</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2">𝐰</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\mathbf{w}_{i}^{t^{*}}=\arg\min_{\mathbf{w}_{i}^{t}}L(\mathbf{w}_{i}^{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.I1.i2.p1.8" class="ltx_p">The updated local model parameters are subsequently sent
to the server.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Step 3 (Global model aggregation and update)</span>: The server aggregates
the local models from participants and then sends the updated
global model parameters <math id="S2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{t+1}" display="inline"><semantics id="S2.I1.i3.p1.1.m1.1a"><msubsup id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">𝐰</mi><mi id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">G</mi><mrow id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.2.cmml">t</mi><mo id="S2.I1.i3.p1.1.m1.1.1.3.1" xref="S2.I1.i3.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S2.I1.i3.p1.1.m1.1.1.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">superscript</csymbol><apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.2">𝐰</ci><ci id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.3">𝐺</ci></apply><apply id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3"><plus id="S2.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.1"></plus><ci id="S2.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.2">𝑡</ci><cn type="integer" id="S2.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">\mathbf{w}_{G}^{t+1}</annotation></semantics></math> back to the data owners.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">The server wants to minimize the global loss function <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="L(\mathbf{w}_{G}^{t})" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mrow id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><mi id="S2.SS2.p5.1.m1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml">​</mo><mrow id="S2.SS2.p5.1.m1.1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p5.1.m1.1.1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.SS2.p5.1.m1.1.1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS2.p5.1.m1.1.1.1.1.1.2.2" xref="S2.SS2.p5.1.m1.1.1.1.1.1.2.2.cmml">𝐰</mi><mi id="S2.SS2.p5.1.m1.1.1.1.1.1.2.3" xref="S2.SS2.p5.1.m1.1.1.1.1.1.2.3.cmml">G</mi><mi id="S2.SS2.p5.1.m1.1.1.1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.SS2.p5.1.m1.1.1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><times id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2"></times><ci id="S2.SS2.p5.1.m1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.3">𝐿</ci><apply id="S2.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1">superscript</csymbol><apply id="S2.SS2.p5.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.1.1.2.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.1.1.1.2.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.1.2.2">𝐰</ci><ci id="S2.SS2.p5.1.m1.1.1.1.1.1.2.3.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.1.2.3">𝐺</ci></apply><ci id="S2.SS2.p5.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">L(\mathbf{w}_{G}^{t})</annotation></semantics></math>, i.e.,</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.1" class="ltx_Math" alttext="L(\mathbf{w}_{G}^{t})=\frac{1}{N}\sum_{i=1}^{N}L(\mathbf{w}_{i}^{t})." display="block"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.E4.m1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐰</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml">G</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml"><mfrac id="S2.E4.m1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.2.3.cmml"><mn id="S2.E4.m1.1.1.1.1.2.3.2" xref="S2.E4.m1.1.1.1.1.2.3.2.cmml">1</mn><mi id="S2.E4.m1.1.1.1.1.2.3.3" xref="S2.E4.m1.1.1.1.1.2.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.2.2.cmml">​</mo><mrow id="S2.E4.m1.1.1.1.1.2.1" xref="S2.E4.m1.1.1.1.1.2.1.cmml"><munderover id="S2.E4.m1.1.1.1.1.2.1.2" xref="S2.E4.m1.1.1.1.1.2.1.2.cmml"><mo movablelimits="false" id="S2.E4.m1.1.1.1.1.2.1.2.2.2" xref="S2.E4.m1.1.1.1.1.2.1.2.2.2.cmml">∑</mo><mrow id="S2.E4.m1.1.1.1.1.2.1.2.2.3" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.cmml"><mi id="S2.E4.m1.1.1.1.1.2.1.2.2.3.2" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.2.cmml">i</mi><mo id="S2.E4.m1.1.1.1.1.2.1.2.2.3.1" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.1.cmml">=</mo><mn id="S2.E4.m1.1.1.1.1.2.1.2.2.3.3" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.1.1.1.1.2.1.2.3" xref="S2.E4.m1.1.1.1.1.2.1.2.3.cmml">N</mi></munderover><mrow id="S2.E4.m1.1.1.1.1.2.1.1" xref="S2.E4.m1.1.1.1.1.2.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.2.1.1.3" xref="S2.E4.m1.1.1.1.1.2.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.2.1.1.2" xref="S2.E4.m1.1.1.1.1.2.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.1.1.1.1.2.1.1.1.1" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.1.2.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.cmml">(</mo><msubsup id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.2.cmml">𝐰</mi><mi id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.3.cmml">i</mi><mi id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.E4.m1.1.1.1.1.2.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S2.E4.m1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3"></eq><apply id="S2.E4.m1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.2"></times><ci id="S2.E4.m1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.3">𝐿</ci><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.2">𝐰</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.3">𝐺</ci></apply><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"><times id="S2.E4.m1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.2"></times><apply id="S2.E4.m1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.3"><divide id="S2.E4.m1.1.1.1.1.2.3.1.cmml" xref="S2.E4.m1.1.1.1.1.2.3"></divide><cn type="integer" id="S2.E4.m1.1.1.1.1.2.3.2.cmml" xref="S2.E4.m1.1.1.1.1.2.3.2">1</cn><ci id="S2.E4.m1.1.1.1.1.2.3.3.cmml" xref="S2.E4.m1.1.1.1.1.2.3.3">𝑁</ci></apply><apply id="S2.E4.m1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1"><apply id="S2.E4.m1.1.1.1.1.2.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2">superscript</csymbol><apply id="S2.E4.m1.1.1.1.1.2.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.1.2.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2">subscript</csymbol><sum id="S2.E4.m1.1.1.1.1.2.1.2.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2.2.2"></sum><apply id="S2.E4.m1.1.1.1.1.2.1.2.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3"><eq id="S2.E4.m1.1.1.1.1.2.1.2.2.3.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.1"></eq><ci id="S2.E4.m1.1.1.1.1.2.1.2.2.3.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E4.m1.1.1.1.1.2.1.2.2.3.3.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.1.1.1.1.2.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.1.2.3">𝑁</ci></apply><apply id="S2.E4.m1.1.1.1.1.2.1.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1"><times id="S2.E4.m1.1.1.1.1.2.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.2"></times><ci id="S2.E4.m1.1.1.1.1.2.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.3">𝐿</ci><apply id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1">superscript</csymbol><apply id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.2">𝐰</ci><ci id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.2.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">L(\mathbf{w}_{G}^{t})=\frac{1}{N}\sum_{i=1}^{N}L(\mathbf{w}_{i}^{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p5.3" class="ltx_p">Steps <math id="S2.SS2.p5.2.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S2.SS2.p5.2.m1.1a"><mn id="S2.SS2.p5.2.m1.1.1" xref="S2.SS2.p5.2.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m1.1b"><cn type="integer" id="S2.SS2.p5.2.m1.1.1.cmml" xref="S2.SS2.p5.2.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m1.1c">2</annotation></semantics></math>-<math id="S2.SS2.p5.3.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S2.SS2.p5.3.m2.1a"><mn id="S2.SS2.p5.3.m2.1.1" xref="S2.SS2.p5.3.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m2.1b"><cn type="integer" id="S2.SS2.p5.3.m2.1.1.cmml" xref="S2.SS2.p5.3.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m2.1c">3</annotation></semantics></math> are repeated until the global loss function converges or a desirable training accuracy is achieved.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.6" class="ltx_p">Note
that the FL training process can be used for different ML
models that essentially use the SGD method such as Support
Vector Machines (SVMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, neural networks, and linear regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
A training dataset usually contains a set of <math id="S2.SS2.p6.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p6.1.m1.1a"><mi id="S2.SS2.p6.1.m1.1.1" xref="S2.SS2.p6.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.1.m1.1b"><ci id="S2.SS2.p6.1.m1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.1.m1.1c">n</annotation></semantics></math> data feature vectors
<math id="S2.SS2.p6.2.m2.3" class="ltx_Math" alttext="\mathbf{x}=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}" display="inline"><semantics id="S2.SS2.p6.2.m2.3a"><mrow id="S2.SS2.p6.2.m2.3.3" xref="S2.SS2.p6.2.m2.3.3.cmml"><mi id="S2.SS2.p6.2.m2.3.3.4" xref="S2.SS2.p6.2.m2.3.3.4.cmml">𝐱</mi><mo id="S2.SS2.p6.2.m2.3.3.3" xref="S2.SS2.p6.2.m2.3.3.3.cmml">=</mo><mrow id="S2.SS2.p6.2.m2.3.3.2.2" xref="S2.SS2.p6.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="S2.SS2.p6.2.m2.3.3.2.2.3" xref="S2.SS2.p6.2.m2.3.3.2.3.cmml">{</mo><msub id="S2.SS2.p6.2.m2.2.2.1.1.1" xref="S2.SS2.p6.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS2.p6.2.m2.2.2.1.1.1.2" xref="S2.SS2.p6.2.m2.2.2.1.1.1.2.cmml">𝐱</mi><mn id="S2.SS2.p6.2.m2.2.2.1.1.1.3" xref="S2.SS2.p6.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.p6.2.m2.3.3.2.2.4" xref="S2.SS2.p6.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p6.2.m2.1.1" xref="S2.SS2.p6.2.m2.1.1.cmml">…</mi><mo id="S2.SS2.p6.2.m2.3.3.2.2.5" xref="S2.SS2.p6.2.m2.3.3.2.3.cmml">,</mo><msub id="S2.SS2.p6.2.m2.3.3.2.2.2" xref="S2.SS2.p6.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS2.p6.2.m2.3.3.2.2.2.2" xref="S2.SS2.p6.2.m2.3.3.2.2.2.2.cmml">𝐱</mi><mi id="S2.SS2.p6.2.m2.3.3.2.2.2.3" xref="S2.SS2.p6.2.m2.3.3.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS2.p6.2.m2.3.3.2.2.6" xref="S2.SS2.p6.2.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.2.m2.3b"><apply id="S2.SS2.p6.2.m2.3.3.cmml" xref="S2.SS2.p6.2.m2.3.3"><eq id="S2.SS2.p6.2.m2.3.3.3.cmml" xref="S2.SS2.p6.2.m2.3.3.3"></eq><ci id="S2.SS2.p6.2.m2.3.3.4.cmml" xref="S2.SS2.p6.2.m2.3.3.4">𝐱</ci><set id="S2.SS2.p6.2.m2.3.3.2.3.cmml" xref="S2.SS2.p6.2.m2.3.3.2.2"><apply id="S2.SS2.p6.2.m2.2.2.1.1.1.cmml" xref="S2.SS2.p6.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p6.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS2.p6.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.p6.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS2.p6.2.m2.2.2.1.1.1.2">𝐱</ci><cn type="integer" id="S2.SS2.p6.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS2.p6.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS2.p6.2.m2.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1">…</ci><apply id="S2.SS2.p6.2.m2.3.3.2.2.2.cmml" xref="S2.SS2.p6.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p6.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS2.p6.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.p6.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS2.p6.2.m2.3.3.2.2.2.2">𝐱</ci><ci id="S2.SS2.p6.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS2.p6.2.m2.3.3.2.2.2.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.2.m2.3c">\mathbf{x}=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}</annotation></semantics></math> and a set of
corresponding data labels<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In the case of unsupervised learning, there is no data label.</span></span></span>
<math id="S2.SS2.p6.3.m3.3" class="ltx_Math" alttext="\mathbf{y}=\{y_{1},\ldots,y_{n}\}" display="inline"><semantics id="S2.SS2.p6.3.m3.3a"><mrow id="S2.SS2.p6.3.m3.3.3" xref="S2.SS2.p6.3.m3.3.3.cmml"><mi id="S2.SS2.p6.3.m3.3.3.4" xref="S2.SS2.p6.3.m3.3.3.4.cmml">𝐲</mi><mo id="S2.SS2.p6.3.m3.3.3.3" xref="S2.SS2.p6.3.m3.3.3.3.cmml">=</mo><mrow id="S2.SS2.p6.3.m3.3.3.2.2" xref="S2.SS2.p6.3.m3.3.3.2.3.cmml"><mo stretchy="false" id="S2.SS2.p6.3.m3.3.3.2.2.3" xref="S2.SS2.p6.3.m3.3.3.2.3.cmml">{</mo><msub id="S2.SS2.p6.3.m3.2.2.1.1.1" xref="S2.SS2.p6.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS2.p6.3.m3.2.2.1.1.1.2" xref="S2.SS2.p6.3.m3.2.2.1.1.1.2.cmml">y</mi><mn id="S2.SS2.p6.3.m3.2.2.1.1.1.3" xref="S2.SS2.p6.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.p6.3.m3.3.3.2.2.4" xref="S2.SS2.p6.3.m3.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p6.3.m3.1.1" xref="S2.SS2.p6.3.m3.1.1.cmml">…</mi><mo id="S2.SS2.p6.3.m3.3.3.2.2.5" xref="S2.SS2.p6.3.m3.3.3.2.3.cmml">,</mo><msub id="S2.SS2.p6.3.m3.3.3.2.2.2" xref="S2.SS2.p6.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS2.p6.3.m3.3.3.2.2.2.2" xref="S2.SS2.p6.3.m3.3.3.2.2.2.2.cmml">y</mi><mi id="S2.SS2.p6.3.m3.3.3.2.2.2.3" xref="S2.SS2.p6.3.m3.3.3.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS2.p6.3.m3.3.3.2.2.6" xref="S2.SS2.p6.3.m3.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.3.m3.3b"><apply id="S2.SS2.p6.3.m3.3.3.cmml" xref="S2.SS2.p6.3.m3.3.3"><eq id="S2.SS2.p6.3.m3.3.3.3.cmml" xref="S2.SS2.p6.3.m3.3.3.3"></eq><ci id="S2.SS2.p6.3.m3.3.3.4.cmml" xref="S2.SS2.p6.3.m3.3.3.4">𝐲</ci><set id="S2.SS2.p6.3.m3.3.3.2.3.cmml" xref="S2.SS2.p6.3.m3.3.3.2.2"><apply id="S2.SS2.p6.3.m3.2.2.1.1.1.cmml" xref="S2.SS2.p6.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p6.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS2.p6.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.p6.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS2.p6.3.m3.2.2.1.1.1.2">𝑦</ci><cn type="integer" id="S2.SS2.p6.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS2.p6.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS2.p6.3.m3.1.1.cmml" xref="S2.SS2.p6.3.m3.1.1">…</ci><apply id="S2.SS2.p6.3.m3.3.3.2.2.2.cmml" xref="S2.SS2.p6.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p6.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS2.p6.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.p6.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS2.p6.3.m3.3.3.2.2.2.2">𝑦</ci><ci id="S2.SS2.p6.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS2.p6.3.m3.3.3.2.2.2.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.3.m3.3c">\mathbf{y}=\{y_{1},\ldots,y_{n}\}</annotation></semantics></math>. In addition, let <math id="S2.SS2.p6.4.m4.2" class="ltx_Math" alttext="\hat{y_{j}}=f(\mathbf{x}_{j};\mathbf{w})" display="inline"><semantics id="S2.SS2.p6.4.m4.2a"><mrow id="S2.SS2.p6.4.m4.2.2" xref="S2.SS2.p6.4.m4.2.2.cmml"><mover accent="true" id="S2.SS2.p6.4.m4.2.2.3" xref="S2.SS2.p6.4.m4.2.2.3.cmml"><msub id="S2.SS2.p6.4.m4.2.2.3.2" xref="S2.SS2.p6.4.m4.2.2.3.2.cmml"><mi id="S2.SS2.p6.4.m4.2.2.3.2.2" xref="S2.SS2.p6.4.m4.2.2.3.2.2.cmml">y</mi><mi id="S2.SS2.p6.4.m4.2.2.3.2.3" xref="S2.SS2.p6.4.m4.2.2.3.2.3.cmml">j</mi></msub><mo id="S2.SS2.p6.4.m4.2.2.3.1" xref="S2.SS2.p6.4.m4.2.2.3.1.cmml">^</mo></mover><mo id="S2.SS2.p6.4.m4.2.2.2" xref="S2.SS2.p6.4.m4.2.2.2.cmml">=</mo><mrow id="S2.SS2.p6.4.m4.2.2.1" xref="S2.SS2.p6.4.m4.2.2.1.cmml"><mi id="S2.SS2.p6.4.m4.2.2.1.3" xref="S2.SS2.p6.4.m4.2.2.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p6.4.m4.2.2.1.2" xref="S2.SS2.p6.4.m4.2.2.1.2.cmml">​</mo><mrow id="S2.SS2.p6.4.m4.2.2.1.1.1" xref="S2.SS2.p6.4.m4.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.p6.4.m4.2.2.1.1.1.2" xref="S2.SS2.p6.4.m4.2.2.1.1.2.cmml">(</mo><msub id="S2.SS2.p6.4.m4.2.2.1.1.1.1" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1.cmml"><mi id="S2.SS2.p6.4.m4.2.2.1.1.1.1.2" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1.2.cmml">𝐱</mi><mi id="S2.SS2.p6.4.m4.2.2.1.1.1.1.3" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1.3.cmml">j</mi></msub><mo id="S2.SS2.p6.4.m4.2.2.1.1.1.3" xref="S2.SS2.p6.4.m4.2.2.1.1.2.cmml">;</mo><mi id="S2.SS2.p6.4.m4.1.1" xref="S2.SS2.p6.4.m4.1.1.cmml">𝐰</mi><mo stretchy="false" id="S2.SS2.p6.4.m4.2.2.1.1.1.4" xref="S2.SS2.p6.4.m4.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.4.m4.2b"><apply id="S2.SS2.p6.4.m4.2.2.cmml" xref="S2.SS2.p6.4.m4.2.2"><eq id="S2.SS2.p6.4.m4.2.2.2.cmml" xref="S2.SS2.p6.4.m4.2.2.2"></eq><apply id="S2.SS2.p6.4.m4.2.2.3.cmml" xref="S2.SS2.p6.4.m4.2.2.3"><ci id="S2.SS2.p6.4.m4.2.2.3.1.cmml" xref="S2.SS2.p6.4.m4.2.2.3.1">^</ci><apply id="S2.SS2.p6.4.m4.2.2.3.2.cmml" xref="S2.SS2.p6.4.m4.2.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.p6.4.m4.2.2.3.2.1.cmml" xref="S2.SS2.p6.4.m4.2.2.3.2">subscript</csymbol><ci id="S2.SS2.p6.4.m4.2.2.3.2.2.cmml" xref="S2.SS2.p6.4.m4.2.2.3.2.2">𝑦</ci><ci id="S2.SS2.p6.4.m4.2.2.3.2.3.cmml" xref="S2.SS2.p6.4.m4.2.2.3.2.3">𝑗</ci></apply></apply><apply id="S2.SS2.p6.4.m4.2.2.1.cmml" xref="S2.SS2.p6.4.m4.2.2.1"><times id="S2.SS2.p6.4.m4.2.2.1.2.cmml" xref="S2.SS2.p6.4.m4.2.2.1.2"></times><ci id="S2.SS2.p6.4.m4.2.2.1.3.cmml" xref="S2.SS2.p6.4.m4.2.2.1.3">𝑓</ci><list id="S2.SS2.p6.4.m4.2.2.1.1.2.cmml" xref="S2.SS2.p6.4.m4.2.2.1.1.1"><apply id="S2.SS2.p6.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p6.4.m4.2.2.1.1.1.1.1.cmml" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p6.4.m4.2.2.1.1.1.1.2.cmml" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1.2">𝐱</ci><ci id="S2.SS2.p6.4.m4.2.2.1.1.1.1.3.cmml" xref="S2.SS2.p6.4.m4.2.2.1.1.1.1.3">𝑗</ci></apply><ci id="S2.SS2.p6.4.m4.1.1.cmml" xref="S2.SS2.p6.4.m4.1.1">𝐰</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.4.m4.2c">\hat{y_{j}}=f(\mathbf{x}_{j};\mathbf{w})</annotation></semantics></math>
denote the predicted result from the model <math id="S2.SS2.p6.5.m5.1" class="ltx_Math" alttext="\mathbf{w}" display="inline"><semantics id="S2.SS2.p6.5.m5.1a"><mi id="S2.SS2.p6.5.m5.1.1" xref="S2.SS2.p6.5.m5.1.1.cmml">𝐰</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.5.m5.1b"><ci id="S2.SS2.p6.5.m5.1.1.cmml" xref="S2.SS2.p6.5.m5.1.1">𝐰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.5.m5.1c">\mathbf{w}</annotation></semantics></math> updated/trained by data
vector <math id="S2.SS2.p6.6.m6.1" class="ltx_Math" alttext="x_{j}" display="inline"><semantics id="S2.SS2.p6.6.m6.1a"><msub id="S2.SS2.p6.6.m6.1.1" xref="S2.SS2.p6.6.m6.1.1.cmml"><mi id="S2.SS2.p6.6.m6.1.1.2" xref="S2.SS2.p6.6.m6.1.1.2.cmml">x</mi><mi id="S2.SS2.p6.6.m6.1.1.3" xref="S2.SS2.p6.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.6.m6.1b"><apply id="S2.SS2.p6.6.m6.1.1.cmml" xref="S2.SS2.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p6.6.m6.1.1.1.cmml" xref="S2.SS2.p6.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p6.6.m6.1.1.2.cmml" xref="S2.SS2.p6.6.m6.1.1.2">𝑥</ci><ci id="S2.SS2.p6.6.m6.1.1.3.cmml" xref="S2.SS2.p6.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.6.m6.1c">x_{j}</annotation></semantics></math>. Table <a href="#S2.T3" title="TABLE III ‣ II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes several
loss functions of common ML models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T3.13.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S2.T3.14.2" class="ltx_text" style="font-size:90%;">Loss functions of common ML models</span></figcaption>
<table id="S2.T3.10" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.1" class="ltx_tr">
<td id="S2.T3.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:54.1pt;">
<span id="S2.T3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.2.1.1" class="ltx_p">Model</span>
</span>
</td>
<td id="S2.T3.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;">
<span id="S2.T3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.1.1.1" class="ltx_p">Loss function <math id="S2.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="L(\mathbf{w}_{i}^{t})" display="inline"><semantics id="S2.T3.1.1.1.1.1.m1.1a"><mrow id="S2.T3.1.1.1.1.1.m1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T3.1.1.1.1.1.m1.1.1.3" xref="S2.T3.1.1.1.1.1.m1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.T3.1.1.1.1.1.m1.1.1.2" xref="S2.T3.1.1.1.1.1.m1.1.1.2.cmml">​</mo><mrow id="S2.T3.1.1.1.1.1.m1.1.1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.2" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml"><mi id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.2" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.2.cmml">𝐰</mi><mi id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.3" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.3" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.1.1.1.1.1.m1.1b"><apply id="S2.T3.1.1.1.1.1.m1.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1"><times id="S2.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.2"></times><ci id="S2.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.3">𝐿</ci><apply id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1">superscript</csymbol><apply id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.2">𝐰</ci><ci id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.1.1.1.1.1.m1.1c">L(\mathbf{w}_{i}^{t})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S2.T3.2.2" class="ltx_tr">
<td id="S2.T3.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:54.1pt;">
<span id="S2.T3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.2.1.1" class="ltx_p">Neural network</span>
</span>
</td>
<td id="S2.T3.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;">
<span id="S2.T3.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.1.1.1" class="ltx_p"><math id="S2.T3.2.2.1.1.1.m1.2" class="ltx_Math" alttext="\frac{1}{n}\sum_{j=1}^{n}(y_{i}-f(\mathbf{x}_{j};\mathbf{w}))^{2}" display="inline"><semantics id="S2.T3.2.2.1.1.1.m1.2a"><mrow id="S2.T3.2.2.1.1.1.m1.2.2" xref="S2.T3.2.2.1.1.1.m1.2.2.cmml"><mfrac id="S2.T3.2.2.1.1.1.m1.2.2.3" xref="S2.T3.2.2.1.1.1.m1.2.2.3.cmml"><mn id="S2.T3.2.2.1.1.1.m1.2.2.3.2" xref="S2.T3.2.2.1.1.1.m1.2.2.3.2.cmml">1</mn><mi id="S2.T3.2.2.1.1.1.m1.2.2.3.3" xref="S2.T3.2.2.1.1.1.m1.2.2.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.2.2.2" xref="S2.T3.2.2.1.1.1.m1.2.2.2.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.cmml"><msubsup id="S2.T3.2.2.1.1.1.m1.2.2.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.cmml"><mo rspace="0em" id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.2.cmml">∑</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.cmml"><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.2.cmml">j</mi><mo id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.2.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.3.cmml">n</mi></msubsup><msup id="S2.T3.2.2.1.1.1.m1.2.2.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.cmml"><mrow id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.cmml"><msub id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.2.cmml">y</mi><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.2.cmml">−</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.T3.2.2.1.1.1.m1.1.1" xref="S2.T3.2.2.1.1.1.m1.1.1.cmml">𝐰</mi><mo stretchy="false" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mn id="S2.T3.2.2.1.1.1.m1.2.2.1.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.2.2.1.1.1.m1.2b"><apply id="S2.T3.2.2.1.1.1.m1.2.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2"><times id="S2.T3.2.2.1.1.1.m1.2.2.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2"></times><apply id="S2.T3.2.2.1.1.1.m1.2.2.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.3"><divide id="S2.T3.2.2.1.1.1.m1.2.2.3.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.3"></divide><cn type="integer" id="S2.T3.2.2.1.1.1.m1.2.2.3.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.3.2">1</cn><ci id="S2.T3.2.2.1.1.1.m1.2.2.3.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.3.3">𝑛</ci></apply><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1"><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.2.2.1.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2">superscript</csymbol><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2">subscript</csymbol><sum id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.2"></sum><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3"><eq id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.1"></eq><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.2">𝑗</ci><cn type="integer" id="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.2.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.2.3">𝑛</ci></apply><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1">superscript</csymbol><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1"><minus id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.2"></minus><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.2">𝑦</ci><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1"><times id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.2"></times><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.3">𝑓</ci><list id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1"><apply id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝐱</ci><ci id="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply><ci id="S2.T3.2.2.1.1.1.m1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1">𝐰</ci></list></apply></apply><cn type="integer" id="S2.T3.2.2.1.1.1.m1.2.2.1.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.2.2.1.1.1.m1.2c">\frac{1}{n}\sum_{j=1}^{n}(y_{i}-f(\mathbf{x}_{j};\mathbf{w}))^{2}</annotation></semantics></math>(Mean
Squared Error)</span>
</span>
</td>
</tr>
<tr id="S2.T3.3.3" class="ltx_tr">
<td id="S2.T3.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:54.1pt;">
<span id="S2.T3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.3.3.2.1.1" class="ltx_p">Linear regression</span>
</span>
</td>
<td id="S2.T3.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;">
<span id="S2.T3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.3.3.1.1.1" class="ltx_p"><math id="S2.T3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\frac{1}{2}\left\|y_{j}-\mathbf{w}^{T}\mathbf{x}_{j}\right\|^{2}" display="inline"><semantics id="S2.T3.3.3.1.1.1.m1.1a"><mrow id="S2.T3.3.3.1.1.1.m1.1.1" xref="S2.T3.3.3.1.1.1.m1.1.1.cmml"><mfrac id="S2.T3.3.3.1.1.1.m1.1.1.3" xref="S2.T3.3.3.1.1.1.m1.1.1.3.cmml"><mn id="S2.T3.3.3.1.1.1.m1.1.1.3.2" xref="S2.T3.3.3.1.1.1.m1.1.1.3.2.cmml">1</mn><mn id="S2.T3.3.3.1.1.1.m1.1.1.3.3" xref="S2.T3.3.3.1.1.1.m1.1.1.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.T3.3.3.1.1.1.m1.1.1.2" xref="S2.T3.3.3.1.1.1.m1.1.1.2.cmml">​</mo><msup id="S2.T3.3.3.1.1.1.m1.1.1.1" xref="S2.T3.3.3.1.1.1.m1.1.1.1.cmml"><mrow id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.2.cmml"><mo id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.2" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.cmml"><msub id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.2" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.1" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.cmml"><msup id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.2" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.2.cmml">𝐰</mi><mi id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.1" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.2" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.2.cmml">𝐱</mi><mi id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.3.cmml">j</mi></msub></mrow></mrow><mo id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.T3.3.3.1.1.1.m1.1.1.1.3" xref="S2.T3.3.3.1.1.1.m1.1.1.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.3.3.1.1.1.m1.1b"><apply id="S2.T3.3.3.1.1.1.m1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1"><times id="S2.T3.3.3.1.1.1.m1.1.1.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.2"></times><apply id="S2.T3.3.3.1.1.1.m1.1.1.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.3"><divide id="S2.T3.3.3.1.1.1.m1.1.1.3.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.3"></divide><cn type="integer" id="S2.T3.3.3.1.1.1.m1.1.1.3.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.3.2">1</cn><cn type="integer" id="S2.T3.3.3.1.1.1.m1.1.1.3.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.3.3">2</cn></apply><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.3.3.1.1.1.m1.1.1.1.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1">superscript</csymbol><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.T3.3.3.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1"><minus id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.1"></minus><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3"><times id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.1"></times><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.2">𝐰</ci><ci id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.2.3">𝑇</ci></apply><apply id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.2">𝐱</ci><ci id="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.1.1.1.3.3.3">𝑗</ci></apply></apply></apply></apply><cn type="integer" id="S2.T3.3.3.1.1.1.m1.1.1.1.3.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.3.3.1.1.1.m1.1c">\frac{1}{2}\left\|y_{j}-\mathbf{w}^{T}\mathbf{x}_{j}\right\|^{2}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S2.T3.6.6" class="ltx_tr">
<td id="S2.T3.6.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:54.1pt;">
<span id="S2.T3.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.6.6.4.1.1" class="ltx_p">K-means</span>
</span>
</td>
<td id="S2.T3.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;">
<span id="S2.T3.6.6.3.3" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.6.6.3.3.3" class="ltx_p"><math id="S2.T3.4.4.1.1.1.m1.2" class="ltx_Math" alttext="\sum_{j}\left\|\mathbf{x}_{j}-f(\mathbf{x}_{j};\mathbf{w})\right\|" display="inline"><semantics id="S2.T3.4.4.1.1.1.m1.2a"><mrow id="S2.T3.4.4.1.1.1.m1.2.2" xref="S2.T3.4.4.1.1.1.m1.2.2.cmml"><msub id="S2.T3.4.4.1.1.1.m1.2.2.2" xref="S2.T3.4.4.1.1.1.m1.2.2.2.cmml"><mo id="S2.T3.4.4.1.1.1.m1.2.2.2.2" xref="S2.T3.4.4.1.1.1.m1.2.2.2.2.cmml">∑</mo><mi id="S2.T3.4.4.1.1.1.m1.2.2.2.3" xref="S2.T3.4.4.1.1.1.m1.2.2.2.3.cmml">j</mi></msub><mrow id="S2.T3.4.4.1.1.1.m1.2.2.1.1" xref="S2.T3.4.4.1.1.1.m1.2.2.1.2.cmml"><mo lspace="0em" id="S2.T3.4.4.1.1.1.m1.2.2.1.1.2" xref="S2.T3.4.4.1.1.1.m1.2.2.1.2.1.cmml">‖</mo><mrow id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.cmml"><msub id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.cmml"><mi id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.2" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.2.cmml">𝐱</mi><mi id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.3" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.3.cmml">j</mi></msub><mo id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.2" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.2.cmml">−</mo><mrow id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.cmml"><mi id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.3" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.2" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.2.cmml">(</mo><msub id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.3" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.2.cmml">;</mo><mi id="S2.T3.4.4.1.1.1.m1.1.1" xref="S2.T3.4.4.1.1.1.m1.1.1.cmml">𝐰</mi><mo stretchy="false" id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.4" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.T3.4.4.1.1.1.m1.2.2.1.1.3" xref="S2.T3.4.4.1.1.1.m1.2.2.1.2.1.cmml">‖</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.4.4.1.1.1.m1.2b"><apply id="S2.T3.4.4.1.1.1.m1.2.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2"><apply id="S2.T3.4.4.1.1.1.m1.2.2.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S2.T3.4.4.1.1.1.m1.2.2.2.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.2">subscript</csymbol><sum id="S2.T3.4.4.1.1.1.m1.2.2.2.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.2.2"></sum><ci id="S2.T3.4.4.1.1.1.m1.2.2.2.3.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.2.3">𝑗</ci></apply><apply id="S2.T3.4.4.1.1.1.m1.2.2.1.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1"><csymbol cd="latexml" id="S2.T3.4.4.1.1.1.m1.2.2.1.2.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.2">norm</csymbol><apply id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1"><minus id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.2"></minus><apply id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.2">𝐱</ci><ci id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.3.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.3.3">𝑗</ci></apply><apply id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1"><times id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.2"></times><ci id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.3.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.3">𝑓</ci><list id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1"><apply id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.2">𝐱</ci><ci id="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.T3.4.4.1.1.1.m1.2.2.1.1.1.1.1.1.1.3">𝑗</ci></apply><ci id="S2.T3.4.4.1.1.1.m1.1.1.cmml" xref="S2.T3.4.4.1.1.1.m1.1.1">𝐰</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.4.4.1.1.1.m1.2c">\sum_{j}\left\|\mathbf{x}_{j}-f(\mathbf{x}_{j};\mathbf{w})\right\|</annotation></semantics></math>(<math id="S2.T3.5.5.2.2.2.m2.2" class="ltx_Math" alttext="f(\mathbf{x}_{j};\mathbf{w})" display="inline"><semantics id="S2.T3.5.5.2.2.2.m2.2a"><mrow id="S2.T3.5.5.2.2.2.m2.2.2" xref="S2.T3.5.5.2.2.2.m2.2.2.cmml"><mi id="S2.T3.5.5.2.2.2.m2.2.2.3" xref="S2.T3.5.5.2.2.2.m2.2.2.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.T3.5.5.2.2.2.m2.2.2.2" xref="S2.T3.5.5.2.2.2.m2.2.2.2.cmml">​</mo><mrow id="S2.T3.5.5.2.2.2.m2.2.2.1.1" xref="S2.T3.5.5.2.2.2.m2.2.2.1.2.cmml"><mo stretchy="false" id="S2.T3.5.5.2.2.2.m2.2.2.1.1.2" xref="S2.T3.5.5.2.2.2.m2.2.2.1.2.cmml">(</mo><msub id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.cmml"><mi id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.2" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.2.cmml">𝐱</mi><mi id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.3" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.3.cmml">j</mi></msub><mo id="S2.T3.5.5.2.2.2.m2.2.2.1.1.3" xref="S2.T3.5.5.2.2.2.m2.2.2.1.2.cmml">;</mo><mi id="S2.T3.5.5.2.2.2.m2.1.1" xref="S2.T3.5.5.2.2.2.m2.1.1.cmml">𝐰</mi><mo stretchy="false" id="S2.T3.5.5.2.2.2.m2.2.2.1.1.4" xref="S2.T3.5.5.2.2.2.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.5.5.2.2.2.m2.2b"><apply id="S2.T3.5.5.2.2.2.m2.2.2.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2"><times id="S2.T3.5.5.2.2.2.m2.2.2.2.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.2"></times><ci id="S2.T3.5.5.2.2.2.m2.2.2.3.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.3">𝑓</ci><list id="S2.T3.5.5.2.2.2.m2.2.2.1.2.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1"><apply id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.1.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.2.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.2">𝐱</ci><ci id="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.3.cmml" xref="S2.T3.5.5.2.2.2.m2.2.2.1.1.1.3">𝑗</ci></apply><ci id="S2.T3.5.5.2.2.2.m2.1.1.cmml" xref="S2.T3.5.5.2.2.2.m2.1.1">𝐰</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.5.5.2.2.2.m2.2c">f(\mathbf{x}_{j};\mathbf{w})</annotation></semantics></math>
is the centroid of all objects assigned to <math id="S2.T3.6.6.3.3.3.m3.1" class="ltx_Math" alttext="x_{j}" display="inline"><semantics id="S2.T3.6.6.3.3.3.m3.1a"><msub id="S2.T3.6.6.3.3.3.m3.1.1" xref="S2.T3.6.6.3.3.3.m3.1.1.cmml"><mi id="S2.T3.6.6.3.3.3.m3.1.1.2" xref="S2.T3.6.6.3.3.3.m3.1.1.2.cmml">x</mi><mi id="S2.T3.6.6.3.3.3.m3.1.1.3" xref="S2.T3.6.6.3.3.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.6.6.3.3.3.m3.1b"><apply id="S2.T3.6.6.3.3.3.m3.1.1.cmml" xref="S2.T3.6.6.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.T3.6.6.3.3.3.m3.1.1.1.cmml" xref="S2.T3.6.6.3.3.3.m3.1.1">subscript</csymbol><ci id="S2.T3.6.6.3.3.3.m3.1.1.2.cmml" xref="S2.T3.6.6.3.3.3.m3.1.1.2">𝑥</ci><ci id="S2.T3.6.6.3.3.3.m3.1.1.3.cmml" xref="S2.T3.6.6.3.3.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.6.6.3.3.3.m3.1c">x_{j}</annotation></semantics></math>’s class)</span>
</span>
</td>
</tr>
<tr id="S2.T3.10.10" class="ltx_tr">
<td id="S2.T3.10.10.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:54.1pt;">
<span id="S2.T3.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.10.10.5.1.1" class="ltx_p">squared-SVM</span>
</span>
</td>
<td id="S2.T3.10.10.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:142.3pt;">
<span id="S2.T3.10.10.4.4" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.10.10.4.4.4" class="ltx_p"><math id="S2.T3.7.7.1.1.1.m1.3" class="ltx_Math" alttext="[\frac{1}{n}\sum_{j=1}^{n}\max(0,1-y_{j}(\mathbf{w}^{T}\mathbf{x}_{j}-bias))]" display="inline"><semantics id="S2.T3.7.7.1.1.1.m1.3a"><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1" xref="S2.T3.7.7.1.1.1.m1.3.3.2.cmml"><mo stretchy="false" id="S2.T3.7.7.1.1.1.m1.3.3.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.2.1.cmml">[</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.cmml"><mfrac id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.cmml"><mn id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.2.cmml">1</mn><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.2.cmml">​</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.cmml"><msubsup id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.cmml"><mo id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.2.cmml">j</mi><mo id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.3.cmml">n</mi></msubsup><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml"><mi id="S2.T3.7.7.1.1.1.m1.1.1" xref="S2.T3.7.7.1.1.1.m1.1.1.cmml">max</mi><mo id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1a" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml">(</mo><mn id="S2.T3.7.7.1.1.1.m1.2.2" xref="S2.T3.7.7.1.1.1.m1.2.2.cmml">0</mn><mo id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml">,</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.cmml"><mn id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.3.cmml">1</mn><mo id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msup id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐰</mi><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝐱</mi><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">j</mi></msub></mrow><mo id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.4" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1b" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.5" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.5.cmml">s</mi></mrow></mrow><mo stretchy="false" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.4" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false" id="S2.T3.7.7.1.1.1.m1.3.3.1.3" xref="S2.T3.7.7.1.1.1.m1.3.3.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.7.7.1.1.1.m1.3b"><apply id="S2.T3.7.7.1.1.1.m1.3.3.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1"><csymbol cd="latexml" id="S2.T3.7.7.1.1.1.m1.3.3.2.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.2">delimited-[]</csymbol><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1"><times id="S2.T3.7.7.1.1.1.m1.3.3.1.1.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.2"></times><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3"><divide id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3"></divide><cn type="integer" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.2">1</cn><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.3.3">𝑛</ci></apply><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1"><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.2"></sum><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3"><eq id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.2">𝑗</ci><cn type="integer" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.2.3">𝑛</ci></apply><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1"><max id="S2.T3.7.7.1.1.1.m1.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.1.1"></max><cn type="integer" id="S2.T3.7.7.1.1.1.m1.2.2.cmml" xref="S2.T3.7.7.1.1.1.m1.2.2">0</cn><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1"><minus id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.2"></minus><cn type="integer" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.3">1</cn><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1"><times id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.2"></times><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><minus id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐰</ci><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑇</ci></apply><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝐱</ci><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.4">𝑎</ci><ci id="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.5.cmml" xref="S2.T3.7.7.1.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.5">𝑠</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.7.7.1.1.1.m1.3c">[\frac{1}{n}\sum_{j=1}^{n}\max(0,1-y_{j}(\mathbf{w}^{T}\mathbf{x}_{j}-bias))]</annotation></semantics></math><math id="S2.T3.8.8.2.2.2.m2.1" class="ltx_Math" alttext="+\lambda\left\|\mathbf{w}^{T}\right\|^{2}" display="inline"><semantics id="S2.T3.8.8.2.2.2.m2.1a"><mrow id="S2.T3.8.8.2.2.2.m2.1.1" xref="S2.T3.8.8.2.2.2.m2.1.1.cmml"><mo id="S2.T3.8.8.2.2.2.m2.1.1a" xref="S2.T3.8.8.2.2.2.m2.1.1.cmml">+</mo><mrow id="S2.T3.8.8.2.2.2.m2.1.1.1" xref="S2.T3.8.8.2.2.2.m2.1.1.1.cmml"><mi id="S2.T3.8.8.2.2.2.m2.1.1.1.3" xref="S2.T3.8.8.2.2.2.m2.1.1.1.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S2.T3.8.8.2.2.2.m2.1.1.1.2" xref="S2.T3.8.8.2.2.2.m2.1.1.1.2.cmml">​</mo><msup id="S2.T3.8.8.2.2.2.m2.1.1.1.1" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.cmml"><mrow id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.2.cmml"><mo id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.2" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.2.1.cmml">‖</mo><msup id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.2" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.2.cmml">𝐰</mi><mi id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.3" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.3.cmml">T</mi></msup><mo id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.3" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.T3.8.8.2.2.2.m2.1.1.1.1.3" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.8.8.2.2.2.m2.1b"><apply id="S2.T3.8.8.2.2.2.m2.1.1.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1"><plus id="S2.T3.8.8.2.2.2.m2.1.1.2.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1"></plus><apply id="S2.T3.8.8.2.2.2.m2.1.1.1.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1"><times id="S2.T3.8.8.2.2.2.m2.1.1.1.2.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.2"></times><ci id="S2.T3.8.8.2.2.2.m2.1.1.1.3.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.3">𝜆</ci><apply id="S2.T3.8.8.2.2.2.m2.1.1.1.1.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.8.8.2.2.2.m2.1.1.1.1.2.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1">superscript</csymbol><apply id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.2.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.2.1.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.2">𝐰</ci><ci id="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.1.1.1.3">𝑇</ci></apply></apply><cn type="integer" id="S2.T3.8.8.2.2.2.m2.1.1.1.1.3.cmml" xref="S2.T3.8.8.2.2.2.m2.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.8.8.2.2.2.m2.1c">+\lambda\left\|\mathbf{w}^{T}\right\|^{2}</annotation></semantics></math>(<math id="S2.T3.9.9.3.3.3.m3.1" class="ltx_Math" alttext="bias" display="inline"><semantics id="S2.T3.9.9.3.3.3.m3.1a"><mrow id="S2.T3.9.9.3.3.3.m3.1.1" xref="S2.T3.9.9.3.3.3.m3.1.1.cmml"><mi id="S2.T3.9.9.3.3.3.m3.1.1.2" xref="S2.T3.9.9.3.3.3.m3.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.T3.9.9.3.3.3.m3.1.1.1" xref="S2.T3.9.9.3.3.3.m3.1.1.1.cmml">​</mo><mi id="S2.T3.9.9.3.3.3.m3.1.1.3" xref="S2.T3.9.9.3.3.3.m3.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.9.9.3.3.3.m3.1.1.1a" xref="S2.T3.9.9.3.3.3.m3.1.1.1.cmml">​</mo><mi id="S2.T3.9.9.3.3.3.m3.1.1.4" xref="S2.T3.9.9.3.3.3.m3.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.9.9.3.3.3.m3.1.1.1b" xref="S2.T3.9.9.3.3.3.m3.1.1.1.cmml">​</mo><mi id="S2.T3.9.9.3.3.3.m3.1.1.5" xref="S2.T3.9.9.3.3.3.m3.1.1.5.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.9.9.3.3.3.m3.1b"><apply id="S2.T3.9.9.3.3.3.m3.1.1.cmml" xref="S2.T3.9.9.3.3.3.m3.1.1"><times id="S2.T3.9.9.3.3.3.m3.1.1.1.cmml" xref="S2.T3.9.9.3.3.3.m3.1.1.1"></times><ci id="S2.T3.9.9.3.3.3.m3.1.1.2.cmml" xref="S2.T3.9.9.3.3.3.m3.1.1.2">𝑏</ci><ci id="S2.T3.9.9.3.3.3.m3.1.1.3.cmml" xref="S2.T3.9.9.3.3.3.m3.1.1.3">𝑖</ci><ci id="S2.T3.9.9.3.3.3.m3.1.1.4.cmml" xref="S2.T3.9.9.3.3.3.m3.1.1.4">𝑎</ci><ci id="S2.T3.9.9.3.3.3.m3.1.1.5.cmml" xref="S2.T3.9.9.3.3.3.m3.1.1.5">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.9.9.3.3.3.m3.1c">bias</annotation></semantics></math>
is the bias parameter and <math id="S2.T3.10.10.4.4.4.m4.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.T3.10.10.4.4.4.m4.1a"><mi id="S2.T3.10.10.4.4.4.m4.1.1" xref="S2.T3.10.10.4.4.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.T3.10.10.4.4.4.m4.1b"><ci id="S2.T3.10.10.4.4.4.m4.1.1.cmml" xref="S2.T3.10.10.4.4.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.10.10.4.4.4.m4.1c">\lambda</annotation></semantics></math> is const.)</span>
</span>
</td>
</tr>
</table>
</figure>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">Global model aggregation is an integral part of FL. A straightforward
and classical algorithm for aggregating the local models is the <span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which is similar to that of local SGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. The pseudocode for <span id="S2.SS2.p7.1.2" class="ltx_text ltx_font_italic">FedAvg</span> is given in Algorithm <a href="#alg1" title="Algorithm 1 ‣ II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<div id="alg1.2" class="ltx_listing ltx_listing">
<div id="alg0.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg0.l1.2" class="ltx_text" style="font-size:70%;">Local minibatch size </span><math id="alg0.l1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="alg0.l1.m1.1a"><mi mathsize="70%" id="alg0.l1.m1.1.1" xref="alg0.l1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg0.l1.m1.1b"><ci id="alg0.l1.m1.1.1.cmml" xref="alg0.l1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l1.m1.1c">B</annotation></semantics></math><span id="alg0.l1.3" class="ltx_text" style="font-size:70%;">, number of participants </span><math id="alg0.l1.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="alg0.l1.m2.1a"><mi mathsize="70%" id="alg0.l1.m2.1.1" xref="alg0.l1.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="alg0.l1.m2.1b"><ci id="alg0.l1.m2.1.1.cmml" xref="alg0.l1.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l1.m2.1c">m</annotation></semantics></math><span id="alg0.l1.4" class="ltx_text" style="font-size:70%;"> per iteration, number of local epochs </span><math id="alg0.l1.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg0.l1.m3.1a"><mi mathsize="70%" id="alg0.l1.m3.1.1" xref="alg0.l1.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg0.l1.m3.1b"><ci id="alg0.l1.m3.1.1.cmml" xref="alg0.l1.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l1.m3.1c">E</annotation></semantics></math><span id="alg0.l1.5" class="ltx_text" style="font-size:70%;">, and learning rate </span><math id="alg0.l1.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg0.l1.m4.1a"><mi mathsize="70%" id="alg0.l1.m4.1.1" xref="alg0.l1.m4.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg0.l1.m4.1b"><ci id="alg0.l1.m4.1.1.cmml" xref="alg0.l1.m4.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l1.m4.1c">\eta</annotation></semantics></math><span id="alg0.l1.6" class="ltx_text" style="font-size:70%;">.
</span>
</div>
<div id="alg0.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg0.l2.2" class="ltx_text" style="font-size:70%;">Global model </span><math id="alg0.l2.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}" display="inline"><semantics id="alg0.l2.m1.1a"><msub id="alg0.l2.m1.1.1" xref="alg0.l2.m1.1.1.cmml"><mi mathsize="70%" id="alg0.l2.m1.1.1.2" xref="alg0.l2.m1.1.1.2.cmml">𝐰</mi><mi mathsize="70%" id="alg0.l2.m1.1.1.3" xref="alg0.l2.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="alg0.l2.m1.1b"><apply id="alg0.l2.m1.1.1.cmml" xref="alg0.l2.m1.1.1"><csymbol cd="ambiguous" id="alg0.l2.m1.1.1.1.cmml" xref="alg0.l2.m1.1.1">subscript</csymbol><ci id="alg0.l2.m1.1.1.2.cmml" xref="alg0.l2.m1.1.1.2">𝐰</ci><ci id="alg0.l2.m1.1.1.3.cmml" xref="alg0.l2.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l2.m1.1c">\mathbf{w}_{G}</annotation></semantics></math><span id="alg0.l2.3" class="ltx_text" style="font-size:70%;">.
</span>
</div>
<div id="alg0.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg0.l3.2" class="ltx_text" style="font-size:70%;">[Participant </span><math id="alg0.l3.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="alg0.l3.m1.1a"><mi mathsize="70%" id="alg0.l3.m1.1.1" xref="alg0.l3.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg0.l3.m1.1b"><ci id="alg0.l3.m1.1.1.cmml" xref="alg0.l3.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l3.m1.1c">i</annotation></semantics></math><span id="alg0.l3.3" class="ltx_text" style="font-size:70%;">]
</span>
</div>
<div id="alg0.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg0.l4.2" class="ltx_text ltx_font_bold" style="font-size:70%;">LocalTraining</span><span id="alg0.l4.3" class="ltx_text" style="font-size:70%;">(</span><math id="alg0.l4.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="alg0.l4.m1.1a"><mi mathsize="70%" id="alg0.l4.m1.1.1" xref="alg0.l4.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg0.l4.m1.1b"><ci id="alg0.l4.m1.1.1.cmml" xref="alg0.l4.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l4.m1.1c">i</annotation></semantics></math><span id="alg0.l4.4" class="ltx_text" style="font-size:70%;">, </span><math id="alg0.l4.m2.1" class="ltx_Math" alttext="\mathbf{w}" display="inline"><semantics id="alg0.l4.m2.1a"><mi mathsize="70%" id="alg0.l4.m2.1.1" xref="alg0.l4.m2.1.1.cmml">𝐰</mi><annotation-xml encoding="MathML-Content" id="alg0.l4.m2.1b"><ci id="alg0.l4.m2.1.1.cmml" xref="alg0.l4.m2.1.1">𝐰</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l4.m2.1c">\mathbf{w}</annotation></semantics></math><span id="alg0.l4.5" class="ltx_text" style="font-size:70%;">):
</span>
</div>
<div id="alg0.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg0.l5.2" class="ltx_text" style="font-size:70%;">Split local dataset </span><math id="alg0.l5.m1.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="alg0.l5.m1.1a"><msub id="alg0.l5.m1.1.1" xref="alg0.l5.m1.1.1.cmml"><mi mathsize="70%" id="alg0.l5.m1.1.1.2" xref="alg0.l5.m1.1.1.2.cmml">D</mi><mi mathsize="70%" id="alg0.l5.m1.1.1.3" xref="alg0.l5.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg0.l5.m1.1b"><apply id="alg0.l5.m1.1.1.cmml" xref="alg0.l5.m1.1.1"><csymbol cd="ambiguous" id="alg0.l5.m1.1.1.1.cmml" xref="alg0.l5.m1.1.1">subscript</csymbol><ci id="alg0.l5.m1.1.1.2.cmml" xref="alg0.l5.m1.1.1.2">𝐷</ci><ci id="alg0.l5.m1.1.1.3.cmml" xref="alg0.l5.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l5.m1.1c">D_{i}</annotation></semantics></math><span id="alg0.l5.3" class="ltx_text" style="font-size:70%;"> to minibatches of size </span><math id="alg0.l5.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="alg0.l5.m2.1a"><mi mathsize="70%" id="alg0.l5.m2.1.1" xref="alg0.l5.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg0.l5.m2.1b"><ci id="alg0.l5.m2.1.1.cmml" xref="alg0.l5.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l5.m2.1c">B</annotation></semantics></math><span id="alg0.l5.4" class="ltx_text" style="font-size:70%;"> which are included into the set </span><math id="alg0.l5.m3.1" class="ltx_Math" alttext="\mathcal{B}_{i}" display="inline"><semantics id="alg0.l5.m3.1a"><msub id="alg0.l5.m3.1.1" xref="alg0.l5.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="alg0.l5.m3.1.1.2" xref="alg0.l5.m3.1.1.2.cmml">ℬ</mi><mi mathsize="70%" id="alg0.l5.m3.1.1.3" xref="alg0.l5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg0.l5.m3.1b"><apply id="alg0.l5.m3.1.1.cmml" xref="alg0.l5.m3.1.1"><csymbol cd="ambiguous" id="alg0.l5.m3.1.1.1.cmml" xref="alg0.l5.m3.1.1">subscript</csymbol><ci id="alg0.l5.m3.1.1.2.cmml" xref="alg0.l5.m3.1.1.2">ℬ</ci><ci id="alg0.l5.m3.1.1.3.cmml" xref="alg0.l5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l5.m3.1c">\mathcal{B}_{i}</annotation></semantics></math><span id="alg0.l5.5" class="ltx_text" style="font-size:70%;">.
</span>
</div>
<div id="alg0.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg0.l6.2" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span><span id="alg0.l6.3" class="ltx_text" style="font-size:70%;"> each local epoch </span><math id="alg0.l6.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="alg0.l6.m1.1a"><mi mathsize="70%" id="alg0.l6.m1.1.1" xref="alg0.l6.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="alg0.l6.m1.1b"><ci id="alg0.l6.m1.1.1.cmml" xref="alg0.l6.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l6.m1.1c">j</annotation></semantics></math><span id="alg0.l6.4" class="ltx_text" style="font-size:70%;"> from </span><math id="alg0.l6.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="alg0.l6.m2.1a"><mn mathsize="70%" id="alg0.l6.m2.1.1" xref="alg0.l6.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="alg0.l6.m2.1b"><cn type="integer" id="alg0.l6.m2.1.1.cmml" xref="alg0.l6.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="alg0.l6.m2.1c">1</annotation></semantics></math><span id="alg0.l6.5" class="ltx_text" style="font-size:70%;"> to </span><math id="alg0.l6.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg0.l6.m3.1a"><mi mathsize="70%" id="alg0.l6.m3.1.1" xref="alg0.l6.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg0.l6.m3.1b"><ci id="alg0.l6.m3.1.1.cmml" xref="alg0.l6.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l6.m3.1c">E</annotation></semantics></math><span id="alg0.l6.6" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l6.7" class="ltx_text ltx_font_bold" style="font-size:70%;">do</span><span id="alg0.l6.8" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg0.l7.2" class="ltx_text" style="font-size:70%;">    </span><span id="alg0.l7.3" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span><span id="alg0.l7.4" class="ltx_text" style="font-size:70%;"> each </span><math id="alg0.l7.m1.1" class="ltx_Math" alttext="b\in\mathcal{B}_{i}" display="inline"><semantics id="alg0.l7.m1.1a"><mrow id="alg0.l7.m1.1.1" xref="alg0.l7.m1.1.1.cmml"><mi mathsize="70%" id="alg0.l7.m1.1.1.2" xref="alg0.l7.m1.1.1.2.cmml">b</mi><mo mathsize="70%" id="alg0.l7.m1.1.1.1" xref="alg0.l7.m1.1.1.1.cmml">∈</mo><msub id="alg0.l7.m1.1.1.3" xref="alg0.l7.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="alg0.l7.m1.1.1.3.2" xref="alg0.l7.m1.1.1.3.2.cmml">ℬ</mi><mi mathsize="70%" id="alg0.l7.m1.1.1.3.3" xref="alg0.l7.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg0.l7.m1.1b"><apply id="alg0.l7.m1.1.1.cmml" xref="alg0.l7.m1.1.1"><in id="alg0.l7.m1.1.1.1.cmml" xref="alg0.l7.m1.1.1.1"></in><ci id="alg0.l7.m1.1.1.2.cmml" xref="alg0.l7.m1.1.1.2">𝑏</ci><apply id="alg0.l7.m1.1.1.3.cmml" xref="alg0.l7.m1.1.1.3"><csymbol cd="ambiguous" id="alg0.l7.m1.1.1.3.1.cmml" xref="alg0.l7.m1.1.1.3">subscript</csymbol><ci id="alg0.l7.m1.1.1.3.2.cmml" xref="alg0.l7.m1.1.1.3.2">ℬ</ci><ci id="alg0.l7.m1.1.1.3.3.cmml" xref="alg0.l7.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l7.m1.1c">b\in\mathcal{B}_{i}</annotation></semantics></math><span id="alg0.l7.5" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l7.6" class="ltx_text ltx_font_bold" style="font-size:70%;">do</span><span id="alg0.l7.7" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg0.l8.2" class="ltx_text" style="font-size:70%;">       </span><math id="alg0.l8.m1.2" class="ltx_Math" alttext="\mathbf{w}\leftarrow\mathbf{w}-\eta\Delta L(\mathbf{w};b)" display="inline"><semantics id="alg0.l8.m1.2a"><mrow id="alg0.l8.m1.2.3" xref="alg0.l8.m1.2.3.cmml"><mi mathsize="70%" id="alg0.l8.m1.2.3.2" xref="alg0.l8.m1.2.3.2.cmml">𝐰</mi><mo mathsize="70%" stretchy="false" id="alg0.l8.m1.2.3.1" xref="alg0.l8.m1.2.3.1.cmml">←</mo><mrow id="alg0.l8.m1.2.3.3" xref="alg0.l8.m1.2.3.3.cmml"><mi mathsize="70%" id="alg0.l8.m1.2.3.3.2" xref="alg0.l8.m1.2.3.3.2.cmml">𝐰</mi><mo mathsize="70%" id="alg0.l8.m1.2.3.3.1" xref="alg0.l8.m1.2.3.3.1.cmml">−</mo><mrow id="alg0.l8.m1.2.3.3.3" xref="alg0.l8.m1.2.3.3.3.cmml"><mi mathsize="70%" id="alg0.l8.m1.2.3.3.3.2" xref="alg0.l8.m1.2.3.3.3.2.cmml">η</mi><mo lspace="0em" rspace="0em" id="alg0.l8.m1.2.3.3.3.1" xref="alg0.l8.m1.2.3.3.3.1.cmml">​</mo><mi mathsize="70%" mathvariant="normal" id="alg0.l8.m1.2.3.3.3.3" xref="alg0.l8.m1.2.3.3.3.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="alg0.l8.m1.2.3.3.3.1a" xref="alg0.l8.m1.2.3.3.3.1.cmml">​</mo><mi mathsize="70%" id="alg0.l8.m1.2.3.3.3.4" xref="alg0.l8.m1.2.3.3.3.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="alg0.l8.m1.2.3.3.3.1b" xref="alg0.l8.m1.2.3.3.3.1.cmml">​</mo><mrow id="alg0.l8.m1.2.3.3.3.5.2" xref="alg0.l8.m1.2.3.3.3.5.1.cmml"><mo maxsize="70%" minsize="70%" id="alg0.l8.m1.2.3.3.3.5.2.1" xref="alg0.l8.m1.2.3.3.3.5.1.cmml">(</mo><mi mathsize="70%" id="alg0.l8.m1.1.1" xref="alg0.l8.m1.1.1.cmml">𝐰</mi><mo mathsize="70%" id="alg0.l8.m1.2.3.3.3.5.2.2" xref="alg0.l8.m1.2.3.3.3.5.1.cmml">;</mo><mi mathsize="70%" id="alg0.l8.m1.2.2" xref="alg0.l8.m1.2.2.cmml">b</mi><mo maxsize="70%" minsize="70%" id="alg0.l8.m1.2.3.3.3.5.2.3" xref="alg0.l8.m1.2.3.3.3.5.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l8.m1.2b"><apply id="alg0.l8.m1.2.3.cmml" xref="alg0.l8.m1.2.3"><ci id="alg0.l8.m1.2.3.1.cmml" xref="alg0.l8.m1.2.3.1">←</ci><ci id="alg0.l8.m1.2.3.2.cmml" xref="alg0.l8.m1.2.3.2">𝐰</ci><apply id="alg0.l8.m1.2.3.3.cmml" xref="alg0.l8.m1.2.3.3"><minus id="alg0.l8.m1.2.3.3.1.cmml" xref="alg0.l8.m1.2.3.3.1"></minus><ci id="alg0.l8.m1.2.3.3.2.cmml" xref="alg0.l8.m1.2.3.3.2">𝐰</ci><apply id="alg0.l8.m1.2.3.3.3.cmml" xref="alg0.l8.m1.2.3.3.3"><times id="alg0.l8.m1.2.3.3.3.1.cmml" xref="alg0.l8.m1.2.3.3.3.1"></times><ci id="alg0.l8.m1.2.3.3.3.2.cmml" xref="alg0.l8.m1.2.3.3.3.2">𝜂</ci><ci id="alg0.l8.m1.2.3.3.3.3.cmml" xref="alg0.l8.m1.2.3.3.3.3">Δ</ci><ci id="alg0.l8.m1.2.3.3.3.4.cmml" xref="alg0.l8.m1.2.3.3.3.4">𝐿</ci><list id="alg0.l8.m1.2.3.3.3.5.1.cmml" xref="alg0.l8.m1.2.3.3.3.5.2"><ci id="alg0.l8.m1.1.1.cmml" xref="alg0.l8.m1.1.1">𝐰</ci><ci id="alg0.l8.m1.2.2.cmml" xref="alg0.l8.m1.2.2">𝑏</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l8.m1.2c">\mathbf{w}\leftarrow\mathbf{w}-\eta\Delta L(\mathbf{w};b)</annotation></semantics></math><span id="alg0.l8.3" class="ltx_text" style="font-size:70%;">   (</span><math id="alg0.l8.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg0.l8.m2.1a"><mi mathsize="70%" id="alg0.l8.m2.1.1" xref="alg0.l8.m2.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg0.l8.m2.1b"><ci id="alg0.l8.m2.1.1.cmml" xref="alg0.l8.m2.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l8.m2.1c">\eta</annotation></semantics></math><span id="alg0.l8.4" class="ltx_text" style="font-size:70%;"> is the learning rate and </span><math id="alg0.l8.m3.1" class="ltx_Math" alttext="\Delta L" display="inline"><semantics id="alg0.l8.m3.1a"><mrow id="alg0.l8.m3.1.1" xref="alg0.l8.m3.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="alg0.l8.m3.1.1.2" xref="alg0.l8.m3.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="alg0.l8.m3.1.1.1" xref="alg0.l8.m3.1.1.1.cmml">​</mo><mi mathsize="70%" id="alg0.l8.m3.1.1.3" xref="alg0.l8.m3.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="alg0.l8.m3.1b"><apply id="alg0.l8.m3.1.1.cmml" xref="alg0.l8.m3.1.1"><times id="alg0.l8.m3.1.1.1.cmml" xref="alg0.l8.m3.1.1.1"></times><ci id="alg0.l8.m3.1.1.2.cmml" xref="alg0.l8.m3.1.1.2">Δ</ci><ci id="alg0.l8.m3.1.1.3.cmml" xref="alg0.l8.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l8.m3.1c">\Delta L</annotation></semantics></math><span id="alg0.l8.5" class="ltx_text" style="font-size:70%;"> is the gradient of </span><math id="alg0.l8.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg0.l8.m4.1a"><mi mathsize="70%" id="alg0.l8.m4.1.1" xref="alg0.l8.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg0.l8.m4.1b"><ci id="alg0.l8.m4.1.1.cmml" xref="alg0.l8.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l8.m4.1c">L</annotation></semantics></math><span id="alg0.l8.6" class="ltx_text" style="font-size:70%;"> on </span><math id="alg0.l8.m5.1" class="ltx_Math" alttext="b" display="inline"><semantics id="alg0.l8.m5.1a"><mi mathsize="70%" id="alg0.l8.m5.1.1" xref="alg0.l8.m5.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="alg0.l8.m5.1b"><ci id="alg0.l8.m5.1.1.cmml" xref="alg0.l8.m5.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l8.m5.1c">b</annotation></semantics></math><span id="alg0.l8.7" class="ltx_text" style="font-size:70%;">.)
</span>
</div>
<div id="alg0.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg0.l9.2" class="ltx_text" style="font-size:70%;">    </span><span id="alg0.l9.3" class="ltx_text ltx_font_bold" style="font-size:70%;">end</span><span id="alg0.l9.4" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l9.5" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span>
</div>
<div id="alg0.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg0.l10.2" class="ltx_text ltx_font_bold" style="font-size:70%;">end</span><span id="alg0.l10.3" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l10.4" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span>
</div>
<div id="alg0.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><span id="alg0.l11.2" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="alg0.l12.2" class="ltx_text" style="font-size:70%;">[Server]
</span>
</div>
<div id="alg0.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span><span id="alg0.l13.2" class="ltx_text" style="font-size:70%;">Initialize </span><math id="alg0.l13.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{0}" display="inline"><semantics id="alg0.l13.m1.1a"><msubsup id="alg0.l13.m1.1.1" xref="alg0.l13.m1.1.1.cmml"><mi mathsize="70%" id="alg0.l13.m1.1.1.2.2" xref="alg0.l13.m1.1.1.2.2.cmml">𝐰</mi><mi mathsize="70%" id="alg0.l13.m1.1.1.2.3" xref="alg0.l13.m1.1.1.2.3.cmml">G</mi><mn mathsize="70%" id="alg0.l13.m1.1.1.3" xref="alg0.l13.m1.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="alg0.l13.m1.1b"><apply id="alg0.l13.m1.1.1.cmml" xref="alg0.l13.m1.1.1"><csymbol cd="ambiguous" id="alg0.l13.m1.1.1.1.cmml" xref="alg0.l13.m1.1.1">superscript</csymbol><apply id="alg0.l13.m1.1.1.2.cmml" xref="alg0.l13.m1.1.1"><csymbol cd="ambiguous" id="alg0.l13.m1.1.1.2.1.cmml" xref="alg0.l13.m1.1.1">subscript</csymbol><ci id="alg0.l13.m1.1.1.2.2.cmml" xref="alg0.l13.m1.1.1.2.2">𝐰</ci><ci id="alg0.l13.m1.1.1.2.3.cmml" xref="alg0.l13.m1.1.1.2.3">𝐺</ci></apply><cn type="integer" id="alg0.l13.m1.1.1.3.cmml" xref="alg0.l13.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l13.m1.1c">\mathbf{w}_{G}^{0}</annotation></semantics></math><span id="alg0.l13.3" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span><span id="alg0.l14.2" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span><span id="alg0.l14.3" class="ltx_text" style="font-size:70%;"> each iteration </span><math id="alg0.l14.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="alg0.l14.m1.1a"><mi mathsize="70%" id="alg0.l14.m1.1.1" xref="alg0.l14.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="alg0.l14.m1.1b"><ci id="alg0.l14.m1.1.1.cmml" xref="alg0.l14.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l14.m1.1c">t</annotation></semantics></math><span id="alg0.l14.4" class="ltx_text" style="font-size:70%;"> from </span><math id="alg0.l14.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="alg0.l14.m2.1a"><mn mathsize="70%" id="alg0.l14.m2.1.1" xref="alg0.l14.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="alg0.l14.m2.1b"><cn type="integer" id="alg0.l14.m2.1.1.cmml" xref="alg0.l14.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="alg0.l14.m2.1c">1</annotation></semantics></math><span id="alg0.l14.5" class="ltx_text" style="font-size:70%;"> to </span><math id="alg0.l14.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg0.l14.m3.1a"><mi mathsize="70%" id="alg0.l14.m3.1.1" xref="alg0.l14.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg0.l14.m3.1b"><ci id="alg0.l14.m3.1.1.cmml" xref="alg0.l14.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l14.m3.1c">T</annotation></semantics></math><span id="alg0.l14.6" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l14.7" class="ltx_text ltx_font_bold" style="font-size:70%;">do</span><span id="alg0.l14.8" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span><span id="alg0.l15.2" class="ltx_text" style="font-size:70%;">    Randomly choose a subset </span><math id="alg0.l15.m1.1" class="ltx_Math" alttext="\mathcal{S}_{t}" display="inline"><semantics id="alg0.l15.m1.1a"><msub id="alg0.l15.m1.1.1" xref="alg0.l15.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="alg0.l15.m1.1.1.2" xref="alg0.l15.m1.1.1.2.cmml">𝒮</mi><mi mathsize="70%" id="alg0.l15.m1.1.1.3" xref="alg0.l15.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg0.l15.m1.1b"><apply id="alg0.l15.m1.1.1.cmml" xref="alg0.l15.m1.1.1"><csymbol cd="ambiguous" id="alg0.l15.m1.1.1.1.cmml" xref="alg0.l15.m1.1.1">subscript</csymbol><ci id="alg0.l15.m1.1.1.2.cmml" xref="alg0.l15.m1.1.1.2">𝒮</ci><ci id="alg0.l15.m1.1.1.3.cmml" xref="alg0.l15.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l15.m1.1c">\mathcal{S}_{t}</annotation></semantics></math><span id="alg0.l15.3" class="ltx_text" style="font-size:70%;"> of </span><math id="alg0.l15.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="alg0.l15.m2.1a"><mi mathsize="70%" id="alg0.l15.m2.1.1" xref="alg0.l15.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="alg0.l15.m2.1b"><ci id="alg0.l15.m2.1.1.cmml" xref="alg0.l15.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l15.m2.1c">m</annotation></semantics></math><span id="alg0.l15.4" class="ltx_text" style="font-size:70%;"> participants from </span><math id="alg0.l15.m3.1" class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="alg0.l15.m3.1a"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="alg0.l15.m3.1.1" xref="alg0.l15.m3.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="alg0.l15.m3.1b"><ci id="alg0.l15.m3.1.1.cmml" xref="alg0.l15.m3.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l15.m3.1c">\mathcal{N}</annotation></semantics></math><span id="alg0.l15.5" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span><span id="alg0.l16.2" class="ltx_text" style="font-size:70%;">    </span><span id="alg0.l16.3" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span><span id="alg0.l16.4" class="ltx_text" style="font-size:70%;"> each partipant </span><math id="alg0.l16.m1.1" class="ltx_Math" alttext="i\in\mathcal{S}_{t}" display="inline"><semantics id="alg0.l16.m1.1a"><mrow id="alg0.l16.m1.1.1" xref="alg0.l16.m1.1.1.cmml"><mi mathsize="70%" id="alg0.l16.m1.1.1.2" xref="alg0.l16.m1.1.1.2.cmml">i</mi><mo mathsize="70%" id="alg0.l16.m1.1.1.1" xref="alg0.l16.m1.1.1.1.cmml">∈</mo><msub id="alg0.l16.m1.1.1.3" xref="alg0.l16.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="alg0.l16.m1.1.1.3.2" xref="alg0.l16.m1.1.1.3.2.cmml">𝒮</mi><mi mathsize="70%" id="alg0.l16.m1.1.1.3.3" xref="alg0.l16.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg0.l16.m1.1b"><apply id="alg0.l16.m1.1.1.cmml" xref="alg0.l16.m1.1.1"><in id="alg0.l16.m1.1.1.1.cmml" xref="alg0.l16.m1.1.1.1"></in><ci id="alg0.l16.m1.1.1.2.cmml" xref="alg0.l16.m1.1.1.2">𝑖</ci><apply id="alg0.l16.m1.1.1.3.cmml" xref="alg0.l16.m1.1.1.3"><csymbol cd="ambiguous" id="alg0.l16.m1.1.1.3.1.cmml" xref="alg0.l16.m1.1.1.3">subscript</csymbol><ci id="alg0.l16.m1.1.1.3.2.cmml" xref="alg0.l16.m1.1.1.3.2">𝒮</ci><ci id="alg0.l16.m1.1.1.3.3.cmml" xref="alg0.l16.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l16.m1.1c">i\in\mathcal{S}_{t}</annotation></semantics></math><span id="alg0.l16.5" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l16.6" class="ltx_text ltx_markedasmath ltx_font_bold" style="font-size:70%;">parallely</span><span id="alg0.l16.7" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l16.8" class="ltx_text ltx_font_bold" style="font-size:70%;">do</span><span id="alg0.l16.9" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg0.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span><span id="alg0.l17.2" class="ltx_text" style="font-size:70%;">       </span><math id="alg0.l17.m1.1" class="ltx_Math" alttext="\mathbf{w}_{i}^{t+1}\leftarrow\textbf{LocalTraining}" display="inline"><semantics id="alg0.l17.m1.1a"><mrow id="alg0.l17.m1.1.1" xref="alg0.l17.m1.1.1.cmml"><msubsup id="alg0.l17.m1.1.1.2" xref="alg0.l17.m1.1.1.2.cmml"><mi mathsize="70%" id="alg0.l17.m1.1.1.2.2.2" xref="alg0.l17.m1.1.1.2.2.2.cmml">𝐰</mi><mi mathsize="70%" id="alg0.l17.m1.1.1.2.2.3" xref="alg0.l17.m1.1.1.2.2.3.cmml">i</mi><mrow id="alg0.l17.m1.1.1.2.3" xref="alg0.l17.m1.1.1.2.3.cmml"><mi mathsize="70%" id="alg0.l17.m1.1.1.2.3.2" xref="alg0.l17.m1.1.1.2.3.2.cmml">t</mi><mo mathsize="70%" id="alg0.l17.m1.1.1.2.3.1" xref="alg0.l17.m1.1.1.2.3.1.cmml">+</mo><mn mathsize="70%" id="alg0.l17.m1.1.1.2.3.3" xref="alg0.l17.m1.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo mathsize="70%" stretchy="false" id="alg0.l17.m1.1.1.1" xref="alg0.l17.m1.1.1.1.cmml">←</mo><mtext class="ltx_mathvariant_bold" mathsize="70%" id="alg0.l17.m1.1.1.3" xref="alg0.l17.m1.1.1.3a.cmml">LocalTraining</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg0.l17.m1.1b"><apply id="alg0.l17.m1.1.1.cmml" xref="alg0.l17.m1.1.1"><ci id="alg0.l17.m1.1.1.1.cmml" xref="alg0.l17.m1.1.1.1">←</ci><apply id="alg0.l17.m1.1.1.2.cmml" xref="alg0.l17.m1.1.1.2"><csymbol cd="ambiguous" id="alg0.l17.m1.1.1.2.1.cmml" xref="alg0.l17.m1.1.1.2">superscript</csymbol><apply id="alg0.l17.m1.1.1.2.2.cmml" xref="alg0.l17.m1.1.1.2"><csymbol cd="ambiguous" id="alg0.l17.m1.1.1.2.2.1.cmml" xref="alg0.l17.m1.1.1.2">subscript</csymbol><ci id="alg0.l17.m1.1.1.2.2.2.cmml" xref="alg0.l17.m1.1.1.2.2.2">𝐰</ci><ci id="alg0.l17.m1.1.1.2.2.3.cmml" xref="alg0.l17.m1.1.1.2.2.3">𝑖</ci></apply><apply id="alg0.l17.m1.1.1.2.3.cmml" xref="alg0.l17.m1.1.1.2.3"><plus id="alg0.l17.m1.1.1.2.3.1.cmml" xref="alg0.l17.m1.1.1.2.3.1"></plus><ci id="alg0.l17.m1.1.1.2.3.2.cmml" xref="alg0.l17.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="alg0.l17.m1.1.1.2.3.3.cmml" xref="alg0.l17.m1.1.1.2.3.3">1</cn></apply></apply><ci id="alg0.l17.m1.1.1.3a.cmml" xref="alg0.l17.m1.1.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="alg0.l17.m1.1.1.3.cmml" xref="alg0.l17.m1.1.1.3">LocalTraining</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l17.m1.1c">\mathbf{w}_{i}^{t+1}\leftarrow\textbf{LocalTraining}</annotation></semantics></math><span id="alg0.l17.3" class="ltx_text" style="font-size:70%;">(</span><math id="alg0.l17.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="alg0.l17.m2.1a"><mi mathsize="70%" id="alg0.l17.m2.1.1" xref="alg0.l17.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg0.l17.m2.1b"><ci id="alg0.l17.m2.1.1.cmml" xref="alg0.l17.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l17.m2.1c">i</annotation></semantics></math><span id="alg0.l17.4" class="ltx_text" style="font-size:70%;">, </span><math id="alg0.l17.m3.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{t}" display="inline"><semantics id="alg0.l17.m3.1a"><msubsup id="alg0.l17.m3.1.1" xref="alg0.l17.m3.1.1.cmml"><mi mathsize="70%" id="alg0.l17.m3.1.1.2.2" xref="alg0.l17.m3.1.1.2.2.cmml">𝐰</mi><mi mathsize="70%" id="alg0.l17.m3.1.1.2.3" xref="alg0.l17.m3.1.1.2.3.cmml">G</mi><mi mathsize="70%" id="alg0.l17.m3.1.1.3" xref="alg0.l17.m3.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="alg0.l17.m3.1b"><apply id="alg0.l17.m3.1.1.cmml" xref="alg0.l17.m3.1.1"><csymbol cd="ambiguous" id="alg0.l17.m3.1.1.1.cmml" xref="alg0.l17.m3.1.1">superscript</csymbol><apply id="alg0.l17.m3.1.1.2.cmml" xref="alg0.l17.m3.1.1"><csymbol cd="ambiguous" id="alg0.l17.m3.1.1.2.1.cmml" xref="alg0.l17.m3.1.1">subscript</csymbol><ci id="alg0.l17.m3.1.1.2.2.cmml" xref="alg0.l17.m3.1.1.2.2">𝐰</ci><ci id="alg0.l17.m3.1.1.2.3.cmml" xref="alg0.l17.m3.1.1.2.3">𝐺</ci></apply><ci id="alg0.l17.m3.1.1.3.cmml" xref="alg0.l17.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l17.m3.1c">\mathbf{w}_{G}^{t}</annotation></semantics></math><span id="alg0.l17.5" class="ltx_text" style="font-size:70%;">)
</span>
</div>
<div id="alg0.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span><span id="alg0.l18.2" class="ltx_text" style="font-size:70%;">    </span><span id="alg0.l18.3" class="ltx_text ltx_font_bold" style="font-size:70%;">end</span><span id="alg0.l18.4" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l18.5" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span>
</div>
<div id="alg0.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span><span id="alg0.l19.2" class="ltx_text" style="font-size:70%;">    </span><math id="alg0.l19.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{t}=\frac{1}{\sum_{i\in\mathcal{N}}D_{i}}\sum_{i=1}^{N}D_{i}\mathbf{w}_{i}^{t}" display="inline"><semantics id="alg0.l19.m1.1a"><mrow id="alg0.l19.m1.1.1" xref="alg0.l19.m1.1.1.cmml"><msubsup id="alg0.l19.m1.1.1.2" xref="alg0.l19.m1.1.1.2.cmml"><mi mathsize="70%" id="alg0.l19.m1.1.1.2.2.2" xref="alg0.l19.m1.1.1.2.2.2.cmml">𝐰</mi><mi mathsize="70%" id="alg0.l19.m1.1.1.2.2.3" xref="alg0.l19.m1.1.1.2.2.3.cmml">G</mi><mi mathsize="70%" id="alg0.l19.m1.1.1.2.3" xref="alg0.l19.m1.1.1.2.3.cmml">t</mi></msubsup><mo mathsize="70%" id="alg0.l19.m1.1.1.1" xref="alg0.l19.m1.1.1.1.cmml">=</mo><mrow id="alg0.l19.m1.1.1.3" xref="alg0.l19.m1.1.1.3.cmml"><mfrac id="alg0.l19.m1.1.1.3.2" xref="alg0.l19.m1.1.1.3.2.cmml"><mn mathsize="70%" id="alg0.l19.m1.1.1.3.2.2" xref="alg0.l19.m1.1.1.3.2.2.cmml">1</mn><mrow id="alg0.l19.m1.1.1.3.2.3" xref="alg0.l19.m1.1.1.3.2.3.cmml"><mstyle displaystyle="false" id="alg0.l19.m1.1.1.3.2.3.1" xref="alg0.l19.m1.1.1.3.2.3.1.cmml"><msub id="alg0.l19.m1.1.1.3.2.3.1a" xref="alg0.l19.m1.1.1.3.2.3.1.cmml"><mo maxsize="49%" minsize="49%" stretchy="true" id="alg0.l19.m1.1.1.3.2.3.1.2" xref="alg0.l19.m1.1.1.3.2.3.1.2.cmml">∑</mo><mrow id="alg0.l19.m1.1.1.3.2.3.1.3" xref="alg0.l19.m1.1.1.3.2.3.1.3.cmml"><mi mathsize="70%" id="alg0.l19.m1.1.1.3.2.3.1.3.2" xref="alg0.l19.m1.1.1.3.2.3.1.3.2.cmml">i</mi><mo mathsize="70%" id="alg0.l19.m1.1.1.3.2.3.1.3.1" xref="alg0.l19.m1.1.1.3.2.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="alg0.l19.m1.1.1.3.2.3.1.3.3" xref="alg0.l19.m1.1.1.3.2.3.1.3.3.cmml">𝒩</mi></mrow></msub></mstyle><msub id="alg0.l19.m1.1.1.3.2.3.2" xref="alg0.l19.m1.1.1.3.2.3.2.cmml"><mi mathsize="70%" id="alg0.l19.m1.1.1.3.2.3.2.2" xref="alg0.l19.m1.1.1.3.2.3.2.2.cmml">D</mi><mi mathsize="70%" id="alg0.l19.m1.1.1.3.2.3.2.3" xref="alg0.l19.m1.1.1.3.2.3.2.3.cmml">i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em" id="alg0.l19.m1.1.1.3.1" xref="alg0.l19.m1.1.1.3.1.cmml">​</mo><mrow id="alg0.l19.m1.1.1.3.3" xref="alg0.l19.m1.1.1.3.3.cmml"><msubsup id="alg0.l19.m1.1.1.3.3.1" xref="alg0.l19.m1.1.1.3.3.1.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="alg0.l19.m1.1.1.3.3.1.2.2" xref="alg0.l19.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="alg0.l19.m1.1.1.3.3.1.2.3" xref="alg0.l19.m1.1.1.3.3.1.2.3.cmml"><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.1.2.3.2" xref="alg0.l19.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo mathsize="70%" id="alg0.l19.m1.1.1.3.3.1.2.3.1" xref="alg0.l19.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn mathsize="70%" id="alg0.l19.m1.1.1.3.3.1.2.3.3" xref="alg0.l19.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.1.3" xref="alg0.l19.m1.1.1.3.3.1.3.cmml">N</mi></msubsup><mrow id="alg0.l19.m1.1.1.3.3.2" xref="alg0.l19.m1.1.1.3.3.2.cmml"><msub id="alg0.l19.m1.1.1.3.3.2.2" xref="alg0.l19.m1.1.1.3.3.2.2.cmml"><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.2.2.2" xref="alg0.l19.m1.1.1.3.3.2.2.2.cmml">D</mi><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.2.2.3" xref="alg0.l19.m1.1.1.3.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="alg0.l19.m1.1.1.3.3.2.1" xref="alg0.l19.m1.1.1.3.3.2.1.cmml">​</mo><msubsup id="alg0.l19.m1.1.1.3.3.2.3" xref="alg0.l19.m1.1.1.3.3.2.3.cmml"><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.2.3.2.2" xref="alg0.l19.m1.1.1.3.3.2.3.2.2.cmml">𝐰</mi><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.2.3.2.3" xref="alg0.l19.m1.1.1.3.3.2.3.2.3.cmml">i</mi><mi mathsize="70%" id="alg0.l19.m1.1.1.3.3.2.3.3" xref="alg0.l19.m1.1.1.3.3.2.3.3.cmml">t</mi></msubsup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l19.m1.1b"><apply id="alg0.l19.m1.1.1.cmml" xref="alg0.l19.m1.1.1"><eq id="alg0.l19.m1.1.1.1.cmml" xref="alg0.l19.m1.1.1.1"></eq><apply id="alg0.l19.m1.1.1.2.cmml" xref="alg0.l19.m1.1.1.2"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.2.1.cmml" xref="alg0.l19.m1.1.1.2">superscript</csymbol><apply id="alg0.l19.m1.1.1.2.2.cmml" xref="alg0.l19.m1.1.1.2"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.2.2.1.cmml" xref="alg0.l19.m1.1.1.2">subscript</csymbol><ci id="alg0.l19.m1.1.1.2.2.2.cmml" xref="alg0.l19.m1.1.1.2.2.2">𝐰</ci><ci id="alg0.l19.m1.1.1.2.2.3.cmml" xref="alg0.l19.m1.1.1.2.2.3">𝐺</ci></apply><ci id="alg0.l19.m1.1.1.2.3.cmml" xref="alg0.l19.m1.1.1.2.3">𝑡</ci></apply><apply id="alg0.l19.m1.1.1.3.cmml" xref="alg0.l19.m1.1.1.3"><times id="alg0.l19.m1.1.1.3.1.cmml" xref="alg0.l19.m1.1.1.3.1"></times><apply id="alg0.l19.m1.1.1.3.2.cmml" xref="alg0.l19.m1.1.1.3.2"><divide id="alg0.l19.m1.1.1.3.2.1.cmml" xref="alg0.l19.m1.1.1.3.2"></divide><cn type="integer" id="alg0.l19.m1.1.1.3.2.2.cmml" xref="alg0.l19.m1.1.1.3.2.2">1</cn><apply id="alg0.l19.m1.1.1.3.2.3.cmml" xref="alg0.l19.m1.1.1.3.2.3"><apply id="alg0.l19.m1.1.1.3.2.3.1.cmml" xref="alg0.l19.m1.1.1.3.2.3.1"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.2.3.1.1.cmml" xref="alg0.l19.m1.1.1.3.2.3.1">subscript</csymbol><sum id="alg0.l19.m1.1.1.3.2.3.1.2.cmml" xref="alg0.l19.m1.1.1.3.2.3.1.2"></sum><apply id="alg0.l19.m1.1.1.3.2.3.1.3.cmml" xref="alg0.l19.m1.1.1.3.2.3.1.3"><in id="alg0.l19.m1.1.1.3.2.3.1.3.1.cmml" xref="alg0.l19.m1.1.1.3.2.3.1.3.1"></in><ci id="alg0.l19.m1.1.1.3.2.3.1.3.2.cmml" xref="alg0.l19.m1.1.1.3.2.3.1.3.2">𝑖</ci><ci id="alg0.l19.m1.1.1.3.2.3.1.3.3.cmml" xref="alg0.l19.m1.1.1.3.2.3.1.3.3">𝒩</ci></apply></apply><apply id="alg0.l19.m1.1.1.3.2.3.2.cmml" xref="alg0.l19.m1.1.1.3.2.3.2"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.2.3.2.1.cmml" xref="alg0.l19.m1.1.1.3.2.3.2">subscript</csymbol><ci id="alg0.l19.m1.1.1.3.2.3.2.2.cmml" xref="alg0.l19.m1.1.1.3.2.3.2.2">𝐷</ci><ci id="alg0.l19.m1.1.1.3.2.3.2.3.cmml" xref="alg0.l19.m1.1.1.3.2.3.2.3">𝑖</ci></apply></apply></apply><apply id="alg0.l19.m1.1.1.3.3.cmml" xref="alg0.l19.m1.1.1.3.3"><apply id="alg0.l19.m1.1.1.3.3.1.cmml" xref="alg0.l19.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.3.1.1.cmml" xref="alg0.l19.m1.1.1.3.3.1">superscript</csymbol><apply id="alg0.l19.m1.1.1.3.3.1.2.cmml" xref="alg0.l19.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.3.1.2.1.cmml" xref="alg0.l19.m1.1.1.3.3.1">subscript</csymbol><sum id="alg0.l19.m1.1.1.3.3.1.2.2.cmml" xref="alg0.l19.m1.1.1.3.3.1.2.2"></sum><apply id="alg0.l19.m1.1.1.3.3.1.2.3.cmml" xref="alg0.l19.m1.1.1.3.3.1.2.3"><eq id="alg0.l19.m1.1.1.3.3.1.2.3.1.cmml" xref="alg0.l19.m1.1.1.3.3.1.2.3.1"></eq><ci id="alg0.l19.m1.1.1.3.3.1.2.3.2.cmml" xref="alg0.l19.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="alg0.l19.m1.1.1.3.3.1.2.3.3.cmml" xref="alg0.l19.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="alg0.l19.m1.1.1.3.3.1.3.cmml" xref="alg0.l19.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="alg0.l19.m1.1.1.3.3.2.cmml" xref="alg0.l19.m1.1.1.3.3.2"><times id="alg0.l19.m1.1.1.3.3.2.1.cmml" xref="alg0.l19.m1.1.1.3.3.2.1"></times><apply id="alg0.l19.m1.1.1.3.3.2.2.cmml" xref="alg0.l19.m1.1.1.3.3.2.2"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.3.2.2.1.cmml" xref="alg0.l19.m1.1.1.3.3.2.2">subscript</csymbol><ci id="alg0.l19.m1.1.1.3.3.2.2.2.cmml" xref="alg0.l19.m1.1.1.3.3.2.2.2">𝐷</ci><ci id="alg0.l19.m1.1.1.3.3.2.2.3.cmml" xref="alg0.l19.m1.1.1.3.3.2.2.3">𝑖</ci></apply><apply id="alg0.l19.m1.1.1.3.3.2.3.cmml" xref="alg0.l19.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.3.2.3.1.cmml" xref="alg0.l19.m1.1.1.3.3.2.3">superscript</csymbol><apply id="alg0.l19.m1.1.1.3.3.2.3.2.cmml" xref="alg0.l19.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="alg0.l19.m1.1.1.3.3.2.3.2.1.cmml" xref="alg0.l19.m1.1.1.3.3.2.3">subscript</csymbol><ci id="alg0.l19.m1.1.1.3.3.2.3.2.2.cmml" xref="alg0.l19.m1.1.1.3.3.2.3.2.2">𝐰</ci><ci id="alg0.l19.m1.1.1.3.3.2.3.2.3.cmml" xref="alg0.l19.m1.1.1.3.3.2.3.2.3">𝑖</ci></apply><ci id="alg0.l19.m1.1.1.3.3.2.3.3.cmml" xref="alg0.l19.m1.1.1.3.3.2.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l19.m1.1c">\mathbf{w}_{G}^{t}=\frac{1}{\sum_{i\in\mathcal{N}}D_{i}}\sum_{i=1}^{N}D_{i}\mathbf{w}_{i}^{t}</annotation></semantics></math><span id="alg0.l19.3" class="ltx_text" style="font-size:70%;">   (Averaging aggregation)
</span>
</div>
<div id="alg0.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg0.l20.1.1.1" class="ltx_text" style="font-size:80%;">20:</span></span><span id="alg0.l20.2" class="ltx_text ltx_font_bold" style="font-size:70%;">end</span><span id="alg0.l20.3" class="ltx_text" style="font-size:70%;"> </span><span id="alg0.l20.4" class="ltx_text ltx_font_bold" style="font-size:70%;">for</span>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.7.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated averaging algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></figcaption>
</figure>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.2" class="ltx_p">As described in Step 1 above, the server first initializes the task (lines 11-16). Thereafter, in Step 2, the participant <math id="S2.SS2.p8.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.p8.1.m1.1a"><mi id="S2.SS2.p8.1.m1.1.1" xref="S2.SS2.p8.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.1.m1.1b"><ci id="S2.SS2.p8.1.m1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.1.m1.1c">i</annotation></semantics></math> implements the local
training and optimizes the target in (<a href="#S2.E3" title="In 2nd item ‣ II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)
on minibatches from the original local dataset (lines 2-8). Note that a minibatch refers to a randomized subset of each participant’s dataset. At the <math id="S2.SS2.p8.2.m2.1" class="ltx_Math" alttext="t^{th}" display="inline"><semantics id="S2.SS2.p8.2.m2.1a"><msup id="S2.SS2.p8.2.m2.1.1" xref="S2.SS2.p8.2.m2.1.1.cmml"><mi id="S2.SS2.p8.2.m2.1.1.2" xref="S2.SS2.p8.2.m2.1.1.2.cmml">t</mi><mrow id="S2.SS2.p8.2.m2.1.1.3" xref="S2.SS2.p8.2.m2.1.1.3.cmml"><mi id="S2.SS2.p8.2.m2.1.1.3.2" xref="S2.SS2.p8.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p8.2.m2.1.1.3.1" xref="S2.SS2.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p8.2.m2.1.1.3.3" xref="S2.SS2.p8.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.2.m2.1b"><apply id="S2.SS2.p8.2.m2.1.1.cmml" xref="S2.SS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p8.2.m2.1.1.1.cmml" xref="S2.SS2.p8.2.m2.1.1">superscript</csymbol><ci id="S2.SS2.p8.2.m2.1.1.2.cmml" xref="S2.SS2.p8.2.m2.1.1.2">𝑡</ci><apply id="S2.SS2.p8.2.m2.1.1.3.cmml" xref="S2.SS2.p8.2.m2.1.1.3"><times id="S2.SS2.p8.2.m2.1.1.3.1.cmml" xref="S2.SS2.p8.2.m2.1.1.3.1"></times><ci id="S2.SS2.p8.2.m2.1.1.3.2.cmml" xref="S2.SS2.p8.2.m2.1.1.3.2">𝑡</ci><ci id="S2.SS2.p8.2.m2.1.1.3.3.cmml" xref="S2.SS2.p8.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.2.m2.1c">t^{th}</annotation></semantics></math>
iteration (line 17), the server minimizes the global loss in (<a href="#S2.E4" title="In II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)
by the averaging aggregation which is formally defined as</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.1" class="ltx_Math" alttext="\mathbf{w}_{G}^{t}=\frac{1}{\sum_{i\in\mathcal{N}}D_{i}}\sum_{i=1}^{N}D_{i}\mathbf{w}_{i}^{t}." display="block"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><msubsup id="S2.E5.m1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.2.cmml"><mi id="S2.E5.m1.1.1.1.1.2.2.2" xref="S2.E5.m1.1.1.1.1.2.2.2.cmml">𝐰</mi><mi id="S2.E5.m1.1.1.1.1.2.2.3" xref="S2.E5.m1.1.1.1.1.2.2.3.cmml">G</mi><mi id="S2.E5.m1.1.1.1.1.2.3" xref="S2.E5.m1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S2.E5.m1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E5.m1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.3.cmml"><mfrac id="S2.E5.m1.1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.3.2.cmml"><mn id="S2.E5.m1.1.1.1.1.3.2.2" xref="S2.E5.m1.1.1.1.1.3.2.2.cmml">1</mn><mrow id="S2.E5.m1.1.1.1.1.3.2.3" xref="S2.E5.m1.1.1.1.1.3.2.3.cmml"><msub id="S2.E5.m1.1.1.1.1.3.2.3.1" xref="S2.E5.m1.1.1.1.1.3.2.3.1.cmml"><mo id="S2.E5.m1.1.1.1.1.3.2.3.1.2" xref="S2.E5.m1.1.1.1.1.3.2.3.1.2.cmml">∑</mo><mrow id="S2.E5.m1.1.1.1.1.3.2.3.1.3" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.3.2.3.1.3.2" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.2.cmml">i</mi><mo id="S2.E5.m1.1.1.1.1.3.2.3.1.3.1" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.1.1.1.1.3.2.3.1.3.3" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.3.cmml">𝒩</mi></mrow></msub><msub id="S2.E5.m1.1.1.1.1.3.2.3.2" xref="S2.E5.m1.1.1.1.1.3.2.3.2.cmml"><mi id="S2.E5.m1.1.1.1.1.3.2.3.2.2" xref="S2.E5.m1.1.1.1.1.3.2.3.2.2.cmml">D</mi><mi id="S2.E5.m1.1.1.1.1.3.2.3.2.3" xref="S2.E5.m1.1.1.1.1.3.2.3.2.3.cmml">i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.3.1" xref="S2.E5.m1.1.1.1.1.3.1.cmml">​</mo><mrow id="S2.E5.m1.1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.3.3.cmml"><munderover id="S2.E5.m1.1.1.1.1.3.3.1" xref="S2.E5.m1.1.1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S2.E5.m1.1.1.1.1.3.3.1.2.2" xref="S2.E5.m1.1.1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E5.m1.1.1.1.1.3.3.1.2.3" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.cmml"><mi id="S2.E5.m1.1.1.1.1.3.3.1.2.3.2" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S2.E5.m1.1.1.1.1.3.3.1.2.3.1" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S2.E5.m1.1.1.1.1.3.3.1.2.3.3" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E5.m1.1.1.1.1.3.3.1.3" xref="S2.E5.m1.1.1.1.1.3.3.1.3.cmml">N</mi></munderover><mrow id="S2.E5.m1.1.1.1.1.3.3.2" xref="S2.E5.m1.1.1.1.1.3.3.2.cmml"><msub id="S2.E5.m1.1.1.1.1.3.3.2.2" xref="S2.E5.m1.1.1.1.1.3.3.2.2.cmml"><mi id="S2.E5.m1.1.1.1.1.3.3.2.2.2" xref="S2.E5.m1.1.1.1.1.3.3.2.2.2.cmml">D</mi><mi id="S2.E5.m1.1.1.1.1.3.3.2.2.3" xref="S2.E5.m1.1.1.1.1.3.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.3.3.2.1" xref="S2.E5.m1.1.1.1.1.3.3.2.1.cmml">​</mo><msubsup id="S2.E5.m1.1.1.1.1.3.3.2.3" xref="S2.E5.m1.1.1.1.1.3.3.2.3.cmml"><mi id="S2.E5.m1.1.1.1.1.3.3.2.3.2.2" xref="S2.E5.m1.1.1.1.1.3.3.2.3.2.2.cmml">𝐰</mi><mi id="S2.E5.m1.1.1.1.1.3.3.2.3.2.3" xref="S2.E5.m1.1.1.1.1.3.3.2.3.2.3.cmml">i</mi><mi id="S2.E5.m1.1.1.1.1.3.3.2.3.3" xref="S2.E5.m1.1.1.1.1.3.3.2.3.3.cmml">t</mi></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><eq id="S2.E5.m1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1"></eq><apply id="S2.E5.m1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.1.1.1.1.2">superscript</csymbol><apply id="S2.E5.m1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.2.1.cmml" xref="S2.E5.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.2.2.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.2">𝐰</ci><ci id="S2.E5.m1.1.1.1.1.2.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.3">𝐺</ci></apply><ci id="S2.E5.m1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S2.E5.m1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3"><times id="S2.E5.m1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.1"></times><apply id="S2.E5.m1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2"><divide id="S2.E5.m1.1.1.1.1.3.2.1.cmml" xref="S2.E5.m1.1.1.1.1.3.2"></divide><cn type="integer" id="S2.E5.m1.1.1.1.1.3.2.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2.2">1</cn><apply id="S2.E5.m1.1.1.1.1.3.2.3.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3"><apply id="S2.E5.m1.1.1.1.1.3.2.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.2.3.1.1.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1">subscript</csymbol><sum id="S2.E5.m1.1.1.1.1.3.2.3.1.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1.2"></sum><apply id="S2.E5.m1.1.1.1.1.3.2.3.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3"><in id="S2.E5.m1.1.1.1.1.3.2.3.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.1"></in><ci id="S2.E5.m1.1.1.1.1.3.2.3.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.2">𝑖</ci><ci id="S2.E5.m1.1.1.1.1.3.2.3.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.1.3.3">𝒩</ci></apply></apply><apply id="S2.E5.m1.1.1.1.1.3.2.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.2.3.2.1.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.2">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.3.2.3.2.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.2.2">𝐷</ci><ci id="S2.E5.m1.1.1.1.1.3.2.3.2.3.cmml" xref="S2.E5.m1.1.1.1.1.3.2.3.2.3">𝑖</ci></apply></apply></apply><apply id="S2.E5.m1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3"><apply id="S2.E5.m1.1.1.1.1.3.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.3.1.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1">superscript</csymbol><apply id="S2.E5.m1.1.1.1.1.3.3.1.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.3.1.2.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1">subscript</csymbol><sum id="S2.E5.m1.1.1.1.1.3.3.1.2.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1.2.2"></sum><apply id="S2.E5.m1.1.1.1.1.3.3.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3"><eq id="S2.E5.m1.1.1.1.1.3.3.1.2.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.1"></eq><ci id="S2.E5.m1.1.1.1.1.3.3.1.2.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S2.E5.m1.1.1.1.1.3.3.1.2.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E5.m1.1.1.1.1.3.3.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S2.E5.m1.1.1.1.1.3.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2"><times id="S2.E5.m1.1.1.1.1.3.3.2.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.1"></times><apply id="S2.E5.m1.1.1.1.1.3.3.2.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.2">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.2.2">𝐷</ci><ci id="S2.E5.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.2.3">𝑖</ci></apply><apply id="S2.E5.m1.1.1.1.1.3.3.2.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.3.2.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3">superscript</csymbol><apply id="S2.E5.m1.1.1.1.1.3.3.2.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.3.2.3.2.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.3.3.2.3.2.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3.2.2">𝐰</ci><ci id="S2.E5.m1.1.1.1.1.3.3.2.3.2.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3.2.3">𝑖</ci></apply><ci id="S2.E5.m1.1.1.1.1.3.3.2.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.2.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">\mathbf{w}_{G}^{t}=\frac{1}{\sum_{i\in\mathcal{N}}D_{i}}\sum_{i=1}^{N}D_{i}\mathbf{w}_{i}^{t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p8.3" class="ltx_p">The FL training process is iterated till the global loss function converges, or a desirable accuracy is achieved.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Statistical Challenges of FL</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Following an elaboration of the FL training process in the previous section, we now proceed to discuss the statistical challenges faced in FL.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In traditional distributed ML, the central server has access to
the whole training dataset. As such, the server can split the dataset
into subsets that follow similar distributions. The subsets are subsequently sent to
participating nodes for distributed training. However, this approach is
impractical for FL since the local dataset is only accessible by the data
owner.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In the FL setting, the participants may have local datasets that follow different distributions, i.e., the datasets of participants are non-IID. While the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> show that the aforementioned <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm is able to achieve desirable accuracy even when data is non-IID across participants, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> found otherwise. For example, the accuracy of a <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">FedAvg</span>-trained CNN model has 51% lower accuracy than centrally-trained CNN model for CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. This deterioration in accuracy is further shown to be quantified by the earth mover’s distance (EMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, i.e., difference in FL participant’s data distribution as compared to the population distribution. As such, when data is non-IID and highly skewed, data-sharing is proposed in which a shared dataset with uniform distribution across all classes is sent by the FL server to each FL participant. Then, the participant trains its local model on its private data together with the received data. The simulation result shows that accuracy can be increased by 30% with 5% shared data due to reduced EMD. However, a common dataset may not always be available for sharing by the FL server. An alternative solution to gather contributions towards building the common dataset is subsequently discussed in section <a href="#S4" title="IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> also find that global imbalance, i.e., the situation in which the collection of data held across all FL participants is class imbalanced, leads to a deterioration in model accuracy. As such, the Astraea framework is proposed. On initialization, the FL participants first send their data distribution to the FL server. A rebalancing step is introduced before training begins in which each participant performs data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> on the minority classes, e.g., through random rotations and shifts. After training on the augmented data, a mediator is created to coordinate intermediate aggregation, i.e., before sending the updated parameters to the FL server for global aggregation. The mediator selects participants with data distributions that best contributes to an uniform distribution when aggregated. This is done through a greedy algorithm approach to minimize the Kullback-Leibler Divergence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> between local data and uniform distribution. The simulation results show accuracy improvement when tested on imbalanced datasets.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Given the heterogeneity of data distribution across devices, there has been an increasing number of studies that borrow concepts from multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> to learn separate, but structurally related models for each participant. Instead of minimizing the conventional loss function presented previously in Table <a href="#S2.T3" title="TABLE III ‣ II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the loss function is modified to also model the relationship amongst tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. Then, the MOCHA algorithm is proposed in which an alternating optimization approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> is used to approximately solve the minimization problem. Interestingly, MOCHA can also be calibrated based on the resource constraints of a participating device. For example, the quality of approximation can be adaptively adjusted based on network conditions and CPU states of the participating devices. However, MOCHA cannot be applied to non-convex DL models.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">Similarly, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> also borrow concepts from multi-task learning to deal with the statistical heterogeneity in FL. The FEDPER approach is proposed in which all FL participants share a set of base layers trained using the <span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm. Then, each participant separately trains another set of personalization layers using its own local data. In particular, this approach is suitable for building recommender’s systems given the diverse preferences of participants. The authors show empirically using the Flickr-AES dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> that the FEDPER approach outperforms a pure <span id="S2.SS3.p6.1.2" class="ltx_text ltx_font_italic">FedAvg</span> approach since the personalization layer is able to represent the personal preference of an FL participant. However, it is worth to note that the collaborative training of the base layers are still important to achieve a high test accuracy, since each participant has insufficient local data samples for purely personalized model training.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p">Apart from data heterogeneity, the convergence of a distributed learning algorithm is always
a concern. Higher convergence rate helps to save a large amount
of time and resources for the FL participants, and also significantly increases
the success rate of the federated training since fewer communication
rounds imply reduced participant dropouts. To ensure convergence, the study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> propose <span id="S2.SS3.p7.1.1" class="ltx_text ltx_font_italic">FedProx</span>, which modifies the loss function to also include a tunable parameter that restricts how much local updates can affect the prevailing model parameters. The <span id="S2.SS3.p7.1.2" class="ltx_text ltx_font_italic">FedProx</span> algorithm can be adaptively tuned, e.g., when training loss is increasing, model updates can be tuned to affect the current parameters less. Similarly, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> also propose the <span id="S2.SS3.p7.1.3" class="ltx_text ltx_font_italic">LoAdaBoost FedAvg</span> algorithm to complement the aforementioned data-sharing approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> in ML on medical data. In <span id="S2.SS3.p7.1.4" class="ltx_text ltx_font_italic">LoAdaBoost FedAvg</span>, participants train the model on their local data and compare the cross-entropy loss with the median loss from the <span id="S2.SS3.p7.1.5" class="ltx_text ltx_font_italic">previous</span> training round. If the current cross-entropy loss is higher, the model is retrained before global aggregation so as to increase learning efficiency. The simulation results show that faster convergence is achieved as a result.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p">In fact, the statistical challenges of FL coexist with other issues that we explore in subsequent sections. For example, the communication costs incurred in FL can be reduced by faster convergence. Similarly, resource allocation policies can also be designed to solve statistical heterogeneity. As such, we revisit these concepts in greater detail subsequently.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">FL protocols and frameworks</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">To improve scalability, an FL protocol
has been proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> from the system level. This protocol deals with
issues regarding unstable device connectivity and communication security etc.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The FL protocol (Fig. <a href="#S2.F5" title="Figure 5 ‣ II-D FL protocols and frameworks ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) consists of three phases in each training round:</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><em id="S2.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Selection:</em> In the participant selection phase, the FL server chooses a subset of connected devices to participate in a training round. The selection criteria may subsequently be calibrated to the server’s needs, e.g., training efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. In Section <a href="#S4" title="IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we further elaborate on proposed participant selection methods.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><em id="S2.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Configuration:</em> The server is configured accordingly to the aggregation mechanism preferred, e.g. simple or secure aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. Then, the server sends the training schedule and global model to each participant.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><em id="S2.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic">Reporting:</em> The server receives updates from participants. Thereafter, the updates can be aggregated, e.g., using the <span id="S2.I2.i3.p1.1.2" class="ltx_text ltx_font_italic">FedAvg</span> algorithm.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">In addition, to manage device connections accordingly to varying FL population size, pace steering is also recommended. Pace steering adaptively manages the optimal time window for participants to reconnect to the FL server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. For example, when the FL population is small, pace steering is used to ensure that there is a sufficient number of participating devices that connect to the server simultaneously. In contrast, when there is a large population, pace steering randomly chooses devices to participate to prevent the situation in which too many participating devices are connected at one point of time.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">Apart from communication efficiency, communication security during local updates transmission is
another problem to be resolved. Specifically,
there are mainly two aspects in communication security:</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<ol id="S2.I3" class="ltx_enumerate">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><em id="S2.I3.i1.p1.1.1" class="ltx_emph ltx_font_italic">Secure aggregation</em>: To prevent local updates from being traced and utilized
to infer the identity of the FL participant,
a virtual and trusted third party server is deployed for local model
aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. The Secret Sharing mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> is also used for
transmission of local updates with authenticated encryption.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><em id="S2.I3.i2.p1.1.1" class="ltx_emph ltx_font_italic">Differential privacy</em>: Similar to secure aggregation, differential
privacy (DP) prevents the FL server from identifying the owner of a local
update. The difference is that to achieve the goal of privacy preservation,
the DP in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> adds a certain degree of noise in
the original local update while providing theoretical guarantees on
the model quality.</p>
</div>
</li>
</ol>
<p id="S2.SS4.p6.1" class="ltx_p">These concepts on privacy and security are presented in detail in Section <a href="#S5" title="V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Recently, some open-source frameworks for FL have been developed as follows:</p>
</div>
<div id="S2.SS4.p7" class="ltx_para">
<ol id="S2.I4" class="ltx_enumerate">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p"><em id="S2.I4.i1.p1.1.1" class="ltx_emph ltx_font_italic">TensorFlow Federated (TFF)</em>: TFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> is a framework based on Tensorflow developed by Google for decentralized ML and other distributed
computations. TFF consists of two layers (i) FL and (ii) Federated Core (FC). The FL layer is a high-level interface that allows the implementation of FL to existing TF models without the user having to apply the FL algorithms personally. The FC layer combines TF with communication operators to allow users to experiment with customized and newly designed FL algorithms.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p"><em id="S2.I4.i2.p1.1.1" class="ltx_emph ltx_font_italic">PySyft</em>: PySyft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> is a framework based on PyTorch
for performing encrypted, privacy-preserving DL and implementations of
related techniques, such as Secure Multiparty Computation (SMPC) and
DP, in untrusted environments while protecting data. Pysyft is developed such that it retains the native Torch interface, i.e., the ways to execute all tensor operations remain unchanged from that of Pytorch. When a SyftTensor is created, a LocalTensor is automatically created to also apply the input command to the native Pytorch tensor. To simulate FL, participants are created as <span id="S2.I4.i2.p1.1.2" class="ltx_text ltx_font_italic">Virtual Workers</span>. Data, i.e., in the structure of tensors, can be split and distributed to the Virtual Workers as a simulation of a practical FL setting. Then, a PointerTensor is created to specify the data owner and storage location. In addition, model updates can be fetched from the Virtual Workers for global aggregation.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p id="S2.I4.i3.p1.1" class="ltx_p"><em id="S2.I4.i3.p1.1.1" class="ltx_emph ltx_font_italic">LEAF</em>: An open source framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> of datasets that can be used as benchmarks in FL, e.g., Federated Extended MNIST (FEMNIST), an MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> dataset partitioned based on writer of each character, and Sentiment140 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, a dataset partitioned based on different users. In these datasets, the writer or user is assumed to be a participant in FL, and their corresponding data is taken to be the local data held in their personal devices. The implementation of newly designed algorithms on these benchmark datasets allow for reliable comparison across studies.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p id="S2.I4.i4.p1.1" class="ltx_p"><em id="S2.I4.i4.p1.1.1" class="ltx_emph ltx_font_italic">FATE</em>: Federated AI Technology Enabler (FATE) is an open-source framework by WeBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> that supports the federated and secure implementation of several ML models.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/1909.11875/assets/fl-system.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="271" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.4.2" class="ltx_text" style="font-size:90%;">Federated learning protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.</span></figcaption>
</figure>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.4.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.5.2" class="ltx_text ltx_font_italic">Unique characteristics and issues of FL</span>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Besides the statistical challenges we present in Section <a href="#S2.SS3" title="II-C Statistical Challenges of FL ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>, FL has some unique characteristics and features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> as compared to other distributed ML approaches:</p>
<ol id="S2.I5" class="ltx_enumerate">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p id="S2.I5.i1.p1.1" class="ltx_p"><span id="S2.I5.i1.p1.1.1" class="ltx_text ltx_font_italic">Slow and unstable communication</span>: In the traditional distributed training
in a data center, the communication environment can be assumed to
be perfect where the information transmission rate is very high and
there is no packet loss. However, these assumptions are not applicable to
the FL environment where heterogeneous devices are involved in training. For example, the Internet upload speed is typically
much slower than download speed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Also, some participants with unstable wireless communication channels may consequently
drop out due to disconnection from the Internet.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p id="S2.I5.i2.p1.1" class="ltx_p"><span id="S2.I5.i2.p1.1.1" class="ltx_text ltx_font_italic">Heterogeneous devices</span>: Apart from bandwidth constraints, FL involves heterogeneous devices with varying resource constraints. For example, the devices can have different computing capabilities, i.e., CPU states and battery level. The devices can also have different levels of <span id="S2.I5.i2.p1.1.2" class="ltx_text ltx_font_italic">willingness</span> to participate, i.e., FL training is resource consuming and given the distributed nature of training across numerous devices, there is a possibility of free ridership.</p>
</div>
</li>
<li id="S2.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I5.i3.p1" class="ltx_para">
<p id="S2.I5.i3.p1.1" class="ltx_p"><span id="S2.I5.i3.p1.1.1" class="ltx_text ltx_font_italic">Privacy and security concerns</span>:
As we have previously discussed, data owners are increasingly privacy sensitive. However, as will be subsequently presented in Section <a href="#S5" title="V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, malicious participants are able to infer sensitive information from shared parameters, which potentially negates privacy preservation. In addition, we have previously assumed that all participants and FL servers are trustful. In reality, they may be malicious.</p>
</div>
</li>
</ol>
<p id="S2.SS5.p1.2" class="ltx_p">These unique characteristics of FL lead to several practical issues in FL implementation
mainly in three aspects that we now proceed to discuss, i.e., (i) statistical challenges (ii) communication costs (iii) resource allocation and (iv) privacy and security. In the following sections, we review related work that address each of these issues.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Communication Cost</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In FL, a number of rounds of communications between the participants and the FL server may be required to achieve a target accuracy (Fig. <a href="#S2.F5" title="Figure 5 ‣ II-D FL protocols and frameworks ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). For complex DL model training involving, e.g. CNN, each update may comprise millions of parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. The high dimensionality of the updates can result in the incurrence of high communication costs and can lead to a training bottleneck. In addition, the bottleneck can be worsened due to (i) unreliable network conditions of participating devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> and (ii) the asymmetry in Internet connection speeds in which upload speed is faster than download speed, resulting in delays in model uploads by participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. As such, there is a need to improve the communication efficiency of FL. The following approaches to reduce communication costs are considered:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Edge and End Computation</span>: In the FL setup, the communication cost often dominates computation cost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The reason is that on-device dataset is relatively small whereas mobile devices of participants have increasingly fast processors. On the other hand, participants may only be willing to participate in the model training only if they are connected to Wi-Fi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. As such, more computation can be performed on edge nodes or on end devices before each global aggregation so as to reduce the number of communication rounds needed for the model training. In addition, algorithms to ensure faster convergence can also reduce number of communication rounds involved, at the expense of more computation on edge servers and end devices.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Model Compression</span>: This is a technique commonly used in distributed learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. Model or gradient compression involves the communication of an update that is transformed to be more compact, e.g., through sparsification, quantization or subsampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, rather than the communication of full update. However, since the compression may introduce noise, the objective is to reduce the size of update transferred during each round of communication while maintaining the quality of trained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Importance-based Updating</span>: This strategy involves selective communication such that only the important or relevant updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> are transmitted in each communication round. In fact, besides saving on communication costs, omitting some updates from participants can even improve the global model performance.</p>
</div>
</li>
</ul>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Edge and End Computation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">To decrease the number of communication rounds, additional computation can be performed on participating end devices before each iteration of communication for global aggregation (Fig. <a href="#S3.F6" title="Figure 6 ‣ III-A Edge and End Computation ‣ III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(a)). The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> consider two ways to increase computation on participating devices: (i) increasing parallelism in which more participants are selected to participate in each round of training and (ii) increasing computation per participant whereby each participant performs more local updates before communication for global aggregation. A comparison is conducted for the FederatedSGD (<span id="S3.SS1.p1.3.1" class="ltx_text ltx_font_italic">FedSGD</span>) algorithm and the proposed <span id="S3.SS1.p1.3.2" class="ltx_text ltx_font_italic">FedAvg</span> algorithm. For the <span id="S3.SS1.p1.3.3" class="ltx_text ltx_font_italic">FedSGD</span> algorithm, all participants are involved and only one pass is made per training round in which the minibatch size comprises of the entirety of the participant’s dataset. This is similar to the full-batch training in centralized DL frameworks. For the proposed <span id="S3.SS1.p1.3.4" class="ltx_text ltx_font_italic">FedAvg</span> algorithm, the hyperparameters are tuned such that more local computations are performed by the participants. For example, the participant can make more passes over its dataset or use a smaller local minibatch size to increase computation before each communication round. The simulation results show that increased parallelism does not lead to significant improvements in reducing communication cost, once a certain threshold is reached. As such, more emphasis is placed on increasing computation per participant while keeping the fraction of selected participants constant. For MNIST CNN simulations, increased computation using the proposed <span id="S3.SS1.p1.3.5" class="ltx_text ltx_font_italic">FedAvg</span> algorithm can reduce communication rounds by more than <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">30</annotation></semantics></math> times when the dataset is IID. For non-IID dataset, the improvement is less significant (<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="2.8" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">2.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="float" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">2.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">2.8</annotation></semantics></math> times) using the same hyperparameters. However, for Long Short Term Memory (LSTM) simulations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>, improvements are more significant even for non-IID data (<math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="95.3" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">95.3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="float" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">95.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">95.3</annotation></semantics></math> times). In addition, <span id="S3.SS1.p1.3.6" class="ltx_text ltx_font_italic">FedAvg</span> increases the accuracy eventually since model averaging produces regularization effects similar to dropout <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>, which prevents overfitting.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">As an extension, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> also validate that a similar concept as that of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> works for vertical FL. In vertical FL, collaborative model training is conducted across the same set of participants with different data features. The Federated Stochastic Block Coordinate Descent (FedBCD) algorithm is proposed in which each participating device performs multiple local updates first before communication for global aggregation. In addition, convergence guarantee is also provided with an approximate calibration of the number of local updates per interval of communication.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/edgeendcompute.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Approaches to increase computation at edge and end devices include (a) Increased computation at end devices, e.g., more passes over dataset before communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> (b) Two-stream training with global model as a reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> and (c) Intermediate edge server aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Another way to decrease communication cost can also be through modifying the training algorithm to increase convergence speed, e.g., through the aforementioned <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">LoAdaBoost FedAvg</span> in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. Similarly, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> also propose increased computation on each participating device by adopting a two-stream model (Fig. <a href="#S3.F6" title="Figure 6 ‣ III-A Edge and End Computation ‣ III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(b)) commonly used in transfer learning and domain adaption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>. During each training round, the global model is received by the participant and fixed as a reference in the training process. During training, the participant learns not just from local data, but also from other participants with reference to the fixed global model. This is done through the incorporation of Maximum Mean Discrepancy (MMD) into the loss function. MMD measures the distance between the means of two data distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>. Through minimizing MMD loss between the local and global models, the participant can extract more generalized features from the global model, thus accelerating the convergence of training process and reducing communication rounds. The simulation results on the CIFAR-10 and MNIST dataset using DL models such as AlexNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and 2-CNN respectively show that the proposed two-stream FL can reach the desirable test accuracy in 20% fewer communication rounds even when data is non-IID. However, while convergence speed is increased, more computation resources have to be consumed by end devices for the aforementioned approaches. Given the energy constraints of participating mobile devices in particular, this necessitates resource allocation optimization that we subsequently discuss in Section <a href="#S4" title="IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">While the aforementioned studies consider increasing computation on participating <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">devices</span>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> propose an edge computing inspired paradigm in which proximate edge <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_italic">servers</span> can serve as intermediary parameter aggregators given that the propagation latency from participant to the edge server is smaller than that of the participant-cloud communication (Fig. <a href="#S3.F6" title="Figure 6 ‣ III-A Edge and End Computation ‣ III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(c)). A hierarchical FL (<span id="S3.SS1.p4.1.3" class="ltx_text ltx_font_italic">HierFAVG</span>) algorithm is proposed whereby for every few local participant updates, the edge server aggregates the collected local models. After a predefined number of edge server aggregations, the edge server communicates with the cloud for global model aggregation. As such, the communication between the participants and the cloud occurs only once after an interval of multiple local updates. Comparatively, for the <span id="S3.SS1.p4.1.4" class="ltx_text ltx_font_italic">FedAvg</span> algorithm proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the global aggregation occurs more frequently since no intermediate edge server aggregation is involved. The authors further prove the covergence of <span id="S3.SS1.p4.1.5" class="ltx_text ltx_font_italic">HierFAVG</span> for both convex and non-convex objective functions given non-IID user data. The simulation results show that for the same number of local updates between two global aggregations, more intermediate edge aggregations before each global aggregation can lead to reduced communication overhead as compared to the <span id="S3.SS1.p4.1.6" class="ltx_text ltx_font_italic">FedAvg</span> algorithm. This result holds for both IID and non-IID data, implying that intermediate aggregation on edge servers may be implemented on top of <span id="S3.SS1.p4.1.7" class="ltx_text ltx_font_italic">FedAvg</span> so as to reduce communication costs. However, when applied to non-IID data, the simulation results show that <span id="S3.SS1.p4.1.8" class="ltx_text ltx_font_italic">HierFAVG</span> fails to converge to the desired accuracy level (90%) in some instances, e.g., when edge-cloud divergence is large or when there are many edge servers involved. As such, a further study is required to better understand the tradeoffs between adjusting local and edge aggregation intervals, so as to ensure that the parameters of the <span id="S3.SS1.p4.1.9" class="ltx_text ltx_font_italic">HierFAVG</span> algorithm can be optimally calibrated to suit other settings. Nevertheless, <span id="S3.SS1.p4.1.10" class="ltx_text ltx_font_italic">HierFAVG</span> is a promising approach for the implementation of FL at mobile edge networks, since it leverages on the proximity of intermediate edge server to reduce communication costs, and potentially relieve the burden on the remote cloud.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Model Compression</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To reduce communication costs, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> propose structured and sketched updates to reduce the size of model updates sent from participants to the FL server during each communication round.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">Structured updates</span> restrict participant updates to have a pre-specified structure, i.e., low rank and random mask. For the low rank structure, each update is enforced to be a low rank matrix expressed as a product of two matrices. Here, one matrix is generated randomly and held constant during each communication round whereas the other is optimized. As such, only the optimized matrix needs to be sent to the server. For the random mask structure, each participant update is restricted to be a sparse matrix following a pre-defined random sparsity pattern generated independently during each round. As such, only the non-zero entries are required to be sent to the server.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">On the other hand, <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">sketched updates</span> refer to the approach of encoding the update in a compressed form before communication with the server, which subsequently decodes the updates before aggregation. One example of sketched update is the subsampling approach, in which each participant communicates only a random subset of the update matrix. The server then averages the subsampled updates to derive an unbiased estimate of the true average. Another example of sketched update is the probabilistic quantization approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>, in which the update matrices are vectorized and quantized for each scalar. To reduce the error from quantization, a structured random rotation that is the product of a Walsh-Hadamard matrix and
binary diagonal matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> can be applied before quantization.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The simulation results on the CIFAR-10 image classification task show that for structured updates, the random mask performs better than that of the low rank approach. The random mask approach also achieves higher accuracy than sketching approaches since the latter involves a removal of some information obtained during training. However, the combination of all three sketching tools, i.e., subsampling, quantization, and rotation, can achieve higher compression rate and faster convergence, albeit with some sacrifices in accuracy. For example, by using 2 bits for quantization and sketching out all but 6.25% of update data, the number of bits needed to represent updates can be reduced by 256 times and the accuracy level achieved is 85%. In addition, sketching updates can achieve higher accuracy in training when there are more participants trained per round. This suggests that for practical implementation of FL where there are many participants available, more participants can be selected for training per round so that subsampling can be more aggressive to reduce communication costs.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/1909.11875/assets/x1.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.2" class="ltx_text" style="font-size:90%;">The compression techniques considered are summarized above by the diagram from authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. (i) Federated dropout to reduce size of model (ii) Lossy compression of model (iii) Decompression for training (iv) Compression of participant updates (v) Decompression (vi) Global aggregation</span></figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> extend the studies in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> by proposing lossy compression and federated dropout to reduce <span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_italic">server-to-participant</span> communication costs. A summary of the proposed techniques are adapted from the authors’ work in Fig. <a href="#S3.F7" title="Figure 7 ‣ III-B Model Compression ‣ III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. For participant-to-server upload of model parameters that we discuss previously, the decompressions can be averaged over many updates to receive an unbiased estimate. However, there is no averaging for server-to-participant communications since the same global model is sent to all participants during each round of communication. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, subsampling and probabilistic quantization are considered. For the application of structured random rotation before subsampling and quantization, Kashin’s representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> is applied instead of the Hadamard transformation since the former is found to perform better in terms of accuracy-size tradeoff.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">In addition to the subsampling and quantization approaches, the federated dropout approach is also considered in which a fixed number of activation functions at each fully-connected layer is removed to derive a smaller sub-model. The sub-model is then sent to the participants for training. The updated submodel can then be mapped back to the global model to derive a complete DNN model with all weights updated during subsequent aggregation. This approach reduces the server-to-participant communication cost, and also the size of participant-to-server updates. In addition, local computation is reduced since fewer parameters have to be updated. The simulations are performed on MNIST, CIFAR-10, and EMNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> datasets. For the lossy compression, it is shown that the subsampling approach taken by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> does not reach an acceptable level of performance. The reason is that the update errors can be averaged out for participant-to-server uploads but not for server-to-participant downloads. On the other hand, quantization with Kashin’s representation can achieve the same performance as the baseline without compression while having communication cost reduced by nearly 8 times when the model is quantized to 4 bits. For federated dropout approaches, the results show that a dropout rate of 25% of weight matrices of fully-connected layers (or filters in the case of CNN) can achieve acceptable accuracy in most cases while ensuring around 43% reduction in size of models communicated. However, if dropout rates are more aggressive, convergence of the model can be slower.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">The aforementioned two studies suggest useful model compression approaches in reducing communication costs for both server-to-participant and participant-to-server communications. As one may expect, the reduction in communication costs come with sacrifices in model accuracy. It will thus be useful to formalize the compression-accuracy tradeoffs, especially since this varies for different tasks, or when different number of FL participants are involved.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Importance-based Updating</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Based on the observation that most parameter values of a DNN model are sparsely distributed and close to zero <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> propose the edge Stochastic Gradient Descent (eSGD) algorithm that selects only a small fraction of important gradients to be communicated to the FL server for parameter update during each communication round. The eSGD algorithm keeps track of loss values at two consecutive training iterations. If the loss value of the current iteration is smaller than the preceding iteration, this implies that current training gradients and model parameters are important for training loss minimalization and thus, their respective hidden weights are assigned a positive value. In addition, the gradient is also communicated to the server for parameter update. Once this does not hold, i.e., the loss increases as compared to the previous iteration, other parameters are selected to be updated based on their hidden weight values. A parameter with larger hidden weight value is more likely to be selected since it has been labeled as important several times during training. To account for small gradient values that can delay convergence if they are ignored and not updated completely <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, these gradient values are accumulated as residual values. Since the residuals may arise from different training iterations, each update to the residual is weighted with a discount factor using the momentum correction technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>. Once the accumulated residual gradient reaches a threshold, they are chosen to replace the least important gradient coordinates according to the hidden weight values. The simulation results show that eSGD with a 50% drop ratio can achieve higher accuracy than that of the thresholdSGD algorithm proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, which uses a fixed threshold value to determine which gradient coordinates to drop. eSGD can also save a large proportion of gradient size communicated. However, eSGD still suffers from accuracy loss as compared to standard SGD approaches. For example, when tested on simple classification tasks using the MNIST dataset, the model accuracy converges to just 91.22% whereas standard SGD can achieve 99.77% accuracy. If extended to more sophisticated tasks, the accuracy can potentially deteriorate to a larger extent. In addition, the accuracy and convergence speed of the eSGD approach fluctuates arbitrarily based on hyperparameters used, e.g., minibatch size. As such, further studies have to be conducted to formally balance the tradeoffs between communication costs and training performance.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">While <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> studies the selective communication of gradients, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> propose the Communication-Mitigated Federated Learning (CMFL) algorithm that uploads only relevant local model updates to reduce communication costs while guaranteeing global convergence. In each iteration, a participant’s local update is first compared with the global update to identify if the update is relevant. A relevance score is computed where the score equates to percentage of same-sign parameters in the local and global update. In fact, the global update is not known in advance before aggregation. As such, the global update made in the previous iteration is used as an estimate for comparison since it was found empirically that more than 99% of the normalized difference of two sequential global updates are smaller than 0.05 in both MNIST CNN and Next-Word-Prediction LSTM. An update is considered to be irrelevant if its relevance score is smaller than a predefined threshold. The simulation results show that CMFL requires 3.47 times and 13.97 times fewer communication rounds to reach 80% accuracy for MNIST CNN and Next-Word-Prediction LSTM, respectively, as compared to the benchmark <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm. In addition, CMFL can save significantly more communication rounds as compared to Gaia. Note that Gaia is a geo-distributed ML approach suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> which measures relevance based on <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">magnitude</span> of updates rather than sign of parameters. When applied with the aforementioned MOCHA algorithm <a href="#S2.SS3" title="II-C Statistical Challenges of FL ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, CMFL can reduce communication rounds by 5.7 times for the Human Activity Recognition dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> and 3.3 times for the Semeion Handwritten Digit dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>. In addition, CMFL can achieve slightly higher accuracy since it involves the elimination of irrelevant updates that are outliers which harm training.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.3.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S3.T4.4.2" class="ltx_text" style="font-size:90%;">Approaches to communication cost reduction in FL. </span></figcaption>
<div id="S3.T4.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:948.5pt;height:350.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S3.T4.5.1" class="ltx_p"><span id="S3.T4.5.1.1" class="ltx_text">
<span id="S3.T4.5.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.1" class="ltx_tr" style="background-color:#C0C0C0;">
<span id="S3.T4.5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S3.T4.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Approaches</span></span>
<span id="S3.T4.5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S3.T4.5.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Ref.</span></span>
<span id="S3.T4.5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S3.T4.5.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Key Ideas</span></span>
<span id="S3.T4.5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S3.T4.5.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Tradeoffs and Shortcomings</span></span></span>
<span id="S3.T4.5.1.1.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S3.T4.5.1.1.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></span>
<span id="S3.T4.5.1.1.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.2.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">More local updates in between communication for global aggregation, to reduce</span></span>
<span id="S3.T4.5.1.1.1.2.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">instances of communication</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.2.4.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.2.4.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Increased computation cost and poor performance</span></span>
<span id="S3.T4.5.1.1.1.2.4.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">in non-IID setting</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.3" class="ltx_tr">
<span id="S3.T4.5.1.1.1.3.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S3.T4.5.1.1.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite></span>
<span id="S3.T4.5.1.1.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.3.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Similar to the ideas of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, but with convergence guarantees for vertical FL</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.3.4.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.3.4.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Increased computation cost and delayed</span></span>
<span id="S3.T4.5.1.1.1.3.4.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">convergence if global aggregation is too infrequent</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.4" class="ltx_tr">
<span id="S3.T4.5.1.1.1.4.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S3.T4.5.1.1.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite></span>
<span id="S3.T4.5.1.1.1.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.4.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Transfer learning-inspired two-stream model for FL participants to learn from the fixed</span></span>
<span id="S3.T4.5.1.1.1.4.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">global model so as to accelerate training convergence</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.4.4.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.4.4.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Increased computation cost and delayed</span></span>
<span id="S3.T4.5.1.1.1.4.4.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">convergence</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.5" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S3.T4.5.1.1.1.5.1.1" class="ltx_text">
<span id="S3.T4.5.1.1.1.5.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.5.1.1.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Edge and End</span></span>
<span id="S3.T4.5.1.1.1.5.1.1.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Computation</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite></span>
<span id="S3.T4.5.1.1.1.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.5.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">MEC-inspired edge server assisted FL that aids in intermediate parameter aggregation to</span></span>
<span id="S3.T4.5.1.1.1.5.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">reduce instances of communication</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.5.4.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.5.4.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">System model is not scalable when there are</span></span>
<span id="S3.T4.5.1.1.1.5.4.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">more edge servers</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.6" class="ltx_tr">
<span id="S3.T4.5.1.1.1.6.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S3.T4.5.1.1.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite></span>
<span id="S3.T4.5.1.1.1.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.6.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Structured and sketched updates to compress local models communicated from participant</span></span>
<span id="S3.T4.5.1.1.1.6.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">to FL server</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Model accuracy and convergence issues</span></span>
<span id="S3.T4.5.1.1.1.7" class="ltx_tr">
<span id="S3.T4.5.1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S3.T4.5.1.1.1.7.1.1" class="ltx_text">
<span id="S3.T4.5.1.1.1.7.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.7.1.1.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.7.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Model</span></span>
<span id="S3.T4.5.1.1.1.7.1.1.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.7.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Compression</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite></span>
<span id="S3.T4.5.1.1.1.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.7.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Similar to the ideas of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, but for communication from FL server</span></span>
<span id="S3.T4.5.1.1.1.7.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">to participants</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Model accuracy and convergence issues</span></span>
<span id="S3.T4.5.1.1.1.8" class="ltx_tr">
<span id="S3.T4.5.1.1.1.8.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S3.T4.5.1.1.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite></span>
<span id="S3.T4.5.1.1.1.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.8.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Selective communication of gradients that are assigned importance scores,</span></span>
<span id="S3.T4.5.1.1.1.8.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.8.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">i.e., to reduce training loss</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.8.4.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.8.4.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Only empirically tested on simple datasets and</span></span>
<span id="S3.T4.5.1.1.1.8.4.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">tasks, with fluctuating results</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.9" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T4.5.1.1.1.9.1.1" class="ltx_text">
<span id="S3.T4.5.1.1.1.9.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.9.1.1.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Importance-based</span></span>
<span id="S3.T4.5.1.1.1.9.1.1.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Updating</span></span>
</span></span></span>
<span id="S3.T4.5.1.1.1.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite></span>
<span id="S3.T4.5.1.1.1.9.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.9.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.9.3.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Selective communication of local model updates that have higher relevance scores when</span></span>
<span id="S3.T4.5.1.1.1.9.3.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">compared to previous global model</span></span>
</span></span>
<span id="S3.T4.5.1.1.1.9.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T4.5.1.1.1.9.4.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.5.1.1.1.9.4.1.1" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Difficult to implement when global aggregations</span></span>
<span id="S3.T4.5.1.1.1.9.4.1.2" class="ltx_tr">
<span id="S3.T4.5.1.1.1.9.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">are less frequent</span></span>
</span></span></span>
</span></span></p>
</span></div>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Summary and Lessons Learned</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this section, we have reviewed three main approaches for communication cost reduction in FL, and for each approach, we discuss the solutions proposed in different studies. We summarize the approaches along with references in Table <a href="#S3.T4" title="TABLE IV ‣ III-C Importance-based Updating ‣ III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. From this review, we gather the following lessons learned:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Communication cost is a key issue to be resolved before we can implement FL at scale. In particular, the state-of-the-art DL models have high inference accuracy but are increasingly complex with millions of parameters. The slow upload speed of mobile devices can thus impede the implementation of efficient FL.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">This section explores several key approaches to communication cost reduction. However, many of the approaches, e.g., model compression, result in a deterioration in model accuracy or incur high computation cost. For example, when too many local updates are implemented between communication rounds, the communication cost is indeed reduced but the convergence can be significantly delayed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>. The tradeoff between these sacrifices and communication cost reduction thus has to be well-managed.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">The current studies of this tradeoff are often mainly empirical in nature, e.g., several experiments have to be done to find the optimal number of local training iterations before communication. With more effective optimization approaches formalized theoretically and tested empirically, FL can eventually become more scalable in nature. For example, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> study the tradeoffs between the completion time of FL training and energy cost expended. Then, a weighted sum of completion time and energy consumption is minimized using an iterative algorithm. For delay-sensitive scenarios, the weights can be adjusted such that the FL participants consume more energy for completion time minimization.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">Apart from working to directly reduce the size of model communicated, studies on FL can draw inspiration from applications and approaches in the MEC paradigm. For example, a simple case study introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> considers the base station as an intermediate model aggregator to reduce instances of device-cloud communication. Unfortunately, there are convergence issues when more edge servers or mobile devices are considered. This is exacerbated by the non-IID distribution of data across different edge nodes. For future works, this statistical challenge can be met, e.g., through inspirations from multi-task learning as we have discussed in Section <a href="#S2.SS3" title="II-C Statistical Challenges of FL ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>. In addition, more effective and innovative system models can be explored such that FL networks can utilize the wealth of computing and storage resources that are closer to the data sources to facilitate efficient FL.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p">For the studies that we have discussed in this section, the heterogeneity among mobile devices, e.g., in computing capabilities, is often not considered. For example, one of the ways to reduce communication cost is to increase computation on edge devices, e.g., by performing more local updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> before each communication round. In fact, this does not merely lead to the expenditure of greater computation cost. The approach may also not be feasible for devices with weak processing power, and can lead to the <span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_italic">straggler effect</span>. As such, we further explore issues on resource allocation in the next section.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Resource Allocation</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">FL involves the participation of heterogeneous devices that have different dataset qualities, computation capabilities, energy states, and willingness to participate. Given the device heterogeneity and resource constraints, i.e., in device energy states and communication bandwidth, resource allocation has to be optimized to maximize the efficiency of the training process. In particular, the following resource allocation issues need to be considered:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Participant Selection:</span> As part of the FL protocol presented in Section <a href="#S2.SS4" title="II-D FL protocols and frameworks ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a>, participant selection refers to the selection of devices to participate in each training round. Typically, a set of participants is randomly selected by the server to participate. Then, the server has to aggregate parameter updates from all participating devices in the round before taking a weighted average of the models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. As such, the training progress of FL is limited by the training time of the slowest participating devices, i.e., stragglers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>. New participant selection protocols are thus investigated in order to address the training bottleneck in FL.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Joint Radio and Computation Resource Management:</span> Even though computation capabilities of mobile devices have grown rapidly, many devices still face a scarcity of radio resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>. Given that local model transmission is an integral part of FL, there has been a growing number of studies that focus on developing novel wireless communication techniques for efficient FL.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Adaptive Aggregation:</span> As discussed in Section <a href="#S2.SS2" title="II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>, FL involves global aggregation in which model parameters are communicated to the FL server for aggregation. The conventional approach to global aggregation is a synchronous one, i.e., global aggregations occur in fixed intervals after all participants complete a certain number of rounds of local computation. However, adaptive calibrations of global aggregation frequency can be investigated to increase training efficiency subject to resource constraints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Incentive Mechanism:</span> In the practical implementation of FL, participants may be reluctant to participate in a federation without receiving compensation since training models is resource-consuming. In addition, there exists information asymmetry between the FL server and participants since participants have greater knowledge of their available computation resources and data quality. Therefore, incentive mechanisms have to be carefully designed to both incentivize participation and reduce the potential adverse impacts of information asymmetry.</p>
</div>
</li>
</ul>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Participant Selection</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To mitigate the training bottleneck, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> propose a new FL protocol called <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">FedCS</span>. This protocol is illustrated in Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-A Participant Selection ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The system model is a MEC framework in which the operator of the MEC is the FL server that coordinates training in a cellular network that comprises participating mobile devices that have heterogeneous resources. Accordingly, the FL server first conducts a <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">Resource Request</span> step to gather information such as wireless channel states and computing capabilities from a subset of randomly selected participants. Based on this information, the MEC operator selects the maximum possible number of participants that can complete the training within a prespecified deadline for the subsequent global aggregation phase. By selecting the maximum possible number of participants in each round, accuracy and efficiency of training are preserved. To solve the maximization problem, a greedy algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> is proposed, i.e., participants that take the least time for model upload and update are iteratively selected for training. The simulation results show that compared with the FL protocol which only accounts for training deadline without performing participant selection, <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">FedCS</span> can achieve higher accuracy since <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_italic">FedCS</span> is able to involve more participants in each training round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. However, <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_italic">FedCS</span> has been tested only on simple DNN models. When extended to the training of more complex models, it may be difficult to estimate how many participants should be selected. For example, more training rounds may be needed for the training of complex models, and the selection of too few participants may lead to poor performance considering that some participants may drop out during training. In addition, there is bias towards selecting participants with devices that have better computing capabilities. These participants may not hold data that is representative of the population distribution. In particular, we revisit the fairness issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> subsequently in this section.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/partselection.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="400" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.4.2" class="ltx_text" style="font-size:90%;">Participant selection under the FedCS and Hybrid-FL protocol.</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">While <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">FedCS</span> addresses heterogeneity of resources among participants in FL, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> extend their work on the <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">FedCS</span> protocol with the Hybrid-FL protocol that deals with differences in data distributions among participants. The dataset of participants participating in FL may be non-IID since it is reflective of each individual user’s specific characteristics. As we have discussed in Section <a href="#S2.SS3" title="II-C Statistical Challenges of FL ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>, the non-IID dataset may significantly degrade the performance of the <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">FedAvg</span> algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. One proposed measure to address the non-IID nature of the dataset is to distribute publicly available data to participants, such that the EMD between their on-device dataset and the population distance is reduced. However, such a dataset may not always exist, and participants may not download them for security reasons. Thus, an alternative solution is to construct an approximately IID dataset using inputs from a limited number of privacy insensitive participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>. In the Hybrid-FL protocol, during the <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_italic">Resource Request</span> step (Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-A Participant Selection ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>), the MEC operator asks random participants if they permit their data to be uploaded. During the participant selection phase, apart from selecting participants based on computing capabilities, participants are selected such that their uploaded data can form an approximately IID dataset in the server, i.e., the amount of collected data in each class has close values (Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-A Participant Selection ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). Thereafter, the server trains a model on the collected IID dataset and merge this model with the global model trained by participants. The simulation results show that even with just 1% of participants sharing their data, classification accuracy for non-IID data can be significantly improved as compared to the aforementioned <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_italic">FedCS</span> benchmark where data is not uploaded at all. However, the recommended protocol can violate the privacy and security of users, especially if the FL server is malicious. In the case when participants are malicious, data can be falsified before uploading, as we will further discuss in Section <a href="#S5" title="V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. In addition, the proposed measure can be costly especially in the case of videos and images. As such, it is unlikely that participants will volunteer for data uploading when they can free ride on the efforts of other volunteers. For feasibility, a well-designed incentive and reputation mechanism is needed to ensure that only trustworthy participants are allowed to upload their data.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In general, the mobile edge network environment in which FL is implemented on is dynamic and uncertain with variable constraints, e.g., wireless network and energy conditions. Thus, this can lead to training bottlenecks. To this end, Deep Q-Learning (DQL) can be used to optimize resource allocation for model training as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>. The system model is a Mobile Crowd Machine Learning (MCML) setting which enables participants in a mobile crowd network to collaboratively train DNN models required by a FL server. The participating mobile devices are constrained by energy, CPU, and wireless bandwidth. Thus, the server needs to determine proper amounts of data, energy, and CPU resources that the mobile devices use for training to minimize energy consumption and training time. Under the uncertainty of the mobile environment, a stochastic optimization problem is formulated. In the problem, the server is the agent, the state space includes the CPU and energy states of the mobile devices, and the action space includes the number of data units and energy units taken from the mobile devices. To achieve the objective, the reward function is defined as a function of the accumulated data, energy consumption, and training latency. To overcome the large state and action space issues of the server, the DQL technique based on Double Deep Q-Network (DDQN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> is adopted to solve the server’s problem. The simulation results show that the DQL scheme can reduce energy consumption by around 31% compared with the greedy algorithm, and training latency is reduced up to 55% compared with the random scheme. However, the proposed scheme is applicable only in federations with few participating mobile devices.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">As an extension to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> also proposes a resource allocation approach using DRL, with the added uncertainty that FL participants are mobile and so they may venture out of the network coverage range with a certain probability. Without prior knowledge of the mobile network, the FL server is able to optimize resource allocation across participants, e.g., channel selection and device energy consumption.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">The aforementioned resource allocation approaches focus on improving the training efficiency of FL. However, this may cause some FL participants to be left out of the aggregation phase because they are stragglers with limited computing or communication resources.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.5" class="ltx_p">One consequence of this <span id="S4.SS1.p6.5.1" class="ltx_text ltx_font_italic">unfair</span> resource allocation, a topic that is commonly explored in resource allocation for wireless networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> and ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>. For example, if the participant selection protocol selects mobile devices with higher computing capabilities to participate in each training round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, the FL model will be overrepresented by the distribution of data owned by participants with devices that have higher computing capabilities. Therefore, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> consider fairness as an additional objective in FL. Fairness is defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> to be the <span id="S4.SS1.p6.5.2" class="ltx_text ltx_font_italic">variance</span> of performance of an FL model across participants. If the variance of the testing accuracy is large, this implies the presence of more bias or less fairness, since the learned model may be highly accurate for certain participants and less so for other underrepresented participants. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> propose the <span id="S4.SS1.p6.5.3" class="ltx_text ltx_font_italic">q</span>-Fair FL (<span id="S4.SS1.p6.5.4" class="ltx_text ltx_font_italic">q</span>-FFL) algorithm that reweighs the objective function in <span id="S4.SS1.p6.5.5" class="ltx_text ltx_font_italic">FedAvg</span> to assign higher weights in the loss function to devices with higher loss. The modified objective function is as follows:</p>
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.2" class="ltx_Math" alttext="\min_{w}F_{q}(w)=\sum_{k=1}^{m}\frac{p_{k}}{q+1}F_{k}^{q+1}(w)" display="block"><semantics id="S4.E6.m1.2a"><mrow id="S4.E6.m1.2.3" xref="S4.E6.m1.2.3.cmml"><mrow id="S4.E6.m1.2.3.2" xref="S4.E6.m1.2.3.2.cmml"><mrow id="S4.E6.m1.2.3.2.2" xref="S4.E6.m1.2.3.2.2.cmml"><munder id="S4.E6.m1.2.3.2.2.1" xref="S4.E6.m1.2.3.2.2.1.cmml"><mi id="S4.E6.m1.2.3.2.2.1.2" xref="S4.E6.m1.2.3.2.2.1.2.cmml">min</mi><mi id="S4.E6.m1.2.3.2.2.1.3" xref="S4.E6.m1.2.3.2.2.1.3.cmml">w</mi></munder><mo lspace="0.167em" id="S4.E6.m1.2.3.2.2a" xref="S4.E6.m1.2.3.2.2.cmml">⁡</mo><msub id="S4.E6.m1.2.3.2.2.2" xref="S4.E6.m1.2.3.2.2.2.cmml"><mi id="S4.E6.m1.2.3.2.2.2.2" xref="S4.E6.m1.2.3.2.2.2.2.cmml">F</mi><mi id="S4.E6.m1.2.3.2.2.2.3" xref="S4.E6.m1.2.3.2.2.2.3.cmml">q</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.3.2.1" xref="S4.E6.m1.2.3.2.1.cmml">​</mo><mrow id="S4.E6.m1.2.3.2.3.2" xref="S4.E6.m1.2.3.2.cmml"><mo stretchy="false" id="S4.E6.m1.2.3.2.3.2.1" xref="S4.E6.m1.2.3.2.cmml">(</mo><mi id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">w</mi><mo stretchy="false" id="S4.E6.m1.2.3.2.3.2.2" xref="S4.E6.m1.2.3.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S4.E6.m1.2.3.1" xref="S4.E6.m1.2.3.1.cmml">=</mo><mrow id="S4.E6.m1.2.3.3" xref="S4.E6.m1.2.3.3.cmml"><munderover id="S4.E6.m1.2.3.3.1" xref="S4.E6.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S4.E6.m1.2.3.3.1.2.2" xref="S4.E6.m1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S4.E6.m1.2.3.3.1.2.3" xref="S4.E6.m1.2.3.3.1.2.3.cmml"><mi id="S4.E6.m1.2.3.3.1.2.3.2" xref="S4.E6.m1.2.3.3.1.2.3.2.cmml">k</mi><mo id="S4.E6.m1.2.3.3.1.2.3.1" xref="S4.E6.m1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E6.m1.2.3.3.1.2.3.3" xref="S4.E6.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E6.m1.2.3.3.1.3" xref="S4.E6.m1.2.3.3.1.3.cmml">m</mi></munderover><mrow id="S4.E6.m1.2.3.3.2" xref="S4.E6.m1.2.3.3.2.cmml"><mfrac id="S4.E6.m1.2.3.3.2.2" xref="S4.E6.m1.2.3.3.2.2.cmml"><msub id="S4.E6.m1.2.3.3.2.2.2" xref="S4.E6.m1.2.3.3.2.2.2.cmml"><mi id="S4.E6.m1.2.3.3.2.2.2.2" xref="S4.E6.m1.2.3.3.2.2.2.2.cmml">p</mi><mi id="S4.E6.m1.2.3.3.2.2.2.3" xref="S4.E6.m1.2.3.3.2.2.2.3.cmml">k</mi></msub><mrow id="S4.E6.m1.2.3.3.2.2.3" xref="S4.E6.m1.2.3.3.2.2.3.cmml"><mi id="S4.E6.m1.2.3.3.2.2.3.2" xref="S4.E6.m1.2.3.3.2.2.3.2.cmml">q</mi><mo id="S4.E6.m1.2.3.3.2.2.3.1" xref="S4.E6.m1.2.3.3.2.2.3.1.cmml">+</mo><mn id="S4.E6.m1.2.3.3.2.2.3.3" xref="S4.E6.m1.2.3.3.2.2.3.3.cmml">1</mn></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.3.3.2.1" xref="S4.E6.m1.2.3.3.2.1.cmml">​</mo><msubsup id="S4.E6.m1.2.3.3.2.3" xref="S4.E6.m1.2.3.3.2.3.cmml"><mi id="S4.E6.m1.2.3.3.2.3.2.2" xref="S4.E6.m1.2.3.3.2.3.2.2.cmml">F</mi><mi id="S4.E6.m1.2.3.3.2.3.2.3" xref="S4.E6.m1.2.3.3.2.3.2.3.cmml">k</mi><mrow id="S4.E6.m1.2.3.3.2.3.3" xref="S4.E6.m1.2.3.3.2.3.3.cmml"><mi id="S4.E6.m1.2.3.3.2.3.3.2" xref="S4.E6.m1.2.3.3.2.3.3.2.cmml">q</mi><mo id="S4.E6.m1.2.3.3.2.3.3.1" xref="S4.E6.m1.2.3.3.2.3.3.1.cmml">+</mo><mn id="S4.E6.m1.2.3.3.2.3.3.3" xref="S4.E6.m1.2.3.3.2.3.3.3.cmml">1</mn></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.3.3.2.1a" xref="S4.E6.m1.2.3.3.2.1.cmml">​</mo><mrow id="S4.E6.m1.2.3.3.2.4.2" xref="S4.E6.m1.2.3.3.2.cmml"><mo stretchy="false" id="S4.E6.m1.2.3.3.2.4.2.1" xref="S4.E6.m1.2.3.3.2.cmml">(</mo><mi id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml">w</mi><mo stretchy="false" id="S4.E6.m1.2.3.3.2.4.2.2" xref="S4.E6.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.2b"><apply id="S4.E6.m1.2.3.cmml" xref="S4.E6.m1.2.3"><eq id="S4.E6.m1.2.3.1.cmml" xref="S4.E6.m1.2.3.1"></eq><apply id="S4.E6.m1.2.3.2.cmml" xref="S4.E6.m1.2.3.2"><times id="S4.E6.m1.2.3.2.1.cmml" xref="S4.E6.m1.2.3.2.1"></times><apply id="S4.E6.m1.2.3.2.2.cmml" xref="S4.E6.m1.2.3.2.2"><apply id="S4.E6.m1.2.3.2.2.1.cmml" xref="S4.E6.m1.2.3.2.2.1"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.2.2.1.1.cmml" xref="S4.E6.m1.2.3.2.2.1">subscript</csymbol><min id="S4.E6.m1.2.3.2.2.1.2.cmml" xref="S4.E6.m1.2.3.2.2.1.2"></min><ci id="S4.E6.m1.2.3.2.2.1.3.cmml" xref="S4.E6.m1.2.3.2.2.1.3">𝑤</ci></apply><apply id="S4.E6.m1.2.3.2.2.2.cmml" xref="S4.E6.m1.2.3.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.2.2.2.1.cmml" xref="S4.E6.m1.2.3.2.2.2">subscript</csymbol><ci id="S4.E6.m1.2.3.2.2.2.2.cmml" xref="S4.E6.m1.2.3.2.2.2.2">𝐹</ci><ci id="S4.E6.m1.2.3.2.2.2.3.cmml" xref="S4.E6.m1.2.3.2.2.2.3">𝑞</ci></apply></apply><ci id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1">𝑤</ci></apply><apply id="S4.E6.m1.2.3.3.cmml" xref="S4.E6.m1.2.3.3"><apply id="S4.E6.m1.2.3.3.1.cmml" xref="S4.E6.m1.2.3.3.1"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.3.1.1.cmml" xref="S4.E6.m1.2.3.3.1">superscript</csymbol><apply id="S4.E6.m1.2.3.3.1.2.cmml" xref="S4.E6.m1.2.3.3.1"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.3.1.2.1.cmml" xref="S4.E6.m1.2.3.3.1">subscript</csymbol><sum id="S4.E6.m1.2.3.3.1.2.2.cmml" xref="S4.E6.m1.2.3.3.1.2.2"></sum><apply id="S4.E6.m1.2.3.3.1.2.3.cmml" xref="S4.E6.m1.2.3.3.1.2.3"><eq id="S4.E6.m1.2.3.3.1.2.3.1.cmml" xref="S4.E6.m1.2.3.3.1.2.3.1"></eq><ci id="S4.E6.m1.2.3.3.1.2.3.2.cmml" xref="S4.E6.m1.2.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S4.E6.m1.2.3.3.1.2.3.3.cmml" xref="S4.E6.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E6.m1.2.3.3.1.3.cmml" xref="S4.E6.m1.2.3.3.1.3">𝑚</ci></apply><apply id="S4.E6.m1.2.3.3.2.cmml" xref="S4.E6.m1.2.3.3.2"><times id="S4.E6.m1.2.3.3.2.1.cmml" xref="S4.E6.m1.2.3.3.2.1"></times><apply id="S4.E6.m1.2.3.3.2.2.cmml" xref="S4.E6.m1.2.3.3.2.2"><divide id="S4.E6.m1.2.3.3.2.2.1.cmml" xref="S4.E6.m1.2.3.3.2.2"></divide><apply id="S4.E6.m1.2.3.3.2.2.2.cmml" xref="S4.E6.m1.2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.3.2.2.2.1.cmml" xref="S4.E6.m1.2.3.3.2.2.2">subscript</csymbol><ci id="S4.E6.m1.2.3.3.2.2.2.2.cmml" xref="S4.E6.m1.2.3.3.2.2.2.2">𝑝</ci><ci id="S4.E6.m1.2.3.3.2.2.2.3.cmml" xref="S4.E6.m1.2.3.3.2.2.2.3">𝑘</ci></apply><apply id="S4.E6.m1.2.3.3.2.2.3.cmml" xref="S4.E6.m1.2.3.3.2.2.3"><plus id="S4.E6.m1.2.3.3.2.2.3.1.cmml" xref="S4.E6.m1.2.3.3.2.2.3.1"></plus><ci id="S4.E6.m1.2.3.3.2.2.3.2.cmml" xref="S4.E6.m1.2.3.3.2.2.3.2">𝑞</ci><cn type="integer" id="S4.E6.m1.2.3.3.2.2.3.3.cmml" xref="S4.E6.m1.2.3.3.2.2.3.3">1</cn></apply></apply><apply id="S4.E6.m1.2.3.3.2.3.cmml" xref="S4.E6.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.3.2.3.1.cmml" xref="S4.E6.m1.2.3.3.2.3">superscript</csymbol><apply id="S4.E6.m1.2.3.3.2.3.2.cmml" xref="S4.E6.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.3.3.2.3.2.1.cmml" xref="S4.E6.m1.2.3.3.2.3">subscript</csymbol><ci id="S4.E6.m1.2.3.3.2.3.2.2.cmml" xref="S4.E6.m1.2.3.3.2.3.2.2">𝐹</ci><ci id="S4.E6.m1.2.3.3.2.3.2.3.cmml" xref="S4.E6.m1.2.3.3.2.3.2.3">𝑘</ci></apply><apply id="S4.E6.m1.2.3.3.2.3.3.cmml" xref="S4.E6.m1.2.3.3.2.3.3"><plus id="S4.E6.m1.2.3.3.2.3.3.1.cmml" xref="S4.E6.m1.2.3.3.2.3.3.1"></plus><ci id="S4.E6.m1.2.3.3.2.3.3.2.cmml" xref="S4.E6.m1.2.3.3.2.3.3.2">𝑞</ci><cn type="integer" id="S4.E6.m1.2.3.3.2.3.3.3.cmml" xref="S4.E6.m1.2.3.3.2.3.3.3">1</cn></apply></apply><ci id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.2.2">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.2c">\min_{w}F_{q}(w)=\sum_{k=1}^{m}\frac{p_{k}}{q+1}F_{k}^{q+1}(w)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p6.4" class="ltx_p">where <math id="S4.SS1.p6.1.m1.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S4.SS1.p6.1.m1.1a"><msub id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml"><mi id="S4.SS1.p6.1.m1.1.1.2" xref="S4.SS1.p6.1.m1.1.1.2.cmml">F</mi><mi id="S4.SS1.p6.1.m1.1.1.3" xref="S4.SS1.p6.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><apply id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p6.1.m1.1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p6.1.m1.1.1.2.cmml" xref="S4.SS1.p6.1.m1.1.1.2">𝐹</ci><ci id="S4.SS1.p6.1.m1.1.1.3.cmml" xref="S4.SS1.p6.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">F_{k}</annotation></semantics></math> refers to the standard loss functions presented in Table <a href="#S2.T3" title="TABLE III ‣ II-B Federated Learning ‣ II Background and Fundamentals of Federated Learning ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, <math id="S4.SS1.p6.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p6.2.m2.1a"><mi id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><ci id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">q</annotation></semantics></math> refers to the calibration of fairness in the system model, i.e., setting <math id="S4.SS1.p6.3.m3.1" class="ltx_Math" alttext="q=0" display="inline"><semantics id="S4.SS1.p6.3.m3.1a"><mrow id="S4.SS1.p6.3.m3.1.1" xref="S4.SS1.p6.3.m3.1.1.cmml"><mi id="S4.SS1.p6.3.m3.1.1.2" xref="S4.SS1.p6.3.m3.1.1.2.cmml">q</mi><mo id="S4.SS1.p6.3.m3.1.1.1" xref="S4.SS1.p6.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p6.3.m3.1.1.3" xref="S4.SS1.p6.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.3.m3.1b"><apply id="S4.SS1.p6.3.m3.1.1.cmml" xref="S4.SS1.p6.3.m3.1.1"><eq id="S4.SS1.p6.3.m3.1.1.1.cmml" xref="S4.SS1.p6.3.m3.1.1.1"></eq><ci id="S4.SS1.p6.3.m3.1.1.2.cmml" xref="S4.SS1.p6.3.m3.1.1.2">𝑞</ci><cn type="integer" id="S4.SS1.p6.3.m3.1.1.3.cmml" xref="S4.SS1.p6.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.3.m3.1c">q=0</annotation></semantics></math> returns the formulation to the typical FL objective, and <math id="S4.SS1.p6.4.m4.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S4.SS1.p6.4.m4.1a"><msub id="S4.SS1.p6.4.m4.1.1" xref="S4.SS1.p6.4.m4.1.1.cmml"><mi id="S4.SS1.p6.4.m4.1.1.2" xref="S4.SS1.p6.4.m4.1.1.2.cmml">p</mi><mi id="S4.SS1.p6.4.m4.1.1.3" xref="S4.SS1.p6.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.4.m4.1b"><apply id="S4.SS1.p6.4.m4.1.1.cmml" xref="S4.SS1.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p6.4.m4.1.1.1.cmml" xref="S4.SS1.p6.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p6.4.m4.1.1.2.cmml" xref="S4.SS1.p6.4.m4.1.1.2">𝑝</ci><ci id="S4.SS1.p6.4.m4.1.1.3.cmml" xref="S4.SS1.p6.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.4.m4.1c">p_{k}</annotation></semantics></math> refers to ratio of local samples to the total number of training samples. In fact, this is a generalization of the Agnostic FL (AFL) algorithm proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, in which the device with the <span id="S4.SS1.p6.4.1" class="ltx_text">highest</span> loss dominates the entire loss function. The simulation results show that the proposed <span id="S4.SS1.p6.4.2" class="ltx_text ltx_font_italic">q</span>-FFL can achieve lower variance of testing accuracy and converges more quickly than the AFL algorithm. However, as expected, for some calibrations of the <span id="S4.SS1.p6.4.3" class="ltx_text ltx_font_italic">q</span>-FFL algorithm, there can be convergence slowdown since stragglers can delay the training process. As such, an asynchronous aggregation approach (to be subsequently discussed in this section) can potentially be considered for use with the <span id="S4.SS1.p6.4.4" class="ltx_text ltx_font_italic">q</span>-FFL algorithm.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">In contrast, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> propose a neural network based approach to estimate the local models of FL participants that are left out during training. In the system model, resource blocks are first allocated by the base station to users whose models have larger effects on the global FL model. In particular, one user is selected to always be connected to the base station. This user’s model parameters are then used as input to the feedforward neural network to estimate the model parameters of users who are left out during the training iteration. This allows the base stations to be able to integrate more locally trained FL model parameters to each iteration of global aggregation, thus improving the FL convergence speed.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Joint Radio and Computation Resource Management</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">While most FL studies have previously assumed orthogonal-access schemes such as Orthogonal Frequency-division Multiple Access (OFDMA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> propose a multi-access Broadband Analog Aggregation (BAA) design for communication-latency reduction in FL. Instead of performing communication and computation separately during global aggregation at the server, the BAA scheme builds on the concept of over-the-air computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> to <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">integrate</span> computation and communication through exploiting the signal superposition property of a multiple-access channel. The proposed BAA scheme allows the reuse of the whole bandwidth (Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-B Joint Radio and Computation Resource Management ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(a)) whereas OFDMA orthogonalizes bandwidth allocation (Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-B Joint Radio and Computation Resource Management ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(b)). As such, for orthogonal-access schemes, communication latency increases in direct proportion with the number of participants whereas for multi-access schemes, latency is independent of the number of participants. The bottleneck of signal-to-noise ratio (SNR) during BAA transmission is the participating device with the longest propagation distance given that devices that are nearer have to lower their transmission power for amplitude alignment with devices located further. To increase SNR, participants with longer propagation distance have to be dropped. However, this leads to the truncation of model parameters. As such, to manage the SNR-truncation tradeoff, three scheduling schemes are considered namely i) <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">Cell-interior scheduling</span>: participants beyond a distance threshold are not scheduled, ii) <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">All-inclusive scheduling</span>: all participants are considered, and iii) <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_italic">Alternating scheduling</span>: edge server alternates between the two aforementioned schemes. The simulation results show that the proposed BAA scheme can achieve similar test accuracy as the OFDMA scheme while achieving latency reduction from 10 times to 1000 times. As a comparison between the three scheduling schemes, the cell-interior scheme outperforms the all-inclusive scheme in terms of test accuracy for high mobility networks where participants have rapidly changing locations. For low mobility networks, the alternating scheduling scheme outperforms cell-interior scheduling.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As an extension, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> also introduce error accumulation and gradient sparsification in addition to over-the-air computation. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>, gradient vectors that are not transmitted as a result of power constraints are completely dropped. To improve the model accuracy, the untransmitted gradient vectors can first be stored in an error accumulation vector. In the next round, local gradient estimates are then corrected using the error vector. In addition, when there are bandwidth limitations, the participating device can apply gradient sparsification to keep only elements with the highest magnitudes for transmission. The elements that are not transmitted are subsequently added on to the error accumulation vector for gradient estimate correction in the next round. The simulation results show that the proposed scheme can achieve higher test accuracy than over-the-air computation without error accumulation or gradient sparsification since it corrects gradient estimates with the error accumulation vector and allows for a more efficient utilization of the bandwidth.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/aircomp.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.4.2" class="ltx_text" style="font-size:90%;">A comparison <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> between (a) BAA by over-the-air computation which reuses bandwidth (above) and (b) OFDMA (below) which uses only the allocated bandwidth.</span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> propose an integration of computation and communication via over-the-air computation. However, it is observed that aggregation error incurred during over-the-air computation can lead to a drop in model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> as a result of signal distortion. As such, a participant selection algorithm is proposed in which the number of devices selected for training is maximized so as to improve statistical learning performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> while keeping the signal distortion below a threshold. Due to the nonconvexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> of the mean-square-error (MSE) constraint and intractability of the optimization problem, a difference-of-convex functions (DC) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite> is proposed to solve the maximization problem. The simulation results show that the proposed DC algorithm is scalable and can also achieve near-optimal performance that is comparable to global optimization, which is non-scalable due to its exponential time complexity. In comparison with other state-of-the-art approaches such as the semidefinite relaxation technique (SDR) proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>, the proposed DC algorithm can also select more participants, thus also achieving higher model accuracy.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Adaptive Aggregation</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The proposed <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm synchronously aggregates parameters as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-C Adaptive Aggregation ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(a) and is thus susceptible to the straggler effect, i.e., each training round only progresses as fast as the slowest device since the FL server waits for <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">all</span> devices to complete local training before global aggregation can take place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>. In addition, the model does not account for participants that can join halfway when the training round is already in progress. As such, the asynchronous model is proposed to improve the scalability and efficiency of FL. For asynchronous FL, the server updates the global model whenever it receives a local update (Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-C Adaptive Aggregation ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(b)). The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> find empirically that an asynchronous approach is robust to participants joining halfway during a training round, as well as when the federation involves participating devices with heterogeneous processing capabilities. However, the model convergence is found to be significantly delayed when data is non-IID and unbalanced. As an improvement, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> propose the <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">FedAsync</span> algorithm in which each newly received local updates are adaptively weighted according to staleness, that is defined as the difference between the current epoch and iteration in which the received update belongs to. For example, a stale update from a straggler is outdated since it should have been received in previous training rounds. As such, it is weighted less. In addition, the authors also prove the convergence guarantee for a restricted family of non-convex problems. However, the current hyperparameters of the <span id="S4.SS3.p1.1.4" class="ltx_text ltx_font_italic">FedAsync</span> algorithm still have to be tuned to ensure convergence in different settings. As such, the algorithm is still unable to generalize to suit the dynamic computation constraints of heterogeneous devices. In fact, given the uncertainty surrounding the reliability of asynchronous FL, synchronous FL remains to be the approach most commonly used today <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/asyncnew.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="385" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.4.2" class="ltx_text" style="font-size:90%;">A comparison between (a) synchronous and (b) asynchronous FL.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">For most existing implementations of the <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm, the global aggregation phase occurs after a fixed number of training rounds. To better manage the dynamic resource constraints, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> propose an adaptive global aggregation scheme which varies the global aggregation frequency so as to ensure desirable model performance while ensuring an efficient use of available resources, e.g., energy, during the FL training process. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, the MEC system model used consists of (i) the local update phase where the model is trained using local data, (ii) edge aggregation phase where the intermediate aggregation occurs and (iii) global aggregation phase where updated model parameters are received and aggregated by the FL server. In particular, the authors study how the training loss is affected when the total number of edge server aggregation and local updates between global aggregation intervals vary. For this, a convergence bound of gradient descent with non-IID data is first derived. Then, a control algorithm is subsequently proposed to adaptively choose the optimal global aggregation frequency based on the most recent system state. For example, if global aggregation is too time consuming, more edge aggregations will take place before communication with the FL server is initiated. The simulation results show that the adaptive aggregation scheme outperforms the fixed aggregation scheme in terms of loss function minimization and accuracy within the same time budget. However, the convergence guarantee of the adaptive aggregation scheme is only considered for convex loss functions currently.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Incentive Mechanism</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> propose a service pricing scheme in which participants serve as training service providers for a model owner. In addition, to overcome energy inefficiency in the transfer of model updates, a cooperative relay network is proposed to support model update transfer and trading. The interaction between participants and model owner is modelled as a Stackelberg game <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> in which the model owner is the buyer and participants are the sellers. The Stackelberg game is proposed in which each rational participant can noncooperatively decide on its own profit maximization price. In the lower-level subgame, the model owner determines size of training data to maximize profits with consideration of the increasing concave relationship between learning accuracy of the model and size of training data. In the upper-level subgame, the participants decide the price per unit of data to maximize their individual profits. The simulation results show that the proposed mechanism can ensure uniqueness of the Stackelberg equilibrium. For example, model updates that contain valuable information are priced higher at the Stackelberg equilibrium. In addition, model updates can be transferred cooperatively, thus reducing congestion in communication and improving energy efficiency. However, the simulation environment only involves relatively few mobile devices.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite> also model the interaction between participants and model owner as a Stackelberg game, which is well-suited to represent the FL server-participant interaction involved in FL.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">However, unlike the aforementioned conventional approaches to solving Stackelberg formulations, a DRL-based approach is adopted together with the Stackelberg game by the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>. In the DRL formulation, the FL server acts as an agent that decides a payment in response to the participation level and payment history of edge nodes, with the objective of minimizing incentive expenses. Then, the edge nodes determine an optimal participation level in response to the payment policy. This learning based incentive mechanism design enables the FL server to derive an optimal policy in response to its observed state, without requiring any prior information.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/contractnew.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="681" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.4.2" class="ltx_text" style="font-size:90%;">Participants with unknown resource constraints maximize their utility only if they choose the bundle that best reflects their constraints.</span></figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> propose an incentive design using a contract theoretic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite> approach to attract participants with high-quality data for FL. In particular, well-designed contracts can reduce information asymmetry through self-revealing mechanisms in which participants select only the contracts specifically designed for their types. For feasibility, each contract must satisfy the Individual Rationality (IR) and Incentive Compatibility (IC) constraints. For IR, each participant is assured of a positive utility when the participant participates in the federation. For IC, every utility maximizing participant only chooses the contract designed for its type. The model owner aims to maximize its own profits subject to IR and IC constraints. As illustrated in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-D Incentive Mechanism ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, the optimal contracts derived are self-revealing such that each high-type participant with higher data quality only chooses contracts designed for its type, whereas each low-type participant with lower data quality does not have the incentive to imitate high-type participants. The simulation results show that all types of participants only achieve maximum utility when they choose the contract that matches their types. In addition, the proposed contract theory approach also has better performance in terms of profit for the model owner compared with the Stackelberg game-based incentive mechanism. This is because under the contract theoretic approach, the model owner can extract more profits from the participants whereas under the Stackelberg game approach, the participants can optimize their individual utilities. In fact, the information asymmetry between FL servers and participants make contract theory a powerful and efficient tool for mechanism design in FL. As an extension, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> introduced a <span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_italic">multi-dimensional</span> contract in which each FL participant determines the optimal computation power and image quality it is willing to contribute for model training, in exchange for contract rewards in each iteration.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> further introduce reputation as a metric to measure the reliability of FL participants and design a reputation-based participant selection scheme for reliable FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. In this setting, each participant has a reputation value <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> derived from two sources, (i) direct reputation opinions from past interactions with the FL server and (ii) indirect reputation opinions from other task publishers, i.e., other FL servers. The indirect reputation opinions are stored in an open-access reputation blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> to ensure secure reputation management in a decentralized manner. Before model training, the participants choose a contract that best fits its dataset accuracy and resource conditions. Then, the FL server chooses the participants that have reputation scores which are larger than a prespecified threshold. After the FL task is completed, i.e., a desirable accuracy is achieved, the FL server updates the reputation opinions, which are subsequently stored in the reputation blockchain. The simulation results show that the proposed scheme can significantly improve the accuracy of the FL model since unreliable workers are detected and not selected for FL training.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">Approaches to resource allocation in FL. </span></figcaption>
<div id="S4.T5.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:1113.5pt;height:528.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T5.5.1" class="ltx_p"><span id="S4.T5.5.1.1" class="ltx_text">
<span id="S4.T5.5.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.1" class="ltx_tr" style="background-color:#C0C0C0;">
<span id="S4.T5.5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T5.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Approaches</span></span>
<span id="S4.T5.5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T5.5.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Ref.</span></span>
<span id="S4.T5.5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T5.5.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Key Ideas</span></span>
<span id="S4.T5.5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T5.5.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="background-color:#C0C0C0;">Tradeoffs and Shortcomings</span></span></span>
<span id="S4.T5.5.1.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S4.T5.5.1.1.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite></span>
<span id="S4.T5.5.1.1.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.2.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedCS to select participants based on computation capabilities so as to</span></span>
<span id="S4.T5.5.1.1.1.2.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">complete FL training before specified deadline</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.2.4.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.2.4.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Difficult to estimate training duration accurately for</span></span>
<span id="S4.T5.5.1.1.1.2.4.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">complex models</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.3" class="ltx_tr">
<span id="S4.T5.5.1.1.1.3.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S4.T5.5.1.1.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite></span>
<span id="S4.T5.5.1.1.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.3.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, Hybrid-FL to select participants so as to accumulate</span></span>
<span id="S4.T5.5.1.1.1.3.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">IID, distributable data for FL model training</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.3.4.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.3.4.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Request of data sharing may defeat the</span></span>
<span id="S4.T5.5.1.1.1.3.4.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">original intent of FL</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.4" class="ltx_tr">
<span id="S4.T5.5.1.1.1.4.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S4.T5.5.1.1.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite></span>
<span id="S4.T5.5.1.1.1.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DRL to determine resource consumption by FL participants</span>
<span id="S4.T5.5.1.1.1.4.4" class="ltx_td ltx_border_r ltx_border_t"></span></span>
<span id="S4.T5.5.1.1.1.5" class="ltx_tr">
<span id="S4.T5.5.1.1.1.5.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S4.T5.5.1.1.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite></span>
<span id="S4.T5.5.1.1.1.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.5.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>, DRL for resource allocation with</span></span>
<span id="S4.T5.5.1.1.1.5.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">mobility-aware FL participants</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.5.1.1.1.5.4.1" class="ltx_text">
<span id="S4.T5.5.1.1.1.5.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.5.4.1.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">DRL models are difficult to train especially</span></span>
<span id="S4.T5.5.1.1.1.5.4.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.5.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">when the number of FL participants are large</span></span>
</span></span></span></span>
<span id="S4.T5.5.1.1.1.6" class="ltx_tr">
<span id="S4.T5.5.1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T5.5.1.1.1.6.1.1" class="ltx_text">
<span id="S4.T5.5.1.1.1.6.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.6.1.1.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.6.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Participant</span></span>
<span id="S4.T5.5.1.1.1.6.1.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.6.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Selection</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite></span>
<span id="S4.T5.5.1.1.1.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fair resource allocation to reduce variance of model performance</span>
<span id="S4.T5.5.1.1.1.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Convergence delays with more fairness</span></span>
<span id="S4.T5.5.1.1.1.7" class="ltx_tr">
<span id="S4.T5.5.1.1.1.7.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S4.T5.5.1.1.1.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite></span>
<span id="S4.T5.5.1.1.1.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.7.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Propose BAA to integrate computation and communication through</span></span>
<span id="S4.T5.5.1.1.1.7.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">exploiting the signal superposition property of multiple-access channel</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.7.4" class="ltx_td ltx_border_r ltx_border_t"></span></span>
<span id="S4.T5.5.1.1.1.8" class="ltx_tr">
<span id="S4.T5.5.1.1.1.8.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S4.T5.5.1.1.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite></span>
<span id="S4.T5.5.1.1.1.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.8.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Improves on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> by accounting for gradient vectors that are not</span></span>
<span id="S4.T5.5.1.1.1.8.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.8.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">transmitted due to power constraints</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.8.4" class="ltx_td ltx_border_r"></span></span>
<span id="S4.T5.5.1.1.1.9" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T5.5.1.1.1.9.1.1" class="ltx_text">
<span id="S4.T5.5.1.1.1.9.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.9.1.1.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Joint Radio and</span></span>
<span id="S4.T5.5.1.1.1.9.1.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Computation</span></span>
<span id="S4.T5.5.1.1.1.9.1.1.1.3" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Resource Management</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite></span>
<span id="S4.T5.5.1.1.1.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.9.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.9.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Improves on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> using the DC algorithm to minimize</span></span>
<span id="S4.T5.5.1.1.1.9.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">aggregation error</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.9.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.5.1.1.1.9.4.1" class="ltx_text">
<span id="S4.T5.5.1.1.1.9.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.9.4.1.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Signal distortion can lead to drop in accuracy,</span></span>
<span id="S4.T5.5.1.1.1.9.4.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">the scalability is also an issue when large</span></span>
<span id="S4.T5.5.1.1.1.9.4.1.1.3" class="ltx_tr">
<span id="S4.T5.5.1.1.1.9.4.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">heterogeneous networks are involved</span></span>
</span></span></span></span>
<span id="S4.T5.5.1.1.1.10" class="ltx_tr">
<span id="S4.T5.5.1.1.1.10.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S4.T5.5.1.1.1.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite></span>
<span id="S4.T5.5.1.1.1.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.10.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.10.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.10.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Asynchronous FL where model aggregation occurs whenever local</span></span>
<span id="S4.T5.5.1.1.1.10.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.10.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">updates are received by FL server</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.10.4.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.10.4.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.10.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Significant delay in convergence in non-IID</span></span>
<span id="S4.T5.5.1.1.1.10.4.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.10.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">and unbalanced dataset</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.11" class="ltx_tr">
<span id="S4.T5.5.1.1.1.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T5.5.1.1.1.11.1.1" class="ltx_text">
<span id="S4.T5.5.1.1.1.11.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.11.1.1.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.11.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Adaptive</span></span>
<span id="S4.T5.5.1.1.1.11.1.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.11.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Aggregation</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite></span>
<span id="S4.T5.5.1.1.1.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Adaptive global aggregation frequency based on resource constraints</span>
<span id="S4.T5.5.1.1.1.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.11.4.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.11.4.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.11.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Convergence guarantees are limited to restrictive</span></span>
<span id="S4.T5.5.1.1.1.11.4.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.11.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">assumptions</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.12" class="ltx_tr">
<span id="S4.T5.5.1.1.1.12.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="S4.T5.5.1.1.1.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite></span>
<span id="S4.T5.5.1.1.1.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.12.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.12.3.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Stackelberg game for incentivizing higher quantities of training data</span></span>
<span id="S4.T5.5.1.1.1.12.3.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.12.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">or compute resource contributed</span></span>
</span></span>
<span id="S4.T5.5.1.1.1.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T5.5.1.1.1.12.4.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.12.4.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.12.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FL server derives lower profits. Also, assumption</span></span>
<span id="S4.T5.5.1.1.1.12.4.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.12.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">that there is only one FL server</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.13" class="ltx_tr">
<span id="S4.T5.5.1.1.1.13.1" class="ltx_td ltx_border_l ltx_border_r"></span>
<span id="S4.T5.5.1.1.1.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite></span>
<span id="S4.T5.5.1.1.1.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Contract theoretic approach to incentivize FL participants</span>
<span id="S4.T5.5.1.1.1.13.4" class="ltx_td ltx_border_r ltx_border_t"></span></span>
<span id="S4.T5.5.1.1.1.14" class="ltx_tr">
<span id="S4.T5.5.1.1.1.14.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r"><span id="S4.T5.5.1.1.1.14.1.1" class="ltx_text">
<span id="S4.T5.5.1.1.1.14.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.5.1.1.1.14.1.1.1.1" class="ltx_tr">
<span id="S4.T5.5.1.1.1.14.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Incentive</span></span>
<span id="S4.T5.5.1.1.1.14.1.1.1.2" class="ltx_tr">
<span id="S4.T5.5.1.1.1.14.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Mechanism</span></span>
</span></span></span>
<span id="S4.T5.5.1.1.1.14.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></span>
<span id="S4.T5.5.1.1.1.14.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Reputation mechanism to select effective workers</span>
<span id="S4.T5.5.1.1.1.14.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S4.T5.5.1.1.1.14.4.1" class="ltx_text">Assumption that there is only one FL server</span></span></span>
</span></span></p>
</span></div>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Summary and Lessons Learned</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In this section, we have discussed four main issues in resource allocation. The issues and approaches are summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-D Incentive Mechanism ‣ IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. From this section, the lessons learned are as follows:</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">In heterogeneous mobile networks, the consideration of resource allocation is important to ensure efficient FL. For example, each training iteration is only conducted as quickly as the slowest FL participant, i.e., the straggler effect. In addition, the model accuracy is highly dependent on the quality of data used for training by FL participants. In this section, we have explored different dimensions of resource heterogeneity for consideration, e.g., varying computation and communication capabilities, willingness to participate, and quality of data for local model training. In addition, we have explored various tools that can be considered for resource allocation. For example, DRL is useful given the dynamic and uncertain wireless network conditions experienced by FL participants, whereas contract theory can serve as a powerful tool in mechanism design under the context of information asymmetry. Naturally, traditional optimization approaches have also been well explored in radio resource management for FL, given the high dependency on communications efficiency in FL.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">In Section <a href="#S3" title="III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, communication cost reduction comes with a sacrifice in terms of either higher computation costs or lower inference accuracy. Similarly, there exist different tradeoffs to be considered in resource allocation. A scalable model is thus one that enables customization to suit varying needs. For example, the study of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> allows the FL server to calibrate levels of fairness when allocating training importance, whereas the study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> enables the tradeoffs between training completion time and energy expense to be calibrated by the FL system adminstrator.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">In synchronous FL, the FL system is susceptible to the straggler effect. As such, asynchronous FL has been proposed as a solution in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>. In addition, asynchronous FL also allows participants to join the FL training halfway even while a training round is in progress. This is more reflective of practical FL settings and can be an important contributing factor towards ensuring the scalability of FL. However, synchronous FL remains to be the most common approach used due to convergence guarantees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. Given the many advantages of asynchronous FL, new asynchronous algorithms should be investigated. In particular, for future proposed algorithms, the convergence guarantee in a non-IID setting for non-convex loss functions needs to be considered.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">The study of incentive mechanism design is a particularly important aspect of FL. In particular, due to data privacy concerns, the FL servers are unable to check for training data quality. With the use of self-revealing mechanisms in contract theory, or through modeling the interactions between FL server and participants with game theoretic concepts, high quality data can be motivated as contributions from FL participants. However, existing studies in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> generally assume that a
federation enjoys a monopoly. In particular, each system model is assumed to only consist of multiple individual participants collaborating with a sole FL server. There can be exceptions to this setting as follows: (i) the participants may be competing data owners who are reluctant to share their model parameters since the competitors
also benefit from a trained global model and (ii) the FL servers
may compete with other FL servers, i.e., model owners.
In this case, the formulation of the incentive mechanism design
will be vastly different from that proposed. A relatively novel approach has been to model the regret <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> of each FL participants in joining the various competing federations for model training. For future works, a system model with multiple competing federations can be considered together with Stackelberg games and contract theoretic approaches.</p>
</div>
</li>
<li id="S4.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i5.p1" class="ltx_para">
<p id="S4.I2.i5.p1.1" class="ltx_p">In this section, we have assumed that FL assures the privacy and security of participants. However, as discussed in the following section, this assumption may not hold in the presence of malicious participants or FL server.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps"> Privacy and Security Issues</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">One of the main objectives of FL is to protect the privacy of participants, i.e., the participants only need to share parameters of the trained model instead of sharing their actual data. However, some recent research works have shown that privacy and security concerns may arise when the FL participants or FL servers are malicious in nature. In particular, this defeats the purpose of FL since the resulting global model can be corrupted, or the participants may even have their privacy compromised during model training. In this section, we discuss the following issues:</p>
</div>
<div id="S5.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Privacy</span>: Even though FL does not require the exchange of data for collaborative model training, a malicious participant can still infer sensitive information, e.g., gender, occupation, and location, from other participants based on their shared models. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>, when training a binary gender classifier on the FaceScrub <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite> dataset, the authors show that they can infer if a certain participant’s inputs are included in the dataset just from inspecting the shared model, with a very high accuracy of up to 90%. Thus, in this section, we discuss privacy issues related to the shared models in FL and review solutions proposed to preserve the privacy of participants.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Security</span>: In FL, the participants locally train the model and share trained parameters with other participants in order to improve the accuracy of prediction. However, this process is susceptible to a variety of attacks, e.g., data and model poisoning, in which a malicious participant can send incorrect parameters or corrupted models to falsify the learning process during global aggregation. Consequently, the global model will be updated incorrectly, and the whole learning system becomes corrupted. This section discusses more details on emerging attacks in FL as well as some recent countermeasures to deal with such attacks.</p>
</div>
</li>
</ul>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Privacy Issues</span>
</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS1.4.1.1" class="ltx_text">V-A</span>1 </span>Information exploiting attacks in machine learning - A brief overview</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">One of the first research works that shows the possibility of extracting information from a trained model is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>. In this paper, the authors show that during the training phase, the correlations implied in the training samples are gathered inside the trained model. Thus, if the trained model is released, it can lead to an unexpected information leakage to attackers. For example, an adversary can infer the ethnicity or gender of a user from its trained voice recognition system. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>, the authors develop a model-inversion algorithm which is very effective in exploiting information from decision tree-based or face recognition trained models. The idea of this approach is to compare the target feature vector with each of the possible value and then derive a weighted probability estimation which is the correct value. The experiment results reveal that by using this technique, the adversary can reconstruct an image of the victim’s face from its label with a very high accuracy.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">Recently, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite> show that it is even possible for an adversary to infer information of a victim through queries to the prediction model. In particular, this occurs when a malicious participant has the access to make prediction queries on a trained model. Then, the malicious participant can use the prediction queries to extract the trained model from the data owner. More importantly, the authors point out that this kind of attack can successfully extract model information from a wide range of training models such as decision trees, logistic regressions, SVMs, and even complex training models including DNNs. Some recent research works have also demonstrated the vulnerabilities of DNN-based training models against model extraction attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite>. Therefore, this raises a serious privacy concern for participants in sharing training models in FL.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS2.4.1.1" class="ltx_text">V-A</span>2 </span>Differential privacy-based protection solutions for FL participants</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">In order to protect the privacy of parameters trained by DNNs, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduce a technique, called <em id="S5.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">differentially private stochastic gradient descent</em>, which can be effectively implemented on DL algorithms. The key idea of this technique is to add some “noise” to the trained parameters by using a differential privacy-preserving randomized mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>, e.g., a Gaussian mechanism, before sending such parameters to the server. In particular, at the gradient averaging step of a normal FL participant, a Gaussian distribution is used to approximate the differentially private stochastic gradient descent. Then, during the training phase, the participant keeps calculating the probability that malicious participants can exploit information from its shared parameters. Once a predefined threshold is reached, the participant will stop its training process. In this way, the participant can mitigate the risk of revealing private information from its shared parameters.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">Inspired by this idea, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> develop an approach which can achieve a better privacy-protection solution for participants. In this approach, the authors propose two main steps to process data before sending trained parameters to the server. In particular, for each learning round, the aggregate server first selects a random number of participants to train the global model. Then, if a participant is selected to train the global model in a learning round, the participant will adopt the method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, i.e., using a Gaussian distribution to add noise to the trained model before sending the trained parameters to the server. In this way, a malicious participant cannot infer information of other participants by using the parameters of shared global model as it has no information regarding who has participated in the training process in each learning round.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS3.4.1.1" class="ltx_text">V-A</span>3 </span>Collaborative training solutions</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">While DP solutions can protect private information of a honest participant from other malicious participants in FL, they only work well if the server is trustful. If the server is malicious, it can result in a more serious privacy threat to all participants in the network. Thus, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite> introduce a collaborative DL framework to render multiple participants to learn the global model without uploading their explicit training models to the server. The key idea of this technique is that instead of uploading the whole set of trained parameters to the server and updating the whole global parameters to its local model, each participant wisely selects the number of gradients to upload and the number of parameters from the global model to update as illustrated in Fig. <a href="#S5.F12" title="Figure 12 ‣ V-A3 Collaborative training solutions ‣ V-A Privacy Issues ‣ V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. In this way, malicious participants cannot infer explicit information from the shared model. One interesting result of this paper is that even when the participants do not share all trained parameters and do not update all parameters from the shared model, the accuracy of proposed solution is still close to that of the case when the server has all dataset to train the global model. For example, for the MNIST dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, the accuracy of prediction model when the participants agree to share 10% and 1% of their parameters are respectively 99.14% and 98.71%, compared with 99.17% for the centralized solution when the server has full data to train. However, the approach is yet to be tested on more complex classification tasks.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/1909.11875/assets/x2.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="497" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S5.F12.4.2" class="ltx_text" style="font-size:90%;">Selective parameters sharing model.</span></figcaption>
</figure>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p">Although selective parameter sharing and DP solutions can make information exploiting attacks more challenging, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> show that these solutions are susceptible to a new type of attack, called powerful attack, developed based on Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>. GANs is a class of ML technique which uses two neural networks, namely generator network and discriminator network, that compete with each other to train data. The generator network tries to generate the fake data by adding some “noise” to the real data. Then, the generated fake data is passed to the discriminator network for classification. After the training process, the GANs can generate new data with the same statistics as the training dataset. Inspired by this idea, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> develop a powerful attack which allows a malicious participant to infer sensitive information from a victim participant even with just a part of shared parameters from the victim as illustrated in Fig. <a href="#S5.F13" title="Figure 13 ‣ V-A3 Collaborative training solutions ‣ V-A Privacy Issues ‣ V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. To deal with the GAN attack, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite> introduce a solution using secret sharing scheme with extreme boosting algorithm. This approach executes a lightweight secret sharing protocol before transmitting the newly trained model in plaintext to the server at each round. Thereby, other participants in the network cannot infer information from the shared model. However, the limitation of this approach is the reliance on a trusted third party to generate signature key pairs.</p>
</div>
<figure id="S5.F13" class="ltx_figure"><img src="/html/1909.11875/assets/x3.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S5.F13.4.2" class="ltx_text" style="font-size:90%;">GAN Attack on collaborative deep learning.</span></figcaption>
</figure>
<div id="S5.SS1.SSS3.p3" class="ltx_para">
<p id="S5.SS1.SSS3.p3.1" class="ltx_p">Different from all aforementioned works, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite> introduce a collaborative training model in which all participants cooperate to train a federated GANs model. The key idea of this method is that the federated GANs model can generate artificial data that can replace participants’ real data, and thus protecting the privacy of real data for the honest participants. In particular, in order to guarantee participants’ data privacy while still maintaining flexibility in training tasks, this approach produces a federated generative model. This model can output artificial data that does not belong to any real user in particular, but comes from the common cross-user data distribution. As a result, this approach can significantly reduce the possibility of malicious exploitation of information from real data. However, this approach inherits existing limitations of GANs, e.g., training instability due to the generated fake data, which can dramatically reduce the performance of collaborative learning models.</p>
</div>
</section>
<section id="S5.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS4.4.1.1" class="ltx_text">V-A</span>4 </span>Encryption-based Solutions</h4>

<div id="S5.SS1.SSS4.p1" class="ltx_para">
<p id="S5.SS1.SSS4.p1.1" class="ltx_p">Encryption is an effective way to protect data privacy of the participants when they want to share the trained parameters in FL. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, the homomorphic encryption technique is introduced to protect privacy of participants’ shared parameters from a honest-but-curious server. A honest-but-curious server is defined to be a user who wants to extract information from the participants’ shared parameters, but keeps all operations in FL in proper working condition. The idea of this solution is that the participants’ trained parameters will be encrypted using the homomorphic encryption technique before they are sent to the server. This approach is effective in protecting sensitive information from the curious server, and also achieves the same accuracy as that of the centralized DL algorithm. A similar concept is also presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> with secret sharing mechanism used to protect information of FL participants.</p>
</div>
<div id="S5.SS1.SSS4.p2" class="ltx_para">
<p id="S5.SS1.SSS4.p2.1" class="ltx_p">Although both the encryption techniques presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> can prevent the curious server from extracting information, they require multi-round communications and cannot preclude collusions between the server and participants. Thus, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> propose a hybrid solution which integrates both additively homomorphic encryption and DP in FL. In particular, before the trained parameters are sent to the server, they will be encrypted using the additively homomorphic encryption mechanism together with intentional noises to perturb the original parameters. As a result, this hybrid scheme can simultaneously prevent the curious server from exploiting information as well as solve the collusion problem between the server and malicious participants. However, in this paper, the authors do not compare the accuracy of the proposed approach with the case without homomorphic encryption and DP. Thus, the performance of proposed approach, i.e., in terms of model accuracy, is not clear.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Security Issues</span>
</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.4.1.1" class="ltx_text">V-B</span>1 </span>Data Poisoning Attacks</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">In FL, a participant trains its data and sends the trained model to the server for further processing. In this case, it is intractable for the server to check the real training data of a participant. Thus, a malicious participant can poison the global model by creating <em id="S5.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">dirty-label</em> data to train the global model with the aim of generating falsified parameters. For example, a malicious participant can generate a number of samples, e.g., photos, under a designed label, e.g., a clothing branch, and use them to train the global model to achieve its business goal, e.g., the prediction model shows results of the targeted clothing branch. Dirty-label data poisoning attacks are demonstrated to achieve high misclassifications in DL processes, up to 90%, when a malicious participant injects relatively few dirty-label samples (around 50) to the training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>. This calls for urgent solutions to deal with data poisoning attacks in FL.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>, the authors investigate impacts of a sybil-based data poisoning attack to a FL system. In particular, for the sybil attack, a malicious participant tries to improve the effectiveness of data poisoning in training the global model by creating multiple malicious participants. In Table <a href="#S5.T6" title="TABLE VI ‣ V-B1 Data Poisoning Attacks ‣ V-B Security Issues ‣ V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, the authors show that with only two malicious participants, the attack success rate can achieve up to 96.2%, and now the FL model is unable to correctly classify the image of “1” (instead it always incorrectly predicts them to be the image of “7”). To mitigate sybil attacks, the authors then propose a defense strategy, namely FoolsGold. The key idea of this approach is that honest participants can be distinguished from sybil participants based on their updated gradients. Specifically, in the non-IID FL setting, each participant’s training data has its own particularities, and sybil participants will contribute gradients that appear more similar to each other than those of other honest participants. With FoolsGold, the system can defend the sybil data poisoning attack with minimal changes to the conventional FL process and without requiring any auxiliary information outside of the learning process. Through simulations results on 3 diverse datasets (MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, KDDCup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>, Amazon Reviews <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>), the authors show that FoolsGold can mitigate the attack under a variety of conditions, including different distributions of participant data, varying poisoning targets, and various attack strategies.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T6.3.1.1" class="ltx_text" style="font-size:90%;">TABLE VI</span>: </span><span id="S5.T6.4.2" class="ltx_text" style="font-size:90%;">The accuracy and attack success rates for no-attack scenario and attacks with 1 and 2 sybils in a FL system with MNIST dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>.</span></figcaption>
<table id="S5.T6.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T6.5.1" class="ltx_tr">
<td id="S5.T6.5.1.1" class="ltx_td ltx_border_ll ltx_border_rr ltx_border_t"></td>
<td id="S5.T6.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.5.1.2.1" class="ltx_text ltx_font_bold">Baseline</span></td>
<td id="S5.T6.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.5.1.3.1" class="ltx_text ltx_font_bold">Attack 1</span></td>
<td id="S5.T6.5.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S5.T6.5.1.4.1" class="ltx_text ltx_font_bold">Attack 2</span></td>
</tr>
<tr id="S5.T6.5.2" class="ltx_tr">
<td id="S5.T6.5.2.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_rr ltx_border_tt">Number of honest participants</td>
<td id="S5.T6.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10</td>
<td id="S5.T6.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10</td>
<td id="S5.T6.5.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">10</td>
</tr>
<tr id="S5.T6.5.3" class="ltx_tr">
<td id="S5.T6.5.3.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_rr ltx_border_t">Number of sybil participants</td>
<td id="S5.T6.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T6.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T6.5.3.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">2</td>
</tr>
<tr id="S5.T6.5.4" class="ltx_tr">
<td id="S5.T6.5.4.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_rr ltx_border_t">The accuracy (digits: 0, 2-9)</td>
<td id="S5.T6.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.2%</td>
<td id="S5.T6.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.4%</td>
<td id="S5.T6.5.4.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">88.8%</td>
</tr>
<tr id="S5.T6.5.5" class="ltx_tr">
<td id="S5.T6.5.5.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_rr ltx_border_t">The accuracy (digit: 1)</td>
<td id="S5.T6.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.5%</td>
<td id="S5.T6.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.7%</td>
<td id="S5.T6.5.5.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.0%</td>
</tr>
<tr id="S5.T6.5.6" class="ltx_tr">
<td id="S5.T6.5.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_ll ltx_border_rr ltx_border_t"><span id="S5.T6.5.6.1.1" class="ltx_text ltx_font_bold">Attack success rate</span></td>
<td id="S5.T6.5.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">0.0%</td>
<td id="S5.T6.5.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">35.9%</td>
<td id="S5.T6.5.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">96.2%</td>
</tr>
</table>
</figure>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.4.1.1" class="ltx_text">V-B</span>2 </span>Model Poisoning Attacks</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Unlike data poisoning attacks which aim to generate fake data to cause adverse impacts to the global model, a model poisoning attack attempts to directly poison the global model that it sends to the server for aggregation. As shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>, model poisoning attacks are much more effective than those of data poisoning attacks, especially for large-scale FL with many participants. The reason is that for data poisoning attacks, a malicious participant’s updates are scaled based on its dataset and the number of participants in the federation. However, for model poisoning attacks, a malicious participant can modify the updated model, which is sent to the server for aggregation, directly. As a result, even with one single attacker, the whole global model can be poisoned. The simulation results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> also confirm that even a highly constrained adversary with limited training data can achieve high success rate in performing model poisoning attacks. Thus, solutions to protect the global model from model poisoning attacks have to be developed.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite>, some solutions are suggested to prevent model poisoning attacks. Firstly, based on an updated model shared from a participant, the server can check whether the shared model can help to improve the global model’s performance or not. If not, the participant will be marked to be a potential attacker, and after few rounds of observing the updated model from this participant, the server can determine whether this is a malicious participant or not. The second solution is based on the comparison among the updated models shared by the participants. In particular, if an updated model from a participant is too different from the others, the participant can potentially be a malicious one. Then, the server will continue observing updates from this participant before it can determine whether this is a malicious user or not. However, model poisoning attacks are extremely difficult to prevent because when training with millions of participants, it is intractable to evaluate the improvement from every single participant. As such, more effective solutions need to be further investigated.</p>
</div>
<div id="S5.SS2.SSS2.p3" class="ltx_para">
<p id="S5.SS2.SSS2.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>, the authors introduce a more effective model poisoning attack which is demonstrated to achieve 100% accuracy on the attacker’s task within just a single learning round. In particular, a malicious participant can share its poisoned model which not only is trained for its intentional purpose, but which also contains a backdoor function. In this paper, the authors consider to use a semantic backdoor function to inject into the global model. The reason is that this function can make the global model misclassify even without a need to modify the input data of the malicious participant. For example, an image classification backdoor function can inject an attacker-chosen label to all images with some certain features, e.g., all dogs with black stripes can be misclassifed to be cats. In the simulations, the authors show that this attack can greatly outperform other conventional FL data poisoning attacks. For example, in a word-prediction task with 80,000 total participants, compromising just eight of them is enough to achieve 50% backdoor accuracy, as compared to 400 malicious participants needed to perform the data-poisoning attack.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS3.4.1.1" class="ltx_text">V-B</span>3 </span>Free-Riding Attacks</h4>

<figure id="S5.F14" class="ltx_figure"><img src="/html/1909.11875/assets/x4.png" id="S5.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S5.F14.4.2" class="ltx_text" style="font-size:90%;">An illustration of (a) conventional FL and (b) the proposed BlockFL architectures.</span></figcaption>
</figure>
<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">Free-riding is another attack in FL that occurs when a participant wants to benefit from the global model without contributing to the learning process. The malicious participant, i.e., free-rider, can pretend that it has very small number of samples to train or it can select a small set of its real dataset to train, e.g., to save its resources. As a result, the honest participants need to contribute more resources in the FL training process. To address this problem, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> introduce a blockchain-based FL architecture, called BlockFL, in which the participants’ local learning model updates are exchanged and verified by leveraging the blockchain technology. In particular, each participant trains and sends the trained global model to its associated miner in the blockchain network and then receives a reward that is proportional to the number of trained data samples as illustrated in Fig. <a href="#S5.F14" title="Figure 14 ‣ V-B3 Free-Riding Attacks ‣ V-B Security Issues ‣ V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. In this way, this framework can not only prevent the participants from free-riding, but also incentivize all participants to contribute to the learning process. A similar blockchain-based model is also introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> to provide data confidentiality, computation auditability, and incentives for the participants of FL. However, the utilization of the blockchain technology implies the incurrence of a significant cost for implementing and maintaining miners to operate the blockchain network. Furthermore, consensus protocols used in blockchain networks, e.g., proof-of-work (PoW), can cause a long delay in information exchange, and thus they may not be appropriate to implement on FL models.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T7.3.1.1" class="ltx_text" style="font-size:90%;">TABLE VII</span>: </span><span id="S5.T7.4.2" class="ltx_text" style="font-size:90%;">A summary of attacks and countermeasures in FL.</span></figcaption>
<table id="S5.T7.5" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.5.1" class="ltx_tr">
<td id="S5.T7.5.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;background-color:#999999;"></td>
<td id="S5.T7.5.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:128.0pt;background-color:#999999;"></td>
<td id="S5.T7.5.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:298.8pt;background-color:#999999;"></td>
</tr>
<tr id="S5.T7.5.2" class="ltx_tr">
<td id="S5.T7.5.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:56.9pt;background-color:#999999;">
<span id="S5.T7.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.2.1.1.1" class="ltx_p" style="background-color:#999999;"><span id="S5.T7.5.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Attack Types</span></span>
</span>
</td>
<td id="S5.T7.5.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:128.0pt;background-color:#999999;">
<span id="S5.T7.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.2.2.1.1" class="ltx_p" style="background-color:#999999;"><span id="S5.T7.5.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Attack Method</span></span>
</span>
</td>
<td id="S5.T7.5.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:298.8pt;background-color:#999999;">
<span id="S5.T7.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.2.3.1.1" class="ltx_p" style="background-color:#999999;"><span id="S5.T7.5.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Countermeasures</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.5.3" class="ltx_tr">
<td id="S5.T7.5.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_tt" style="width:56.9pt;">
<span id="S5.T7.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.3.1.1.1" class="ltx_p"><span id="S5.T7.5.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Information exploiting attacks (privacy issues)</span></span>
</span>
</td>
<td id="S5.T7.5.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:128.0pt;">
<span id="S5.T7.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.3.2.1.1" class="ltx_p"><span id="S5.T7.5.3.2.1.1.1" class="ltx_text" style="font-size:70%;">Attackers try to illegally exploit information from the shared model.</span></span>
</span>
</td>
<td id="S5.T7.5.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:298.8pt;">
<span id="S5.T7.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.I2" class="ltx_itemize">
<span id="S5.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i1.p1" class="ltx_para">
<span id="S5.I2.i1.p1.1" class="ltx_p"><em id="S5.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Differentially private stochastic gradient descent</em><span id="S5.I2.i1.p1.1.2" class="ltx_text" style="font-size:70%;">: Add “noise” to the trained parameters by using a differential privacy-preserving randomized mechanism </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I2.i1.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S5.I2.i1.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I2.i1.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
<span id="S5.I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i2.p1" class="ltx_para">
<span id="S5.I2.i2.p1.1" class="ltx_p"><em id="S5.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Differentially private and selective participants</em><span id="S5.I2.i2.p1.1.2" class="ltx_text" style="font-size:70%;">: Add “noise” to the trained parameters and select randomly participants to train global model in each round </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I2.i2.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib161" title="" class="ltx_ref">161</a><span id="S5.I2.i2.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I2.i2.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
<span id="S5.I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i3.p1" class="ltx_para">
<span id="S5.I2.i3.p1.1" class="ltx_p"><em id="S5.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Selective parameter sharing</em><span id="S5.I2.i3.p1.1.2" class="ltx_text" style="font-size:70%;">: Each participant wisely selects the number of gradients to upload and the number of parameters from the global model to update </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I2.i3.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib162" title="" class="ltx_ref">162</a><span id="S5.I2.i3.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I2.i3.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
<span id="S5.I2.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i4.p1" class="ltx_para">
<span id="S5.I2.i4.p1.1" class="ltx_p"><em id="S5.I2.i4.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Secrete sharing scheme with extreme boosting algorithm</em><span id="S5.I2.i4.p1.1.2" class="ltx_text" style="font-size:70%;">: This approach executes a lightweight secret sharing protocol before transmitting the newly trained model in plaintext to the server at each round </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I2.i4.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib166" title="" class="ltx_ref">166</a><span id="S5.I2.i4.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I2.i4.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
<span id="S5.I2.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i5.p1" class="ltx_para">
<span id="S5.I2.i5.p1.1" class="ltx_p"><em id="S5.I2.i5.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">GAN model training</em><span id="S5.I2.i5.p1.1.2" class="ltx_text" style="font-size:70%;">: All participants are cooperative to train a federated GANs model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I2.i5.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib167" title="" class="ltx_ref">167</a><span id="S5.I2.i5.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I2.i5.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T7.5.4" class="ltx_tr">
<td id="S5.T7.5.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;">
<span id="S5.T7.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.4.1.1.1" class="ltx_p"><span id="S5.T7.5.4.1.1.1.1" class="ltx_text" style="font-size:70%;">Data poisoning attacks</span></span>
</span>
</td>
<td id="S5.T7.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:128.0pt;">
<span id="S5.T7.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.4.2.1.1" class="ltx_p"><span id="S5.T7.5.4.2.1.1.1" class="ltx_text" style="font-size:70%;">Attackers poison the global model by creating </span><em id="S5.T7.5.4.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">dirty-label</em><span id="S5.T7.5.4.2.1.1.3" class="ltx_text" style="font-size:70%;"> data and use such data to train the global model.</span></span>
</span>
</td>
<td id="S5.T7.5.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:298.8pt;">
<span id="S5.T7.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.I3" class="ltx_itemize">
<span id="S5.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I3.i1.p1" class="ltx_para">
<span id="S5.I3.i1.p1.1" class="ltx_p"><em id="S5.I3.i1.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">FoolsGoal</em><span id="S5.I3.i1.p1.1.2" class="ltx_text" style="font-size:70%;">: Distinguish honest participants based on their updated gradients. It is based on the fact that in the non-IID FL setting, each participant’s training data has its own particularities, and malicious participants will contribute gradients that appear more similar to each other than those of the honest participants </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I3.i1.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib171" title="" class="ltx_ref">171</a><span id="S5.I3.i1.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I3.i1.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T7.5.5" class="ltx_tr">
<td id="S5.T7.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;">
<span id="S5.T7.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.5.1.1.1" class="ltx_p"><span id="S5.T7.5.5.1.1.1.1" class="ltx_text" style="font-size:70%;">Model poisoning attacks</span></span>
</span>
</td>
<td id="S5.T7.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:128.0pt;">
<span id="S5.T7.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.5.2.1.1" class="ltx_p"><span id="S5.T7.5.5.2.1.1.1" class="ltx_text" style="font-size:70%;">Attackers attempt to directly poison the global model that they send to the server for aggregation.</span></span>
</span>
</td>
<td id="S5.T7.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:298.8pt;">
<span id="S5.T7.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.I4" class="ltx_itemize">
<span id="S5.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i1.p1" class="ltx_para">
<span id="S5.I4.i1.p1.1" class="ltx_p"><span id="S5.I4.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Based on an updated model shared from a participant, the server can check whether the shared model can help to improve the global model’s performance or not. If not, the participant will be marked to be a potential attacker </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I4.i1.p1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib173" title="" class="ltx_ref">173</a><span id="S5.I4.i1.p1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I4.i1.p1.1.4" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
<span id="S5.I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i2.p1" class="ltx_para">
<span id="S5.I4.i2.p1.1" class="ltx_p"><span id="S5.I4.i2.p1.1.1" class="ltx_text" style="font-size:70%;">Compare among the updated global models shared by the participants, and if an updated global model from a participant is too different from others, it could be a potential malicious participant </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I4.i2.p1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib173" title="" class="ltx_ref">173</a><span id="S5.I4.i2.p1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I4.i2.p1.1.4" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T7.5.6" class="ltx_tr">
<td id="S5.T7.5.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;">
<span id="S5.T7.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.6.1.1.1" class="ltx_p"><span id="S5.T7.5.6.1.1.1.1" class="ltx_text" style="font-size:70%;">Free-riding attacks</span></span>
</span>
</td>
<td id="S5.T7.5.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:128.0pt;">
<span id="S5.T7.5.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.5.6.2.1.1" class="ltx_p"><span id="S5.T7.5.6.2.1.1.1" class="ltx_text" style="font-size:70%;">Attackers benefit from the global model without contributing to the learning process, e.g., by pretending that they have very small number of samples to train.</span></span>
</span>
</td>
<td id="S5.T7.5.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:298.8pt;">
<span id="S5.T7.5.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.I5" class="ltx_itemize">
<span id="S5.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I5.i1.p1" class="ltx_para">
<span id="S5.I5.i1.p1.1" class="ltx_p"><em id="S5.I5.i1.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">BlockFL</em><span id="S5.I5.i1.p1.1.2" class="ltx_text" style="font-size:70%;">: Participants’ local learning model updates are exchanged and verified by leveraging blockchain technology. In particular, each participant trains and sends the trained global model to its associated miner in the blockchain network and then receives a reward that is proportional to the number of trained data samples </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.I5.i1.p1.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib175" title="" class="ltx_ref">175</a><span id="S5.I5.i1.p1.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.I5.i1.p1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span></span>
</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Summary and Lessons Learned</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this section, we have discussed two key issues, i.e., privacy and security, when trained models are exchanged in FL. In general, it is believed that FL is an effective privacy-preserving learning solution for participants to perform collaborative model training. However, in this section, we have shown that a malicious participant can exploit the process and gain access to sensitive information of other participants. Furthermore, we have also shown that by using the shared model in FL, an attacker can perform attacks which can not only breakdown the whole learning system, but also falsify the trained model to achieve its malicious goal. In addition, solutions to deal with these issues have also been reviewed, which are especially important in order to guide FL system administrators in designing and implementing the appropriate countermeasures. We summarize the key information of attacks and their corresponding countermeasures in Table VII.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Applications of Federated Learning for Mobile Edge Computing</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In the aforementioned studies, we have discussed the issues pertaining to the implementation of FL as an enabling technology that allows collaborative learning at mobile edge networks. In this section, we focus instead on the applications of FL for mobile edge network optimization.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">As highlighted by the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, the increasing complexity and heterogeneity of wireless networks enhance the appeal of adopting a data-driven ML based approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for optimizing system designs and resource allocation decision making for mobile edge networks. However, as discussed in previous sections, the private data of users may be sensitive in nature. As such, existing learning based approach can be combined with FL for privacy-preserving applications.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In this section, we consider four applications of FL in edge computing:</p>
</div>
<div id="S6.p4" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Cyberattack Detection:</span> The ubiquity of IoT devices and increasing sophistication of cyberattacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> imply that there is a need to improve existing cyberattack detection tools. Recently, DL has been widely successful in cyberattack detection. Coupled with FL, cyberattack detection models can be learned collaboratively while maintaining user privacy.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Edge Caching and Computation Offloading:</span> Given the computation and storage capacity constraints of edge servers, some computationally intensive tasks of end devices have to be offloaded to the remote cloud server for computation. In addition, commonly requested files or services should be placed on edge servers for faster retrieval, i.e., users do not have to communicate with the remote cloud when they want to access these files or services. As such, an optimal caching and computation offloading scheme can be collaboratively learned and optimized with FL.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Base Station Association:</span> In a dense network, it is important to optimize base station association so as to limit interference faced by users. However, traditional learning based approaches that utilize user data often assume that such data is centrally available. Given user privacy constraints, an FL based approach can be adopted instead.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Vehicular Networks:</span> The Internet of Vehicles (IoV) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> features smart vehicles with data collection, computation and communication capabilities for relevant functions, e.g., navigation and traffic management. However, this wealth of knowledge is again, private and sensitive in nature since it can reveal the driver’s location and personal information. In this section, we discuss the use of an FL based approach in traffic queue length prediction and energy demand in electric vehicle charging stations done at the edge of IoV networks.</p>
</div>
</li>
</ul>
</div>
<figure id="S6.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T8.3.1.1" class="ltx_text" style="font-size:90%;">TABLE VIII</span>: </span><span id="S6.T8.4.2" class="ltx_text" style="font-size:90%;">FL based approaches for mobile edge network optimization. </span></figcaption>
<table id="S6.T8.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T8.5.1" class="ltx_tr">
<td id="S6.T8.5.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T8.5.1.1.1" class="ltx_text ltx_font_bold">Applications</span></td>
<td id="S6.T8.5.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T8.5.1.2.1" class="ltx_text ltx_font_bold">Ref.</span></td>
<td id="S6.T8.5.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T8.5.1.3.1" class="ltx_text ltx_font_bold">Description</span></td>
</tr>
<tr id="S6.T8.5.2" class="ltx_tr">
<td id="S6.T8.5.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S6.T8.5.2.1.1" class="ltx_text">Cyberattack Detection</span></td>
<td id="S6.T8.5.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite></td>
<td id="S6.T8.5.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Cyberattack detection with edge nodes as participants</td>
</tr>
<tr id="S6.T8.5.3" class="ltx_tr">
<td id="S6.T8.5.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite></td>
<td id="S6.T8.5.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Cyberattack detection with IoT gateways as participants</td>
</tr>
<tr id="S6.T8.5.4" class="ltx_tr">
<td id="S6.T8.5.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite></td>
<td id="S6.T8.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Blockchain to store model updates</td>
</tr>
<tr id="S6.T8.5.5" class="ltx_tr">
<td id="S6.T8.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span id="S6.T8.5.5.1.1" class="ltx_text">Edge caching and computation offloading</span></td>
<td id="S6.T8.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></td>
<td id="S6.T8.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DRL for caching and offloading in UEs</td>
</tr>
<tr id="S6.T8.5.6" class="ltx_tr">
<td id="S6.T8.5.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite></td>
<td id="S6.T8.5.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DRL for computation offloading in IoT devices</td>
</tr>
<tr id="S6.T8.5.7" class="ltx_tr">
<td id="S6.T8.5.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite></td>
<td id="S6.T8.5.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Stacked autoencoder learning for proactive caching</td>
</tr>
<tr id="S6.T8.5.8" class="ltx_tr">
<td id="S6.T8.5.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite></td>
<td id="S6.T8.5.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Greedy algorithm to optimize service placement schemes</td>
</tr>
<tr id="S6.T8.5.9" class="ltx_tr">
<td id="S6.T8.5.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T8.5.9.1.1" class="ltx_text">Base station assoication</span></td>
<td id="S6.T8.5.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite></td>
<td id="S6.T8.5.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep echo state networks for VR application</td>
</tr>
<tr id="S6.T8.5.10" class="ltx_tr">
<td id="S6.T8.5.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></td>
<td id="S6.T8.5.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Mean field game with imitation for cell association</td>
</tr>
<tr id="S6.T8.5.11" class="ltx_tr">
<td id="S6.T8.5.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T8.5.11.1.1" class="ltx_text">Vehicular networks</span></td>
<td id="S6.T8.5.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite></td>
<td id="S6.T8.5.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Extreme value theory for large queue length prediction</td>
</tr>
<tr id="S6.T8.5.12" class="ltx_tr">
<td id="S6.T8.5.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite></td>
<td id="S6.T8.5.12.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Energy demand learning in electric vehicular networks</td>
</tr>
</table>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Cyberattack Detection</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Cyberattack detection is one of the most important steps to promptly prevent and mitigate serious consequences of attacks in mobile edge networks. Among different approaches to detect cyberattacks, DL is considered to be the most effective tool to detect a wide range of attacks with high accuracy. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>, the authors show that DL can outperform all conventional ML techniques with very high accuracy in detecting intrusions on three datasets, i.e., KDDcup 1999, NSL-KDD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, and UNSW-NB15 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>. However, the detection accuracy of solutions based on DL depends very much on the available datasets. Specifically, DL algorithm only can outperform other ML techniques when given sufficient data to train. However, this data may be sensitive in nature. Therefore, some FL-based attack detection models for mobile edge networks have been introduced recently to address this problem.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>, the authors propose a cyberattack detection model for an edge network empowered by FL. In this model, each edge node operates as a participant who owns a set of data for intrusion detection. To improve the accuracy in detecting attacks, after training the global model, each participant will send its trained model to the FL server. The server will aggregate all parameters from the participants and send the updated global model back to all the participants as illustrated in Fig. <a href="#S6.F15" title="Figure 15 ‣ VI-A Cyberattack Detection ‣ VI Applications of Federated Learning for Mobile Edge Computing ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. In this way, each edge node can learn from other edge nodes without a need of sharing its real data. As a result, this method can not only improve accuracy in detecting attacks, but also enhance the privacy of intrusion data at the edge nodes and reduce traffic load for the whole network. A similar idea is also presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> in which IoT gateways operate as FL participants and an IoT security service provider works as a server node to aggregate trained models shared by the participants. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> show empirically that by using FL, the system can successfully detect 95.6% of attacks in approximately 257 ms without raising any false alarm when evaluated in a real-world smart home deployment setting.</p>
</div>
<figure id="S6.F15" class="ltx_figure"><img src="/html/1909.11875/assets/x5.png" id="S6.F15.g1" class="ltx_graphics ltx_centering ltx_img_square" width="231" height="277" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S6.F15.4.2" class="ltx_text" style="font-size:90%;">FL-based attack detection architecture for IoT edge networks.</span></figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">In both <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>, it is assumed that the participants, i.e., edge nodes and IoT gateways, are honest, and they are willing to contribute in training their updated model parameters. However, if some of the participants are malicious, they can make the whole intrusion detection corrupted. Thus, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> propose to use blockchain technology in managing data shared by the participants. By using the blockchain, all incremental updates to the anomaly detection ML model are stored in the ledger, and thus a malicious participant can be easily identified. Furthermore, based on shared models from honest participants stored in the ledger, the intrusion detection system can easily recover the proper global model if the current global model is poisoned.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Edge Caching and Computation Offloading</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">To account for the dynamic and time-varying conditions in a MEC system, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> propose the use of DRL with FL to optimize caching and computation offloading decisions in an MEC system. The MEC system consists of a set of user equipments (UEs) covered by base stations. For caching, the DRL agent makes the decision to cache or not to cache the downloaded file, and which local file to replace should caching occur. For computation offloading, the UEs can choose to either offload
computation tasks to the edge node via wireless channels,
or perform the tasks locally. This caching and offloading decision process is illustrated in Fig. <a href="#S6.F16" title="Figure 16 ‣ VI-B Edge Caching and Computation Offloading ‣ VI Applications of Federated Learning for Mobile Edge Computing ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. The states of the MEC system include wireless network conditions,
UE energy consumption, and task queuing states, whereas the reward function is
defined as quality of experience (QoE) of the UEs. Given the large state and action space in the MEC environment, a DDQN approach is adopted. To protect the privacy of users, an FL approach is proposed in which training can occur with data remaining on the UEs. In addition, existing FL algorithms, e.g., <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, can also ensure that training is robust to the unbalanced and non-IID data of the UEs. The simulation
results show that the DDQN with FL approach achieves similar average
utilities among UEs as compared to the centralized DDQN
approach, while consuming less communication resources and
preserving user privacy. However, the simulations are only performed with <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mn id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><cn type="integer" id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">10</annotation></semantics></math> UEs. If the implementation is expanded to target a larger number of heterogeneous UEs, there can be significant delays in the training process especially since the training of a DRL model is computationally intensive. As an extension, transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite> can be used to increase the efficiency of training, i.e., training is not initialized from scratch.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> propose the use of DRL in optimizing computation offloading decisions in IoT systems. The system model consists of IoT devices and edge nodes. The IoT devices can harvest energy units <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> from the edge nodes to be stored in the energy queue. In addition, an IoT device also maintains a local task queue with unprocessed and unsuccessfully processed tasks. These tasks can be processed locally or offloaded to the edge nodes for processing, in a First In First Out (FIFO) order <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In the DRL problem formulation, the network states are defined to be a function of energy queue length, task execution delay, task handover delay from edge node association, and channel gain between the IoT device and edge nodes. A task can fail to be executed, e.g., when there is insufficient energy units or communication bandwidth for computation offloading. The utility considered is a function of task execution delay, task queuing delay, number of failed tasks and penalty of execution failure. The DRL agent makes the decision to either offload computation to the edge nodes or perform computation locally. To ensure privacy of users, the agent is trained without users having to upload their own data to a centralized server. In each training round, a random set of IoT devices are selected to download the model parameters of the DRL agent from the edge networks. The model parameters are then updated using their own data, e.g., energy resource level, channel gain, and local sensing data. Then, the updated parameters of the DRL agent are sent to the edge nodes for model aggregation. The simulation results show that the FL based approach can achieve same levels of total utility as the centralized DRL approach. This is robust to varying task generation probabilities. In addition, when task generation probabilities are higher, i.e., there are more tasks for computation in the IoT device, the FL based scheme can achieve a lower number of dropped tasks and shorter queuing delay than the centralized DRL scheme. However, the simulation only involves <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mn id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><cn type="integer" id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">15</annotation></semantics></math> IoT devices serviced by relatively many edge nodes. To better reflect practical scenarios where fewer edge nodes have to cover several IoT devices, further studies can be conducted on optimizing the edge-IoT collaboration. For example, the limited communication bandwidth can cause significant task handover delay during computation offloading. In addition, with more IoT devices, the DRL training will take a longer time to converge especially since the devices have heterogeneous computation capabilities.</p>
</div>
<figure id="S6.F16" class="ltx_figure"><img src="/html/1909.11875/assets/Figs/caching.png" id="S6.F16.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="747" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F16.3.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="S6.F16.4.2" class="ltx_text" style="font-size:90%;">FL-based (a) caching and (b) computation offloading.</span></figcaption>
</figure>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Instead of using a DRL approach, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> propose the use of an FL based stacked autoencoder learning model, i.e., FL based proactive content caching scheme (FPCC), to predict content popularity for optimized caching while protecting user privacy. In the system model, each user is equipped with a mobile device that connects to the base station that covers its geographical location. Using a stacked autoencoder learning model, the latent representation of a user’s information, e.g., location, and file rating, i.e., content request history, is learned. Then, a similarity matrix between the user and its historically requested files is obtained in which each element of the matrix represents the distance between the user and the file. Based on this similarity matrix, the <math id="S6.SS2.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.SS2.p3.1.m1.1a"><mi id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><ci id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">K</annotation></semantics></math> nearest neighbours of each user are determined, and the similarity between the user’s historical watch list and the neighbours’ are computed. An aggregation approach is then used to predict the most popular files for caching, i.e., files with highest similarity scores across all users. Being the most popular files across users that are most frequently retrieved, the cached files need not be re-downloaded from its source server everytime it is demanded. To protect the privacy of users, FL is adopted to learn the parameters of the stacked autoencoder without the user having to reveal its personal information or its content request history to the FL server. In each training round, the user first downloads a global model from the FL server. Then, the model is trained and updated using their local data. The updated models are subsequently uploaded to the FL server and aggregated using the <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm. The simulation results show that the proposed FPCC scheme could achieve the highest cache efficiency, i.e, the ratio of cached files matching user requests, as compared to other caching methods such as the Thompson sampling methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite>. In addition, privacy of the user is preserved.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> introduce a privacy-aware service placement scheme to deploy user-preferred services on edge servers with consideration for resource constraints in the edge cloud. The system model consists of a mobile edge cloud serving various mobile devices. The user’s preference model is first built based on information such as number of times of requests for a service, and other user context information, e.g., ages and locations. However, since this can involve sensitive personal information, an FL based approach is proposed to train the preference model while keeping users’ data on their personal devices. Then, an optimization problem is formulated in which the objective is to maximize quantity of services demanded from the edge based on user preferences, subject to constraints of storage capacity, computation capability, uplink and downloading bandwidth. The optimization problem is then solved using a greedy algorithm, i.e., the service which most improves the objective function is added till resource constraints are met. The simulation results show that the proposed scheme can outperform the popular service placement scheme, i.e., where only the most popular services are placed on the edge cloud, in terms of number of requests processed on edge clouds since it also considers the aforementioned resource constraints in maximizing quantity of services.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic">Base Station Association</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> propose an FL based deep echo state networks (ESNs) approach to minimize breaks in presence (BIPs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> for users of virtual reality (VR) applications. A BIP event can be a result of delay in information transmission which can be caused when the user’s body movements obstruct the wireless link. BIPs cause the user to be aware that they are in a virtual environment, thus reducing their quality of experience. As such, a user association policy has to be designed such that BIPs are minimized. The system model consists of base stations that cover a set of VR users. The base stations receive uploaded tracking information from each associated user, e.g., physical location and orientation, while the users download VR videos for their use in the VR application. For data transmission, the VR users have to associate with one of the base stations. As such, a minimization problem is formulated where BIPs are minimized with respect to expected locations and orientations of the VR user. To derive a prediction of user locations and orientations, the base station has to rely on the historical information of users. However, the historical information stored at each base station only collects partial data from each user, i.e., a user connects to multiple base stations and its data is distributed across them. As such, an FL based approach is implemented whereby each base station first trains a local model using its partial data. Then, the local models are aggregated to form a global model capable of generalization, i.e., comprehensively predicting a user’s mobility and orientations. The simulation results show that the federated ESN algorithm can achieve lower BIPs experienced by users as compared to the centralized ESN algorithm proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite>, since a centralized approach only makes partial prediction with the incomplete data from sole base stations, whereas the federated ESN approach can make predictions based on a model learned collaboratively from more complete data.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">Following the ubiquity of IoT devices, the traditional cloud-based approach may no longer be sufficient to cater to dense cellular networks. As computation and storage moves to the edge of networks, the association of users to base stations are increasingly important to facilitate efficient ML model training among the end users. To this end, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> consider solving the problem of cell association in dense wireless networks with a collaborative learning approach.
In the system model, the base stations cover a set of users in an LTE cellular system. In a cellular system, users are likely to face similar channel conditions as their neighbors and thus can benefit from learning from their neighbours that are already associated with base stations. As such, the cell association problem is formulated as a mean-field game (MFG) with imitation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> in which each user maximizes its own throughput while minimizing the cost of imitation.
The MFG is further
reduced into a single-user Markov decision process that is then solved
by a neural Q-learning algorithm. In
most other proposed solution for cell association, it is assumed that
all information is known to the base stations and users. However, given
privacy concerns, the assumption of information sharing may
not be practical. As such, a collaborative learning approach can be considered where only the outcome of the learning algorithm is exchanged
during the learning process whereas usage data
is kept locally in each user’s own device. The simulation results show that imitating users can attain higher utility within a shorter
training duration as compared to non-imitating users.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS4.4.1.1" class="ltx_text">VI-D</span> </span><span id="S6.SS4.5.2" class="ltx_text ltx_font_italic">Vehicular Networks</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Ultra reliable low latency communication (URLLC) in vehicular networks
is an essential prerequisite towards developing an intelligent transport system.
However, existing radio resource management techniques do
not account for rare events such as large queue lengths at
the tail-end distribution. To model the occurrence of such low
probability events, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> propose the use of extreme value theory (EVT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>. The approach requires
sufficient samples of queue state information (QSI) and data
exchange among vehicles. As such, an FL approach is proposed in which
vehicular users (VUEs) train the learning model with data kept locally and
upload only their updated model parameters to the roadside units (RSU). The
RSU then averages out the model parameters and return an updated
global model to the VUEs. In a synchronous approach, all
VUEs upload their models at the end of a prespecified interval. However, the simultaneous uploading by multiple vehicles can lead to delays in communication.
In contrast for an asynchronous approach, each VUE only
evaluates and uploads their model parameters after a predefined number
of QSI samples are collected. The global model is also updated whenever a local update is received, thus reducing communication delays. To further reduce overhead,
Lyapunov optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite> for power allocation is also utilized. The
simulation results show that under this framework, there is
a reduction of the number of vehicles experiencing large queue lengths
whereas FL can ensure minimal data exchange relative to a
centralized approach.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">Apart from QSI, the vehicles in vehicular networks are also exposed to a wealth of useful captured images that can be adopted to build better inference models, e.g., for traffic optimization. However, these images are sensitive in nature since they can give away the location information of vehicular clients. As such, an FL approach can be used to facilitate collaborative ML while ensuring privacy preservation. However, the images captured by vehicles are often varying in quality due to motion blurs. In addition, another source of heterogeneity is the difference in computing capabilities of vehicles. Given the information asymmetry involved, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> propose a multi-dimensional contract design in which the FL server designs contract bundles comprising varying levels of data quality, compute resources, and contractual payoffs. Then, the vehicular client chooses the contract bundle that maximizes its utility, in accordance to its hidden type. Similar to the results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite>, the simulation results show that the FL server derives greatest utility under the proposed contract theoretic approach, in contrast to the linear pricing or Stackelberg game approach.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite> propose a federated energy demand learning (FEDL) approach to manage energy resources in charging stations (CSs) for electric vehicles (EVs). When a large number of EVs congregate at a CS, this can lead to energy transfer congestion. To resolve this, energy is supplied from the power grids and reserved in advance to meet the real-time demands from the EVs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>, rather than having the CSs request for energy from the power grid only upon receiving charging requests. As such, there is a need to forecast energy demand for EV networks using historical charging data. However, this data is usually stored separately at each of the CS that the EVs utilize and is private in nature. As such, an FEDL approach is adopted in which each CS trains the demand prediction model on its own dataset before sending only the gradient information to the charging station provider (CSP). Then, the gradient information from the CS is aggregated for global model training. To further improve model accuracy, the CSs are clustered using the constrained K-means algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite> based on their physical locations. The clustering-based FEDL reduces the cost of biased prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>. The simulation results show that the root mean squared error of a clustered FEDL model is lower than conventional ML algorithms, e.g., multi-layer perceptron regressor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite>. However, the privacy of user data is still not protected by this approach, since user data is stored in each of the CS. As an extension, the user data can possibly be stored in each EVs separately, and model training can be conducted in the EVs rather than the CSs. This can allow more user features to be considered to enhance the accuracy of EDL, e.g., user consumption habits.</p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<p id="S6.SS4.p4.1" class="ltx_p"><span id="S6.SS4.p4.1.1" class="ltx_text ltx_font_italic">Summary:</span> In this section, we discuss that FL can also be used for mobile edge network optimization. In particular, DL and DRL approaches are suitable for modelling the dynamic environment of increasingly complex edge networks but require sufficient data for training. With FL, model training can be carried out while preserving the privacy of users. A summary of the approaches are presented in Table <a href="#S6.T8" title="TABLE VIII ‣ VI Applications of Federated Learning for Mobile Edge Computing ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Challenges and Future Research Directions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Apart from the aforementioned issues, there are still challenges new research directions in deploying FL at scale to be discussed as follows.
</p>
</div>
<div id="S7.p2" class="ltx_para">
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p"><span id="S7.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Dropped participants:</span> The approaches discussed in Section <a href="#S4" title="IV Resource Allocation ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>, propose new algorithms for participant selection and resource allocation to address the training bottleneck and resource heterogeneity. In these approaches, the wireless connections of participants are assumed to be always available. However, in practice, participating mobile devices may go offline and can drop out from the FL system due to connectivity or energy constraints. A large number of dropped devices from the training participation can significantly degrade the performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, e.g., accuracy and convergence speed, of the FL system. New FL algorithms need to be robust to device drop out in the networks and anticipate the scenarios in which only a small number of participants are left connected to participate in a training round. One potential solution is that the FL model owner provides free dedicated/special connection, e.g., cellular connections, as an incentive to the participants to avoid drop out.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p"><span id="S7.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Privacy concerns:</span> FL is able to protect the privacy of each participants since the model training may be conducted locally, with just the model parameters exchanged with the FL server. However, as specified in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>, communicating the model updates during the training process can still reveal sensitive information to an adversary or a third-party. The current approaches propose security solutions such as DP, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>, and collaborative training, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>. However, the adoption of these approaches sacrifices the performance, i.e., model accuracy. They also require significant computation on participating mobile devices. Thus, the tradeoff between privacy guarantee and system performance has to be well balanced when implementing the FL system.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p"><span id="S7.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Unlabeled data:</span> It is important to note that the approaches reviewed in the survey are proposed for supervised learning tasks. This means that the approaches assume that labels exist for all of the data in the federated network. However, in practice, the data generated in the network may be unlabeled or mislabeled <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite>. This poses a big challenge to the server to find participants with appropriate data for model training. Tackling this challenge may require the challenges of
scalability, heterogeneity, and privacy in the FL systems to be addressed. One possible solution is to enable mobile devices to construct their labeled data by learning the “labeled data” from each other. Emerging studies have also considered the use of semi-supervised learning inspired techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib205" title="" class="ltx_ref">205</a>]</cite>.</p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p"><span id="S7.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Interference among mobile devices:</span> The existing resource allocation approaches, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>, address the participant selection based on the resource states of their mobile devices. In fact, these mobile devices may be geographically close to each other, i.e., in the same cell. This introduces an interference issue when they update local models to the server. As such, channel allocation policies may need to be combined with the resource allocation approaches to address the interference issue. While studies in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> consider multi-access schemes and over-the-air computation, it remains to be seen if such approaches are scalable, i.e., able to support a large federation of many participants. To this end, data driven learning based solutions, e.g., federated DRL, can be considered to model the dynamic environment of mobile edge networks and make optimized decisions.</p>
</div>
</li>
<li id="S7.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i5.p1" class="ltx_para">
<p id="S7.I1.i5.p1.1" class="ltx_p"><span id="S7.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">Communication security:</span> The privacy and security threats studied in Section <a href="#S5.SS2" title="V-B Security Issues ‣ V Privacy and Security Issues ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a> revolve mainly around data-related compromises, e.g., data and model poisoning. Due to the exposed nature of the wireless medium, FL is also vulnerable to communication security issues such as Distributed Denial-of-Service (DoS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite> and jamming attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>. In particular, for jamming attacks, an attacker can transmit radio frequency jamming signals with high power to disrupt or cause interference to the communications between the mobile devices and the server. Such an attack can cause errors to the model uploads/downloads and consequently degrade the performance, i.e., accuracy, of the FL systems. Anti-jamming schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite> such as frequency hopping, e.g., sending one more copy of the model update over different frequencies, can be adopted to address the issue.</p>
</div>
</li>
<li id="S7.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i6.p1" class="ltx_para">
<p id="S7.I1.i6.p1.1" class="ltx_p"><span id="S7.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">Asynchronous FL:</span> In synchronous FL, each training round only progresses as quickly as the slowest device, i.e., the FL system is susceptible to the straggler effect. As such, asynchronous FL has been proposed as a solution in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>. In addition, asynchronous FL also allows participants to join the FL training halfway even while a training round is in progress. This is more reflective of practical FL settings and can be an important contributing factor towards ensuring the scalability of FL. However, synchronous FL remains to be the most common approach used due to convergence guarantees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. Given the many advantages of asynchronous FL, new asynchronous algorithms should be explored. In particular, for future proposed algorithms, the convergence guarantee in a non-IID setting for non-convex loss functions need to be considered. An approach to be considered is the possibile inclusion of <span id="S7.I1.i6.p1.1.2" class="ltx_text ltx_font_italic">backup</span> workers following the studies of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>.</p>
</div>
</li>
<li id="S7.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i7.p1" class="ltx_para">
<p id="S7.I1.i7.p1.1" class="ltx_p"><span id="S7.I1.i7.p1.1.1" class="ltx_text ltx_font_italic">Comparisons with other distributed learning methods:</span> Following the increased scrutiny on data privacy, there has been a growing effort on developing new privacy preserving distributed learning algorithms. One study proposes <span id="S7.I1.i7.p1.1.2" class="ltx_text ltx_font_italic">split learning</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>]</cite>, which also enables collaborative ML without requiring the exchange of raw data with an external server. In split learning, each participant first trains the neural network up to a cut layer. Then, the outputs from training are transmitted to an external server that completes the other layers of training. The resultant gradients are then back propagated up to the cut layer, and eventually returned to the participants to complete the local training. In contrast, FL typically involves the communication of full model parameters. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite> conduct an empirical comparison between the communication efficiencies of split learning and FL. The simulation results show that split learning performs well when the model size involved is larger, or when there are more participants involved, since the participants do not have to transmit the weights to an aggregating server. However, FL is much easier to implement since the participants and FL server are running the same global model, i.e., the FL server is just in charge of aggregation and thus FL can work with one of the participants serving as the master node. As such, more research efforts can be directed towards guiding system administrators to make an informed decision as to which scenario warrants the use of either learning methods.</p>
</div>
</li>
<li id="S7.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i8.p1" class="ltx_para">
<p id="S7.I1.i8.p1.1" class="ltx_p"><span id="S7.I1.i8.p1.1.1" class="ltx_text ltx_font_italic">Further studies on learning convergence:</span> One of the essential considerations of FL is the
convergence of the algorithm. FL finds weights to minimize the global model aggregation. This is actually a distributed optimization problem, and the convergence is not always guaranteed. Theoretical analysis and evaluations
on the convergence bounds of the gradient descent based
FL for convex and non-convex loss functions are important research directions. While existing studies have covered this topic, many of the guarantees are limited to restrictions, e.g., convexity of the loss function.</p>
</div>
</li>
<li id="S7.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i9.p1" class="ltx_para">
<p id="S7.I1.i9.p1.1" class="ltx_p"><span id="S7.I1.i9.p1.1.1" class="ltx_text ltx_font_italic">Usage of tools to quantify statistical heterogeneity:</span> Mobile devices typically generate and collect data in a non-IID manner across the network. Moreover, the number of data samples among the mobile devices may vary significantly. To improve the convergence of FL algorithm, the statistical heterogeneity of the data needs to be quantified. Recent works, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>, have developed tools for quantifying statistical heterogeneity through metrics such as local dissimilarity. However, these metrics cannot be easily calculated over the federated network before training begins. The importance of these metrics motivates future directions such as the development of efficient algorithms to quickly determine the level of heterogeneity in federated networks.</p>
</div>
</li>
<li id="S7.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i10.p1" class="ltx_para">
<p id="S7.I1.i10.p1.1" class="ltx_p"><span id="S7.I1.i10.p1.1.1" class="ltx_text ltx_font_italic">Combined algorithms for communication reduction:</span> Currently, there are three common techniques of communication reduction in FL as discussed in Section <a href="#S3" title="III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. It is important to study how these techniques can be combined with each other to improve the performance further. For example, the model compression technique can be combined with the edge server-assisted FL. The combination is able to significantly reduce the size of model updates, as well as the instances of communication with the FL server. However, the feasibility of this combination has not been explored. In addition, the tradeoff between accuracy and communication overhead for the combination technique needs to be further evaluated. In particular, for simulation results we discuss in Section <a href="#S3" title="III Communication Cost ‣ Federated Learning in Mobile Edge Networks: A Comprehensive Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the accuracy-communication cost reduction tradeoff is difficult to manage since it varies for different settings, e.g., data distribution, quantity, number of edge servers, and number of participants.</p>
</div>
</li>
<li id="S7.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i11.p1" class="ltx_para">
<p id="S7.I1.i11.p1.1" class="ltx_p"><span id="S7.I1.i11.p1.1.1" class="ltx_text ltx_font_italic">Cooperative mobile crowd ML:</span> In the existing approaches, mobile devices need to communicate with the server directly and this may increase the energy consumption. In fact, mobile devices nearby can be grouped in a cluster, and the model downloading/uploading between the server and the mobile devices can be facilitated by a “cluster head” that serves as a relay node <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>. The model exchange between the mobile devices and the cluster head can then be done in Device-to-Device (D2D) connections. Such a model can improve the energy efficiency significantly. Efficient coordination schemes for the cluster head can thus be designed to further improve the energy efficiency of a FL system.</p>
</div>
</li>
<li id="S7.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i12.p1" class="ltx_para">
<p id="S7.I1.i12.p1.1" class="ltx_p"><span id="S7.I1.i12.p1.1.1" class="ltx_text ltx_font_italic">Applications of FL:</span> Given the advantages of guaranteeing data privacy, FL has an increasingly important role to play in many
applications, e.g., healthcare, finance and transport systems. For most current studies on FL applications, the focus mainly lies in the federated training of the learning model, with the implementation challenges neglected.
For future studies on the applications of FL, besides
a need to consider the aforementioned issues in the survey,
i.e., communication costs, resource allocation, and privacy
and security, there is also a need to consider the specific issues related to the system model in which FL will be adopted in. For example, for delay critical applications, e.g, in vehicular networks, there will be more emphasis on training efficiency and less on energy expense.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This paper has presented a tutorial of FL and a comprehensive survey on the issues regarding FL implementation. Firstly, we begin with an introduction to the motivation for MEC, and how FL can serve as an enabling technology for collaborative model training at mobile edge networks. Then, we describe the fundamentals of DNN model training, FL, and system design towards FL at scale. Afterwards, we provide detailed reviews, analyses, and comparisons of approaches for emerging implementation challenges in FL. The issues include communication cost, resource allocation, data privacy and data security. Furthermore, we also discuss the implementation of FL for privacy-preserving mobile edge network optimization. Finally, we discuss challenges and future research directions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. L. Lueth, “State of the iot 2018: Number of iot devices now at 7b Ð market
accelerating,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IOT Analytics</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R. Pryss, M. Reichert, J. Herrmann, B. Langguth, and W. Schlee, “Mobile crowd
sensing in clinical and psychological trials–a case study,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2015
IEEE 28th International Symposium on Computer-Based Medical Systems</em>.   IEEE, 2015, pp. 23–24.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. K. Ganti, F. Ye, and H. Lei, “Mobile crowdsensing: current state and future
challenges,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>, vol. 49, no. 11, pp.
32–39, 2011.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">nature</em>, vol. 521,
no. 7553, p. 436, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. Oletic and V. Bilas, “Design of sensor node for air quality crowdsensing,”
in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2015 IEEE Sensors Applications Symposium (SAS)</em>.   IEEE, 2015, pp. 1–5.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Jing, B. Guo, Z. Wang, V. O. Li, J. C. Lam, and Z. Yu, “Crowdtracker:
Optimized urban moving object tracking using mobile crowd sensing,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 5, no. 5, pp. 3452–3463, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H.-J. Hong, C.-L. Fan, Y.-C. Lin, and C.-H. Hsu, “Optimizing cloud-based video
crowdsensing,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 3, no. 3, pp.
299–313, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W. He, G. Yan, and L. Da Xu, “Developing vehicular data cloud services in the
iot environment,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>,
vol. 10, no. 2, pp. 1587–1595, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P. Li, J. Li, Z. Huang, T. Li, C.-Z. Gao, S.-M. Yiu, and K. Chen, “Multi-key
privacy-preserving deep learning in cloud computing,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Future
Generation Computer Systems</em>, vol. 74, pp. 76–85, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Custers, A. Sears, F. Dechesne, I. Georgieva, T. Tani, and S. van der Hof,
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">EU Personal Data Protection in Policy and Practice</em>.   Springer, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. M. Gaff, H. E. Sussman, and J. Geetter, “Privacy and big data,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Computer</em>, vol. 47, no. 6, pp. 7–9, 2014.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey on mobile
edge computing: The communication perspective,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Communications
Surveys &amp; Tutorials</em>, vol. 19, no. 4, pp. 2322–2358, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
G. Ananthanarayanan, P. Bahl, P. Bodík, K. Chintalapudi, M. Philipose,
L. Ravindranath, and S. Sinha, “Real-time video analytics: The killer app
for edge computing,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">computer</em>, vol. 50, no. 10, pp. 58–67, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Li, K. Ota, and M. Dong, “Learning iot in edge: deep learning for the
internet of things with edge computing,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 32,
no. 1, pp. 96–101, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Han, X. Wang, V. Leung, D. Niyato, X. Yan, and X. Chen, “Convergence of
edge computing and deep learning: A comprehensive survey,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1907.08349</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Cisco, “Cisco global cloud index: Forecast and methodology, 2016Ð2021 white
paper.”

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision and
challenges,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 3, no. 5, pp.
637–646, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Chen, L. Jiao, W. Li, and X. Fu, “Efficient multi-user computation
offloading for mobile-edge cloud computing,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on
Networking</em>, vol. 24, no. 5, pp. 2795–2808, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P. Mach and Z. Becvar, “Mobile edge computing: A survey on architecture and
computation offloading,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
vol. 19, no. 3, pp. 1628–1656, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications
Security</em>.   ACM, 2016, pp. 308–318.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of
deep networks using model averaging,” 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F. Cicirelli, A. Guerrieri, G. Spezzano, A. Vinci, O. Briante, A. Iera, and
G. Ruggeri, “Edge computing and social internet of things for large-scale
smart environments development,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>,
vol. 5, no. 4, pp. 2557–2571, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Communication-efficient learning of deep networks from decentralized
data,” <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Hard, K. Rao, R. Mathews, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon,
and D. Ramage, “Federated learning for mobile keyboard prediction,”
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
T. S. Brisimi, R. Chen, T. Mela, A. Olshevsky, I. C. Paschalidis, and W. Shi,
“Federated learning of predictive models from federated electronic health
records,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International journal of medical informatics</em>, vol. 112, pp.
59–67, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. Powell, “Nvidia clara federated learning to deliver ai to hospitals while
protecting patient data,”
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">https://blogs.nvidia.com/blog/2019/12/01/clara-federated-learning/</em>,
2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. Verma, S. Julier, and G. Cirincione, “Federated ai for building ai
solutions across multiple agencies,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.10036</em>,
2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
O. Simeone, “A very brief introduction to machine learning with applications
to communication systems,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cognitive
Communications and Networking</em>, vol. 4, no. 4, pp. 648–664, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and wireless
networking: A survey,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and D. I.
Kim, “Applications of deep reinforcement learning in communications and
networking: A survey,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. Hamidouche, A. T. Z. Kasgari, W. Saad, M. Bennis, and M. Debbah,
“Collaborative artificial intelligence (ai) for user-cell association in
ultra-dense cellular systems,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference
on Communications Workshops (ICC Workshops)</em>.   IEEE, 2018, pp. 1–6.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, “In-edge ai:
Intelligentizing mobile edge computing, caching and communication by
federated learning,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.07857</em>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Samarakoon, M. Bennis, W. Saady, and M. Debbah, “Distributed federated
learning for ultra-reliable low-latency vehicular communications,”
<em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.08127</em>, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em>, vol. 10, no. 2, p. 12, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Niknam, H. S. Dhillon, and J. H. Reed, “Federated learning for wireless
communications: Motivation, opportunities and challenges,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1908.06847</em>, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1908.07873</em>, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge intelligence:
Paving the last mile of artificial intelligence with edge computing,”
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10083</em>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
L. Cui, S. Yang, F. Chen, Z. Ming, N. Lu, and J. Qin, “A survey on application
of machine learning for internet of things,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Journal of
Machine Learning and Cybernetics</em>, vol. 9, no. 8, pp. 1399–1417, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
K. Kumar, J. Liu, Y.-H. Lu, and B. Bhargava, “A survey of computation
offloading for mobile systems,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Mobile Networks and Applications</em>,
vol. 18, no. 1, pp. 129–140, 2013.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, “Mobile edge computing: A
survey,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 5, no. 1, pp.
450–465, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
S. Wang, X. Zhang, Y. Zhang, L. Wang, J. Yang, and W. Wang, “A survey on
mobile edge networks: Convergence of computing, caching and communications,”
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 5, pp. 6757–6779, 2017.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J. Yao, T. Han, and N. Ansari, “On mobile edge caching,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE
Communications Surveys &amp; Tutorials</em>, vol. 21, no. 3, pp. 2525–2553, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convolutional network
for image super-resolution,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">European conference on computer
vision</em>.   Springer, 2014, pp. 184–199.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J. Schmidhuber, “Deep learning in neural networks: An overview,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Neural
networks</em>, vol. 61, pp. 85–117, 2015.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
X.-W. Chen and X. Lin, “Big data deep learning: challenges and perspectives,”
<em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, vol. 2, pp. 514–525, 2014.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou,
B. Schuller, and S. Zafeiriou, “Adieu features? end-to-end speech emotion
recognition using a deep convolutional recurrent network,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">2016
IEEE international conference on acoustics, speech and signal processing
(ICASSP)</em>.   IEEE, 2016, pp. 5200–5204.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, “Efficient processing of deep
neural networks: A tutorial and survey,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>,
vol. 105, no. 12, pp. 2295–2329, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
R. Hecht-Nielsen, “Theory of the backpropagation neural network,” in
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Neural networks for perception</em>.   Elsevier, 1992, pp. 65–93.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi, “Learning activation
functions to improve deep neural networks,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1412.6830</em>, 2014.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1708.07747</em>, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
G. Hinton, N. Srivastava, and K. Swersky, “Neural networks for machine
learning lecture 6a overview of mini-batch gradient descent,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Cited
on</em>, vol. 14, p. 8, 2012.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
X. J. Zhu, “Semi-supervised learning literature survey,” University of
Wisconsin-Madison Department of Computer Sciences, Tech. Rep., 2005.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
with deep convolutional generative adversarial networks,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1511.06434</em>, 2015.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and
M. Riedmiller, “Playing atari with deep reinforcement learning,”
<em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.5602</em>, 2013.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
H. Bourlard and Y. Kamp, “Auto-association by multilayer perceptrons and
singular value decomposition,” <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Biological cybernetics</em>, vol. 59, no.
4-5, pp. 291–294, 1988.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with
deep convolutional neural networks,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, 2012, pp. 1097–1105.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and
S. Khudanpur, “Recurrent neural network based language model,” in
<em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Eleventh annual conference of the international speech communication
association</em>, 2010.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Q. Zhang, L. T. Yang, Z. Chen, and P. Li, “A survey on deep learning for big
data,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, vol. 42, pp. 146–157, 2018.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “A brief
survey of deep reinforcement learning,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1708.05866</em>, 2017.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
J. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu, “Advanced deep-learning
techniques for salient and category-specific object detection: a survey,”
<em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, vol. 35, no. 1, pp. 84–100, 2018.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
B. Zhao, J. Feng, X. Wu, and S. Yan, “A survey on deep learning-based
fine-grained object classification and semantic segmentation,”
<em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">International Journal of Automation and Computing</em>, vol. 14, no. 2, pp.
119–135, 2017.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
J. Wang, Y. Chen, S. Hao, X. Peng, and L. Hu, “Deep learning for sensor-based
activity recognition: A survey,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters</em>, vol.
119, pp. 3–11, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
reinforcement learning: A brief survey,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing
Magazine</em>, vol. 34, no. 6, pp. 26–38, 2017.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive mechanism for
reliable federated learning: A joint optimization approach to combining
reputation and contract theory,” 09 2019.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
C. J. Burges, “A tutorial on support vector machines for pattern
recognition,” <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Data mining and knowledge discovery</em>, vol. 2, no. 2, pp.
121–167, 1998.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
R. H. Myers and R. H. Myers, <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Classical and modern regression with
applications</em>.   Duxbury press Belmont,
CA, 1990, vol. 2.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan,
“Adaptive federated learning in resource constrained edge computing
systems,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi, “Don’t use large mini-batches,
use local sgd,” <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.07217</em>, 2018.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning
with non-iid data,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>, 2018.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
A. Krizhevsky, G. Hinton <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Learning multiple layers of features
from tiny images,” Citeseer, Tech. Rep., 2009.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Y. Rubner, C. Tomasi, and L. J. Guibas, “The earth mover’s distance as a
metric for image retrieval,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">International journal of computer
vision</em>, vol. 40, no. 2, pp. 99–121, 2000.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M. Duan, “Astraea: Self-balancing federated learning for improving
classification accuracy of mobile deep learning applications,” <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1907.01132</em>, 2019.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
S. C. Wong, A. Gatt, V. Stamatescu, and M. D. McDonnell, “Understanding data
augmentation for classification: when to warp?” in <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">2016 international
conference on digital image computing: techniques and applications
(DICTA)</em>.   IEEE, 2016, pp. 1–6.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
J. M. Joyce, “Kullback-leibler divergence,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">International encyclopedia
of statistical science</em>, pp. 720–722, 2011.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
M. Jaggi, V. Smith, M. Takác, J. Terhorst, S. Krishnan, T. Hofmann, and
M. I. Jordan, “Communication-efficient distributed dual coordinate ascent,”
in <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2014, pp.
3068–3076.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated multi-task
learning,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
2017, pp. 4424–4434.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
J. C. Bezdek and R. J. Hathaway, “Convergence of alternating optimization,”
<em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Neural, Parallel &amp; Scientific Computations</em>, vol. 11, no. 4, pp.
351–368, 2003.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
M. G. Arivazhagan, V. Aggarwal, A. K. Singh, and S. Choudhary, “Federated
learning with personalization layers,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1912.00818</em>, 2019.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
J. Ren, X. Shen, Z. Lin, R. Mech, and D. J. Foran, “Personalized image
aesthetics,” in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision</em>, 2017, pp. 638–647.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence of fedavg
on non-iid data,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.02189</em>, 2019.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
L. Huang, Y. Yin, Z. Fu, S. Zhang, H. Deng, and D. Liu, “Loadaboost:
Loss-based adaboost federated machine learning on medical data,” <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1811.12629</em>, 2018.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards
federated learning at scale: System design,” <em id="bib.bib82.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1902.01046</em>, 2019.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
T. Nishio and R. Yonetani, “Client selection for federated learning with
heterogeneous resources in mobile edge,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1804.08333</em>, 2018.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for
privacy-preserving machine learning,” in <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.   ACM, 2017, pp. 1175–1191.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
J. Bloemer, “How to share a secret,” <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Communications of the Acm</em>,
vol. 22, no. 22, pp. 612–613, 2011.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
H. B. Mcmahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
private recurrent language models,” <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">international conference on
learning representations</em>, 2018.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Google, “Tensorflow federated: Machine learning on decentralized data.”

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
T. Ryffel, A. Trask, M. Dahl, B. Wagner, J. Mancuso, D. Rueckert, and
J. Passerat-Palmbach, “A generic framework for privacy preserving deep
learning,” <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.04017</em>, 2018.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
S. Caldas, P. Wu, T. Li, J. Konečnỳ, H. B. McMahan, V. Smith, and
A. Talwalkar, “Leaf: A benchmark for federated settings,” <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1812.01097</em>, 2018.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,”
<em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">AT&amp;T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist</em>,
vol. 2, p. 18, 2010.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
A. Go, R. Bhayani, and L. Huang, “Sentiment140,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Site Functionality,
2013c. URL http://help. sentiment140. com/site-functionality. Abruf am</em>,
vol. 20, 2016.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
WeBank, “Fate: An industrial grade federated learning framework.”
<em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">https://fate.fedai.org,2018.</em>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, D. Ramage, and P. Richtárik,
“Federated optimization: Distributed machine learning for on-device
intelligence,” <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
L. Wang, W. Wang, and B. Li, “Cmfl: Mitigating communication overhead for
federated learning.”

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright,
“Atomo: Communication-efficient learning via atomic sparsification,” in
<em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2018, pp.
9850–9861.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
S. U. Stich, J.-B. Cordonnier, and M. Jaggi, “Sparsified sgd with memory,” in
<em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2018, pp.
4447–4458.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
S. Caldas, J. Konečny, H. B. McMahan, and A. Talwalkar, “Expanding the
reach of federated learning by reducing client resource requirements,”
<em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.07210</em>, 2018.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Z. Tao and Q. Li, “esgd: Communication efficient distributed deep learning on
the edge,” in <em id="bib.bib100.2.2" class="ltx_emph ltx_font_italic"><math id="bib.bib100.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib100.1.1.m1.1a"><mo stretchy="false" id="bib.bib100.1.1.m1.1.1" xref="bib.bib100.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib100.1.1.m1.1b"><ci id="bib.bib100.1.1.m1.1.1.cmml" xref="bib.bib100.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib100.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib100.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib100.2.2.m2.1a"><mo stretchy="false" id="bib.bib100.2.2.m2.1.1" xref="bib.bib100.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib100.2.2.m2.1b"><ci id="bib.bib100.2.2.m2.1.1.cmml" xref="bib.bib100.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib100.2.2.m2.1c">\}</annotation></semantics></math> Workshop on Hot Topics in Edge Computing
(HotEdge 18)</em>, 2018.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Neural
computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
“Dropout: a simple way to prevent neural networks from overfitting,”
<em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">The journal of machine learning research</em>, vol. 15, no. 1, pp.
1929–1958, 2014.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Kang, X. Zhang, L. Li, Y. Cheng, T. Chen, M. Hong, and Q. Yang, “A
communication efficient vertical federated learning framework,” <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1912.11187</em>, 2019.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
X. Yao, C. Huang, and L. Sun, “Two-stream federated learning: Reduce the
communication costs,” in <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Visual Communications and Image
Processing (VCIP)</em>.   IEEE, 2018, pp.
1–4.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
L. Liu, J. Zhang, S. Song, and K. B. Letaief, “Edge-assisted hierarchical
federated learning with non-iid data,” <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1905.06641</em>, 2019.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
M. Long, Y. Cao, J. Wang, and M. I. Jordan, “Learning transferable features
with deep adaptation networks,” in <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd
International Conference on International Conference on Machine
Learning-Volume 37</em>.   JMLR. org, 2015,
pp. 97–105.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu, “Transfer feature learning
with joint distribution adaptation,” in <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</em>, 2013, pp. 2200–2207.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding,” <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1510.00149</em>, 2015.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
A. T. Suresh, F. X. Yu, S. Kumar, and H. B. McMahan, “Distributed mean
estimation with limited communication,” in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th
International Conference on Machine Learning-Volume 70</em>.   JMLR. org, 2017, pp. 3329–3337.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
B. S. Kashin, “Diameters of some finite-dimensional sets and classes of smooth
functions,” <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Izvestiya Rossiiskoi Akademii Nauk. Seriya
Matematicheskaya</em>, vol. 41, no. 2, pp. 334–351, 1977.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
G. Cohen, S. Afshar, J. Tapson, and A. van Schaik, “Emnist: an extension of
mnist to handwritten letters,” <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1702.05373</em>, 2017.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
N. Strom, “Scalable distributed dnn training using commodity gpu cloud
computing,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Sixteenth Annual Conference of the International Speech
Communication Association</em>, 2015.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
J. Chen, X. Pan, R. Monga, S. Bengio, and R. Jozefowicz, “Revisiting
distributed synchronous sgd,” <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.00981</em>, 2016.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, “Deep gradient compression:
Reducing the communication bandwidth for distributed training,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1712.01887</em>, 2017.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
K. Hsieh, A. Harlap, N. Vijaykumar, D. Konomis, G. R. Ganger, P. B. Gibbons,
and O. Mutlu, “Gaia: Geo-distributed machine learning approaching
<math id="bib.bib115.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib115.1.m1.1a"><mo stretchy="false" id="bib.bib115.1.m1.1.1" xref="bib.bib115.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib115.1.m1.1b"><ci id="bib.bib115.1.m1.1.1.cmml" xref="bib.bib115.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib115.1.m1.1c">\{</annotation></semantics></math>LAN<math id="bib.bib115.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib115.2.m2.1a"><mo stretchy="false" id="bib.bib115.2.m2.1.1" xref="bib.bib115.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib115.2.m2.1b"><ci id="bib.bib115.2.m2.1.1.cmml" xref="bib.bib115.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib115.2.m2.1c">\}</annotation></semantics></math> speeds,” in <em id="bib.bib115.6.4" class="ltx_emph ltx_font_italic">14th <math id="bib.bib115.3.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib115.3.1.m1.1a"><mo stretchy="false" id="bib.bib115.3.1.m1.1.1" xref="bib.bib115.3.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib115.3.1.m1.1b"><ci id="bib.bib115.3.1.m1.1.1.cmml" xref="bib.bib115.3.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib115.3.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib115.4.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib115.4.2.m2.1a"><mo stretchy="false" id="bib.bib115.4.2.m2.1.1" xref="bib.bib115.4.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib115.4.2.m2.1b"><ci id="bib.bib115.4.2.m2.1.1.cmml" xref="bib.bib115.4.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib115.4.2.m2.1c">\}</annotation></semantics></math> Symposium on Networked
Systems Design and Implementation (<math id="bib.bib115.5.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib115.5.3.m3.1a"><mo stretchy="false" id="bib.bib115.5.3.m3.1.1" xref="bib.bib115.5.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib115.5.3.m3.1b"><ci id="bib.bib115.5.3.m3.1.1.cmml" xref="bib.bib115.5.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib115.5.3.m3.1c">\{</annotation></semantics></math>NSDI<math id="bib.bib115.6.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib115.6.4.m4.1a"><mo stretchy="false" id="bib.bib115.6.4.m4.1.1" xref="bib.bib115.6.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib115.6.4.m4.1b"><ci id="bib.bib115.6.4.m4.1.1.cmml" xref="bib.bib115.6.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib115.6.4.m4.1c">\}</annotation></semantics></math> 17)</em>, 2017, pp. 629–647.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A public
domain dataset for human activity recognition using smartphones.” in
<em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Esann</em>, 2013.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
M. Buscema and S. Terzi, “Semeion handwritten digit data set,” <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Center
for Machine Learning and Intelligent Systems, California, USA</em>, 2009.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy efficient
federated learning over wireless communication networks,” <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1911.02417</em>, 2019.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
M. R. Sprague, A. Jalalirad, M. Scavuzzo, C. Capota, M. Neun, L. Do, and
M. Kopp, “Asynchronous federated learning for geospatial applications,” in
<em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Joint European Conference on Machine Learning and Knowledge Discovery
in Databases</em>.   Springer, 2018, pp.
21–28.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
M. I. Jordan, J. D. Lee, and Y. Yang, “Communication-efficient distributed
statistical inference,” <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Journal of the American Statistical
Association</em>, vol. 114, no. 526, pp. 668–681, 2019.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
M. Sviridenko, “A note on maximizing a submodular set function subject to a
knapsack constraint,” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">Operations Research Letters</em>, vol. 32, no. 1,
pp. 41–43, 2004.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
M. Mohri, G. Sivek, and A. T. Suresh, “Agnostic federated learning,”
<em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.00146</em>, 2019.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
N. Yoshida, T. Nishio, M. Morikura, K. Yamamoto, and R. Yonetani, “Hybrid-fl:
Cooperative learning mechanism using non-iid data in wireless networks,”
<em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07210</em>, 2019.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
T. T. Anh, N. C. Luong, D. Niyato, D. I. Kim, and L.-C. Wang, “Efficient
training management for mobile crowd-machine learning: A deep reinforcement
learning approach,” <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.03633</em>, 2018.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
double q-learning,” in <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">Thirtieth AAAI conference on artificial
intelligence</em>, 2016.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
H. T. Nguyen, N. C. Luong, J. Zhao, C. Yuen, and D. Niyato, “Resource
allocation in mobility-aware federated learning networks: A deep
reinforcement learning approach,” <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.09172</em>,
2019.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
M. J. Neely, E. Modiano, and C.-P. Li, “Fairness and optimal stochastic
control for heterogeneous networks,” <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions On
Networking</em>, vol. 16, no. 2, pp. 396–409, 2008.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
M. Feldman, “Computational fairness: Preventing machine-learned
discrimination,” 2015.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
T. Li, M. Sanjabi, and V. Smith, “Fair resource allocation in federated
learning,” <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10497</em>, 2019.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
M. Chen, H. V. Poor, W. Saad, and S. Cui, “Convergence time optimization for
federated learning over wireless networks,” <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2001.07845</em>, 2020.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
D. López-Pérez, A. Valcarce, G. De La Roche, and J. Zhang, “Ofdma
femtocells: A roadmap on interference avoidance,” <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">IEEE Communications
Magazine</em>, vol. 47, no. 9, pp. 41–48, 2009.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
G. Zhu, Y. Wang, and K. Huang, “Low-latency broadband analog aggregation for
federated edge learning,” <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.11494</em>, 2018.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
B. Nazer and M. Gastpar, “Compute-and-forward: Harnessing interference through
structured codes,” <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Theory</em>, vol. 57,
no. 10, pp. 6463–6486, 2011.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
M. M. Amiri and D. Gunduz, “Federated learning over wireless fading
channels,” <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.09769</em>, 2019.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-the-air
computation,” <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.11750</em>, 2018.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On
large-batch training for deep learning: Generalization gap and sharp
minima,” <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.04836</em>, 2016.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
S. Boyd and L. Vandenberghe, <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">Convex optimization</em>.   Cambridge university press, 2004.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
P. D. Tao <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The dc (difference of convex functions) programming
and dca revisited with dc models of real world nonconvex optimization
problems,” <em id="bib.bib138.2.2" class="ltx_emph ltx_font_italic">Annals of operations research</em>, vol. 133, no. 1-4, pp.
23–46, 2005.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Z.-Q. Luo, N. D. Sidiropoulos, P. Tseng, and S. Zhang, “Approximation bounds
for quadratic optimization with homogeneous quadratic constraints,”
<em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">SIAM Journal on optimization</em>, vol. 18, no. 1, pp. 1–28, 2007.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
C. Xie, S. Koyejo, and I. Gupta, “Asynchronous federated optimization,”
<em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.03934</em>, 2019.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
S. Feng, D. Niyato, P. Wang, D. I. Kim, and Y.-C. Liang, “Joint service
pricing and cooperative relay communication for federated learning,”
<em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.12082</em>, 2018.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
M. J. Osborne <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">et al.</em>, <em id="bib.bib142.2.2" class="ltx_emph ltx_font_italic">An introduction to game theory</em>.   Oxford university press New York, 2004, vol. 3,
no. 3.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Y. Sarikaya and O. Ercetin, “Motivating workers in federated learning: A
stackelberg game perspective,” <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03092</em>, 2019.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
L. U. Khan, N. H. Tran, S. R. Pandey, W. Saad, Z. Han, M. N. Nguyen, and C. S.
Hong, “Federated learning for edge networks: Resource optimization and
incentive mechanism,” <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.05642</em>, 2019.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Y. Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, “A learning-based incentive
mechanism for federated learning,” <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>,
2020.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
J. Kang, Z. Xiong, D. Niyato, H. Yu, Y.-C. Liang, and D. I. Kim, “Incentive
design for efficient federated learning in mobile networks: A contract theory
approach,” <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07479</em>, 2019.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
P. Bolton, M. Dewatripont <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">et al.</em>, <em id="bib.bib147.2.2" class="ltx_emph ltx_font_italic">Contract theory</em>.   MIT press, 2005.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
D. Ye, R. Yu, M. Pan, and Z. Han, “Federated learning in vehicular edge
computing: A selective model aggregation approach,” <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>,
2020.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
R. Jurca and B. Faltings, “An incentive compatible reputation mechanism,” in
<em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">EEE International Conference on E-Commerce, 2003. CEC 2003.</em>   IEEE, 2003, pp. 285–292.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
R. Dennis and G. Owen, “Rep on the block: A next generation reputation system
based on the blockchain,” in <em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">2015 10th International Conference for
Internet Technology and Secured Transactions (ICITST)</em>.   IEEE, 2015, pp. 131–138.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
H. Yu, Z. Liu, Y. Liu, T. Chen, M. Cong, X. Weng, D. Niyato, and Q. Yang, “A
fairness-aware incentive scheme for federated learning,” <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">AIES 20:
Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 2020.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting unintended
feature leakage in collaborative learning.”   IEEE, 2019.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
H.-W. Ng and S. Winkler, “A data-driven approach to cleaning large face
datasets,” in <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">2014 IEEE International Conference on Image Processing
(ICIP)</em>.   IEEE, 2014, pp. 343–347.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici,
“Hacking smart machines with smarter ones: How to extract meaningful data
from machine learning classifiers,” <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">International Journal of
Security</em>, vol. 10, no. 3, pp. 137–150, 2015.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
exploit confidence information and basic countermeasures,” in
<em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security</em>.   ACM, 2015,
pp. 1322–1333.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in <em id="bib.bib156.4.4" class="ltx_emph ltx_font_italic">25th <math id="bib.bib156.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib156.1.1.m1.1a"><mo stretchy="false" id="bib.bib156.1.1.m1.1.1" xref="bib.bib156.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib156.1.1.m1.1b"><ci id="bib.bib156.1.1.m1.1.1.cmml" xref="bib.bib156.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib156.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib156.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib156.2.2.m2.1a"><mo stretchy="false" id="bib.bib156.2.2.m2.1.1" xref="bib.bib156.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib156.2.2.m2.1b"><ci id="bib.bib156.2.2.m2.1.1.cmml" xref="bib.bib156.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib156.2.2.m2.1c">\}</annotation></semantics></math>
Security Symposium (<math id="bib.bib156.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib156.3.3.m3.1a"><mo stretchy="false" id="bib.bib156.3.3.m3.1.1" xref="bib.bib156.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib156.3.3.m3.1b"><ci id="bib.bib156.3.3.m3.1.1.cmml" xref="bib.bib156.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib156.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib156.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib156.4.4.m4.1a"><mo stretchy="false" id="bib.bib156.4.4.m4.1.1" xref="bib.bib156.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib156.4.4.m4.1b"><ci id="bib.bib156.4.4.m4.1.1.cmml" xref="bib.bib156.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib156.4.4.m4.1c">\}</annotation></semantics></math> Security 16)</em>, 2016, pp. 601–618.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
attacks against machine learning models,” in <em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on
Security and Privacy (SP)</em>.   IEEE,
2017, pp. 3–18.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples,”
<em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1605.07277</em>, 2016.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
P. Laskov <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Practical evasion of a learning-based classifier: A
case study,” in <em id="bib.bib159.2.2" class="ltx_emph ltx_font_italic">2014 IEEE symposium on security and privacy</em>.   IEEE, 2014, pp. 197–211.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” in <em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">Theory of cryptography
conference</em>.   Springer, 2006, pp.
265–284.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated
learning: A client level perspective,” <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1712.07557</em>, 2017.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
<em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security</em>.   ACM, 2015,
pp. 1310–1321.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Y. LeCun, L. Bottou, Y. Bengio, P. Haffner <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Gradient-based
learning applied to document recognition,” <em id="bib.bib163.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>,
vol. 86, no. 11, pp. 2278–2324, 1998.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
B. Hitaj, G. Ateniese, and F. Pérez-Cruz, “Deep models under the gan:
information leakage from collaborative deep learning,” in <em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications
Security</em>.   ACM, 2017, pp. 603–618.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial nets,” in
<em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2014, pp.
2672–2680.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Y. Liu, Z. Ma, S. Ma, S. Nepal, and R. Deng, “Boosting privately:
Privacy-preserving federated extreme boosting for mobile crowdsensing,”
<em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">https://arxiv.org/pdf/1907.10218.pdf</em>, 2019.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
A. Triastcyn and B. Faltings, “Federated generative privacy,” <em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1910.08385</em>, 2019.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Y. Aono, T. Hayashi, L. Wang, S. Moriai <em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Privacy-preserving
deep learning via additively homomorphic encryption,” <em id="bib.bib168.2.2" class="ltx_emph ltx_font_italic">IEEE
Transactions on Information Forensics and Security</em>, vol. 13, no. 5, pp.
1333–1345, 2017.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
M. Hao, H. Li, G. Xu, S. Liu, and H. Yang, “Towards efficient and
privacy-preseving federated deep learning,” in <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International
Conference on Communications</em>.   IEEE,
2019, pp. 1–6.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks on deep
learning systems using data poisoning,” <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1712.05526</em>, 2017.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
C. Fung, C. J. Yoon, and I. Beschastnikh, “Mitigating sybils in federated
learning poisoning,” <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.04866</em>, 2018.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
A. Asuncion and D. Newman, “Uci machine learning repository,” 2007.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing federated
learning through an adversarial lens,” <em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1811.12470</em>, 2018.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor
federated learning,” <em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.00459</em>, 2018.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
H. Kim, J. Park, M. Bennis, and S.-L. Kim, “On-device federated learning via
blockchain and its latency analysis,” <em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1808.03949</em>, 2018.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
J. Weng, J. Weng, J. Zhang, M. Li, Y. Zhang, and W. Luo, “Deepchain: Auditable
and privacy-preserving deep learning with blockchain-based incentive,”
<em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">Cryptology ePrint Archive, Report 2018/679</em>, 2018.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
I. Stojmenovic, S. Wen, X. Huang, and H. Luan, “An overview of fog computing
and its security issues,” <em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">Concurrency and Computation: Practice and
Experience</em>, vol. 28, no. 10, pp. 2991–3005, 2016.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
M. Gerla, E.-K. Lee, G. Pau, and U. Lee, “Internet of vehicles: From
intelligent grid to autonomous cars and vehicular clouds,” in <em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">2014
IEEE world forum on internet of things (WF-IoT)</em>.   IEEE, 2014, pp. 241–246.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
A. Abeshu and N. Chilamkurti, “Deep learning: the frontier for distributed
attack detection in fog-to-things computing,” <em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">IEEE Communications
Magazine</em>, vol. 56, no. 2, pp. 169–175, 2018.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
T. D. Nguyen, S. Marchal, M. Miettinen, M. H. Dang, N. Asokan, and A.-R.
Sadeghi, “D<math id="bib.bib180.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib180.1.m1.1a"><mo id="bib.bib180.1.m1.1.1" xref="bib.bib180.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib180.1.m1.1b"><ci id="bib.bib180.1.m1.1.1.cmml" xref="bib.bib180.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib180.1.m1.1c">\backslash</annotation></semantics></math>" iot: A crowdsourced self-learning approach for
detecting compromised iot devices,” <em id="bib.bib180.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.07474</em>,
2018.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
D. Preuveneers, V. Rimmer, I. Tsingenopoulos, J. Spooren, W. Joosen, and
E. Ilie-Zudor, “Chained anomaly detection models for federated learning: An
intrusion detection case study,” <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 8, no. 12, p.
2663, 2018.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
J. Ren, H. Wang, T. Hou, S. Zheng, and C. Tang, “Federated learning-based
computation offloading optimization in edge computing-supported internet of
things,” <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 7, pp. 69 194–69 201, 2019.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Z. Yu, J. Hu, G. Min, H. Lu, Z. Zhao, H. Wang, and N. Georgalas, “Federated
learning based proactive content caching in edge computing,” in <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">2018
IEEE Global Communications Conference (GLOBECOM)</em>.   IEEE, 2018, pp. 1–6.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Y. Qian, L. Hu, J. Chen, X. Guan, M. M. Hassan, and A. Alelaiwi,
“Privacy-aware service placement for mobile edge computing via federated
learning,” <em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, vol. 505, pp. 562–570, 2019.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
M. Chen, O. Semiari, W. Saad, X. Liu, and C. Yin, “Federated echo state
learning for minimizing breaks in presence in wireless virtual reality
networks,” <em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01202</em>, 2018.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Federated learning for
ultra-reliable low-latency v2v communications,” in <em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Global
Communications Conference (GLOBECOM)</em>.   IEEE, 2018, pp. 1–7.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Y. Saputra, H. Dinh, D. Nguyen, E. Dutkiewicz, M. Mueck, and S. Srikanteswara,
“Energy demand prediction with federated learning for electric vehicle
networks,” in <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">IEEE Global Communications Conference (GLOBECOM)</em>, 2019.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
K. K. Nguyen, D. T. Hoang, D. Niyato, P. Wang, D. Nguyen, and E. Dutkiewicz,
“Cyberattack detection in mobile cloud computing: A deep learning
approach,” in <em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Wireless Communications and Networking
Conference (WCNC)</em>.   IEEE, 2018, pp.
1–6.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
T. U. of New Brunswick, “Nsl-kdd https://www.unb.ca/cic/datasets/nsl.html.”

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
N. Moustafa and J. Slay, “Unsw-nb15: a comprehensive data set for network
intrusion detection systems (unsw-nb15 network data set),” in <em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">2015
military communications and information systems conference (MilCIS)</em>.   IEEE, 2015, pp. 1–6.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on knowledge and data engineering</em>, vol. 22, no. 10, pp.
1345–1359, 2009.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
S. Priya and D. J. Inman, <em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">Energy harvesting technologies</em>.   Springer, 2009, vol. 21.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
O. Chapelle and L. Li, “An empirical evaluation of thompson sampling,” in
<em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2011, pp.
2249–2257.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
J. Chung, H.-J. Yoon, and H. J. Gardner, “Analysis of break in presence during
game play using a linear mixed model,” <em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">ETRI journal</em>, vol. 32, no. 5,
pp. 687–694, 2010.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong, “Caching in
the sky: Proactive deployment of cache-enabled unmanned aerial vehicles for
optimized quality-of-experience,” <em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in
Communications</em>, vol. 35, no. 5, pp. 1046–1061, 2017.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
O. Guéant, J.-M. Lasry, and P.-L. Lions, “Mean field games and
applications,” in <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">Paris-Princeton lectures on mathematical finance
2010</em>.   Springer, 2011, pp. 205–266.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
L. De Haan and A. Ferreira, <em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">Extreme value theory: an introduction</em>.   Springer Science &amp; Business Media, 2007.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
M. J. Neely, “Stochastic network optimization with application to
communication and queueing systems,” <em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">Synthesis Lectures on
Communication Networks</em>, vol. 3, no. 1, pp. 1–211, 2010.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
P. You and Z. Yang, “Efficient optimal scheduling of charging station with
multiple electric vehicles via v2v,” in <em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">2014 IEEE International
Conference on Smart Grid Communications (SmartGridComm)</em>.   IEEE, 2014, pp. 716–721.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
P. Bradley, K. Bennett, and A. Demiriz, “Constrained k-means clustering,”
<em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">Microsoft Research, Redmond</em>, vol. 20, no. 0, p. 0, 2000.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
W. Li, T. Logenthiran, V.-T. Phan, and W. L. Woo, “Implemented iot-based
self-learning home management system (shms) for singapore,” <em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">IEEE
Internet of Things Journal</em>, vol. 5, no. 3, pp. 2212–2219, 2018.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
R. Boutaba, M. A. Salahuddin, N. Limam, S. Ayoubi, N. Shahriar,
F. Estrada-Solano, and O. M. Caicedo, “A comprehensive survey on machine
learning for networking: evolution, applications and research
opportunities,” <em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">Journal of Internet Services and Applications</em>,
vol. 9, no. 1, p. 16, 2018.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
L. Jiang, R. Tan, X. Lou, and G. Lin, “On lightweight privacy-preserving
collaborative learning for internet-of-things objects,” 2019.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Z. Gu, H. Jamjoom, D. Su, H. Huang, J. Zhang, T. Ma, D. Pendarakis, and
I. Molloy, “Reaching data confidentiality and model accountability on the
caltrain,” in <em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">2019 49th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN)</em>.   IEEE, 2019, pp. 336–348.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
A. Albaseer, B. S. Ciftler, M. Abdallah, and A. Al-Fuqaha, “Exploiting
unlabeled data in smart cities using federated learning,” <em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2001.04030</em>, 2020.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
F. Lau, S. H. Rubin, M. H. Smith, and L. Trajkovic, “Distributed denial of
service attacks,” in <em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">Smc 2000 conference proceedings. 2000 ieee
international conference on systems, man and cybernetics.’cybernetics
evolving to systems, humans, organizations, and their complex
interactions’(cat. no. 0</em>, vol. 3.   IEEE, 2000, pp. 2275–2280.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
W. Xu, K. Ma, W. Trappe, and Y. Zhang, “Jamming sensor networks: attack and
defense strategies,” <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">IEEE network</em>, vol. 20, no. 3, pp. 41–47, 2006.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
M. Strasser, C. Popper, S. Capkun, and M. Cagalj, “Jamming-resistant key
establishment using uncoordinated frequency hopping,” in <em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">2008 IEEE
Symposium on Security and Privacy (sp 2008)</em>.   IEEE, 2008, pp. 64–78.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
P. Vepakomma, O. Gupta, T. Swedish, and R. Raskar, “Split learning for health:
Distributed deep learning without sharing raw patient data,” <em id="bib.bib209.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1812.00564</em>, 2018.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
A. Singh, P. Vepakomma, O. Gupta, and R. Raskar, “Detailed comparison of
communication efficiency of split learning and federated learning,”
<em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.09145</em>, 2019.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
I. I. Eliazar and I. M. Sokolov, “Measuring statistical heterogeneity: The
pietra index,” <em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">Physica A: Statistical Mechanics and its Applications</em>,
vol. 389, no. 1, pp. 117–125, 2010.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
J.-S. Leu, T.-H. Chiang, M.-C. Yu, and K.-W. Su, “Energy efficient clustering
scheme for prolonging the lifetime of wireless sensor network with isolated
nodes,” <em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">IEEE communications letters</em>, vol. 19, no. 2, pp. 259–262,
2014.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1909.11874" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1909.11875" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1909.11875">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1909.11875" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1909.11876" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 10:45:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
