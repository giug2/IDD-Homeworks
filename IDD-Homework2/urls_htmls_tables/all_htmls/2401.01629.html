<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.01629] Synthetic Data in AI: Challenges, Applications, and Ethical Implications</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data in AI: Challenges, Applications, and Ethical Implications">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data in AI: Challenges, Applications, and Ethical Implications">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.01629">

<!--Generated on Tue Feb 27 10:55:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data in AI: Challenges, Applications, and Ethical Implications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuang Hao, Wenfeng Han, Tao Jiang, Yiping Li, Haonan Wu,
<br class="ltx_break">Chunlin Zhong, Zhangjun Zhou, He Tang 
<br class="ltx_break">
<br class="ltx_break">School of Software Engineering, Huazhong University of Science and Technology
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">hetang@hust.edu.cn</span>
</span><span class="ltx_author_notes">Corresponding author: <span id="id2.2.id1" class="ltx_text ltx_font_typewriter">hetang@hust.edu.cn</span></span></span>
</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant. This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor. It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains. The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The generation of synthetic data</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Real data typically refers to data collected directly from the real world, covering text, images, video, audio and so on. However, due to its inherent limitations and incompleteness, issues such as data imbalance<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and data discrimination<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> arise in practical applications. Since it is difficult to satisfy the demand relying solely on real data, researchers start to employ diverse methods for generating synthetic data through existing real data. These methods range from traditional statistical models to contemporary advanced techniques based on deep learning models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Statistical Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Statistical Models for generating synthetic datasets often rely on the analysis of the distribution, relationships, and characteristics of real data and attempt to generate synthetic data with similar properties by simulating these statistical properties. The main methods are summarized as follows:

<br class="ltx_break"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Distribution-based methods.</span> This method aims to simulate the distribution characteristics of the original data. For continuous data, probability density functions (PDFs) are used to describe the distribution, while probability mass functions (PMFs) are used for discrete data. When synthesizing data, new data points can be generated by sampling from the distribution of existing data.

<br class="ltx_break"><span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_bold">Interpolation and Extrapolation.</span> Interpolation and extrapolation involve generating new data points between or beyond existing data points. This is particularly useful for time series, geographical data, etc. One common interpolation method is linear interpolation, where the value of a new point is determined by a linear relationship between two adjacent known points.

<br class="ltx_break"><span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_bold">Monte Carlo Simulation.</span> Monte Carlo simulation employs random sampling to simulate the uncertainty in real systems. In data synthesis, this method is utilized to generate new samples by randomly sampling from known distributions. It finds common applications in finance, engineering, and physics modeling.

<br class="ltx_break"><span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_bold">Model-based Sampling.</span> This method involves utilizing statistical models of existing data to predict new data points. For example, a linear regression model can be fitted to existing data, and new data points can be generated by randomly sampling the model parameters. This approach is particularly effective for data exhibiting linear relationships.

<br class="ltx_break"><span id="S2.SS1.p1.1.5" class="ltx_text ltx_font_bold">Kernel Density Estimation.</span> Kernel density estimation involves placing kernels (typically Gaussian kernels) around each data point and calculating the contribution at each point to estimate the probability density function. When generating new samples, random sampling can be performed from the estimated probability density function. This is useful for capturing the complexity and multimodality of data distributions.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Deep learning based generation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In light of the rapid advancements in deep learning over recent years, scholars have increasingly directed their attention toward harnessing deep learning methodologies for the generation of synthetic datasets. In contrast to traditional statistical-based methods, deep learning approaches have the capability to acquire more intricate feature representations of data without the need for manually designing feature extractors. Their inherent nonlinearity makes them well-suited for adapting to the complex nonlinear relationships within data, enabling a more effective capture of the essential characteristics in the data.
The following section outlines several typical approaches to data generation based on deep learning.

<br class="ltx_break"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Variational Auto-Encoder(VAE).</span>
VAE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is a kind of probabilistic generative model, which employs the encoder-decoder architecture. The encoder maps the input data to the underlying latent space corresponding to the parameters of the variational distribution and the decoder projects features from the latent space back into the input space. By capturing the distribution of latent space features, the VAE can generate multiple distinct samples that follow the same distribution.The inherent randomness of the VAE introduces a degree of diversity in the generated data, making them more representative of the complexity in real datasets.

<br class="ltx_break"><span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_bold">Generative Adversarial Networks(GAN).</span>
GAN was first proposed by Goodfellow <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.3.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S2.SS2.p1.1.3.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> in 2014, which consists of a generator and a discriminator. The generator randomly samples from the latent space, in order to produce samples resembling the training set. Meanwhile, the discriminator whether the sample belongs to real or synthetic data. These two parts engages in a continual adversarial learning process and updates parameters alternately until the generator is able to synthesise high quality samples to fool the discriminator.

<br class="ltx_break"><span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_bold">Diffusion models.</span>
Diffusion model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> stands as a robust method for crafting synthetic datasets, relying on a systematic approach to emulate intricate temporal dependencies within data. Rooted in the principles of diffusion processes, where information spreads through a network over time, this technique integrates these models into the data generation process. By doing so, it enables the replication of nuanced patterns and relationships observed in authentic data. The crux lies in accurately capturing the dynamic evolution of information over time. This technique, grounded in diffusion models, not only reproduces statistical characteristics but also adeptly mirrors the complex temporal dynamics present in real-world datasets. This algorithmic approach proves invaluable for generating synthetic datasets, offering a sophisticated tool for simulating realistic data scenarios essential for the training and evaluation of machine learning models across diverse domains. To date, synthetic data generated from diffusion models has found extensive application across various vision tasks, notably enhancing performance in areas such as image classification<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and semantic segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.

<br class="ltx_break"><span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_bold">Large language models.</span>
In the past years, large language models (LLMs) have emerged as a revolutionary approach for generating synthetic datasets.
For instance, models such as GPT-3.5, with their exceptional in-context learning capabilities and extensive pre-trained linguistic knowledge, exemplify the capacity of large language models (LLMs) to produce synthetic datasets. This capability facilitates the training of models on smaller domains, effectively addressing the challenge of data scarcity in specific areas.
The Generative Pre-trained Transformer (GPT) family<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> comprises a series of Natural Language Processing (NLP) models developed by OpenAI, employing the Transformer architecture to capture long-distance dependencies in input sequences. GPT is unsupervisedly pre-trained on large-scale Internet text corpora, for example, to predict the next word in a given context. This pre-training strategy enables the model to acquire a profound understanding of linguistic statistical structure and contextual relationships and ultimately to perform a variety of natural language processing tasks in a pre-training-fine-tuning format.
For example, Josifoski <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.6.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S2.SS2.p1.1.6.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> synthetically generate a dataset of 1.8M data points in a reverse manner and demonstrate the effectiveness of this approach on closed information extraction. Whitehouse <span id="S2.SS2.p1.1.7" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.7.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.SS2.p1.1.7.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4 to augment three datasets and assess the naturalness and logical coherence of the generated examples.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The usage of synthetic data.</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Existing synthetic datasets.</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Synthetic data possesses several advantages that natural data lacks, making it an attractive choice in many fields. Compared to natural data, synthetic datasets are relatively easy to acquire and can provide data in rare or challenging scenarios, thereby addressing diversity issues in certain datasets. Additionally, this technology can effectively avoid privacy concerns, safeguarding user information security. As this technology gradually becomes more prominent, its practical applications are becoming increasingly widespread. This section will discuss several domains where generated data has had a significant impact.

<br class="ltx_break"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Vision</span>
Synthetic datasets can greatly address the challenge of acquiring natural data in certain domains. In the early days of computer vision, the generation of corresponding datasets relied primarily on computer graphics engines<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. For example, in the re-identification (Re-ID) domain, Sun <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S3.SS1.p1.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> created the PersonX dataset using the a engine based on Unity. This dataset includes three pure color backgrounds and three scene backgrounds. It consists of 1266 hand-crafted identities (547 females and 719 males), with each identity having 36 viewpoints. At that time, this dataset effectively addressed the lack of multi-viewpoint data. Furthermore<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, there are also generated methods based on Generative Adversarial Networks (GAN). In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, GIRAFFE introduces synthetic neural feature maps, enabling control over camera poses, object placements and orientations, as well as object shapes and appearances during generation. Moreover, GIRAFFE allows for the free addition of multiple objects in a scene, expanding the generated scenes from single-object to multi-object scenarios.

<br class="ltx_break"><span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">Audio</span>
Synthetic data is widely employed in the field of audio, and its rapid development is truly remarkable. Take, for instance, the Speech Commands Dataset proposed by Chris Donahue <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">et al.</span>, which leverages WaveGAN, a generative adversarial network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. WaveGAN excels in synthesizing one-second audio waveform slices with global coherence, making it particularly well-suited for sound effects generation. Even without labels, when trained on small vocabulary speech datasets, WaveGAN adeptly learns to generate intelligible words and can extend its synthesis capabilities to audio from diverse domains, including drums, bird sounds, and piano.Furthermore, Zhang <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">et al.</span>introduced Stutter-TTS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, tailored to tackle the performance challenges faced by existing Automatic Speech Recognition (ASR) interfaces when dealing with stuttered speech. Stutter-TTS stands as an end-to-end neural text-to-speech model with the proficiency to synthesize various forms of stuttered speech. It employs a simple yet effective prosody control strategy and incorporates additional markers during training to represent specific stuttering features. By strategically selecting the positions of these markers, Stutter-TTS provides word-level control over the occurrence of stuttering in the synthesized speech.

<br class="ltx_break"><span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_bold">Natural Language Processing (NLP)</span>
The growing interest in synthetic data has propelled the thriving development of numerous deep generative models in the field of Natural Language Processing (NLP)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. In recent years, machine learning has demonstrated its formidable capabilities in tasks such as classification, routing, filtering, and information retrieval across various domains<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.Addressing the challenge of synonym variations arising from contextual changes in NLP, researchers have introduced the BLEURT model. Built upon BERT, this model simulates human judgment by utilizing a limited set of training examples that may exhibit biases. To enhance the model’s generalization, innovative pre-training approaches have been developed using millions of synthetic examples<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.Furthermore, RelGAN, developed by Rice University, represents a significant breakthrough in text generation using Generative Adversarial Networks (GAN). Comprising three key components - a relation memory-based generator, Gumbel-Softmax relaxation algorithm, and multiple embedded representations in the discriminator - RelGAN outperforms several state-of-the-art models in benchmark tests, showcasing its remarkable performance in sample quality and diversity. This underscores its potential for further research and application across a wide range of NLP tasks and challenges<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.

<br class="ltx_break"><span id="S3.SS1.p1.1.7" class="ltx_text ltx_font_bold">Health</span>
In the realm of healthcare, the generation of synthetic data plays a pivotal role in comprehending diseases while upholding patient confidentiality and privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Synthetic data possesses the capability to mirror the original data distribution without disclosing actual patient information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. A notable example in healthcare is MedGAN, a model introduced by Edward Choi <span id="S3.SS1.p1.1.8" class="ltx_text ltx_font_italic">et al.</span>, leveraging adversarial networks to generate authentic synchronized medical records. Through the integration of autoencoders and generative adversarial networks, MedGAN proficiently produces high-dimensional discrete variables (e.g., binary features and counting features) based on genuine medical records [16]. Synthetic patient records generated by MedGAN have demonstrated comparable performance to real data across various experiments, encompassing distribution statistics, predictive modeling tasks, and assessments by medical experts. Furthermore, synthetic data finds widespread application in the realm of drug discovery. The predominant approach involves learning the distribution of drug molecules from existing databases and subsequently deriving new samples (i.e., drug molecules) from the acquired knowledge of drug molecule distributions. Numerous implementations of this process exist, such as variant autoencoders (VAEs)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, energy-based models (EBMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, reinforcement learning (RL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, genetic algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, sampling-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, and others.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Summarization of Representative Works in Synthetic Data Generation.</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Datasets</th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Generated Method</th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Applicantion</th>
<th id="S3.T1.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Size</th>
<th id="S3.T1.4.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Content</th>
<th id="S3.T1.4.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">DL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<td id="S3.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.4.2.1.1.1" class="ltx_text">Kubric</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">3D-Rendered</td>
<td id="S3.T1.4.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Vison</td>
<td id="S3.T1.4.2.1.4" class="ltx_td ltx_align_left ltx_border_t">——</td>
<td id="S3.T1.4.2.1.5" class="ltx_td ltx_align_left ltx_border_t">3D Image/Video</td>
<td id="S3.T1.4.2.1.6" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<td id="S3.T1.4.3.2.1" class="ltx_td ltx_align_left">
<span id="S3.T1.4.3.2.1.1" class="ltx_text">Structured3D</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_left">3D-Rendered</td>
<td id="S3.T1.4.3.2.3" class="ltx_td ltx_align_left">Vison</td>
<td id="S3.T1.4.3.2.4" class="ltx_td ltx_align_left">196,515 Frames</td>
<td id="S3.T1.4.3.2.5" class="ltx_td ltx_align_left">3D Image/Video</td>
<td id="S3.T1.4.3.2.6" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<td id="S3.T1.4.4.3.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.4.4.3.1.1" class="ltx_text">PersonX</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_left ltx_border_t">Physically Realistic Engines</td>
<td id="S3.T1.4.4.3.3" class="ltx_td ltx_align_left ltx_border_t">Vison</td>
<td id="S3.T1.4.4.3.4" class="ltx_td ltx_align_left ltx_border_t">1266 Images</td>
<td id="S3.T1.4.4.3.5" class="ltx_td ltx_align_left ltx_border_t">Person image</td>
<td id="S3.T1.4.4.3.6" class="ltx_td ltx_align_left ltx_border_t">✕</td>
</tr>
<tr id="S3.T1.4.5.4" class="ltx_tr">
<td id="S3.T1.4.5.4.1" class="ltx_td ltx_align_left">
<span id="S3.T1.4.5.4.1.1" class="ltx_text">GCC</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S3.T1.4.5.4.2" class="ltx_td ltx_align_left">Physically Realistic Engines</td>
<td id="S3.T1.4.5.4.3" class="ltx_td ltx_align_left">Vison</td>
<td id="S3.T1.4.5.4.4" class="ltx_td ltx_align_left">15,212 Images</td>
<td id="S3.T1.4.5.4.5" class="ltx_td ltx_align_left">Crowd image</td>
<td id="S3.T1.4.5.4.6" class="ltx_td ltx_align_left">✕</td>
</tr>
<tr id="S3.T1.4.6.5" class="ltx_tr">
<td id="S3.T1.4.6.5.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.4.6.5.1.1" class="ltx_text">BigGANs</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S3.T1.4.6.5.2" class="ltx_td ltx_align_left ltx_border_t">GAN</td>
<td id="S3.T1.4.6.5.3" class="ltx_td ltx_align_left ltx_border_t">Vison</td>
<td id="S3.T1.4.6.5.4" class="ltx_td ltx_align_left ltx_border_t">——</td>
<td id="S3.T1.4.6.5.5" class="ltx_td ltx_align_left ltx_border_t">Image Annotation</td>
<td id="S3.T1.4.6.5.6" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S3.T1.4.7.6" class="ltx_tr">
<td id="S3.T1.4.7.6.1" class="ltx_td ltx_align_left">
<span id="S3.T1.4.7.6.1.1" class="ltx_text">GIRAFFE</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S3.T1.4.7.6.2" class="ltx_td ltx_align_left">GAN</td>
<td id="S3.T1.4.7.6.3" class="ltx_td ltx_align_left">Vison</td>
<td id="S3.T1.4.7.6.4" class="ltx_td ltx_align_left">——</td>
<td id="S3.T1.4.7.6.5" class="ltx_td ltx_align_left">Mutil-View image</td>
<td id="S3.T1.4.7.6.6" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S3.T1.4.8.7" class="ltx_tr">
<td id="S3.T1.4.8.7.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.4.8.7.1.1" class="ltx_text">SyntheticData</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.4.8.7.2" class="ltx_td ltx_align_left ltx_border_t">Diffusion Model</td>
<td id="S3.T1.4.8.7.3" class="ltx_td ltx_align_left ltx_border_t">Vison</td>
<td id="S3.T1.4.8.7.4" class="ltx_td ltx_align_left ltx_border_t">——</td>
<td id="S3.T1.4.8.7.5" class="ltx_td ltx_align_left ltx_border_t">Generate Rare Species Data</td>
<td id="S3.T1.4.8.7.6" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S3.T1.4.9.8" class="ltx_tr">
<td id="S3.T1.4.9.8.1" class="ltx_td ltx_align_left">
<span id="S3.T1.4.9.8.1.1" class="ltx_text"></span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.4.9.8.2" class="ltx_td ltx_align_left">Diffusion Model</td>
<td id="S3.T1.4.9.8.3" class="ltx_td ltx_align_left">Vison</td>
<td id="S3.T1.4.9.8.4" class="ltx_td ltx_align_left">——</td>
<td id="S3.T1.4.9.8.5" class="ltx_td ltx_align_left">Improved Imagenet</td>
<td id="S3.T1.4.9.8.6" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S3.T1.4.10.9" class="ltx_tr">
<td id="S3.T1.4.10.9.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.4.10.9.1.1" class="ltx_text">SynthIE</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T1.4.10.9.2" class="ltx_td ltx_align_left ltx_border_t">LLM</td>
<td id="S3.T1.4.10.9.3" class="ltx_td ltx_align_left ltx_border_t">NLP</td>
<td id="S3.T1.4.10.9.4" class="ltx_td ltx_align_left ltx_border_t">1.8M data
points</td>
<td id="S3.T1.4.10.9.5" class="ltx_td ltx_align_left ltx_border_t">Text</td>
<td id="S3.T1.4.10.9.6" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S3.T1.4.11.10" class="ltx_tr">
<td id="S3.T1.4.11.10.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.4.11.10.1.1" class="ltx_text">Gen-X</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T1.4.11.10.2" class="ltx_td ltx_align_left ltx_border_bb">LLM</td>
<td id="S3.T1.4.11.10.3" class="ltx_td ltx_align_left ltx_border_bb">NLP</td>
<td id="S3.T1.4.11.10.4" class="ltx_td ltx_align_left ltx_border_bb">——</td>
<td id="S3.T1.4.11.10.5" class="ltx_td ltx_align_left ltx_border_bb">Data Augmentation</td>
<td id="S3.T1.4.11.10.6" class="ltx_td ltx_align_left ltx_border_bb">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The data distribution of synthetic datasets.</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the rapid advancement of artificial intelligence and machine learning, synthetic datasets have become an essential resource. These datasets, typically algorithmically generated, are used for training and testing a variety of models. However, the generation process of synthetic datasets often harbors implicit issues, especially regarding the fairness and representativeness of data distribution. These issues can affect the performance of models and potentially lead to biases and discriminatory practices in real-world applications.

<br class="ltx_break"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Distribution Issues in Synthetic Datasets.</span>
The generation of synthetic datasets often lacks sufficient consideration for demographic diversity, which can lead to unbalanced data distributions in terms of gender, age, race, etc. For instance, in the creation of datasets, if the data is primarily based on individuals from specific racial or age groups, the trained models might perform poorly when dealing with other groups. This situation can lead to severe discriminatory issues in real-world applications, such as certain racial groups being incorrectly identified or entirely overlooked. For instance, if a dataset used to train a facial recognition system disproportionately represents certain demographics over others, the resulting AI models may exhibit biased performance, leading to unfair or discriminatory outcomes. These biases in the data can manifest in various forms, such as overrepresentation or underrepresentation of specific groups, leading to skewed perceptions and decisions made by AI systems.

<br class="ltx_break"><span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">Biases in AI Artistic Creation.</span> Biases in AI artistic creation exemplify how dataset distribution issues can profoundly affect the application of artificial intelligence, often reflecting and even amplifying underlying societal biases. Artistic creation, as an arena where AI has been increasingly applied, offers a stark view into the ramifications of these biases.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">When AI systems are employed to process, interpret, or generate artworks from diverse cultural backgrounds or artistic styles, they often manifest clear biases towards certain genres, styles, or races. This phenomenon primarily stems from the composition of the training datasets. If a dataset is heavily skewed towards artworks from a particular cultural or stylistic background, the AI is more likely to develop a bias towards that specific style or culture. This bias can significantly influence the AI’s artistic outputs, subtly shaping the characteristics of the generated artwork in ways that reflect the predominant features of the training data.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The case of GoArt applying an expressionist filter to Clementine Hunter’s painting ”Black Matriarch” is a telling example<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. In this scenario, the AI altered the black skin tones to red, a choice that seems inexplicable without considering the training data’s influence. In contrast, when processing a sculpture like Desiderio’s ”Giovinetto,” which features a figure with white skin, the AI preserved the artwork’s original color palette. These differences in treatment can be indicative of the AI’s learned preferences, potentially influenced by the nature of the training data, which might have contained more frequent representations of certain skin tones within specific artistic contexts.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Such biases in AI artistic creation are not merely academic concerns; they have real-world implications. They can inadvertently perpetuate stereotypes, reinforce cultural hegemonies, and marginalize underrepresented groups. This issue underscores the importance of curating diverse and inclusive datasets, especially in fields like art, where representation and expression are crucial. Furthermore, it highlights the necessity for continuous scrutiny and evaluation of AI models and their outputs to identify and mitigate biases.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">In this context, it’s crucial to view AI not just as a technical tool but as an entity shaped by human decisions and societal structures. The choices made in dataset compilation and algorithm design profoundly impact the AI’s behavior, echoing the creators’ conscious and unconscious biases. Addressing these issues requires a concerted effort to integrate ethical considerations, cultural sensitivity, and diversity into every stage of AI development, from dataset creation to model training and application.

<br class="ltx_break"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Lack of Ethical and Legal Constraints.</span>
Beyond the challenges posed by statistical distribution, synthetic datasets may also suffer from a lack of necessary ethical and legal constraints during their creation process. This oversight can lead to significant issues, particularly in the context of how data generation algorithms process and interpret the input data. Often, these algorithms may inadvertently learn and replicate biases that are inherent in real-world data sources. This issue is especially pronounced in scenarios involving gender or racial biases.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">Moreover, the problem is exacerbated when synthetic datasets rely on publicly available internet data. The internet, as a vast repository of human-generated content, inherently contains a myriad of biases and prejudices that exist within society. This data is often unfiltered and includes implicit societal biases, stereotypes, and even offensive or harmful representations. When such data is used without critical filtering or ethical consideration, the resulting synthetic datasets can inadvertently become a medium through which these societal biases are perpetuated and amplified. The roots of such biases are multifaceted, yet they all converge on a common issue: societal prejudices can infiltrate the process of AI’s artistic creation.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">Research<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> indicates that this problem is not limited to synthetic datasets; it also plagues datasets collected from the internet. A prime example is the ”Tiny Images” dataset from the Massachusetts Institute of Technology. Compiled using extensive image and label data aggregated from search engines, this dataset was found to contain tendencies of racial and gender discrimination, and even instances of pedophilia, leading to its eventual permanent removal. The emergence of these issues is partly due to the influence of societal biases and partly reflects negligence in the construction of datasets.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">In essence, the generation of synthetic datasets requires not only a sophisticated understanding of statistical methodologies but also a deep consideration of the ethical and legal implications. Ensuring fairness and representativeness in these datasets necessitates a comprehensive approach that includes ethical oversight, legal compliance, and an active effort to identify and mitigate any potential biases. This approach should be an integral part of the dataset creation process, ensuring that AI systems trained on these datasets do not perpetuate existing societal biases but rather contribute to fair and equitable outcomes.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Risks and Challenges in Utilizing Synthetic Datasets for AI.</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">While synthetic data plays a crucial role in AI applications, the current methods of generating synthetic datasets bring forth notable challenges and potential risks, necessitating careful consideration of their applications.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Shortcomings of Synthetic Data.</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Data Distribution Bias.</span> A discernible incongruity exists between synthetic datasets and their authentic counterparts, encompassing notable disparities in feature distribution, class distribution, and other pertinent statistical attributes. This bias imparts a proclivity for models to engender misleading prognostications within practical applications, thereby compromising their fidelity to faithfully encapsulate real-world phenomena.

<br class="ltx_break"><span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">Incomplete Data.</span>The presence of lacunae or partial information within synthetic datasets, ostensibly stemming from imperfections, errors, or an inadequacy in encapsulating the manifold changes inherent in authentic datasets during the synthetic generation process. This dearth of comprehensive information may impede the model’s capacity to accurately prognosticate or manage scenarios characterized by data incompleteness, thereby influencing the model’s resilience and pragmatic utility.

<br class="ltx_break"><span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_bold">Inaccurate Data.</span>
The manifestation of errors, noise, or inaccuracies within synthetic datasets, diverging significantly from the veracity of real-world datasets. This discrepancy may arise from algorithmic imperfections, noise injection, or other contributory factors that give rise to inaccuracies. Consequently, the model may internalize erroneous patterns, thereby inducing biased predictions and undermining the overall performance and reliability of the model when confronted with genuine data.

<br class="ltx_break"><span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_bold">Insufficient Noise Level.</span>
Synthetic datasets may evince an undue sterility, lacking the multifarious noise and intricacies inherent in real-world data. In authentic scenarios, data invariably incorporates diverse interferences, errors, and uncertainties. The paucity of such features within synthetic datasets may hamper the model’s efficacy within realistic environments.

<br class="ltx_break"><span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_bold">Over-Smoothing.</span>
In the process of synthetic data generation, certain models may overly attenuate or oversimplify the data, resulting in an attenuated representation devoid of the nuanced details and diversity inherent in authentic datasets. This propensity may precipitate challenges for the model in assimilating complex variations within genuine data.

<br class="ltx_break"><span id="S4.SS1.p1.1.6" class="ltx_text ltx_font_bold">Neglecting Temporal and Dynamic Aspects.</span>
Certain methodologies for synthetic data generation may inadequately capture temporal and dynamic nuances, facets that are inherently pivotal within authentic datasets. The consequential failure to faithfully simulate these temporal intricacies may culminate in the ineffectuality of models in real-world applications.

<br class="ltx_break"><span id="S4.SS1.p1.1.7" class="ltx_text ltx_font_bold">Inconsistency.</span>
The paucity of inconsistency within synthetic datasets, relative to the rich tapestry inherent in authentic datasets. The latter frequently embodies variations stemming from diverse sources, temporal epochs, and environmental conditions, aspects that synthetic datasets often fail to encapsulate. This shortfall may engender challenges for models in adapting to the multifaceted vicissitudes originating from disparate sources, temporal epochs, and environmental conditions, thereby precipitating a decrement in the generalization performance vis-à-vis diverse datasets.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Risks in Synthetic Data Application.</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">General Model Performance</span>
Widespread use of synthetic data may constrain the generalization performance of AI models. For instance, datasets like PersonX, generated within a gaming data engine, may deviate significantly from real-world data. In natural language processing, relying on fine-tuning with large language models may restrict downstream tasks to the performance and biases of the selected model. In healthcare, an abundance of non-real cases during model training may undermine confidence in diagnostic results among healthcare professionals and patients.

<br class="ltx_break"><span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">Ethical and Social Implications</span>
The use of synthetic data may give rise to ethical and social concerns. Creating fictional characters or scenarios through synthetic data raises questions about AI responsibility in generating fictional content, potentially leading to misinformation, misunderstandings, or the dissemination of false information with detrimental societal impacts.

<br class="ltx_break"><span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_bold">Security and Adversarial Risks</span>
The introduction of synthetic data brings forth security and adversarial attack risks. Malicious use of synthetic data may render AI models unstable during adversarial attacks, as models may not adequately learn the complexity and diversity of the real world during training. This susceptibility increases the likelihood of deception or manipulation, posing threats to the credibility and security of the system.

<br class="ltx_break"><span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_bold">Legal Compliance Challenges</span>
In certain domains, the use of synthetic data may present challenges regarding legal compliance. For instance, employing synthetic data for risk assessment in the financial sector may encounter regulatory hurdles. Regulatory authorities often require transparent and interpretable models, and the synthetic data generation process may face difficulties in meeting these standards.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In summary, the use of synthetic datasets in AI introduces a range of potential risks that require careful assessment and management.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>New Approaches to Synthetic Data.</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To address the current issues associated with synthetic data generation, there is a need for continuous development and adoption of new methods.

<br class="ltx_break"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Adopting more advanced generative models. </span>One potential approach involves the use of advanced generative models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). These models possess stronger learning capabilities, allowing for more accurate modeling of the complex distribution of real-world data. By employing these advanced models, it becomes possible to better avoid distribution shift issues, enhance the diversity of generated data, and more effectively simulate the noise and uncertainty present in the real world.

<br class="ltx_break"><span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_bold">Integrating domain-specific expertise to enhance the realism of synthetic data. </span>Integrating domain-specific knowledge, such as computer graphics, physics, and cognitive science, can contribute to improving the realism of synthetic data. A deeper understanding of the physical laws behind scenes and cognitive processes can lead to more precise generation of various scenarios, making synthetic data closer to real-world situations.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Regulating the Use of Synthetic Data.</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To regulate the use of synthetic data, establishing a set of clear guidelines is crucial. 
<br class="ltx_break"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Establishing industry standards. </span>Industry standards should be developed to outline best practices for synthetic data generation, covering the selection of data generation models, parameter settings, and the correlation between synthetic and real data.

<br class="ltx_break"><span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">Transparency and documentation. </span>Transparency and documentation are equally essential. Researchers and practitioners should clearly document the methods and parameter settings used for generating synthetic data, providing detailed documentation about the synthetic data set. This aids other researchers in understanding the source and characteristics of the data, facilitating a better assessment of the model’s performance.

<br class="ltx_break"><span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">Emphasizing model validation and evaluation. </span>Emphasizing model validation and evaluation is a crucial step in regulating the use of synthetic data. In addition to training on synthetic data, models should be validated on real data to ensure their generalization performance and robustness. Regularly updating synthetic data sets to adapt to new scenarios and changes in data distribution is also a vital means of maintaining model performance.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
F. Thabtah, S. Hammoud, F. Kamalov, and A. Gonsalves, “Data imbalance in classification: Experimental evaluation,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Information Sciences</span>, vol. 513, pp. 429–441, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Favaretto, E. De Clercq, and B. S. Elger, “Big data and discrimination: perils, promises and solutions. a systematic review,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Journal of Big Data</span>, vol. 6, no. 1, pp. 1–27, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, vol. 63, no. 11, pp. 139–144, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 6840–6851, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet, “Synthetic data from diffusion models improves imagenet classification,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.08466</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Fang, B. Han, S. Zhang, S. Zhou, C. Hu, and W.-M. Ye, “Data augmentation for object detection via controllable diffusion models,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">WACV 2024</span>, 2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Q. H. Nguyen, T. T. Vu, A. T. Tran, and K. Nguyen, “Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Improving language understanding by generative pre-training,” 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Language models are unsupervised multitask learners,” <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">OpenAI blog</span>, vol. 1, no. 8, p. 9, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Language models are few-shot learners,” <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Josifoski, M. Sakota, M. Peyrard, and R. West, “Exploiting asymmetry for synthetic training data generation: SynthIE and the case of information extraction,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span> (H. Bouamor, J. Pino, and K. Bali, eds.), (Singapore), pp. 1555–1574, Association for Computational Linguistics, Dec. 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Whitehouse, M. Choudhury, and A. Aji, “LLM-powered data augmentation for enhanced cross-lingual performance,” in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span> (H. Bouamor, J. Pino, and K. Bali, eds.), (Singapore), pp. 671–686, Association for Computational Linguistics, Dec. 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X. Sun and L. Zheng, “Dissecting person re-identification from the viewpoint of viewpoint,” in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 608–617, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Yao, L. Zheng, X. Yang, M. Naphade, and T. Gedeon, “Simulating content consistent vehicle datasets with attribute descent,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16</span>, pp. 775–791, Springer, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Tang, M. Naphade, S. Birchfield, J. Tremblay, W. Hodge, R. Kumar, S. Wang, and X. Yang, “Pamtri: Pose-aware multi-task learning for vehicle re-identification using highly randomized synthetic data,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 211–220, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Q. Wang, J. Gao, W. Lin, and Y. Yuan, “Learning from synthetic data for crowd counting in the wild,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 8198–8207, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pp. 2223–2232, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R. Torkzadehmahani, P. Kairouz, and B. Paten, “Dp-cgan: Differentially private synthetic data and label generation,” in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</span>, pp. 0–0, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for high fidelity natural image synthesis,” in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Niemeyer and A. Geiger, “Giraffe: Representing scenes as compositional generative neural feature fields,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 11453–11464, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Donahue, J. McAuley, and M. Puckette, “Adversarial audio synthesis,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.04208</span>, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
X. Zhang, I. Vallés-Pérez, A. Stolcke, C. Yu, J. Droppo, O. Shonibare, R. Barra-Chicote, and V. Ravichandran, “Stutter-tts: Controlled synthesis and improved recognition of stuttered speech,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2211.09731</span>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C. Dewi, R.-C. Chen, Y.-T. Liu, and S.-K. Tai, “Synthetic data generation using dcgan for improved traffic sign recognition,” <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Neural Computing and Applications</span>, vol. 34, no. 24, pp. 21465–21480, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
G. Forman, “An extensive empirical study of feature selection metrics for text classification,” <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, vol. 3, pp. 1289–1305, 2003.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
X. Yue, H. A. Inan, X. Li, G. Kumar, J. McAnallen, H. Sun, D. Levitan, and R. Sim, “Synthetic text generation with differential privacy: A simple and practical recipe,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.14348</span>, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
X. Zheng, Y. Liu, D. Gunceler, and D. Willett, “Using synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end asr systems,” in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
W. Nie, N. Narodytska, and A. Patel, “Relgan: Relational generative adversarial networks for text generation,” in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">International conference on learning representations</span>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Z. Zhao, A. Zhu, Z. Zeng, B. Veeravalli, and C. Guan, “Act-net: Asymmetric co-teacher network for semi-supervised memory-efficient medical image segmentation,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">2022 IEEE International Conference on Image Processing (ICIP)</span>, pp. 1426–1430, IEEE, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Dahmen and D. Cook, “Synsys: A synthetic data generation system for healthcare applications,” <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 19, no. 5, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Y. Lu, Y. T. Chang, E. P. Hoffman, G. Yu, and Y. Wang, “Integrated identification of disease specific pathways using multi-omics data,” <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Cold Spring Harbor Laboratory</span>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Z. Wang, P. Myles, and A. Tucker, “Generating and evaluating cross:ectional synthetic electronic healthcare data: Preserving data utility and patient privacy,” <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Computational Intelligence</span>, no. 3, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
W. Jin, R. Barzilay, and T. Jaakkola, “Junction tree variational autoencoder for molecular graph generation,” 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, “Automatic chemical design using a data-driven continuous representation of molecules,” <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">ACS central science</span>, vol. 4, no. 2, pp. 268–276, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
B. Zhang, Y. Fu, Y. Lu, Z. Zhang, R. Clarke, J. E. Van Eyk, D. M. Herrington, and Y. Wang, “Ddn2. 0: R and python packages for differential dependency network analysis of biological systems,” <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">bioRxiv</span>, pp. 2021–04, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
N. De Cao and T. Kipf, “Molgan: An implicit generative model for small molecular graphs. arxiv 2018,” <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1805.11973</span>, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T. Fu and J. Sun, “Antibody complementarity determining regions (cdrs) design using constrained energy model,” in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</span>, pp. 389–399, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T. Fu, W. Gao, C. Xiao, J. Yasonik, C. W. Coley, and J. Sun, “Differentiable scaffolding tree for molecular optimization,” <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.10469</span>, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, “Geodiff: A geometric diffusion model for molecular conformation generation,” <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.02923</span>, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Z. Zhou, S. Kearnes, L. Li, R. N. Zare, and P. Riley, “Optimization of molecules via deep reinforcement learning,” <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Scientific reports</span>, vol. 9, no. 1, p. 10752, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen, “Molecular de-novo design through deep reinforcement learning,” <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Journal of cheminformatics</span>, vol. 9, no. 1, pp. 1–14, 2017.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
T. Fu, W. Gao, C. Coley, and J. Sun, “Reinforced genetic algorithm for structure-based drug design,” <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 12325–12338, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. H. Jensen, “A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space,” <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Chemical science</span>, vol. 10, no. 12, pp. 3567–3572, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
T. Fu, C. Xiao, X. Li, L. M. Glass, and J. Sun, “Mimosa: Multi-constraint molecule sampling for molecule optimization,” in <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, vol. 35, pp. 125–133, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. Fu and J. Sun, “Sipf: Sampling method for inverse protein folding,” in <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</span>, pp. 378–388, 2022.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Kundu, A. Tagliasacchi, A. Y. Mak, A. Stone, C. Doersch, C. Oztireli, C. Herrmann, D. Gnanapragasam, D. Duckworth, D. Rebain, <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Kubric: A scalable dataset generator,” 2022.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou, “Structured3d: A large photo-realistic dataset for structured 3d modeling,” in <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16</span>, pp. 519–535, Springer, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S. Gowal, S.-A. Rebuffi, O. Wiles, F. Stimberg, D. A. Calian, and T. A. Mann, “Improving robustness using generated data,” <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol. 34, pp. 4218–4233, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. Qi, “Is synthetic data from generative models ready for image recognition?,” <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.07574</span>, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
R. Srinivasan and K. Uchino, “Biases in generative art - A causal look from the lens of art history,” <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2010.13266, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
V. U. Prabhu and A. Birhane, “Large image datasets: A pyrrhic win for computer vision?,” <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2006.16923, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.01628" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.01629" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.01629">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.01629" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.01630" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 10:55:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
