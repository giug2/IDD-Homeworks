<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.12293] ACL-Fig: A Dataset for Scientific Figure Classification</title><meta property="og:description" content="Most existing large-scale academic search engines are built to retrieve text-based information. However, there are no large-scale retrieval services for scientific figures and tables. One challenge for such services is‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ACL-Fig: A Dataset for Scientific Figure Classification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ACL-Fig: A Dataset for Scientific Figure Classification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.12293">

<!--Generated on Fri Mar  1 04:33:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ACL-Fig: A Dataset for Scientific Figure Classification
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zeba Karishma, Shaurya Rohatgi, Kavya Shrinivas Puranik 
<br class="ltx_break">The Pennsylvania State University 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">zebakarishma@gmail.com, {szr207, kzp5555}@psu.edu</span> 
<br class="ltx_break"><span id="id3.2.id2" class="ltx_ERROR undefined">\And</span>Jian Wu 
<br class="ltx_break">Old Dominion University 
<br class="ltx_break"><span id="id4.3.id3" class="ltx_text ltx_font_typewriter">jwu@cs.odu.edu</span> 
<br class="ltx_break"><span id="id5.4.id4" class="ltx_ERROR undefined">\And</span>C. Lee Giles 
<br class="ltx_break">The Pennsylvania State University 
<br class="ltx_break"><span id="id6.5.id5" class="ltx_text ltx_font_typewriter">clg20@psu.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Most existing large-scale academic search engines are built to retrieve text-based information. However, there are no large-scale retrieval services for scientific figures and tables. One challenge for such services is understanding scientific figures‚Äô semantics, such as their types and purposes. A key obstacle is the need for datasets containing annotated scientific figures and tables, which can then be used for classification, question-answering, and auto-captioning. Here, we develop a pipeline that extracts figures and tables from the scientific literature and a deep-learning-based framework that classifies scientific figures using visual features. Using this pipeline, we built the first large-scale automatically annotated corpus, <span id="id1.1.1" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>‚Äâ consisting of 112,052 scientific figures extracted from <math id="id1.1.m1.1" class="ltx_Math" alttext="\approx 56" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">‚âà</mo><mn id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">56</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><approx id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3">56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\approx 56</annotation></semantics></math>K research papers in the ACL Anthology. The <span id="id1.1.2" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†dataset contains 1,671 manually labeled scientific figures belonging to 19 categories. The dataset is accessible at
<a target="_blank" href="https://huggingface.co/datasets/citeseerx/ACL-fig" title="" class="ltx_ref ltx_href">https://huggingface.co/datasets/citeseerx/ACL-fig</a>
under a CC BY-NC license.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Figures are ubiquitous in scientific papers illustrating experimental and analytical results. We refer to these figures as <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">scientific figures</em> to distinguish them from natural images, which usually contain richer colors and gradients. Scientific figures provide a compact way to present numerical and categorical data, often facilitating researchers in drawing insights and conclusions. Machine understanding of scientific figures can assist in developing effective retrieval systems from the hundreds of millions of scientific papers readily available on the Web <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The state-of-the-art machine learning models can parse captions and shallow semantics for specific categories of scientific figures. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> However, the task of reliably classifying general scientific figures based on their visual features remains a challenge.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2301.12293/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="276" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example figures of each type in <span id="S1.F1.2.1" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Here, we propose a pipeline to build categorized and contextualized scientific figure datasets. Applying the pipeline on 55,760 papers in the ACL Anthology (downloaded from https://aclanthology.org/ in mid-2021), we built two datasets: <span id="S1.p2.1.1" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>¬†and <span id="S1.p2.1.2" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>. <span id="S1.p2.1.3" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>¬†consists of 112,052 scientific figures, their captions, inline references, and metadata. <span id="S1.p2.1.4" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†(Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ ACL-Fig: A Dataset for Scientific Figure Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is a subset of unlabeled <span id="S1.p2.1.5" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>, consisting of 1671 scientific figures, which were manually labeled into 19 categories. The <span id="S1.p2.1.6" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†dataset was used as a benchmark for scientific figure classification. The pipeline is open-source and configurable, enabling others to expand the datasets from other scholarly datasets with pre-defined or new labels.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scientific Figures Extraction</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Automatically extracting figures from scientific papers is essential for many downstream tasks, and many frameworks have been developed.
A multi-entity extraction framework called PDFMEF incorporating a figure extraction module was proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Shared tasks such as ImageCLEF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> drew attention to compound figure detection and separation. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposed a framework called <span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">PDFFigures</span> that extracted figures and captions in research papers. The authors extended their work and built a more robust framework called <span id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">PDFFigures2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. <span id="S2.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">DeepFigures</span> was later proposed to incorporate deep neural network models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scientific Figure Classification</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Scientific figure classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> aids machines in understanding figures. Early work used a visual bag-of-words representation with a support vector machine classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> applied hough transforms to recognize bar charts in document images. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> used handcrafted features to classify charts in scientific documents. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> combined convolutional neural networks (CNNs) and the deep belief networks, which showed improved performance compared with feature-based classifiers .</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.T1.1" class="ltx_p ltx_figure_panel ltx_align_center">[b]


<span id="S2.T1.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S2.T1.1.1.1.1" class="ltx_tr">
<span id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Dataset</span>
<span id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Labels</span></span>
<span id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S2.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">#Figures</span></span>
<span id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S2.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Image Source</span></span></span>
<span id="S2.T1.1.1.2.2" class="ltx_tr">
<span id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Deepchart</span>
<span id="S2.T1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">5</span>
<span id="S2.T1.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">5,000</span>
<span id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">Web Image</span></span>
<span id="S2.T1.1.1.3.3" class="ltx_tr">
<span id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Figureseer<sup id="S2.T1.1.1.3.3.1.1" class="ltx_sup">1</sup></span>
<span id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">5</span>
<span id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r">30,600</span>
<span id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_left">Web Image</span></span>
<span id="S2.T1.1.1.4.4" class="ltx_tr">
<span id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Prasad et al.</span>
<span id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">5</span>
<span id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r">653</span>
<span id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_left">Web Image</span></span>
<span id="S2.T1.1.1.5.5" class="ltx_tr">
<span id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Revision</span>
<span id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">10</span>
<span id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r">2,000</span>
<span id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_left">Web Image</span></span>
<span id="S2.T1.1.1.6.6" class="ltx_tr">
<span id="S2.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FigureQA<sup id="S2.T1.1.1.6.6.1.1" class="ltx_sup">3</sup></span>
<span id="S2.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">5</span>
<span id="S2.T1.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r">100,000</span>
<span id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_left">Synthetic figures</span></span>
<span id="S2.T1.1.1.7.7" class="ltx_tr">
<span id="S2.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DeepFigures</span>
<span id="S2.T1.1.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">2</span>
<span id="S2.T1.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1,718,000</span>
<span id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_t">Scientific Papers</span></span>
<span id="S2.T1.1.1.8.8" class="ltx_tr">
<span id="S2.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DocFigure<sup id="S2.T1.1.1.8.8.1.1" class="ltx_sup">2</sup></span>
<span id="S2.T1.1.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">28</span>
<span id="S2.T1.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r">33,000</span>
<span id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_left">Scientific Papers</span></span>
<span id="S2.T1.1.1.9.9" class="ltx_tr">
<span id="S2.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S2.T1.1.1.9.9.1.1" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span></span>
<span id="S2.T1.1.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S2.T1.1.1.9.9.2.1" class="ltx_text ltx_font_bold">19</span></span>
<span id="S2.T1.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.1.1.9.9.3.1" class="ltx_text ltx_font_bold">1,671</span></span>
<span id="S2.T1.1.1.9.9.4" class="ltx_td ltx_align_left">Scientific Papers</span></span>
<span id="S2.T1.1.1.10.10" class="ltx_tr">
<span id="S2.T1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S2.T1.1.1.10.10.1.1" class="ltx_text ltx_font_smallcaps">ACL-Fig</span> (inferred)<sup id="S2.T1.1.1.10.10.1.2" class="ltx_sup">4</sup></span>
<span id="S2.T1.1.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">-</span>
<span id="S2.T1.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S2.T1.1.1.10.10.3.1" class="ltx_text ltx_font_bold">112,052</span></span>
<span id="S2.T1.1.1.10.10.4" class="ltx_td ltx_align_left ltx_border_bb">Scientific Papers</span></span>
</span>
</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Scientific figure classification datasets. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S2.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span> 
<div id="S2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.ix1.p1.1" class="ltx_p">Only 1000 images are public.</p>
</div>
</li>
<li id="S2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span> 
<div id="S2.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.ix2.p1.1" class="ltx_p">Not publicly available.</p>
</div>
</li>
<li id="S2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3</span> 
<div id="S2.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.ix3.p1.1" class="ltx_p">Scientific-style synthesized data.</p>
</div>
</li>
<li id="S2.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4</span> 
<div id="S2.I1.ix4.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.ix4.p1.1" class="ltx_p"><span id="S2.I1.ix4.p1.1.1" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>-inferred does not contain human-assigned labels.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Figure classification Datasets</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">There are several existing datasets for figure classification such as DocFigure <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, FigureSeer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Revision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and datasets presented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (Table¬†<a href="#S2.T1" title="Table 1 ‚Ä£ Scientific Figure Classification ‚Ä£ 2 Related Work ‚Ä£ ACL-Fig: A Dataset for Scientific Figure Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). FigureQA is a public dataset that is similar to ours, consisting of over one million question-answer pairs grounded in over 100,000 synthesized scientific images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> with five styles.
Our dataset is different from FigureQA because the figures were directly extracted from research papers. Especially, the training data of <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">DeepFigures</span> are from arXiv and PubMed, labeled with only ‚Äúfigure‚Äù and ‚Äútable‚Äù, and does not include fine-granular labels. Our dataset contains fine-granular labels, inline context, and is compiled from a different domain.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2301.12293/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="230" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the data generation pipeline.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data Mining Methodology</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">The ACL Anthology is a sizable, well-maintained PDF corpus with clean metadata covering papers in computational linguistics with freely available full-text. Previous work on figure classification used a set of pre-defined categories (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which may only cover some figure types. We use an unsupervised method to determine figure categories to overcome this limitation. After the category label is assigned, each figure is automatically annotated with metadata, captions, and inline references. The pipeline includes 3 steps: figure extraction, clustering, and automatic annotation (Figure¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ Figure classification Datasets ‚Ä£ 2 Related Work ‚Ä£ ACL-Fig: A Dataset for Scientific Figure Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Figure Extraction</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">To mitigate the potential bias of a single figure extractor, we extracted figures using <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">pdffigures2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">deepfigures</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> which work in different ways. <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">PDFFigures2</span> first identifies captions and the body text because they are identified relatively accurately. Regions containing figures can then be located by identifying rectangular bounding boxes adjacent to captions that do not overlap with the body text. <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">DeepFigures</span> uses the distant supervised learning method to induce labels of figures from a large collection of scientific documents in LaTeX and XML format. The model is based on TensorBox, applying the Overfeat detection architecture to image embeddings generated using ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We utilized the publicly available model weights<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/allenai/deepfigures-open</span></span></span> trained on 4M induced figures and 1M induced tables for extraction. The model outputs the bounding boxes of figures and tables. Unless otherwise stated, we collectively refer to figures and tables together as ‚Äúfigures‚Äù.
We used multi-processing to process PDFs. Each process extracts figures following the steps below. The system processed, on average, 200 papers per minute on a Linux server with 24 cores.</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Retrieve a paper identifier from the job queue.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Pull the paper from the file system.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Extract figures and captions from the paper.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Crop the figures out of the rendered PDFs using detected bounding boxes.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i5.p1.1" class="ltx_p">Save cropped figures in PNG format and the metadata in JSON format.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Clustering Methods</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Next, we use an unsupervised method to label
extracted figures automatically. We extract visual features using
VGG16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. All input figures are scaled to a dimension of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">224\times 224</annotation></semantics></math> to be compatible with the input requirement of VGG16. The features were extracted from the second last hidden (dense) layer, consisting of
4096 features. Principal Component Analysis was adopted to reduce the dimension to 1000.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Next, we cluster figures represented by the 1000-dimension vectors using <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">k</annotation></semantics></math>-means clustering. We compare two heuristic methods to determine the optimal number of clusters, including the Elbow method and the Silhouette Analysis <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The Elbow method examines the <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">explained variation</em>, a measure that quantifies the difference between the between-group variance to the total variance, as a function of the number of clusters. The pivot point (elbow) of the curve determines the number of clusters.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">Silhouette Analysis determines the number of clusters by measuring the distance between clusters.
It considers multiple factors such as variance, skewness, and high-low differences and is usually preferred to the Elbow method. The Silhouette plot displays how close each point in one cluster is to points in the neighboring clusters, allowing us to assess the cluster number visually.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Linking Figures to Metadata</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">This module associates figures to metadata, including captions, inline reference, figure type, figure boundary coordinates, caption boundary coordinates, and figure text (text appearing on figures, only available for results from <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">PDFFigures2</span>). The figure type is determined in the clustering step above. The inline references are obtained using GROBID (see below). The other metadata fields were output by figure extractors. <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">PDFFigures2</span> and <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_smallcaps">DeepFigures</span> extract the same metadata fields except for ‚Äúimage text‚Äù and ‚Äúregionless captions‚Äù (captions for which no figure regions were found), which are only available for results of <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_smallcaps">PDFFigures2</span>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">An inline reference is a text span that contains a reference to a figure or a table. Inline references can help to understand the relationship between text and the objects it refers to. After processing a paper, GROBID outputs a TEI file (a type of XML file), containing marked-up full-text and references. We locate inline references using regular expressions and extract the sentences containing reference marks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Figure Extraction</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2301.12293/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Numbers of extracted images.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.2" class="ltx_p">The numbers of figures extracted by <span id="S4.SS1.p1.2.1" class="ltx_text ltx_font_smallcaps">PDFFigures2</span> and <span id="S4.SS1.p1.2.2" class="ltx_text ltx_font_smallcaps">DeepFigures</span> are illustrated in Figure¬†<a href="#S4.F3" title="Figure 3 ‚Ä£ 4.1 Figure Extraction ‚Ä£ 4 Results ‚Ä£ ACL-Fig: A Dataset for Scientific Figure Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which indicates a significant overlap between figures extracted by two software packages. However, either package extracted (<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\approx 5\%" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">‚âà</mo><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">5</mn><mo id="S4.SS1.p1.1.m1.1.1.3.1" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><approx id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\approx 5\%</annotation></semantics></math>) figures that were not extracted by the other package. By inspecting a random sample of figures extracted by either software package, we found that <span id="S4.SS1.p1.2.3" class="ltx_text ltx_font_smallcaps">DeepFigures</span> tended to miss cases in which two figures were vertically adjacent to each other.
We took the union of all figures extracted by both software packages to build the <span id="S4.SS1.p1.2.4" class="ltx_text ltx_font_smallcaps">ACL-Fig</span> dataset, which contains a total of 263,952 figures. All images extracted are converted to 100 DPI using standard OpenCV libraries. The total size of the data is <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\sim 25" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">‚àº</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\sim 25</annotation></semantics></math>GB before compression. Inline references were extracted using GROBID. About 78% figures have inline references.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Automatic Figure Annotation</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.3" class="ltx_p">The extraction outputs 151,900 tables and 112,052 figures. Only the figures were clustered using the <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">k</annotation></semantics></math>-means algorithm. We varied <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">k</annotation></semantics></math> from 2 to 20 with an increment of 1 to determine the number of clusters. The results were analyzed using the Elbow method and Silhouette Analysis. No evident elbow was observed in the Elbow method curve. The Silhouette diagram, a plot of the number of clusters versus silhouette score exhibited a clear turning point at <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="k=15" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">k</mi><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">ùëò</ci><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">k=15</annotation></semantics></math>, where the score reached the global maximum. Therefore, we grouped the figures into 15 clusters.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">To validate the clustering results, 100 figures randomly sampled from each cluster were visually inspected. During the inspection, we identified three new figure types: <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">word cloud</em>, <em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">pareto</em>, and <em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">venn diagram</em>. The <span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†dataset was then built using all manually inspected figures. Two annotators manually labeled and inspected these clusters. The consensus rate was measured using Cohen‚Äôs Kappa coefficient, which was <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\kappa-0.78" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">Œ∫</mi><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">‚àí</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">0.78</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><minus id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></minus><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">ùúÖ</ci><cn type="float" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">0.78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\kappa-0.78</annotation></semantics></math> (substantial agreement) for the <span id="S4.SS2.p2.1.5" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span> dataset.
For completeness, we added 100 randomly selected tables. Therefore, the <span id="S4.SS2.p2.1.6" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†dataset contains a total of 1671 figures and tables labeled with 19 classes. The distribution of all classes is shown in Figure¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Automatic Figure Annotation ‚Ä£ 4 Results ‚Ä£ ACL-Fig: A Dataset for Scientific Figure Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2301.12293/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Figure class distribution in the <span id="S4.F4.2.1" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†dataset.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Supervised Scientific Figure Classification</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.3" class="ltx_p">Based on the <span id="S5.p1.3.1" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>¬†dataset, we train supervised classifiers. The dataset was split into a training and a test set (8:2 ratio). Three baseline models were investigated. Model¬†1 is a 3-Layer CNN, trained with a categorical cross-entropy loss function and the Adam optimizer. The model contains three typical convolutional layers, each followed by a max-pooling and a drop-out layer, and three fully-connected layers. The dimensions are reduced from <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mn id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">32</cn><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">32\times 32</annotation></semantics></math> to <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mn id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><times id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></times><cn type="integer" id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">16</cn><cn type="integer" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">16\times 16</annotation></semantics></math> to <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mn id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">√ó</mo><mn id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><times id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1"></times><cn type="integer" id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">8</cn><cn type="integer" id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">8\times 8</annotation></semantics></math>. The last fully connected layer classifies the encoded vector into 19 classes.
This classifier achieves an accuracy of 59%.
</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">Model¬†2 was trained based on the VGG16 architecture ,except that the last three fully-connected layers in the original network were replaced by a long short-term memory layer, followed by dense layers for classification. This model achieved an accuracy of <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\sim 79\%" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mi id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml"></mi><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">‚àº</mo><mrow id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml"><mn id="S5.p2.1.m1.1.1.3.2" xref="S5.p2.1.m1.1.1.3.2.cmml">79</mn><mo id="S5.p2.1.m1.1.1.3.1" xref="S5.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">absent</csymbol><apply id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.3.1.cmml" xref="S5.p2.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.p2.1.m1.1.1.3.2.cmml" xref="S5.p2.1.m1.1.1.3.2">79</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\sim 79\%</annotation></semantics></math>, 20% higher than Model¬†1.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">Model¬†3 was the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, in which a figure was split into fixed-size patches. Each patch was then linearly embedded, supplemented by position embeddings. The resulting sequence of vectors was fed to a standard Transformer encoder. The ViT model achieved the best performance, with 83% accuracy.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Based on the ACL Anthology papers, we designed a pipeline and used it to build a corpus of automatically labeled scientific figures with associated metadata and context information. This corpus, named <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>, consists of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\approx 250" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml"></mi><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">‚âà</mo><mn id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">250</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><approx id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">250</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\approx 250</annotation></semantics></math>k objects, of which about 42% are figures and about 58% are tables. We also built <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">ACL-Fig-pilot</span>, a subset of <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">ACL-Fig</span>, consisting of 1671 scientific figures with 19 manually verified labels. Our dataset includes figures extracted from real-world data and contains more classes than existing datasets, e.g., DeepFigures and FigureQA.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">One limitation of our pipeline is that it used VGG16 pre-trained on ImageNet. In the future, we will improve figure representation by retraining more sophisticated models, e.g., CoCa, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, on scientific figures. Another limitation was that determining the number of clusters required visual inspection. We will consider density-based methods to fully automate the clustering module.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Madian Khabsa and C.¬†Lee Giles.

</span>
<span class="ltx_bibblock">The number of scholarly documents on the public web.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">PLoS ONE</span>, 9(5):e93949, May 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Noah Siegel, Nicholas Lourie, Russell Power, and Waleed Ammar.

</span>
<span class="ltx_bibblock">Extracting scientific figures with distantly supervised neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 18th ACM/IEEE on JCDL</span>, May 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jian Wu, Jason Killian, Huaiyu Yang, Kyle Williams, Sagnik¬†Ray Choudhury,
Suppawong Tuarob, Cornelia Caragea, and C.¬†Lee Giles.

</span>
<span class="ltx_bibblock">Pdfmef: A multi-entity knowledge extraction framework for scholarly
documents and semantic search.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the 8th International Conference on Knowledge
Capture</span>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Alba Garc√≠a¬†Seco de¬†Herrera, Henning M√ºller, and Stefano Bromuri.

</span>
<span class="ltx_bibblock">Overview of the imageclef 2015 medical classification task.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">CLEF</span>, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Christopher Clark and S.¬†Divvala.

</span>
<span class="ltx_bibblock">Looking beyond text: Extracting figures, tables and captions from
computer science papers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">AAAI Workshop: Scholarly Big Data</span>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Christopher Clark and S.¬†Divvala.

</span>
<span class="ltx_bibblock">Pdffigures 2.0: Mining figures from research papers.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)</span>,
pages 143‚Äì152, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M.¬†Savva, Nicholas Kong, Arti Chhajta, Li¬†Fei-Fei, Maneesh Agrawala, and
J.¬†Heer.

</span>
<span class="ltx_bibblock">Revision: automated classification, analysis and redesign of chart
images.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of 24th annual ACM symposium on User interface
software and tech</span>, 2011.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sagnik¬†Ray Choudhury and C.¬†Lee Giles.

</span>
<span class="ltx_bibblock">An architecture for information extraction from figures in digital
libraries.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 24th International Conference on World Wide
Web</span>, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y.¬†Zhou and C.¬†Tan.

</span>
<span class="ltx_bibblock">Hough technique for bar charts detection and recognition in document
images.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings 2000 International Conference on Image Processing
(Cat. No.00CH37101)</span>, 2:605‚Äì608 vol.2, 2000.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Noah Siegel, Zachary Horvitz, Roie Levin, S.¬†Divvala, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Figureseer: Parsing result-figures in research papers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Binbin Tang, Xiao Liu, Jie Lei, Mingli Song, Dapeng Tao, Shuifa Sun, and
Fangmin Dong.

</span>
<span class="ltx_bibblock">Deepchart: Combining deep convolutional networks and deep belief
networks in chart classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Signal Processing</span>, 124:156‚Äì161, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K.¬†V. Jobin, Ajoy Mondal, and C.¬†V. Jawahar.

</span>
<span class="ltx_bibblock">Docfigure: A dataset for scientific document figure classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">2019 International Conference on Document Analysis and
Recognition Workshops (ICDARW)</span>, pages 74‚Äì79, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
V.¬†Karthikeyani and S.¬†Nagarajan.

</span>
<span class="ltx_bibblock">Machine learning classification algorithms to recognize chart types
in portable document format (pdf) files.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Applications</span>, 39:1‚Äì5, 2012.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Samira¬†Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam
Trischler, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Figureqa: An annotated figure dataset for visual reasoning, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>, 2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li¬†Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2009 IEEE Conference on CVPR</span>, pages 248‚Äì255, 2009.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Peter¬†J. Rousseeuw.

</span>
<span class="ltx_bibblock">Silhouettes: A graphical aid to the interpretation and validation of
cluster analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Journal of Computational and Applied Mathematics</span>, 20:53‚Äì65,
1987.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et¬†al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
Yonghui Wu.

</span>
<span class="ltx_bibblock">Coca: Contrastive captioners are image-text foundation models.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2205.01917, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.12292" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.12293" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.12293">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.12293" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.12296" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 04:33:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
