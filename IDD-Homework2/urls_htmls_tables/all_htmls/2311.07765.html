<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.07765] FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition</title><meta property="og:description" content="Motion sensors integrated into wearable and mobile devices provide valuable information about the device users. Machine learning and, recently, deep learning techniques have been used to characterize sensor data. Mostl…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.07765">

<!--Generated on Tue Feb 27 19:32:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\jmlrvolume</span>
<p id="p1.2" class="ltx_p">222
<span id="p1.2.1" class="ltx_ERROR undefined">\jmlryear</span>2023
<span id="p1.2.2" class="ltx_ERROR undefined">\jmlrworkshop</span>ACML 2023


</p>
</div>
<h1 class="ltx_title ltx_title_document">FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\Name</span>Egemen İşgüder
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.id1" class="ltx_ERROR undefined">\Name</span>Özlem Durmaz İncel 
<br class="ltx_break"><span id="id3.2.id2" class="ltx_ERROR undefined">\Email</span>{egemen.isguder, ozlem.durmaz}@boun.edu.tr 
<br class="ltx_break"><span id="id4.3.id3" class="ltx_ERROR undefined">\addr</span>Bogazici University
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Istanbul
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Turkey
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Motion sensors integrated into wearable and mobile devices provide valuable information about the device users. Machine learning and, recently, deep learning techniques have been used to characterize sensor data. Mostly, a single task, such as recognition of activities, is targeted, and the data is processed centrally at a server or in a cloud environment. However, the same sensor data can be utilized for multiple tasks and distributed machine-learning techniques can be used without the requirement of the transmission of data to a centre. This paper explores <span id="id5.id1.1" class="ltx_text ltx_font_italic">Federated Transfer Learning in a Multi-Task</span> manner for both sensor-based human activity recognition and device position identification tasks. The OpenHAR framework is used to train the models, which contains ten smaller datasets. The aim is to obtain model(s) applicable for both tasks in different datasets, which may include only some label types. Multiple experiments are carried in the Flower federated learning environment using the DeepConvLSTM architecture. Results are presented for federated and centralized versions under different parameters and restrictions. By utilizing transfer learning and training a task-specific and personalized federated model, we obtained a similar accuracy with training each client individually and higher accuracy than a fully centralized approach.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Federated transfer learning, multi-task learning, human activity recognition.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Motion sensors available on wearable and mobile devices are commonly used to characterize their users: activities performed, who and where they are, and where they carry their devices. Particularly, monitoring people’s physical activity levels can help them stay active and reduce their risk of chronic diseases such as obesity. In the literature, machine learning and, recently, deep learning techniques are applied to the sensor data. Although there are efforts to train and run the learning algorithms on the device, still the wearables are resource-limited regarding computation and memory. Hence, processing is performed at a central server or a cloud environment. Wearable and mobile devices are personal devices, and the integrated sensors collect privacy-sensitive personal data. Centrally, the collection and processing of such data may violate the users’ privacy. In this respect, federated learning is an emerging approach.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning is a novel technique in machine learning first announced by Google. The main goal of federated learning is to treat data in a privacy-preserving manner. Especially with the recent updates on the General Data Protection Regulation (GDPR), data privacy has become a significant national and international concern. Federated learning offers a critical solution at this point. Instead of sharing data among learning participants (clients), model parameters are shared, and the learning is realized in a distributed manner. This way, data privacy is assured, and the wireless communication costs decrease because only model parameters are shared between participants instead of big chunks of data. This also helps devices with resource constraints like wearable devices, smartphones and IoT devices.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">An area where federated learning is utilized is sensor-based human activity recognition (HAR). Data from movement sensors such as accelerometers and gyroscopes are processed with machine learning techniques to monitor a user’s activities (walking, sitting, etc.) or special activities such as sports exercises or activities that may include life-threatening situations (e.g. falling).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, for the recognition of human activities, DeepConvLSTM <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Ordóñez and Roggen</span> (<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> is applied for centralized and federated learning, both with various methods and settings. For the learning part, the OpenHAR framework <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Siirtola et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, which consists of ten smaller datasets, is used. In total, it contains accelerometer sensor data positioned in various parts of the body collected from 211 participants. Every dataset is owned by a different client in the federated learning experiments. In contrast, two versions exist in centralized learning: one similar to the federated setup, in which each dataset is used for centralized learning separately at each client, and another form where all data is combined in a big pile for centralized learning. Federated experiments are also categorized as one-task, multi-task and multi-task with a layered hierarchy. The tasks to learn during the experiments are the activities of the users and the positions of the devices. In the OpenHAR dataset, the devices were not located in a specific position. To the best of our knowledge, this paper is the first to implement a multi-task classification problem in multiple different datasets with a federated transfer learning method, where all datasets may not contain every type of label.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">For the federated learning part, the layered hierarchy for learning consists of pre-trained, common, task-specific and personalized layers inspired by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Keçeci et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, where they focus on multiple image recognition tasks. One of the clients is selected to pre-train the initial weights, which constitutes the pre-trained layer, and then all clients, without any task restriction, participate in one federated loop, which creates the common layer. Afterwards, there are two federated loops (one for each task) to implement the task-specific layers. Ultimately, every client uses its own data for simple training to obtain the personalized layer. In the end, a model with an accuracy of 72.4% is obtained (which is 0.2% less than the baseline), which works on multiple tasks, 14 different labels, and opens the path to learning new labels and tasks with its layered hierarchy.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The paper continues as follows: Section <a href="#S2" title="2 Related Work ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> examines the recent works in the area, Section <a href="#S3" title="3 Dataset and Federated Multi-Task Learning ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> explains the used dataset in experiments, the selected classification algorithm and the federated learning architecture. Section <a href="#S4" title="4 Experiments ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> focuses on the results of experiments realized in different conditions with various challenges. Section <a href="#S5" title="5 Conclusion &amp; Future Studies ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the results.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">FedHealth <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Chen et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> suggests a federated learning platform for wearable healthcare devices. The UCI public dataset for smartphones <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Garcia-Gonzalez et al.</span> (<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> is used for training in the FedHealth study. This dataset is just one of the OpenHAR-integrated datasets used in our study. In FedHealth, machine learning techniques such as KNN and SVM are used instead of deep learning techniques.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In another study, authors  <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Ek et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> use yet again a dataset from OpenHAR-integrated datasets, which is RealWorldHAR <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Sztyler and Stuckenschmidt</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, in order to train a deep artificial neural network based on CNN. In addition, they compare the success of different parameter aggregation algorithms (FedAvg, FedPer, FedMA). It is reported that the CNN classifier, alongside the FedAvg algorithm, has achieved the highest results. The FedAR study <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Presotto et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022a</span></a>)</cite>, which focuses on the challenge of finding labelled data in human activity recognition applications and also semi-supervised machine learning in that context, designed experiments using two separate datasets from OpenHAR. Comparing the results of MLP and CNN-based architectures, the authors report that MLP achieves better success. The same authors  <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Presotto et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022b</span></a>)</cite> also proposed a clustering-based federated learning approach. It is also reported that FedCLAR achieved better results than FedAvg and FedHealth within both datasets.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In the study of  <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Keçeci et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, the writers propose a novel federated learning architecture combining multi-task and federated learning. Their proposed algorithm is composed of four different types of layers: pre-trained, common, task-specific and personalized. During their experiments, they use an artificially generated network consisting of face images. Their dataset’s labels are binary, containing only two different types. They achieve close results with their baseline (individually trained) and their proposed method. In our study, instead of dividing one dataset into separate clients, every client uses a whole different dataset, which may or may not contain all the types of labels, which is the crucial difference. Regarding similar works in the field, although federated multi-task learning was utilized for different tasks in the image recognition domain or federated transfer learning in the human activity recognition domain, this paper is the first to implement human activity recognition with a federated multi-task transfer learning architecture.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset and Federated Multi-Task Learning</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>OpenHAR Framework</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">OpenHAR <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Siirtola et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, offers a platform that contains ten public datasets. It consolidates the data from different datasets at different sampling rates to 10 Hz. Data is collected through accelerometers placed on various positions of the body. The whole data has 17 different daily activity labels collected from 211 participants and 14 different body positions.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">All datasets contain these columns: user ID information, type of activity, sensor position, timestamp and x-y-z axes accelerometer readings. After some initial training experiments, some labels’ success rates from the confusion matrix were observed to be less than the others. Since some labels are much scarcer than others, some of the similar labels are combined to obtain a better label distribution. In the data, there were five different labels for “walking” activity: “Walking”, “Walking inc. stairs”, “Walking stairs up”, “Walking stairs down”, and “Walking at stairs” which are all combined into one label,“Walking”. The same is applied to the position labels, i.e.“Foot, left”,“Foot, right” are all combined into“Leg/Foot” label. After preprocessing, the resulting distributions of the labels are given in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 OpenHAR Framework ‣ 3 Dataset and Federated Multi-Task Learning ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2311.07765/assets/figures/Distributions.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="389" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Distribution of Activity (Left) and Position (Right) Labels.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Architecture and Federated Transfer Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We used the DeepConvLSTM architecture  <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Ordóñez and Roggen</span> (<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> in training the models. It combines Deep Convolutional Neural Network with LSTM (Long Short Term Memory) Network. The architecture of the algorithm consists of four convolutional layers, then two softmax layers and one softmax classifier layer at the end. The convolutional layers of the first part act as feature extractors and provide feature maps of the tabular data. In contrast, the recurrent layers model the temporality of the obtained feature maps. The algorithm is chosen as it is proven to be effective in HAR classification problems.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">FedAvg, FedProx, and Federated Matched Averaging are some of the most commonly used averaging algorithms in Federated Learning. This study uses FedAvg as the server’s aggregation algorithm <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">McMahan et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>. In FedAvg, the parameter aggregation is based on weighted averages. The central server aggregates all local model parameters into one new global model and then resends the model to a sub-group of clients. The main federated learning algorithm loops until the desired number of communication rounds.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We use a Federated Multi-Task Transfer learning approach inspired by the architecture in <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Keçeci et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> for image recognition tasks. The main layered hierarchy can be seen in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Architecture and Federated Transfer Learning ‣ 3 Dataset and Federated Multi-Task Learning ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. However, in this study, instead of using separate servers for each task, the same server trains each layer of the hierarchy, and every time a layer’s training is finished, its parameters are frozen, and the training is applied to the subsequent layers.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.07765/assets/figures/Layered_Hierarchy.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="305" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated Multi-Task Transfer Learning Architecture (from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Keçeci et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We performed different experiments to evaluate the performance of the explained techniques individually and in combination. For all experiments, data is always split into train and test sets with 80%-20% ratio. Flower environment <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Beutel et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> is used in the experiments.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Centralized, Individual and Federated Learning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In centralized experiments, all data from all datasets are combined into one big pile, which does not conserve data privacy. <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">Cross entropy loss</span> is used as the default loss criteria, and <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">stochastic gradient descent</span> is used as the optimizer. For the second class of experiments, instead of combining data into one big pile, each client (each dataset) is treated individually, using only their data, conserving data privacy. However, they cannot benefit from other clients’ data. The position identification task is only trained with three clients because they are the only clients with more than two types of position labels in their data. The accuracy is reported in terms of the weighted average accuracy of all clients (each client’s data size is taken as weight). Individual training is considered as the baseline for further comparison with the federated experiments. Thirdly, in the federated experiments, the setup is similar to the individual training, but this time, there is model parameter sharing, hence the federated learning. For the one-task experiments, only one type of label’s clients are trained at a time. The percentages are the weighted average accuracies of each type of the client.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracies of Centralized, Individual and Federated Experiments.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method/Label Type</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Activity Label</span></th>
<th id="S4.T1.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Position Label</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.2.1" class="ltx_tr">
<th id="S4.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Centralized in Bulk</span></th>
<td id="S4.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.2.1" class="ltx_text" style="font-size:70%;">69.8%</span></td>
<td id="S4.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.3.1" class="ltx_text" style="font-size:70%;">59.2%</span></td>
</tr>
<tr id="S4.T1.3.3.2" class="ltx_tr">
<th id="S4.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.3.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Individual Training</span></th>
<td id="S4.T1.3.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.2.2.1" class="ltx_text" style="font-size:70%;">76.7%</span></td>
<td id="S4.T1.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.2.3.1" class="ltx_text" style="font-size:70%;">65.0%</span></td>
</tr>
<tr id="S4.T1.3.4.3" class="ltx_tr">
<th id="S4.T1.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.3.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Federated One Task</span></th>
<td id="S4.T1.3.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.4.3.2.1" class="ltx_text" style="font-size:70%;">61.5%</span></td>
<td id="S4.T1.3.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.4.3.3.1" class="ltx_text" style="font-size:70%;">57.2%</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The results are presented in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Centralized, Individual and Federated Learning ‣ 4 Experiments ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Position identification results are lower than the activity recognition results because the number of clients with position data is less than the number of activity clients, and position labels vary more between clients than activity label types. Federated learning results are the lowest because of the heterogeneity between label types in different clients, negatively affecting parameter sharing.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Federated Multi-Task and Federated Multi-Task Transfer Learning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In the multi-task learning experiments, we considered two scenarios. During the simple federated-multi-task experiments, rather than learning one label at a time (separate simulations), both labels are learned in the same experiment. The other method is multi-task transfer learning, which combines federated learning, transfer learning and multi-task learning, as presented in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Architecture and Federated Transfer Learning ‣ 3 Dataset and Federated Multi-Task Learning ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. First, a simple dataset is trained for pre-trained layers. In this case, dataset-9 in OpenHAR (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">Sztyler and Stuckenschmidt</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>) is used since, for position identification, it revealed the best results. Then, the next layer is trained with all 13 clients, which creates the common layers. Afterwards, two different training sessions were performed for activity (10 clients) and position (3 clients) tasks, which resulted in the task-specific layers. Finally, every client is fine-tuned, and the last layers of their models are trained with their own data. Before training each layer, the previous layers are frozen.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2311.07765/assets/figures/Fed_Lay_Lay4.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Weighted Average of Accuracies of 13 Clients For Different Methods/Layers.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The performance of training with different layers can be seen in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Federated Multi-Task and Federated Multi-Task Transfer Learning ‣ 4 Experiments ‣ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The percentages are the weighted average accuracies of all the clients. The first three columns present the baselines, centralized, individual (one-task) and simple federated learning (multi-task). The latest personalized layer has an average weighted accuracy of 72.4%, whereas the individual learning’s accuracy is 72.6% and federated multi-task is 42.7%. From these results, we can say that personalization works quite well, similar to training the clients individually, and the accuracy loss in task-specific layers is tolerable, but the pre-trained and common layers’ accuracies are not close to this individual training baseline. Also, the layered hierarchy achieves more success than simple federated multi-task training. Federated multi-task transfer learning achieved similar results to individual training and better results than centralized training. One may argue that each client can be trained with its own data. However, collecting labelled data to train a model is a difficult process, and by using a transfer learning approach, the clients can get similar results with a limited amount of labelled data, which is used in the personalization step. Due to space limitations, we cannot present the accuracy results for each client. However, we observe that at every layer, the accuracy increases and gets closer to the individual training baseline.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion &amp; Future Studies </h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper focuses on the comparison between various models of centralized learning, federated learning and federated multi-task transfer learning combined with the DeepConvLSTM based architecture for human activity classification and device position identification with motion sensor data. Models are trained using the integrated OpenHAR dataset containing ten smaller datasets. In multi-task federated transfer learning, the obtained success rates are similar to the baseline’s success (individual training) and are better than a fully centralized approach. For future studies, different averaging algorithms than FedAvg and different classifiers’ effects on success will be analyzed. In addition, experiments can be implemented with different percentages of training data to analyze the effect of the data amount.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Beutel et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Flower: A friendly federated learning research framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.14390</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Fedhealth: A federated transfer learning framework for wearable healthcare.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Intelligent Systems</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 35(4):83–93, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Ek et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Sannara Ek, François Portet, Philippe Lalanda, and German Vega.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Evaluation of federated learning aggregation algorithms: Application to human activity recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Adjunct Proceedings UbiComp/ISWC ’20</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, page 638–643, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Garcia-Gonzalez et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Daniel Garcia-Gonzalez, Daniel Rivero, Enrique Fernandez-Blanco, and Miguel R. Luaces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">A public domain dataset for real-life human activity recognition using smartphone sensors.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sensors</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 20(8), 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Keçeci et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Cihat Keçeci, Mohammad Shaqfeh, Hayat Mbayed, and Erchin Serpedin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Multi-task and transfer learning for federated learning applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.08147</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">McMahan et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient learning of deep networks from decentralized data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Artificial Intelligence and Statistics</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 1273–1282. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.4.4.1" class="ltx_text" style="font-size:90%;">Ordóñez and Roggen (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">
Francisco Javier Ordóñez and Daniel Roggen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sensors</em><span id="bib.bib7.9.2" class="ltx_text" style="font-size:90%;">, 16(1), 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Presotto et al. (2022a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Riccardo Presotto, Gabriele Civitarese, and Claudio Bettini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Semi-supervised and personalized federated activity recognition based on active learning and label propagation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Personal and Ubiquitous Computing</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 26:1–18, 06 2022a.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Presotto et al. (2022b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Riccardo Presotto, Gabriele Civitarese, and Claudio Bettini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Fedclar: Federated clustering for personalized sensor-based human activity recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE International Conference on Pervasive Computing and Communications (PerCom)</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages 227–236, 2022b.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Siirtola et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Pekka Siirtola, Heli Koskimäki, and Juha Röning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Openhar: A matlab toolbox for easy access to publicly open human activity data sets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Adjunct Proceedings UbiComp/ISWC ’18</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, page 1396–1403, New York, NY, USA, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Sztyler and Stuckenschmidt (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
Timo Sztyler and Heiner Stuckenschmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">On-body localization of wearable devices: An investigation of position-aware activity recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2016 IEEE International Conference on Pervasive Computing and Communications (PerCom)</em><span id="bib.bib11.10.3" class="ltx_text" style="font-size:90%;">, pages 1–9, 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.07764" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.07765" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.07765">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.07765" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.07766" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 19:32:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
