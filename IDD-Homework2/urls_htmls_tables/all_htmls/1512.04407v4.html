<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1512.04407] We Are Humor Beings: Understanding and Predicting Visual Humor</title><meta property="og:description" content="Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="We Are Humor Beings: Understanding and Predicting Visual Humor">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="We Are Humor Beings: Understanding and Predicting Visual Humor">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1512.04407">

<!--Generated on Sun Mar  3 11:19:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">We Are Humor Beings: Understanding and Predicting Visual Humor</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arjun Chandrasekaran<sup id="id14.14.id1" class="ltx_sup">1</sup>  Ashwin K. Vijayakumar<sup id="id15.15.id2" class="ltx_sup">1</sup>  Stanislaw Antol<sup id="id16.16.id3" class="ltx_sup">1</sup>  Mohit Bansal<sup id="id17.17.id4" class="ltx_sup">2</sup> 
<br class="ltx_break">Dhruv Batra<sup id="id18.18.id5" class="ltx_sup">1</sup>  C. Lawrence Zitnick<sup id="id19.19.id6" class="ltx_sup">3</sup>  Devi Parikh<sup id="id20.20.id7" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id21.21.id8" class="ltx_sup">1</sup>Virginia Tech <sup id="id22.22.id9" class="ltx_sup">2</sup>TTI-Chicago <sup id="id23.23.id10" class="ltx_sup">3</sup>Facebook AI Research 
<br class="ltx_break"><sup id="id24.24.id11" class="ltx_sup"><span id="id24.24.id11.1" class="ltx_text" style="font-size:90%;">1</span></sup><span id="id25.25.id12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{carjun, ashwinkv, santol, dbatra, parikh}@vt.edu</span>  <sup id="id26.26.id13" class="ltx_sup"><span id="id26.26.id13.1" class="ltx_text" style="font-size:90%;">2</span></sup><span id="id27.27.id14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mbansal@ttic.edu</span>  <sup id="id28.28.id15" class="ltx_sup"><span id="id28.28.id15.1" class="ltx_text" style="font-size:90%;">3</span></sup><span id="id29.29.id16" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zitnick@fb.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id30.id1" class="ltx_p">Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question – what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies.
We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene.
We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">An adult laughs 18 times a day <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> on average. A good sense of humor is related to communication competence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, helps raise an individual’s social status <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, popularity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and helps attract compatible mates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Humor in the workplace improves camaraderie and helps workers cope with daily stresses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and loneliness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
<em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">fMRI</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> studies of the brain reveal that humor activates the components of the brain that are involved in reward processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. This probably explains why we actively seek to experience and create humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the tremendous impact that humor has on our lives, the lack of a rigorous definition of humor has hindered humor-related research in the past <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. While verbal humor is better understood today <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, visual humor remains unexplored. As vision and AI researchers we are interested in the following question – what content in an image causes it to be funny? Our work takes a step in the direction of building computational models for visual humor.
</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf1.1" class="ltx_p"><span id="S1.F1.sf1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3BWI6RSP7G905DWQTY39PDCSOECE77_raccoon_party.png" id="S1.F1.sf1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="167" height="96" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><em id="S1.F1.sf1.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Funny scene:</em><span id="S1.F1.sf1.6.3" class="ltx_text" style="font-size:90%;"> Raccoons are drunk at a picnic.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf2.1" class="ltx_p"><span id="S1.F1.sf2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/dogDining.png" id="S1.F1.sf2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="167" height="96" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><em id="S1.F1.sf2.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Funny scene:</em><span id="S1.F1.sf2.6.3" class="ltx_text" style="font-size:90%;"> Dogs feast while the girl sits in a pet bed.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf3.1" class="ltx_p"><span id="S1.F1.sf3.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3LS2AMNW5FQWNMEUJBFBUGN1X9HOQW_rats_stealing_before.png" id="S1.F1.sf3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="167" height="95" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><em id="S1.F1.sf3.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Funny scene:</em><span id="S1.F1.sf3.6.3" class="ltx_text" style="font-size:90%;"> Rats steal food while the cats are asleep.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf4.1" class="ltx_p"><span id="S1.F1.sf4.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3LS2AMNW5FQWNMEUJBFBUGN1X9HOQW_00_rats_stealing_after.png" id="S1.F1.sf4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="167" height="95" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf4.5.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><em id="S1.F1.sf4.6.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Funny Object Replaced (unfunny) counterpart:</em><span id="S1.F1.sf4.7.3" class="ltx_text" style="font-size:90%;"> Rats in <em id="S1.F1.sf4.7.3.1" class="ltx_emph ltx_font_italic">(c)</em> are replaced by food.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">
(a), (b) are selected funny scenes in the Abstract Visual Humor dataset. (c) is an originally funny scene in the Funny Object Replaced dataset. The objects contributing to humor in (c) are replaced by a human with other objects, to create an unfunny counterpart.
</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Computational visual humor is useful for a number of applications: to create better photo editing tools, smart cameras that pick the right moment to take a (funny) picture, recommendation tools that rate funny pictures higher (say, to post on social media), video summarization tools that summarize only the funny frames, automatically generating funny scenes for entertainment, identifying and catering to personalized humor, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">etc</em>.
As AI systems interact more with humans, it is vital that they understand subtleties of human emotions and expressions. In that sense, being able to identify humor can contribute to their <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">common sense</em>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Understanding visual humor is fraught with challenges such as having to detect all objects in the scene, observing the interactions between objects, and understanding context, which are currently unsolved problems.
In this work, we argue that, by using scenes made from clipart <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, we can study visual humor without having to wait for these detailed recognition problems to be solved. Abstract scenes are inherently densely annotated (<em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.2" class="ltx_text"></span> all objects and their locations are known), and so enable us to learn fine-grained semantics of a scene that causes it to be funny.
In this paper, we collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level (Fig. <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>, Fig. <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>) and the object-level (Fig. <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a>, Fig. <a href="#S1.F1.sf4" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(d)</span></a>). We propose a model that predicts how funny a scene is using semantic visual features of the scene such as occurrence of objects, and their relative locations.
We also build computational models for a particular source of humor, <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.4" class="ltx_text"></span>, humor due to the presence of objects in an unusual context. This source of humor is explained by the <em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">incongruity theory</em> of humor which states that a playful violation of the subjective expectations of a perceiver causes humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. <em id="S1.p4.1.6" class="ltx_emph ltx_font_italic">E.g</em>.<span id="S1.p4.1.7" class="ltx_text"></span>, Fig. <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a> is funny because our expectation is that people eat at tables and dogs sit in pet beds and this is violated when we see the roles of people and dogs swapped.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The scene-level Abstract Visual Humor (AVH) dataset contains funny scenes (Fig. <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>, Fig. <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>) and unfunny scenes with human ratings for funniness of each scene. Using the ground truth rating, we demonstrate that we can reliably predict a <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">funniness score</em> for a given scene.
The object-level Funny Object Replaced (FOR) dataset contains scenes that are originally funny (Fig. <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a>) and their unfunny counterparts (Fig. <a href="#S1.F1.sf4" title="In Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(d)</span></a>). The unfunny counterparts are created by humans by replacing objects that contribute to humor such that the scene is not funny anymore. The ground truth of replaced objects is used to train models to alter the funniness of a scene – to make a funny scene unfunny and vice versa. Our models outperform natural baselines and ablated versions of our system in quantitative evaluation. They also demonstrate good qualitative performance via human studies.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our main contributions are as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We collect two abstract scene datasets consisting of scenes created by humans which are publicly available.</p>
<ol id="S1.I1.i1.I1" class="ltx_enumerate">
<li id="S1.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">i.</span> 
<div id="S1.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.I1.i1.p1.1" class="ltx_p">The scene-level Abstract Visual Humor (AVH) dataset consists of funny and unfunny abstract scenes (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Each scene also contains a brief explanation of the humor in the scene.</p>
</div>
</li>
<li id="S1.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">ii.</span> 
<div id="S1.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i1.I1.i2.p1.1" class="ltx_p">The object-level Funny Object Replaced (FOR) dataset consists of funny scenes and their corresponding unfunny counterparts resulting from object replacement (Sec. <a href="#S3.SS3" title="3.3 Funny Object Replaced (FOR) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We analyze the different sources of humor techniques depicted in the AVH dataset via human studies (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We learn distributed representations for each object category which encode the context in which an object naturally appears, <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.I1.i3.p1.1.2" class="ltx_text"></span>, in an unfunny setting. (Sec. <a href="#S4.SS1" title="4.1 Features ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We model two tasks to demonstrate an understanding of visual humor:</p>
<ol id="S1.I1.i4.I1" class="ltx_enumerate">
<li id="S1.I1.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">i.</span> 
<div id="S1.I1.i4.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i4.I1.i1.p1.1" class="ltx_p">Predicting how funny a given scene is (Sec. <a href="#S5.SS1" title="5.1 Predicting Funniness Score ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>).</p>
</div>
</li>
<li id="S1.I1.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">ii.</span> 
<div id="S1.I1.i4.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i4.I1.i2.p1.1" class="ltx_p">Automatically altering the funniness of a given scene (Sec. <a href="#S5.SS2" title="5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To the best of our knowledge, this is the first work that deals with understanding and building computational models for visual humor.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Humor Theories.</span>
Humor has been a topic of study since the time of Plato <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Aristotle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and Bharata <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Over the years, philosophical studies and psychological research have sought to explain why we laugh. There are three theories of humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> that are popular in contemporary academic literature. According to the incongruity theory, a perceiver encounters an incongruity when expectations about the stimulus are violated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The two stage model of humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> further states that the process of discarding prior assumptions and reinterpreting the incongruity in a new context (resolution) is crucial to the comprehension of humor.
Superiority theory suggests that the misfortunes of others which reflects our own superiority is a source of humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
According to the relief theory, humor is the release of pent-up tension or mental energy. Feelings of hostility, aggression, or sexuality that are expressed bypassing any societal norms are said to be enjoyed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Previous attempts to characterize the stimuli that induce humor have mostly dealt with linguistic or verbal humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.2" class="ltx_text"></span>, script-based semantic theory of humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and its revised version, the general theory of verbal humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Computational Models of Humor.</span>
A number of computational models are developed to recognize language-based humor <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p3.1.3" class="ltx_text"></span>, one-liners <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, sarcasm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and <em id="S2.p3.1.4" class="ltx_emph ltx_font_italic">knock-knock</em> jokes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. Other work in this area includes exploring features of humorous texts that help detection of humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and identifying the set of words or phrases in a sentence that could contribute to humor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Some computational humor models that generate verbal humor are JAPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> which is a pun-based riddle generating program, HAHAcronym <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> which is an automatic funny acronym generator, and an unsupervised model that produces “<span id="S2.p4.1.1" class="ltx_text ltx_font_italic">I like my X like I like my Y, Z</span>” jokes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
While the above works investigate detection and generation of verbal humor, in this work we deal purely with <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">visual</em> humor.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Recent works predict the best text to go along with a given (presumably funny) raw image such as a meme <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> or a cartoon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. In addition, Radev <em id="S2.p5.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p5.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> develop unsupervised methods to rank funniness of captions for a cartoon. They also analyze the characteristics of the funniest captions.
Unlike our work, these works do not predict whether a <em id="S2.p5.1.3" class="ltx_emph ltx_font_italic">scene</em> is funny or which components of the scene contribute to the humor.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Buijzen and Valkenburg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> analyze humorous commercials to develop and investigate a typology of humor. Our contributions are different as we study the sources of humor in static images, as opposed to audiovisual media.
To the best of our knowledge, ours is the first work to study <em id="S2.p6.1.1" class="ltx_emph ltx_font_italic">visual</em> humor in a computational framework.</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text ltx_font_bold">Human Perception of Images.</span>
A number of works investigate the intrinsic characteristics of an image that influence human perception <em id="S2.p7.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p7.1.3" class="ltx_text"></span>, memorability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, popularity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, visual interestingness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and virality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
In this work, we study what content in a scene causes people to perceive it as funny, and explore a method of altering the funniness of a scene. </p>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.1" class="ltx_p"><span id="S2.p8.1.1" class="ltx_text ltx_font_bold">Learning from Visual Abstraction.</span>
Visual abstractions have been used to explore high-level semantic scene understanding tasks like identifying visual features that are semantically important <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, learning mappings between visual features and text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, learning visually grounded word embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, modeling fine-grained interactions between pairs of people <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and learning (temporal and static) common sense <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. In this work, we use abstract scenes to understand the semantics in a scene that cause humor, a problem that has not been studied before.

</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We introduce two new abstract scenes datasets – the Abstract Visual Humor (AVH) dataset (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and the Funny Object Replaced (FOR) dataset (Sec. <a href="#S3.SS3" title="3.3 Funny Object Replaced (FOR) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) using the interfaces described in Sec. <a href="#S3.SS1" title="3.1 Abstract Scenes Interface ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. The AVH dataset (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) consists of both funny and unfunny scenes along with funniness ratings. The FOR dataset (Sec. <a href="#S3.SS3" title="3.3 Funny Object Replaced (FOR) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) consists of funny scenes and their altered unfunny counterparts. Both the datasets are made publicly available on the project webpage.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Abstract Scenes Interface</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Abstract scenes enable researchers to explore high-level semantics of a scene without waiting for low-level recognition tasks to be solved.
We use the clipart interface<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a href="www.github.com/VT-vision-lab/abstract_scenes_v002" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.github.com/VT-vision-lab/abstract_scenes_v002</a></span></span></span> developed by Antol <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which allows for indoor and outdoor scenes to be created. The clipart vocabulary consists of 20 deformable human models, 31 animals in various poses, and around 100 objects that are found in indoor (<em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.1.4" class="ltx_text"></span>, chair, table, sofa, fireplace, notebook, painting) and outdoor (<em id="S3.SS1.p1.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.1.6" class="ltx_text"></span>, sun, cloud, tree, grill, campfire, slide) scenes.
The human models span different genders, races, and ages with 8 different expressions. They have limbs that are adjustable to allow for continuous pose variations. This combined with the large vocabulary of objects result in diverse scenes with rich semantics. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (<em id="S3.SS1.p1.1.7" class="ltx_emph ltx_font_italic">Top Row</em>) shows scenes that AMT workers created using this abstract scenes interface and vocabulary. Additional details, example scenes, and a sample of clipart objects are available on the project webpage.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S3.F2.sf1.1" class="ltx_p ltx_align_center"><span id="S3.F2.sf1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/score_0_1.png" id="S3.F2.sf1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.4.2" class="ltx_text" style="font-size:90%;">0.1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S3.F2.sf2.1" class="ltx_p ltx_align_center"><span id="S3.F2.sf2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/score_1_5.png" id="S3.F2.sf2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.4.2" class="ltx_text" style="font-size:90%;">1.5</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S3.F2.sf3.1" class="ltx_p ltx_align_center"><span id="S3.F2.sf3.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/drunkInPark.png" id="S3.F2.sf3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F2.sf3.4.2" class="ltx_text" style="font-size:90%;">4.0</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S3.F2.sf4.1" class="ltx_p ltx_align_center"><span id="S3.F2.sf4.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/score_4_0.png" id="S3.F2.sf4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F2.sf4.4.2" class="ltx_text" style="font-size:90%;">4.0</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.5.2" class="ltx_text" style="font-size:90%;">Spectrum of scenes <em id="S3.F2.5.2.1" class="ltx_emph ltx_font_italic">(left to right)</em> in ascending order of funniness score, <span id="S3.F2.5.2.2" class="ltx_text ltx_font_italic">F<sub id="S3.F2.5.2.2.1" class="ltx_sub">i</sub></span> (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) as rated by AMT workers.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Abstract Visual Humor (AVH) Dataset</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This dataset consists of funny and unfunny scenes created by AMT workers, facilitating the study of visual humor at the scene level.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Collecting Funny Scenes. </span>
We collect 3.2K scenes via AMT by asking workers to create funny scenes that are meaningful, realistic, and that other people would also consider funny. This is to encourage workers to refrain from creating scenes with inside jokes or catering to a very personalized form of humor. A screenshot of the interface used to collect the data is available on the project webpage.
We provide a random subset of the clipart vocabulary to each worker out of which at least 6 clipart objects are to be used to create a scene.
In addition, we also ask the worker to give a brief description of why the scene is funny in a short phrase or sentence. We find that this encourages workers to be more thoughtful and detailed regarding the scene they create. Note that this is different from providing a caption to an image since this is a simple explanation of what the worker had in mind while creating the scene. Mining this data may be useful to better understand visual humor. However, in this work we focus on the harder task of understanding purely <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">visual</em> humor and do not use these explanations.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We also use an equal number (3.2K) of abstract scenes from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which are realistic, everyday scenes. We expect most of these scenes to be mundane (<em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>., not funny).</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Labeling Scene Funniness. </span>

Anyone who has tried to be funny knows that humor is a subjective notion. A well-intending worker may create a scene that other people do not find very funny. We obtain funniness ratings for each scene in the dataset from 10 different workers on AMT who do not see the creator’s explanation of funniness. The ratings are on a scale of 1 to 5, where 1 is not funny and 5 is extremely funny.
We define the <em id="S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">funniness score</em> <span id="S3.SS2.p4.1.3" class="ltx_text ltx_font_italic">F<sub id="S3.SS2.p4.1.3.1" class="ltx_sub">i</sub></span>  of a scene <span id="S3.SS2.p4.1.4" class="ltx_text ltx_font_italic">i</span>, as the average of the 10 ratings for the scene. We found 10 ratings to be sufficient for good inter-human agreement. Further analysis is provided on the project webpage.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">By plotting a distribution of these scores, we determine the optimal threshold that best separates scenes that were intended to be funny (<em id="S3.SS2.p5.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p5.1.2" class="ltx_text"></span>, workers were specifically asked to create a funny scene) and other scenes (<em id="S3.SS2.p5.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p5.1.4" class="ltx_text"></span>, everyday scenes from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, where workers were not asked to create funny scenes).
We label all scenes that have a <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\textit{F\textsubscript{i}}\geqslant\text{threshold}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mrow id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2e.cmml"><mtext class="ltx_mathvariant_italic" id="S3.SS2.p5.1.m1.1.1.2a" xref="S3.SS2.p5.1.m1.1.1.2e.cmml">F</mtext><mtext id="S3.SS2.p5.1.m1.1.1.2b" xref="S3.SS2.p5.1.m1.1.1.2e.cmml"><sub id="S3.SS2.p5.1.m1.1.1.2.2nest" class="ltx_sub"><span id="S3.SS2.p5.1.m1.1.1.2.2.1nest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow><mo id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">⩾</mo><mtext id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3a.cmml">threshold</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><geq id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"></geq><ci id="S3.SS2.p5.1.m1.1.1.2e.cmml" xref="S3.SS2.p5.1.m1.1.1.2"><mrow id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S3.SS2.p5.1.m1.1.1.2a.cmml" xref="S3.SS2.p5.1.m1.1.1.2">F</mtext><mtext id="S3.SS2.p5.1.m1.1.1.2b.cmml" xref="S3.SS2.p5.1.m1.1.1.2"><sub id="S3.SS2.p5.1.m1.1.1.2.2anest" class="ltx_sub"><span id="S3.SS2.p5.1.m1.1.1.2.2.1anest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></ci><ci id="S3.SS2.p5.1.m1.1.1.3a.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><mtext id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">threshold</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\textit{F\textsubscript{i}}\geqslant\text{threshold}</annotation></semantics></math> as <em id="S3.SS2.p5.1.5" class="ltx_emph ltx_font_italic">funny</em> and all scenes with a lower <span id="S3.SS2.p5.1.6" class="ltx_text ltx_font_italic">F<sub id="S3.SS2.p5.1.6.1" class="ltx_sub">i</sub></span> as <em id="S3.SS2.p5.1.7" class="ltx_emph ltx_font_italic">unfunny</em>.
This re-labeling results in 522 <em id="S3.SS2.p5.1.8" class="ltx_emph ltx_font_italic">unintentionally funny</em> scenes (i.e., scenes from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which were determined to be funny), and 682 <em id="S3.SS2.p5.1.9" class="ltx_emph ltx_font_italic">unintentionally unfunny</em> scenes (<em id="S3.SS2.p5.1.10" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p5.1.11" class="ltx_text"></span>, well-intentioned worker outputs which were deemed not funny by the crowd).</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">In total, this dataset contains 6,400 scenes (3,028 funny scenes and 3,372 unfunny scenes). We randomly split these scenes into train, val, and test sets having 60%, 20%, and 20% of the scenes, respectively. We refer to this dataset as the AVH dataset.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p"><span id="S3.SS2.p7.1.1" class="ltx_text ltx_font_bold">Humor Techniques. </span>

To better understand the different sources of humor in our dataset, we collect human annotations of the different techniques are used to depict humor in each scene. We create a list of humor techniques that are motivated by existing humor theories, based on patterns that we observe in funny scenes, and the audio-visual humor typology by Buijzen <em id="S3.SS2.p7.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p7.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>:
<em id="S3.SS2.p7.1.4" class="ltx_emph ltx_font_italic">person doing something unusual</em>, <em id="S3.SS2.p7.1.5" class="ltx_emph ltx_font_italic">animal doing something unusual</em>, <em id="S3.SS2.p7.1.6" class="ltx_emph ltx_font_italic">clownish behavior (<em id="S3.SS2.p7.1.6.1" class="ltx_emph ltx_font_upright">i.e</em>.<span id="S3.SS2.p7.1.6.2" class="ltx_text"></span>, goofiness)</em>, <em id="S3.SS2.p7.1.7" class="ltx_emph ltx_font_italic">too many objects</em>, <em id="S3.SS2.p7.1.8" class="ltx_emph ltx_font_italic">somebody getting hurt</em>, <em id="S3.SS2.p7.1.9" class="ltx_emph ltx_font_italic">somebody getting scared</em> and <em id="S3.SS2.p7.1.10" class="ltx_emph ltx_font_italic">somebody getting angry</em>.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.4" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/animalUnusual.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S3.F3.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/personUnusual_2.png" id="S3.F3.2.2.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S3.F3.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/getting_hurt_1.png" id="S3.F3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S3.F3.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/getting_scared_1.png" id="S3.F3.4.4.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.12.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.13.2" class="ltx_text" style="font-size:90%;">Top voted scenes by humor technique (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). From <em id="S3.F3.13.2.1" class="ltx_emph ltx_font_italic">left</em> to <em id="S3.F3.13.2.2" class="ltx_emph ltx_font_italic">right</em>: <em id="S3.F3.13.2.3" class="ltx_emph ltx_font_italic">animal doing something unusual</em>, <em id="S3.F3.13.2.4" class="ltx_emph ltx_font_italic">person doing something unusual</em>, <em id="S3.F3.13.2.5" class="ltx_emph ltx_font_italic">somebody getting hurt</em>, and <em id="S3.F3.13.2.6" class="ltx_emph ltx_font_italic">somebody getting scared</em>.</span></figcaption>
</figure>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">We choose a subset of 200 funny scenes from the AVH dataset. We show each of these scenes to 10 different AMT workers and ask them to choose all the humor techniques that are depicted. Our options also included <em id="S3.SS2.p8.1.1" class="ltx_emph ltx_font_italic">none of the above reasons</em>, which also prompted workers to briefly explain what other unlisted technique depicted in the scene made it funny.
However, we observe that this option was rarely used by workers. This may indicate that most of our scenes can be explained well by one of the listed humor techniques.
Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the top voted images corresponding to the 4 most popular techniques of humor. We find that the techniques that involve animate objects – <em id="S3.SS2.p8.1.2" class="ltx_emph ltx_font_italic">animal doing something unusual</em> and <em id="S3.SS2.p8.1.3" class="ltx_emph ltx_font_italic">person doing something unusual</em> are voted higher than any other technique by a large margin. For 75% of the scenes, at least 3 out of 10 workers picked one of these two techniques. We observe that this <em id="S3.SS2.p8.1.4" class="ltx_emph ltx_font_italic">unusualness</em> or <em id="S3.SS2.p8.1.5" class="ltx_emph ltx_font_italic">incongruity</em> is generally caused by objects occurring in an unusual context in the scene.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<p id="S3.SS2.p9.1" class="ltx_p">Introducing or eliminating incongruities can alter the funniness of a scene. An elderly person kicking a football while simultaneously skateboarding (Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.3 Funny Object Replaced (FOR) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, <em id="S3.SS2.p9.1.1" class="ltx_emph ltx_font_italic">bottom</em>) is incongruous and hence considered funny. However, when the person is replaced by a young girl, this is is not incongruous and hence not funny. Such incongruities that can alter the funniness of a scene serves as our motivation to collect the Funny Object Replaced dataset which we describe next.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Funny Object Replaced (FOR) Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Replacing objects in a scene is a technique to manipulate incongruities (and hence funniness) in a scene. For instance, we can change funny interactions (which are unexpected by our common sense) to interactions that are <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">normal</em> according to our mental model of the world. We use this technique to collect a dataset which consists of funny scenes and their altered unfunny counterparts. This enables the study of humor in a scene at the <em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">object-level</em>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We show funny scenes from the AVH dataset and ask AMT workers to make the least number of replacements in the scene to render the originally funny scene unfunny. The motivation behind this is to get a precise signal of which objects in the scene contribute to humor and what they can be replaced with to reduce/eliminate humor, while keeping the underlying structure of the scene the same. We ask workers to replace an object with another object that is as similar as possible to the first object and keep the scene realistic. This helps us understand fine-grained semantics that causes a specific object category to contribute to humor. There could be other ways to manipulate humor, <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS3.p2.1.2" class="ltx_text"></span>, by adding, removing, or moving objects in a scene, <em id="S3.SS3.p2.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S3.SS3.p2.1.4" class="ltx_text"></span> but in our work we employ only the technique of replacing objects. We find that this technique is very effective in altering the funniness of a scene. Our interface did not allow people to add, remove, or move the objects in the scene. A screenshot of the interface used to collect this dataset is available on the project webpage.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">For each of the 3,028 funny scenes in the AVH dataset, we collect <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">object-replaced</em> scenes from 5 different workers resulting in 15,140 unfunny counterpart scenes. As a sanity check, we collect funniness ratings (via AMT) for 750 unfunny counterpart scenes. We observe that they indeed have an average <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_italic">F<sub id="S3.SS3.p3.1.2.1" class="ltx_sub">i</sub></span> of 1.10, which is smaller than that of their corresponding original funny scenes (whose average <span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_italic">F<sub id="S3.SS3.p3.1.3.1" class="ltx_sub">i</sub></span> is 2.66). Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.3 Funny Object Replaced (FOR) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows two pairs of funny scenes and their object-replaced unfunny counterparts. We refer to this dataset as the FOR dataset.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Given the task posed to workers (altering a funny scene to make it unfunny), it is natural to use this dataset to train a model to reduce the humor in a scene. However, this dataset can also be used to train flipped models that can increase the humor in a scene as shown in Sec. <a href="#S5.SS2.SSS3" title="5.2.3 Making a Scene Funny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.4" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/old_people_dancing-before.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S3.F4.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/old_people_dancing-after_1.png" id="S3.F4.2.2.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span>
<span id="S3.F4.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/grandmaKick_before.png" id="S3.F4.3.3.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S3.F4.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/grandmaKick_after.png" id="S3.F4.4.4.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.9.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.10.2" class="ltx_text" style="font-size:90%;">Funny scenes (<em id="S3.F4.10.2.1" class="ltx_emph ltx_font_italic">left</em>) and <em id="S3.F4.10.2.2" class="ltx_emph ltx_font_italic">one</em> among the 5 corresponding object-replaced unfunny counterparts (<em id="S3.F4.10.2.3" class="ltx_emph ltx_font_italic">right</em>) from the FOR dataset (see Sec. <a href="#S3.SS3" title="3.3 Funny Object Replaced (FOR) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). For each funny scene, we collect an unfunny counterpart from a different worker.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Approach</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose and model two tasks that we believe demonstrate an understanding of some aspects of visual humor:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Predicting how funny a given scene is.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Altering the funniness of a scene.</p>
</div>
</li>
</ol>
<p id="S4.p1.2" class="ltx_p">The models that perform the above tasks are described in Sec. <a href="#S4.SS2" title="4.2 Predicting Funniness Score ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> and Sec. <a href="#S4.SS3" title="4.3 Altering Funniness of a Scene ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, respectively. The features used in the models are described first (Sec. <a href="#S4.SS1" title="4.1 Features ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Features</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Abstract scenes are trivially densely annotated which we use to compute rich semantic features. Recall that our interface allows two types of scenes (indoor and outdoor) and our vocabulary consists of 150 object categories. We compute both scene-level and instance-level features.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Instance-Level Features</span></p>
<ol id="S4.I2.i1.I1" class="ltx_enumerate">
<li id="S4.I2.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S4.I2.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Object embedding (150-d)</span>

is a distributed representation that captures the context in which an object category usually occurs. We learn this representation using a word2vec-style continuous Bag-of-Words model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
The model tries to predict the presence of an object category in the scene, given the context provided by other instances of objects in the scene. Specifically, in a scene, given 5 (randomly chosen) instances, the model tries to predict the object category of the 6th instance.
We train the single-layer (150-d) neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> with multiple 6-item subsets of instances from each scene.
The network is trained using Stochastic Gradient Descent (SGD) with a momentum of 0.9. We use 11K scenes (that were not intended to be funny) from the dataset collected in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to train the model. Thus, we learn representations of objects occurring in natural contexts which are not funny.
A visualization of the object embeddings is available on the project webpage.</p>
</div>
</li>
<li id="S4.I2.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S4.I2.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Local embedding (150-d)</span> For each instantiation of an object in the scene, we compute a weighted sum of object embeddings of all the other instances in the scene. The weight of every other instance is its inverse square-root distance w.r.t.<span id="S4.I2.i1.I1.i2.p1.1.2" class="ltx_text"></span> the instance under consideration. 
<br class="ltx_break"></p>
</div>
</li>
</ol>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Scene-Level Features</span></p>
<ol id="S4.I2.i2.I1" class="ltx_enumerate">
<li id="S4.I2.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S4.I2.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I2.i2.I1.i1.p1.1" class="ltx_p"><span id="S4.I2.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Cardinality (150-d)</span> is a Bag-of-Words representation that indicates the number of instances of each object category that are present in the scene.</p>
</div>
</li>
<li id="S4.I2.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S4.I2.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I2.i2.I1.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Location (300-d)</span> is a vector of the horizontal and vertical coordinates of every object in the scene.
When multiple instances of an object category are present, we consider location of the instance closest to the center of the scene.</p>
</div>
</li>
<li id="S4.I2.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S4.I2.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I2.i2.I1.i3.p1.1" class="ltx_p"><span id="S4.I2.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Scene Embedding (150-d)</span> is the sum of object embeddings of all objects present in the scene.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Predicting Funniness Score</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We train a Support Vector Regressor (SVR) that predicts the funniness score, <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">F<sub id="S4.SS2.p1.1.1.1" class="ltx_sub">i</sub></span> for a given scene <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">i</annotation></semantics></math>. The model regresses to the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">F<sub id="S4.SS2.p1.1.2.1" class="ltx_sub">i</sub></span> computed from ratings given by AMT workers (described in Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) on scenes from the AVH dataset (Sec. <a href="#S3.SS2" title="3.2 Abstract Visual Humor (AVH) Dataset ‣ 3 Datasets ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
We train the SVR on the scene-level features (described in Sec. <a href="#S4.SS1" title="4.1 Features ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and perform an ablation study.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Altering Funniness of a Scene</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">We learn models to alter the funniness of a scene – from funny to unfunny and <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">vice versa</em>. Our two-stage pipeline involves:</p>
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Detecting objects that contribute to humor.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">Identifying suitable replacement objects from 1. to make the scene unfunny (or funny), while keeping it realistic.</p>
</div>
</li>
</ol>
<p id="S4.SS3.p1.2" class="ltx_p"><span id="S4.SS3.p1.2.1" class="ltx_text ltx_font_bold">Detecting Humor. </span>We train a multi-layer perceptron (MLP) on scenes from the FOR dataset to make a binary prediction on each object instance in the scene – whether it should be replaced to alter the funniness of a scene or not. The input is a 300-d vector formed by concatenating object embedding and local embedding features. The MLP has two hidden layers comprising of 300 and 100 units respectively, to which ReLU activation is applied. The final layer has 2 neurons and is used to perform binary classification (replace or not) using cross-entropy loss.
We train the model using SGD with a base learning rate of 0.01 and momentum of 0.9. We also trained a model with skip-connections that considers the predictions made on other objects when making a prediction on a given object. However, this did not result in significant performance gains.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Altering Humor. </span>
We train an MLP to perform a 150-way classification to predict potential replacer objects (from the clipart vocabulary), given an object predicted to be replaced in a scene. The model’s input is a 300-d vector formed by concatenating local embedding and object embedding features. The classifier has 3 hidden layers of 300 units each, with ReLU non-linearities. The output layer has 150 units over which we compute soft-max loss. We train the model using SGD with a base learning rate of 0.1, momentum of 0.9, and a dropout ratio of 0.5. The label for an instance is the index of the replacer object category used by the worker. Due to the large diversity of viable replacer objects that can alter humor in a scene, we also analyze the top-5 predictions of this model.
We train two models – one on funny scenes, and another on their unfunny counterparts from the FOR dataset. Thus, we learn models to alter the funniness in a scene in one direction – funny to unfunny or vice versa. Although we could train the pipeline end-to-end, we train each stage separately so that we can evaluate them separately and isolate their errors (for better interpretability).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We discuss the performance of our models in the two visual humor tasks of:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Predicting how funny a given scene is (Sec. <a href="#S5.SS1" title="5.1 Predicting Funniness Score ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>)</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Altering funniness of a scene (Sec. <a href="#S5.SS2" title="5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
</li>
</ol>
<p id="S5.p1.2" class="ltx_p">We discuss the quantitative results of our model in altering an unfunny scene to make it funny in Sec. <a href="#S5.SS2.SSS2" title="5.2.2 Making a Scene Unfunny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>), and the <em id="S5.p1.2.1" class="ltx_emph ltx_font_italic">vice versa</em> in Sec. <a href="#S5.SS2.SSS3" title="5.2.3 Making a Scene Funny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>. In Sec. <a href="#S5.SS3" title="5.3 Human Evaluation ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>, we report qualitative results through human studies.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Predicting Funniness Score</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">This section presents performance of the SVR (Sec. <a href="#S4.SS2" title="4.2 Predicting Funniness Score ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) that predicts the funniness score <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">F<sub id="S5.SS1.p1.1.1.1" class="ltx_sub">i</sub></span> of a scene.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Metric.</span>
We use average relative error to quantify our model’s performance computed as follows:</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.1" class="ltx_Math" alttext="\frac{1}{N}\sum_{i=1}^{N}\frac{|Predicted\ \textit{F\textsubscript{i}}-Ground\ Truth\ \textit{F\textsubscript{i}}|}{Ground\ Truth\ \textit{F\textsubscript{i}}}" display="block"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.2" xref="S5.E1.m1.1.2.cmml"><mfrac id="S5.E1.m1.1.2.2" xref="S5.E1.m1.1.2.2.cmml"><mn id="S5.E1.m1.1.2.2.2" xref="S5.E1.m1.1.2.2.2.cmml">1</mn><mi id="S5.E1.m1.1.2.2.3" xref="S5.E1.m1.1.2.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.2.1" xref="S5.E1.m1.1.2.1.cmml">​</mo><mrow id="S5.E1.m1.1.2.3" xref="S5.E1.m1.1.2.3.cmml"><munderover id="S5.E1.m1.1.2.3.1" xref="S5.E1.m1.1.2.3.1.cmml"><mo movablelimits="false" id="S5.E1.m1.1.2.3.1.2.2" xref="S5.E1.m1.1.2.3.1.2.2.cmml">∑</mo><mrow id="S5.E1.m1.1.2.3.1.2.3" xref="S5.E1.m1.1.2.3.1.2.3.cmml"><mi id="S5.E1.m1.1.2.3.1.2.3.2" xref="S5.E1.m1.1.2.3.1.2.3.2.cmml">i</mi><mo id="S5.E1.m1.1.2.3.1.2.3.1" xref="S5.E1.m1.1.2.3.1.2.3.1.cmml">=</mo><mn id="S5.E1.m1.1.2.3.1.2.3.3" xref="S5.E1.m1.1.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S5.E1.m1.1.2.3.1.3" xref="S5.E1.m1.1.2.3.1.3.cmml">N</mi></munderover><mfrac id="S5.E1.m1.1.1" xref="S5.E1.m1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1" xref="S5.E1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S5.E1.m1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.2.cmml"><mi id="S5.E1.m1.1.1.1.1.1.2.2" xref="S5.E1.m1.1.1.1.1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.3" xref="S5.E1.m1.1.1.1.1.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1a" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.4" xref="S5.E1.m1.1.1.1.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1b" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.5" xref="S5.E1.m1.1.1.1.1.1.2.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1c" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.6" xref="S5.E1.m1.1.1.1.1.1.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1d" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.7" xref="S5.E1.m1.1.1.1.1.1.2.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1e" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.8" xref="S5.E1.m1.1.1.1.1.1.2.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1f" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.9" xref="S5.E1.m1.1.1.1.1.1.2.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1g" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.2.10" xref="S5.E1.m1.1.1.1.1.1.2.10.cmml">d</mi><mo lspace="0.500em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2.1h" xref="S5.E1.m1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S5.E1.m1.1.1.1.1.1.2.11" xref="S5.E1.m1.1.1.1.1.1.2.11e.cmml"><mtext class="ltx_mathvariant_italic" id="S5.E1.m1.1.1.1.1.1.2.11a" xref="S5.E1.m1.1.1.1.1.1.2.11e.cmml">F</mtext><mtext id="S5.E1.m1.1.1.1.1.1.2.11b" xref="S5.E1.m1.1.1.1.1.1.2.11e.cmml"><sub id="S5.E1.m1.1.1.1.1.1.2.11.2nest" class="ltx_sub"><span id="S5.E1.m1.1.1.1.1.1.2.11.2.1nest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></mrow><mo id="S5.E1.m1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S5.E1.m1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.3.cmml"><mi id="S5.E1.m1.1.1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.3" xref="S5.E1.m1.1.1.1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1a" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.4" xref="S5.E1.m1.1.1.1.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1b" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.5" xref="S5.E1.m1.1.1.1.1.1.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1c" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.6" xref="S5.E1.m1.1.1.1.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1d" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.7" xref="S5.E1.m1.1.1.1.1.1.3.7.cmml">d</mi><mo lspace="0.500em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1e" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.8" xref="S5.E1.m1.1.1.1.1.1.3.8.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1f" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.9" xref="S5.E1.m1.1.1.1.1.1.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1g" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.10" xref="S5.E1.m1.1.1.1.1.1.3.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1h" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.11" xref="S5.E1.m1.1.1.1.1.1.3.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1i" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.1.3.12" xref="S5.E1.m1.1.1.1.1.1.3.12.cmml">h</mi><mo lspace="0.500em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.3.1j" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S5.E1.m1.1.1.1.1.1.3.13" xref="S5.E1.m1.1.1.1.1.1.3.13e.cmml"><mtext class="ltx_mathvariant_italic" id="S5.E1.m1.1.1.1.1.1.3.13a" xref="S5.E1.m1.1.1.1.1.1.3.13e.cmml">F</mtext><mtext id="S5.E1.m1.1.1.1.1.1.3.13b" xref="S5.E1.m1.1.1.1.1.1.3.13e.cmml"><sub id="S5.E1.m1.1.1.1.1.1.3.13.2nest" class="ltx_sub"><span id="S5.E1.m1.1.1.1.1.1.3.13.2.1nest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></mrow></mrow><mo stretchy="false" id="S5.E1.m1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S5.E1.m1.1.1.3" xref="S5.E1.m1.1.1.3.cmml"><mi id="S5.E1.m1.1.1.3.2" xref="S5.E1.m1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.3" xref="S5.E1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1a" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.4" xref="S5.E1.m1.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1b" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.5" xref="S5.E1.m1.1.1.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1c" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.6" xref="S5.E1.m1.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1d" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.7" xref="S5.E1.m1.1.1.3.7.cmml">d</mi><mo lspace="0.500em" rspace="0em" id="S5.E1.m1.1.1.3.1e" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.8" xref="S5.E1.m1.1.1.3.8.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1f" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.9" xref="S5.E1.m1.1.1.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1g" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.10" xref="S5.E1.m1.1.1.3.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1h" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.11" xref="S5.E1.m1.1.1.3.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.3.1i" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.3.12" xref="S5.E1.m1.1.1.3.12.cmml">h</mi><mo lspace="0.500em" rspace="0em" id="S5.E1.m1.1.1.3.1j" xref="S5.E1.m1.1.1.3.1.cmml">​</mo><mrow id="S5.E1.m1.1.1.3.13" xref="S5.E1.m1.1.1.3.13e.cmml"><mtext class="ltx_mathvariant_italic" id="S5.E1.m1.1.1.3.13a" xref="S5.E1.m1.1.1.3.13e.cmml">F</mtext><mtext id="S5.E1.m1.1.1.3.13b" xref="S5.E1.m1.1.1.3.13e.cmml"><sub id="S5.E1.m1.1.1.3.13.2nest" class="ltx_sub"><span id="S5.E1.m1.1.1.3.13.2.1nest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.2.cmml" xref="S5.E1.m1.1.2"><times id="S5.E1.m1.1.2.1.cmml" xref="S5.E1.m1.1.2.1"></times><apply id="S5.E1.m1.1.2.2.cmml" xref="S5.E1.m1.1.2.2"><divide id="S5.E1.m1.1.2.2.1.cmml" xref="S5.E1.m1.1.2.2"></divide><cn type="integer" id="S5.E1.m1.1.2.2.2.cmml" xref="S5.E1.m1.1.2.2.2">1</cn><ci id="S5.E1.m1.1.2.2.3.cmml" xref="S5.E1.m1.1.2.2.3">𝑁</ci></apply><apply id="S5.E1.m1.1.2.3.cmml" xref="S5.E1.m1.1.2.3"><apply id="S5.E1.m1.1.2.3.1.cmml" xref="S5.E1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.1.1.cmml" xref="S5.E1.m1.1.2.3.1">superscript</csymbol><apply id="S5.E1.m1.1.2.3.1.2.cmml" xref="S5.E1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.1.2.1.cmml" xref="S5.E1.m1.1.2.3.1">subscript</csymbol><sum id="S5.E1.m1.1.2.3.1.2.2.cmml" xref="S5.E1.m1.1.2.3.1.2.2"></sum><apply id="S5.E1.m1.1.2.3.1.2.3.cmml" xref="S5.E1.m1.1.2.3.1.2.3"><eq id="S5.E1.m1.1.2.3.1.2.3.1.cmml" xref="S5.E1.m1.1.2.3.1.2.3.1"></eq><ci id="S5.E1.m1.1.2.3.1.2.3.2.cmml" xref="S5.E1.m1.1.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S5.E1.m1.1.2.3.1.2.3.3.cmml" xref="S5.E1.m1.1.2.3.1.2.3.3">1</cn></apply></apply><ci id="S5.E1.m1.1.2.3.1.3.cmml" xref="S5.E1.m1.1.2.3.1.3">𝑁</ci></apply><apply id="S5.E1.m1.1.1.cmml" xref="S5.E1.m1.1.1"><divide id="S5.E1.m1.1.1.2.cmml" xref="S5.E1.m1.1.1"></divide><apply id="S5.E1.m1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1"><abs id="S5.E1.m1.1.1.1.2.1.cmml" xref="S5.E1.m1.1.1.1.1.2"></abs><apply id="S5.E1.m1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1"><minus id="S5.E1.m1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1"></minus><apply id="S5.E1.m1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.2"><times id="S5.E1.m1.1.1.1.1.1.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.2.1"></times><ci id="S5.E1.m1.1.1.1.1.1.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.2.2">𝑃</ci><ci id="S5.E1.m1.1.1.1.1.1.2.3.cmml" xref="S5.E1.m1.1.1.1.1.1.2.3">𝑟</ci><ci id="S5.E1.m1.1.1.1.1.1.2.4.cmml" xref="S5.E1.m1.1.1.1.1.1.2.4">𝑒</ci><ci id="S5.E1.m1.1.1.1.1.1.2.5.cmml" xref="S5.E1.m1.1.1.1.1.1.2.5">𝑑</ci><ci id="S5.E1.m1.1.1.1.1.1.2.6.cmml" xref="S5.E1.m1.1.1.1.1.1.2.6">𝑖</ci><ci id="S5.E1.m1.1.1.1.1.1.2.7.cmml" xref="S5.E1.m1.1.1.1.1.1.2.7">𝑐</ci><ci id="S5.E1.m1.1.1.1.1.1.2.8.cmml" xref="S5.E1.m1.1.1.1.1.1.2.8">𝑡</ci><ci id="S5.E1.m1.1.1.1.1.1.2.9.cmml" xref="S5.E1.m1.1.1.1.1.1.2.9">𝑒</ci><ci id="S5.E1.m1.1.1.1.1.1.2.10.cmml" xref="S5.E1.m1.1.1.1.1.1.2.10">𝑑</ci><ci id="S5.E1.m1.1.1.1.1.1.2.11e.cmml" xref="S5.E1.m1.1.1.1.1.1.2.11"><mrow id="S5.E1.m1.1.1.1.1.1.2.11.cmml" xref="S5.E1.m1.1.1.1.1.1.2.11"><mtext class="ltx_mathvariant_italic" id="S5.E1.m1.1.1.1.1.1.2.11a.cmml" xref="S5.E1.m1.1.1.1.1.1.2.11">F</mtext><mtext id="S5.E1.m1.1.1.1.1.1.2.11b.cmml" xref="S5.E1.m1.1.1.1.1.1.2.11"><sub id="S5.E1.m1.1.1.1.1.1.2.11.2anest" class="ltx_sub"><span id="S5.E1.m1.1.1.1.1.1.2.11.2.1anest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></ci></apply><apply id="S5.E1.m1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3"><times id="S5.E1.m1.1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.1"></times><ci id="S5.E1.m1.1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2">𝐺</ci><ci id="S5.E1.m1.1.1.1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3.3">𝑟</ci><ci id="S5.E1.m1.1.1.1.1.1.3.4.cmml" xref="S5.E1.m1.1.1.1.1.1.3.4">𝑜</ci><ci id="S5.E1.m1.1.1.1.1.1.3.5.cmml" xref="S5.E1.m1.1.1.1.1.1.3.5">𝑢</ci><ci id="S5.E1.m1.1.1.1.1.1.3.6.cmml" xref="S5.E1.m1.1.1.1.1.1.3.6">𝑛</ci><ci id="S5.E1.m1.1.1.1.1.1.3.7.cmml" xref="S5.E1.m1.1.1.1.1.1.3.7">𝑑</ci><ci id="S5.E1.m1.1.1.1.1.1.3.8.cmml" xref="S5.E1.m1.1.1.1.1.1.3.8">𝑇</ci><ci id="S5.E1.m1.1.1.1.1.1.3.9.cmml" xref="S5.E1.m1.1.1.1.1.1.3.9">𝑟</ci><ci id="S5.E1.m1.1.1.1.1.1.3.10.cmml" xref="S5.E1.m1.1.1.1.1.1.3.10">𝑢</ci><ci id="S5.E1.m1.1.1.1.1.1.3.11.cmml" xref="S5.E1.m1.1.1.1.1.1.3.11">𝑡</ci><ci id="S5.E1.m1.1.1.1.1.1.3.12.cmml" xref="S5.E1.m1.1.1.1.1.1.3.12">ℎ</ci><ci id="S5.E1.m1.1.1.1.1.1.3.13e.cmml" xref="S5.E1.m1.1.1.1.1.1.3.13"><mrow id="S5.E1.m1.1.1.1.1.1.3.13.cmml" xref="S5.E1.m1.1.1.1.1.1.3.13"><mtext class="ltx_mathvariant_italic" id="S5.E1.m1.1.1.1.1.1.3.13a.cmml" xref="S5.E1.m1.1.1.1.1.1.3.13">F</mtext><mtext id="S5.E1.m1.1.1.1.1.1.3.13b.cmml" xref="S5.E1.m1.1.1.1.1.1.3.13"><sub id="S5.E1.m1.1.1.1.1.1.3.13.2anest" class="ltx_sub"><span id="S5.E1.m1.1.1.1.1.1.3.13.2.1anest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></ci></apply></apply></apply><apply id="S5.E1.m1.1.1.3.cmml" xref="S5.E1.m1.1.1.3"><times id="S5.E1.m1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.3.1"></times><ci id="S5.E1.m1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.3.2">𝐺</ci><ci id="S5.E1.m1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.3.3">𝑟</ci><ci id="S5.E1.m1.1.1.3.4.cmml" xref="S5.E1.m1.1.1.3.4">𝑜</ci><ci id="S5.E1.m1.1.1.3.5.cmml" xref="S5.E1.m1.1.1.3.5">𝑢</ci><ci id="S5.E1.m1.1.1.3.6.cmml" xref="S5.E1.m1.1.1.3.6">𝑛</ci><ci id="S5.E1.m1.1.1.3.7.cmml" xref="S5.E1.m1.1.1.3.7">𝑑</ci><ci id="S5.E1.m1.1.1.3.8.cmml" xref="S5.E1.m1.1.1.3.8">𝑇</ci><ci id="S5.E1.m1.1.1.3.9.cmml" xref="S5.E1.m1.1.1.3.9">𝑟</ci><ci id="S5.E1.m1.1.1.3.10.cmml" xref="S5.E1.m1.1.1.3.10">𝑢</ci><ci id="S5.E1.m1.1.1.3.11.cmml" xref="S5.E1.m1.1.1.3.11">𝑡</ci><ci id="S5.E1.m1.1.1.3.12.cmml" xref="S5.E1.m1.1.1.3.12">ℎ</ci><ci id="S5.E1.m1.1.1.3.13e.cmml" xref="S5.E1.m1.1.1.3.13"><mrow id="S5.E1.m1.1.1.3.13.cmml" xref="S5.E1.m1.1.1.3.13"><mtext class="ltx_mathvariant_italic" id="S5.E1.m1.1.1.3.13a.cmml" xref="S5.E1.m1.1.1.3.13">F</mtext><mtext id="S5.E1.m1.1.1.3.13b.cmml" xref="S5.E1.m1.1.1.3.13"><sub id="S5.E1.m1.1.1.3.13.2anest" class="ltx_sub"><span id="S5.E1.m1.1.1.3.13.2.1anest" class="ltx_text ltx_font_italic">i</span></sub></mtext></mrow></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">\frac{1}{N}\sum_{i=1}^{N}\frac{|Predicted\ \textit{F\textsubscript{i}}-Ground\ Truth\ \textit{F\textsubscript{i}}|}{Ground\ Truth\ \textit{F\textsubscript{i}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S5.SS1.p2.2" class="ltx_p">where <em id="S5.SS1.p2.2.1" class="ltx_emph ltx_font_italic">N</em> is the number of test scenes and <span id="S5.SS1.p2.2.2" class="ltx_text ltx_font_italic">F<sub id="S5.SS1.p2.2.2.1" class="ltx_sub">i</sub></span> is the funniness score for the test scene <em id="S5.SS1.p2.2.3" class="ltx_emph ltx_font_italic">i</em>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Baseline:</span>
The baseline model always predicts the average funniness score of the training scenes.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Model.</span>
As shown in Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Predicting Funniness Score ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we observe that our model trained using combinations of different scene-level features (described in Sec. <a href="#S4.SS1" title="4.1 Features ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) performs better than the baseline model. We see that Location features perform slightly better than Cardinality. This makes sense because Location features also have occurrence information. The Embedding does not have location information and hence does worse. Due to some redundancy (all features have occurrence information), combining them does not improve performance.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.2.1.1" class="ltx_tr">
<th id="S5.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Features</span></th>
<th id="S5.T1.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Avg. Rel. Err.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.2.2.1" class="ltx_tr">
<th id="S5.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Avg. Prediction Baseline</span></th>
<td id="S5.T1.2.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.2.1.2.1" class="ltx_text" style="font-size:90%;">0.3151</span></td>
</tr>
<tr id="S5.T1.2.3.2" class="ltx_tr">
<th id="S5.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.3.2.1.1" class="ltx_text" style="font-size:90%;">Embedding</span></th>
<td id="S5.T1.2.3.2.2" class="ltx_td ltx_align_right" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.3.2.2.1" class="ltx_text" style="font-size:90%;">0.2516</span></td>
</tr>
<tr id="S5.T1.2.4.3" class="ltx_tr">
<th id="S5.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.4.3.1.1" class="ltx_text" style="font-size:90%;">Cardinality</span></th>
<td id="S5.T1.2.4.3.2" class="ltx_td ltx_align_right" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.4.3.2.1" class="ltx_text" style="font-size:90%;">0.2450</span></td>
</tr>
<tr id="S5.T1.2.5.4" class="ltx_tr">
<th id="S5.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.5.4.1.1" class="ltx_text" style="font-size:90%;">Location</span></th>
<td id="S5.T1.2.5.4.2" class="ltx_td ltx_align_right" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.5.4.2.1" class="ltx_text" style="font-size:90%;">0.2400</span></td>
</tr>
<tr id="S5.T1.2.6.5" class="ltx_tr">
<th id="S5.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.6.5.1.1" class="ltx_text" style="font-size:90%;">Embedding + Cardinality + Location</span></th>
<td id="S5.T1.2.6.5.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T1.2.6.5.2.1" class="ltx_text" style="font-size:90%;">0.2400</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of different feature combinations in predicting funniness score <span id="S5.T1.7.1" class="ltx_text ltx_font_italic">F<sub id="S5.T1.7.1.1" class="ltx_sub">i</sub></span> of a scene.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Altering Funniness of a Scene</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We discuss the performance in the tasks of identifying objects in a scene that contribute to humor (Sec. <a href="#S4.SS2" title="4.2 Predicting Funniness Score ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) and replacing those objects with other objects to reduce (or increase) humor (Sec. <a href="#S4.SS3" title="4.3 Altering Funniness of a Scene ‣ 4 Approach ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Predicting Objects to be Replaced</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">We train this model to detect objects instances that are funny in the scene. It makes a binary prediction whether each instance should be replaced or not. 
<br class="ltx_break"><span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Metric.</span> Along with naïve accuracy (% of correct predictions, <em id="S5.SS2.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.SSS1.p1.1.3" class="ltx_text"></span>, Acc.), we also report average class-wise accuracy (<em id="S5.SS2.SSS1.p1.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.SSS1.p1.1.5" class="ltx_text"></span>, Avg. Cl. Acc.) to determine the performance of our model for this task. As the data is skewed, with the majority class being <em id="S5.SS2.SSS1.p1.1.6" class="ltx_emph ltx_font_italic">not-replace</em>, we require our model to perform well both class-wise and as a whole. 
<br class="ltx_break"><span id="S5.SS2.SSS1.p1.1.7" class="ltx_text ltx_font_bold">Baselines:</span></p>
<ol id="S5.I2" class="ltx_enumerate">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Priors.</span> We always predict that an instance should not be replaced. We also compute a stronger baseline that replaces an object if it is replaced at least T% of the time in training data. T was set to 20 based on the validation set.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Anomaly Detection.</span> From the scene embedding, we subtract the object embedding of the object under consideration. We then compute the cosine similarity of the resultant scene embedding with the object embedding. Objects with the least similarity with the scene are the anomalous objects in the scene.
This is similar to finding the odd-one-out given a group of words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Objects that have a cosine similarity less than a threshold T with the scene are predicted as anomalous objects and are replaced. A modification to this baseline is to replace K objects that are least similar to the scene. Based on performance on the validation set, T and K are determined to be 0.8 and 4, respectively.</p>
</div>
</li>
</ol>
<p id="S5.SS2.SSS1.p1.2" class="ltx_p"><span id="S5.SS2.SSS1.p1.2.1" class="ltx_text ltx_font_bold">Model.</span>
Table <a href="#S5.T2" title="Table 2 ‣ 5.2.2 Making a Scene Unfunny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the performance of our model with the baselines described above. We observe that the baseline based on priors performs better than anomaly detection. This is perhaps not surprising because the prior-based baseline, while naïve, is “supervised” in the sense that it relies on statistics from the training dataset of which objects tend to get replaced. On the other hand, anomaly detection is completely unsupervised since it only captures the context of objects in <em id="S5.SS2.SSS1.p1.2.2" class="ltx_emph ltx_font_italic">normal</em> scenes. Our approach performs better than the baseline approaches in identifying objects that contribute to humor.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">On average, we observe that our model replaces 3.67 objects for a given image as compared to an average of 2.54 objects replaced in the ground truth. This bias to replace more objects ensures that a given scene becomes significantly less funny than the original scene.
We observe that the model learns that in general, animate objects like humans and animals are potentially stronger sources of humor compared to inanimate objects. It is interesting to note that the model also learns fine-grained detail, <em id="S5.SS2.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS2.SSS1.p2.1.2" class="ltx_text"></span>, to replace older people playing outdoors (which may be considered funny) with younger people (Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.2.2 Making a Scene Unfunny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, top row).</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Making a Scene Unfunny</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Given that an object is predicted to be replaced in the scene, the model has to also predict a suitable replacer object. In this section, we discuss the performance of the model in predicting these replacer objects.
This model is trained and evaluated using ground truth annotations of objects that are replaced by humans in a scene. This helps us isolate performance between predicting <em id="S5.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">which objects to replace</em> and predicting <em id="S5.SS2.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">suitable replacers</em> .</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p"><span id="S5.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Metric.</span>
In order to evaluate the performance of the model on the task of replacing funny objects in the scene to make it unfunny, we use the top-5 metric (similar to ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>), <em id="S5.SS2.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.SSS2.p2.1.3" class="ltx_text"></span>, if any of our 5 most confident predictions match the ground truth, we consider that as a correct prediction. 
<br class="ltx_break"><span id="S5.SS2.SSS2.p2.1.4" class="ltx_text ltx_font_bold">Baselines:</span></p>
<ol id="S5.I3" class="ltx_enumerate">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Priors.</span> Every object is replaced by one of its 5 most frequent replacers in the training set.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Anomaly Detection.</span> We subtract the embedding of the object that is to be replaced from the scene embedding. The 5 objects from the clipart vocabulary that are most similar (in the embedding space) to this resultant scene embedding are the ones that contextually “fit in”.</p>
</div>
</li>
</ol>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Avg. Cl. Acc.</span></th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<td id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Priors (do not replace)</span></td>
<td id="S5.T2.2.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.2.1.2.1" class="ltx_text" style="font-size:90%;">50% %</span></td>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<span id="S5.T2.2.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">79.86</span><span id="S5.T2.2.2.1.3.2" class="ltx_text" style="font-size:90%;">%</span>
</td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr">
<td id="S5.T2.2.3.2.1" class="ltx_td ltx_align_left" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.3.2.1.1" class="ltx_text" style="font-size:90%;">Priors (object’s tendency to be replaced)</span></td>
<td id="S5.T2.2.3.2.2" class="ltx_td ltx_align_right" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.3.2.2.1" class="ltx_text" style="font-size:90%;">73.13 %</span></td>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.3.2.3.1" class="ltx_text" style="font-size:90%;">71.5%</span></td>
</tr>
<tr id="S5.T2.2.4.3" class="ltx_tr">
<td id="S5.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.4.3.1.1" class="ltx_text" style="font-size:90%;">Anomaly detection (threshold distance)</span></td>
<td id="S5.T2.2.4.3.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.4.3.2.1" class="ltx_text" style="font-size:90%;">62.16 %</span></td>
<td id="S5.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.4.3.3.1" class="ltx_text" style="font-size:90%;">58.30%</span></td>
</tr>
<tr id="S5.T2.2.5.4" class="ltx_tr">
<td id="S5.T2.2.5.4.1" class="ltx_td ltx_align_left" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.5.4.1.1" class="ltx_text" style="font-size:90%;">Anomaly detection (top-K objects)</span></td>
<td id="S5.T2.2.5.4.2" class="ltx_td ltx_align_right" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.5.4.2.1" class="ltx_text" style="font-size:90%;">63.01 %</span></td>
<td id="S5.T2.2.5.4.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.5.4.3.1" class="ltx_text" style="font-size:90%;">64.31%</span></td>
</tr>
<tr id="S5.T2.2.6.5" class="ltx_tr">
<td id="S5.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.6.5.1.1" class="ltx_text" style="font-size:90%;">Our model</span></td>
<td id="S5.T2.2.6.5.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.6.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">74.45%</span></td>
<td id="S5.T2.2.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T2.2.6.5.3.1" class="ltx_text" style="font-size:90%;">74.74%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.4.2" class="ltx_text" style="font-size:90%;">Performance of predicting whether an object should be replaced or not, for the task of altering a funny scene to make it unfunny. As the data is skewed with the majority class being “not-replace”, we require our model to perform well both class-wise and as a whole.</span></figcaption>
</figure>
<div id="S5.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p3.1" class="ltx_p"><span id="S5.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Model.</span>
We observe that the performance trend in Table <a href="#S5.T3" title="Table 3 ‣ 5.2.2 Making a Scene Unfunny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is similar to that observed in the previous section (Sec. <a href="#S5.SS2.SSS1" title="5.2.1 Predicting Objects to be Replaced ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>), <em id="S5.SS2.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">i.e</em>., our model performs better than priors, which performs better than anomaly detection. By qualitative inspection, we find that our top prediction is intelligent, but lazy. It eliminates humor in most scenes by choosing to replace objects contributing to humor with other objects that blend well into the background. By relegating an object to the background, it is rendered inactive and hence, cannot be contribute to humor in the scene. For <em id="S5.SS2.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS2.SSS2.p3.1.4" class="ltx_text"></span>, the top prediction is frequently “plant” in indoor scenes and “butterfly” in outdoor scenes. The 2nd prediction is both intelligent and creative. It effectively reduces humor while also ensuring diversity of replacer objects.
Subsequent predictions from the model tend to be less meaningful. Qualitatively, we find the 2nd most confident prediction to be the best compromise.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Top-5 accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Priors (top 5 GT replacers)</span></th>
<td id="S5.T3.2.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.2.1.2.1" class="ltx_text" style="font-size:90%;">24.53%</span></td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.3.2.1.1" class="ltx_text" style="font-size:90%;">Anomaly detection (object that “fits” into scene)</span></th>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_right" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.3.2.2.1" class="ltx_text" style="font-size:90%;">7.69%</span></td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.4.3.1.1" class="ltx_text" style="font-size:90%;">Our model</span></th>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S5.T3.2.4.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">29.65%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.4.2" class="ltx_text" style="font-size:90%;">Performance of predicting which object to replace with, for the task of altering a funny scene to make it unfunny.</span></figcaption>
</figure>
<div id="S5.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p4.1" class="ltx_p"><span id="S5.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Full pipeline.</span>
Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.2.2 Making a Scene Unfunny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows qualitative results from our full pipeline (predicting objects to replace and predicting their replacers) using the 2nd predictions made by our model.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<p id="S5.F5.4" class="ltx_p ltx_align_center"><span id="S5.F5.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/funny2unfunny_image_before_res.png" id="S5.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S5.F5.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/funny2unfunny_image_after_res.png" id="S5.F5.2.2.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span>
<span id="S5.F5.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/before_2.png" id="S5.F5.3.3.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S5.F5.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/after_2.png" id="S5.F5.4.4.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.8.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.9.2" class="ltx_text" style="font-size:90%;">Fully automatic result of altering an input funny scene <em id="S5.F5.9.2.1" class="ltx_emph ltx_font_italic">(left)</em> into an unfunny scene <em id="S5.F5.9.2.2" class="ltx_emph ltx_font_italic">(right)</em>.</span></figcaption>
</figure>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Making a Scene Funny</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">We train our full pipeline model used in Sec. <a href="#S5.SS2.SSS2" title="5.2.2 Making a Scene Unfunny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.2</span></a> on scenes from the FOR dataset to perform the task of altering an unfunny scene to make it funny. Some qualitative results are shown in Fig. <a href="#S5.F6" title="Figure 6 ‣ 5.2.3 Making a Scene Funny ‣ 5.2 Altering Funniness of a Scene ‣ 5 Results ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<p id="S5.F6.4" class="ltx_p ltx_align_center"><span id="S5.F6.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/unfunny2funny_indoors_before.png" id="S5.F6.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S5.F6.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/unfunny2funny_indoors_after.png" id="S5.F6.2.2.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span>
<span id="S5.F6.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/unfunny2funny_outdoors_before.png" id="S5.F6.3.3.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"> </span><span id="S5.F6.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/unfunny2funny_outdoors_after.png" id="S5.F6.4.4.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.8.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.9.2" class="ltx_text" style="font-size:90%;">Fully automatic result of altering an input unfunny scene <em id="S5.F6.9.2.1" class="ltx_emph ltx_font_italic">(left)</em> into a funny scene <em id="S5.F6.9.2.2" class="ltx_emph ltx_font_italic">(right)</em>.</span></figcaption>
</figure>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Human Evaluation</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">We conducted two human studies to evaluate our full pipeline:</p>
<ol id="S5.I4" class="ltx_enumerate">
<li id="S5.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I4.i1.p1" class="ltx_para">
<p id="S5.I4.i1.p1.1" class="ltx_p"><span id="S5.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Absolute:</span> We ask 10 workers to rate the funniness of the scene predicted by our model on a scale of 1-5. We then compare this with the <span id="S5.I4.i1.p1.1.2" class="ltx_text ltx_font_italic">F<sub id="S5.I4.i1.p1.1.2.1" class="ltx_sub">i</sub></span> of the input funny scene.</p>
</div>
</li>
<li id="S5.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I4.i2.p1" class="ltx_para">
<p id="S5.I4.i2.p1.1" class="ltx_p"><span id="S5.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Relative:</span> We show 5 workers the input scene and the predicted scene (in random order) and ask them to indicate which scene is funnier.</p>
</div>
</li>
</ol>
<p id="S5.SS3.p1.2" class="ltx_p"><span id="S5.SS3.p1.2.1" class="ltx_text ltx_font_bold">Funny to unfunny.</span>
As expected, the output scenes from our model are less funny than the input funny scenes on average. The average <span id="S5.SS3.p1.2.2" class="ltx_text ltx_font_italic">F<sub id="S5.SS3.p1.2.2.1" class="ltx_sub">i</sub></span> of the input funny test scenes is 2.69. This is 1.05 points higher than the output unfunny scenes whose average <span id="S5.SS3.p1.2.3" class="ltx_text ltx_font_italic">F<sub id="S5.SS3.p1.2.3.1" class="ltx_sub">i</sub></span> is 1.64. Unsurprisingly, in relative evaluation, workers find our output scenes to be less funny than the input funny scenes 95% of the time. 
<br class="ltx_break"><span id="S5.SS3.p1.2.4" class="ltx_text ltx_font_bold">Unfunny to funny.</span> During absolute evaluation, we find that the average <span id="S5.SS3.p1.2.5" class="ltx_text ltx_font_italic">F<sub id="S5.SS3.p1.2.5.1" class="ltx_sub">i</sub></span> of scenes made funny by our model is 2.14. This is a relatively high score, considering that the average <span id="S5.SS3.p1.2.6" class="ltx_text ltx_font_italic">F<sub id="S5.SS3.p1.2.6.1" class="ltx_sub">i</sub></span> score of the corresponding originally funny scenes that were created by workers is 2.69. Interestingly, the relative evaluation can be perceived as a <em id="S5.SS3.p1.2.7" class="ltx_emph ltx_font_italic">Turing test</em> of sorts, where we show workers the model’s output funny scene and the original funny scene created by workers. 28% of the time, workers picked the model’s scenes to be <em id="S5.SS3.p1.2.8" class="ltx_emph ltx_font_italic">funnier</em>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Humor is a subtle and complex human behavior. It has many forms ranging from slapstick which has a simple physical nature, to satire which is nuanced and requires an understanding of social context <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. Understanding the entire spectrum of humor is a challenging task. It demands perception of fine-grained differences between seemingly similar scenarios. <em id="S6.p1.1.1" class="ltx_emph ltx_font_italic">E.g</em>.<span id="S6.p1.1.2" class="ltx_text"></span>, a teenager falling off his skateboard (such as in America’s Funniest Home Videos<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a href="www.afv.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.afv.com</a></span></span></span>) could be considered funny but an old person falling down the stairs is typically horrifying. Due to these challenges some people even consider computational humor to be an “AI-complete” problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">While understanding fine-grained semantics is important, it is interesting to note that there exists a qualitative difference in the way humor is perceived in abstract and real scenes. Since abstract scenes are not photorealistic, they afford us “suspension of reality”. Unlike real images, the content depicted in an abstract scene is benign. Thus, people are likely to find the depiction more funny <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. In our everyday lives, we come across a significant amount of humorous content in the form of comics and cartoons to which our computational models of humor are directly applicable. They can also be applied to learn semantics that can extend to photorealistic images as demonstrated by Antol <em id="S6.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S6.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Recognizing funniness involves violation of our mental model of how the world “ought to be” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. In verbal humor, the first few lines of the joke (set-up) build up the world model and the last line (punch line) goes against it. It is unclear what forms our mental model when we look at images. Is it our priors about the world around us formed from our past experiences? Is it because we attend to different regions of the image when we look at it and gradually build an expectation of what to see in the rest of the image? These are some interesting questions regarding visual humor that remain unanswered.</p>
</div>
</section>
<section id="S7" class="ltx_section ltx_pruned_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we take a step towards understanding and predicting visual humor. We collect two datasets of abstract scenes which enable the study of humor at different levels of granularity. We train a model to predict the <em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">funniness score</em> of a given scene. We also explore the different sources of humor depicted in the funny scenes via human studies. We train models using incongruity-based humor to alter a scene’s funniness. The models learn that in general, animate objects like humans and animals contribute more to humor compared to inanimate objects. Our model outperforms a strong anomaly detection baseline, demonstrating that detecting humor involves something more than just anomaly detection. In human studies of the task of making an originally funny scene unfunny, humans find our model’s output to be less funny 95% of the time. In the task of making a normal scene funny, our evaluation can be interpreted as a <em id="S7.p1.1.2" class="ltx_emph ltx_font_italic">Turing test</em> of sorts. Scenes made funny by our model were found to be funnier 28% of the time when compared with the original funny scenes created by workers. Note that our model would match humans at 50%. We hope that addressing the problem of studying visual humor using abstract scenes and the two datasets that are made public would stimulate further research in this new direction.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements. </span>We thank the anonymous reviewers for their valuable comments and suggestions. This work was supported in part by the Paul G. Allen Family Foundation via an award to D.P. DB was partially supported by the National Science Foundation CAREER award, the Army Research Office YIP award, and an Office of Naval Research grant N00014-14-1-0679. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor. We thank Xinlei Chen for his work on earlier versions of the clipart interface.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Overview Of Appendix</span></p>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.1" class="ltx_p">In the following appendix we provide:</p>
<ol id="S7.I1" class="ltx_enumerate">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I.</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">Inter-human agreement on funniness ratings in the Abstract Visual Humor (AVH) dataset.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II.</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Details of the model architecture used to learn object embeddings and visualizations of its embeddings.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III.</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">A sample of objects from the abstract scenes vocabulary.</p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">IV.</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p">Examples of scenes from our datasets.</p>
</div>
</li>
<li id="S7.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">V.</span> 
<div id="S7.I1.i5.p1" class="ltx_para">
<p id="S7.I1.i5.p1.1" class="ltx_p">Analysis of occurrences of different object types in scenes from our datasets.</p>
</div>
</li>
<li id="S7.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">VI.</span> 
<div id="S7.I1.i6.p1" class="ltx_para">
<p id="S7.I1.i6.p1.1" class="ltx_p">The user interfaces used to collect scenes for the AVH and Funny Object Replaced (FOR) datasets. 
<br class="ltx_break"></p>
</div>
</li>
</ol>
<p id="S7.p4.2" class="ltx_p"><span id="S7.p4.2.1" class="ltx_text ltx_font_bold">Appendix I: Inter-human Agreement</span></p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">In this section, we describe our experiment to determine inter-human agreement in funniness ratings of scenes. The Abstract Visual Humor (AVH) dataset contains 3,028 funny scenes and 3,372 unfunny scenes that were created by Amazon Mechanical Turk (AMT) workers. The funniness of each scene in the dataset is rated by 10 different workers on a scale of 1-5. We define the <em id="S7.p5.1.1" class="ltx_emph ltx_font_italic">funniness score</em> of a scene, as the average of all ratings for a scene. In this section, we investigate the extent to which people agree regarding the funniness of a scene.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p id="S7.p6.1" class="ltx_p">Perception of an image differs from one person to another. Moran <em id="S7.p6.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p6.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> treat humor appreciation by people as a personality characteristic. We investigate to what extent people agree how funny each scene in our dataset is. We split the votes we received for each scene into two groups, keeping each individual worker’s ratings in the same group to the extent possible. We compute the <em id="S7.p6.1.3" class="ltx_emph ltx_font_italic">funniness score</em> of each scene across workers in each group. We compute Pearson’s correlation between the two groups. Fig. <a href="#S7.F7" title="Figure 7 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a plot of Pearson’s correlation (y-axis) <em id="S7.p6.1.4" class="ltx_emph ltx_font_italic">vs</em>.<span id="S7.p6.1.5" class="ltx_text"></span> the number of workers (x-axis). We can see that inter-human agreement increases as we increase the number of workers in a group and that the trend is gradually saturating. This indicates that ratings from 10 workers is sufficient to compute a reliable <em id="S7.p6.1.6" class="ltx_emph ltx_font_italic">funniness score</em>.</p>
</div>
<div id="S7.p7" class="ltx_para">
<p id="S7.p7.1" class="ltx_p">We observed that the standard deviation among ratings from 10 different workers for funny scenes is 1.09, and for unfunny scenes is 0.73. <em id="S7.p7.1.1" class="ltx_emph ltx_font_italic">I.e</em>.<span id="S7.p7.1.2" class="ltx_text"></span>, people agree more on scenes that are clearly not funny than on ones that are funny, matching our intuition that humor is subjective, while the lack thereof is not.</p>
</div>
<figure id="S7.F7" class="ltx_figure">
<p id="S7.F7.1" class="ltx_p ltx_align_center"><span id="S7.F7.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/inter_human_agreement.png" id="S7.F7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="309" height="216" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.4.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S7.F7.5.2" class="ltx_text" style="font-size:90%;">Inter-human agreement (y-axis) as we collect funniness ratings from more workers (x-axis). We see can see that by 10 ratings, we are starting to saturate with high agreement, indicating that 10 ratings is sufficient for a reliable <em id="S7.F7.5.2.1" class="ltx_emph ltx_font_italic">funniness score</em>.</span></figcaption>
</figure>
<div id="S7.p9" class="ltx_para">
<p id="S7.p9.1" class="ltx_p"><span id="S7.p9.1.1" class="ltx_text ltx_font_bold">Appendix II: Object Embeddings</span></p>
</div>
<figure id="S7.F8" class="ltx_figure">
<p id="S7.F8.2.2" class="ltx_p ltx_align_center"><span id="S7.F8.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/embdng_top75.png" id="S7.F8.1.1.1.g1" class="ltx_graphics ltx_img_square" width="309" height="309" alt="Refer to caption"> </span>
<span id="S7.F8.2.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/humor_embdng_top75.png" id="S7.F8.2.2.2.g1" class="ltx_graphics ltx_img_square" width="309" height="309" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F8.6.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><em id="S7.F8.7.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Left</em><span id="S7.F8.8.3" class="ltx_text" style="font-size:90%;">. Visualization of “normal” object embeddings of 75 most frequent objects in unfunny scenes. We see that closely placed objects have semantically similar meanings. <em id="S7.F8.8.3.1" class="ltx_emph ltx_font_italic">Right</em>. Visualization of “humor” embeddings of 75 most frequent objects in funny scenes. We see that objects that are close in the “humor” embedding space may be semantically very different.</span></figcaption>
</figure>
<figure id="S7.F9" class="ltx_figure">
<p id="S7.F9.1.1" class="ltx_p ltx_align_center"><span id="S7.F9.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/embed_model.png" id="S7.F9.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="309" height="137" alt="Refer to caption"> </span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S7.F9.4.2" class="ltx_text" style="font-size:90%;">The continuous Bag-of-Words model used to obtain the object embeddings.</span></figcaption>
</figure>
<div id="S7.p10" class="ltx_para">
<p id="S7.p10.1" class="ltx_p">In this section, we describe our model that learns embeddings for clipart objects and present visualizations of these embeddings. We learn distributed representations for each object category in the abstract scenes vocabulary using a word2vec-style continuous Bag-of-Words model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. During training, subsets of 6 objects are sampled from all of the objects present in a scene and the model tries to predict one of the objects, given the other 5. Each object is assigned a 150-d vector, which is randomly initialized.
The vectors corresponding to the 5 context objects
are projected to an embedding space via a single layer whose parameters are shared between the 5 objects.
This (randomly initialized) layer consists of 150 hidden units without a non-linearity after it.
The sum of these 5 object projections is used to compute a softmax over the 150 classes in the object vocabulary.
Using the correct label (<em id="S7.p10.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p10.1.2" class="ltx_text"></span>, the object category of the 6th object), the cross-entropy loss is computed and backpropagated to learn all network parameters.
The model is trained using Stochastic Gradient Descent with a base learning rate of 0.0001 and a momentum update of 0.9. The learning rate was reduced by a factor of two after each epoch.
A diagram of the model can be seen in Fig. <a href="#S7.F9" title="Figure 9 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S7.p11" class="ltx_para">
<p id="S7.p11.1" class="ltx_p">The context provided by the 5 objects ensures that the representations learnt reflect the relationships between objects. <em id="S7.p11.1.1" class="ltx_emph ltx_font_italic">I.e</em>.<span id="S7.p11.1.2" class="ltx_text"></span>, objects that are semantically related tend to have similar representations. We learn the “normal” embeddings (<em id="S7.p11.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p11.1.4" class="ltx_text"></span>, the object embedding instance-level features from the main paper) from 11K scenes collected by Antol <em id="S7.p11.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p11.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. As these scenes were not intended to be humorous, the relationships captured in the embeddings are the ones that occur naturally in the abstract scenes world.</p>
</div>
<div id="S7.p12" class="ltx_para">
<p id="S7.p12.1" class="ltx_p">Fig. <a href="#S7.F8" title="Figure 8 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> <em id="S7.p12.1.1" class="ltx_emph ltx_font_italic">(left)</em> is a t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> visualization of the “normal” embeddings for the 75 most frequent objects in unfunny scenes. In Fig. <a href="#S7.F8" title="Figure 8 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> <em id="S7.p12.1.2" class="ltx_emph ltx_font_italic">(right)</em>, we also visualize “humor” embeddings, which were not used as features but provide us with insights. These are learnt from the 3,028 funny scenes in the AVH dataset.</p>
</div>
<div id="S7.p13" class="ltx_para">
<p id="S7.p13.1" class="ltx_p">We observe that the “normal” embeddings encode a notion for which object categories occur in similar contexts. We also observe that closely placed objects in the “normal” embedding space have semantically similar meanings. For instance, humans are clustered together around coordinates <em id="S7.p13.1.1" class="ltx_emph ltx_font_italic">(10, -7)</em>. Interestingly, “dog” and “puppy” (coordinates <em id="S7.p13.1.2" class="ltx_emph ltx_font_italic">(10, -5)</em>) are placed together and furniture like “chair”, “bookshelf”, “armchair”,  <em id="S7.p13.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S7.p13.1.4" class="ltx_text"></span> are placed together (coordinates <em id="S7.p13.1.5" class="ltx_emph ltx_font_italic">(10, 5)</em>). This follows from the distributional hypothesis, which states that words which occur in the similar contexts tend to have similar meanings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<figure id="S7.F10" class="ltx_figure">
<p id="S7.F10.1.1" class="ltx_p ltx_align_center"><span id="S7.F10.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/objects_clipart_humans.jpg" id="S7.F10.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="220" alt="Refer to caption"> </span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S7.F10.4.2" class="ltx_text" style="font-size:90%;">A subset of clipart objects from the abstract scenes vocabulary.</span></figcaption>
</figure>
<figure id="S7.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf1.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3EKVH9QMEY4OR6LKRRBUN4DZDFV2DE_00.png" id="S7.F11.sf1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S7.F11.sf1.4.2" class="ltx_text" style="font-size:90%;">1.3</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf2.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/37U1UTWH9VMTEV6EP9FF2K76HY6R87_01.png" id="S7.F11.sf2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S7.F11.sf2.4.2" class="ltx_text" style="font-size:90%;">2.8</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf3.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf3.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/373ERPL3YO8CCFY2S7QATG3TVBUTR1_01.png" id="S7.F11.sf3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S7.F11.sf3.4.2" class="ltx_text" style="font-size:90%;">3.2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf4.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf4.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3IOEN3P9S7JIHCO9Y032CFT24SN167_01.png" id="S7.F11.sf4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S7.F11.sf4.4.2" class="ltx_text" style="font-size:90%;">4.4</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf5.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf5.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3L70J4KAZGMDW9GO4649TMWXP1RADT_00.png" id="S7.F11.sf5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf5.3.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S7.F11.sf5.4.2" class="ltx_text" style="font-size:90%;">1.1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf6.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf6.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3J88R45B2GYYHJR7KPNIDOOK47WXPV_00.png" id="S7.F11.sf6.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf6.3.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S7.F11.sf6.4.2" class="ltx_text" style="font-size:90%;">2.7</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf7" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf7.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf7.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3KAKFY4PGU2UKZXPN5QAXBY4L0JI31_00.png" id="S7.F11.sf7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf7.3.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S7.F11.sf7.4.2" class="ltx_text" style="font-size:90%;">3.5</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S7.F11.sf8" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S7.F11.sf8.1" class="ltx_p ltx_align_center"><span id="S7.F11.sf8.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/3DIP6YHAPCS45R1933VF3D8N9SLE8V_01.png" id="S7.F11.sf8.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="94" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf8.3.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="S7.F11.sf8.4.2" class="ltx_text" style="font-size:90%;">4.1</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S7.F11.4.2" class="ltx_text" style="font-size:90%;">Spectrum of scenes from our AVH dataset that are arranged in ascending order of <em id="S7.F11.4.2.1" class="ltx_emph ltx_font_italic">funniness
score</em> (shown in the sub-caption)</span></figcaption>
</figure>
<div id="S7.p15" class="ltx_para">
<p id="S7.p15.1" class="ltx_p">In contrast, in the “humor” embeddings, visualized in Fig. <a href="#S7.F8" title="Figure 8 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> <em id="S7.p15.1.1" class="ltx_emph ltx_font_italic">(right)</em>, we see that objects that are close in the embedding space may be semantically very different. For instance, “dog” and “wine glass” are placed together at coordinates <em id="S7.p15.1.2" class="ltx_emph ltx_font_italic">(0, 0)</em>. These are placed far apart (at opposite ends) in the “normal” embedding. However, in the “humor” embedding, these two categories are extremely close to each other; even closer than semantically similar categories like two breeds of dogs. We hypothesize that this because our dataset contains funny scenes consisting of dogs with wine glasses, <em id="S7.p15.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.p15.1.4" class="ltx_text"></span>, Fig. <a href="#S7.F11.sf2" title="In Figure 11 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(b)</span></a>. It is interesting to note that “background” objects that do not contribute to humor in a scene are also placed together. For example, “chair”, “couch”, and “window” are placed together in the “humor” embedding as well (coordinates <em id="S7.p15.1.5" class="ltx_emph ltx_font_italic">(4, 5)</em>).</p>
</div>
<div id="S7.p16" class="ltx_para">
<p id="S7.p16.1" class="ltx_p">The understanding of semantically similar object categories that can occur in a context, represented by the “normal” embeddings, can be interpreted as a person’s mental model of the world. The “humor” embeddings capture deviations or incongruities from this “normal” view that might cause humor.</p>
</div>
<figure id="S7.F12" class="ltx_figure">
<p id="S7.F12.10" class="ltx_p"><span id="S7.F12.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/bf1.png" id="S7.F12.1.1.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/af1.png" id="S7.F12.2.2.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/bf5.png" id="S7.F12.3.3.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/af5.png" id="S7.F12.4.4.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.5.5" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/bf3.png" id="S7.F12.5.5.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.6.6" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/af3.png" id="S7.F12.6.6.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.7.7" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/bf6.png" id="S7.F12.7.7.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.8.8" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/af6.png" id="S7.F12.8.8.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.9.9" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/bf4.png" id="S7.F12.9.9.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span>
<span id="S7.F12.10.10" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/af4.png" id="S7.F12.10.10.g1" class="ltx_graphics ltx_img_landscape" width="161" height="92" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F12.14.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S7.F12.15.2" class="ltx_text" style="font-size:90%;">Some example originally funny scenes (<em id="S7.F12.15.2.1" class="ltx_emph ltx_font_italic">left</em>) and their object-replaced unfunny counterparts (<em id="S7.F12.15.2.2" class="ltx_emph ltx_font_italic">right</em>) from the FOR dataset.</span></figcaption>
</figure>
<div id="S7.p18" class="ltx_para">
<p id="S7.p18.1" class="ltx_p"><span id="S7.p18.1.1" class="ltx_text ltx_font_bold">Appendix III: Abstract Scenes Vocabulary</span></p>
</div>
<div id="S7.p19" class="ltx_para">
<p id="S7.p19.1" class="ltx_p">The abstract scenes interface developed by Antol <em id="S7.p19.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p19.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> consists of 20 “deformable” humans, 31 animals in different poses, and about 100 objects that can be found in indoor scenes (<em id="S7.p19.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.p19.1.4" class="ltx_text"></span>, couch, picture, doll, door, window, plant, fireplace) or outdoor scenes (<em id="S7.p19.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.p19.1.6" class="ltx_text"></span>, tree, pond, sun, clouds, bench, bike, campfire, grill, skateboard). In addition to the 8 different expressions available for humans, the ability to vary the pose of a human at a fine-grained level enables these abstract scenes to effectively capture the semantics of a scene. The large clipart vocabulary (of which only a fraction is shown to a worker during creation of a scene) ensures diversity in the scenes being depicted. A subset of objects from our Abstract Scenes vocabulary is shown in Fig. <a href="#S7.F10" title="Figure 10 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. 
<br class="ltx_break">
<br class="ltx_break"></p>
</div>
<div id="S7.p20" class="ltx_para ltx_noindent">
<p id="S7.p20.1" class="ltx_p"><span id="S7.p20.1.1" class="ltx_text ltx_font_bold">Appendix IV: Example Scenes</span></p>
</div>
<div id="S7.p21" class="ltx_para">
<p id="S7.p21.1" class="ltx_p">In this section, we present examples of scenes that were created using the abstract scenes interface. Fig. <a href="#S7.F11" title="Figure 11 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, depicts a spectrum of scenes from the AVH dataset in ascending order of <em id="S7.p21.1.1" class="ltx_emph ltx_font_italic">funniness score</em>. These scenes were created by AMT workers using the interface presented in Fig. <a href="#S7.F15" title="Figure 15 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>.</p>
</div>
<div id="S7.p22" class="ltx_para">
<p id="S7.p22.1" class="ltx_p">Fig. <a href="#S7.F12" title="Figure 12 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows originally funny scenes (<em id="S7.p22.1.1" class="ltx_emph ltx_font_italic">left</em>) and their unfunny counterparts (<em id="S7.p22.1.2" class="ltx_emph ltx_font_italic">right</em>) from the FOR dataset. AMT workers created the counterparts by replacing as few objects in the originally funny scene such that the resulting scene is not funny anymore. A screenshot of the interface that was used to create the unfunny counterparts is shown in Fig. <a href="#S7.F16" title="Figure 16 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. 
<br class="ltx_break"></p>
</div>
<div id="S7.p23" class="ltx_para ltx_noindent">
<p id="S7.p23.1" class="ltx_p"><span id="S7.p23.1.1" class="ltx_text ltx_font_bold">Appendix V: Object Type Occurrences</span></p>
</div>
<div id="S7.p24" class="ltx_para">
<p id="S7.p24.1" class="ltx_p">In this section, we first analyze the occurrence of each object type in funny and unfunny scenes. We then analyze the most commonly cooccurring object types in funny scenes as compared to unfunny scenes.</p>
</div>
<figure id="S7.F13" class="ltx_figure">
<p id="S7.F13.1" class="ltx_p ltx_align_center"><span id="S7.F13.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/salient_frac_co_occur_mat.png" id="S7.F13.1.1.g1" class="ltx_graphics ltx_img_square" width="309" height="309" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F13.7.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S7.F13.8.2" class="ltx_text" style="font-size:90%;">Top 100 object pairs that have the highest probabilities of cooccurring in a funny scene. Please note that repeated entries for an object type (<em id="S7.F13.8.2.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.F13.8.2.2" class="ltx_text"></span>, “dog”), correspond to slightly different versions (<em id="S7.F13.8.2.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.F13.8.2.4" class="ltx_text"></span>, breeds) of the same object type.</span></figcaption>
</figure>
<div id="S7.p26" class="ltx_para ltx_noindent">
<p id="S7.p26.1" class="ltx_p"><span id="S7.p26.1.1" class="ltx_text ltx_font_bold">Distribution of Object Types.</span> We analyze the distribution of object types in funny and unfunny scenes across all scenes in our dataset. We compute the frequency of appearance of each object type in funny and unfunny scenes. We use this to compute the probability of a scene being funny, given that an object is present in the scene, which is shown in <em id="S7.p26.1.2" class="ltx_emph ltx_font_italic">blue</em> in Fig. <a href="#S7.F14" title="Figure 14 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. Since we have more unfunny scenes than funny scenes, we use normalized counts.</p>
</div>
<div id="S7.p27" class="ltx_para">
<p id="S7.p27.1" class="ltx_p">We observe that the humans that most appear in funny scenes are elderly people. This is probably because a number of scenes in our dataset depict old men behaving unexpectedly, <em id="S7.p27.1.1" class="ltx_emph ltx_font_italic">e.g</em>., dancing or playing in the park as shown in Fig. <a href="#S7.F11.sf3" title="In Figure 11 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(c)</span></a>, which is funny.
Interestingly, we also observe that in general, animals appear more frequently in funny scenes. Animals like “mouse”, “rat”, “raccoon” and “bee” appear in funny scenes significantly more than they do in unfunny scenes. Other objects having a strong bias towards appearing in funny scenes include “wine bottle”, “pen”, “scissors”, “tape”, “game” and “beehive”. Thus, we see that certain object types have a tendency to appear in funny scenes. A possible reason for this is that these objects are involved in funny interactions, or are intrinsically funny, and hence contribute to humor in these scenes. 
<br class="ltx_break"><span id="S7.p27.1.2" class="ltx_text ltx_font_bold">Funny Cooccurrence Matrix.</span> We populate two object cooccurrence matrices – <span id="S7.p27.1.3" class="ltx_text ltx_font_bold">F</span> and <span id="S7.p27.1.4" class="ltx_text ltx_font_bold">U</span>, corresponding to funny scenes and unfunny scenes, respectively. Each element in <span id="S7.p27.1.5" class="ltx_text ltx_font_bold">F</span> and <span id="S7.p27.1.6" class="ltx_text ltx_font_bold">U</span> corresponds to the count of the cooccurrence of a pair of objects across all funny and unfunny scenes, respectively. To enable the study of types of cooccurrences that contribute to humor, we compute the probability of a scene being funny, given that a pair of objects cooccur in the scene as <math id="S7.p27.1.m1.1" class="ltx_Math" alttext="\frac{\textbf{F}}{\textbf{F}+\textbf{U}}" display="inline"><semantics id="S7.p27.1.m1.1a"><mfrac id="S7.p27.1.m1.1.1" xref="S7.p27.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S7.p27.1.m1.1.1.2" xref="S7.p27.1.m1.1.1.2a.cmml">F</mtext><mrow id="S7.p27.1.m1.1.1.3" xref="S7.p27.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S7.p27.1.m1.1.1.3.2" xref="S7.p27.1.m1.1.1.3.2a.cmml">F</mtext><mo id="S7.p27.1.m1.1.1.3.1" xref="S7.p27.1.m1.1.1.3.1.cmml">+</mo><mtext class="ltx_mathvariant_bold" id="S7.p27.1.m1.1.1.3.3" xref="S7.p27.1.m1.1.1.3.3a.cmml">U</mtext></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S7.p27.1.m1.1b"><apply id="S7.p27.1.m1.1.1.cmml" xref="S7.p27.1.m1.1.1"><divide id="S7.p27.1.m1.1.1.1.cmml" xref="S7.p27.1.m1.1.1"></divide><ci id="S7.p27.1.m1.1.1.2a.cmml" xref="S7.p27.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S7.p27.1.m1.1.1.2.cmml" xref="S7.p27.1.m1.1.1.2">F</mtext></ci><apply id="S7.p27.1.m1.1.1.3.cmml" xref="S7.p27.1.m1.1.1.3"><plus id="S7.p27.1.m1.1.1.3.1.cmml" xref="S7.p27.1.m1.1.1.3.1"></plus><ci id="S7.p27.1.m1.1.1.3.2a.cmml" xref="S7.p27.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S7.p27.1.m1.1.1.3.2.cmml" xref="S7.p27.1.m1.1.1.3.2">F</mtext></ci><ci id="S7.p27.1.m1.1.1.3.3a.cmml" xref="S7.p27.1.m1.1.1.3.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S7.p27.1.m1.1.1.3.3.cmml" xref="S7.p27.1.m1.1.1.3.3">U</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p27.1.m1.1c">\frac{\textbf{F}}{\textbf{F}+\textbf{U}}</annotation></semantics></math>, which is shown in Fig. <a href="#S7.F13" title="Figure 13 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> for the top 100 probable combinations that exist in a funny scene. Please note that repeated entries for an object type (<em id="S7.p27.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.p27.1.8" class="ltx_text"></span>, “dog”), correspond to slightly different versions (<em id="S7.p27.1.9" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.p27.1.10" class="ltx_text"></span>, breeds) of the same object type. An interesting set of object pairs that are present in funny scenes are “rat” appearing alongside “kitten”, “cat”, “stool”, and “dog”. Another interesting set of combinations is “raccoon” cooccurring with “bee”, “hamburger”, “basket”, and “wine glass”. We observe that this matrix captures interesting and unusual combinations of objects that appear together frequently in funny scenes. 
<br class="ltx_break"></p>
</div>
<div id="S7.p28" class="ltx_para ltx_noindent">
<p id="S7.p28.1" class="ltx_p"><span id="S7.p28.1.1" class="ltx_text ltx_font_bold">Appendix VI: User Interfaces</span></p>
</div>
<div id="S7.p29" class="ltx_para">
<p id="S7.p29.1" class="ltx_p">In this section, we present the user interfaces that were used to collect data from AMT. Fig. <a href="#S7.F15" title="Figure 15 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows a screenshot of the user interface that we used to collect funny scenes. Objects in the clipart library (on the <em id="S7.p29.1.1" class="ltx_emph ltx_font_italic">right</em> in the screenshot) can be dragged on to any part of the empty canvas shown in the figure. The pose, flip (<em id="S7.p29.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p29.1.3" class="ltx_text"></span>, lateral orientation), and size of all objects can be changed once they are placed in the scene. In the case of humans, one of 8 expressions must be chosen (initially humans have blank faces) and fine-grained pose adjustments are required.</p>
</div>
<div id="S7.p30" class="ltx_para">
<p id="S7.p30.1" class="ltx_p">Fig. <a href="#S7.F16" title="Figure 16 ‣ 7 Conclusion ‣ We Are Humor Beings: Understanding and Predicting Visual Humor" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows the interface that we used to collect “object-replaced” scenes for our FOR dataset. We showed workers an originally funny scene and asked them to replace objects in that scene so that the scene is not funny anymore. On clicking an object in the original scene, the object gets highlighted in green. A replacer object can then be chosen from the clipart library (displayed on the <em id="S7.p30.1.1" class="ltx_emph ltx_font_italic">right</em> in the screenshot). Objects that are replaced in the original scene show up in the empty canvas below. At any point, to undo a replacement, a user can click on the object in the below canvas and the corresponding object will be placed at its original position in the scene. The interface does not allow for the movement or the removal of objects.</p>
</div>
<figure id="S7.F14" class="ltx_figure">
<p id="S7.F14.1.1" class="ltx_p ltx_align_center"><span id="S7.F14.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/probFunny.jpg" id="S7.F14.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="336" height="916" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S7.F14.4.2" class="ltx_text" style="font-size:90%;">Probability of scene being funny, given object.</span></figcaption>
</figure>
<figure id="S7.F15" class="ltx_figure">
<p id="S7.F15.1.1" class="ltx_p ltx_align_center"><span id="S7.F15.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/create_funny.jpg" id="S7.F15.1.1.1.g1" class="ltx_graphics ltx_img_square" width="685" height="774" alt="Refer to caption"> </span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S7.F15.4.2" class="ltx_text" style="font-size:90%;">User interface used to create the funny scenes in the AVH dataset.</span></figcaption>
</figure>
<figure id="S7.F16" class="ltx_figure">
<p id="S7.F16.3" class="ltx_p"><span id="S7.F16.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/replace_obj_1.png" id="S7.F16.1.1.g1" class="ltx_graphics ltx_img_landscape" width="548" height="287" alt="Refer to caption"></span>
<span id="S7.F16.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/replace_obj_2.jpg" id="S7.F16.2.2.g1" class="ltx_graphics ltx_img_landscape" width="548" height="305" alt="Refer to caption"></span>
<span id="S7.F16.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="/html/1512.04407/assets/replace_obj_3.jpg" id="S7.F16.3.3.g1" class="ltx_graphics ltx_img_landscape" width="548" height="242" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F16.5.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="S7.F16.6.2" class="ltx_text" style="font-size:90%;">User interface to replace objects for the FOR dataset.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:80%;">VQA: Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
S. Antol, C. L. Zitnick, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:80%;">Zero-Shot Learning via Visual Abstraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">European Conference on Computer Vision, ECCV</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
Aristotle and R. McKeon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">The Basic Works of Aristotle</span><span id="bib.bib3.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:80%;">Modern Library, 2001.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
S. Attardo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Linguistic theories of humor</span><span id="bib.bib4.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:80%;">Walter de Gruyter, 1994.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
Bharata-Muni and M. Ghosh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:80%;">Natya shastra (with english translations).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:80%;">1951.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
K. Binsted, B. Bergen, D. O’Mara, S. Coulson, A. Nijholt, O. Stock,
C. Strapparava, G. Ritchie, R. Manurung, and H. Pain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:80%;">Computational humor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Intelligent Systems</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:80%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
K. Binsted and G. Ritchie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:80%;">Computational rules for generating punning riddles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Humor: International Journal of Humor Research</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:80%;">, 1997.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
E. R. Bressler, R. A. Martin, and S. Balshine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:80%;">Production and appreciation of humor as sexually selected traits.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Evolution and Human Behavior</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:80%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
M. Buijzen and P. M. Valkenburg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:80%;">Developing a typology of humor in audiovisual media.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Media Psychology</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:80%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
D. M. Buss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:80%;">The evolution of human intrasexual competition: Tactics of mate
attraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Journal of Personality and Social Psychology</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:80%;">, 1988.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
D. Davidov, O. Tsur, and A. Rappoport.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:80%;">Semi-supervised recognition of sarcastic sentences in twitter and
amazon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Conference on Computational Natural Language Learning</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:80%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
L. V. der Maaten and G. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:80%;">Visualizing data using t-SNE.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Journal of Machine Learning Research</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:80%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
A. Deza and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:80%;">Understanding image virality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
R. L. Duran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:80%;">Communicative adaptability: A measure of social communicative
competence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Communication Quarterly</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:80%;">, 1983.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
R. L. Duran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:80%;">Communicative adaptability: A review of conceptualization and
measurement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Communication Quarterly</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:80%;">, 1992.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
J. R. Firth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">A synopsis of linguistic theory</span><span id="bib.bib16.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:80%;">Blackwell, 1957.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
D. F. Fouhey and C. L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:80%;">Predicting object dynamics in scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
S. Freud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">The Joke and Its Relation to the Unconscious</span><span id="bib.bib18.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:80%;">Penguin, 2003.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
J. D. Goodchilds, J. Goldstein, and P. McGhee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:80%;">On being titty: Causes, correlates, and consequences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">The Psychology of Humor: Theoretical Perspectives and Empirical
Issues</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:80%;">, 1972.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
M. Gygli, H. Grabner, H. Riemenschneider, F. Nater, and L. Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:80%;">The interestingness of images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
Z. S. Harris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:80%;">Distributional structure. word, 10 (2-3): 146–162. reprinted in
fodor, j. a and katz, jj (eds.), readings in the philosophy of language,
1954.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
M. M. Hurley, D. C. Dennett, and R. B. Adams.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Inside jokes: Using humor to reverse-engineer the mind</span><span id="bib.bib22.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:80%;">MIT Press, 2011.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
P. Isola, D. Parikh, A. Torralba, and A. Oliva.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:80%;">Understanding the intrinsic memorability of images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NIPS</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:80%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
A. Khosla, A. Das Sarma, and R. Hamid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:80%;">What makes an image popular?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">International Conference on World Wide Web</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
S. Kottur, R. Vedantam, J. M. Moura, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:80%;">Visual word2vec (vis-w2v): Learning visually grounded word embeddings
using abstract scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:80%;">2015.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
X. Lin and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:80%;">Don’t just listen, use your imagination: Leveraging visual common
sense for non-visual tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
A. Mahapatra and J. Srivastava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:80%;">Incongruity versus incongruity resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 2013 International Conference on Social
Computing</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
R. A. Martin and N. A. Kuiper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:80%;">Daily occurrence of laughter: Relationships with age, gender, and
type a personality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Humor</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:80%;">, 1999.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
P. E. McGhee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:80%;">Chapter 5: The contribution of humor to children’s social
development.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Journal of Children in Contemporary Society</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:80%;">, 1989.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
A. P. McGraw and C. Warren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:80%;">Benign violations making immoral behavior funny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Psychological Science</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:80%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
R. Mihalcea.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:80%;">The multidisciplinary facets of research on humour.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">International Workshop on Fuzzy Logic and Applications</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:80%;">,
2007.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
R. Mihalcea and S. Pulman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:80%;">Characterizing humour: An exploration of features in humorous texts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Computational Linguistics and Intelligent Text Processing</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:80%;">,
2007.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
R. Mihalcea and C. Strapparava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:80%;">Making computers laugh: Investigations in automatic humor
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">EMNLP</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:80%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:80%;">
T. Mikolov, K. Chen, G. Corrado, and J. Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:80%;">Efficient estimation of word representations in vector space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1301.3781</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:80%;">
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:80%;">Distributed representations of words and phrases and their
compositionality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Advances in neural information processing systems</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:80%;">
D. Mobbs, M. D. Greicius, E. Abdel-Azim, V. Menon, and A. L. Reiss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:80%;">Humor modulates the mesolimbic reward centers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Neuron</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:80%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:80%;">
J. M. Moran, M. Rain, E. Page-Gould, and R. A. Mar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:80%;">Do i amuse you? asymmetric predictors for humor appreciation and
humor production.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Journal of Research in Personality</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:80%;">
M. P. Mulder and A. Nijholt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Humor Research: State of the Art</span><span id="bib.bib38.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:80%;">University of Twente, Centre for Telematics and Information
Technology, 2002.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:80%;">
B. I. Murstein and R. G. Brust.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:80%;">Humor and interpersonal attraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Journal of Personality Assessment</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:80%;">, 1985.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:80%;">
S. Petrovic and D. Matthews.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:80%;">Unsupervised joke generation from big data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:80%;">
Plato, E. Hamilton, and H. Cairns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">The Collected Dialogues of Plato, Including the Letters.</span><span id="bib.bib41.3.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:80%;">Pantheon Books, 1961.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:80%;">
B. Plester.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:80%;">Healthy humour: Using humour to cope at work.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">New Zealand Journal of Social Sciences Online</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:80%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:80%;">
D. Radev, A. Stent, J. Tetreault, A. Pappu, A. Iliakopoulou, A. Chanfreau,
P. de Juan, J. Vallmitjana, A. Jaimes, and R. Jha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:80%;">Humor in collective discourse: Unsupervised funniness detection in
the new yorker cartoon caption contest.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1506.08126</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:80%;">
P. Rinck.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:80%;">Magnetic resonance in medicine. the basic textbook of the european
magnetic resonance forum. 8th edition; 2014.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:80%;">
W. Ruch, S. Attardo, and V. Raskin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:80%;">Toward an empirical verification of the general theory of verbal
humor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Humor: International Journal of Humor Research</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:80%;">, 1993.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:80%;">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:80%;">Imagenet large scale visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">International Journal of Computer Vision</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:80%;">
P. Salovey, A. J. Rothman, J. B. Detweiler, and W. T. Steward.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:80%;">Emotional states and physical health.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">American Psychologist</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:80%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:80%;">
A. Salvatore and V. Raskin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:80%;">Script rheory revisited: Joke similarity and joke representation
model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Humor-International Journal of Humor Research</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:80%;">, 1991.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:80%;">
D. Shahaf, E. Horvitz, and R. Mankoff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:80%;">Inside jokes: Identifying humorous cartoon captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">SIGKDD International Conference on Knowledge Discovery and
Data Mining</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:80%;">
G. Sinicropi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:80%;">La struttura della parodia— avvero: Bradamante in arli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Strumenti Critici Torino</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:80%;">, 1981.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:80%;">
O. Stock and C. Strapparava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:80%;">HAHAcronym: A computational humor system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ACL</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:80%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:80%;">
J. M. Suls.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:80%;">A two-stage model for the appreciation of jokes and cartoons: An
information-processing analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">The Psychology of Humor: Theoretical Perspectives and Empirical
Issues</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:80%;">, 1972.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:80%;">
J. Taylor and L. Mazlack.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:80%;">Computationally recognizing wordplay in jokes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of CogSci</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:80%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:80%;">
R. Vedantam, X. Lin, T. Batra, C. L. Zitnick, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:80%;">Learning common sense through visual abstraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:80%;">
W. Y. Wang and M. Wen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:80%;">I can has cheezburger? a nonparanormal approach to combining textual
and visual information for predicting and generating popular meme
descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">NAACL</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:80%;">
M. B. Wanzer, M. Booth-Butterfield, and S. Booth-Butterfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:80%;">Are funny people popular? an examination of humor orientation,
loneliness, and social attraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Communication Quarterly</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:80%;">, 1996.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:80%;">
K. K. Watson, B. J. Matthews, and J. M. Allman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:80%;">Brain activation during sight gags and language-dependent humor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Cerebral Cortex</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:80%;">, 2007.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:80%;">
Wikipedia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:80%;">Humor, November 2015.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:80%;">
Wikipedia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:80%;">Theories of humor, April 2016.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:80%;">
D. Yang, A. Lavie, C. Dyer, and E. Hovy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:80%;">Humor recognition and humor anchor extraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:80%;">2015.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:80%;">
C. L. Zitnick and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:80%;">Bringing semantics into focus using visual abstraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:80%;">
C. L. Zitnick, D. Parikh, and L. Vanderwende.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:80%;">Learning the visual interpretation of sentences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">ICCV</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:80%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:80%;">
C. L. Zitnick, R. Vedantam, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:80%;">Adopting abstract images for semantic scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">PAMI</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:80%;">, 2014.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1512.04406" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1512.04407" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1512.04407">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1512.04407" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1512.04408" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 11:19:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
