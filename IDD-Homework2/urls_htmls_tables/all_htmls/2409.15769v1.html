<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives</title>
<!--Generated on Tue Sep 24 05:43:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Online Art Appreciation,  Anthropomorphic,  Character Design,  Emotional Engagement,  Cognitive Engagement" lang="en" name="keywords"/>
<base href="/html/2409.15769v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S1" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>INTRODUCTION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S2" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S2.SS1" title="In 2. Related Work ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Art Appreciation and Interactive Engagement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S2.SS2" title="In 2. Related Work ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>LLM-enabled Anthropomorphism for Different Role-plays</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S2.SS3" title="In 2. Related Work ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Multiple Characters’ Perspectives in Art Appreciation Context</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S3" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_italic">EyeSee</span> DESIGN AND IMPLEMENTATION</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S3.SS1" title="In 3. EyeSee DESIGN AND IMPLEMENTATION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_bold ltx_font_italic">EyeSee<span class="ltx_text ltx_font_upright"> Interaction Design</span></span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S3.SS2" title="In 3. EyeSee DESIGN AND IMPLEMENTATION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_italic">EyeSee</span> Backend and Implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS1" title="In 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS2" title="In 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiment Materials and Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS3" title="In 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experiment Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS4" title="In 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Measures and Data Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS4.SSS1" title="In 4.4. Measures and Data Analysis ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span><span class="ltx_text ltx_font_bold">Narrative Session</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS4.SSS2" title="In 4.4. Measures and Data Analysis ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span><span class="ltx_text ltx_font_bold">From the Narrative to Recommendation Session</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results of the <span class="ltx_text ltx_font_italic">Narrative Session</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="In 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Overall Perceived Engagement (RQ1a) - Survey 1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2" title="In 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS1" title="In 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span><span class="ltx_text ltx_font_bold">Behavioral Engagement: When Interacting with the <span class="ltx_text" style="color:#00FF00;">In-Situ Mode</span>, Users’ Interaction Time was Longer, and They Asked More Questions Proactively.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS2" title="In 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span><span class="ltx_text ltx_font_bold">Emotional Engagement (EE): <span class="ltx_text" style="color:#00FF00;">In-Situ</span> Yielded the Highest Hedonic Value, Followed by the <span class="ltx_text" style="color:#FF8000;">Artist Mode.</span></span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS3" title="In 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span><span class="ltx_text ltx_font_bold">Cognitive Engagement (CE): <span class="ltx_text ltx_font_italic" style="color:#FF8000;">Artist Mode</span> Yielded the Lowest Epistemic Value, Compared to the <span class="ltx_text ltx_font_italic" style="color:#0000FF;">Narrator Mode</span> and <span class="ltx_text ltx_font_italic" style="color:#00FF00;">In-Situ Mode.</span></span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results across the <span class="ltx_text ltx_font_italic">Narrative</span> and <span class="ltx_text ltx_font_italic">Recommendation</span> <span class="ltx_text ltx_font_italic">Sessions</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="In 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2" title="In 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Across-session Association of Engagement Factors in the <span class="ltx_text ltx_font_italic">Narrative Session</span> (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2.SSS1" title="In 6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span><span class="ltx_text ltx_font_bold">Perceived consistency in the recommendation session was associated with cognitive engagement in the narrative session and stereotypicality was associated with behavioral engagement.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2.SSS2" title="In 6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span><span class="ltx_text ltx_font_bold">Satisfactions with generated recommended images were associated with engagement factors in the <span class="ltx_text ltx_font_italic">Narrative session</span>.</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7" title="In In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>DISCUSSION</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7.SS1" title="In 7. DISCUSSION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>First-Person Perspective: A Double-Edged Sword for User Engagement and Critical Response</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7.SS2" title="In 7. DISCUSSION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Influence of Task Type on Role Perception: Unpacking Consistency and Stereotypes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7.SS3" title="In 7. DISCUSSION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Design Implications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7.SS3.SSS1" title="In 7.3. Design Implications ‣ 7. DISCUSSION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.1 </span><span class="ltx_text ltx_font_bold">Art Appreciation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7.SS3.SSS2" title="In 7.3. Design Implications ‣ 7. DISCUSSION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.2 </span><span class="ltx_text ltx_font_bold">Diverse Educational Settings</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S7.SS4" title="In 7. DISCUSSION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Limitations and Future Research</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yongming Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Xi’an Jiaotong University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Xi’an, Shaanxi</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:lym18733500195@stu.xjtu.edu.cn">lym18733500195@stu.xjtu.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hangyue Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of Illinois Urbana-Champaign</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Champaign, Illinois</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hz85@illinois.edu">hz85@illinois.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrea Yaoyun Cui
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">University of Illinois Urbana-Champaign</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Champaign, Illinois</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yaoyunc2@illinois.edu">yaoyunc2@illinois.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zisong Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of Illinois Urbana-Champaign</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Champaign, Illinois</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zisongm2@illinois.edu">zisongm2@illinois.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunpeng Song
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Xi’an Jiaotong University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Xi’an, Shaanxi</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sypxjtu@gmail.com">sypxjtu@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhongmin Cai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Xi’an Jiaotong University</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Xi’an, Shaanxi</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zmcai@sei.xjtu.edu.cn">zmcai@sei.xjtu.edu.cn</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yun Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">University of Illinois Urbana-Champaign</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Champaign, Illinois</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yunhuang@illinois.edu">yunhuang@illinois.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2025)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id22.id1">Art appreciation serves as a crucial medium for emotional communication and sociocultural dialogue. In the digital era, fostering deep user engagement on online art appreciation platforms remains a challenge. Leveraging generative AI technologies, we present <span class="ltx_text ltx_font_italic" id="id22.id1.1">EyeSee</span>, a system designed to engage users through anthropomorphic characters. We implemented and evaluated three modes– <span class="ltx_text ltx_font_italic" id="id22.id1.2" style="color:#0000FF;">Narrator</span>, <span class="ltx_text ltx_font_italic" id="id22.id1.3" style="color:#FF8000;">Artist</span>, and <span class="ltx_text ltx_font_italic" id="id22.id1.4" style="color:#00FF00;">In-Situ</span>–acting as a third-person narrator, a first-person creator, and first-person created objects, respectively, across two sessions: <span class="ltx_text ltx_font_italic" id="id22.id1.5">Narrative</span> and <span class="ltx_text ltx_font_italic" id="id22.id1.6">Recommendation</span>. We conducted a within-subject study with 24 participants. In the <span class="ltx_text ltx_font_italic" id="id22.id1.7">Narrative session</span>, we found that the <span class="ltx_text ltx_font_italic" id="id22.id1.8">In-Situ</span> and <span class="ltx_text ltx_font_italic" id="id22.id1.9">Artist modes</span> had higher aesthetic appeal than the <span class="ltx_text ltx_font_italic" id="id22.id1.10">Narrator mode</span>, although the <span class="ltx_text ltx_font_italic" id="id22.id1.11">Artist mode </span>showed lower perceived usability. Additionally, from the <span class="ltx_text ltx_font_italic" id="id22.id1.12">Narrative</span> to <span class="ltx_text ltx_font_italic" id="id22.id1.13">Recommendation session</span>, we found that user-perceived relatability and believability within each interaction mode were sustained, but the user-perceived consistency and stereotypicality changed. Our findings suggest novel implications for applying anthropomorphic in-situ narratives to other educational settings.</p>
</div>
<div class="ltx_keywords">Online Art Appreciation, Anthropomorphic, Character Design, Emotional Engagement, Cognitive Engagement
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2025</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>CCS Human-centered computing</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human computer interaction (HCI)</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>HCI design and evaluation methods</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>User studies</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="224" id="S0.F1.g1" src="extracted/5874852/FIGsTABs/Figure/0.0.0ThreeCharacters.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span class="ltx_text ltx_font_typewriter" id="S0.F1.2.1">The <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.1">EyeSee</span> system introduces three interaction modes, designed to enhance art appreciation through anthropomorphic narratives. The <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.2" style="color:#0000FF;">Narrator mode</span> (panel (A)) functions as a third-person storyteller, providing users with contextual information and background about the artwork. The <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.3" style="color:#FF8000;">Artist mode</span> (panel (B)) adopts the perspective of the artwork’s creator, offering insights into the artistic process and motivations. The <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.4" style="color:#00FF00;">In-Situ mode</span> (panel (C)) presents the viewpoint of an object or figure within the artwork, allowing users to engage with the narrative from an internal perspective. Note that no modifications were made to the original artwork; the eyes on the apple in the third panel are included for illustrative purposes only.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S0.F1.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S0.F1.4">The left picture shows a tour guide explaining the artwork to a visitor. In the middle picture, a woman resembling an artist is providing an explanation of the painting to the same visitor. In the picture on the right, there are no other people present except for the visitor and the artwork itself. The objects within the painting come to life and begin explaining the piece to the visitor on their own.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>INTRODUCTION</h2>
<div class="ltx_para" id="S1.p1">
<blockquote class="ltx_quote" id="S1.p1.1">
<p class="ltx_p" id="S1.p1.1.1"><span class="ltx_text ltx_font_italic" id="S1.p1.1.1.1">”Art asks us to think differently, see differently, hear differently, and ultimately to act differently, which is why art has moral force.” –Jeanette Winterson <cite class="ltx_cite ltx_citemacro_citep">(Sherman and Morrissey, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib91" title="">2017</a>)</cite> </span></p>
</blockquote>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Art has long been recognized as a critical medium for expressing emotions, communicating ideas, and reflecting on cultural and personal experiences <cite class="ltx_cite ltx_citemacro_citep">(Dervin and Tian, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib27" title="">2023</a>; Silva, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib95" title="">2006</a>)</cite>. <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Art appreciation</span> entails the viewer’s active engagement, as the meaning and value of visual art are constructed through the interactions and conversations between the artwork, the artist, and the viewer <cite class="ltx_cite ltx_citemacro_citep">(Holub, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib41" title="">2013</a>; Kemp, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib49" title="">1998</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib55" title="">2024</a>)</cite>. The appreciation of visual art makes us think about ‘who we are’, ‘how we interact with others’, and ‘our place in society’, thus it serves a crucial role not only in enriching personal life but also in reflecting societal values and fostering a deeper understanding of cultural differences <cite class="ltx_cite ltx_citemacro_citep">(Smith, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib96" title="">2014</a>)</cite>. Analyzing complex visual arts contributes to intellectual and emotional growth and discussions about art often extend to broader conversations about ethics and values <cite class="ltx_cite ltx_citemacro_citep">(Barnard, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib6" title="">1998</a>; Acharya, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib2" title="">2023</a>)</cite>.
However, engaging people with art in an era dominated by digital experiences remains a challenge. Though traditional online art appreciation platforms have made art more accessible to a broader audience by overcoming geographical limitations and reducing costs through virtual tours <cite class="ltx_cite ltx_citemacro_citep">(Ilić, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib45" title="">2019</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib55" title="">2024</a>)</cite>, they often lack interactivity <cite class="ltx_cite ltx_citemacro_citep">(Sa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib80" title="">2024</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib92" title="">2024</a>)</cite>, resulting in a passive viewing experience that fails to offer the depth of engagement necessary for a profound appreciation of visual art. This gap highlights the importance of enhancing the interactivity and engagement levels of online art platforms <cite class="ltx_cite ltx_citemacro_citep">(Morse et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib71" title="">2022</a>; MacDonald, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib64" title="">2015</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In response to this challenge, the development of generative AI technologies offers new opportunities to enhance user engagement by simulating anthropomorphic characters <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib79" title="">2024</a>; Salminen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib83" title="">2024</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib59" title="">2024b</a>)</cite> and providing more personalized interactive experiences  <cite class="ltx_cite ltx_citemacro_citep">(Tao and Xie, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib98" title="">[n. d.]</a>; Hayashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib37" title="">2024</a>)</cite>. For example, researchers found that LLMs improved the ability of non-experts to discern truth in debates by simulating diverse expert characters <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib50" title="">2024</a>)</cite>, and diverse LLM-simulated student characters helped students practice mathematical modeling skills in educational settings <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib109" title="">2024</a>)</cite>.
In the context of art appreciation, AI-powered chatbots can simulate a third-person docent character to generate reflective questions about paintings, thereby helping users engage more deeply with the artwork <cite class="ltx_cite ltx_citemacro_citep">(Gollapalli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib34" title="">2023</a>)</cite>. Similarly, Lee et al. <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib55" title="">2024</a>)</cite> simulated a personal tutor for art appreciation to provide personalized support and enhance users’ comprehension of the artworks. However, despite these advancements, there is still a need to explore how different character perspectives, such as those of created objects in a painting and the painting’s creator, can promote sustained user engagement, particularly in open art appreciation contexts.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we present <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">EyeSee</span>, designed to address this gap by introducing multi-character interactions that engage users through anthropomorphism. We designed, implemented, and evaluated three distinct modes: the <span class="ltx_text ltx_font_italic" id="S1.p4.1.2" style="color:#0000FF;">Narrator</span>, the <span class="ltx_text ltx_font_italic" id="S1.p4.1.3" style="color:#FF8000;">Artist</span>, and the <span class="ltx_text ltx_font_italic" id="S1.p4.1.4" style="color:#00FF00;">In-Situ</span> modes. The <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">Narrator</span> provides objective explanations of the artwork, the <span class="ltx_text ltx_font_italic" id="S1.p4.1.6">Artist</span> offers insights from the creator’s perspective, and the <span class="ltx_text ltx_font_italic" id="S1.p4.1.7">In-Situ</span> mode enables interaction with the created objects or figures within the artwork. This approach is inspired by “characters built with contextual data”<cite class="ltx_cite ltx_citemacro_citep">(Miaskiewicz and Kozar, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib69" title="">2011</a>)</cite>, which involves creating characters based on contextual information, and the concept of ”thing-centered narratives”<cite class="ltx_cite ltx_citemacro_citep">(Cila et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib20" title="">2015</a>)</cite>, which focuses on narratives centered around objects.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To explore how different interaction modes affect users’ aesthetic appeal and immersive experiences, we conducted a within-subject study with 24 participants, where each participant experienced all three modes, each offering both <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">Recommendation</span> features during interactions. In the <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">Narrative</span> <span class="ltx_text ltx_font_italic" id="S1.p5.1.4">session</span>,
the participants engaged with the narratives about painting objects and we found that both the <span class="ltx_text ltx_font_italic" id="S1.p5.1.5">Artist</span> and <span class="ltx_text ltx_font_italic" id="S1.p5.1.6">In-Situ</span> modes elicited higher aesthetic appeal, which was reflected in increased emotional engagement, particularly related to themes such as time travel, empathy, and anthropomorphism. The use of first-person perspectives in these modes might have contributed to this effect. However, the usability of the <span class="ltx_text ltx_font_italic" id="S1.p5.1.7">Artist</span> mode was perceived as lower, possibly due to higher knowledge expectations and stricter accuracy demands that diminished cognitive engagement. Transitioning from the <span class="ltx_text ltx_font_italic" id="S1.p5.1.8">Narrative</span> to the <span class="ltx_text ltx_font_italic" id="S1.p5.1.9">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S1.p5.1.10">session</span>, the <span class="ltx_text ltx_font_italic" id="S1.p5.1.11">In-Situ</span> mode consistently scored highest in relatability and believability. Moreover, compared to the <span class="ltx_text ltx_font_italic" id="S1.p5.1.12">Narrative</span> <span class="ltx_text ltx_font_italic" id="S1.p5.1.13">session</span>, the <span class="ltx_text ltx_font_italic" id="S1.p5.1.14">Artist</span> mode exhibited improved perceived response consistency during the <span class="ltx_text ltx_font_italic" id="S1.p5.1.15">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S1.p5.1.16">session</span>. This can be attributed to the artist character’s suitability for making contextual recommendations, thus enhancing perceived epistemic value.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our work makes novel and significant contributions to the HCI field. <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">First</span>, our study demonstrates the superior performance of <span class="ltx_text ltx_font_italic" id="S1.p6.1.2">In-Situ</span> design over traditional narrative formats in enhancing user engagement. The <span class="ltx_text ltx_font_italic" id="S1.p6.1.3">In-Situ</span> design excels across various metrics including focused attention, usability, aesthetic appeal, and reward. These improvements underscore the role of anthropomorphism and contextual narratives in fostering deeper connections with art.
<span class="ltx_text ltx_font_italic" id="S1.p6.1.4">Second</span>, our empirical findings reveal that employing multi-perspective interactions, especially through first-person narratives, significantly boosts emotional engagement with visual art, enriching the user’s art appreciation experience.
<span class="ltx_text ltx_font_italic" id="S1.p6.1.5">Third</span>, our study demonstrates different interaction modes and user engagement levels, particularly cognitive and emotional engagement in interactive <span class="ltx_text ltx_font_italic" id="S1.p6.1.6">Narrative session</span>, significantly influence satisfaction of the recommended content and perception of recommendation reasons provided by character (i.e. relatability, and believability).
<span class="ltx_text ltx_font_italic" id="S1.p6.1.7">Fourth</span>, we explore the application of our approach, driven by a Multimodal Large Language Model (MLLM), in educational contexts. Here, multi-perspective strategies could foster more meaningful engagement with arts and other humanities disciplines, thus enhancing educational experiences.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Art Appreciation and Interactive Engagement</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Art appreciation enriches personal life and fosters cultural reflection and sociocultural communication. Engaging with art, even in short art appreciation sessions, has been shown to offer mental and physical benefits, such as reducing stress, improving mood <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib40" title="">2015</a>)</cite>, and lowering cortisol levels <cite class="ltx_cite ltx_citemacro_citep">(Clow and Fredhoi, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib22" title="">2006</a>)</cite> and blood pressure <cite class="ltx_cite ltx_citemacro_citep">(Mastandrea et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib67" title="">2019b</a>)</cite>. Beyond these immediate health benefits, art appreciation allows individuals to immerse themselves in diverse historical contexts and viewpoints, fostering personal reflection and self-expression <cite class="ltx_cite ltx_citemacro_citep">(Belfiore and Bennett, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib7" title="">2007</a>; Sherman and Morrissey, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib91" title="">2017</a>; Durlak et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib28" title="">2011</a>)</cite>. The emotional and cognitive engagement required in art appreciation evokes profound emotional responses and stimulates introspective thought <cite class="ltx_cite ltx_citemacro_citep">(Sherman and Morrissey, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib91" title="">2017</a>)</cite>. Furthermore, at the societal level, art serves as a medium for discourse <cite class="ltx_cite ltx_citemacro_citep">(Dervin and Tian, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib27" title="">2023</a>; Silva, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib95" title="">2006</a>)</cite>. Communities form around shared artistic interests, leading to discussions that extend to broader topics such as ethics, values, and societal norms <cite class="ltx_cite ltx_citemacro_citep">(Gaztambide-Fernández, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib33" title="">2008</a>; Van Maanen, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib101" title="">2009</a>)</cite>. These discussions help foster a deeper understanding of cultural differences and promote inclusivity <cite class="ltx_cite ltx_citemacro_citep">(Cole, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib24" title="">1998</a>; Sherman and Morrissey, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib91" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Digital platforms have transformed the art appreciation process, enabling more profound and contemplative engagement with artworks <cite class="ltx_cite ltx_citemacro_citep">(Walmsley, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib103" title="">2016</a>)</cite>. These platforms have addressed traditional barriers such as geographical limitations and cost by offering virtual tours and high-resolution images of art collections, thus making art more accessible to a broader audience <cite class="ltx_cite ltx_citemacro_citep">(Ilić, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib45" title="">2019</a>)</cite>. Beyond accessibility, digital platforms have contributed significantly to art education. For example, artificial intelligence, particularly deep learning, has been used to assist students in understanding and categorizing artworks, thereby enriching their educational experiences and enjoyment <cite class="ltx_cite ltx_citemacro_citep">(Chiu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib19" title="">2024</a>; Yi, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib108" title="">2022</a>; Hung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib44" title="">2022</a>)</cite>. In the domain of online platforms, interactive engagement plays a crucial role in enhancing the appreciation experience <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib90" title="">2009</a>)</cite>. By integrating elements such as virtual reality and conversation agents, online platforms transform passive viewers into active participants <cite class="ltx_cite ltx_citemacro_citep">(Moon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib70" title="">2024</a>; Scholz and Smith, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib86" title="">2016</a>; Yuen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib110" title="">2011</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib104" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>LLM-enabled Anthropomorphism for Different Role-plays</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Anthropomorphism refers to the psychological phenomenon of “attributing human characteristics to the nonhuman entities” <cite class="ltx_cite ltx_citemacro_citep">(Seeger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib89" title="">2018</a>)</cite>. In AI systems, anthropomorphic design can significantly influence user expectations, trust, and interaction quality<cite class="ltx_cite ltx_citemacro_citep">(Jensen, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib46" title="">2021</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib63" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib106" title="">2024</a>)</cite>.
Design features of anthropomorphic characters are generally categorized into social cues and verbal cues. Social cues, such as the use of text-to-speech voices, have been shown to enhance the perceived anthropomorphism of conversational interfaces compared to text-only interactions <cite class="ltx_cite ltx_citemacro_citep">(Cohn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib23" title="">2024</a>; Moussawi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib72" title="">2021</a>)</cite>. Verbal cues, like the use of first-person pronouns (”I”), have been found to increase perceived information accuracy and reduce risk in specific contexts, such as medication counseling <cite class="ltx_cite ltx_citemacro_citep">(Cohn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib23" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Large Language Models (LLMs) have demonstrated remarkable capabilities in generating anthropomorphism characters to improve user experience.
For example, in the mental health context, Louie et al. <cite class="ltx_cite ltx_citemacro_citep">(Louie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib60" title="">2024a</a>)</cite> simulated patient characters to help novice counselors practice their social skills. They found that a novel principle-adherence prompting pipeline improved response quality and adherence to expert-defined principles by 30%. In education, LLMs have been employed to embody various anthropomorphic characters to enhance the accuracy and professionalism of their generated text <cite class="ltx_cite ltx_citemacro_citep">(Hu and Collier, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib43" title="">2024</a>; Louie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib61" title="">2024b</a>)</cite>. For example, when assigned specific characters like historians or scientists, LLMs produced more precise and domain-specific responses, enhancing both creativity and accuracy <cite class="ltx_cite ltx_citemacro_citep">(Hu and Collier, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib43" title="">2024</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib62" title="">2024</a>)</cite>. Similarly, Arguedas and Daradoumis <cite class="ltx_cite ltx_citemacro_citep">(Arguedas and Daradoumis, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib5" title="">2021</a>)</cite> demonstrated that a pedagogical tutor character providing cognitive and affective feedback positively influenced students’ perceptions by stimulating engagement and guiding learning activities. In the realm of art appreciation, Lee et al. <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib55" title="">2024</a>)</cite> applied LLMs to simulate student and teacher characters, leading to the development of the LLaVA-Docent. This multimodal large language model was designed as a personal tutor for art appreciation, providing interactive, engaging experiences that support deeper learning and engagement with artworks. These studies signify a shift from the traditional, one-size-fits-all generic agent character to more personalized and specialized AI-enabled characters, tailored to enhance user engagement across different domains.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Multiple Characters’ Perspectives in Art Appreciation Context</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The narrative perspective plays a crucial role in how users connect with the anthropomorphic character. Different perspectives can substantially enhance engagement and user experience in narrative contexts <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib17" title="">2024</a>; Chen and Bunescu, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib18" title="">2021</a>)</cite>. While there is ongoing debate among scholars about the fundamental differences between first-person and third-person perspectives <cite class="ltx_cite ltx_citemacro_citep">(Stanzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib97" title="">1984</a>; Kaufman and Libby, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib48" title="">2012</a>)</cite>, both perspectives can enhance user experience and emotional connection in specific contexts. Specifically, third-person perspectives were found helpful in increasing user trust in characters <cite class="ltx_cite ltx_citemacro_citep">(Van Lissa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib100" title="">2016</a>)</cite> and helping readers understand characters’ actions and thoughts <cite class="ltx_cite ltx_citemacro_citep">(Al-Alami, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib3" title="">2019</a>)</cite>. However, because the characters’ thoughts and feelings are described from an anonymous external viewpoint, they seem more distant and abstract to readers <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib51" title="">2020</a>)</cite>. Compared to third-person perspectives, adopting a first-person perspective often elicits greater narrative engagement <cite class="ltx_cite ltx_citemacro_citep">(Busselle and Bilandzic, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib15" title="">2009</a>)</cite>. For example, Salem et al. <cite class="ltx_cite ltx_citemacro_citep">(Salem et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib81" title="">2017</a>)</cite> showed that first-person narration boosts the connection to the protagonist. Another study by Samur et al. <cite class="ltx_cite ltx_citemacro_citep">(Samur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib84" title="">2021</a>)</cite> proved that first-person stories elicit greater narrative engagement compared to third-person stories. Brennan <cite class="ltx_cite ltx_citemacro_citep">(Brennan, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib13" title="">2024</a>)</cite> proposed that writing research articles in the first person made them more engaging, creative, and interesting for readers. Additionally, personal experiences, cultural backgrounds, and emotional states shaped users’ subjective interpretations of art <cite class="ltx_cite ltx_citemacro_citep">(Belfiore and Bennett, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib7" title="">2007</a>)</cite>, all of which emphasized the need to incorporate diverse perspectives <cite class="ltx_cite ltx_citemacro_citep">(Hooper-Greenhill, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib42" title="">2000</a>)</cite> in online platforms integrating. Generative AI present a promising approach to broadening narrative perspectives, thereby enriching users’ engagement and appreciation of art.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span><span class="ltx_text ltx_font_italic" id="S3.1.1">EyeSee</span> DESIGN AND IMPLEMENTATION</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduced <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">EyeSee</span>, a multi-character prototype designed to explore <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">how users perceive and engage with anthropomorphic characters across three modes—<span class="ltx_text ltx_font_italic" id="S3.p1.1.2.1" style="color:#0000FF;">Narrator</span>, <span class="ltx_text ltx_font_italic" id="S3.p1.1.2.2" style="color:#FF8000;">Artist</span>, and <span class="ltx_text ltx_font_italic" id="S3.p1.1.2.3" style="color:#00FF00;">In-Situ</span>—differently, and why</span>. Unlike previous studies that focused on single-perspective chatbots, <span class="ltx_text ltx_font_italic" id="S3.p1.1.3">EyeSee</span> incorporates both first-person and third-person perspectives, providing a more comprehensive analysis of user engagement, pleasure, and knowledge gained. Previous research showed that the first-person perspective is associated with higher emotional and cognitive engagement <cite class="ltx_cite ltx_citemacro_citep">(Van Lissa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib100" title="">2016</a>)</cite>. In particular, first-person narratives have been found to generate emotional engagement and prompt behavioral intent compared to the third-person perspective <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib58" title="">2024a</a>)</cite>. To leverage these benefits, we applied this perspective to two modes: the <span class="ltx_text ltx_font_italic" id="S3.p1.1.4">Artist</span> and the <span class="ltx_text ltx_font_italic" id="S3.p1.1.5">In-Situ</span> modes.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p2.1.1">Artist mode</span> was designed to interpret the artwork from the creator’s perspective. Drawing on Bullot et al. <cite class="ltx_cite ltx_citemacro_citep">(Bullot and Reber, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib14" title="">2013</a>)</cite>, who emphasized that an understanding of the artist’s background and creative motives could enhance viewer appreciation, this mode sought to provide information from the <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">Artist</span> <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">character</span> perspective.
The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p2.1.4">In-Situ mode</span> presented the viewpoint of an object or figure within the artwork, allowing users to engage with the narrative from an internal perspective. This mode was built on Cila et al. <cite class="ltx_cite ltx_citemacro_citep">(Cila et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib20" title="">2015</a>)</cite>, who introduced ”thing-centered narratives,” a concept demonstrating how objects could convey human-like information and offer new viewpoints on familiar practices. Recent work in the HCI field, such as Coskun et al. <cite class="ltx_cite ltx_citemacro_citep">(Coskun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib25" title="">2022</a>)</cite>, also confirmed the potential of super-human design perspectives.
Finally, the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p2.1.5">Narrator mode </span>employed a third-person perspective, similar to traditional museum guides <cite class="ltx_cite ltx_citemacro_citep">(Hein, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib39" title="">2002</a>; Best, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib9" title="">2012</a>)</cite>. This mode provided objective explanations and contextual information about the artwork.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">To reflect common use cases in online art appreciation platforms and to investigate <span class="ltx_text ltx_font_bold" id="S3.p3.1.1">how user perception evolves across different task sessions, and why</span>, we designed two task sessions: the <span class="ltx_text ltx_font_italic" id="S3.p3.1.2">Narrative</span> <span class="ltx_text ltx_font_italic" id="S3.p3.1.3">session</span> and the <span class="ltx_text ltx_font_italic" id="S3.p3.1.4">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S3.p3.1.5">session</span>. Current platforms like Google Arts &amp; Culture and virtual museums primarily relied on static images and textual descriptions, which limited user engagement and interaction <cite class="ltx_cite ltx_citemacro_citep">(Sa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib80" title="">2024</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib92" title="">2024</a>)</cite>. Also, users often faced challenges when using tools like GPT to locate specific objects in artworks. The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p3.1.6">Narrative session</span> allowed users to explore specific areas of interest within artworks. Current art recommendation systems often failed to fully utilize user interaction data and lacked transparency. <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p3.1.7">Recommendation session</span> addressed this by providing personalized recommendations based on minimal user interaction, accompanied by explanations from multiple perspectives.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS1.1.1">EyeSee<span class="ltx_text ltx_font_upright" id="S3.SS1.1.1.1"> Interaction Design</span></span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this session, we present the final version of the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">EyeSee</span> system. The <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">EyeSee</span> system includes one study setup interface and two task interfaces: <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">Narrative</span> interface and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.4">Recommendation</span> interface.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">First, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S3.F2" title="Figure 2 ‣ 3.1. EyeSee Interaction Design ‣ 3. EyeSee DESIGN AND IMPLEMENTATION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">2</span></a>, the components (A1) and (A2) are designed for experiment setup: (A1) allows users to set the AI agent mode, offering a choice between the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">Narrator</span>, the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">Artist</span>, and the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">In-Situ modes</span>, and (A2) provides step-by-step task instructions to help users understand the required actions in two task sessions.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="S3.F2.g1" src="extracted/5874852/FIGsTABs/Figure/4.3.1EyeSeeFrontEndNarrative.png" width="509"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span><span class="ltx_text ltx_font_typewriter ltx_font_italic" id="S3.F2.2.1">Narrative<span class="ltx_text ltx_font_upright" id="S3.F2.2.1.1"> Interface Include (A1) AI Agent Character Selection Area, Including Narrator, Artist, and In-Situ Modes; (A2) Task Instruction Panel; (B1) Area of Interest Selection, with Buttons to Add, Remove, or Reset Selection Areas; (B2) Attention Area Display; (C1) Basic Metadata Information: Name, Style, Artist, and Year; (C2) Shortcuts for Art Appreciation Information: Describe, Describe + Analysis, Describe + Analysis + Interpret, and Judge; (C3) Free Question Query; and (D) Artwork Recommendations.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F2.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F2.4">This screenshot showcases the user interface of the EyeSee narration system while exploring the artwork ”The Ambassadors.” The artwork is positioned in the center of the screen. On the left side, a series of five interactive tasks are listed vertically, each corresponding to different objects within the artwork for user engagement. Above the artwork, a toolbar allows users to select different modes of interaction. Next to this toolbar, there is an additional panel where users can zoom in or out on selected areas or objects within the artwork, or clear their selection. In this case, the lute in the artwork is highlighted in blue, indicating that the user has chosen this object for the current interaction. To the right of the artwork, detailed information about the piece is displayed, including the title, artist, year, and style. Below this information, several vertically arranged buttons allow users to perform actions such as describing, analyzing, or interpreting specific elements of the painting. In this screenshot, the “Describe” mode is active, as indicated by the selection in the system, and the system’s response regarding the lute is shown in the chatbox on the far right, providing information about the object’s historical significance and design.</p>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="252" id="S3.F3.g1" src="extracted/5874852/FIGsTABs/Figure/4.3.2EyeSeeFrontEndRemmondation.png" width="509"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span><span class="ltx_text ltx_font_typewriter ltx_font_italic" id="S3.F3.2.1">Recommendation<span class="ltx_text ltx_font_upright" id="S3.F3.2.1.1"> Interface Include (D1) Recommended Artwork Display; (D2) Rating for Recommended Artworks; (E1) Recommendation Reasons; and (E2) Rating for Recommendation Reasons.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F3.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F3.4">This screenshot showcases the user interface of the EyeSee recommendation system after the user selected the lute in the previous artwork, The Ambassadors. As a result, the system generated recommendations, and the user is currently viewing the first recommended artwork, The Lute Player by Caravaggio, which is displayed prominently in the center of the screen. On the left side (D1), there are four thumbnail images representing the recommended artworks. The top two thumbnails are based on object similarity, specifically featuring a lute, while the bottom two are holistic recommendations, suggesting artworks that share a similar overall style to The Ambassadors. The user can click on any of these thumbnails to view them in detail. On the right side (E1), the system presents the reasons for recommending The Lute Player, while below (E2), the user is invited to rate the reasons for the recommendation on a scale of 1 to 7. Similarly, at the bottom left (D2), the user is prompted to rate the recommended artwork itself.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Second, the components (B1) and (B2) are designed for the attention area selection. In the B1 area, the three buttons function as follows: ”Add Area” allows users to add clicked areas to the selected attention area, ”Remove Area” subtracts clicked areas from the selected attention area, and ”Reset” clears all attention areas. In the B2, users can upload paintings and see the selected attention areas displayed in real-time. The components (C1), (C2), and (C3) are designed for information type selection. C1 provides the name, style, artist, and year of the artwork. C2 draws on
Feldman’s Model of Art Criticism <cite class="ltx_cite ltx_citemacro_citep">(Feldman, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib30" title="">1994</a>)</cite> to offer shortcuts about the description, analysis, interpretation, and judgment of selected attention areas. C3 allows users to ask questions freely. The system offers the above information by interactive dialogue based on the selected character. The component D guides users to the EyeSee recommendation interface, where personalized artwork recommendations are provided based on the user’s selected attention areas and preferences.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Third, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S3.F3" title="Figure 3 ‣ 3.1. EyeSee Interaction Design ‣ 3. EyeSee DESIGN AND IMPLEMENTATION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">3</span></a>, the components (D1) and (D2) are designed for displaying and evaluating recommended artworks. D1 provides four personalized recommendations:
the first two paintings are recommended based on the user’s interest in the painting style, while the last two are based on the user’s selected areas of interest during the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">Narrative</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">session</span>. Users can click on paintings to preview them. D2 is used to collect satisfaction ratings for the recommended paintings. In the deployed system, D2 enables users to bookmark paintings that interest them, making it easy to revisit the collection. The components (E1) and (E2) are designed for displaying and evaluating the reasons behind the recommendations. E1 displays the recommendation reason when users click on an image in the preview area. The reason is based on the relationship between the recommended artwork and the original artwork using the same character perspective (<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.3">Narrator, Artist, or In-Situ</span>) as in the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.4">Narrative</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.5">session</span>. E2 allows users to rate the recommendation reasons on a scale from 1 to 7, where 1 represents “very dissatisfied” and 7 represents “very satisfied”.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">EyeSee</span> Backend and Implementation</h3>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="276" id="S3.F4.g1" src="extracted/5874852/FIGsTABs/Figure/4.3.3EyeSeeBackEnd.png" width="509"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span class="ltx_text ltx_font_typewriter" id="S3.F4.2.1">EyeSee framework include: (F1) Visual Thinking Strategies Method; (F2) Chain of Thought Method; (G1) Style-based Pipeline; (G2)Object-based Pipeline.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F4.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F4.4">This flowchart illustrates the backend process of the EyeSee system. The process begins when the user selects an area of interest in the artwork. The system, using the VTS Method (F1), identifies the selected object, such as the lute, and queries the visual art knowledge base through GPT-4. The system generates a narrative output based on the selected object, reflecting its historical and artistic context. Simultaneously, the system applies both a Style-based Pipeline (G1) and an Object-based Pipeline (G2), using machine learning models (like FAISS) to recommend artworks similar in style or containing similar objects. The recommendations are then processed through the CoT Method (F2), which combines object-relevant and recommendation information to generate personalized outputs that provide both narrative and recommendation insights for the user.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The backend of the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">EyeSee</span> system utilizes three modes– <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Narrator</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">Artist</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4">In-Situ</span>–to customize MLLM-based agents for generating narratives and recommendations based on users’ areas of interest. Two major functions of the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.5">EyeSee</span> backend include: 1) the narrative generation ability of the MLLM agent and 2) the artworks retrieval module that supports artwork recommendation. The framework of <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.6">EyeSee</span> system is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S3.F4" title="Figure 4 ‣ 3.2. EyeSee Backend and Implementation ‣ 3. EyeSee DESIGN AND IMPLEMENTATION ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Generating Narrative with MLLM-based Agent.</span>
The <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">EyeSee</span> system leverages the Visual Thinking Strategies (VTS) method (F1) to build the visual art knowledge base and the Chain of Thought (CoT) method (F2) to customize the MLLM-based agent to different characters for generating narratives. Developed at the Museum of Modern Art in New York City, Visual Thinking Strategies <cite class="ltx_cite ltx_citemacro_citep">(Yenawine, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib107" title="">2013</a>)</cite> have been widely adopted in art appreciation education for facilitating participants to express their interpretations of the artwork. This method guides the GPT-4o model in generating a visual art knowledge base. This method relies on three open-ended questions, that is ”What’s going on in this picture?”, ”What do you see that makes you say that?”, and ”What more can you find?”, to build the visual art knowledge base.
Secondly, drawing upon the proven efficacy of chain-of-thought’s applications in diverse fields <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib52" title="">2023</a>; Si et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib94" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib31" title="">2024</a>)</cite>, the <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.3">EyeSee</span> system incorporates the Chain of Thought method to generate character-based narratives through intermediate steps. This process involves three steps: (1) extracting and identifying the objects based on the Segment Anything Model <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib53" title="">2023</a>)</cite> (a groundbreaking image segmentation algorithm) and labels (some named entities extracted from the knowledge base); (2) extracting relevant statements from the knowledge base based on the object’s name and information types; (3) transforming these relevant statements into narratives based on the perspective of the chosen character. Examples of prompts used to generate narratives are available in the Appendix.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Related Artworks Retrieval.</span>
The <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">EyeSee</span> system employs two retrieval pipelines to recommend existing artworks based on the user’s interest in painting styles and selected objects.
First, the style-based retrieval pipeline (G1) is designed to recommend artworks that match the users’ preferred painting style by comparing the similarity of the image feature vector. The <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.3">EyeSee</span> system utilizes the Siglip model (siglip-base-patch16-224) <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib111" title="">2023</a>)</cite> to extract features from the painting, with these vectors capturing the essence of the artwork’s style. Then, the system uses FAISS <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib47" title="">2019</a>)</cite>, a fast similarity search tool, to retrieve two paintings with the most similar features from the Wikiart datasets <cite class="ltx_cite ltx_citemacro_citep">(Danielczuk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib26" title="">2019</a>)</cite>, ensuring the recommendations align with the user’s stylistic preferences.
Second, the object-based retrieval pipeline (G2) recommends artworks containing user-selected objects using image segmentation and recognition models. The system follows the object detection pipeline proposed by Louie Meyer et al. <cite class="ltx_cite ltx_citemacro_citep">(Meyer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib68" title="">2024</a>)</cite>. with the Wikiart datasets are annotated using 13 categories and 120 labels. Specifically, during the data annotation process, the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.4">EyeSee</span> system utilizes the pre-trained GLIP model (glip-tiny-model-o365-goldg) <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib57" title="">2022</a>)</cite> to compare the vector representation of an object label with the vectors extracted from image patches, matching the most similar ones and generating labeled bounding boxes within the painting. Then, the system uses the Segment Anything Model <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib53" title="">2023</a>)</cite> to extract objects from paintings. It then applies FAISS <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib47" title="">2019</a>)</cite> to compare the vector representation of the selected object with those in the Wikiart dataset <cite class="ltx_cite ltx_citemacro_citep">(Danielczuk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib26" title="">2019</a>)</cite>, retrieving two similar paintings based on similarity scores. Additionally, the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.5">EyeSee</span> system also incorporates the Chain of Thought method into recommendation reason generation: (1) extracting and identifying the object; (2) analyzing the recommendation reasons based on two paintings’ knowledge base; (3) transforming these relevant statements into recommendation reasons.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Iterative Design and Implementation.</span> The design and implementation of the <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.2">EyeSee</span> system were carried out by three authors of this paper between March to June 2024, following the Action Design Research <cite class="ltx_cite ltx_citemacro_citep">(Hansen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib36" title="">2019</a>; Mullarkey and Hevner, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib73" title="">2019</a>)</cite>, which includes diagnosis, design, implementation, and evolution phases. The system underwent five iterative design cycles, each guided by usability testing and technical considerations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Method</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Participants in the Experience</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:390.3pt;height:309.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-103.2pt,81.9pt) scale(0.654180045618565,0.654180045618565) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.2.1">Gender</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.3.1">Age</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.4.1">Occupation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.5.1.1.1.1">Interested</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.5.1.2.1.1">in Art</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.1.6.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.1.1.6.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.6.1.1.1.1">How often</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.1.1.6.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.6.1.2.1.1">visit Museums</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.1.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.1.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.1.1.7.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.7.1.1.1.1">Art as</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.1.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.1.1.7.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.7.1.2.1.1">Profession or Hobby</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.2.1.1">P1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.2.1.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.2.1.3">18</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.4">High school student</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.5">Interested</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.6">Once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.3.2.1">P2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.3.2.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.3.2.3">18</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.4">High school student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.3.2.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.4.3.1">P3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.4.3.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.4.3.3">24</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.4">Undergrad student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.4.3.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.5.4.1">P4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.5.4.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.5.4.3">23</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.4.4">Undergrad student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.4.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.4.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.5.4.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.6.5.1">P5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.6.5.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.6.5.3">32</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.5.4">AI industry practitioners</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.5.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.5.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.6.5.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.7.6.1">P6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.7.6.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.7.6.3">25</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.6.4">Entrepreneur</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.6.5">Extremely interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.6.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.7.6.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.8.7.1">P7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.8.7.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.8.7.3">26</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.7.4">Art teacher</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.7.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.7.6">Once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.8.7.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.9.8.1">P8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.9.8.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.9.8.3">24</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.8.4">Undergrad student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.8.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.8.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.9.8.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.10.9.1">P9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.10.9.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.10.9.3">28</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.9.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.9.5">Extremely interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.9.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.10.9.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.11.10.1">P10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.11.10.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.11.10.3">24</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.10.4">Financial practitioner</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.10.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.10.6">Once a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.11.10.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.12.11.1">P11</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.12.11.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.12.11.3">22</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.11.4">Undergrad student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.11.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.11.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.12.11.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.13.12.1">P12</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.13.12.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.13.12.3">23</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.12.4">Art archaeology researcher</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.12.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.12.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.13.12.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.14.13.1">P13</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.14.13.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.14.13.3">24</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.13.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.13.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.13.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.14.13.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.15.14.1">P14</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.15.14.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.15.14.3">22</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.15.14.4">Freelance artist</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.15.14.5">Extremely interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.15.14.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.15.14.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.16.15.1">P15</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.16.15.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.16.15.3">20</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.15.4">Undergrad student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.15.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.15.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.16.15.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.17.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.17.16.1">P16</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.17.16.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.17.16.3">25</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.17.16.4">Museum staff member</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.17.16.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.17.16.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.17.16.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.18.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.18.17.1">P17</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.18.17.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.18.17.3">31</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.18.17.4">Graphic Designer</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.18.17.5">Extremely interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.18.17.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.18.17.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.19.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.19.18.1">P18</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.19.18.2">Non-binary</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.19.18.3">25</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.18.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.18.5">Extremely interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.18.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.19.18.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.20.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.20.19.1">P19</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.20.19.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.20.19.3">24</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.20.19.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.20.19.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.20.19.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.20.19.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.21.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.21.20.1">P20</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.21.20.2">Prefer not to say</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.21.20.3">23</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.20.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.20.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.20.6">More than once a month</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.21.20.7">Profession</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.22.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.22.21.1">P21</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.22.21.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.22.21.3">26</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.21.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.21.5">Interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.21.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.22.21.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.23.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.23.22.1">P22</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.23.22.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.23.22.3">23</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.22.4">Painter</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.22.5">Very interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.22.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.23.22.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.24.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.24.23.1">P23</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.24.23.2">Female</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.24.23.3">27</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.23.4">Master’s student</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.23.5">Somewhat interested</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.23.6">Once a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.24.23.7">Hobby</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.25.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.1.25.24.1">P24</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.1.25.24.2">Male</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.1.25.24.3">34</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.25.24.4">Art teacher</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.25.24.5">Very interested</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.25.24.6">Several times a year</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.1.1.25.24.7">Profession</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="213" id="S4.F5.g1" src="extracted/5874852/FIGsTABs/Figure/5.2.1Materials.png" width="538"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span><span class="ltx_text ltx_font_typewriter" id="S4.F5.2.1">Experiment Materials</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F5.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F5.4">The diagram illustrates experiment materials including three paintings, organized from left to right: (1) <span class="ltx_text ltx_font_italic" id="S4.F5.4.1">The Ambassadors</span> (Hans Holbein the Younger, 1533, Northern Renaissance); (2) <span class="ltx_text ltx_font_italic" id="S4.F5.4.2">Football Player</span> (Albert Gleizes, 1912-1913, Cubism); and (3) <span class="ltx_text ltx_font_italic" id="S4.F5.4.3">Along the River During the Qingming Festival</span> (Zhang Zeduan, Song Dynasty, Chinese Landscape Painting).</p>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="296" id="S4.F6.g1" src="extracted/5874852/FIGsTABs/Figure/5.3.1EyeSeeStudyProcess.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span><span class="ltx_text ltx_font_typewriter" id="S4.F6.2.1">Experiment Procedure Includes Onboarding Session, <span class="ltx_text ltx_font_italic" id="S4.F6.2.1.1">Narrative Session</span>, and <span class="ltx_text ltx_font_italic" id="S4.F6.2.1.2">Recommendation Session</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F6.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F6.4">This flowchart outlines the process of using the EyeSee system, starting with the Onboarding phase, where users go through an introduction, training, and a pre-survey, and are then randomly assigned to one of three modes: Narrator, Artist, or In-Situ. In the Narrative Session, users interact with four objects, performing tasks like describing, analyzing, interpreting, and answering questions. Following this, they rate their satisfaction and complete a survey. In the Recommendation Session, users are recommended artworks and given rationales, after which they again rate satisfaction and complete a survey. Finally, the Mode Switch phase allows users to switch modes randomly until all three modes have been explored, after which the process ends.</p>
</div>
</div>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Participants</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.T1" title="Table 1 ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">1</span></a>, 24 participants participated in the experiment (age Mean = 23.63, SD = 3.84; 14 identified as female, 8 as male, and 2 as other). Participants were recruited through electronic flyers and emails, using snowball sampling to target individuals with a demonstrated interest in art appreciation, as art enthusiasts were more likely to provide meaningful and insightful feedback. The participants represented a diverse range of backgrounds, including 5 graduate students, 11 undergraduates, 2 recent high school graduates, 2 art teachers, 1 freelance artist, 1 graphic designer, 1 art archaeology researcher, and 1 museum staff member. The experiment received approval from the Institutional Review Board (IRB) at the researchers’ institution. All participants signed the informed consent form. and were compensated at a rate of US$20 per hour upon completing the experiment.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Experiment Materials and Tools</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.F5" title="Figure 5 ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5</span></a>, we used the three paintings in the experiment: (1) <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">The Ambassadors</span> (Hans Holbein the Younger, 1533, Realism); (2) <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">Football Player</span> (Albert Gleizes, 1912-1913, Cubism); and (3) <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">Along the River During the Qingming Festival </span>(Zhang Zeduan, Song Dynasty, Chinese Landscape Painting). The paintings were chosen to ensure the experiment materials include various styles and periods. Additionally, we selected four objects in each painting: two human figures and two non-human objects for the object-based interaction. These paintings were obtained from three online museum websites<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.nationalgallery.org.uk/</span></span></span> <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.nga.gov/</span></span></span> <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.comuseum.com/</span></span></span>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The experiment was conducted remotely using the following tools: (1) a laptop, a mouse, and earphones when the participant visited the online <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">EyeSee</span> system. (2) a PC and remote meeting recording software used to collect participant’s interaction logs and feedback. In addition, the scales used in the experiment were created in Qualtrics, and the collected data were analyzed in Python.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Experiment Procedure</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.F6" title="Figure 6 ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6</span></a>, the experiment procedure consists three sessions: (1) an onboarding session where participants received instructions, familiarized themselves with the prototype, and completed pre-survey; (2) a <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Narrative session</span> where participants interacted with the three modes of <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">EyeSee</span> to obtain information about the selected objects and evaluate engagement during the narrative task; (3) a <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.3">Recommendation session</span> where participants rated the recommended artworks and recommendation reasons. Participants were allowed to pause the experiment and take breaks between sessions as needed. The average duration of the experiment was 90 minutes.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To avoid the effect of the chronological order of the experiment modes on the engagement results, we implemented a counterbalanced design <cite class="ltx_cite ltx_citemacro_citep">(Pollatsek and Well, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib77" title="">1995</a>)</cite>. Each participant completed the <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">Recommendation sessions</span> three paintings, with each mode (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.3" style="color:#0000FF;">Narrator Mode</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.4" style="color:#FF8000;">Artist Mode</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.5" style="color:#00FF00;">In-Situ Mode</span>) linking to a different painting in a randomly assigned order. The counterbalanced design ensured that all possible orders were evenly distributed.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Onboarding.</span>
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.F6" title="Figure 6 ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6</span></a>, in the onboarding session, we first offered a brief introduction about the experiment and the three sessions involved. We then provided participants with training on how to use the prototype. They practiced with an example image and were encouraged to ask questions at any time until they became proficient with the prototype. After that, they completed a pre-survey about demographics and artistic backgrounds.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS3.p4.1.1">Narrative Session<span class="ltx_text ltx_font_upright" id="S4.SS3.p4.1.1.1">.</span></span>
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.F6" title="Figure 6 ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6</span></a>, participants completed the interactive narrative tasks for four objects under each assigned mode. For each object, participants selected the object, chose the information types, and evaluated the narrative result by think-aloud<cite class="ltx_cite ltx_citemacro_citep">(Van Someren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib102" title="">1994</a>)</cite>. Think-aloud is a method where participants verbalize their thoughts and reasoning while performing a task, providing insights into their decision-making process. For the first and third objects, participants could choose four information types (Description, Description + Analysis, Description + Analysis + Interpretation, or Question). For the second and fourth objects, participants chose from five information types, with ”Judge” as a mandatory option. This allowed us to simulate scenarios with different information requirements. After completing the interactive narrative task in each assigned mode, they filled out the user engagement scale, followed by the hedonic value scale, epistemic value scale, and perceived character evaluation scale, and then participated in an interview. These measures were used to evaluate their perception of the system and characters, as detailed in the Session <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.SS4" title="4.4. Measures and Data Analysis ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS3.p5.1.1">Recommendation Session<span class="ltx_text ltx_font_upright" id="S4.SS3.p5.1.1.1">.</span></span> As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S4.F6" title="Figure 6 ‣ 4. Method ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6</span></a>, after completing the <span class="ltx_text ltx_font_italic" id="S4.SS3.p5.1.2">Narrative</span> session in the assigned mode, participants proceeded to the <span class="ltx_text ltx_font_italic" id="S4.SS3.p5.1.3">Recommendation session</span>, maintaining the same mode used in the <span class="ltx_text ltx_font_italic" id="S4.SS3.p5.1.4">Narrative session</span>. This session consists of the recommended painting evaluation, recommendation reason evaluation, and character evaluation. First, the character provided two style-oriented recommendations and two object-oriented recommendations and participants rated their satisfaction with each of the four recommended paintings. After clicking each painting, the character provided the reasons for its recommendations. Participants then rated their satisfaction with each recommendation reason and provided a verbal explanation for their ratings by think-aloud. After the satisfaction rating tasks, they filled out the perceived character evaluation scale, specifically for the recommendation task.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Measures and Data Analysis</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.1.1">Narrative Session</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">In the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p1.1.1">Narrative session</span>, we used the UES-SF (User Engagement Scale-Short Form) <cite class="ltx_cite ltx_citemacro_citep">(O’Brien et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib75" title="">2018</a>)</cite> to analyze how participants <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p1.1.2">perceived</span> the overall engagement level when using different modes of the EyeSee system. This 5-point Likert scale was commonly used to measure the overall perceived engagement across various digital contexts <cite class="ltx_cite ltx_citemacro_citep">(Bitrián et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib11" title="">2021</a>; Gabrielli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib32" title="">2021</a>; Ciotoli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib21" title="">2021</a>)</cite> and was chosen for its efficiency in reducing survey completion time. This scale was shortened from 31 items to 12 items, comprising Focused Attention (3 items), Perceived Usability (3 items), Aesthetic Appeal (3 items), and Reward Factor (3 items that includes Endurability, Novelty, and Felt Involvement components from the original UES). The complete survey is provided in Appendix A.
We also collected the interaction logs, user-perceived hedonic and epistemic value (5-point Likert scale) <cite class="ltx_cite ltx_citemacro_citep">(Ponsignon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib78" title="">2024</a>)</cite>, and think-aloud data to understand how participants <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p1.1.3">engaged</span> with the different characters on the EyeSee system.
First, following previous research <cite class="ltx_cite ltx_citemacro_citep">(Ben-Eliyahu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib8" title="">2018</a>)</cite>, we collected the task complete time, interaction counts by information types, and satisfaction rating to analyze users’ behavioral engagement.
Second, emotional engagement (hedonic value) was measured using statements like “I had fun with this character” and “This experience was entertaining”. Cognitive engagement (epistemic value) was assessed using statements such as “I learned a lot from this character” and “It was a real learning experience”. The think-aloud data during the recommendation task provided deeper insight into the participants’ emotional and cognitive engagement <cite class="ltx_cite ltx_citemacro_citep">(Ponsignon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib78" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">After collecting survey and user logs, we performed a one-way ANOVA with a randomized effect to compare the effect of character mode on (1) the overall perceived engagement (2) behavioral engagement based on interaction logs (3) emotional and cognitive engagement based on hedonic and epistemic value survey. A random effect “1/PID” (Participant ID) was included to account for individual differences that could not be explained by the fixed effects in the model <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib16" title="">2023</a>)</cite>. Post hoc analyses were conducted for pairwise comparisons between the modes. The assumptions for using ANOVA, including normality, homogeneity of variances, and independence of observations, were tested and met. The think-aloud data were analyzed using thematic analysis <cite class="ltx_cite ltx_citemacro_citep">(Braun and Clarke, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib12" title="">2006</a>)</cite> to provide insight into emotional and cognitive engagement. The analysis involved several rounds of coding, where three researchers independently reviewed the data and labeled specific segments. After comparison and discussion, they consolidated the different codes into potential overarching themes related to the pros and cons of emotional and cognitive engagement in the three modes. Finally, the researchers independently assigned the final codes to the think-aloud data. Any disagreements during this process were resolved through discussions to ensure consensus. The occurrences of codes were counted and qualitative findings were incorporated in the Section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS2" title="5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS3" title="5.2.3. Cognitive Engagement (CE): Artist Mode Yielded the Lowest Epistemic Value, Compared to the Narrator Mode and In-Situ Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.1.1">From the Narrative to Recommendation Session</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">To analyze the across-session <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p1.1.1">evolution</span> of the character perceptions from the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.2">Narrative</span> to the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.3">Recommendation session</span>, we collect the character perceptions based on  <cite class="ltx_cite ltx_citemacro_citep">(Salminen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib83" title="">2024</a>)</cite>, including (a) consistency, (b) relatability, (c) believability, and (d)stereotypicality.
In the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.4">Recommendation session</span>, in addition to the character perception scale, we used a 7-point Likert scale to collect satisfaction of recommended paintings and recommendation reasons.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">We performed regression analysis to examine how perceptions in the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.1">Narrative session</span> influenced the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.2">Recommendation session</span>. Additionally, we conducted a one-way ANOVA with a randomized effect to analyze the character perception across three modes in two sessions and performed two linear regression analyses to investigate which engagement factors in the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.3">Narrative session</span> were <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p2.1.4">associated</span> with improved satisfaction and perceptions in the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.5">Recommendation session</span>.
Specifically, the dependent variables in two sets of regressions were: 1) participants’ ratings for art recommendations (i.e., satisfaction ratings of recommended images and recommendation reasons); 2) participants’ survey scores on their character perceptions (i.e., consistency, relatability, believability, and stereotypically).
These analyses examined the relationship between the users’ behavioral, emotional, and cognitive engagement in the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.6">Narrative session</span> and their perception of characters and outcomes satisfaction of the recommendations in the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.7">Recommendation session</span>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results of the <span class="ltx_text ltx_font_italic" id="S5.1.1">Narrative Session</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">[RQ1]</span> How do users <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.p1.1.2">perceive</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.p1.1.3">engage</span> with the anthropomorphic characters in three modes-<span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p1.1.4">Narrator</span>, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p1.1.5">Artist</span>, and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p1.1.6">In-Situ</span>-differently in the <span class="ltx_text ltx_font_italic" id="S5.p1.1.7">Narrative</span> <span class="ltx_text ltx_font_italic" id="S5.p1.1.8">session</span>?</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Sections <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2" title="5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2</span></a> assessed RQ1. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.1</span></a> examined users’ overall perceived engagement levels in the three modes of the <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">EyeSee</span> system. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2" title="5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2</span></a> further explained the results of <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.1</span></a> by interpreting the engagement levels through the lenses of behavioral, emotional, and cognitive engagement during user interactions. In these sections, three colors will be used in figures and tables to differentiate the three modes: <span class="ltx_text ltx_font_italic" id="S5.p2.1.2" style="color:#0000FF;">Narrator mode<span class="ltx_text ltx_font_upright" id="S5.p2.1.2.1"> (blue)</span></span>, <span class="ltx_text ltx_font_italic" id="S5.p2.1.3" style="color:#FF8000;">Artist mode<span class="ltx_text ltx_font_upright" id="S5.p2.1.3.1"> (orange)</span></span>, and <span class="ltx_text ltx_font_italic" id="S5.p2.1.4" style="color:#00FF00;">In-Situ mode<span class="ltx_text ltx_font_upright" id="S5.p2.1.4.1"> (green)</span></span>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Overall Perceived Engagement (RQ1a) - Survey 1</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">[RQ1a]</span> How do users <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS1.p1.1.2">perceive</span> the overall engagement level when using three modes in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">Narrative</span> <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">session</span>?</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="231" id="S5.F7.g1" src="extracted/5874852/FIGsTABs/Figure/6.1.1Engagement.png" width="538"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span><span class="ltx_text ltx_font_typewriter" id="S5.F7.2.1">Survey Results: Overall Perceived Engagement Level</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F7.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F7.4">Bar charts showing the mean scores for Focused Attention, Perceived Usability, Aesthetic Appeal, and Reward Factor across three modes (Narrator, Artist, In-Situ), with error bars indicating variability and significant differences marked by asterisks, consistently reveal In-Situ characters as the most engaging across all metrics.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">Narrative</span> <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">session</span>, participants were asked to rank their engagement level while interacting with three different characters in three modes of the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.3">EyeSee</span> system. The majority (75%) selected the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.4">In-Situ mode</span> as the most engaging, 25% chose the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.5">Narrator mode</span> as the most engaging, and 87.5% considered the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.6">Artist mode</span> as the least engaging.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F7" title="Figure 7 ‣ 5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">7</span></a>, we conducted a one-way ANOVA to assess the overall perceived engagement differences across the three modes, followed by post hoc analyses for pairwise comparisons.
The overall perceived engagement consists of (a) focused attention, (b) perceived usability, (c) aesthetic appeal, and (d) reward factor. Based on the comparison analysis of the survey, significant differences in the user perception of system-related engagement across the three modes were observed.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Focused Attention:</span> There was a significant difference in the users’ perception of focused attention when interacting with <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">EyeSee</span> across the three modes (<span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.3">F</span>(2,51)=11.88, <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.4">p</span>¡.001).
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F7" title="Figure 7 ‣ 5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">7</span></a> (a), participants reported the highest focused attention when interacting with characters in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.5">In-Situ mode</span> (M=4.31, SD=0.64); the focused attention in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.6">Narrator mode</span> (M=3.57, SD=0.68) was slightly higher than that in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.7">Artist mode</span> (M=3.35, SD=0.80). The post hoc analysis revealed that participants exhibited significantly higher focused attention in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.8">In-Situ mode</span> than in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.9">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.10">Artist mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.11">p</span>¡.01 and <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.12">p</span>¡.001 respectively), but there was no significant difference in the focused attention level between the <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.13">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.14">Artist mode</span>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Perceived Usability:</span>
There was a significant difference in the users’ perception of usability across the three modes (<span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.2">F</span>(2,51)=14.32, <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.3">p</span>¡.001).
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F7" title="Figure 7 ‣ 5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">7</span></a> (b), participants reported the highest perceived usability with the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.4">In-Situ mode</span> (M=4.68, SD=0.39) compared to the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.5">Narrator mode</span> (M= 4.13, SD=0.72) and the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.6">Artist mode</span> (M=3.51, SD=1.02). A post hoc analysis showed that participants perceived significantly higher usability in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.7">In-Situ mode</span> than the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.8">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.9">Artist mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.10">p</span>¡.05 and <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.11">p</span>¡.001 respectively). Additionally, participants perceived higher usability in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.12">Narrator mode</span> than the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.13">Artist mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.14">p</span>¡.05).</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">Aesthetic Appeal:</span> There was a significant difference in the users’ perception of aesthetic appeal across the three modes (<span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.2">F</span>(2,51)=12.24, <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.3">p</span>¡.001).
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F7" title="Figure 7 ‣ 5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">7</span></a> (c), the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.4">In-Situ mode</span> (M=4.17, SD=0.65) showed the greatest aesthetic appeal, followed by the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.5">Artist mode</span> (M=3.63, SD=0.69), and the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.6">Narrator mode</span> (M=3.07, SD=0.93) showed the lowest aesthetic appeal. The post hoc analysis showed that the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.7">In-Situ mode</span> had significantly greater aesthetic appeal than the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.8">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.9">Artist mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.10">p</span>¡.001 and <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.11">p</span>¡.05 respectively), and the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.12">Artist mode</span> had significantly greater aesthetic appeal than the <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.13">Narrator mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.14">p</span>¡.05).</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p6.1.1">Reward Factor:</span>
There was a significant difference in the users’ perception of reward factor (<span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.2">F</span>(2,51)=9.54, <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.3">p</span>¡.001).
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F7" title="Figure 7 ‣ 5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">7</span></a> (d), the participants reported the highest reward factor when interacting with the characters in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.4">In-Situ mode</span> (M=4.56, SD=0.51) on the 5-point Likert scales. The scale included questions assessing whether the experience was ”<span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.5">worthwhile</span>”, ”<span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.6">rewarding</span>”, and ”<span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.7">enjoyable</span>”. The post hoc analysis showed that participants perceived a significantly higher reward factor in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.8">In-Situ mode</span> compared to the <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.9">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.10">Artist mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.11">p</span>¡.01 and <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.12">p</span>¡.001 respectively), but there was no significant difference in the reward factor between the <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.13">Narrator mode</span> (M=3.99, SD=0.53) and <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.14">Artist mode</span> (M=3.90, SD=0.64).</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p7.1.1">Summary-RQ1a:</span>
According to Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F7" title="Figure 7 ‣ 5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">7</span></a>, the <span class="ltx_text ltx_font_italic" id="S5.SS1.p7.1.2" style="color:#00FF00;">In-Situ mode</span> was rated the best user engagement across the four engagement dimensions in terms of focused attention, perceived usability, aesthetic appeal, and reward factor. Between the <span class="ltx_text ltx_font_italic" id="S5.SS1.p7.1.3" style="color:#FF8000;">Artist mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p7.1.4" style="color:#0000FF;">Narrator mode</span>, an interesting contrast was found: the <span class="ltx_text ltx_font_italic" id="S5.SS1.p7.1.5" style="color:#FF8000;">Artist mode</span> had higher aesthetic appeal but was perceived to have lower usability.
Next, Section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2" title="5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2</span></a> will explain the potential rationale for these results from the behavioral, emotional, and cognitive engagement perspectives.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">[RQ1b]</span> How do users <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS2.p1.1.2">engage</span> with different characters in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.3">Narrative</span> <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.4">session</span>?</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.1.1">Behavioral Engagement: When Interacting with the <span class="ltx_text" id="S5.SS2.SSS1.1.1.1" style="color:#00FF00;">In-Situ Mode</span>, Users’ Interaction Time was Longer, and They Asked More Questions Proactively.</span>
</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.T2" title="Table 2 ‣ 5.2.1. Behavioral Engagement: When Interacting with the In-Situ Mode, Users’ Interaction Time was Longer, and They Asked More Questions Proactively. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">2</span></a>, we investigated the differences in participants’ behavioral engagement across the three modes in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.1">Narrative session</span>, focusing on interaction time, interaction counts by information type, and satisfaction with the character’s responses. First, participants exhibited a significantly longer mean engagement time (29.00 minutes, SD=8.47) when interacting with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.2">In-Situ mode</span> compared tothe <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.3">Artist mode</span> (17.96 minutes, SD=7.41) and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.4">Narrator mode</span> (15.29 minutes, SD=5.15) (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.5">F</span>(2,51)=24.84, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.6">p</span>¡.001). Furthermore, the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.7">In-Situ mode</span> evoked the highest number of ”Describe” (50), ”Analyze” (38), ”Interpret” (61), and ”Question” (88) behaviors, indicating a more active behavioral engagement.
Satisfaction ratings showed that the responses of the characters in <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.8">In-Situ mode</span> received the highest approval, with 65% (SD=0.22) marked as ”Like”, compared to 59% (SD=0.27) in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.9">Narrator mode</span> and 50% (SD=0.24) in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.10">Artist mode</span>. The percentage of ”Neutral” and ”Dislike” ratings were also lower in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.11">In-Situ mode</span>, further indicating its higher overall satisfaction level. These findings highlighted the effectiveness of <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.12">In-Situ mode</span> in enhancing user behavioral engagement in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.13">Narrative session</span>. The results might also explain why the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.14">In-Situ mode</span> was rated the highest across the four engagement dimensions discussed in section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Behavioral engagement results in the narrative session. Time: Mean (SD). #: interaction counts by information types.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<td class="ltx_td ltx_border_t" id="S5.T2.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S5.T2.1.1.1.2">(i) Art appreciation behaviors based on system log analysis</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S5.T2.1.1.1.3">(ii) Satisfaction with output</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<td class="ltx_td" id="S5.T2.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.2.1">Time ***</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.3.1">Describe#</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.4.1">Analyze#</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.5.1">Interpret#</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.6.1">Judge#</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.7.1">Question#</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.8.1">Like</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.9.1">Neutral</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.10.1">Dislike</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.3.3.1.1">Narrator</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.2">15.29 (5.15)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.3">45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.4">37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.5">59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.6">52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.7">30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.8">59%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.9">9%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.10">32%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.1.1">Artist</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.2">17.96 (7.41)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.3">44</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.4">36</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.5">50</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.6">57</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.7">20</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.8">50%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.9">26%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.10">24%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T2.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.1.1">In-Situ</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.2.1">29.00 (8.47)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.3">50</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.4">38</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.5">61</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.6">50</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.7.1">88</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.8">65%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.9">13%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.10">22%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="4" id="S5.T2.1.6.6.1">
<span class="ltx_text ltx_font_italic" id="S5.T2.1.6.6.1.1">Note</span>: * p¡0.05, ** p¡0.01, *** p¡0.001</td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.6.6.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.6.6.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.6.6.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.6.6.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.6.6.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.6.6.7"></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.1.1">Emotional Engagement (EE): <span class="ltx_text" id="S5.SS2.SSS2.1.1.1" style="color:#00FF00;">In-Situ</span> Yielded the Highest Hedonic Value, Followed by the <span class="ltx_text" id="S5.SS2.SSS2.1.1.2" style="color:#FF8000;">Artist Mode.</span></span>
</h4>
<figure class="ltx_figure" id="S5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="198" id="S5.F8.g1" src="extracted/5874852/FIGsTABs/Figure/6.1.2Value.png" width="269"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span><span class="ltx_text ltx_font_typewriter" id="S5.F8.2.1">Hedonic Value and Epistemic Value</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F8.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F8.4">Bar charts showing the mean values for Hedonic and Epistemic engagement across three character types (Narrator, Artist, In-Situ), with error bars indicating variability and significant differences marked by asterisks, consistently reveal that In-Situ characters yield the highest values in both engagement types.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">There was a significant difference in the users’ perception of hedonic value across the three modes (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.1">F</span>(2,51)=8.31, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.2">p</span>¡.001). As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F8" title="Figure 8 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">8</span></a> (a),
on average, participants rated their emotional engagement as highest in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.3">In-Situ mode</span> (M=4.64, SD=0.39) on 5-point Likert scales. A post hoc analysis revealed that emotional engagement was significantly higher in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.4">In-Situ mode</span> than the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.5">Artist mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.6">Narrator mode</span> (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.7">p</span>¡.01 and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.8">p</span>¡.001 respectively).
While no significant difference was observed between the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.9">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.10">Artist mode</span>, the emotional engagement in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.11">Artist mode</span> (M=3.98, SD=0.60) was slightly higher than that in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.12">Narrator mode</span> (M=3.85, SD=0.65).
The think-aloud results indicated that the users’ emotional engagement in the interactions with the painting object characters (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.13">In-Situ mode</span>) and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.14">artist characters</span> (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.15">Artist mode</span>) was greatly enhanced by the immersive experience involving time travel <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p1.1.16">(EE1)</span>, empathy <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p1.1.17">(EE2)</span>, and anthropomorphism <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p1.1.18">(EE3)</span>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p2.1.1">EE1: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.1.1.1" style="color:#FF8000;">Artist</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.1.1.2" style="color:#00FF00;">In-Situ</span>——Time Travel——From Modern to Ancient and From Ancient to Modern.</span> Time travel in this context refers to the immersive experience where users feel transported between different historical periods during their interactions with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.1.2">In-Situ</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.1.3">Artist characters</span>. This experience allows participants to engage with characters from both ancient and modern times, leading a deeper understanding of the cultural and historical context behind the artworks.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">Thirteen participants reported experiencing time travel when interacting with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p3.1.1">In-Situ character</span>, and four participants reported these experiences with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p3.1.2">Artist character</span>. Participants expressed that the vivid and detailed narratives created a sense of time travel, transporting them from the modern to the ancient or bringing objects depicted in the painting from the ancient to the modern.
As shown in the examples of EE1 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F9" title="Figure 9 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">9</span></a>, P1 felt that the term <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p3.1.3">“medicinal supply chain”</span> transformed ancient baskets into symbols of historical development, as if they were time travelers narrating the evolution of trade and medicine. P21 highlighted the power of first-person narrative, which made him feel as if he were conversing directly with historical figures or artists. P16 was attracted by the statement <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p3.1.4">”my eyes were fixed on the athlete on the right”</span> and felt as though he was an active participant in a modern athletic event. These responses suggested that detailed, first-person storytelling in art appreciation can enhance users’ emotional engagement and provide a more profound emotional connection with historical content.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p4">
<p class="ltx_p" id="S5.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p4.1.1">EE2: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p4.1.1.1" style="color:#FF8000;">Artist</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p4.1.1.2" style="color:#00FF00;">In-Situ</span>——Empathy——Sharing of a Specific Feeling, or Emotion with Character.</span>
Empathy, in this context, refers to the emotional connections that participants formed with the characters, where they felt the characters’ emotions, understood their motivations, and expressed care or support during the interaction. The empathy was often triggered by detailed emotional and atmospheric descriptions.</p>
</div>
<figure class="ltx_figure" id="S5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="688" id="S5.F9.g1" src="extracted/5874852/emotion.png" width="568"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span><span class="ltx_text ltx_font_typewriter" id="S5.F9.2.1">Examples of Emotional Engagement</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F9.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F9.4">This image contains six cards, each showcasing user interactions with different objects, including the system’s feedback and the users’ comments on that feedback. The cards are categorized by various emotional engagement codes, such as ”Time Travel” (from ancient to modern), ”Empathy,” and ”Multi-sensory Anthropomorphism.” These cards illustrate how users emotionally engage with objects through the system’s interpretation, drawing on personal experiences or reflections. For example, users might feel a connection between ancient objects and modern concepts or feel immersed in the experience through detailed descriptions.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p5">
<p class="ltx_p" id="S5.SS2.SSS2.p5.1">Five participants reported feeling empathy when interacting with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p5.1.1">In-Situ character</span> and three with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p5.1.2">Artist character</span>.
As shown in the examples of EE2 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F9" title="Figure 9 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">9</span></a>, P9 noted that the description of a character’s nervousness and hope while waiting for a doctor’s diagnosis evoked personal memories of hospital visits, leading to a shared emotional experience. Similarly, P24 remarked that the depiction of tension, dynamism, and determination in a cubist-style football game effectively conveyed the artist’s intended emotions, allowing the participant to feel the energy and passion behind the artwork. These empathetic experiences helped deepen participants’ emotional engagement with the characters and the artworks.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p6">
<p class="ltx_p" id="S5.SS2.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p6.1.1">EE3: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p6.1.1.1" style="color:#00FF00;">In-Situ</span>——Anthropomorphism——Providing a Richer Multi-sensory and Interesting Experience.</span>
Anthropomorphism refers to attributing human characteristics to non-human entities. When participants interacted with anthropomorphic <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p6.1.2">In-Situ characters</span>, the experience could be multi-sensory. It included hearing, sight, smell, and so on. This multi-sensory experience could enhance the immersion and interestingness in art appreciation.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p7">
<p class="ltx_p" id="S5.SS2.SSS2.p7.1">Seven participants specifically noted the multi-sensory aspects of their interactions with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.1">In-Situ character</span>. Fifteen participants mentioned <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.2">”interest”</span>, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.3">”enjoyment”</span>, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.4">”fun”</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.5">”happiness”</span> when interacting with the anthropomorphic <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.6">In-Situ character</span>.
As shown in the example of EE3 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F9" title="Figure 9 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">9</span></a>, P15 appreciated the anthropomorphic multi-sensory narrative that included the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.7">In-Situ character</span>’s hearing, sight, and smell. Similarly, P16 emphasized the auditory dimension by asking the football, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.8">”In this real environment, according to the laws of physics, what sound can you produce?”</span>. This illustrated how participants actively engaged with the sensory elements to deepen their connection with the objects. P9 noted the humorous anthropomorphized narrative of the carrying pole, remarking that the description—<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p7.1.9">”As the carrying pole, I quietly bear the weight of the herbs, contributing my strength to the daily operations of the clinic.”</span>—made the interaction more entertaining. These examples suggested that combining anthropomorphic descriptions with multi-sensory storytelling could significantly enhance emotional engagement, making the experience more immersive and enjoyable.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p8">
<p class="ltx_p" id="S5.SS2.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p8.1.1">EE4, EE5, EE6: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p8.1.1.1">Three Modes</span>——Negative Emotion——Dis-appointment, Distrust, and Boredom.</span>
While the dialogue with the characters generally aimed to enhance user engagement, some participants experienced negative emotions during their interactions, such as disappointment, distrust, and boredom.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p9">
<p class="ltx_p" id="S5.SS2.SSS2.p9.1">Disappointment occurred when the characters’ responses did not meet participants’ expectations or failed to provide sufficient information. For example, P7 felt disappointed by the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p9.1.1">Narrator character</span>’s narrative, noting the absence of detailed academic and historical explanations about the evolution of astronomical instruments. Similarly, P18 was disappointed with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p9.1.2">Artist character</span> for not providing the structural and color analysis of the paintings.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p10">
<p class="ltx_p" id="S5.SS2.SSS2.p10.1">Distrust emerged when participants felt that the characters’ responses were not genuine. For example, P14 expressed skepticism about the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p10.1.1">In-Situ character</span>’s responses, perceiving them as less credible and objective compared to the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p10.1.2">Narrator character</span>. P1 distrusted the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p10.1.3">Artist character</span>, believing that <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p10.1.4">artists</span> should avoid subjectively praising their own work, as this limits the viewers’ freedom to interpret and evaluate the art. Three participants also mentioned that the use of bullet points in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p10.1.5">In-Situ mode</span> made the responses feel less authentic. P19 specifically noted that GPT tends to excel at describing things in bullet points, which can make the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p10.1.6">In-Situ characters</span> less genuine.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p11">
<p class="ltx_p" id="S5.SS2.SSS2.p11.1">Boredom resulted from repetitive information and a lack of novelty in the dialogue. Seven participants reported that repeated content, particularly in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p11.1.1">Artist mode</span>, made the interactions feel tedious.</p>
</div>
<figure class="ltx_figure" id="S5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="698" id="S5.F10.g1" src="extracted/5874852/cognitive.png" width="568"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span><span class="ltx_text ltx_font_typewriter" id="S5.F10.2.1">Examples of Cognitive Engagement</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F10.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F10.4">This image contains six cards that represent various user interactions with objects, along with system-generated feedback and users’ reflections, focusing on different aspects of cognitive engagement. The cards are labeled with cognitive engagement codes, such as ”Understanding,” ”Association,” and ”Analysis,” highlighting how users process and evaluate the information provided. Some cards illustrate users gaining new knowledge about historical objects or understanding the relevance of details, while others show users critically analyzing the system’s accuracy or artistic interpretations. Through these interactions, users engage with both the factual and interpretive elements of the objects, sometimes questioning the content, identifying perceived inaccuracies, or making subjective judgments about the artistic choices presented.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3. </span><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.1.1">Cognitive Engagement (CE): <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.1.1.1" style="color:#FF8000;">Artist Mode</span> Yielded the Lowest Epistemic Value, Compared to the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.1.1.2" style="color:#0000FF;">Narrator Mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.1.1.3" style="color:#00FF00;">In-Situ Mode.</span></span>
</h4>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">There was a significant difference in the users’ perception of epistemic value across the three modes (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.1">F</span>(2,51)=8.39, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.2">p</span>¡.001).
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F8" title="Figure 8 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">8</span></a> (b), the participants rated their cognitive engagement as lowest in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.3">Artist mode</span>. A post hoc analysis revealed that cognitive engagement was significantly lower in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.4">Artist mode</span> (M=3.52, SD=0.61) compared to the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.5">Narrator mode</span> (M=4.02, SD=0.43) and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.6">In-Situ mode</span> (M=4.33, SD=0.61) (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.7">p</span>¡.05 and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.8">p</span>¡.001 respectively). There was no significant difference in cognitive engagement between the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.9">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.10">In-Situ mode</span>. The think-aloud results provided insight into these findings. First, participants usually had higher expectations for the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.11">Artist characters</span>, anticipating more understanding of the creative process and background knowledge from the artist’s perspective <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p1.1.12">(CE1)</span>. Second, the participants found it unacceptable when the information provided by the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.13">Artist character</span> was inaccurate <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p1.1.14">(CE2)</span>. Additionally, the participants analyzed the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.15">Artist character</span> and noted the inconsistencies in content and subjective judgments <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p1.1.16">(CE3)</span>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p2">
<p class="ltx_p" id="S5.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p2.1.1">CE1: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p2.1.1.1">Three Modes</span>——Understanding——All Provided New Knowledge, but <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p2.1.1.2" style="color:#FF8000;">Artist</span> Needs More.</span> Participants gained new knowledge and enhanced their understanding of the artwork through interactions with the characters in three modes. When it came to the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p2.1.2">Artist character</span>, participants expected the artist to provide unique insights only the artist knows, such as the creative process or the story behind the work, to deepen their understanding of the painting.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p3">
<p class="ltx_p" id="S5.SS2.SSS3.p3.1">Twelve participants gained new insights from the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p3.1.1">In-Situ character</span>, and ten from the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p3.1.2">Narrator</span>, while only six did from the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p3.1.3">Artist</span>. Notably, nine participants expressed a desire for the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p3.1.4">Artist character</span> to provide more detailed information.
As depicted in CE1 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F10" title="Figure 10 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">10</span></a>, P13 acquired knowledge about the attire of ancient nobility from the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p3.1.5">Narrator character</span>. Meanwhile, P1 requested more detailed information from the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p3.1.6">Artist character</span> regarding the painting process. Similarly, P9 was interested in learning more about the artist’s creative intentions, and both P12 and P18 found the artist’s information to be lacking in specificity. These examples suggested that expanding the knowledge base related to the artist perspective could better align with user expectations.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p4">
<p class="ltx_p" id="S5.SS2.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p4.1.1">CE2: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p4.1.1.1">Three Modes</span>——Associate——All Perceived Relevance, but <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p4.1.1.2" style="color:#FF8000;">Artist</span> Noticed Inaccuracy.</span>
Participants associated the information provided by characters with their personal experiences or real-life situations to assess their relevance and accuracy. This association was more prominent in interactions with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p4.1.2">In-Situ</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p4.1.3">Narrator characters</span>, but less so with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p4.1.4">Artist</span>, where inaccuracies were more frequently noted.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p5">
<p class="ltx_p" id="S5.SS2.SSS3.p5.1">Nine participants connected information from the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.1">In-Situ character</span> to their own experiences, while seven did so with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.2">Narrator</span> and five with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.3">Artist</span>. For example, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F10" title="Figure 10 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">10</span></a>, P18, using the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.4">Narrator character</span>, associated the cabinets in TV dramas and agreed with the description <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.5">”with rich and vivid details”</span>. Eight participants noticed inaccuracy while interacting with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.6">Artist character</span>. For example, P8 observed that the cabinet in the painting did not match the size of real pharmacy cabinets, which led them to doubt the artist’s claim of realism. This suggested that ensuring accuracy is essential to maintaining trust and engagement with digital characters, particularly in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p5.1.7">Artist mode</span>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p6">
<p class="ltx_p" id="S5.SS2.SSS3.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p6.1.1">CE3: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p6.1.1.1" style="color:#FF8000;">Artist</span>——Analysis——Inconsistency with Content and Subjective Judgments.</span>
Participants identified two key factors that diminished their cognitive engagement with the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p6.1.2">Artist character</span>. First, the information was accurate, but the way it was presented did not align with the artist’s historical identity. Second, the artist’s evaluations were perceived as subjective.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p7">
<p class="ltx_p" id="S5.SS2.SSS3.p7.1">Six participants noted that the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p7.1.1">Artist</span>’s language and expression did not align with the historical context they expected, while five participants felt that the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p7.1.2">Artist</span>’s judgments were overly subjective.
As shown in the example of CE3 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.F10" title="Figure 10 ‣ 5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">10</span></a>, P8 expressed frustration that the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p7.1.3">Artist</span>’s modern language was inconsistent with the tone and style of the period, reducing the authenticity of the interaction. Similarly, P16 argued that the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p7.1.4">Artist</span> should remain neutral and open, avoiding subjectivity. When the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p7.1.5">Artist</span>’s narrative did not match historical expectations or was seen as biased, participants struggled to fully engage with the content, which reduced the epistemic value of the experience.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p8">
<p class="ltx_p" id="S5.SS2.SSS3.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p8.1.1">CE4: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p8.1.1.1" style="color:#00FF00;">In-Situ</span>——Curiosity——Active Exploration with <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p8.1.1.2" style="color:#00FF00;">In-Situ</span> Characters.</span>
Participants exhibited strong curiosity when interacting with <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p8.1.2">In-Situ characters</span>, actively exploring the content to satisfy their interests.
Eight participants reported this kind of curiosity in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p8.1.3">In-Situ mode</span>.
For example, P16 asked three consecutive questions about a particular object and expressed a desire to continue asking questions. This active engagement demonstrated that <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p8.1.4">In-Situ characters</span> successfully sparked participants’ curiosity, promoting a deep understanding of the painting.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p9">
<p class="ltx_p" id="S5.SS2.SSS3.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p9.1.1">CE5: <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p9.1.1.1">Three Modes</span>——Reflection——Correcting Characters’ Errors.</span>
Participants recognized and corrected errors in the characters’ responses by reflecting and adjusting their interactions. This reflection allowed users to improve the system’s understanding of their needs, leading to more accurate information. For instance, they could reselect targets or add contextual information to guide the characters to provide correct interpretation. In one example, when a doctor was mistakenly identified as a patient because the doctor’s stool was not selected, the participant corrected the error by adding the stool to the scene. This action not only resolved the immediate mistake but also demonstrated how users could actively engage in the process. Characters should respond to user interventions and learn from corrections to improve future responses.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p10">
<p class="ltx_p" id="S5.SS2.SSS3.p10.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS3.p10.1.1">Summary-RQ1b:</span> Our findings demonstrated participants’ varying levels of engagement with different characters in three modes. First, for behavioral engagement, participants spent significantly longer interaction time and asked more questions when interacting with the characters in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p10.1.2" style="color:#00FF00;">In-Situ mode</span>. Second, regarding emotional engagement, participants experienced the highest hedonic value when interacting with characters in the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p10.1.3" style="color:#00FF00;">In-Situ mode</span>, due to immersive experience involving time travel (EE1), anthropomorphism (EE2), and empathy (EE3), followed by the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p10.1.4" style="color:#FF8000;">Artist mode</span>. The emotional engagement explained the aesthetic appeal results in Session <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.1</span></a>. Third, concerning cognitive engagement, participants reported the lowest epistemic value when engaging with the characters in <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p10.1.5" style="color:#FF8000;">Artist mode</span> compared the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p10.1.6" style="color:#0000FF;">Narrator mode</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p10.1.7" style="color:#00FF00;">In-Situ mode</span>, due to higher knowledge expectations (CE1) and stricter demands for accuracy (CE2) and consistency (CE3), which explained the usability in Session <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS1" title="5.1. Overall Perceived Engagement (RQ1a) - Survey 1 ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.1</span></a>. The Session <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2" title="6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.2</span></a> explored perception evolution across two sessions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Results across the <span class="ltx_text ltx_font_italic" id="S6.1.1">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S6.2.2">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.3.3">Sessions</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">[RQ2]</span> How do users perception <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.p1.1.2">evolve</span> between the <span class="ltx_text ltx_font_italic" id="S6.p1.1.3">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S6.p1.1.4">Recommendation sessions,</span> and what engagement factors in the <span class="ltx_text ltx_font_italic" id="S6.p1.1.5">Narrative</span> <span class="ltx_text ltx_font_italic" id="S6.p1.1.6">session</span> are <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.p1.1.7">associated</span> with the changes?</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Sections <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2" title="6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.2</span></a> assessed RQ2. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.1</span></a> examined users’ perceptions of the characters’ responses across two sessions. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2" title="6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.2</span></a> further explained the results of <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.1</span></a> by analyzing the underlying factors in the <span class="ltx_text ltx_font_italic" id="S6.p2.1.1">Narrative session</span> that are associated with these perceptions, including behavioral, emotional, and cognitive engagement.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p1.1.1">[RQ2a]</span> How do users’ perception of the character’s responses <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.SS1.p1.1.2">evolve</span> between the <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.3">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.4">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.5">sessions</span>?</p>
</div>
<figure class="ltx_figure" id="S6.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="434" id="S6.F11.g1" src="extracted/5874852/FIGsTABs/Figure/6.2.1NarrativeRecommendation.png" width="538"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span><span class="ltx_text ltx_font_typewriter" id="S6.F11.2.1">Survey Results: Users’ Perception of the Characters’ Responses across Three Modes in Two Sessions.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F11.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.F11.4">Violin plots showing survey scores for Consistency, Relatability, Believability, and Stereotypicality across three character types (<span class="ltx_text ltx_font_italic" id="S6.F11.4.1">Narrator</span>, <span class="ltx_text ltx_font_italic" id="S6.F11.4.2">Artist</span>, <span class="ltx_text ltx_font_italic" id="S6.F11.4.3">In-Situ</span>) in both <span class="ltx_text ltx_font_italic" id="S6.F11.4.4">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S6.F11.4.5">Recommendation</span> contexts, with error bars and significant differences marked by asterisks, indicate that <span class="ltx_text ltx_font_italic" id="S6.F11.4.6">In-Situ characters</span> consistently receive the highest scores in all categories except Stereotypicality, where no significant differences were observed.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.F11" title="Figure 11 ‣ 6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">11</span></a>, We performed regression analysis to examine how perceptions in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.1">Narrative session</span> influenced the <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.2">Recommendation session</span>. Additionally, we conducted a one-way ANOVA with a randomized effect to analyze the character perception across three modes in two sessions, followed by post hoc analyses for pairwise comparisons. The perception of characters consists of (a) consistency, (b) relatability, (c) believability, and (d) stereotypicality. Based on comparison results from the survey, significant differences in perceptions of character were observed when interacting with the three types of characters across the <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.3">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.4">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.5">sessions</span>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Consistency:</span>
The regression analysis showed no correlation between the perceived response consistency in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2">Narrative</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.3">session</span> and the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.4">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.5">session</span> (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mi id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><ci id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">italic_β</annotation></semantics></math>=0.17, S.E.=0.11), suggesting that users’ perceptions of information consistency may vary independently across the two sessions. There was no significant difference in perceived character response consistency across the three modes in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.6">Narrative</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.7">session</span>. In contrast, in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.8">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.9">session</span>, there was a significant difference in the perceived character response consistency (<span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.10">F</span>(2,51)=6.92, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.11">p</span>¡.01). The post hoc analysis found that the perceived response consistency of the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.12">In-Situ characters</span> (M=5.63, SD=1.01) was significantly higher than that of the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.13">Narrator characters</span> (M=4.83, SD=1.13) and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.14">Artist characters</span> (M=4.46, SD=1.18) (<span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.15">p</span>¡.05 and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.16">p</span>¡.01 respectively) in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.17">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.18">session</span>. The median perceived response consistency in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.19">Recommendation</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.20">session</span> (median=5.0) was higher than that in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.21">Narrative</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.22">session</span> (median=4.5).</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Relatability:</span>
The regression analysis showed a significant correlation between the perceived response relatability in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.2">Narrative session</span> and the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.3">Recommendation session</span> (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS1.p4.1.m1.1"><semantics id="S6.SS1.p4.1.m1.1a"><mi id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><ci id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p4.1.m1.1d">italic_β</annotation></semantics></math>=0.30, S.E.=0.10, <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.4">p</span>¡0.01), indicating that perceived relatability in <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.5">Narrative session</span> had a positive effect on that in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.6">Recommendation session</span>.
In the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.7">Narrative session</span>, there was a significant difference in perceived characters’ responses relatability across the three modes (<span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.8">F</span>(2,51)=6.81, <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.9">p</span>¡.01). Similarly, the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.10">Recommendation session</span> showed a significant difference in perceived characters’ responses relatability (<span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.11">F</span>(2,51)=6.98, <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.12">p</span>¡.01).
The post hoc analysis found that <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.13">In-Situ characters’</span> (M=6.13, SD=0.61) responses had significantly higher perceived reliability than <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.14">Narrator characters’</span> (M=5.08, SD=1.21) and <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.15">Artist characters’</span> (M=5.04, SD=1.43) (both <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.16">p</span>¡.05). The median perceived characters’ responses relatability in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.17">Recommendation session</span> (median=5.0) was higher than that in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.18">Narrative session</span> (median=4.5).</p>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<p class="ltx_p" id="S6.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p5.1.1">Believability:</span>
The regression analysis showed a significant correlation between the perceived response believability in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.2">Narrative session</span> and the <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.3">Recommendation session</span> (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS1.p5.1.m1.1"><semantics id="S6.SS1.p5.1.m1.1a"><mi id="S6.SS1.p5.1.m1.1.1" xref="S6.SS1.p5.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.1.m1.1b"><ci id="S6.SS1.p5.1.m1.1.1.cmml" xref="S6.SS1.p5.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p5.1.m1.1d">italic_β</annotation></semantics></math>=0.45, S.E.=0.11, <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.4">p</span>¡0.001), indicating that perceived believability in <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.5">Narrative session</span> had a positive effect that in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.6">Recommendation session</span>.
There was a significant difference in perceived characters’ responses believability across the three modes in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.7">Narrative session</span> (<span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.8">F</span>(2,51)=16.35, <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.9">p</span>¡.001).
In the <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.10">Recommendation session</span>, there was also a significant difference in perceived characters’ responses believability, with similar trends observed across all modes (<span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.11">F</span>(2,51)=16.71, <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.12">p</span>¡.001).
The post hoc analysis showed that <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.13">In-Situ characters’</span> (M=5.75, SD=1.11) responses had significantly greater believability than <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.14">Narrator characters’</span> (M=4.79, SD=1.35) and <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.15">Artist characters’</span> (M=3.67, SD=1.27) (<span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.16">p</span>¡.05 and <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.17">p</span>¡.001 respectively). Additionally, <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.18">Narrator characters’</span> responses were perceived as significantly more believable than <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.19">Artist characters’</span> (<span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.20">p</span>¡.01).</p>
</div>
<div class="ltx_para" id="S6.SS1.p6">
<p class="ltx_p" id="S6.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p6.1.1">Stereotypicality:</span>
The regression analysis showed no correlation between the perceived response stereotypicality in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p6.1.2">Narrative session</span> and the <span class="ltx_text ltx_font_italic" id="S6.SS1.p6.1.3">Recommendation session</span> (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS1.p6.1.m1.1"><semantics id="S6.SS1.p6.1.m1.1a"><mi id="S6.SS1.p6.1.m1.1.1" xref="S6.SS1.p6.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p6.1.m1.1b"><ci id="S6.SS1.p6.1.m1.1.1.cmml" xref="S6.SS1.p6.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p6.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p6.1.m1.1d">italic_β</annotation></semantics></math>=-0.06, S.E.=0.08).
In the <span class="ltx_text ltx_font_italic" id="S6.SS1.p6.1.4">Narrative session</span>, there was a significant difference in perceived characters’ responses stereotypicality across the three modes (F(2,51)=4.67, <span class="ltx_text ltx_font_italic" id="S6.SS1.p6.1.5">p</span>¡.05). However,
In the <span class="ltx_text ltx_font_italic" id="S6.SS1.p6.1.6">Recommendation session</span>, there was no significant difference in perceived characters’ responses stereotypicality.
The lack of significant differences in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p6.1.7">Recommendation session</span> suggested that participants might have focused more on the practical content of the characters’ recommendations, rather than their stereotypical traits. In contrast, the open-ended nature of the Narrative task may have led participants to perceive some characters as conforming to stereotypes, as their responses were less structured.</p>
</div>
<div class="ltx_para" id="S6.SS1.p7">
<p class="ltx_p" id="S6.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p7.1.1">Summary-RQ2a:</span>
We compared participants’ perceptions of the characters’ responses across three modes in the two sessions. First, the <span class="ltx_text ltx_font_italic" id="S6.SS1.p7.1.2" style="color:#00FF00;">In-Situ characters</span> consistently received the highest scores in consistency, relatability, and believability in both interaction sessions (i.e., <span class="ltx_text ltx_font_italic" id="S6.SS1.p7.1.3">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S6.SS1.p7.1.4">Recommendation</span>). Second, we observed the gap in the perceived consistency between the <span class="ltx_text ltx_font_italic" id="S6.SS1.p7.1.5" style="color:#FF8000;">Artist character</span> and the <span class="ltx_text ltx_font_italic" id="S6.SS1.p7.1.6" style="color:#0000FF;">Narrator character</span> narrowed in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p7.1.7">Recommendation session</span> and the perceived stereotypicality significantly changed across the two sessions. Next, Session <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS2" title="6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.2</span></a> explained the potential rationale for these results from interactive engagement perspectives.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Across-session Association of Engagement Factors in the <span class="ltx_text ltx_font_italic" id="S6.SS2.1.1">Narrative Session</span> (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">[RQ2b]</span> What engagement factors in the <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">Narrative Session</span> are <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.SS2.p1.1.3">associated</span> with the user perception in the <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.4">Recommendation session</span>?</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">To address this research question, we performed two linear regression analyses to explore how engagement factors in the <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">Narrative Session</span> influenced participants’ character perception and outcomes satisfaction in the <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.2">Recommendation session</span>.
The two sets of regressions aim to yield insights about users’ perceptions from two perspectives: 1) the first set of regressions used participants’ character response perception survey scores (i.e.,
consistency, relatability, believability, and stereotypicality) as the dependent variables; 2) the second set of regressions adopted participants’ satisfaction ratings for recommendation images and reasons as dependent variables respectively. Based on our previous findings, we used factors that could represent users’ interactive engagement as predictors: behavioral engagement (measured by interaction time), emotional engagement (measured by hedonic value), and cognitive engagement (measured by epistemic value). As discussed in Sections <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS1" title="5.2.1. Behavioral Engagement: When Interacting with the In-Situ Mode, Users’ Interaction Time was Longer, and They Asked More Questions Proactively. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS2" title="5.2.2. Emotional Engagement (EE): In-Situ Yielded the Highest Hedonic Value, Followed by the Artist Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS3" title="5.2.3. Cognitive Engagement (CE): Artist Mode Yielded the Lowest Epistemic Value, Compared to the Narrator Mode and In-Situ Mode. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>, these factors differ significantly across different characters.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Regression results with users’ engagement data in the narrative session as predictors and perceived character survey ratings in the recommendation session as dependent variables. Each column in the table represents one regression performed with the corresponding rating item as the dependent variable.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S6.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.4.5.1">
<td class="ltx_td ltx_align_top ltx_border_t" id="S6.T3.4.5.1.1"></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="4" id="S6.T3.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.4.5.1.2.1">Dependent Variables — Experience (Survey Scores)</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.6.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.6.2.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.6.2.1.1">
<span class="ltx_p" id="S6.T3.4.6.2.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.4.6.2.1.1.1.1">Predictors</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T3.4.6.2.2"><span class="ltx_text ltx_font_bold" id="S6.T3.4.6.2.2.1">Consistency</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T3.4.6.2.3"><span class="ltx_text ltx_font_bold" id="S6.T3.4.6.2.3.1">Relatability</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T3.4.6.2.4"><span class="ltx_text ltx_font_bold" id="S6.T3.4.6.2.4.1">Believability</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T3.4.6.2.5"><span class="ltx_text ltx_font_bold" id="S6.T3.4.6.2.5.1">Stereotypicality</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.4">
<td class="ltx_td ltx_align_top" id="S6.T3.4.4.5"></td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T3.1.1.1">
<math alttext="\beta" class="ltx_Math" display="inline" id="S6.T3.1.1.1.m1.1"><semantics id="S6.T3.1.1.1.m1.1a"><mi id="S6.T3.1.1.1.m1.1.1" xref="S6.T3.1.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.T3.1.1.1.m1.1d">italic_β</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1">(S.E.)</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T3.2.2.2">
<math alttext="\beta" class="ltx_Math" display="inline" id="S6.T3.2.2.2.m1.1"><semantics id="S6.T3.2.2.2.m1.1a"><mi id="S6.T3.2.2.2.m1.1.1" xref="S6.T3.2.2.2.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.2.m1.1b"><ci id="S6.T3.2.2.2.m1.1.1.cmml" xref="S6.T3.2.2.2.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.2.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.2.2.m1.1d">italic_β</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.2.1">(S.E.)</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T3.3.3.3">
<math alttext="\beta" class="ltx_Math" display="inline" id="S6.T3.3.3.3.m1.1"><semantics id="S6.T3.3.3.3.m1.1a"><mi id="S6.T3.3.3.3.m1.1.1" xref="S6.T3.3.3.3.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.T3.3.3.3.m1.1b"><ci id="S6.T3.3.3.3.m1.1.1.cmml" xref="S6.T3.3.3.3.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.3.3.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.T3.3.3.3.m1.1d">italic_β</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T3.3.3.3.1">(S.E.)</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T3.4.4.4">
<math alttext="\beta" class="ltx_Math" display="inline" id="S6.T3.4.4.4.m1.1"><semantics id="S6.T3.4.4.4.m1.1a"><mi id="S6.T3.4.4.4.m1.1.1" xref="S6.T3.4.4.4.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.T3.4.4.4.m1.1b"><ci id="S6.T3.4.4.4.m1.1.1.cmml" xref="S6.T3.4.4.4.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.4.4.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.T3.4.4.4.m1.1d">italic_β</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T3.4.4.4.1">(S.E.)</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.7.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.4.7.3.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.7.3.1.1">
<span class="ltx_p" id="S6.T3.4.7.3.1.1.1" style="width:85.4pt;">Behavioral
Engagement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.4.7.3.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.7.3.2.1">
<span class="ltx_p" id="S6.T3.4.7.3.2.1.1" style="width:56.9pt;">0.01 (0.01)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.4.7.3.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.7.3.3.1">
<span class="ltx_p" id="S6.T3.4.7.3.3.1.1" style="width:56.9pt;">0.03 (0.02)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.4.7.3.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.7.3.4.1">
<span class="ltx_p" id="S6.T3.4.7.3.4.1.1" style="width:56.9pt;">0.01 (0.01)</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T3.4.7.3.5">0.04 (0.02)*</td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.8.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.8.4.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.8.4.1.1">
<span class="ltx_p" id="S6.T3.4.8.4.1.1.1" style="width:85.4pt;">Emotional
Engagement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.8.4.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.8.4.2.1">
<span class="ltx_p" id="S6.T3.4.8.4.2.1.1" style="width:56.9pt;">0.32 (0.18)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.8.4.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.8.4.3.1">
<span class="ltx_p" id="S6.T3.4.8.4.3.1.1" style="width:56.9pt;">0.14 (0.19)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.8.4.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.8.4.4.1">
<span class="ltx_p" id="S6.T3.4.8.4.4.1.1" style="width:56.9pt;">0.74 (0.14)***</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T3.4.8.4.5">-0.13 (0.20)</td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.9.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.9.5.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.9.5.1.1">
<span class="ltx_p" id="S6.T3.4.9.5.1.1.1" style="width:85.4pt;">Cognitive
Engagement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.9.5.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.9.5.2.1">
<span class="ltx_p" id="S6.T3.4.9.5.2.1.1" style="width:56.9pt;">0.52 (0.18)**</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.9.5.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.9.5.3.1">
<span class="ltx_p" id="S6.T3.4.9.5.3.1.1" style="width:56.9pt;">0.48 (0.19)*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.4.9.5.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.4.9.5.4.1">
<span class="ltx_p" id="S6.T3.4.9.5.4.1.1" style="width:56.9pt;">0.53 (0.14)***</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T3.4.9.5.5">0.01 (0.21)</td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.10.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="4" id="S6.T3.4.10.6.1">
<span class="ltx_text ltx_font_italic" id="S6.T3.4.10.6.1.1">Note</span>: * <span class="ltx_text ltx_font_italic" id="S6.T3.4.10.6.1.2">p</span>¡0.05, ** <span class="ltx_text ltx_font_italic" id="S6.T3.4.10.6.1.3">p</span>¡0.01, *** <span class="ltx_text ltx_font_italic" id="S6.T3.4.10.6.1.4">p</span>¡0.001</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.4.10.6.2"></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.1.1">Perceived consistency in the recommendation session was associated with cognitive engagement in the narrative session and stereotypicality was associated with behavioral engagement.</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.T3" title="Table 3 ‣ 6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">3</span></a> showed the regression results with survey scores of characters’ responses perception in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.1">Recommendation session</span> as dependent variables and interactive engagement factors in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.2">Narrative session</span> as predictors.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.2">For consistency and relatability, we found a significant positive association between participants’ cognitive engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.1">Narrative session</span> and their perceived consistency(<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p2.1.m1.1"><semantics id="S6.SS2.SSS1.p2.1.m1.1a"><mi id="S6.SS2.SSS1.p2.1.m1.1.1" xref="S6.SS2.SSS1.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.1.m1.1b"><ci id="S6.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p2.1.m1.1d">italic_β</annotation></semantics></math>=0.52, S.E.=0.18, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.2">p</span>¡0.01) and relatability (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p2.2.m2.1"><semantics id="S6.SS2.SSS1.p2.2.m2.1a"><mi id="S6.SS2.SSS1.p2.2.m2.1.1" xref="S6.SS2.SSS1.p2.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.2.m2.1b"><ci id="S6.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S6.SS2.SSS1.p2.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p2.2.m2.1d">italic_β</annotation></semantics></math>=0.48, S.E.=0.19, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.3">p</span>¡0.05) of the character’s responses in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.4">Recommendation session</span>. This suggested that when participants experience a higher level of cognitive engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.5">Narrative session</span>, they tend to perceive the character responses as more consistent and relatable during the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.6">Recommendation session</span>. Participants’ reflections during the think-aloud process supported these findings. For example, one comment from P20 was <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.2.7">”Because I felt his answers were good in the previous chat stage, I think his recommendations are consistent with his identity.”</span>.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p3">
<p class="ltx_p" id="S6.SS2.SSS1.p3.3">For believability, we found a significant positive association between participants’ emotional engagement (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.1.m1.1"><semantics id="S6.SS2.SSS1.p3.1.m1.1a"><mi id="S6.SS2.SSS1.p3.1.m1.1.1" xref="S6.SS2.SSS1.p3.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p3.1.m1.1b"><ci id="S6.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p3.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p3.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p3.1.m1.1d">italic_β</annotation></semantics></math>=0.74, S.E.=0.14, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p3.3.1">p</span>¡0.001) and cognitive engagement (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.2.m2.1"><semantics id="S6.SS2.SSS1.p3.2.m2.1a"><mi id="S6.SS2.SSS1.p3.2.m2.1.1" xref="S6.SS2.SSS1.p3.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p3.2.m2.1b"><ci id="S6.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S6.SS2.SSS1.p3.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p3.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p3.2.m2.1d">italic_β</annotation></semantics></math>=0.53, S.E.=0.14, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p3.3.2">p</span>¡0.001) in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p3.3.3">Narrative session</span> and their perceived believability for the character’s responses in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p3.3.4">Recommendation session</span>. This indicated that both emotional and cognitive engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p3.3.5">Narrative session</span> played critical roles in shaping participants’ trust in the character’s recommendations. Participants also reflected on this during their think-aloud process. For example, P19 illustrated this sentiment, stating <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p3.3.6">”Generally, when I use these characters, if they give me incorrect analysis, I will tolerate it and use them a second time. But if they are wrong again, I will never use them again and won’t trust their recommendations anymore.”</span>. No statistically significant association was found between behavioral engagement (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.3.m3.1"><semantics id="S6.SS2.SSS1.p3.3.m3.1a"><mi id="S6.SS2.SSS1.p3.3.m3.1.1" xref="S6.SS2.SSS1.p3.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p3.3.m3.1b"><ci id="S6.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S6.SS2.SSS1.p3.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p3.3.m3.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p3.3.m3.1d">italic_β</annotation></semantics></math>=0.01, S.E.=0.01) and believability.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p4">
<p class="ltx_p" id="S6.SS2.SSS1.p4.1">Third, stereotypicality was found to be associated with interaction time (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p4.1.m1.1"><semantics id="S6.SS2.SSS1.p4.1.m1.1a"><mi id="S6.SS2.SSS1.p4.1.m1.1.1" xref="S6.SS2.SSS1.p4.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p4.1.m1.1b"><ci id="S6.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p4.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p4.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p4.1.m1.1d">italic_β</annotation></semantics></math>=0.04, S.E.=0.02, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p4.1.1">p</span>¡0.05), indicating that the brevity of interactions in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p4.1.2">Recommendation session</span> may not have provided sufficient time for users to perceive differences in stereotypicality. Participant reflections support this finding. For example, P24 noted, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p4.1.3">“I didn’t really notice much difference in how stereotypical the characters were during the short recommendation task, but when I spent more time with them in the Narrative task, those differences became more obvious.”</span>.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Regression Results with outcomes satisfaction ratings as dependent variables and users’ engagement data as predictors.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S6.T4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.2.3.1">
<td class="ltx_td ltx_align_top ltx_border_t" id="S6.T4.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2" id="S6.T4.2.3.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.2.3.1.2.1">Dependent Variables — Outcomes</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.4.2">
<td class="ltx_td ltx_align_top" id="S6.T4.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_align_top" colspan="2" id="S6.T4.2.4.2.2"><span class="ltx_text ltx_font_bold" id="S6.T4.2.4.2.2.1">(Recommendation Ratings)</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.5.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.2.5.3.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.5.3.1.1">
<span class="ltx_p" id="S6.T4.2.5.3.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.5.3.1.1.1.1">Predictors</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T4.2.5.3.2"><span class="ltx_text ltx_font_bold" id="S6.T4.2.5.3.2.1">Image</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T4.2.5.3.3"><span class="ltx_text ltx_font_bold" id="S6.T4.2.5.3.3.1">Reasons</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.6.4">
<td class="ltx_td ltx_align_top" id="S6.T4.2.6.4.1"></td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T4.2.6.4.2"><span class="ltx_text ltx_font_bold" id="S6.T4.2.6.4.2.1">Satisfaction</span></td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T4.2.6.4.3"><span class="ltx_text ltx_font_bold" id="S6.T4.2.6.4.3.1">Satisfaction</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2">
<td class="ltx_td ltx_align_top" id="S6.T4.2.2.3"></td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T4.1.1.1">
<math alttext="\beta" class="ltx_Math" display="inline" id="S6.T4.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.m1.1a"><mi id="S6.T4.1.1.1.m1.1.1" xref="S6.T4.1.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.m1.1d">italic_β</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1">(S.E.)</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T4.2.2.2">
<math alttext="\beta" class="ltx_Math" display="inline" id="S6.T4.2.2.2.m1.1"><semantics id="S6.T4.2.2.2.m1.1a"><mi id="S6.T4.2.2.2.m1.1.1" xref="S6.T4.2.2.2.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.m1.1b"><ci id="S6.T4.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.m1.1d">italic_β</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.1">(S.E.)</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.7.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T4.2.7.5.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.7.5.1.1">
<span class="ltx_p" id="S6.T4.2.7.5.1.1.1" style="width:85.4pt;">Behavioral Engagement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T4.2.7.5.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.7.5.2.1">
<span class="ltx_p" id="S6.T4.2.7.5.2.1.1" style="width:56.9pt;">0.03 (0.01)***</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S6.T4.2.7.5.3">0.03 (0.02)</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.8.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.2.8.6.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.8.6.1.1">
<span class="ltx_p" id="S6.T4.2.8.6.1.1.1" style="width:85.4pt;">Emotional Engagement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.2.8.6.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.8.6.2.1">
<span class="ltx_p" id="S6.T4.2.8.6.2.1.1" style="width:56.9pt;">0.39 (0.10)***</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T4.2.8.6.3">-0.03 (0.22)</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.9.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.2.9.7.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.9.7.1.1">
<span class="ltx_p" id="S6.T4.2.9.7.1.1.1" style="width:85.4pt;">Cognitive Engagement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.2.9.7.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.2.9.7.2.1">
<span class="ltx_p" id="S6.T4.2.9.7.2.1.1" style="width:56.9pt;">0.53 (0.10)***</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top" id="S6.T4.2.9.7.3">0.36 (0.23)</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.10.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="3" id="S6.T4.2.10.8.1">
<span class="ltx_text ltx_font_italic" id="S6.T4.2.10.8.1.1">Note</span>: * <span class="ltx_text ltx_font_italic" id="S6.T4.2.10.8.1.2">p</span>¡0.05, ** <span class="ltx_text ltx_font_italic" id="S6.T4.2.10.8.1.3">p</span>¡0.01, *** <span class="ltx_text ltx_font_italic" id="S6.T4.2.10.8.1.4">p</span>¡0.001</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.1.1">Satisfactions with generated recommended images were associated with engagement factors in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.1.1.1">Narrative session</span>.</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.3">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.T4" title="Table 4 ‣ 6.2.1. Perceived consistency in the recommendation session was associated with cognitive engagement in the narrative session and stereotypicality was associated with behavioral engagement. ‣ 6.2. Across-session Association of Engagement Factors in the Narrative Session (RQ2b) - System Log, Survey 1&amp;2, Think-Aloud ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">4</span></a>, a positive association was identified between recommended image satisfaction ratings in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.1">Recommandation session</span> and behavioral engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.2">Narrative session</span> (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p1.1.m1.1"><semantics id="S6.SS2.SSS2.p1.1.m1.1a"><mi id="S6.SS2.SSS2.p1.1.m1.1.1" xref="S6.SS2.SSS2.p1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.1.m1.1b"><ci id="S6.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p1.1.m1.1d">italic_β</annotation></semantics></math>=0.03, S.E.=0.01, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.3">p</span>¡0.001). Additionally, emotional engagement (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p1.2.m2.1"><semantics id="S6.SS2.SSS2.p1.2.m2.1a"><mi id="S6.SS2.SSS2.p1.2.m2.1.1" xref="S6.SS2.SSS2.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.2.m2.1b"><ci id="S6.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS2.SSS2.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p1.2.m2.1d">italic_β</annotation></semantics></math>=0.39, S.E.=0.10, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.4">p</span>¡0.001) and cognitive engagement (<math alttext="\beta" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p1.3.m3.1"><semantics id="S6.SS2.SSS2.p1.3.m3.1a"><mi id="S6.SS2.SSS2.p1.3.m3.1.1" xref="S6.SS2.SSS2.p1.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.3.m3.1b"><ci id="S6.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS2.SSS2.p1.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.3.m3.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p1.3.m3.1d">italic_β</annotation></semantics></math>=0.53, S.E.=0.10, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.5">p</span>¡0.001) in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.6">Narrative session</span> were also significantly associated with satisfaction ratings for the recommendation images. These findings suggested that participants who were more engaged during the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.7">Narrative session</span> perceived the outcome of the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.3.8">Recommendation session</span> as higher in quality.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1">In contrast, satisfaction with recommendation reasons showed weaker associations with engagement factors, and all were non-significant. This suggested that participants’ engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p2.1.1">Narrative session</span> did not influence how satisfied they were with the explanations given for the recommendations.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p3">
<p class="ltx_p" id="S6.SS2.SSS2.p3.1">Participants provided additional insight into these findings during their think-aloud process. Some participants, such as P3, P6, P16, and P19, expressed a preference for straightforward, factual explanations—whether object-based, stylistic, or author-centric—indicating that they did not require detailed justifications. This feedback suggested that for some users, a deep familiarity with art might reduced the need for recommendation reasons. However, other participants noted that the explanations were often too superficial, citing descriptions like ”the two paintings share the same style” as overly generic and uninformative. These superficial explanations could potentially erode trust when factual inaccuracies were present.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p4">
<p class="ltx_p" id="S6.SS2.SSS2.p4.1">Participants also suggested that the effectiveness of the recommendations could be improved by aligning the types of recommendations with specific characters. For example, the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p4.1.1">Artist character</span> could present recommendations based on the same artist, while Narrator could deliver composition-based recommendations. <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p4.1.2">In-Situ character</span> could introduce Object-specific recommendations. This feedback highlighted the importance of character differentiation in enhancing the relevance and appeal of the recommendations.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p5">
<p class="ltx_p" id="S6.SS2.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p5.1.1">Summary-RQ2b:</span> Our findings demonstrated participants’ engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.2">Narrative session</span> was associated with their perceptions of character responses in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.3">Recommendation session</span>. Specifically, we identified three key associations between engagement factors and character response perception. First, cognitive engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.4">Narrative session</span> was positively associated with perceived consistency in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.5">Recommendation session</span>. This helped explain the improvement in the perceived response consistency of <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.6" style="color:#FF8000;">Artist character</span> in Session <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.1</span></a>, as users who were more cognitively engaged during the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.7">Narrative session</span> likely viewed the character’s recommendations as more coherent and aligned. Second, both emotional and cognitive engagement in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.8">Narrative session</span> was related to believability in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p5.1.9">Recommendation session</span>. Third, interaction time was associated with stereotypicality, which suggested the brief recommendation interactions were not sufficient for users to perceive differences in stereotypicality of <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S6.SS1" title="6.1. The Evolution of Character Response Perception in the Two Sessions (RQ2a)- Survey 1&amp;2 ‣ 6. Results across the Narrative and Recommendation Sessions ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">6.1</span></a> between characters.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>DISCUSSION</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">A major contribution of our work is to demonstrate that multi-character interaction–<span class="ltx_text ltx_font_italic" id="S7.p1.1.1" style="color:#0000FF;">Narrator</span>, <span class="ltx_text ltx_font_italic" id="S7.p1.1.2" style="color:#FF8000;">Artist</span>, and <span class="ltx_text ltx_font_italic" id="S7.p1.1.3" style="color:#00FF00;">In-Situ</span>–in art appreciation systems can enhance user engagement in the <span class="ltx_text ltx_font_italic" id="S7.p1.1.4">Narrative session</span>. These engagement factors in the <span class="ltx_text ltx_font_italic" id="S7.p1.1.5">Narrative session</span> were associated with participants’ perceptions of the character responses in the <span class="ltx_text ltx_font_italic" id="S7.p1.1.6">Recommendation session</span>. By fostering meaningful interactions using first-person anthropomorphic narratives, the characters shaped users’ perceptions (i.e. satisfaction, reliability, and trust) of their recommendations in the <span class="ltx_text ltx_font_italic" id="S7.p1.1.7">Recommendation session</span>. In turn, users’ expectations of characters’ ability to complete tasks and the fitness between characters and tasks will also shape users’ perception of characters.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>First-Person Perspective: A Double-Edged Sword for User Engagement and Critical Response</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Regarding RQ 1, which investigates how users <span class="ltx_text ltx_framed ltx_framed_underline" id="S7.SS1.p1.1.1">perceive</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S7.SS1.p1.1.2">engage</span> with anthropomorphic characters in three modes differently in the <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.3">Narrative session</span>, our findings suggest that the overall perceived engagement in terms of aesthetic appeal and perceived usability varied across the three modes. This variation is further interpreted through the lenses of behavioral, emotional, and cognitive engagement during user interactions.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">For aesthetic appeal, both the <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1">In-Situ</span> and <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.2">Artist modes</span> were rated higher than the <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.3">Narrator mode</span>. This aligns with previous research <cite class="ltx_cite ltx_citemacro_citep">(Busselle and Bilandzic, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib15" title="">2009</a>; Salem et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib81" title="">2017</a>; Samur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib84" title="">2021</a>; Brennan, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib13" title="">2024</a>)</cite>
, which suggested that first-person narratives in interactive systems can effectively enhance the aesthetic appeal of the systems. Our study contributes to the existing literature by demonstrating that users’ emotional engagement play a key role in shaping their perception of system aesthetic appeal. Participants in the first-person <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.4">In-Situ mode</span> displayed the highest level of interaction time, posed the most questions, and experienced the greatest hedonic value. This immersive mode innovated the perspective and identity through which information was presented, allowing participants to experience elements like time travel (EE1), empathy (EE2), and anthropomorphism (EE3) without a need for an expanded knowledge base. These features contributed to a sense of immersion, suggesting that future <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.5">In-Situ characters</span> equipped with a more robust knowledge base might yield even greater engagement.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">Interestingly, <span class="ltx_text ltx_font_italic" id="S7.SS1.p3.1.1">Artist mode</span> was scored lower on perceived usability compared to <span class="ltx_text ltx_font_italic" id="S7.SS1.p3.1.2">Narrator</span> and <span class="ltx_text ltx_font_italic" id="S7.SS1.p3.1.3">In-Situ mode</span>. The findings suggest that the character’s identity in this mode plays a crucial role in shaping the perceived usability. Two key cognitive factors may explain this. First, users often expect the artist to provide unique insights only the artist knows, such as the details about the artwork’s creative process. According to the Expectation-Confirmation Model (ECM)<cite class="ltx_cite ltx_citemacro_citep">(Bhattacherjee, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib10" title="">2001</a>)</cite>, when users’ high expectations are not fully met, the high expectations might lead to lower perceived usefulness and usability, especially through first-person narratives. Second, users tend to view the artist as a professional and authority, meaning their tolerance for mistakes or inconsistencies of such character is lower. This could be explained using Authority Theory <cite class="ltx_cite ltx_citemacro_citep">(Pace and Hemmings, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib76" title="">2006</a>; Komorowska-Mach and Szczepura, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib54" title="">2021</a>)</cite>, which suggested that when a person makes statements about their intentions or beliefs, others might be more inclined to question or be less certain about the authority of those statements compared to statements about inner phenomenal states (such as sensations and emotions). Therefore, we suggest future research to deploy different types of experts in the diverse education settings, to explore the impact of potential expert character information on users’ use of the system.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Influence of Task Type on Role Perception: Unpacking Consistency and Stereotypes</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Regarding RQ2, which explores how users <span class="ltx_text ltx_framed ltx_framed_underline" id="S7.SS2.p1.1.1">perception</span> evolve between <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.2">Narrative</span> and <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.3">Recommendation session</span>s and the across-session <span class="ltx_text ltx_framed ltx_framed_underline" id="S7.SS2.p1.1.4">association</span> of the engagement factors in the <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.5">Narrative session</span>, our results suggest that tasks types had a significant influence on users’ character perception (i.e., consistency and stereotypicality) across art <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.6">Narrative session</span> and art <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.7">Recommendation session</span>.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">For consistency, the <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1">In-Situ characters</span> consistently achieved the highest scores across both sessions. In contrast, the response consistency score of the <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.2">Artist character</span> improved from the <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.3">Narrator session</span> to <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.4">Recommendation session</span>, which might also be explained by the Expectation-Confirmation Model (ECM) <cite class="ltx_cite ltx_citemacro_citep">(Bhattacherjee, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib10" title="">2001</a>)</cite>. First, because the <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.5">Artist character</span> provided new epistemic values aligned with users’ expectations during the <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.6">Recommendation session</span> and there was a correlation between users’ perception of the characters’ response consistency and their cognitive engagement during interactions. Secondly, users expressed that it was natural and appropriate for the <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.7">Artist character</span> to recommend their works, related artworks, or those from the same period. They found it seems appropriate for <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.8">In-Situ characters</span> to provide object-based recommendations. This expands the concept of character consistency error described by Welleck et al. <cite class="ltx_cite ltx_citemacro_citep">(Welleck et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib105" title="">2018</a>)</cite>, which defines a consistency error as an utterance unlikely to be made by a character defined by a specific set of traits. Here, users’ perceptions of character consistency encompass not just the relationship between the character and the information, but also the consistency between the character’s role and the task at hand. Third, these shifts suggest that as users moved from a somehow subjective task to an objective task, their expectations of character responses shifted, particularly regarding how consistent the character responses were perceived in their recommendations.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">For stereotypicality, we found that the <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.1">In-Situ characters</span> showed lower character stereotype in the <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.2">Narrative session</span>, but there was no statistical difference between the three character stereotype perceptions in the <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.3">Recommendation session</span>. According to the regression analysis results, users’ perception of character stereotypes was associated with their behavioral engagement (interaction time) during interactions. This could be due to the fact that <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.4">Narrative session</span> involved a prolonged, multi-interaction process, whereas the <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.5">Recommendation</span> involved a single interaction. This finding aligns with previous research findings <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib56" title="">2020</a>)</cite>, that interaction time significantly affects the relationship building between humans and chatbots. Furthermore, according to <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#S5.SS2.SSS1" title="5.2.1. Behavioral Engagement: When Interacting with the In-Situ Mode, Users’ Interaction Time was Longer, and They Asked More Questions Proactively. ‣ 5.2. Users’ Behavioral, Emotional and Cognitive Engagement with the Characters (RQ1b) - System Log, Survey 1, Think-Aloud ‣ 5. Results of the Narrative Session ‣ In-Situ Mode: Generative AI-Driven Characters Transforming Art Engagement Through Anthropomorphic Narratives"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>, users interacting with <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.6">In-Situ characters</span> were more willing to ask questions because they felt surprised as the <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.7">In-Situ character</span> provided unexpected information during multiple narrative interactions, that broke the stereotype. This is consistent with observations of Ha et al. <cite class="ltx_cite ltx_citemacro_citep">(Ha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib35" title="">2024</a>)</cite>, who noted that conversations tended to be longer when users found them more engaging. Thus, these results suggest that stereotypes can be mitigated by enhancing engagement and prolonging interaction time.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Design Implications</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">Based on our discussions, we propose the following design implications for art appreciation and diverse education settings.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1. </span><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS1.1.1">Art Appreciation</span>
</h4>
<div class="ltx_para" id="S7.SS3.SSS1.p1">
<p class="ltx_p" id="S7.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS1.p1.1.1">Leverage <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS1.p1.1.1.1">In-Situ Character</span> Interactions to Enhance Art Appreciation</span>
While previous research has proposed various mechanisms behind the benefits of art appreciation <cite class="ltx_cite ltx_citemacro_citep">(Fancourt and Finn, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib29" title="">2019</a>; Mastandrea et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib66" title="">2019a</a>)</cite>, experimental evidence remains limited <cite class="ltx_cite ltx_citemacro_citep">(Trupp et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib99" title="">2023</a>)</cite>. Our findings demonstrate the unique potential of <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS1.p1.1.2">In-Situ character</span> interactions in enhancing user engagement, extending beyond traditional digital technologies. While short art appreciation sessions have been proved to offer mental and physical benefits for users <cite class="ltx_cite ltx_citemacro_citep">(Clow and Fredhoi, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib22" title="">2006</a>; Mastandrea et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib67" title="">2019b</a>; Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib40" title="">2015</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS1.p1.1.3">EyeSee</span> allows for more sustained aesthetic experiences, with characters serving as novel mediators of artistic communication <cite class="ltx_cite ltx_citemacro_citep">(Dervin and Tian, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib27" title="">2023</a>; Silva, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib95" title="">2006</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p2">
<p class="ltx_p" id="S7.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS1.p2.1.1">Expand Virtual Art Spaces with Immersive <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS1.p2.1.1.1">In-Situ Character</span> Design.</span>
Incorporating anthropomorphic characters in digital art systems not only enhances user engagement but also introduces a new interaction medium for the artistic community. Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib104" title="">2024</a>)</cite> introduced VirtuWander, a voice-controlled virtual museum prototype that highlights five interaction designs (Voice, Avatar, Text Window, Highlight, Virtual Screen), demonstrating the value of single-character systems. Adding <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS1.p2.1.2">In-Situ characters</span> in virtual spaces can further enrich the design space and provide a deeper and more immersive art appreciation experience.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2. </span><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS2.1.1">Diverse Educational Settings</span>
</h4>
<div class="ltx_para" id="S7.SS3.SSS2.p1">
<p class="ltx_p" id="S7.SS3.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS2.p1.1.1">Prioritize <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS2.p1.1.1.1">In-Situ Mode</span> to Enhance User Engagement and Trust.</span>
Meta-analyses show agents have a small, positive effect on learning <cite class="ltx_cite ltx_citemacro_citep">(Schroeder et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib87" title="">2013</a>)</cite>, but a systematic review found no significant differences <cite class="ltx_cite ltx_citemacro_citep">(Heidig and Clarebout, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib38" title="">2011</a>)</cite>. Our results indicate that emotional engagement in the <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS2.p1.1.2">In-Situ mode</span> was significantly higher than the other two modes in art appreciation. suggesting future systems could prioritize this mode to analyze the impact of <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS2.p1.1.3">In-Situ mode</span> on learning outcomes. Additionally, we found this emotional engagement correlated strongly with perceptions of believability in the <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS2.p1.1.4">Recommendation session</span>. This inspires researchers to focus on relationship building through interactive recommendation in learning environments where virtual teachers are required to provide advice, to improve users’ satisfaction and trust in information.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS2.p2">
<p class="ltx_p" id="S7.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS2.p2.1.1">Use the First-person Sparingly in Authority Roles.</span>
First-person characters, while engaging, may be perceived as overly authoritative, negatively impacting system usability. Research shows a tension between authoritative and dialogic approaches, as the introduction of new ideas is authoritative to support learning <cite class="ltx_cite ltx_citemacro_citep">(Scott et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib88" title="">2006</a>)</cite>. For authority figures, using first-person sparingly could prevent this tension <cite class="ltx_cite ltx_citemacro_citep">(Marsh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib65" title="">2014</a>)</cite>.
This has inspired the use of character perspective in some online learning scenarios that may lead users to develop a sense of authority.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS2.p3">
<p class="ltx_p" id="S7.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS2.p3.1.1">Make the Character Adaptive (Objective and Subjective).</span>
Our findings show that characters are perceived differently across tasks, suggesting that adaptive algorithms should adjust character behavior based on both the learning goals and the context in which learning occurs. Subjective and objective education emphasize different values <cite class="ltx_cite ltx_citemacro_citep">(Sanasintani and Munte, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib85" title="">2024</a>)</cite>. Therefore, combining our conclusions about hedonic and epistemic values can inspire research on using characters in two relatively distinct disciplines, such as STEM and history.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4. </span>Limitations and Future Research</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">Our study, while shedding light on the diverse user art appreciation experiences with diverse characters in LLMs, has several limitations that must be acknowledged. Firstly, our participant pool consisted exclusively of art enthusiasts, which may influence the generalizability of our findings to broader populations. Future studies should include participants with varying levels of art interest to broaden the applicability of the results. Secondly, the characters creation on the <span class="ltx_text ltx_font_italic" id="S7.SS4.p1.1.1">EyeSee</span> system relied on prompt injection, which may lack depth in specialized domains <cite class="ltx_cite ltx_citemacro_citep">(Ankush et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib4" title="">2023</a>)</cite>. Future research could explore fine-tuning techniques <cite class="ltx_cite ltx_citemacro_citep">(Nguyen-Mau et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib74" title="">2024</a>; Shrestha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib93" title="">2024</a>)</cite> or external memory integration <cite class="ltx_cite ltx_citemacro_citep">(Salminen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib82" title="">2018</a>; Ha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15769v1#bib.bib35" title="">2024</a>)</cite> to improve character customization and enhance personalized art experience. Thirdly, personalization of the art experience could be enhanced by allowing users to select artwork they are interested in, offering greater flexibility and personalization. Future work should also explore alternative interaction methods, such as mobile apps or VR, to improve engagement. Lastly, we did not examine long-term user experiences with <span class="ltx_text ltx_font_italic" id="S7.SS4.p1.1.2">EyeSee</span>. Future research should conduct longitudinal studies to explore how user interactions evolve over time, providing deeper insights into sustained engagement with conversational agents like <span class="ltx_text ltx_font_italic" id="S7.SS4.p1.1.3">EyeSee</span>.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This project was made possible in part by the Institute of Museum and Library Services RE-252329-OLS-22. The views, findings, conclusions or recommendations expressed in this article do not necessarily represent those of the Institute of Museum and Library Services.

</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acharya (2023)</span>
<span class="ltx_bibblock">
Bipin Acharya. 2023.

</span>
<span class="ltx_bibblock">Applicability of Visual Arts in the Age of Globalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Cognition</em> 5, 1 (2023), 6–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Alami (2019)</span>
<span class="ltx_bibblock">
Suhair Al-Alami. 2019.

</span>
<span class="ltx_bibblock">Point of view in narrative.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Theory and Practice in Language Studies</em> 9, 8 (2019), 911–916.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ankush et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Agarwal Ankush, Gawade Sakharam, Azad Amar Prakash, and Bhattacharyya Pushpak. 2023.

</span>
<span class="ltx_bibblock">KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 20th International Conference on Natural Language Processing (ICON)</em>. 202–294.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arguedas and Daradoumis (2021)</span>
<span class="ltx_bibblock">
Marta Arguedas and Thanasis Daradoumis. 2021.

</span>
<span class="ltx_bibblock">Analysing the role of a pedagogical agent in psychological and cognitive preparatory activities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Journal of Computer Assisted Learning</em> 37, 4 (2021), 1167–1180.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnard (1998)</span>
<span class="ltx_bibblock">
Malcolm Barnard. 1998.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Art, design and visual culture: An introduction</em>.

</span>
<span class="ltx_bibblock">Bloomsbury Publishing.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belfiore and Bennett (2007)</span>
<span class="ltx_bibblock">
Eleonora Belfiore and Oliver Bennett. 2007.

</span>
<span class="ltx_bibblock">Determinants of impact: Towards a better understanding of encounters with the arts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Cultural trends</em> 16, 3 (2007), 225–275.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben-Eliyahu et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Adar Ben-Eliyahu, Debra Moore, Rena Dorph, and Christian D Schunn. 2018.

</span>
<span class="ltx_bibblock">Investigating the multidimensionality of engagement: Affective, behavioral, and cognitive engagement across science activities and contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Contemporary Educational Psychology</em> 53 (2018), 87–105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Best (2012)</span>
<span class="ltx_bibblock">
Katie Best. 2012.

</span>
<span class="ltx_bibblock">Making museum tours better: Understanding what a guided tour really is and what a tour guide really does.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Museum Management and Curatorship</em> 27, 1 (2012), 35–52.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattacherjee (2001)</span>
<span class="ltx_bibblock">
Anol Bhattacherjee. 2001.

</span>
<span class="ltx_bibblock">Understanding information systems continuance: An expectation-confirmation model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">MIS quarterly</em> (2001), 351–370.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bitrián et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Paula Bitrián, Isabel Buil, and Sara Catalán. 2021.

</span>
<span class="ltx_bibblock">Enhancing user engagement: The role of gamification in mobile apps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Journal of Business Research</em> 132 (2021), 170–185.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun and Clarke (2006)</span>
<span class="ltx_bibblock">
Virginia Braun and Victoria Clarke. 2006.

</span>
<span class="ltx_bibblock">Using thematic analysis in psychology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Qualitative research in psychology</em> 3, 2 (2006), 77–101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brennan (2024)</span>
<span class="ltx_bibblock">
Eric B Brennan. 2024.

</span>
<span class="ltx_bibblock">“I” versus “the author”: The power of first-person voice when writing about science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the National Academy of Sciences</em> 121, 22 (2024), e2316966121.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bullot and Reber (2013)</span>
<span class="ltx_bibblock">
Nicolas J Bullot and Rolf Reber. 2013.

</span>
<span class="ltx_bibblock">The artful mind meets art history: Toward a psycho-historical framework for the science of art appreciation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Behavioral and brain sciences</em> 36, 2 (2013), 123–137.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Busselle and Bilandzic (2009)</span>
<span class="ltx_bibblock">
Rick Busselle and Helena Bilandzic. 2009.

</span>
<span class="ltx_bibblock">Measuring narrative engagement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Media psychology</em> 12, 4 (2009), 321–347.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Joseph Chee Chang, Amy X Zhang, Jonathan Bragg, Andrew Head, Kyle Lo, Doug Downey, and Daniel S Weld. 2023.

</span>
<span class="ltx_bibblock">Citesee: Augmenting citations in scientific papers with persistent and personalized historical context. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, et al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">From persona to personalization: A survey on role-playing language agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">arXiv preprint arXiv:2404.18231</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Bunescu (2021)</span>
<span class="ltx_bibblock">
Mike Chen and Razvan Bunescu. 2021.

</span>
<span class="ltx_bibblock">Changing the narrative perspective: From deictic to anaphoric point of view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Information Processing &amp; Management</em> 58, 4 (2021), 102559.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiu et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Min-Chi Chiu, Gwo-Jen Hwang, Lu-Ho Hsia, and Fong-Ming Shyu. 2024.

</span>
<span class="ltx_bibblock">Artificial intelligence-supported art education: A deep learning-based system for promoting university students’ artwork appreciation and painting outcomes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Interactive Learning Environments</em> 32, 3 (2024), 824–842.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cila et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Nazli Cila, Elisa Giaccardi, Fionn Tynan-O’Mahony, Chris Speed, and Melissa Caldwell. 2015.

</span>
<span class="ltx_bibblock">Thing-Centered Narratives: A study of object personas. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 3rd seminar international research network for design anthropology</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ciotoli et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Luca Ciotoli, Morteza Alinam, and Ilaria Torre. 2021.

</span>
<span class="ltx_bibblock">Augmented museum experience through Tangible Narrative. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 20th International Conference on Mobile and Ubiquitous Multimedia</em>. 214–216.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clow and Fredhoi (2006)</span>
<span class="ltx_bibblock">
Angela Clow and Cathrine Fredhoi. 2006.

</span>
<span class="ltx_bibblock">Normalisation of salivary cortisol levels and self-report stress by a brief lunchtime visit to an art gallery by London City workers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Journal of Holistic Healthcare</em> 3, 2 (2006), 29–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohn et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Michelle Cohn, Mahima Pushkarna, Gbolahan O Olanubi, Joseph M Moran, Daniel Padgett, Zion Mengesha, and Courtney Heldreth. 2024.

</span>
<span class="ltx_bibblock">Believing Anthropomorphism: Examining the Role of Anthropomorphic Cues on Trust in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cole (1998)</span>
<span class="ltx_bibblock">
Michael Cole. 1998.

</span>
<span class="ltx_bibblock">Can cultural psychology help us think about diversity?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Mind, culture, and activity</em> 5, 4 (1998), 291–304.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coskun et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aykut Coskun, Nazli Cila, Iohanna Nicenboim, Christopher Frauenberger, Ron Wakkary, Marc Hassenzahl, Clara Mancini, Elisa Giaccardi, and Laura Forlano. 2022.

</span>
<span class="ltx_bibblock">More-than-human Concepts, Methodologies, and Practices in HCI. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">CHI Conference on Human Factors in Computing Systems Extended Abstracts</em>. 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danielczuk et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew Lee, Jeffrey Mahler, and Ken Goldberg. 2019.

</span>
<span class="ltx_bibblock">Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dervin and Tian (2023)</span>
<span class="ltx_bibblock">
Fred Dervin and Xiaowen Tian. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Critical and reflective intercultural communication education: Practicing interculturality through visual Art</em>.

</span>
<span class="ltx_bibblock">Springer Nature.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Durlak et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Joseph A Durlak, Roger P Weissberg, Allison B Dymnicki, Rebecca D Taylor, and Kriston B Schellinger. 2011.

</span>
<span class="ltx_bibblock">The impact of enhancing students’ social and emotional learning: A meta-analysis of school-based universal interventions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Child development</em> 82, 1 (2011), 405–432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fancourt and Finn (2019)</span>
<span class="ltx_bibblock">
Daisy Fancourt and Saoirse Finn. 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">What is the evidence on the role of the arts in improving health and well-being? A scoping review</em>.

</span>
<span class="ltx_bibblock">World Health Organization. Regional Office for Europe.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman (1994)</span>
<span class="ltx_bibblock">
Edmund Burke Feldman. 1994.

</span>
<span class="ltx_bibblock">Practical art criticism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">(No Title)</em> (1994).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2024.

</span>
<span class="ltx_bibblock">Towards revealing the mystery behind chain of thought: a theoretical perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabrielli et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Silvia Gabrielli, Silvia Rizzi, Giulia Bassi, Sara Carbone, Rosa Maimone, Michele Marchesoni, and Stefano Forti. 2021.

</span>
<span class="ltx_bibblock">Engagement and effectiveness of a healthy-coping intervention via chatbot for university students during the COVID-19 pandemic: mixed methods proof-of-concept study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">JMIR mHealth and uHealth</em> 9, 5 (2021), e27965.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaztambide-Fernández (2008)</span>
<span class="ltx_bibblock">
Rubén A Gaztambide-Fernández. 2008.

</span>
<span class="ltx_bibblock">The artist in society: Understandings, expectations, and curriculum implications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Curriculum inquiry</em> 38, 3 (2008), 233–265.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gollapalli et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sujatha Das Gollapalli, Mingzhe Du, and See-Kiong Ng. 2023.

</span>
<span class="ltx_bibblock">Generating Reflective Questions for Engaging Gallery Visitors in ArtMuse. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 37. 16434–16436.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Juhye Ha, Hyeon Jeon, Daeun Han, Jinwook Seo, and Changhoon Oh. 2024.

</span>
<span class="ltx_bibblock">CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Nicolai Brodersen Hansen, Christian Dindler, Kim Halskov, Ole Sejer Iversen, Claus Bossen, Ditte Amund Basballe, and Ben Schouten. 2019.

</span>
<span class="ltx_bibblock">How participatory design works: mechanisms and effects. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 31st Australian Conference on Human-Computer-Interaction</em>. 30–41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hayashi et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kazuki Hayashi, Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, and Taro Watanabe. 2024.

</span>
<span class="ltx_bibblock">Artwork Explanation in Large-scale Vision Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv preprint arXiv:2403.00068</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heidig and Clarebout (2011)</span>
<span class="ltx_bibblock">
Steffi Heidig and Geraldine Clarebout. 2011.

</span>
<span class="ltx_bibblock">Do pedagogical agents make a difference to student motivation and learning?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Educational Research Review</em> 6, 1 (2011), 27–54.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hein (2002)</span>
<span class="ltx_bibblock">
George E Hein. 2002.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Learning in the Museum</em>.

</span>
<span class="ltx_bibblock">routledge.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Rainbow TH Ho, Jordan S Potash, Fan Fang, and Judy Rollins. 2015.

</span>
<span class="ltx_bibblock">Art viewing directives in hospital settings effect on mood.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">HERD: Health Environments Research &amp; Design Journal</em> 8, 3 (2015), 30–43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holub (2013)</span>
<span class="ltx_bibblock">
Robert C Holub. 2013.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Reception theory</em>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooper-Greenhill (2000)</span>
<span class="ltx_bibblock">
Eilean Hooper-Greenhill. 2000.

</span>
<span class="ltx_bibblock">Changing values in the art museum: Rethinking communication and learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">International journal of heritage studies</em> 6, 1 (2000), 9–31.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu and Collier (2024)</span>
<span class="ltx_bibblock">
Tiancheng Hu and Nigel Collier. 2024.

</span>
<span class="ltx_bibblock">Quantifying the Persona Effect in LLM Simulations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.10811" title="">http://arxiv.org/abs/2402.10811</a>
</span>
<span class="ltx_bibblock">arXiv:2402.10811 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hung et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mai Cong Hung, Ryohei Nakatsu, Naoko Tosa, and Takashi Kusumi. 2022.

</span>
<span class="ltx_bibblock">Learning of art style using AI and its evaluation based on psychological experiments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">International Journal of Arts and Technology</em> 14, 3 (2022), 171–191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilić (2019)</span>
<span class="ltx_bibblock">
Vojislav Ilić. 2019.

</span>
<span class="ltx_bibblock">Information and communication technology in visual art education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">University of Pristina publisher, Kosovska Mitrovica</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jensen (2021)</span>
<span class="ltx_bibblock">
Theodore Jensen. 2021.

</span>
<span class="ltx_bibblock">Disentangling trust and anthropomorphism toward the design of human-centered AI systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">International Conference on Human-Computer Interaction</em>. Springer, 41–58.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">IEEE Transactions on Big Data</em> 7, 3 (2019), 535–547.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaufman and Libby (2012)</span>
<span class="ltx_bibblock">
Geoff F Kaufman and Lisa K Libby. 2012.

</span>
<span class="ltx_bibblock">Changing beliefs and behavior through experience-taking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Journal of personality and social psychology</em> 103, 1 (2012), 1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kemp (1998)</span>
<span class="ltx_bibblock">
Wolfgang Kemp. 1998.

</span>
<span class="ltx_bibblock">The work of art and its beholder. The methodology of the aesthetics of reception.

</span>
<span class="ltx_bibblock">(1998).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktäschel, and Ethan Perez. 2024.

</span>
<span class="ltx_bibblock">Debating with more persuasive llms leads to more truthful answers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2402.06782</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nuri Kim, Hye Kyung Kim, Magdalena Wojcieszak, Juan-José Igartua, and Cui Min Lim. 2020.

</span>
<span class="ltx_bibblock">The presence of the protagonist: Explaining narrative perspective effects through social presence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Media Psychology</em> 23, 6 (2020), 891–914.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Seungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, and Jinyoung Yeo. 2023.

</span>
<span class="ltx_bibblock">CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</em>, Danilo Croce and Luca Soldaini (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 195–208.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.eacl-demo.23" title="">https://doi.org/10.18653/v1/2023.eacl-demo.23</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al<span class="ltx_text" id="bib.bib53.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Segment anything. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.4.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 4015–4026.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komorowska-Mach and Szczepura (2021)</span>
<span class="ltx_bibblock">
Joanna Komorowska-Mach and Andrzej Szczepura. 2021.

</span>
<span class="ltx_bibblock">First-Person Authority Through the Lens of Experimental Philosophy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Filozofia Nauki</em> 29, 2 (2021), 209–227.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Unggi Lee, Minji Jeon, Yunseo Lee, Gyuri Byun, Yoorim Son, Jaeyoon Shin, Hongkyu Ko, and Hyeoncheol Kim. 2024.

</span>
<span class="ltx_bibblock">LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">arXiv preprint arXiv:2402.06264</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yi-Chieh Lee, Naomi Yamashita, Yun Huang, and Wai Fu. 2020.

</span>
<span class="ltx_bibblock">” I hear you, I feel you”: encouraging deep self-disclosure through a chatbot. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Proceedings of the 2020 CHI conference on human factors in computing systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al<span class="ltx_text" id="bib.bib57.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Grounded language-image pre-training. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.4.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10965–10975.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Lin Liu, Shizhu Lu, Yuqing Guo, Qiuyu Huang, Xiaolie Yi, and Jifa Zhang. 2024a.

</span>
<span class="ltx_bibblock">Analysis of the Impact on Immersive Experience: Narrative Effects in First and Third Person Perspectives. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">International Conference on Human-Computer Interaction</em>. Springer, 78–97.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhengyuan Liu, Stella Xin Yin, Geyu Lin, and Nancy F Chen. 2024b.

</span>
<span class="ltx_bibblock">Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv preprint arXiv:2404.06762</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Louie et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, and Diyi Yang. 2024a.

</span>
<span class="ltx_bibblock">Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">arXiv preprint arXiv:2407.00870</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Louie et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, and Diyi Yang. 2024b.

</span>
<span class="ltx_bibblock">Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2407.00870" title="">http://arxiv.org/abs/2407.00870</a>
</span>
<span class="ltx_bibblock">arXiv:2407.00870 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, and Shao-Hua Sun. 2024.

</span>
<span class="ltx_bibblock">LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.06373" title="">http://arxiv.org/abs/2405.06373</a>
</span>
<span class="ltx_bibblock">arXiv:2405.06373 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zilin Ma, Yiyang Mei, and Zhaoyuan Su. 2023.

</span>
<span class="ltx_bibblock">Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">AMIA Annual Symposium Proceedings</em>, Vol. 2023. American Medical Informatics Association, 1105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacDonald (2015)</span>
<span class="ltx_bibblock">
Craig MacDonald. 2015.

</span>
<span class="ltx_bibblock">Assessing the user experience (UX) of online museum collections: Perspectives from design and museum professionals. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Museums and the Web</em>, Vol. 2015.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marsh et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Scott Marsh, Manjula Waniganayake, and John De Nobile. 2014.

</span>
<span class="ltx_bibblock">Improving learning in schools: the overarching influence of ‘presence’on the capacity of authoritative leaders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">International Journal of Leadership in Education</em> 17, 1 (2014), 23–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mastandrea et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Stefano Mastandrea, Sabrina Fagioli, and Valeria Biasi. 2019a.

</span>
<span class="ltx_bibblock">Art and psychological well-being: Linking the brain to the aesthetic emotion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Frontiers in psychology</em> 10 (2019), 739.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mastandrea et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Stefano Mastandrea, Fridanna Maricchiolo, Giuseppe Carrus, Ilaria Giovannelli, Valentina Giuliani, and Daniele Berardi. 2019b.

</span>
<span class="ltx_bibblock">Visits to figurative art museums may lower blood pressure and stress.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Arts &amp; health</em> 11, 2 (2019), 123–132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meyer et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Louie Meyer, Johanne Engel Aaen, Anitamalina Regitse Tranberg, Peter Kun, Matthias Freiberger, Sebastian Risi, and Anders Sundnes Løvlie. 2024.

</span>
<span class="ltx_bibblock">Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib68.4.2">(CHI ’24)</em>. Association for Computing Machinery, New York, NY, USA, Article 29, 18 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3613904.3642157" title="">https://doi.org/10.1145/3613904.3642157</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miaskiewicz and Kozar (2011)</span>
<span class="ltx_bibblock">
Tomasz Miaskiewicz and Kenneth A Kozar. 2011.

</span>
<span class="ltx_bibblock">Personas and user-centered design: How can personas benefit product design processes?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Design studies</em> 32, 5 (2011), 417–430.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hayoun Moon, Mia Saade, Daniel Enriquez, Zachary Duer, Hye Sung Moon, Sang Won Lee, and Myounghoon Jeon. 2024.

</span>
<span class="ltx_bibblock">Mixed-reality art as shared experience for cross-device users: Materialize, understand, and explore.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">International Journal of Human-Computer Studies</em> 190 (2024), 103291.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morse et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Christopher Morse, Blandine Landau, Carine Lallemand, Lars Wieneke, and Vincent Koenig. 2022.

</span>
<span class="ltx_bibblock">From# museumathome to# athomeatthemuseum: digital museums and dialogical engagement beyond the COVID-19 pandemic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">ACM Journal on Computing and Cultural Heritage (JOCCH)</em> 15, 2 (2022), 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moussawi et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Sara Moussawi, Marios Koufaris, and Raquel Benbunan-Fich. 2021.

</span>
<span class="ltx_bibblock">How perceptions of intelligence and anthropomorphism affect adoption of personal intelligent agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Electronic Markets</em> 31, 2 (2021), 343–364.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mullarkey and Hevner (2019)</span>
<span class="ltx_bibblock">
Matthew T Mullarkey and Alan R Hevner. 2019.

</span>
<span class="ltx_bibblock">An elaborated action design research process model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">European journal of information systems</em> 28, 1 (2019), 6–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen-Mau et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Toan Nguyen-Mau, Anh-Cuong Le, Duc-Hong Pham, and Van-Nam Huynh. 2024.

</span>
<span class="ltx_bibblock">An information fusion based approach to context-based fine-tuning of GPT models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Information Fusion</em> 104 (2024), 102202.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Brien et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Heather L O’Brien, Paul Cairns, and Mark Hall. 2018.

</span>
<span class="ltx_bibblock">A practical approach to measuring user engagement with the refined user engagement scale (UES) and new UES short form.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">International Journal of Human-Computer Studies</em> 112 (2018), 28–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pace and Hemmings (2006)</span>
<span class="ltx_bibblock">
Judith L Pace and Annette Hemmings. 2006.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Classroom authority: Theory, research, and practice</em>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pollatsek and Well (1995)</span>
<span class="ltx_bibblock">
Alexander Pollatsek and Arnold D Well. 1995.

</span>
<span class="ltx_bibblock">On the use of counterbalanced designs in cognitive research: a suggestion for a better and more powerful analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Journal of Experimental psychology: Learning, memory, and Cognition</em> 21, 3 (1995), 785.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponsignon et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Frederic Ponsignon, David Alexandre Jaud, François Durrieu, and Renaud Lunardo. 2024.

</span>
<span class="ltx_bibblock">The ability of experience design characteristics to elicit epistemic value, hedonic value, and visitor satisfaction in a wine museum.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">International Journal of Contemporary Hospitality Management</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hua Xuan Qin, Shan Jin, Ze Gao, Mingming Fan, and Pan Hui. 2024.

</span>
<span class="ltx_bibblock">CharacterMeet: Supporting Creative Writers’ Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sa et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Qier Sa, Zaiming Qu, Yangyang Liu, and Weilun Shan. 2024.

</span>
<span class="ltx_bibblock">The strategy of traditional Chinese settlement digitization: a landscape gene information chain theory-based perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">Heritage Science</em> 12, 1 (2024), 234.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salem et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Susanna Salem, Thomas Weskott, and Anke Holler. 2017.

</span>
<span class="ltx_bibblock">Does narrative perspective influence readers’ perspective-taking? An empirical study on free indirect discourse, psycho-narration and first-person narration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">Glossa: a journal of general linguistics</em> 2, 1 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salminen et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Joni Salminen, Haewoon Kwak, João M Santos, Soon-Gyo Jung, Jisun An, and Bernard J Jansen. 2018.

</span>
<span class="ltx_bibblock">Persona perception scale: developing and validating an instrument for human-like representations of data. In <em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</em>. 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salminen et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Joni Salminen, Chang Liu, Wenjing Pian, Jianxing Chi, Essi Häyhänen, and Bernard J Jansen. 2024.

</span>
<span class="ltx_bibblock">Deus Ex Machina and Personas from Large Language Models: Investigating the Composition of AI-Generated Persona Descriptions. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samur et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dalya Samur, Mattie Tops, Ringailė Slapšinskaitė, and Sander L Koole. 2021.

</span>
<span class="ltx_bibblock">Getting lost in a story: How narrative engagement emerges from narrative perspective and individual differences in alexithymia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">Cognition and emotion</em> 35, 3 (2021), 576–588.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanasintani and Munte (2024)</span>
<span class="ltx_bibblock">
Sanasintani Sanasintani and Alfonso Munte. 2024.

</span>
<span class="ltx_bibblock">Philosophical analysis of Mortimer J. Adler’s Christian education and global education management.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Journal of Education and Learning (EduLearn)</em> 18, 4 (2024), 1385–1393.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scholz and Smith (2016)</span>
<span class="ltx_bibblock">
Joachim Scholz and Andrew N Smith. 2016.

</span>
<span class="ltx_bibblock">Augmented reality: Designing immersive experiences that maximize consumer engagement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Business horizons</em> 59, 2 (2016), 149–161.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schroeder et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Noah L Schroeder, Olusola O Adesope, and Rachel Barouch Gilbert. 2013.

</span>
<span class="ltx_bibblock">How effective are pedagogical agents for learning? A meta-analytic review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">Journal of Educational Computing Research</em> 49, 1 (2013), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scott et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Philip H Scott, Eduardo F Mortimer, and Orlando G Aguiar. 2006.

</span>
<span class="ltx_bibblock">The tension between authoritative and dialogic discourse: A fundamental characteristic of meaning making interactions in high school science lessons.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">Science education</em> 90, 4 (2006), 605–631.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seeger et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Anna-Maria Seeger, Jella Pfeiffer, and Armin Heinzl. 2018.

</span>
<span class="ltx_bibblock">Designing anthropomorphic conversational agents: Development and empirical evaluation of a design framework.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Liping Shen, Minjuan Wang, and Ruimin Shen. 2009.

</span>
<span class="ltx_bibblock">Affective e-learning: Using “emotional” data to improve learning in pervasive learning environment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">Journal of Educational Technology &amp; Society</em> 12, 2 (2009), 176–189.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sherman and Morrissey (2017)</span>
<span class="ltx_bibblock">
Aleksandra Sherman and Clair Morrissey. 2017.

</span>
<span class="ltx_bibblock">What is art good for? The socio-epistemic value of art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Frontiers in human neuroscience</em> 11 (2017), 411.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xingzhi Shi, Mengyao Guo, and Ze Gao. 2024.

</span>
<span class="ltx_bibblock">Reconstructing Identity: An Augmented Reality Exploring Self-Objectification. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">Proceedings of the 16th Conference on Creativity &amp; Cognition</em>. 527–531.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrestha et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Palistha Shrestha, Jeevan Kandel, Hilal Tayara, and Kil To Chong. 2024.

</span>
<span class="ltx_bibblock">Post-translational modification prediction via prompt-based fine-tuning of a GPT-2 model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Nature Communications</em> 15, 1 (2024), 6699.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. 2023.

</span>
<span class="ltx_bibblock">Getting more out of mixture of language model reasoning experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">arXiv preprint arXiv:2305.14628</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva (2006)</span>
<span class="ltx_bibblock">
Elizabeth B Silva. 2006.

</span>
<span class="ltx_bibblock">Distinction through visual art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">cultural trends</em> 15, 2-3 (2006), 141–158.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith (2014)</span>
<span class="ltx_bibblock">
Jeffrey K Smith. 2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">The museum effect: How museums, libraries, and cultural institutions educate and civilize society</em>.

</span>
<span class="ltx_bibblock">Rowman &amp; Littlefield.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanzel (1984)</span>
<span class="ltx_bibblock">
Franz Karl Stanzel. 1984.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">A theory of narrative</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao and Xie ([n. d.])</span>
<span class="ltx_bibblock">
Muzi Tao and Saining Xie. [n. d.].

</span>
<span class="ltx_bibblock">What Does a Visual Formal Analysis of the World’s 500 Most Famous Paintings Tell Us About Multimodal LLMs?. In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">The Second Tiny Papers Track at ICLR 2024</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trupp et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
MacKenzie D Trupp, Giacomo Bignardi, Eva Specker, Edward A Vessel, and Matthew Pelowski. 2023.

</span>
<span class="ltx_bibblock">Who benefits from online art viewing, and how: The role of pleasure, meaningfulness, and trait aesthetic responsiveness in computer-based art interventions for well-being.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Computers in Human Behavior</em> 145 (2023), 107764.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Lissa et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Caspar J Van Lissa, Marco Caracciolo, Thom Van Duuren, and Bram Van Leuveren. 2016.

</span>
<span class="ltx_bibblock">Difficult Empathy: The Effect of Narrative Perspective on Reader’s Engagement with a First-Person Narrator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">DIEGESIS: Interdisciplinary E-Journal for Narrative Research</em> 5, 1 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Maanen (2009)</span>
<span class="ltx_bibblock">
Hans Van Maanen. 2009.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">How to study art worlds: On the societal functioning of aesthetic values</em>.

</span>
<span class="ltx_bibblock">Amsterdam University Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Someren et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (1994)</span>
<span class="ltx_bibblock">
Maarten Van Someren, Yvonne F Barnard, and J Sandberg. 1994.

</span>
<span class="ltx_bibblock">The think aloud method: a practical approach to modelling cognitive.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">London: AcademicPress</em> 11, 6 (1994).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walmsley (2016)</span>
<span class="ltx_bibblock">
Ben Walmsley. 2016.

</span>
<span class="ltx_bibblock">From arts marketing to audience enrichment: How digital engagement can deepen and democratize artistic exchange with audiences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Poetics</em> 58 (2016), 66–78.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhan Wang, Lin-Ping Yuan, Liangwei Wang, Bingchuan Jiang, and Wei Zeng. 2024.

</span>
<span class="ltx_bibblock">Virtuwander: Enhancing multi-modal interaction for virtual tour guidance through large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">Proceedings of the CHI conference on human factors in computing systems</em>. 1–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welleck et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2018.

</span>
<span class="ltx_bibblock">Dialogue natural language inference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">arXiv preprint arXiv:1811.00671</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Siyi Wu, Feixue Han, Bingsheng Yao, Tianyi Xie, Xuan Zhao, and Dakuo Wang. 2024.

</span>
<span class="ltx_bibblock">Sunnie: An Anthropomorphic LLM-Based Conversational Agent for Mental Well-Being Activity Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">arXiv preprint arXiv:2405.13803</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yenawine (2013)</span>
<span class="ltx_bibblock">
Philip Yenawine. 2013.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Visual thinking strategies: Using art to deepen learning across school disciplines</em>.

</span>
<span class="ltx_bibblock">Harvard Education Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi (2022)</span>
<span class="ltx_bibblock">
Xiaofen Yi. 2022.

</span>
<span class="ltx_bibblock">Driis: research on image classification of art education system based on deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">International Journal of Cooperative Information Systems</em> 31, 01n02 (2022), 2150007.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. 2024.

</span>
<span class="ltx_bibblock">MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">arXiv preprint arXiv:2404.06711</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuen et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Steve Chi-Yin Yuen, Gallayanee Yaoyuneyong, and Erik Johnson. 2011.

</span>
<span class="ltx_bibblock">Augmented reality: An overview and five directions for AR in education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">Journal of Educational Technology Development and Exchange (JETDE)</em> 4, 1 (2011), 11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023.

</span>
<span class="ltx_bibblock">Sigmoid Loss for Language Image Pre-Training.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.15343 [cs.CV]

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 05:43:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
