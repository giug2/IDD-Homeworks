<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.01820] Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention</title><meta property="og:description" content="Machine learning systems require representations of the real world for training and testing - they require data, and lots of it. Collecting data at scale has logistical and ethical challenges, and synthetic data promis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.01820">

<!--Generated on Wed Jun  5 13:47:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="synthetic data,  dataset development,  ethical guidelines,  responsible model development,  standards">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cedric Deslandes Whitney
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:cedricwhitney@berkeley.edu">cedricwhitney@berkeley.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of California, Berkeley</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Berkeley</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_state">California</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">USA</span><span id="id5.5.id5" class="ltx_text ltx_affiliation_postcode">94609</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Justin Norman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:justin.norman@berkeley.edu">justin.norman@berkeley.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_affiliation_institution">University of California, Berkeley</span><span id="id7.2.id2" class="ltx_text ltx_affiliation_city">Berkeley</span><span id="id8.3.id3" class="ltx_text ltx_affiliation_state">California</span><span id="id9.4.id4" class="ltx_text ltx_affiliation_country">USA</span><span id="id10.5.id5" class="ltx_text ltx_affiliation_postcode">94609</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id11.id1" class="ltx_p">Machine learning systems require representations of the real world for training and testing - they require data, and lots of it. Collecting data at scale has logistical and ethical challenges, and synthetic data promises a solution to these challenges. Instead of needing to collect photos of real people’s faces to train a facial recognition system, a model creator could create and use photo-realistic, synthetic faces. The comparative ease of generating this synthetic data rather than relying on collecting data has made it a common practice. We present two key risks of using synthetic data in model development. First, we detail the high risk of false confidence when using synthetic data to increase dataset diversity and representation. We base this in the examination of a real world use-case of synthetic data, where synthetic datasets were generated for an evaluation of facial recognition technology. Second, we examine how using synthetic data risks circumventing consent for data usage. We illustrate this by considering the importance of consent to the U.S. Federal Trade Commission’s regulation of data collection and affected models. Finally, we discuss how these two risks exemplify how synthetic data complicates existing governance and ethical practice; by decoupling data from those it impacts, synthetic data is prone to consolidating power away those most impacted by algorithmically-mediated harm.</p>
</div>
<div class="ltx_keywords">synthetic data, dataset development, ethical guidelines, responsible model development, standards
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>ACM Conference on Fairness, Accountability, and Transparency; June 3–6, 2024; Rio de Janeiro, Brazil</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT ’24), June 3–6, 2024, Rio de Janeiro, Brazil</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3630106.3659002</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0450-5/24/06</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics Governmental surveillance</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics Privacy policies</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Computer vision</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Facial recognition technology (FRT) has become commonplace, used from flight check-in at airports to police crowd-monitoring. Bias in FRT models has resulted in mis-identification and expanded surveillance, causing unjust incarceration and other discriminatory outcomes. Attempts to solve these issues by increasing the accuracy of FRT run headfirst into problems; for a machine learning-based computer vision system to be considered robust enough for a given real-world task, it must “generalize” to images that vary widely in quality and domain (image granularity, race, age, gender, background, head pose, hats, glasses, etc.). Datasets with this level of granular design and annotation, that are also large enough for use in deep learning, are nearly impossible to find due to logistical and ethical concerns. As a result, researchers have turned to synthetic data generation, where data is generated to resemble something without being a representation of an instance of it — a drawing of a generic face as compared to a photograph of a real person. Synthetic data has been used to augment existing datasets and create new datasets for better training and evaluation of FRT models. Logistical and ethical challenges to data collection exist outside of FRT, and synthetic data usage has become commonplace across machine learning, from computer vision to large language models. This paper examines two key risks of using synthetic data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic data is fundamentally useful where real data is not fit to task, necessitating that synthetic data must be both similar enough to be meaningful, but different enough to mitigate the reason the real data is not usable <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>. Jordan et al. propose three attributes of synthetic data that must be met for it to function in lieu of real data: utility, fidelity, and privacy. This paper focuses on facial recognition because it clearly articulates the risks of synthetic data, inherently forcing trade-offs between these attributes. There is high difficulty in making a picture of a face private but still usable as training data (privacy vs. utility) — a face which has been obscured to the point where an identity could not be gleaned is less useful <cite class="ltx_cite ltx_citemacro_citep">(Chamikara et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. Achieving fidelity in facial datasets, a measure of how well synthetic data matches the real world, is also saliently difficult in facial recognition use cases, as we examine below.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The first risk we focus on is the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">high risk of false confidence</span> in the ability of synthetic datasets to mitigate bias in data distribution and representation. We demonstrate this through the real-world example of using synthetic data for a facial recognition model evaluation. This paper was motivated by the realization of the under-explored risks of synthetic data while conducting the evaluation, and we present it both to provide an example of how synthetic data is used and to detail the concerns that conducting it made apparent. In brief, synthetic data offers a way of diversifying datasets, but diversity in real-world faces often follows from cultural practices that are qualitative and meaning-laden rather than quantitative. Creating a synthetic dataset or adding synthetic data to existing datasets in an attempt to diversify that dataset runs the risk of <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">diversity-washing</span> — appearing to resolve valid criticism regarding a dataset’s distribution and representation but in a way that is superficial. As a result, using synthetic data risks legitimizing technologies such as FRT despite potentially continuing to propagate bias by achieving false fidelity.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The second risk we examine is how using synthetic data risks <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">circumventing consent for data usage</span>, illustrating the impacts by considering the importance of consent to the U.S. Federal Trade Commission’s regulation of data collection and affected models. Synthetic data provides an avenue for model developers to side-step thorny issues around collecting large-scale representative facial datasets. Proper consent to data usage is foundational to the privacy enforcement tools that the FTC has used to require companies delete ML models trained on improperly collected data, a key regulatory hurdle to improper data collection and resulting harmful model deployment. Using synthetic data risks circumventing and obfuscating consent, thus complicating deterrence and enforcement.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper proceeds as follows: We begin in Section <a href="#S2" title="2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> by summarizing related prior work. We first examine work on datasets and representation, before discussing participation and consent and power over data and models. Finally, we discuss synthetic data and its use. We then proceed to the two titular risks of synthetic data that this paper focuses on — diversity-washing (Section <a href="#S3" title="3. Risk 1: Diversity-Washing ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and the circumvention of consent (Section <a href="#S4" title="4. Risk 2: Circumvented Consent ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). We draw upon two real-world examples: a facial recognition evaluation task conducted using synthetic data, and the FTC’s enforcement actions against models trained on deceptively collected data to illustrate these risks. Finally, in Section <a href="#S5" title="5. Discussion ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we expand upon how these two risks are examples of irresponsible use of synthetic data: consolidating power in the hands of model creators, and decoupling data from those it represents and those who are harmed by its improper use. It is our intention for this research to contribute to the field by presenting tangible examples and background for the challenges inherent in responsible use of synthetic data, thus laying foundations for further work and debate. We call for future work to examine the breadth and usage of synthetic data and to work towards both mitigating synthetic data’s risks and enabling its potential for participatory empowerment.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, first, we focus on the datasets that underpin machine learning systems, and detail how that work treats the specific issue of data distribution and representation in those datasets (Section <a href="#S2.SS1" title="2.1. Datasets, Diversity and Representation ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). Next, we discuss prior work on participatory governance, consent and data privacy, and attempts to capture some power over dataset creation and usage by those most affected (Section <a href="#S2.SS2" title="2.2. Participation, Consent and Privacy ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>). Finally, we provide a summary of work detailing what synthetic data is and how it is used (Section <a href="#S2.SS3" title="2.3. Synthetic Data ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Datasets, Diversity and Representation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Datasets are the hidden infrastructure behind machine learning, most visible when the systems built on them break <cite class="ltx_cite ltx_citemacro_citep">(Jackson, <a href="#bib.bib59" title="" class="ltx_ref">2014</a>)</cite>. Models developed and dependent on these large datasets can lead to biased and harmful effects, with models used for bureaucratic categorization in particular having a long history of harm <cite class="ltx_cite ltx_citemacro_citep">(Bullock, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Alkhatib and Bernstein, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Spade, <a href="#bib.bib96" title="" class="ltx_ref">2015</a>)</cite>. The collection of data is then frequently the starting point for ML-disseminated discrimination and bias in domains such as hiring <cite class="ltx_cite ltx_citemacro_citep">(Raghavan et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2020</a>)</cite>, advertising <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite>, pricing <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2022</a>)</cite>, the application of law, and government allocation of resources <cite class="ltx_cite ltx_citemacro_citep">(Abebe and Goldner, <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>; as well as being vital for identifying and enforcing against discrimination <cite class="ltx_cite ltx_citemacro_citep">(Andrus and Villeneuve, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. The stakes of responsible dataset development, then, are high, and we build on critical previous work <cite class="ltx_cite ltx_citemacro_citep">(Paullada et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2021</a>; Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2021</a>; Hutchinson et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2021</a>)</cite> in focusing on the ways that dataset creators have significant impact on the harms that occur downstream via their development, usage and deployment <cite class="ltx_cite ltx_citemacro_citep">(Khan and Hanna, <a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>. More narrowly, we hope to bring focus to important risks present when synthetic data is used in the process of creating and using datasets in machine learning development.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Datasets used for facial recognition models, where the goal of the model is matching an image or video of a face to an identity, have received much scrutiny — specifically for violating privacy <cite class="ltx_cite ltx_citemacro_citep">(Harvey and LaPlace, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>; Birhane and Prabhu, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. An analysis by Crawford &amp; Paglen <cite class="ltx_cite ltx_citemacro_citep">(Crawford and Paglen, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> of ImageNet, a frequently-used large dataset, demonstrated active labeling of faces with offensive and derogatory classifications, and Prabhu &amp; Birhane <cite class="ltx_cite ltx_citemacro_citep">(Birhane and Prabhu, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> make the point that beyond obvious privacy harms such as blackmail, the creation of one of these datasets causes similar datasets to propagate. The recent identification of CSAM material in the popular LAION dataset is an example of this <cite class="ltx_cite ltx_citemacro_citep">(Thiel, <a href="#bib.bib99" title="" class="ltx_ref">2023</a>)</cite>. In the case of synthetic data, where the data is frequently derivative of previously collected data (as expanded upon below in Section <a href="#S2.SS3" title="2.3. Synthetic Data ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>), this then risks the continued propagation of non-consensual imagery. The above highlights the need to focus beyond just the models. Much of the critical AI literature focuses on vital interventions to change model outputs that discriminate against protected classes. This work, instead, more closely follows work such as Buolamwini &amp; Gebru’s ”Gender Shades” <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> that is focused on the <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">data</span> which is fundamental to AI system development.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">To understand the risks posed by using synthetic data to create and add to datasets used in machine learning development, it is necessary to understand the landscape of both machine learning development and the stakeholders impacted by it. We will use the taxonomy of dataset development stages and subjects presented by Khan &amp; Hanna <cite class="ltx_cite ltx_citemacro_citep">(Khan and Hanna, <a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>. We lean on this taxonomy throughout the paper, finding it to be clear in its intention of “providing a common language for conversations across datasets” between practitioners, scholars and regulators. Starting from first principles, machine learning is not rules-based like traditional software development, but instead consists of a practitioner teaching a model to identify patterns in a dataset. To begin, Khan &amp; Hanna assert, the <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">task</span> for which the model is being trained must be formulated and constrained. Next, <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">data</span> must be collected, meeting the constraints of what is necessary to train a model to achieve said task. That data collection is usually broad, requiring the data be <span id="S2.SS1.p3.1.3" class="ltx_text ltx_font_italic">cleaned</span> before it is annotated. The <span id="S2.SS1.p3.1.4" class="ltx_text ltx_font_italic">labels</span> attributed to data by the annotator are of vital importance to machine learning systems, as the systems are taught to identify those labels in their training data. After this point, model <span id="S2.SS1.p3.1.5" class="ltx_text ltx_font_italic">training, valuation, implementation</span>, etc. may occur. Khan &amp; Hannah define multiple <span id="S2.SS1.p3.1.6" class="ltx_text ltx_font_italic">stakeholders</span> in the process of creating the datasets used for ML development: the curator, the data annotator, the data subject, the copyright holder, and the model subject. The <span id="S2.SS1.p3.1.7" class="ltx_text ltx_font_italic">curator</span> is the entity responsible for dataset creation, while the <span id="S2.SS1.p3.1.8" class="ltx_text ltx_font_italic">annotator</span> is (frequently outsourced <cite class="ltx_cite ltx_citemacro_citep">(Gray and Suri, <a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>) responsible for annotating the dataset. The <span id="S2.SS1.p3.1.9" class="ltx_text ltx_font_italic">data subject</span> is the person whose biometric information is present within the collected data, the <span id="S2.SS1.p3.1.10" class="ltx_text ltx_font_italic">copyright holder</span> may hold exclusive rights over that data<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>not all data is copyright protected, and even when it is, different legal regimes have different limitations and exceptions to exclusive intellectual property rights</span></span></span>, and the <span id="S2.SS1.p3.1.11" class="ltx_text ltx_font_italic">model subject</span> is the person who is impacted by the decisions made by the model trained on the data. The last three categories are fluid, and can consist of the same person or of two or three distinct entities.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Creating a training dataset that is representative of model subjects is challenging, and when done poorly, results in inaccurate and frequently harmful outputs; here we consider how such harm might arise across different stages. In producing a representative dataset, different dimensions of identities are frequently missed. Datasets are frequently biased by necessity to meet a specific intended use of a model, but when categories are socially constructed (such as race and gender), the observed, inferred data that dataset curators use to bound and constrain data collection (and that annotators must use to label) can clash with how the data subject self-identifies. Biased data representation is also a concern, with annotation reflecting social biases and stereotypes across gender, race, and more <cite class="ltx_cite ltx_citemacro_citep">(Scheuerman et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2020</a>)</cite>. This can lead to rampant misrepresentation and miscategorization of both data and model subjects, producing forms of control. Annotation work is inherently an interpretive project, but results in data that is perceived to be ground truth <cite class="ltx_cite ltx_citemacro_citep">(Bowman and Dahl, <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>. In reality, as shown by Recht et al. in work on testing the generalizability of ImageNet, when attempts to replicate annotation are made, different distributional properties for the same data emerge <cite class="ltx_cite ltx_citemacro_citep">(Recht et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite>. Annotation is also frequently the cause of artifacts in datasets that allow for models to overfit to training data when solving a task, with many of the concerns arising from how human data annotators are instructed to label <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2020</a>)</cite>. Beyond concerns with annotation, the question of what data to collect when constructing a dataset to correctly answer a question is complex. The task of annotation itself presupposes that the question being asked of a model is one that can be answered — Aguera y Arcas et al. demonstrate how a model trained on ‘gaydar’ data was in reality labeled around stereotyped aesthetic traits, showcasing an example of labels being generated not because of any model-pertinent aspects of the data, but rather simply because the question was being asked <cite class="ltx_cite ltx_citemacro_citep">(y Arcas, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Beyond the above challenges to creating a representative dataset, there are ethical issues that are raised by efforts to produce a dataset that represents a diverse community. Data collection requires infrastructure, and that infrastructure is frequently co-constitutive with surveillance infrastructure. Even when data collection is initiated in service of providing services to the most disenfranchised, rendering the members of those communities hyper-visible frequently serves to hurt those same communities, as decisions are made for them by others <cite class="ltx_cite ltx_citemacro_citep">(Andrus and Villeneuve, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. These decisions can reinforce oppressive norms, such as visual gender binaries <cite class="ltx_cite ltx_citemacro_citep">(Bivens, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>; Hamidi et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite>, further delegitimizing disenfranchised groups in a clear example of administrative violence <cite class="ltx_cite ltx_citemacro_citep">(Spade, <a href="#bib.bib96" title="" class="ltx_ref">2015</a>)</cite>. Even when categorization schema of data subjects are correct, their use as prescriptive instead of explanatory can lead to attribution errors, co-opting classification in a oppressed group as a reason for that very oppression. Machine learning systems used to predict recidivism are a prime example <cite class="ltx_cite ltx_citemacro_citep">(Christin, <a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite>, where factors like race, which make a group member more likely to be targeted for discrimination, are frequently used instead as a predictive factor when individuals are made model subjects.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Participation, Consent and Privacy</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Participatory approaches are frequently fronted as a way of mitigating the harm that results from AI systems, both at the dataset level as described above, and in model training and deployment. These approaches focus on engaging the public, and build on policy approaches such as feedback sessions, public hearings and impact assessments <cite class="ltx_cite ltx_citemacro_citep">(Glucker et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2013</a>; Hügel and Davies, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>. Participatory design methods in particular focus on co-design to incorporate user context, needs and values <cite class="ltx_cite ltx_citemacro_citep">(Bratteteig and Wagner, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>; Sloane, <a href="#bib.bib92" title="" class="ltx_ref">2022</a>; Iversen et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2012</a>; Halloran et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2009</a>)</cite>, designing systems <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">with</span> those affected instead of <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">for</span> them. Recently, participatory AI work has explicitly focused on those for which AI most frequently exacerbates harm <cite class="ltx_cite ltx_citemacro_citep">(Irgens et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Robertson and Salehi, <a href="#bib.bib88" title="" class="ltx_ref">2020</a>)</cite>. Patel at al. <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2021</a>)</cite> draw from previous work, including Arnstein’s influential Ladder of Citizen Participation <cite class="ltx_cite ltx_citemacro_citep">(Arnstein, <a href="#bib.bib7" title="" class="ltx_ref">1969</a>)</cite>, to detail five levels of participation in data stewardship, including: 1) <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">informing</span> people about how their data is used through methods such as model cards, 2) <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">consulting</span> people through UX research and surveys, 3) <span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">involving</span> people in data governance through panels and public deliberation, 4) <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">co-design</span> of data governance and consequent technologies through structures such as data trusts, and 5) enabling <span id="S2.SS2.p1.1.7" class="ltx_text ltx_font_italic">decision-making</span> through citizen-led governance boards. We will return to Patel’s framework when discussing the risk of synthetic data circumventing consent (Section <a href="#S4" title="4. Risk 2: Circumvented Consent ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">There is a wide — and growing! — diversity of participatory work in AI. Examples range from crowdsourcing impacts <cite class="ltx_cite ltx_citemacro_citep">(Barnett and Diakopoulos, <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Diaz et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite> and data labeling <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2019</a>)</cite> to eliciting preferences for dataset collecting and design decisions <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>. Peng et al. <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2021</a>)</cite> recommend that dataset creators make ethically salient information clear and accessible while actively stewarding the dataset and its future use, and encourage retrospective study of datasets due to the difficulties in understanding issues at the beginning. Hutchinson et al. <cite class="ltx_cite ltx_citemacro_citep">(Hutchinson et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2021</a>)</cite> detail documentation requirements at each stage of the dataset development lifecycle, with different document types for each stage, and call for frameworks for transparency and accountability. These fall across the range of Patel et al.’s framework, <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2021</a>)</cite> and critiques of these methods include characterization of it as ‘participation-washing’ <cite class="ltx_cite ltx_citemacro_citep">(Gilman, <a href="#bib.bib42" title="" class="ltx_ref">2022</a>; Sloane et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2020</a>)</cite>, with Arnstein describing approaches such as public requests for comment as “tokenizing” and “inadequate in shifting power” <cite class="ltx_cite ltx_citemacro_citep">(Arnstein, <a href="#bib.bib7" title="" class="ltx_ref">1969</a>)</cite>. Sloane et al. <cite class="ltx_cite ltx_citemacro_citep">(Sloane et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2020</a>)</cite> argue that these approaches can function as unrecognized labor, and the line between tokenization and participation in cases such as crowdsourcing is quite blurry. Birhane et al. <cite class="ltx_cite ltx_citemacro_citep">(Birhane et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> show examples of community inclusion in annotating datasets, improving documentation and increasing the utility of large language models for under-served languages, and other examples include community organizations such as the Detroit URC3 which evaluates potential partnerships between community organizations and researchers to avoid exploitation <cite class="ltx_cite ltx_citemacro_citep">(Corbett et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, and examples from Indigenous Data Sovereignty <cite class="ltx_cite ltx_citemacro_citep">(Rainie et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2019</a>)</cite>. At a large scale however, there are still major hurdles. Groves et al. investigate the hurdle of making participatory approaches work in the commercial AI labs that are the primary site for AI research, and find that “corporate profit motive and concern around exploitation are at present functioning as significant barriers to the use of participatory methods in AI” <cite class="ltx_cite ltx_citemacro_citep">(Groves et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>, p. 10)</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">While the above participatory approaches center shifting decision-making power to include data subjects and model subjects, these approaches tend to require model creators to opt in at least for now. Consent, though enacting a more limited form of participation, <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">requires</span> model creators to be wary of unfair and deceptive practices that overstep expressly-informed consent when collecting and using data. Indeed, consent violation is a legally cognizable privacy harm, one with potential repercussions. In the U.S., state information privacy laws do some work to enforce this, with the Illinois Biometric Privacy Act (BIPA) both resulting in a significant number of lawsuits alleging violation, and being responsible for the largest settlement amounts from companies who have breached BIPA by deploying FRT <cite class="ltx_cite ltx_citemacro_citep">(Strickler, <a href="#bib.bib97" title="" class="ltx_ref">2020</a>; Yew and Xiang, <a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>). For instance, in <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">Vance v. IBM</span>, the court affirmed that IBM violated BIPA by not receiving written consent before collecting and disseminating individuals’ images in their ”Diversity in Faces” dataset <cite class="ltx_cite ltx_citemacro_citep">(Goldenfein, <a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite>, even though the images used were public. Publicly accessible personal information comes with an intended context of use, which can be violated by memorized and regurgitated data <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. As will be discussed in Section <a href="#S4.SS1" title="4.1. Consent, Deception and Model Deletion ‣ 4. Risk 2: Circumvented Consent ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, to date, the Federal Trade Commission’s enforcement power around unfair and deceptive data practices has centered upon the absence of consent. Practically, this instills consent as the most direct way for data subjects and model subjects to participate in decision-making around the models which affect them, albeit mostly <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">ex post facto</span> through their ability to prompt enforcement when discovering that their consent has been violated.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Consent violations frequently occur through improperly scoped consent, where data collected for one purpose is repurposed. This can result in adverse effects beyond the concrete privacy harms <cite class="ltx_cite ltx_citemacro_citep">(Solove, <a href="#bib.bib94" title="" class="ltx_ref">2012</a>)</cite> that are most often legally enforced in cases such as data breach, e.g., identity theft. As an example, data used beyond its consented purpose leaves data subjects at risk of discrimination harms, facing miscategorization and expansion of surveillance, as detailed above. Such data can also be sold and shared with third parties, further increasing the odds that it is not being used for the purpose it was collected, and therefore that it is frequently in violation of the consent of data subjects <cite class="ltx_cite ltx_citemacro_citep">(Calo, <a href="#bib.bib23" title="" class="ltx_ref">2011</a>)</cite>. Even when consent is nominally obtained, transparency is often in name only, with data subjects overwhelmed by opaque and all-encompassing digital policies, terms, and conditions <cite class="ltx_cite ltx_citemacro_citep">(Pasquale, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Ultimately, the question of consent is complex. Brown et al. <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> argue that the current paradigm of training on publicly accessible data makes it highly challenging to distinguish what public data was made public with blanket versus contextual consent, and that, therefore, obtaining informed consent is difficult at best. They make the case for training solely on data explicitly consented for public dissemination. We will return to this argument in Section <a href="#S5" title="5. Discussion ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, as it supports using synthetic data generated from properly-consented real data or responsibly procedurally created data, and criticizes using synthetic data generated and used in a manner that exacerbates concerns around consent.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Synthetic Data</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Synthetic data in machine learning is defined by its driving goal of mimicking real-world data — it is synthesized to be used as though it were real data for training machine learning algorithms <cite class="ltx_cite ltx_citemacro_citep">(Jacobsen, <a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite>. It differs from what is usually referred to as ‘data’, i.e. non-synthetic data, in that it does not have an explicit 1:1 real-world referent. When training a computer vision model to recognize a face, the data traditionally used are representations of real faces, photographs taken of real people. The same holds true for other forms of data — scientific data records representations of physical phenomena such as sensor readings, natural language data is text composed by a real person, etc. Synthetic data is made to resemble these things, but is not explicitly a representation of a real thing. Using the example of a face, a synthetic face could be a drawing that looks for all intents and purposes like a face, but that is does not represent a specific person.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In actually creating synthetic data, however, things become muddier. The term encompasses data generated by generative models, more traditionally augmented data, and procedurally created data <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2021</a>)</cite>. We differentiate between the first two and the latter category based on how derivative of a real-world training dataset they are. <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">Generated data</span> is the output of generative models: ML systems that produce a (supposedly novel  <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>) output from an input by abstracting over their training data. One generative model that has captured popular attention <cite class="ltx_cite ltx_citemacro_citep">(Perez, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite> is StableDiffusion, which generates art from a user-provided input sentence <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>. <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">Augmented data</span> is fuzzier, but equally derivative of a real-world training dataset; the term tends to refer to any real-world data to which modifications have been made. A model creator seeking to increase the performance of a model on its specific task may create many versions of each image sampled from the input dataset, creating augmented data. This type of synthetic data cannot be considered inherently private or unbiased, with generative models explicitly being found to frequently regurgitate memorized training data <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>. At larger scales, this type of data can be used as training and evaluation datasets too. As detailed by by Khan &amp; Hanna <cite class="ltx_cite ltx_citemacro_citep">(Khan and Hanna, <a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>, datasets are vital components the larger model development cycle, priming synthetic data to reinforce and scale skewed values and requirements that are embedded within models, datasets, and benchmarks.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Generated and augmented data differs from techniques for <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">procedural creation</span> of data, where dataset designers make active decisions to create ‘net-new’ representations of data similar to what might be found in the natural world. There are some important differences. The easiest way to visualize how procedural creation works is considering video game character creation — a player starts with a base face shape, and adds the features they want. Notably, with procedural creation of faces, the base volumetric face scans that are used are very far removed from the people that were scanned — they are low fidelity representations, making data lineage even murkier. Instead of outputs being generated from, and therefore bound by training data, procedural creation can result in ‘net-new’ data that never existed previously.
Or, as an example outside of computer vision and facial recognition, consider procedurally created finance data which uses agent-based models to mimic real world data generation by creating representative agents and attempting to model money laundering <cite class="ltx_cite ltx_citemacro_citep">(Lopez-Rojas and Axelsson, <a href="#bib.bib69" title="" class="ltx_ref">2012</a>)</cite>. It must be remembered, however, that both agent- and procedural-based synthetic data are highly determined by preconfiguration and design. As such, making inferences about the real world based on procedurally created data is difficult at best. Recalling Jordan et al.’s framework of synthetic’s data usability, this type of synthetic data faces utility and fidelity hurdles  <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Synthetic data has frequently been explored as a method to avoid privacy concerns, increase model performance and to reduce model bias. Privacy concerns have the longest history of motivating synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>. Healthcare <cite class="ltx_cite ltx_citemacro_citep">(Gonzales et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> and financial <cite class="ltx_cite ltx_citemacro_citep">(Assefa et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> domains have been particularly attracted to synthetic solutions due to the sensitivity of their data. Examples including simulation studies in population health <cite class="ltx_cite ltx_citemacro_citep">(Ngufor et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2019</a>)</cite>; synthetic clinical records used for IT development, education, and training <cite class="ltx_cite ltx_citemacro_citep">(Davis et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2010</a>)</cite>; money-laundering detection <cite class="ltx_cite ltx_citemacro_citep">(Lopez-Rojas and Axelsson, <a href="#bib.bib69" title="" class="ltx_ref">2012</a>)</cite>; and public release of augmented financial and healthcare datasets to enable open science and research <cite class="ltx_cite ltx_citemacro_citep">(Harron et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2016</a>)</cite>. In contexts of societal bias, synthetic data has been explored as a way to remove disparate impact <cite class="ltx_cite ltx_citemacro_citep">(Feldman et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2015</a>; Kamiran and Calders, <a href="#bib.bib62" title="" class="ltx_ref">2009</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib112" title="" class="ltx_ref">2016</a>)</cite>, to suppress imbalance effects and to racially balance datasets <cite class="ltx_cite ltx_citemacro_citep">(Kortylewski et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>, as well as to remove sensitive information and blind models to race <cite class="ltx_cite ltx_citemacro_citep">(Wang and Huang, <a href="#bib.bib102" title="" class="ltx_ref">2019</a>)</cite>. However, the latter has been found to not always be effective in practice, with applying a ‘veil of ignorance’ not having any notable influence on accuracy of FRT on under-represented categories <cite class="ltx_cite ltx_citemacro_citep">(Wehrli et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Increasing model performance by using synthetic data has usually meant enlarging datasets to provide robustness to outliers <cite class="ltx_cite ltx_citemacro_citep">(Wong et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2016</a>; Fawaz et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2018</a>; Dai et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2017</a>)</cite>. Additionally, operating at a slightly different scale of enlargement (from 0), it has been used in situations where real world data is difficult to access. Google recently demonstrated AlphaGeometry, an AI system purported to solve “Olympiad geometry problems at a level approaching a human-gold medalist”, trained solely on a dataset of 100 million synthetic math proofs — a dataset which could not exist using human generated proofs <cite class="ltx_cite ltx_citemacro_citep">(Trinh et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2024</a>)</cite>. Computer vision systems require (often impossibly) large amounts of labeled training data in a specific domain <cite class="ltx_cite ltx_citemacro_citep">(Birhane and Prabhu, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">Finally, consider the context of facial recognition. Though large, open datasets specifically developed for facial recognition tasks exist, such datasets are either extremely basic (i.e. passport photos with great lighting), too narrow (only contain a biased subset of race, gender, head/body pose, etc.), or simply contain glaring and challenging shortcomings <cite class="ltx_cite ltx_citemacro_citep">(Raji and Fried, <a href="#bib.bib84" title="" class="ltx_ref">2021</a>)</cite>. As a result, dataset creators utilize synthetic data. In FRT, this is either (1) procedurally created ‘synthetic’ data, where a bone structure scan is used as a basis for volumetric face models, and then textures representing features are stretched across that model and swapped out <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2014</a>)</cite>, or (2) the perturbation of existing data to produce more diverse samples from the existing distribution, including both simple techniques and advanced techniques such as diffusion models and generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal and Nichol, <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. We expand upon the use of synthetic data for FRT in the following section.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Risk 1: Diversity-Washing</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section presents an example of using synthetic data; first describing a dataset that is partially-synthetic, a blend of augmented data and real data, and then describing a dataset that is procedurally created synthetic data. In both cases, we detail how using synthetic data risks creating datasets — and subsequently training and evaluating models from that data — that fail to mitigate bias in data distribution and representation. Furthermore, there is a risk of propagating harm through a patina of legitimacy, and through diversity-washing potentially harmful models.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The example we use is a real world example, where one of the authors had previously created synthetic datasets to evaluate facial recognition technology (FRT) <cite class="ltx_cite ltx_citemacro_citep">(Norman et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2023</a>)</cite>. We present this example to illustrate a risk of synthetic data and ground it in a real world setting — this is not an attempt to present novel work on FRT evaluation.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To provide a brief background: FRT is created with the aim of matching images of identifying faces. Companies that sell facial recognition such as Clearview often tout accuracy rates of their systems of 97% or more <cite class="ltx_cite ltx_citemacro_citep">(Taboh, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite> — but these calculations are often made under ideal conditions. In real world conditions, such as surveillance camera footage, captured images of faces may be poor quality. FRT has been shown to be prone to make erroneous matches (i.e. identifying someone incorrectly as a match) when using low quality images as input <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite>. However, users of these systems, such as the police, and adjudicators such as judges or members of Congress, who are not experts in FRT or ML/AI, are at a distinct disadvantage in evaluating companies’ claims.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Augmented Partially-Synthetic Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In order to evaluate FRT models in real world settings, first, benchmark performance for FRT models needed to be established. This occurred by stimulating the (highly unreliable) process of a human identifying an individual from a visual lineup of other humans with similar characteristics. To do so, a source image of a selected identity was identified from the base dataset, detailed below, and a “digital lineup” of (mathematically) similar faces from that base dataset were created. Augmented data was then created by progressively degrading the image of the source identity, and then this augmented data was compared to the similar identities in the digital lineup, as well as to the source, in order to mimic real world settings. The success of the evaluated models was defined by the rate the correct identity was selected with the augmented data, the degraded source images, as input.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">A significant body of knowledge already exists concerning both the obvious and non-obvious potential harms in gathering image data containing human subjects, and the real harms of processing such information through FRT <cite class="ltx_cite ltx_citemacro_citep">(Raji and Fried, <a href="#bib.bib84" title="" class="ltx_ref">2021</a>)</cite>. As such, it was important to begin with core datasets that had already been evaluated thoroughly in the literature, rather than collect wholly new human subject data. As such, CASIA-Webface, one of the two most widely used and evaluated public datasets <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2014</a>; Kawulok et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2016</a>)</cite>, was selected as the dataset for use as both non-augmented data and as the base for augmented (in this case, degraded) data. This was chosen due to its sourcing from crawled and scraped publicly available images of celebrities, strict rules prohibiting commercial use, wide variation in image quality, large number of identities (depth), and large number of images per identity (width) — important features for the dataset that was the provenance for later augmentation. As detailed in Section <a href="#S2.SS1" title="2.1. Datasets, Diversity and Representation ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, this choice of base dataset is inherently political despite frequently being rendered neutral. Using it as the base for the creation of synthetic data makes it inherently more so due to the downstream effects of using the base dataset in generating derivative data.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The data augmentation techniques used to generate the augmented portion of this mixed dataset can differentially and unpredictably scale issues — making images black and white, as an example, could further segment training data by racial presentation. Model creators frequently augment data while training models, past the stages of development where they are considering dataset collection and annotation. This process has a set of well documented risks for model fairness. However, in using this process to create large base datasets, there is a change of framing that re-introduces these risks. For example, the dataset created for the FRT evaluation (created by augmenting data and combining with real data) was created with the explicit goal of being more representative of real world conditions. Datasets are frequently treated as ground truth <cite class="ltx_cite ltx_citemacro_citep">(Birhane and Prabhu, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>, hiding the decisions and processes by which they were created. This risks ignoring issues that can occur from augmenting data. Even if synthetic data appears ’diverse’, the generation of that data cannot be unwound from the particular datasets and models that it is being generated from, and any attendant shortcomings. To start, any biased representations would at best replicate from the original dataset. If data augmentation technique(s) impacted some subjects differently than others, the resulting impact could be unintended bias in the dataset. Since the presence of such relationships are rarely known, much less understood statistically in datasets, it is also possible that the sampling strategy used to choose which data from the dataset will be used in training may actually exacerbate harm by over-representing biased representations. Deep learning techniques are generally already susceptible to overfitting, where a model learns how to predict patterns in a way that pays too much attention to the training data, and doesn’t generalize to other data — it learns specific idiosyncrasies and meaningless data artifacts. Synthetic data seems like it should have the capacity to remedy overfitting, through careful and bespoke dataset construction that debiases data distributions. One could assume this would increase fidelity and enable better generalization over a more diverse training space. In reality however, when synthetic data is overfitted, these idiosyncrasies can go through the entire model training process unnoticed. As such, synthetic data instead increases the likelihood of overfitting errors being propagated through, necessitating that further technical care is taken to prevent overfitting. In our FRT evaluation example, such preventative measures were taken by curating the dataset so that visible artifacts such as skin tone were at parity with acceptable real world dataset distributions. However, the perils of overfitting are a way in which synthetic data can struggle to meet the standard of utility necessary to work as a replacement for real world data <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Procedurally Created Fully Synthetic Dataset</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Beyond the above partially-synthetic dataset, there was a need to better evaluate performance on specific types of data not present in the original datasets that the FRT models were trained on. So, a synthetic dataset consisting of procedurally created data, namely mixed examples of non-degraded and degraded computer-generated faces <cite class="ltx_cite ltx_citemacro_citep">(Qiu et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2021</a>; Basak et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, was developed. As previously detailed, this is a common usage of synthetic data — needed representative data was not available for collection, and so generating synthetic data was the easiest method of proceeding.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The Synthesis.AI software<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Synthesis AI (https://synthesis.ai)</span></span></span> used to create the fully synthetic dataset (as with most procedural synthetic human/object generation tools) works by providing unprecedented control over how a dataset and its inherent metadata parameters are specified. This software employs a combination of classic rendering and generative synthesis to create photo-realistic images of human faces, bodies, and environments <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko, <a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite>. A user is able to decide how much and which type of each characteristic (in our case age, race, gender, hair type, pose, lighting etc.). However, the tool did not make any suggestions or place any controls based on sociotechnical norms or demographic data (such as the census etc.) when creating a synthetic human dataset of any type. When first testing the Synthesis.AI API, a dramatically racially imbalanced dataset was returned, even though the specification given was for randomization of the race characteristic. At first glance, the dataset appeared diverse and was numerically at parity for gender. However, the software lacked permutations for Asian people, Middle Eastern people and Black women, leading to a stark racial disparity upon deeper inspection, and a preponderance of white men and white women despite attempts at balancing racial demographics. Such a system allows any user to easily create an unintentionally biased dataset, which could then be used to train a biased model. Instead of mitigating data distribution and representation concerns, this risks extending them.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">As a further example, Microsoft’s FaceSynthetics <cite class="ltx_cite ltx_citemacro_citep">(Wood et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite> is a procedurally created synthetic dataset of 100,000 individuals, with faces derived from representative 511 base scans. However, these 511 base scans include only  30 Black men, and even fewer Hispanic/Arab/Indian men and Black/Hispanic/Arab/Indian women (borrowing the reported demographic categories), meaning that the fully diverse population they claim include multiple racial categories fully defined by the ways in which these ¡30 faces can be manipulated through a generative process. These manipulations include fine-tuning hair, expression, and clothing, but published details on the process of how these potentially racially-coded aspects were chosen are sparse. It is not known how those features are distributed in real faces, and attempting to extrapolate a portion of a representatively diverse dataset from such a small set of base faces leads to a risk of <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">statistical diversity without representational diversity</span>, compared to a representative dataset of real images with both statistical and representational diversity. Synthetic data here falls flat in addressing these complex, cultural and deeply contextualized factors.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">These tools risk falling into the ‘panacea of legitimization’ that Frank Pasquale describes <cite class="ltx_cite ltx_citemacro_citep">(Pasquale, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>, where ethical concerns are not only routed around, but are routed around in such a manner that they can reify malpractice due to the co-constitutive nature of ML practices and computing platforms <cite class="ltx_cite ltx_citemacro_citep">(Berman, <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>. We point to recent work focused on toolkits for supporting practitioners in contextualizing ML system work as an avenue for improving upon this <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Risk 2: Circumvented Consent</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Consent has become a key component of privacy and data protection, both via regulatory enforcement and as a necessary foundation for the participatory approaches that have emerged as ethical practice for preventing harm. Consent also plays a role in U.S. sectoral statutes such as HIPPA, U.S. state laws such as California’s California Privacy Rights Act and Illinois’ Biometric Information Privacy Act, as well as laws in many other countries, with the EU’s GDPR a notable example. These statutes share a common goal: to provide people with control over their personal data, via notification, access, and consent regarding the collection, use, and disclosure of personal data. This type of privacy regulation is referred to as “privacy self-management” by Solove <cite class="ltx_cite ltx_citemacro_citep">(Solove, <a href="#bib.bib94" title="" class="ltx_ref">2012</a>)</cite>, and focuses solely on whether or not data subjects have consented, rather than on value judgements of privacy practices. This section will focus on illustrating the risk that synthetic data poses to consent-based frameworks by expanding upon how the Federal Trade Commission (FTC) has functionally used consent as a key aspect of conducting enforcement against companies using ML systems. The analysis is guided by one author’s experience at the FTC, but draws upon solely public knowledge.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Consent, Deception and Model Deletion</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The FTC plays a vital role in the current U.S. privacy legal framework. This framework emphasizes individuals’ notice of, and consent to, the collection and use of their data. The FTC is an independent agency of the United States government that is tasked with protecting consumers and promoting competition in the marketplace. In the absence of federal privacy law, the FTC has played the role of de facto privacy enforcement, primarily based on its authority to police unfair and deceptive business practices. The Federal Trade Commission Act, and specifically Section 5, is a broadly applicable federal statute prohibiting “unfair or deceptive acts and practices” <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>15 U.S.C. Sec. 45(a)</span></span></span>. An <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">unfair</span> practice ”causes or is likely to cause substantial injury to consumers which is not reasonably avoidable by consumers themselves and not outweighed by countervailing benefits to consumers or to competition”, while a <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">deceptive</span> practice includes “any ‘representation, omission, or practice’ that is (i) material, and (ii) likely to mislead consumers who are acting reasonably under the circumstances” <cite class="ltx_cite ltx_citemacro_citep">(Solove and Hartzog, <a href="#bib.bib95" title="" class="ltx_ref">2014</a>)</cite>. Notably, deception does not require any proof of intent. The FTC has brought deception claims against companies who have violated the terms of their privacy policies, failed to uphold promises of data security, or have failed to provide sufficient notice regarding data collection and use <cite class="ltx_cite ltx_citemacro_citep">(Solove and Hartzog, <a href="#bib.bib95" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In settling cases against companies that have deceptively collected data, the FTC has required not only that the data in question be deleted and the affected users be notified, but also that all ”affected work product” <cite class="ltx_cite ltx_citemacro_citep">(Li, <a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> be deleted as well — including models trained on that data. This approach is referred to as <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">model deletion</span>. The FTC posits that this approach is necessary in order to ensure that companies do not profit from the unfair or deceptive collection of data, and to prevent them from using the data in the future. Intellectual property is typically a tech company’s most valuable asset; it is an important factor for securing venture capital funding and the sale or licensing of IP often comprises tech companies’ core business models. In forcing a company to delete models, the FTC has also significantly changed the deterrence calculus for companies: from paying (relatively) small fines, to potentially losing a vital business asset <cite class="ltx_cite ltx_citemacro_citep">(Elder, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>. The FTC has used the concept of model deletion in recent enforcement actions against companies that have collected data deceptively, including Amazon Ring, RiteAid, Everalbum, Clearview and Kurbo (WeightWatchers)  <cite class="ltx_cite ltx_citemacro_citep">(Hutson and Winters, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Synthetic Data and FTC Enforcement</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The use of synthetic data risks undermining the utility of deception-based enforcement in regulating the collection of data, and therefore also undermines the regulation of models trained on such synthetic data. As previously described in Section <a href="#S2.SS1" title="2.1. Datasets, Diversity and Representation ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, datasets play a foundational role in the models trained on them, and trusting models trained on deceptively collected datasets to operate without harm seems foolhardy. Enforcement by the FTC has hinged upon arguments that data was collected and used deceptively — often argued due to the absence of proper consent. By using synthetic data, however, it becomes easy for model creators to obfuscate the origins and consent of the data being used to create models. In the case of a procedurally created synthetic dataset, consent is no longer a procedural hook to limit downstream harms flowing from use, while in other synthetic datasets, unless data lineage is carefully recorded, traceability to the original data is at risk <cite class="ltx_cite ltx_citemacro_citep">(Scheuerman et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Synthetic data also exacerbates existing logistical challenges for model deletion as an enforcement tool. Synthetic data brings questions of data lineage to the forefront, as ever-more-complicated sets of original, augmented, and derivative data are produced based on new face datasets with millions of people. As a small example, keeping track of whether a single version of a dataset has undergone ethical testing, or was sectioned off as a test dataset, is a challenge for FTC enforcers — let alone when datasets include different scaling factors and different degradations, with different subsets of identities (generated, procedurally created or real) and different levels of augmentation. One of the key hurdles to model deletion is the requirement for a high level of internal company documentation and logging. This documentation and logging is essential to identify the data that was collected illegally or deceptively, as well as the work product that was developed using that data. However, not all companies have robust internal documentation and logging systems, which can make it difficult for the FTC to determine the extent of the harm caused by the illegal or deceptive data collection practices. Another challenge is authentication and audit. Companies must demonstrate that they have successfully deleted the affected work product, and the FTC or other enforcement bodies must have a way of verifying that the company is being honest. However, this can be difficult, as it requires a level of trust in the company and its processes. In a setting where the FTC has to this point relied on settlement agreements, synthetic data further complicates existing logistical challenges, presenting important friction to enforcement.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Beyond Deception</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In considering enforcement against companies using ML systems, it is important to note that beyond deception, the FTC has also enforced its unfairness authority. This occurred in the case of RiteAid, where a biased facial recognition model was used to falsely identify people in certain protected classes as more likely to commit crime. This case was first-in-kind, but demonstrates that the FTC is not solely beholden to deceptive data collection as an avenue for enforcement. However, the details of the case were particularly egregious, with RiteAid failing to undertake even the most basic risk assessments, and in part hinged on a violation of a previous settlement. Additionally, the system was trained on in-store camera footage without consent from data subjects, and model subjects were not notified or able to opt-out. Thus if RiteAid had been investigated for deceptive data collection, harm resulting from this system could have been prevented at the point where the system was trained non-consensually. But what would happen if RiteAid had trained its FRT model on synthetic data? FTC enforcement would have to hinge on unfair practice alone. While successful here, the case against RiteAid was egregious. The Supreme Court’s neutering of the FTC’s power to levy fines <cite class="ltx_cite ltx_citemacro_citep">(Chopra and Levine, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, in addition to both the deception and unfairness enforcements occurring through settlement rather than being decided in court, means the boundaries of the FTC’s ability to engage in this type of enforcement are still undefined. As such, the FTC’s ability to intervene both at the data collection stage (deception) and the model deployment stage (unfairness) gives options<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>While there exists the potential for both unfairness at data collection and deception at model deployment, cases to-date have lined up in this fashion</span></span></span>. Synthetic data complicates the usage of a demonstrably useful tool for protecting data subjects and model subjects, by complicating the use of the deception standard.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Finally, it cannot be forgotten that while risking increased friction and obfuscation, synthetic datasets composed of augmented or generated data are demonstrably derivative, inherently based on real data representing real data subjects. And, despite the veneer provided by language such as ‘net-new’, procedural creation of synthetic data is also derivative, and thus suffers from issues of consent and participation too. In the example discussed in Section <a href="#S3.SS2" title="3.2. Procedurally Created Fully Synthetic Dataset ‣ 3. Risk 1: Diversity-Washing ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the procedurally created synthetic dataset for FRT evaluation, the dataset was generated using Synthesis.AI’s commercial software. Software tools such as Synthesis.AI often utilize face and body scanning technology as the foundation of their generative processes, raising concerns around the limits of informed consent. The data subjects upon which these technologies are trained are rendered invisible and thus the use of such software is predicated on, at best <cite class="ltx_cite ltx_citemacro_citep">(Raji et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2020</a>)</cite>, ambiguous consent. Similarly, it is important to acknowledge that the data subjects whose likenesses are captured in the CASIA-Webface, while primarily scraped from public sources, were not asked for informed consent regarding their data’s use — even when it has been shown that having your face included in such a dataset increases the accuracy of facial recognition models on your specific face <cite class="ltx_cite ltx_citemacro_citep">(Dulhanty and Wong, <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>. As detailed by Peng et al. <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2021</a>)</cite>, using derivatives of common datasets introduces scaling concerns around propagation of improperly consented data, and as such, using synthetic data risks <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">further</span> scaling propagation of this issue. In decoupling data subjects from their data, this also removes their capacity to participate. Reconsider the Participatory Data Stewardship Framework mentioned above in Section <a href="#S2.SS2" title="2.2. Participation, Consent and Privacy ‣ 2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>; all five levels require that data subjects have at the very least visibility, and preferably control, over their data  <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2021</a>)</cite>. In further removing the ability for data subjects to consent, not only is that minimal level of agency reduced, but the potential for involvement in decision-making that directly effects them is erased.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The positioning of synthetic data as a panacea to problems of representation and deceptive data collection, furthered by its portrayal as synthetic, as neutral, as created without lineage, risks placing the means of fixing those problems in the hands of those who created them and trusting that they’ll get it right. Instead, the kinds of racialized misrecognition and bias that Ruha Benjamin, Safiya Noble, and others have drawn attention to must be considered when determining whether to use synthetic data. As Benjamin argues, our current machine learning development ecosystem must reckon with a history of discriminatory design in which racist values and assumptions are built into our technical systems. The ’new Jim Code’, as she terms it, works to deepen the production of disparate harm, even while cloaked in neutrality and the language of innovation <cite class="ltx_cite ltx_citemacro_citep">(Benjamin, <a href="#bib.bib13" title="" class="ltx_ref">2019</a>; Irani, <a href="#bib.bib56" title="" class="ltx_ref">2019</a>)</cite>. Discriminatory practices are inherent to the current state of AI system development, privileging whiteness and discriminating against people of color, specifically women of color <cite class="ltx_cite ltx_citemacro_citep">(Noble, <a href="#bib.bib73" title="" class="ltx_ref">2021</a>)</cite>. Sara Ahmed’s work on the phenomenology of whiteness highlights the danger of a solution that further enables a reification of non-whiteness as a space outside. She details that ”institutional spaces are shaped by the proximity of some bodies and not others: white bodies gather, and cohere to form the edges of such spaces” <cite class="ltx_cite ltx_citemacro_citep">(Ahmed, <a href="#bib.bib4" title="" class="ltx_ref">2007</a>)</cite>. Synthetic data as a fix in this racialized context risks further enabling amplification of racial hierarchies, allowing for those within the boundaries to actively constitute the exclusionary and weaponized edges of these spaces: to define a face, train a model based on that definition, and decide its performance based on labeling racial boundaries. It risks not alleviating but instead contributing to
<span id="S5.p1.1.1" class="ltx_text ltx_font_italic">race as a technology,</span> designed to “stratify and sanctify” social injustice in the architecture of everyday life <cite class="ltx_cite ltx_citemacro_citep">(Benjamin, <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>; an added consolidation of power.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Another example of the risks of consolidation of power through synthetic data arises when considering the inherent relational aspects of data privacy. Solon Barocas and Helen Nissenbaum identify the risk of a ”tyranny of the minority” in big-data analytics when ”the volunteered information of the few can unlock the same information about the many” <cite class="ltx_cite ltx_citemacro_citep">(Barocas and Nissenbaum, <a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite>. More recently, Salome Viljoen emphasizes the importance of a relational theory of data governance <cite class="ltx_cite ltx_citemacro_citep">(Viljoen, <a href="#bib.bib101" title="" class="ltx_ref">2021</a>)</cite>. As Viljoen explains, dataflows entail not only ‘vertical’ relations between a particular individual and a data collector, but also ‘horizontal’ relations between the individual and others sharing relevant population features. Viljoen focuses on the manner in which informational infrastructures rely on group classification to make sense of individuals by taking a ‘relevant shared feature,’ generating a prediction based upon that shared feature, and then applying this prediction. When those shared features are derived from synthetic data, decoupled from any real context and perhaps even specifically created to rectify gaps in representation, we hand power to those creating that synthetic data. We risk imposing designers’ decision-making in lieu of and upon those least likely to have been represented and most likely to be harmed by both the diversity-washing and the side-stepping of consent. After all, if they were represented or able to consent in the first place, there would be no need for additional synthetic data. Data minimization and lineage principles <cite class="ltx_cite ltx_citemacro_citep">(Hutson and Winters, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite> are a first step towards mitigating this issue by requiring documentation and its requisite transparency into where data has come from. The need for this is also readily apparent when considering contexts such as the EU’s Right to be Forgotten <cite class="ltx_cite ltx_citemacro_citep">(Razmetaeva, <a href="#bib.bib86" title="" class="ltx_ref">2020</a>)</cite>, where synthetic data further complicates the ability to be removed from a dataset. In making it harder to decouple data from its context through the use of synthetic data, there is an avenue for mitigating consolidation of power and ensuring consent.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Many of the risks discussed in this paper propagate from the centralized decision-making nature that synthetic data imposes. Participatory governance structures, as mentioned in Section <a href="#S2" title="2. Related Work ‣ Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, offer a potential solution here — synthetic data could be created to represent concerned groups <span id="S5.p3.1.1" class="ltx_text ltx_font_italic">by</span> those self-same groups, re-establishing control and mitigating some concerns around consent and contextualization. Many have called for training data to be restricted to only data that is explicitly consented to be used, though consent is difficult (if not impossible) to establish and propagate over multiple degrees of separation <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>. But synthetic data, when generated to purpose by concerned communities, can provide a potential solution. Models such as those presented in the field of Indigenous Data Sovereignty, where there has been effective push back against external categorization schemas <cite class="ltx_cite ltx_citemacro_citep">(Rainie et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2019</a>)</cite> show potential for participatory governance models to address group misrepresentation <cite class="ltx_cite ltx_citemacro_citep">(Andrus and Villeneuve, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Additionally, there are practical considerations that make ’participatory synthetic data’ an attractive path forward. Both language and computer vision models are beginning to contend with a phenomena commonly referred to as ‘garbage-in garbage-out’ <cite class="ltx_cite ltx_citemacro_citep">(Shumailov et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2023</a>; Martínez et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>. This refers to the advent of generated data becoming commonplace and public, and the related struggles by those capturing data to differentiate between that data and real data, leading to data generated by a system such as ChatGPT becoming its own training data in the future. Work by Agnew et al. <cite class="ltx_cite ltx_citemacro_citep">(Agnew et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2024</a>)</cite> examines the use of these models to replace participants in industry research, highlighting how proposals to do so are motivated by cost reduction and data diversity. They identify these proposals as facing issues in aligning with the values human participants identify as important, specifically including inclusion and representation, necessitating further contextualization and bespoke dataset creation. In such a world, large tech companies may have business motivations for engaging with responsibly created synthetic data, and as demonstrated by Deng et al. <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite>, methods exist for enabling machine learning practitioners to better contextualize the work they do — a vital aspect of any future responsible synthetic data work.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Further responsible dataset development frameworks that explicitly attend to the particulars of synthetic data, as well as tooling and practice that examines and makes transparent the provenance of synthetic data, are needed. As examples of this, we propose ’how could less risky synthetic data be produced?’, as well as ’how could governance approach consent issues with synthetic data?’ as important future research questions. In future work, we hope to follow the call of Denton et al. <cite class="ltx_cite ltx_citemacro_citep">(Denton et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, contesting machine learning datasets and focusing on the “contingent, historical, and value-laden work practices of actual machine learning researchers” to better understand how the practice of using synthetic data is motivated, the contingent conditions that have lead to its common usage, and the norms and routines that surround it. In so doing, there is the opportunity to survey and better understand the use of synthetic data and create better tools and frameworks for both mitigating its potential for harmful power consolidation, as well as to envision how it can be used as a tool for taking power back <cite class="ltx_cite ltx_citemacro_citep">(Whitney et al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we build on prior responsible dataset development work by focusing on the under-explored impacts of synthetic data on dataset development. Synthetic data will continue to play an ever-increasing role in the training of machine learning systems as real-world data becomes harder to capture, and we must attend to language that paints it as a panacea. We show two examples of the risks of synthetic data, diversity-washing and consent circumvention, and discuss how it is a complicated tool, gravitationally prone to consolidation of power, but with potential for being used to enable participatory governance instead of squashing it.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Researcher Ethics and Social Impact</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Researcher Positionality Statement</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">The first author is a white Latino AI researcher significantly influenced by their research, which has examined how policy and technical practice interplay and talk past each other, and how this dynamic affects those most likely to be harmed by AI systems. They worked in ML before moving into academia. The second author is a Black AI researcher with a variety of experiences in government, industry and now academia. They have access to the resources necessary to conduct their research, and recognize that that they have access to resources that many others do not. They strive to be conscious of their biases and to mitigate their impact on their work as much as possible. Both authors were motivated to write this paper by the realization that the risks of a commonplace technical practice were under-explored when discussing a real world example (detailed in this paper), and hoped to provide a starting point for understanding how using synthetic data could go wrong. Both researchers are based in the U.S., and that heavily influences both the examples they draw upon to show risks, the harms that they find salient, and the Overton window through which they view the world.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Ethical Considerations Statement</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">This work focuses on illustrating risks through the analysis and description of public-facing information and prior work through a new lens. As this is an example of RAI work that is focused on human impact but that does not involve study participants or create or deploy new technology, the main ethical consideration is in how this prior work is presented, where we actively attempted to avoid falling into some of the same traps we discuss — we do not wish to make the technology seem inevitable or help to legitimize it, while we also do not want to forestall the opportunity for the risks we present to be mitigated and it to be used in participatory and ethical manners. We believe that the FRT evaluation example provided in this paper, created to assist in preventing unfounded FRT claims being used in the criminal justice setting, necessitated the creation of these datasets and the usage of synthetic data, but that is far from an ever-present conclusion.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Adverse Impact Statement</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">The largest adverse impact we are wary of is that these risks could be taken as playbooks — we hope that nobody comes away thinking that there is opportunity to take advantage of them. We think that by making them public we are doing more of a good, as these examples demonstrate that the potential for these things already exists, and active exploration and research focused on mitigating and re-directing potential is the best way forward. We also see that this work could potentially draw scrutiny to legitimate uses of synthetic data, but hope that any added friction there is worth preventing potential malpractice.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research was supported by UC Berkeley’s Center for Long Term Cybersecurity AI Policy Hub, where the first author conducted research as a Fellow. This research was also supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1752814. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation or of the CLTC. We thank those who helped us workshop and develop this material, specifically including Professor Deirdre Mulligan, Professor Jenna Burrell, Professor Hany Farid, Professor Brandie Nonnecke, Jessica Newman, Lauren Chambers and Seyi Olojo.

</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abebe and Goldner (2018)</span>
<span class="ltx_bibblock">
Rediet Abebe and Kira
Goldner. 2018.

</span>
<span class="ltx_bibblock">Mechanism Design for Social Good.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">AI Matters</em> 4,
3 (Oct. 2018),
27–34.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3284751.3284761" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3284751.3284761</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agnew et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
William Agnew, A. Stevie
Bergman, Jennifer Chien, Mark Díaz,
Seliem El-Sayed, Jaylen Pittman,
Shakir Mohamed, and Kevin R. McKee.
2024.

</span>
<span class="ltx_bibblock">The Illusion of Artificial Inclusion.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3613904.3642703" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3613904.3642703</a>
arXiv:2401.08572 [cs]

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmed (2007)</span>
<span class="ltx_bibblock">
Sara Ahmed.
2007.

</span>
<span class="ltx_bibblock">A Phenomenology of Whiteness.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Feminist Theory</em> 8,
2 (Aug. 2007),
149–168.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/1464700107078139" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/1464700107078139</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alkhatib and Bernstein (2019)</span>
<span class="ltx_bibblock">
Ali Alkhatib and Michael
Bernstein. 2019.

</span>
<span class="ltx_bibblock">Street-Level Algorithms: A Theory at the
Gaps Between Policy and Decisions. In
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on
Human Factors in Computing Systems</em>. ACM,
Glasgow Scotland Uk, 1–13.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3290605.3300760" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3290605.3300760</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andrus and Villeneuve (2022)</span>
<span class="ltx_bibblock">
McKane Andrus and Sarah
Villeneuve. 2022.

</span>
<span class="ltx_bibblock">Demographic-Reliant Algorithmic Fairness:
Characterizing the Risks of Demographic Data Collection in the
Pursuit of Fairness. In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2022 ACM
Conference on Fairness, Accountability, and Transparency</em>
<em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">(FAccT ’22)</em>. Association for
Computing Machinery, New York, NY, USA,
1709–1721.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3531146.3533226" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3531146.3533226</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arnstein (1969)</span>
<span class="ltx_bibblock">
Sherry R. Arnstein.
1969.

</span>
<span class="ltx_bibblock">A Ladder Of Citizen Participation.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Journal of the American Institute of
Planners</em> 35, 4 (July
1969), 216–224.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1080/01944366908977225" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/01944366908977225</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assefa et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Samuel A. Assefa, Danial
Dervovic, Mahmoud Mahfouz, Robert E.
Tillman, Prashant Reddy, and Manuela
Veloso. 2021.

</span>
<span class="ltx_bibblock">Generating Synthetic Data in Finance:
Opportunities, Challenges and Pitfalls. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the First ACM International
Conference on AI in Finance</em> <em id="bib.bib8.4.2" class="ltx_emph ltx_font_italic">(ICAIF
’20)</em>. Association for Computing Machinery,
New York, NY, USA, 1–8.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3383455.3422554" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3383455.3422554</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ching-Yuan Bai, Hsuan-Tien
Lin, Colin Raffel, and Wendy Chi-wen
Kan. 2021.

</span>
<span class="ltx_bibblock">On Training Sample Memorization: Lessons
from Benchmarking Generative Modeling with a Large-scale
Competition. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>
<em id="bib.bib9.4.2" class="ltx_emph ltx_font_italic">(KDD ’21)</em>. Association for
Computing Machinery, New York, NY, USA,
2534–2542.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3447548.3467198" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3447548.3467198</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett and Diakopoulos (2022)</span>
<span class="ltx_bibblock">
Julia Barnett and
Nicholas Diakopoulos. 2022.

</span>
<span class="ltx_bibblock">Crowdsourcing Impacts: Exploring the
Utility of Crowds for Anticipating Societal Impacts of
Algorithmic Decision Making. In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2022 AAAI/ACM Conference on AI, Ethics, and
Society</em>. 56–67.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3514094.3534145" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3514094.3534145</a>
arXiv:2207.09525 [cs]

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barocas and Nissenbaum (2014)</span>
<span class="ltx_bibblock">
Solon Barocas and Helen
Nissenbaum. 2014.

</span>
<span class="ltx_bibblock">Big Data’s End Run around Procedural Privacy
Protections.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 57,
11 (Oct. 2014),
31–33.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2668897" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2668897</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Basak et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Shubhajit Basak, Peter
Corcoran, Faisal Khan, Rachel Mcdonnell,
and Michael Schukat. 2021.

</span>
<span class="ltx_bibblock">Learning 3D Head Pose From Synthetic Data: A
Semi-Supervised Approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 9
(2021), 37557–37573.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/ACCESS.2021.3063884" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2021.3063884</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benjamin (2019)</span>
<span class="ltx_bibblock">
Ruha Benjamin.
2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Race after Technology: Abolitionist Tools
for the New Jim Code</em>.

</span>
<span class="ltx_bibblock">Polity, Medford, MA.

</span>
<span class="ltx_bibblock">


</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berman (2023)</span>
<span class="ltx_bibblock">
Glen Berman.
2023.

</span>
<span class="ltx_bibblock">Machine Learning Practices and
Infrastructures. In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023
AAAI/ACM Conference on AI, Ethics, and Society</em>
<em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">(AIES ’23)</em>. Association for
Computing Machinery, New York, NY, USA,
466–481.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3600211.3604689" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3600211.3604689</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Abeba Birhane, William
Isaac, Vinodkumar Prabhakaran, Mark
Díaz, Madeleine Clare Elish, Iason
Gabriel, and Shakir Mohamed.
2022.

</span>
<span class="ltx_bibblock">Power to the People? Opportunities and
Challenges for Participatory AI. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Equity
and Access in Algorithms, Mechanisms, and Optimization</em>.
1–8.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3551624.3555290" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3551624.3555290</a>
arXiv:2209.07572 [cs]

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane and Prabhu (2021)</span>
<span class="ltx_bibblock">
Abeba Birhane and
Vinay Uday Prabhu. 2021.

</span>
<span class="ltx_bibblock">Large Image Datasets: A Pyrrhic Win for
Computer Vision?. In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Winter
Conference on Applications of Computer Vision (WACV)</em>.
IEEE Computer Society, 1536–1546.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/WACV48630.2021.00158" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/WACV48630.2021.00158</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bivens (2017)</span>
<span class="ltx_bibblock">
Rena Bivens.
2017.

</span>
<span class="ltx_bibblock">The Gender Binary Will Not Be Deprogrammed: Ten
Years of Coding Gender on Facebook.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">New Media &amp; Society</em> 19,
6 (June 2017),
880–898.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/1461444815621527" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/1461444815621527</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman and Dahl (2021)</span>
<span class="ltx_bibblock">
Samuel R. Bowman and
George E. Dahl. 2021.

</span>
<span class="ltx_bibblock">What Will It Take to Fix Benchmarking in
Natural Language Understanding?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2104.02145" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2104.02145</a>
arXiv:2104.02145 [cs]

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bratteteig and Wagner (2016)</span>
<span class="ltx_bibblock">
Tone Bratteteig and Ina
Wagner. 2016.

</span>
<span class="ltx_bibblock">Unpacking the Notion of Participation in
Participatory Design.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Computer Supported Cooperative Work</em>
25, 6 (Dec.
2016), 425–475.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10606-016-9259-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10606-016-9259-4</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hannah Brown, Katherine
Lee, Fatemehsadat Mireshghallah, Reza
Shokri, and Florian Tramèr.
2022.

</span>
<span class="ltx_bibblock">What Does It Mean for a Language Model
to Preserve Privacy?. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">2022 ACM
Conference on Fairness, Accountability, and Transparency</em>.
ACM, Seoul Republic of Korea,
2280–2292.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3531146.3534642" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3531146.3534642</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bullock (2019)</span>
<span class="ltx_bibblock">
Justin B. Bullock.
2019.

</span>
<span class="ltx_bibblock">Artificial Intelligence, Discretion, and
Bureaucracy.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">The American Review of Public
Administration</em> 49, 7
(Oct. 2019), 751–761.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/0275074019856123" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/0275074019856123</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and Gebru (2018)</span>
<span class="ltx_bibblock">
Joy Buolamwini and
Timnit Gebru. 2018.

</span>
<span class="ltx_bibblock">Gender Shades: Intersectional Accuracy
Disparities in Commercial Gender Classification. In
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Conference on
Fairness, Accountability and Transparency</em>.
PMLR, 77–91.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calo (2011)</span>
<span class="ltx_bibblock">
Ryan Calo.
2011.

</span>
<span class="ltx_bibblock">The Boundaries of Privacy Harm Essay.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Indiana Law Journal</em> 86,
3 (2011), 1131–1162.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hanqun Cao, Cheng Tan,
Zhangyang Gao, Yilun Xu,
Guangyong Chen, Pheng-Ann Heng, and
Stan Z. Li. 2023.

</span>
<span class="ltx_bibblock">A Survey on Generative Diffusion Model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2209.02646 [cs]

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Jamie
Hayes, Milad Nasr, Matthew Jagielski,
Vikash Sehwag, Florian Tramèr,
Borja Balle, Daphne Ippolito, and
Eric Wallace. 2023.

</span>
<span class="ltx_bibblock">Extracting Training Data from Diffusion
Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2301.13188" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2301.13188</a>
arXiv:2301.13188 [cs]

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chamikara et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
M. A. P. Chamikara, P.
Bertok, I. Khalil, D. Liu, and
S. Camtepe. 2020.

</span>
<span class="ltx_bibblock">Privacy Preserving Face Recognition Utilizing
Differential Privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Computers &amp; Security</em> 97
(Oct. 2020), 101951.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.cose.2020.101951" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.cose.2020.101951</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chopra and Levine (2022)</span>
<span class="ltx_bibblock">
Rohit Chopra and
Samuel A.A. Levine. 2021/2022.

</span>
<span class="ltx_bibblock">The Case for Resurrecting the FTC Act’s
Penalty Offense Authority.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">University of Pennsylvania Law Review</em>
170, 1 (2021/2022),
71–124.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Paul Christiano, Jan
Leike, Tom B. Brown, Miljan Martic,
Shane Legg, and Dario Amodei.
2023.

</span>
<span class="ltx_bibblock">Deep Reinforcement Learning from Human Preferences.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1706.03741" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1706.03741</a>
arXiv:1706.03741 [cs, stat]

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christin (2017)</span>
<span class="ltx_bibblock">
Angèle Christin.
2017.

</span>
<span class="ltx_bibblock">Algorithms in Practice: Comparing Web
Journalism and Criminal Justice.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Big Data &amp; Society</em> 4,
2 (Dec. 2017),
2053951717718855.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/2053951717718855" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/2053951717718855</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corbett et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Eric Corbett, Emily
Denton, and Sheena Erete.
2023.

</span>
<span class="ltx_bibblock">Power and Public Participation in AI. In
<em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd ACM Conference on
Equity and Access in Algorithms, Mechanisms, and
Optimization</em> <em id="bib.bib30.4.2" class="ltx_emph ltx_font_italic">(EAAMO ’23)</em>.
Association for Computing Machinery,
New York, NY, USA, 1–13.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3617694.3623228" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3617694.3623228</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crawford and Paglen (2021)</span>
<span class="ltx_bibblock">
Kate Crawford and Trevor
Paglen. 2021.

</span>
<span class="ltx_bibblock">Excavating AI: The Politics of Images in
Machine Learning Training Sets.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">AI &amp; SOCIETY</em> 36,
4 (Dec. 2021),
1105–1116.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s00146-021-01162-8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s00146-021-01162-8</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang,
Fan Yang, William W. Cohen, and
Ruslan Salakhutdinov. 2017.

</span>
<span class="ltx_bibblock">Good Semi-supervised Learning That Requires a
Bad GAN.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1705.09783" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1705.09783</a>
arXiv:1705.09783 [cs]

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Peter Davis, Roy
Lay-Yee, and Janet Pearson.
2010.

</span>
<span class="ltx_bibblock">Using Micro-Simulation to Create a Synthesised Data
Set and Test Policy Options: The Case of Health Service Effects under
Demographic Ageing.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Health Policy (Amsterdam, Netherlands)</em>
97, 2-3 (Oct.
2010), 267–274.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.healthpol.2010.05.014" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.healthpol.2010.05.014</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Wesley Hanwen Deng, Manish
Nagireddy, Michelle Seng Ah Lee, Jatinder
Singh, Zhiwei Steven Wu, Kenneth
Holstein, and Haiyi Zhu.
2022.

</span>
<span class="ltx_bibblock">Exploring How Machine Learning Practitioners
(Try To) Use Fairness Toolkits. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">2022
ACM Conference on Fairness, Accountability, and
Transparency</em>. ACM, Seoul
Republic of Korea, 473–484.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3531146.3533113" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3531146.3533113</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denton et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Emily Denton, Alex Hanna,
Razvan Amironesei, Andrew Smart, and
Hilary Nicole. 2021.

</span>
<span class="ltx_bibblock">On the Genealogy of Machine Learning Datasets:
A Critical History of ImageNet.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Big Data &amp; Society</em> 8,
2 (July 2021),
20539517211035955.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/20539517211035955" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/20539517211035955</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and
Alex Nichol. 2021.

</span>
<span class="ltx_bibblock">Diffusion Models Beat GANs on Image
Synthesis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2105.05233" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2105.05233</a>
arXiv:2105.05233 [cs, stat]

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diaz et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Mark Diaz, Ian D.
Kivlichan, Rachel Rosen, Dylan K. Baker,
Razvan Amironesei, Vinodkumar
Prabhakaran, and Emily Denton.
2022.

</span>
<span class="ltx_bibblock">CrowdWorkSheets: Accounting for
Individual and Collective Identities Underlying Crowdsourced Dataset
Annotation. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">2022 ACM Conference on
Fairness, Accountability, and Transparency</em>.
2342–2351.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3531146.3534647" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3531146.3534647</a>
arXiv:2206.08931 [cs]

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dulhanty and Wong (2020)</span>
<span class="ltx_bibblock">
Chris Dulhanty and
Alexander Wong. 2020.

</span>
<span class="ltx_bibblock">Investigating the Impact of Inclusion in
Face Recognition Training Data on Individual Face Identification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2001.03071 [cs]

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elder (2022)</span>
<span class="ltx_bibblock">
Emma Elder.
2022.

</span>
<span class="ltx_bibblock">Wrongful Improvers as a Guiding Principle
for Application of the FTC’s IP Deletion Requirement.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">WASHINGTON LAW REVIEW</em> 97
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fawaz et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Hassan Ismail Fawaz,
Germain Forestier, Jonathan Weber,
Lhassane Idoumghar, and Pierre-Alain
Muller. 2018.

</span>
<span class="ltx_bibblock">Data Augmentation Using Synthetic Data for Time
Series Classification with Deep Residual Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1808.02455" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1808.02455</a>
arXiv:1808.02455 [cs]

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Michael Feldman, Sorelle
Friedler, John Moeller, Carlos
Scheidegger, and Suresh Venkatasubramanian.
2015.

</span>
<span class="ltx_bibblock">Certifying and Removing Disparate Impact.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1412.3756" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1412.3756</a>
arXiv:1412.3756 [cs, stat]

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilman (2022)</span>
<span class="ltx_bibblock">
Michele E. Gilman.
2022.

</span>
<span class="ltx_bibblock">Beyond Window Dressing: Public Participation
for Marginalized Communities in the Datafied Society.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glucker et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Anne N. Glucker, Peter
P. J. Driessen, Arend Kolhoff, and Hens
A. C. Runhaar. 2013.

</span>
<span class="ltx_bibblock">Public Participation in Environmental Impact
Assessment: Why, Who and How?

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Environmental Impact Assessment Review</em>
43 (Nov. 2013),
104–111.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.eiar.2013.06.003" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.eiar.2013.06.003</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldenfein (2023)</span>
<span class="ltx_bibblock">
Jake Goldenfein.
2023.

</span>
<span class="ltx_bibblock">Privacy’s Loose Grip on Facial Recognition -
Law and the Operational Image.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.2139/ssrn.4656713" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.2139/ssrn.4656713</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonzales et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Aldren Gonzales,
Guruprabha Guruswamy, and Scott R.
Smith. 2023.

</span>
<span class="ltx_bibblock">Synthetic Data in Health Care: A Narrative
Review.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">PLOS Digital Health</em> 2,
1 (Jan. 2023),
e0000082.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1371/journal.pdig.0000082" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1371/journal.pdig.0000082</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gray and Suri (2019)</span>
<span class="ltx_bibblock">
Mary L. Gray and
Siddharth Suri. 2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Ghost Work: How to Stop Silicon
Valley from Building a New Global Underclass</em>.

</span>
<span class="ltx_bibblock">Eamon Dolan Books.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groves et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Lara Groves, Aidan
Peppin, Andrew Strait, and Jenny
Brennan. 2023.

</span>
<span class="ltx_bibblock">Going Public: The Role of Public Participation
Approaches in Commercial AI Labs. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">2023
ACM Conference on Fairness, Accountability, and
Transparency</em>. ACM, Chicago IL
USA, 1162–1173.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3593013.3594071" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3593013.3594071</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halloran et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
John Halloran, Eva
Hornecker, Mark Stringer, Eric Harris,
and Geraldine Fitzpatrick.
2009.

</span>
<span class="ltx_bibblock">The Value of Values: Resourcing Co-Design of
Ubiquitous Computing.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">CoDesign</em> 5,
4 (Dec. 2009),
245–273.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1080/15710880902920960" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/15710880902920960</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamidi et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Foad Hamidi, Morgan Klaus
Scheuerman, and Stacy M. Branham.
2018.

</span>
<span class="ltx_bibblock">Gender Recognition or Gender Reductionism?
The Social Implications of Embedded Gender Recognition Systems. In
<em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems</em> <em id="bib.bib49.4.2" class="ltx_emph ltx_font_italic">(CHI
’18)</em>. Association for Computing Machinery,
New York, NY, USA, 1–13.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3173574.3173582" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3173574.3173582</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harron et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Katie Harron, Ruth
Gilbert, David Cromwell, and Jan
van der Meulen. 2016.

</span>
<span class="ltx_bibblock">Linking Data for Mothers and Babies in
De-Identified Electronic Health Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">PLOS ONE</em> 11,
10 (Oct. 2016),
e0164667.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1371/journal.pone.0164667" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1371/journal.pone.0164667</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harvey and LaPlace (2021)</span>
<span class="ltx_bibblock">
Adam Harvey and Jules
LaPlace. 2021.

</span>
<span class="ltx_bibblock">Researchers Gone Wild: Origins and
Endpoints of Image Training Datasets Created ”In the Wild”.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Design</em> (1 ed.),
Bianca Herlo, Daniel
Irrgang, Gesche Joost, and Andreas
Unteidig (Eds.). Vol. 54. transcript
Verlag, Bielefeld, Germany, 289–310.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.14361/9783839457603-016" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.14361/9783839457603-016</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chengsong Hu, Bishwa B.
Sapkota, J. Alex Thomasson, and
Muthukumar V. Bagavathiannan.
2021.

</span>
<span class="ltx_bibblock">Influence of Image Quality and Light
Consistency on the Performance of Convolutional Neural Networks for
Weed Mapping.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Remote Sensing</em> 13,
11 (Jan. 2021),
2140.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/rs13112140" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/rs13112140</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hügel and Davies (2020)</span>
<span class="ltx_bibblock">
Stephan Hügel and
Anna R. Davies. 2020.

</span>
<span class="ltx_bibblock">Public Participation, Engagement, and Climate
Change Adaptation: A Review of the Research Literature.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Wiley Interdisciplinary Reviews. Climate
Change</em> 11, 4 (2020),
e645.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1002/wcc.645" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1002/wcc.645</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutchinson et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ben Hutchinson, Andrew
Smart, Alex Hanna, Emily Denton,
Christina Greer, Oddur Kjartansson,
Parker Barnes, and Margaret Mitchell.
2021.

</span>
<span class="ltx_bibblock">Towards Accountability for Machine Learning
Datasets: Practices from Software Engineering and
Infrastructure. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021
ACM Conference on Fairness, Accountability, and
Transparency</em> <em id="bib.bib54.4.2" class="ltx_emph ltx_font_italic">(FAccT ’21)</em>.
Association for Computing Machinery,
New York, NY, USA, 560–575.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3442188.3445918" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3442188.3445918</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutson and Winters (2022)</span>
<span class="ltx_bibblock">
Jevan Hutson and Ben
Winters. 2022.

</span>
<span class="ltx_bibblock">America’s Next ’Stop Model!’: Model
Disgorgement.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.2139/ssrn.4225003" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.2139/ssrn.4225003</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irani (2019)</span>
<span class="ltx_bibblock">
Lilly Irani.
2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Chasing Innovation: Making Entrepreneurial
Citizens in Modern India</em>.

</span>
<span class="ltx_bibblock">Princeton University Press,
Princeton, New Jersey ; Oxford, United Kingdom.

</span>
<span class="ltx_bibblock">


</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irgens et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Golnaz Arastoopour Irgens,
Ibrahim Adisa, Cinamon Bailey, and
Hazel Vega Quesada. 2022.

</span>
<span class="ltx_bibblock">Designing with and for Youth: A Participatory
Design Research Approach for Critical Machine Learning Education.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Educational Technology &amp; Society</em>
25, 4 (2022),
126–141.

</span>
<span class="ltx_bibblock">
arXiv:48695986

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iversen et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Ole Sejer Iversen, Kim
Halskov, and Tuck W. Leong.
2012.

</span>
<span class="ltx_bibblock">Values-Led Participatory Design.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">CoDesign</em> 8,
2-3 (June 2012),
87–103.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1080/15710882.2012.672575" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/15710882.2012.672575</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jackson (2014)</span>
<span class="ltx_bibblock">
Steven J. Jackson.
2014.

</span>
<span class="ltx_bibblock">Rethinking Repair.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Media Technologies</em>,
Tarleton Gillespie,
Pablo J. Boczkowski, and Kirsten A.
Foot (Eds.). The MIT Press, 221–240.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.7551/mitpress/9780262525374.003.0011" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.7551/mitpress/9780262525374.003.0011</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobsen (2023)</span>
<span class="ltx_bibblock">
Benjamin N Jacobsen.
2023.

</span>
<span class="ltx_bibblock">Machine Learning and the Politics of Synthetic
Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Big Data &amp; Society</em> 10,
1 (Jan. 2023),
20539517221145372.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/20539517221145372" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/20539517221145372</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jordon et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
James Jordon, Lukasz
Szpruch, Florimond Houssiau, Mirko
Bottarelli, Giovanni Cherubin, Carsten
Maple, Samuel N. Cohen, and Adrian
Weller. 2022.

</span>
<span class="ltx_bibblock">Synthetic Data – What, Why and How?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2205.03257 [cs]

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamiran and Calders (2009)</span>
<span class="ltx_bibblock">
Faisal Kamiran and Toon
Calders. 2009.

</span>
<span class="ltx_bibblock">Classifying without Discriminating. In
<em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Control and Communication 2009 2nd
International Conference on Computer</em>. 1–6.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/IC4.2009.4909197" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IC4.2009.4909197</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kawulok et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Michal Kawulok, M. Emre
Celebi, and Bogdan Smolka (Eds.).
2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">Advances in Face Detection and Facial
Image Analysis</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing,
Cham.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-319-25958-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-319-25958-1</a>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan and Hanna (2022)</span>
<span class="ltx_bibblock">
Mehtab Khan and Alex
Hanna. 2022.

</span>
<span class="ltx_bibblock">The Subjects and Stages of AI Dataset
Development: A Framework for Dataset Accountability.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.2139/ssrn.4217148" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.2139/ssrn.4217148</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kortylewski et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Adam Kortylewski, Bernhard
Egger, Andreas Schneider, Thomas Gerig,
Andreas Morel-Forster, and Thomas
Vetter. 2019.

</span>
<span class="ltx_bibblock">Analyzing and Reducing the Damage of
Dataset Bias to Face Recognition With Synthetic Data. In
<em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops (CVPRW)</em>.
IEEE, Long Beach, CA, USA,
2261–2268.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/CVPRW.2019.00279" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPRW.2019.00279</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nicol Turner Lee, Paul
Resnick, and Genie Barton.
2019.

</span>
<span class="ltx_bibblock">Algorithmic Bias Detection and Mitigation: Best
Practices and Policies to Reduce Consumer Harms.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li (2022)</span>
<span class="ltx_bibblock">
Tiffany C. Li.
2022.

</span>
<span class="ltx_bibblock">Algorithmic Destruction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.2139/ssrn.4066845" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.2139/ssrn.4066845</a>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jialin Liu, Sam
Snodgrass, Ahmed Khalifa, Sebastian
Risi, Georgios N. Yannakakis, and
Julian Togelius. 2021.

</span>
<span class="ltx_bibblock">Deep Learning for Procedural Content Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em>
33, 1 (Jan.
2021), 19–37.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s00521-020-05383-8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s00521-020-05383-8</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Rojas and Axelsson (2012)</span>
<span class="ltx_bibblock">
Edgar Alonso Lopez-Rojas and
Stefan Axelsson. 2012.

</span>
<span class="ltx_bibblock">Money Laundering Detection Using Synthetic
Data. In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Annual Workshop of the Swedish
Artificial Intelligence Society (SAIS)</em>.
Linköping University Electronic Press, Linköpings
universitet.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martínez et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Gonzalo Martínez,
Lauren Watson, Pedro Reviriego,
José Alberto Hernández, Marc
Juarez, and Rik Sarkar.
2023.

</span>
<span class="ltx_bibblock">Towards Understanding the Interplay of
Generative Artificial Intelligence and the Internet.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2306.06130" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2306.06130</a>
arXiv:2306.06130 [cs]

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ngufor et al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Che Ngufor, Holly
Van Houten, Brian S. Caffo, Nilay D.
Shah, and Rozalina G. McCoy.
2019.

</span>
<span class="ltx_bibblock">Mixed Effect Machine Learning: A Framework for
Predicting Longitudinal Change in Hemoglobin A1c.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>
89 (Jan. 2019),
56–67.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.jbi.2018.09.001" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.jbi.2018.09.001</a>

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko (2022)</span>
<span class="ltx_bibblock">
Sergey I. Nikolenko.
2022.

</span>
<span class="ltx_bibblock">Synthesis Humans. Create Perfectly Labeled
Images &amp; Video for ML Models.

</span>
<span class="ltx_bibblock">https://synthesis.ai/synthesis-humans/.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noble (2021)</span>
<span class="ltx_bibblock">
Safiya Noble.
2021.

</span>
<span class="ltx_bibblock">Algorithms of Oppression: How Search Engines
Reinforce Racism.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Science</em> 374,
6567 (Oct. 2021),
542–542.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1126/science.abm5861" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1126/science.abm5861</a>

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Norman et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Justin Norman, Shruti
Agarwal, and Hany Farid.
2023.

</span>
<span class="ltx_bibblock">An Evaluation of Forensic Facial
Recognition.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2311.06145" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2311.06145</a>
arXiv:2311.06145 [cs]

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Junwon Park, Ranjay
Krishna, Pranav Khadpe, Li Fei-Fei,
and Michael Bernstein. 2019.

</span>
<span class="ltx_bibblock">AI-Based Request Augmentation to Increase
Crowdsourcing Participation.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Human
Computation and Crowdsourcing</em> 7 (Oct.
2019), 115–124.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1609/hcomp.v7i1.5282" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1609/hcomp.v7i1.5282</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasquale (2021)</span>
<span class="ltx_bibblock">
Frank Pasquale.
2021.

</span>
<span class="ltx_bibblock">Licensure as Data Governance.

</span>
<span class="ltx_bibblock">(Sept. 2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span id="bib.bib77.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Reema Patel, Aidan
Peppin, Valentina Pavel, Jenny Brennan,
Imogen Parker, and Cansu Safak.
2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.3.1" class="ltx_emph ltx_font_italic">Participatory Data Stewardship</em>.

</span>
<span class="ltx_bibblock">Technical Report. Ada
Lovelace Institute.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paullada et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Amandalynne Paullada,
Inioluwa Deborah Raji, Emily M. Bender,
Emily Denton, and Alex Hanna.
2021.

</span>
<span class="ltx_bibblock">Data and Its (Dis)Contents: A Survey of Dataset
Development and Use in Machine Learning Research.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">Patterns</em> 2,
11 (Nov. 2021),
100336.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.patter.2021.100336" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.patter.2021.100336</a>

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Kenny Peng, Arunesh
Mathur, and Arvind Narayanan.
2021.

</span>
<span class="ltx_bibblock">Mitigating Dataset Harms Requires Stewardship:
Lessons from 1000 Papers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2108.02922" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2108.02922</a>
arXiv:2108.02922 [cs]

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez (2023)</span>
<span class="ltx_bibblock">
Daniel Perez.
2023.

</span>
<span class="ltx_bibblock">ChatGPT: Learning Tool — or Threat?
How a Texas College Is Eyeing New AI Program.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Haibo Qiu, Baosheng Yu,
Dihong Gong, Zhifeng Li,
Wei Liu, and Dacheng Tao.
2021.

</span>
<span class="ltx_bibblock">SynFace: Face Recognition With Synthetic
Data. In <em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>.
10880–10890.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghavan et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Manish Raghavan, Solon
Barocas, Jon Kleinberg, and Karen
Levy. 2020.

</span>
<span class="ltx_bibblock">Mitigating Bias in Algorithmic Hiring: Evaluating
Claims and Practices. In <em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency</em>
<em id="bib.bib82.4.2" class="ltx_emph ltx_font_italic">(FAT* ’20)</em>. Association for
Computing Machinery, New York, NY, USA,
469–481.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3351095.3372828" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3351095.3372828</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rainie et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Stephanie Carroll Rainie,
Tahu Kukutai, Maggie Walter,
Oscar Luis Figueroa-Rodríguez,
Jennifer Walker, and Per Axelsson.
2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">Indigenous Data Sovereignty</em>.

</span>
<span class="ltx_bibblock">African Minds and the International Development
Research Centre (IDRC). 300–319 pages.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raji and Fried (2021)</span>
<span class="ltx_bibblock">
Inioluwa Deborah Raji and
Genevieve Fried. 2021.

</span>
<span class="ltx_bibblock">About Face: A Survey of Facial Recognition
Evaluation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2102.00813" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2102.00813</a>
arXiv:2102.00813 [cs]

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raji et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Inioluwa Deborah Raji,
Timnit Gebru, Margaret Mitchell,
Joy Buolamwini, Joonseok Lee, and
Emily Denton. 2020.

</span>
<span class="ltx_bibblock">Saving Face: Investigating the Ethical
Concerns of Facial Recognition Auditing.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2001.00964" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2001.00964</a>
arXiv:2001.00964 [cs]

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Razmetaeva (2020)</span>
<span class="ltx_bibblock">
Yulia Razmetaeva.
2020.

</span>
<span class="ltx_bibblock">The Right to Be Forgotten in the European
Perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">TalTech Journal of European Studies</em>
10, 1 (May
2020), 58–76.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1515/bjes-2020-0004" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1515/bjes-2020-0004</a>

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Recht et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Benjamin Recht, Rebecca
Roelofs, Ludwig Schmidt, and Vaishaal
Shankar. 2019.

</span>
<span class="ltx_bibblock">Do ImageNet Classifiers Generalize to
ImageNet?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1902.10811" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1902.10811</a>
arXiv:1902.10811 [cs, stat]

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Salehi (2020)</span>
<span class="ltx_bibblock">
Samantha Robertson and
Niloufar Salehi. 2020.

</span>
<span class="ltx_bibblock">What If I Don’t Like Any Of The Choices?
The Limits of Preference Elicitation for Participatory Algorithm
Design.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2007.06718" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2007.06718</a>
arXiv:2007.06718 [cs]

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheuerman et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Morgan Klaus Scheuerman,
Kandrea Wade, Caitlin Lustig, and
Jed R. Brubaker. 2020.

</span>
<span class="ltx_bibblock">How We’ve Taught Algorithms to See
Identity: Constructing Race and Gender in Image Databases for
Facial Analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Human-Computer
Interaction</em> 4, CSCW1
(May 2020), 58:1–58:35.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3392866" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3392866</a>

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheuerman et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Morgan Klaus Scheuerman,
Katy Weathington, Tarun Mugunthan,
Emily Denton, and Casey Fiesler.
2023.

</span>
<span class="ltx_bibblock">From Human to Data to Dataset:
Mapping the Traceability of Human Subjects in Computer Vision
Datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Human-Computer
Interaction</em> 7, CSCW1
(April 2023), 1–33.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3579488" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3579488</a>

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar
Shumaylov, Yiren Zhao, Yarin Gal,
Nicolas Papernot, and Ross Anderson.
2023.

</span>
<span class="ltx_bibblock">The Curse of Recursion: Training on
Generated Data Makes Models Forget.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.17493" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.17493</a>
arXiv:2305.17493 [cs]

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sloane (2022)</span>
<span class="ltx_bibblock">
Mona Sloane.
2022.

</span>
<span class="ltx_bibblock">To Make AI Fair, Here’s What We Must Learn to
Do.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Nature</em> 605
(May 2022), 9–9.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1038/d41586-022-01202-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/d41586-022-01202-3</a>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sloane et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mona Sloane, Emanuel
Moss, Olaitan Awomolo, and Laura
Forlano. 2020.

</span>
<span class="ltx_bibblock">Participation Is Not a Design Fix for Machine
Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2007.02423" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2007.02423</a>
arXiv:2007.02423 [cs]

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solove (2012)</span>
<span class="ltx_bibblock">
Daniel J Solove.
2012.

</span>
<span class="ltx_bibblock">Privacy Self-Management and the Consent
Dilemma.

</span>
<span class="ltx_bibblock">(Nov. 2012).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solove and Hartzog (2014)</span>
<span class="ltx_bibblock">
Daniel J. Solove and
Woodrow Hartzog. 2014.

</span>
<span class="ltx_bibblock">The FTC and the New Common Law of
Privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Columbia Law Review</em> 114,
3 (2014), 583–676.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spade (2015)</span>
<span class="ltx_bibblock">
Dean Spade.
2015.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Normal Life: Administrative
Violence, Critical Trans Politics, and the Limits of Law</em>.

</span>
<span class="ltx_bibblock">Duke University Press.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strickler (2020)</span>
<span class="ltx_bibblock">
Meg Strickler.
2020.

</span>
<span class="ltx_bibblock">Recent Developments in Privacy Law.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Bus. LAw.</em> 76
(2020), 269.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taboh (2021)</span>
<span class="ltx_bibblock">
Julie Taboh.
2021.

</span>
<span class="ltx_bibblock">Facial Recognition Technology Solves Crimes, but
at What Cost?

</span>
<span class="ltx_bibblock">https://www.voanews.com/a/silicon-valley-technology_facial-recognition-technology-solves-crimes-what-cost/6207137.html.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thiel (2023)</span>
<span class="ltx_bibblock">
David Thiel.
2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Identifying and Eliminating CSAM in
Generative ML Training Data and Models</em>.

</span>
<span class="ltx_bibblock">Technical Report. Stanford
Internet Observatory.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh et al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Trieu H. Trinh, Yuhuai
Wu, Quoc V. Le, He He, and
Thang Luong. 2024.

</span>
<span class="ltx_bibblock">Solving Olympiad Geometry without Human
Demonstrations.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">Nature</em> 625,
7995 (Jan. 2024),
476–482.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1038/s41586-023-06747-5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/s41586-023-06747-5</a>

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Viljoen (2021)</span>
<span class="ltx_bibblock">
Salomé Viljoen.
2021.

</span>
<span class="ltx_bibblock">A Relational Theory of Data Governance.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">the yale law journal</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Huang (2019)</span>
<span class="ltx_bibblock">
Xiaoqian Wang and Heng
Huang. 2019.

</span>
<span class="ltx_bibblock">Approaching Machine Learning Fairness through
Adversarial Network.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1909.03013" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1909.03013</a>
arXiv:1909.03013 [cs, stat]

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wehrli et al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Samuel Wehrli, Corinna
Hertweck, Mohammadreza Amirian, Stefan
Glüge, and Thilo Stadelmann.
2022.

</span>
<span class="ltx_bibblock">Bias, Awareness, and Ignorance in
Deep-Learning-Based Face Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">AI and Ethics</em> 2,
3 (Aug. 2022),
509–522.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s43681-021-00108-6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s43681-021-00108-6</a>

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whitney et al<span id="bib.bib104.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Cedric Deslandes Whitney,
Teresa Naval, Elizabeth Quepons,
Simrandeep Singh, Steven R Rick, and
Lilly Irani. 2021.

</span>
<span class="ltx_bibblock">HCI Tactics for Politics from Below:
Meeting the Challenges of Smart Cities. In
<em id="bib.bib104.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems</em>. ACM,
Yokohama Japan, 1–15.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3411764.3445314" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3411764.3445314</a>

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong et al<span id="bib.bib105.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Sebastien C. Wong, Adam
Gatt, Victor Stamatescu, and Mark D.
McDonnell. 2016.

</span>
<span class="ltx_bibblock">Understanding Data Augmentation for Classification:
When to Warp?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1609.08764 [cs]

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wood et al<span id="bib.bib106.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Erroll Wood, Tadas
Baltrušaitis, Charlie Hewitt,
Sebastian Dziadzio, Thomas J. Cashman,
and Jamie Shotton. 2021.

</span>
<span class="ltx_bibblock">Fake It Till You Make It: Face Analysis in
the Wild Using Synthetic Data Alone. In
<em id="bib.bib106.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>. 3681–3691.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhiyan Wu, Yuan Yang,
Jiahui Zhao, and Youqing Wu.
2022.

</span>
<span class="ltx_bibblock">The Impact of Algorithmic Price
Discrimination on Consumers’ Perceived Betrayal.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.3.1" class="ltx_emph ltx_font_italic">Frontiers in Psychology</em>
13 (2022).

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">y Arcas (2018)</span>
<span class="ltx_bibblock">
Blaise Aguera y Arcas.
2018.

</span>
<span class="ltx_bibblock">Do Algorithms Reveal Sexual Orientation or Just
Expose Our Stereotypes?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kaiyu Yang, Klint Qinami,
Li Fei-Fei, Jia Deng, and
Olga Russakovsky. 2020.

</span>
<span class="ltx_bibblock">Towards Fairer Datasets: Filtering and
Balancing the Distribution of the People Subtree in the
ImageNet Hierarchy. In <em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency</em>.
547–558.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3351095.3375709" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3351095.3375709</a>
arXiv:1912.07726 [cs]

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yew and Xiang (2022)</span>
<span class="ltx_bibblock">
Rui-Jie Yew and Alice
Xiang. 2022.

</span>
<span class="ltx_bibblock">Regulating Facial Processing Technologies:
Tensions Between Legal and Technical Considerations in the
Application of Illinois BIPA. In <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">2022
ACM Conference on Fairness, Accountability, and
Transparency</em>. ACM, Seoul
Republic of Korea, 1017–1027.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3531146.3533163" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3531146.3533163</a>

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span id="bib.bib111.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Dong Yi, Zhen Lei,
Shengcai Liao, and Stan Z. Li.
2014.

</span>
<span class="ltx_bibblock">Learning Face Representation from Scratch.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1411.7923 [cs]

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib112.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Lu Zhang, Yongkai Wu,
and Xintao Wu. 2016.

</span>
<span class="ltx_bibblock">A Causal Framework for Discovering and Removing
Direct and Indirect Discrimination.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1611.07509" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1611.07509</a>
arXiv:1611.07509 [cs]

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.01819" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.01820" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.01820">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.01820" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.01821" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 13:47:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
