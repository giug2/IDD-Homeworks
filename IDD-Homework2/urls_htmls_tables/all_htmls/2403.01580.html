<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Declaration</title>
<!--Generated on Sun Mar  3 17:57:35 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.01580v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1" title="Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S1" title="1.1 Neural Networks: an overview ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Neural Networks: an overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S2" title="1.2 Hyperparameters and Subword Models ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Hyperparameters and Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S3" title="1.3 Neural Machine Translation ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Neural Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S4" title="1.4 Research Questions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Research Questions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S4.SS0.SSSx1" title="RQ₁ How can hyperparameters and subword models be optimised for Transformer-based MT in low-resource language settings? ‣ 1.4 Research Questions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title">RQ<math alttext="{}_{1}" class="ltx_Math" display="inline"><semantics><msub><mi></mi><mn>1</mn></msub><annotation-xml encoding="MathML-Content"><apply><cn type="integer">1</cn></apply></annotation-xml><annotation encoding="application/x-tex">{}_{1}</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT</annotation></semantics></math> How can hyperparameters and subword models be optimised for Transformer-based MT in low-resource language settings?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S4.SS0.SSSx2" title="RQ₂ - What is the impact of using small in-domain datasets in improving the performance of MT models for low-resource language pairs, and how can these datasets be effectively developed and utilised? ‣ 1.4 Research Questions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title">RQ<math alttext="{}_{2}" class="ltx_Math" display="inline"><semantics><msub><mi></mi><mn>2</mn></msub><annotation-xml encoding="MathML-Content"><apply><cn type="integer">2</cn></apply></annotation-xml><annotation encoding="application/x-tex">{}_{2}</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> - What is the impact of using small in-domain datasets in improving the performance of MT models for low-resource language pairs, and how can these datasets be effectively developed and utilised?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S4.SS0.SSSx3" title="RQ₃ - How do MT systems, specifically Transformer and RNN-based models, differ in terms of accuracy and fluency errors when evaluated using a human evaluation technique such as the Multidimensional Quality Metrics (MQM) error taxonomy? ‣ 1.4 Research Questions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title">RQ<math alttext="{}_{3}" class="ltx_Math" display="inline"><semantics><msub><mi></mi><mn>3</mn></msub><annotation-xml encoding="MathML-Content"><apply><cn type="integer">3</cn></apply></annotation-xml><annotation encoding="application/x-tex">{}_{3}</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT</annotation></semantics></math> - How do MT systems, specifically Transformer and RNN-based models, differ in terms of accuracy and fluency errors when evaluated using a human evaluation technique such as the Multidimensional Quality Metrics (MQM) error taxonomy?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S4.SS0.SSSx4" title="RQ₄ - How can the process of NMT development, evaluation, and deployment be streamlined for both developers and translators, while also considering environmental sustainability? ‣ 1.4 Research Questions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title">RQ<math alttext="{}_{4}" class="ltx_Math" display="inline"><semantics><msub><mi></mi><mn>4</mn></msub><annotation-xml encoding="MathML-Content"><apply><cn type="integer">4</cn></apply></annotation-xml><annotation encoding="application/x-tex">{}_{4}</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 4 end_FLOATSUBSCRIPT</annotation></semantics></math> - How can the process of NMT development, evaluation, and deployment be streamlined for both developers and translators, while also considering environmental sustainability?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S5" title="1.5 Thesis Outline ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Thesis Outline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S6" title="1.6 Research Contributions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.6 </span>Research Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S7" title="1.7 Publications ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.7 </span>Publications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2" title="Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Transformers for Low-Resource Languages</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S1" title="2.1 Context ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S2" title="2.2 Abstract ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S3" title="2.3 Introduction ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4" title="2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS1" title="2.4.1 Irish Language ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Irish Language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS2" title="2.4.2 Hyperparameter Optimisation ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Hyperparameter Optimisation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS2.SSSx1" title="Recurrent Neural Networks ‣ 2.4.2 Hyperparameter Optimisation ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Recurrent Neural Networks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS2.SSSx2" title="Transformer ‣ 2.4.2 Hyperparameter Optimisation ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS3" title="2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span>Subword Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS3.SSSx1" title="Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Byte Pair Encoding compared with Unigram</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S5" title="2.5 Proposed Approach ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S5.SS1" title="2.5.1 Architecture Tuning ‣ 2.5 Proposed Approach ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.1 </span>Architecture Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S5.SS2" title="2.5.2 Subword Models ‣ 2.5 Proposed Approach ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.2 </span>Subword Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6" title="2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS1" title="2.6.1 Experimental Setup ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS1.SSSx1" title="Datasets ‣ 2.6.1 Experimental Setup ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS1.SSSx2" title="Infrastructure ‣ 2.6.1 Experimental Setup ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS1.SSSx3" title="Metrics ‣ 2.6.1 Experimental Setup ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS2" title="2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS2.SSSx1" title="Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Performance of subword models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS2.SSSx2" title="Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title">Transformer performance compared with RNN</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S7" title="2.7 Environmental Impact ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.7 </span>Environmental Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S8" title="2.8 Discussion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.8 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S9" title="2.9 Conclusion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.9 </span>Conclusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3" title="Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MT in the Covid domain: Shared Task for LoResMT2021</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S1" title="3.1 Context ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S2" title="3.2 Abstract ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S3" title="3.3 Introduction ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S4" title="3.4 Background ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S4.SS1" title="3.4.1 Transformer ‣ 3.4 Background ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S4.SS2" title="3.4.2 Domain Adaptation ‣ 3.4 Background ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Domain Adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S5" title="3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S5.SS1" title="3.5.1 Architecture Tuning ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Architecture Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6" title="3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS1" title="3.6.1 Experimental Setup ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS1.SSSx1" title="Datasets ‣ 3.6.1 Experimental Setup ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS1.SSSx2" title="Infrastructure ‣ 3.6.1 Experimental Setup ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title">Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS1.SSSx3" title="Metrics ‣ 3.6.1 Experimental Setup ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS2" title="3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S7" title="3.7 Discussion ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S8" title="3.8 Conclusion and Future Work ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.8 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4" title="Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_bold ltx_font_italic">gaHealth<span class="ltx_text ltx_font_upright">: EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo mathvariant="normal" stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>normal-↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>GA Bilingual Corpus of Health Data</span></span></span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S1" title="4.1 Context ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S2" title="4.2 Abstract ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S3" title="4.3 Introduction ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4" title="4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4.SS1" title="4.4.1 Transformer ‣ 4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4.SS2" title="4.4.2 Transformer Hyperparameter Optimisation ‣ 4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Transformer Hyperparameter Optimisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4.SS3" title="4.4.3 Neural MT ‣ 4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Neural MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5" title="4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS1" title="4.5.1 Sources for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Sources for <span class="ltx_text ltx_font_italic">gaHealth</span> Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2" title="4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Toolchain used for <span class="ltx_text ltx_font_italic">gaHealth</span> Development</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px1" title="Text extractors ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Text extractors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px2" title="Unicode normalizer ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Unicode normalizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px3" title="Language detector ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Language detector</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px4" title="Sentence splitters ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Sentence splitters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px5" title="Document aligner ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Document aligner</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px6" title="Sentence aligner ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Sentence aligner</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2.SSS0.Px7" title="Text cleaners ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title">Text cleaners</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS3" title="4.5.3 Guidelines ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.3 </span>Guidelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS4" title="4.5.4 Transformer Architecture ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.4 </span>Transformer Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6" title="4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6.SS1" title="4.6.1 Infrastructure ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.1 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6.SS2" title="4.6.2 Metrics ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6.SS3" title="4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.3 </span>Results: Automatic Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S7" title="4.7 Discussion ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S8" title="4.8 Conclusion and Future Work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5" title="Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Human Evaluation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>GA Transformer-Based NMT</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S1" title="5.1 Context ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S2" title="5.2 Abstract ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S3" title="5.3 Introduction ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4" title="5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS1" title="5.4.1 Hyperparameter Optimisation ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>Hyperparameter Optimisation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS1.SSSx1" title="RNN ‣ 5.4.1 Hyperparameter Optimisation ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">RNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS1.SSSx2" title="Transformer ‣ 5.4.1 Hyperparameter Optimisation ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS2" title="5.4.2 SentencePiece ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>SentencePiece</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS3" title="5.4.3 Human Evaluation ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.3 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5" title="5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS1" title="5.5.1 Architecture Tuning ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.1 </span>Architecture Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS2" title="5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.2 </span>Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS3" title="5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.3 </span>Human Evaluation of NMT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS3.SSSx1" title="Scalar Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Scalar Quality Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS3.SSSx2" title="Multidimensional Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Multidimensional Quality Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS3.SSSx3" title="Annotation Setup ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Annotation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS3.SSSx4" title="Inter-Annotator Agreement ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Inter-Annotator Agreement</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6" title="5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS1" title="5.6.1 Experimental Setup ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS1.SSSx1" title="Datasets ‣ 5.6.1 Experimental Setup ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS1.SSSx2" title="Infrastructure ‣ 5.6.1 Experimental Setup ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS1.SSSx3" title="Metrics ‣ 5.6.1 Experimental Setup ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS2" title="5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.2 </span>Automatic Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS2.SSSx1" title="Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Performance of Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS2.SSSx2" title="Transformer Performance Compared with RNN ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Transformer Performance Compared with RNN</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS3" title="5.6.3 Human Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.3 </span>Human Evaluation Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S7" title="5.7 Environmental Impact ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Environmental Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8" title="5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS1" title="5.8.1 Inter-Annotator Reliability ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.1 </span>Inter-Annotator Reliability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS2" title="5.8.2 Performance of Models Relative to Google ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.2 </span>Performance of Models Relative to Google</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3" title="5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.3 </span>Linguistic Observations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3.SSSx1" title="Interpreting Meaning ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Interpreting Meaning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3.SSSx2" title="Core Grammatical Errors ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Core Grammatical Errors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3.SSSx3" title="Commonly-Used Irregular Verbs ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Commonly-Used Irregular Verbs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3.SSSx4" title="Performance of RNN Approach Relative to Transformer Approach ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title">Performance of RNN Approach Relative to Transformer Approach</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS4" title="5.8.4 Limitations of the Study ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.4 </span>Limitations of the Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S9" title="5.9 Conclusions and Future Work ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.9 </span>Conclusions and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6" title="Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Design of an Open-Source Architecture for NMT</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S1" title="6.1 Context ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S2" title="6.2 Abstract ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S3" title="6.3 Credits ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Credits</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S4" title="6.4 Introduction ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5" title="6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5.SS1" title="6.5.1 NMT ‣ 6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.1 </span>NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5.SS2" title="6.5.2 NMT Tools ‣ 6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.2 </span>NMT Tools</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6" title="6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Architecture of adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1" title="6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6.1 </span>adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1.SSSx1" title="Initialisation and Logging ‣ 6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title">Initialisation and Logging</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1.SSSx2" title="Modes of Operation ‣ 6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title">Modes of Operation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1.SSSx3" title="Customisation of Models ‣ 6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title">Customisation of Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1.SSSx4" title="Use of Subword Segmentation ‣ 6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title">Use of Subword Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1.SSSx5" title="Translation and Evaluation ‣ 6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title">Translation and Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS2" title="6.6.2 Infrastructure ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6.2 </span>Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S7" title="6.7 Discussion ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S8" title="6.8 Conclusion and Future Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.8 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7" title="Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>adaptNMT: Open-Source Neural Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S1" title="7.1 Context ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S2" title="7.2 Abstract ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S3" title="7.3 Graphical abstract ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Graphical abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S4" title="7.4 Introduction ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5" title="7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>Neural Networks for MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS1" title="7.5.1 Recurrent Neural Network Architecture ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.1 </span>Recurrent Neural Network Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS2" title="7.5.2 Transformer Architecture ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.2 </span>Transformer Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS3" title="7.5.3 Attention ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.3 </span>Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS4" title="7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.4 </span>NMT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS4.SSSx1" title="Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Modelling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS4.SSSx2" title="Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS4.SSSx3" title="Inference ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS5" title="7.5.5 Subword Models ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.5 </span>Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS6" title="7.5.6 NMT Tools ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.6 </span>NMT Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS7" title="7.5.7 Hyperparameter Optimisation ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.7 </span>Hyperparameter Optimisation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6" title="7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Architecture of adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1" title="7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.1 </span>adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1.SSSx1" title="Initialisation and logging ‣ 7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Initialisation and logging</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1.SSSx2" title="Modes of operation ‣ 7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Modes of operation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1.SSSx3" title="Customisation of models ‣ 7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Customisation of models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1.SSSx4" title="Use of subword segmentation ‣ 7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Use of subword segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1.SSSx5" title="Translation and evaluation ‣ 7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title">Translation and evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS2" title="7.6.2 serverNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.2 </span>serverNMT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7" title="7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS1" title="7.7.1 Infrastructure ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.1 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS2" title="7.7.2 Metrics ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS3" title="7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.3 </span>Results: Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS4" title="7.7.4 Environmental Impact ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.4 </span>Environmental Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS5" title="7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.5 </span>Stochastic Nuances</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S8" title="7.8 Discussion ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.8 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S9" title="7.9 Conclusion and Future Work ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.9 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8" title="Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>adaptMLLM: Fine-Tuning Multilingual Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S1" title="8.1 Context ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S2" title="8.2 Abstract ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S3" title="8.3 Graphical abstract ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Graphical abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S4" title="8.4 Introduction ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5" title="8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS1" title="8.5.1 Transformer Architecture ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.1 </span>Transformer Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS2" title="8.5.2 Multilingual Language Models - NLLB ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.2 </span>Multilingual Language Models - NLLB</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS3" title="8.5.3 Large Language Models ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.3 </span>Large Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS3.SSSx1" title="GPT-J ‣ 8.5.3 Large Language Models ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">GPT-J</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS3.SSSx2" title="GPT-4 ‣ 8.5.3 Large Language Models ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">GPT-4</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS3.SSSx3" title="BARD ‣ 8.5.3 Large Language Models ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">BARD</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS4" title="8.5.4 DeepSpeed ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.4 </span>DeepSpeed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS5" title="8.5.5 HuggingFace ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.5 </span>HuggingFace</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS6" title="8.5.6 Human Evaluation ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.6 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6" title="8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6.SS1" title="8.6.1 Language Pairs ‣ 8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.1 </span>Language Pairs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6.SS2" title="8.6.2 Shared Task Datasets ‣ 8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.2 </span>Shared Task Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7" title="8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS1" title="8.7.1 Initialisation and Pre-processing ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.1 </span>Initialisation and Pre-processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS2" title="8.7.2 Modes of Operation ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.2 </span>Modes of Operation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS3" title="8.7.3 Fine-tuning and Visualisation ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.3 </span>Fine-tuning and Visualisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS4" title="8.7.4 Deployment ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.4 </span>Deployment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS5" title="8.7.5 Green Report ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.5 </span>Green Report</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS6" title="8.7.6 MLLM Translation and Evaluation ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.6 </span>MLLM Translation and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS7" title="8.7.7 LLM Playgrounds ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.7 </span>LLM Playgrounds</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8" title="8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS1" title="8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.1 </span>Infrastructure and Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS2" title="8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.2 </span>Results: Automatic Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS2.SSSx1" title="Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Translation in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>GA directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS2.SSSx2" title="Translation in the EN↔MR directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Translation in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>MR directions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3" title="8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.3 </span>Human Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3.SSSx1" title="Scalar Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Scalar Quality Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3.SSSx2" title="Multidimensional Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Multidimensional Quality Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3.SSSx3" title="Annotation Setup ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Annotation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3.SSSx4" title="Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Inter-Annotator Agreement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3.SSSx5" title="Inter-Annotator Reliability ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Inter-Annotator Reliability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS4" title="8.8.4 Environmental Impact ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.4 </span>Environmental Impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9" title="8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS1" title="8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9.1 </span>Performance of adaptMLLM Relative to Google Translate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS2" title="8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9.2 </span>Linguistic Observations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS2.SSSx1" title="Interpreting Meaning ‣ 8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Interpreting Meaning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS2.SSSx2" title="Core Grammatical Errors ‣ 8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title">Core Grammatical Errors</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S10" title="8.10 Conclusion and Future Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.10 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S11" title="8.11 Limitations of the Study ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.11 </span>Limitations of the Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9" title="Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1" title="9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>Research contributions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS1" title="9.1.1 HPO for Low-resource Languages ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.1 </span>HPO for Low-resource Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS2" title="9.1.2 Corpus Development and Guidelines ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.2 </span>Corpus Development and Guidelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS3" title="9.1.3 Human Evaluation of Low-resource Languages ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.3 </span>Human Evaluation of Low-resource Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS4" title="9.1.4 Explainable AI Architectures ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.4 </span>Explainable AI Architectures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S2" title="9.2 Research Impact ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>Research Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S3" title="9.3 Lessons Learnt ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3 </span>Lessons Learnt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4" title="9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4 </span>Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS1" title="9.4.1 Hyperparameter Tuning ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.1 </span>Hyperparameter Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS2" title="9.4.2 Corpus Development ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.2 </span>Corpus Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS3" title="9.4.3 Human Evaluation ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.3 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS4" title="9.4.4 Explainable AI Architectures ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.4 </span>Explainable AI Architectures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS4.SSSx1" title="Roadmap for adaptNMT ‣ 9.4.4 Explainable AI Architectures ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title">Roadmap for adaptNMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS4.SSSx2" title="Roadmap for adaptMLLM ‣ 9.4.4 Explainable AI Architectures ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title">Roadmap for adaptMLLM</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S5" title="9.5 Final Remarks ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.5 </span>Final Remarks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#A1" title="Appendix A No Language Left Behind"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>No Language Left Behind</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.01580v1 [cs.CL] 03 Mar 2024</div></div>
<article class="ltx_document" lang="en">
<div class="ltx_titlepage" id="id1">
<p class="ltx_p ltx_align_center" id="id1.2"><span class="ltx_text ltx_font_bold" id="id1.2.1" style="font-size:298%;">Enhancing Neural Machine Translation of Low-Resource Languages:<span class="ltx_text ltx_font_medium" id="id1.2.1.1"></span></span></p>
<p class="ltx_p ltx_align_center" id="id1.3"><span class="ltx_text" id="id1.3.1" style="font-size:173%;">Corpus Development, Human Evaluation and Explainable AI Architectures</span></p>
<p class="ltx_p ltx_align_center" id="id1.4"><span class="ltx_text ltx_font_bold" id="id1.4.1" style="font-size:144%;">Séamus Lankford MSc, MBA, BEng<span class="ltx_text ltx_font_medium" id="id1.4.1.1"></span></span></p>
<p class="ltx_p ltx_align_center" id="id1.5"><span class="ltx_text" id="id1.5.1" style="font-size:144%;">Supervised by Prof. Andy Way</span></p>
<p class="ltx_p ltx_align_center" id="id1.6"><span class="ltx_text" id="id1.6.1" style="font-size:144%;">and Dr Haithem Afli (Munster Technological University)</span></p>
<p class="ltx_p ltx_align_center" id="id1.1"><img alt="[Uncaptioned image]" class="ltx_graphics" id="id1.1.g1" src="dcu_logo.png"/><span class="ltx_text" id="id1.1.1" style="font-size:144%;"></span></p>
<p class="ltx_p ltx_align_center" id="id1.7"><span class="ltx_text" id="id1.7.1" style="font-size:144%;">A Thesis Submitted for the Award of Doctor of Philosophy</span></p>
<p class="ltx_p ltx_align_center" id="id1.8"><span class="ltx_text ltx_font_smallcaps" id="id1.8.1" style="font-size:173%;">School of Computing</span></p>
<p class="ltx_p ltx_align_center" id="id1.9"><span class="ltx_text ltx_font_smallcaps" id="id1.9.1" style="font-size:173%;">Dublin City University<span class="ltx_text ltx_font_upright" id="id1.9.1.1"></span></span></p>
<p class="ltx_p ltx_align_center" id="id1.10"><span class="ltx_text" id="id1.10.1" style="font-size:144%;">January 2024</span></p>
</div>
<section class="ltx_chapter" id="Chx1">
<h2 class="ltx_title ltx_title_chapter">Declaration</h2>
<div class="ltx_para" id="Chx1.p1">
<p class="ltx_p" id="Chx1.p1.1">I hereby certify that this material, which I now submit for assessment on the programme of study leading to the award of Doctor of Philosophy (PhD) is entirely my own work, and that I have exercised reasonable care to ensure that the work is original, and does not to the best of my knowledge breach any law of copyright, and has not been taken from the work of others save and to the extent that such work has been cited and acknowledged within the text of my work.

<br class="ltx_break"/>
<br class="ltx_break"/>Signed: 
<br class="ltx_break"/>ID No.: 20216607
<br class="ltx_break"/>Date: January 2nd, 2024
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_chapter" id="Chx2">
<h2 class="ltx_title ltx_title_chapter">Dedication</h2>
<div class="ltx_para" id="Chx2.p1">
<p class="ltx_p" id="Chx2.p1.1">Letty agus Cormac atá caillte. I ndorn Dé go raibh a n-anamacha dílse.</p>
</div>
</section>
<section class="ltx_chapter" id="Chx3">
<h2 class="ltx_title ltx_title_chapter">Acknowledgements</h2>
<div class="ltx_para" id="Chx3.p1">
<p class="ltx_p" id="Chx3.p1.1">I wish to thank my PhD supervisors, Professor Andy Way and Dr Haithem Afli for their unwavering support. Right throughout, Andy has been an exceptional mentor in my career development as a researcher. Despite a busy schedule, he was always available for our scheduled meetings and I was given a great deal of autonomy in how my research should be directed. Furthermore, Andy stepped in to steer me in the right direction whenever needed. Given his decades of experience, Andy helped in identifying research gaps and opportunities. It is a testament to Andy’s professionalism that he has retained his drive and interest in developing his students throughout this career and I consider myself fortunate that our paths have crossed. Likewise, I count myself lucky to have worked with Haithem who not only adopted the role of supervisor but also became an important ally in research projects not directly linked to my PhD. Haithem always exudes a positive attitude which helped greatly when the going got tough. The funding received from MTU was very much appreciated as was the support from Dr Teresa Lynn in the early days of my PhD journey. I must also thank my wife, Helen, who as always was steadfast in her support despite the countless hours I put in. My children, Darragh and Julie must also be acknowledged, not only for their support but also for their love of the Irish language which has been, in part, an inspiration for this work. Táim faoi chomaoin acu.</p>
</div>
</section>
<section class="ltx_chapter" id="Chx4">
<h2 class="ltx_title ltx_title_chapter">Motivation</h2>
<div class="ltx_para" id="Chx4.p1">
<p class="ltx_p" id="Chx4.p1.1">The seeds of my interest in the fields of deep learning (DL) and natural language processing (NLP) were sown while taking an MSc in Artificial Intelligence during the Covid-19 pandemic. The decision to pursue such an MSc was motivated by the rise of artificial intelligence (AI) which can be used both as a force for positive and negative change. Motivated by how AI could be used to impact society positively, I signed up for the MSc programme which culminated with the publication of my mini-thesis: Automatic Neural Architecture using AutoML, Swarm Intelligence and Ensemble methods.</p>
</div>
<div class="ltx_para" id="Chx4.p2">
<p class="ltx_p" id="Chx4.p2.1">I grew up in a family where Irish was spoken with ease as a second language which created an awareness that language is a matter of inheritance, in the same way our place and family are. In my case, my second language was an inheritance from two generations previously, for I had the privilege of knowing and interacting with bilingual grandparents. They had acquired their early speech patterns in the 1890s, when vernacular in Ireland was still transitioning from Irish to English.</p>
</div>
<div class="ltx_para" id="Chx4.p3">
<p class="ltx_p" id="Chx4.p3.1">Given my background as an Irish language speaker, it was clear the foundational principles which I learned throughout the MSc could be applied in the context of a PhD study focused on the neural machine translation (NMT) of low-resource languages. Initially, the research questions were not immediately clear. However, it soon emerged that enhancing NMT of low-resource languages should concentrate on three key pillars namely corpus development, improved human evaluation and the development of transparent and easily understood open-source NMT architectures.</p>
</div>
</section>
<section class="ltx_chapter" id="Chx5">
<h2 class="ltx_title ltx_title_chapter">List of Publications</h2>
<div class="ltx_para" id="Chx5.p1">
<ol class="ltx_enumerate" id="Chx5.S0.I1">
<li class="ltx_item" id="Chx5.S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Chx5.S0.I1.i1.p1">
<p class="ltx_p" id="Chx5.S0.I1.i1.p1.1">Lankford, S., Afli, H. and Way, A., 2023. adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with integrated LLM playgrounds. In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i1.p1.1.1">Information 14.12</span>, issn: 2078-2489, doi: 10.3390/info14120638, url: https://www.mdpi.com/2078-2489/14/12/638.</p>
</div>
</li>
<li class="ltx_item" id="Chx5.S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Chx5.S0.I1.i2.p1">
<p class="ltx_p" id="Chx5.S0.I1.i2.p1.1">Lankford, S., Afli, H. and Way, A., 2023. adaptNMT: an open-source, language-agnostic development environment for neural machine translation. In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i2.p1.1.1">Language Resources and Evaluation</span>, pp.1-26. url: https://doi.org/10.1007/s10579-023-09671-2</p>
</div>
</li>
<li class="ltx_item" id="Chx5.S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Chx5.S0.I1.i3.p1">
<p class="ltx_p" id="Chx5.S0.I1.i3.p1.1">Lankford, S., Afli, H. and Way, A., 2023. Design of an Open-Source Architecture for Neural Machine Translation. In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i3.p1.1.1">Proceedings of the 1st Workshop on Open Community-Driven Machine Translation</span>. Tampere, Finland: European Association for Machine Translation, pp. 15–20. 
<br class="ltx_break"/>url: https://aclanthology.org/2023.crowdmt-1.2.</p>
</div>
</li>
<li class="ltx_item" id="Chx5.S0.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Chx5.S0.I1.i4.p1">
<p class="ltx_p" id="Chx5.S0.I1.i4.p1.1">Lankford, S., Afli, H. and Way, A., 2022. Human Evaluation of English-Irish Transformer-Based NMT. In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i4.p1.1.1">Information 13.7</span>, issn: 2078-2489. doi: 10.3390/info13070309. url: https://www.mdpi.com/2078-2489/13/7/309.</p>
</div>
</li>
<li class="ltx_item" id="Chx5.S0.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Chx5.S0.I1.i5.p1">
<p class="ltx_p" id="Chx5.S0.I1.i5.p1.1">Lankford, S., Afli, H., Ní Loinsigh Ó., Way, A., 2022. “gaHealth: An English–Irish Bilingual Corpus of Health Data”. In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i5.p1.1.1">Proceedings of the Thirteenth Language Resources and Evaluation Conference</span>. Marseille, France: European Language Resources Association, pp. 6753–6758. 
<br class="ltx_break"/>url: https://aclanthology.org/2022.lrec-1.727.</p>
</div>
</li>
<li class="ltx_item" id="Chx5.S0.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Chx5.S0.I1.i6.p1">
<p class="ltx_p" id="Chx5.S0.I1.i6.p1.1">Lankford, S., Afli, H., and Way, A. 2021. “Transformers for Low-Resource Languages: Is Féidir Linn!” In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i6.p1.1.1">Proceedings of Machine Translation Summit XVIII: Research Track</span>. Virtual: Association for Machine Translation in the Americas, pp. 48–60. url: https://aclanthology.org/2021.mtsummit-research.5.</p>
</div>
</li>
<li class="ltx_item" id="Chx5.S0.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Chx5.S0.I1.i7.p1">
<p class="ltx_p" id="Chx5.S0.I1.i7.p1.1">Lankford, S., Afli, H., and Way, A. 2021. “Machine Translation in the Covid domain: an EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.S0.I1.i7.p1.1.m1.1"><semantics id="Chx5.S0.I1.i7.p1.1.m1.1a"><mo id="Chx5.S0.I1.i7.p1.1.m1.1.1" stretchy="false" xref="Chx5.S0.I1.i7.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.S0.I1.i7.p1.1.m1.1b"><ci id="Chx5.S0.I1.i7.p1.1.m1.1.1.cmml" xref="Chx5.S0.I1.i7.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.S0.I1.i7.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.S0.I1.i7.p1.1.m1.1d">↔</annotation></semantics></math>GA case study for LoResMT 2021”. In: <span class="ltx_text ltx_font_italic" id="Chx5.S0.I1.i7.p1.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</span>. Virtual: Association for Machine Translation in the Americas, pp. 144–150. url: https://aclanthology.org/2021.mtsummit-loresmt.15</p>
</div>
</li>
</ol>
</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1" title="Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S1" title="1.1 Neural Networks: an overview ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Neural Networks: an overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S2" title="1.2 Hyperparameters and Subword Models ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Hyperparameters and Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S3" title="1.3 Neural Machine Translation ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Neural Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S4" title="1.4 Research Questions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Research Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S5" title="1.5 Thesis Outline ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Thesis Outline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S6" title="1.6 Research Contributions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.6 </span>Research Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.S7" title="1.7 Publications ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.7 </span>Publications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2" title="Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Transformers for Low-Resource Languages</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S1" title="2.1 Context ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S2" title="2.2 Abstract ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S3" title="2.3 Introduction ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4" title="2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS1" title="2.4.1 Irish Language ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Irish Language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS2" title="2.4.2 Hyperparameter Optimisation ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Hyperparameter Optimisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S4.SS3" title="2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span>Subword Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S5" title="2.5 Proposed Approach ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S5.SS1" title="2.5.1 Architecture Tuning ‣ 2.5 Proposed Approach ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.1 </span>Architecture Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S5.SS2" title="2.5.2 Subword Models ‣ 2.5 Proposed Approach ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.2 </span>Subword Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6" title="2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS1" title="2.6.1 Experimental Setup ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S6.SS2" title="2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S7" title="2.7 Environmental Impact ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.7 </span>Environmental Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S8" title="2.8 Discussion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.8 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.S9" title="2.9 Conclusion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.9 </span>Conclusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3" title="Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MT in the Covid domain: Shared Task for LoResMT2021</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S1" title="3.1 Context ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S2" title="3.2 Abstract ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S3" title="3.3 Introduction ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S4" title="3.4 Background ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S4.SS1" title="3.4.1 Transformer ‣ 3.4 Background ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S4.SS2" title="3.4.2 Domain Adaptation ‣ 3.4 Background ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Domain Adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S5" title="3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S5.SS1" title="3.5.1 Architecture Tuning ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Architecture Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6" title="3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS1" title="3.6.1 Experimental Setup ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S6.SS2" title="3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S7" title="3.7 Discussion ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.S8" title="3.8 Conclusion and Future Work ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.8 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4" title="Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_bold ltx_font_italic">gaHealth<span class="ltx_text ltx_font_upright">: EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo mathvariant="normal" stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>normal-↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>GA Bilingual Corpus of Health Data</span></span></span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S1" title="4.1 Context ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S2" title="4.2 Abstract ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S3" title="4.3 Introduction ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4" title="4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4.SS1" title="4.4.1 Transformer ‣ 4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4.SS2" title="4.4.2 Transformer Hyperparameter Optimisation ‣ 4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Transformer Hyperparameter Optimisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S4.SS3" title="4.4.3 Neural MT ‣ 4.4 Related work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Neural MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5" title="4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS1" title="4.5.1 Sources for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Sources for <span class="ltx_text ltx_font_italic">gaHealth</span> Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS2" title="4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Toolchain used for <span class="ltx_text ltx_font_italic">gaHealth</span> Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS3" title="4.5.3 Guidelines ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.3 </span>Guidelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S5.SS4" title="4.5.4 Transformer Architecture ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.4 </span>Transformer Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6" title="4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6.SS1" title="4.6.1 Infrastructure ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.1 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6.SS2" title="4.6.2 Metrics ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S6.SS3" title="4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.3 </span>Results: Automatic Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S7" title="4.7 Discussion ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.S8" title="4.8 Conclusion and Future Work ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5" title="Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Human Evaluation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>GA Transformer-Based NMT</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S1" title="5.1 Context ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S2" title="5.2 Abstract ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S3" title="5.3 Introduction ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4" title="5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS1" title="5.4.1 Hyperparameter Optimisation ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>Hyperparameter Optimisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS2" title="5.4.2 SentencePiece ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>SentencePiece</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4.SS3" title="5.4.3 Human Evaluation ‣ 5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.3 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5" title="5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS1" title="5.5.1 Architecture Tuning ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.1 </span>Architecture Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS2" title="5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.2 </span>Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5.SS3" title="5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.3 </span>Human Evaluation of NMT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6" title="5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS1" title="5.6.1 Experimental Setup ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS2" title="5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.2 </span>Automatic Evaluation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6.SS3" title="5.6.3 Human Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.3 </span>Human Evaluation Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S7" title="5.7 Environmental Impact ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Environmental Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8" title="5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS1" title="5.8.1 Inter-Annotator Reliability ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.1 </span>Inter-Annotator Reliability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS2" title="5.8.2 Performance of Models Relative to Google ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.2 </span>Performance of Models Relative to Google</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3" title="5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.3 </span>Linguistic Observations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS4" title="5.8.4 Limitations of the Study ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.4 </span>Limitations of the Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S9" title="5.9 Conclusions and Future Work ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.9 </span>Conclusions and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6" title="Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Design of an Open-Source Architecture for NMT</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S1" title="6.1 Context ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S2" title="6.2 Abstract ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S3" title="6.3 Credits ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Credits</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S4" title="6.4 Introduction ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5" title="6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5.SS1" title="6.5.1 NMT ‣ 6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.1 </span>NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5.SS2" title="6.5.2 NMT Tools ‣ 6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.2 </span>NMT Tools</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6" title="6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Architecture of adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS1" title="6.6.1 adaptNMT ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6.1 </span>adaptNMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6.SS2" title="6.6.2 Infrastructure ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6.2 </span>Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S7" title="6.7 Discussion ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S8" title="6.8 Conclusion and Future Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.8 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7" title="Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>adaptNMT: Open-Source Neural Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S1" title="7.1 Context ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S2" title="7.2 Abstract ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S3" title="7.3 Graphical abstract ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Graphical abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S4" title="7.4 Introduction ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5" title="7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>Neural Networks for MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS1" title="7.5.1 Recurrent Neural Network Architecture ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.1 </span>Recurrent Neural Network Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS2" title="7.5.2 Transformer Architecture ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.2 </span>Transformer Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS3" title="7.5.3 Attention ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.3 </span>Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS4" title="7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.4 </span>NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS5" title="7.5.5 Subword Models ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.5 </span>Subword Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS6" title="7.5.6 NMT Tools ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.6 </span>NMT Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS7" title="7.5.7 Hyperparameter Optimisation ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.7 </span>Hyperparameter Optimisation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6" title="7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Architecture of adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1" title="7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.1 </span>adaptNMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS2" title="7.6.2 serverNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.2 </span>serverNMT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7" title="7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS1" title="7.7.1 Infrastructure ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.1 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS2" title="7.7.2 Metrics ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS3" title="7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.3 </span>Results: Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS4" title="7.7.4 Environmental Impact ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.4 </span>Environmental Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS5" title="7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7.5 </span>Stochastic Nuances</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S8" title="7.8 Discussion ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.8 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S9" title="7.9 Conclusion and Future Work ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.9 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8" title="Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>adaptMLLM: Fine-Tuning Multilingual Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S1" title="8.1 Context ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S2" title="8.2 Abstract ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S3" title="8.3 Graphical abstract ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Graphical abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S4" title="8.4 Introduction ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5" title="8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS1" title="8.5.1 Transformer Architecture ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.1 </span>Transformer Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS2" title="8.5.2 Multilingual Language Models - NLLB ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.2 </span>Multilingual Language Models - NLLB</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS3" title="8.5.3 Large Language Models ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.3 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS4" title="8.5.4 DeepSpeed ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.4 </span>DeepSpeed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS5" title="8.5.5 HuggingFace ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.5 </span>HuggingFace</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS6" title="8.5.6 Human Evaluation ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5.6 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6" title="8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6.SS1" title="8.6.1 Language Pairs ‣ 8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.1 </span>Language Pairs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6.SS2" title="8.6.2 Shared Task Datasets ‣ 8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.2 </span>Shared Task Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7" title="8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS1" title="8.7.1 Initialisation and Pre-processing ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.1 </span>Initialisation and Pre-processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS2" title="8.7.2 Modes of Operation ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.2 </span>Modes of Operation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS3" title="8.7.3 Fine-tuning and Visualisation ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.3 </span>Fine-tuning and Visualisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS4" title="8.7.4 Deployment ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.4 </span>Deployment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS5" title="8.7.5 Green Report ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.5 </span>Green Report</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS6" title="8.7.6 MLLM Translation and Evaluation ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.6 </span>MLLM Translation and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7.SS7" title="8.7.7 LLM Playgrounds ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7.7 </span>LLM Playgrounds</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8" title="8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS1" title="8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.1 </span>Infrastructure and Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS2" title="8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.2 </span>Results: Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS3" title="8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.3 </span>Human Evaluation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8.SS4" title="8.8.4 Environmental Impact ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8.4 </span>Environmental Impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9" title="8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS1" title="8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9.1 </span>Performance of adaptMLLM Relative to Google Translate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS2" title="8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9.2 </span>Linguistic Observations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S10" title="8.10 Conclusion and Future Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.10 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S11" title="8.11 Limitations of the Study ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.11 </span>Limitations of the Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9" title="Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1" title="9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>Research contributions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS1" title="9.1.1 HPO for Low-resource Languages ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.1 </span>HPO for Low-resource Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS2" title="9.1.2 Corpus Development and Guidelines ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.2 </span>Corpus Development and Guidelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS3" title="9.1.3 Human Evaluation of Low-resource Languages ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.3 </span>Human Evaluation of Low-resource Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S1.SS4" title="9.1.4 Explainable AI Architectures ‣ 9.1 Research contributions ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.4 </span>Explainable AI Architectures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S2" title="9.2 Research Impact ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>Research Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S3" title="9.3 Lessons Learnt ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3 </span>Lessons Learnt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4" title="9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4 </span>Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS1" title="9.4.1 Hyperparameter Tuning ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.1 </span>Hyperparameter Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS2" title="9.4.2 Corpus Development ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.2 </span>Corpus Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS3" title="9.4.3 Human Evaluation ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.3 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S4.SS4" title="9.4.4 Explainable AI Architectures ‣ 9.4 Future Work ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4.4 </span>Explainable AI Architectures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch9.S5" title="9.5 Final Remarks ‣ Chapter 9 Conclusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.5 </span>Final Remarks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#A1" title="Appendix A No Language Left Behind"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>No Language Left Behind</span></a></li>
</ol></nav>
<nav class="ltx_TOC ltx_list_lof ltx_toc_lof"><h6 class="ltx_title ltx_title_contents">List of Figures</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.F1" title="Figure 1.1 ‣ 1.1 Neural Networks: an overview ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Simplified feed-forward NN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.F2" title="Figure 1.2 ‣ 1.6 Research Contributions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Timeline of publications and mapping to thesis chapters.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F1" title="Figure 2.1 ‣ Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Proposed approach of Transformers for low-resource languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F2" title="Figure 2.2 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>BLEU performance for all model architectures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F3" title="Figure 2.3 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>TER performance for all model architectures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F4" title="Figure 2.4 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Training DGT Transformer baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F5" title="Figure 2.5 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Training DGT Transformer 16k BPE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F1" title="Figure 3.1 ‣ 3.1 Context ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Proposed approach for LoResMT2021 Shared Task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F2" title="Figure 3.2 ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Proposed approach of MT in Covid domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F3" title="Figure 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Translation performance using Transformers with 2 heads</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F2.sf1" title="3.2(a) ‣ Figure 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">(a) </span>BLEU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F2.sf2" title="3.2(b) ‣ Figure 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">(b) </span>TER</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F1" title="Figure 4.1 ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Corpus development process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F2" title="Figure 4.2 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text ltx_font_italic">gaHealth</span> en2ga* system: training EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F3" title="Figure 4.3 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>adapt covid_extended system:training EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F4" title="Figure 4.4 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span><span class="ltx_text ltx_font_italic">gaHealth</span> ga2en system: training GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F1" title="Figure 5.1 ‣ 5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Proposed approach to evaluate RNN and Transformer architectures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F2" title="Figure 5.2 ‣ Multidimensional Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>The core set of error categories proposed by the MQM guidelines.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F3" title="Figure 5.3 ‣ Transformer Performance Compared with RNN ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>BLEU performance for all model architectures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F4" title="Figure 5.4 ‣ Transformer Performance Compared with RNN ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>TER performance for all model architectures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F5" title="Figure 5.5 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Transformer baseline.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F6" title="Figure 5.6 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Transformer 16k BPE subword model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.F1" title="Figure 6.1 ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Proposed architecture for adaptNMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F1" title="Figure 7.1 ‣ 7.3 Graphical abstract ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Graphical abstract summarising the adaptNMT system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F2" title="Figure 7.2 ‣ 7.5.2 Transformer Architecture ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Transformer architecture using an encoder-decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F3" title="Figure 7.3 ‣ 7.5.3 Attention ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Multi-head attention in the decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F4" title="Figure 7.4 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Neurons within an RNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F5" title="Figure 7.5 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>Encoder-decoder architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F6" title="Figure 7.6 ‣ Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Beam Search Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F7" title="Figure 7.7 ‣ 7.5.7 Hyperparameter Optimisation ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>Proposed architecture for adaptNMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F8" title="Figure 7.8 ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.8 </span>adaptNMT and serverNMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F7.sf1" title="7.7(a) ‣ Figure 7.8 ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">(a) </span><span class="ltx_text" style="font-size:80%;">Overview of adaptNMT. Key areas include initialisation, pre-processing, environment setup, visualisation, auto and custom NMT, training of subword model, training of main model, evaluation and deployment (cf. Section <span class="ltx_text ltx_ref_tag">7.6.1</span>). </span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F7.sf2" title="7.7(b) ‣ Figure 7.8 ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">(b) </span><span class="ltx_text" style="font-size:80%;">Overview of serverNMT. Highlighted cells include initialisation, environment setup, Anvil server, API functions, translation, model building, adaptNMT and running the server (cf. Section <span class="ltx_text ltx_ref_tag">7.6.2</span>).</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F9" title="Figure 7.9 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.9 </span>adapt covid_extended system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F10" title="Figure 7.10 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.10 </span><span class="ltx_text ltx_font_italic">gaHealth</span> en2ga* system: training EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F11" title="Figure 7.11 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.11 </span><span class="ltx_text ltx_font_italic">gaHealth</span> ga2en system: training GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F1" title="Figure 8.1 ‣ 8.1 Context ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Fine-tuned MLLM approach of adaptMLLM for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>GA translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F2" title="Figure 8.2 ‣ 8.1 Context ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>
Fine-tuned MLLM approach of adaptMLLM for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">↔</mo><annotation-xml encoding="MathML-Content"><ci>↔</ci></annotation-xml><annotation encoding="application/x-tex">\leftrightarrow</annotation><annotation encoding="application/x-llamapun">↔</annotation></semantics></math>MR translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F3" title="Figure 8.3 ‣ 8.3 Graphical abstract ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Graphical abstract summarising the adaptMLLM system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F4" title="Figure 8.4 ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Proposed architecture for adaptMLLM: a system for fine-tuning MLLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F5" title="Figure 8.5 ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5 </span>Overview of adaptMLLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#A1.F1" title="Figure A.1 ‣ Appendix A No Language Left Behind"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Overview of the NLLB approach <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">32</span>]</cite></span></a></li>
</ol></nav>
<nav class="ltx_TOC ltx_list_lot ltx_toc_lot"><h6 class="ltx_title ltx_title_contents">List of Tables</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T1" title="Table 2.1 ‣ Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Hyperparameter optimisation for Transformer models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T2" title="Table 2.2 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>RNN performance on DGT dataset of 52k lines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T3" title="Table 2.3 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>RNN performance on PA dataset of 88k lines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T4" title="Table 2.4 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Transformer performance on 52k DGT dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T5" title="Table 2.5 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Transformer performance on 88k PA dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T6" title="Table 2.6 ‣ 2.8 Discussion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Samples of human reference translations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T7" title="Table 2.7 ‣ 2.8 Discussion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.7 </span>Transformer model compared with Google Translate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T1" title="Table 3.1 ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets used in proposed approach for MT in Covid domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T2" title="Table 3.2 ‣ 3.5.1 Architecture Tuning ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Hyperparameter optimisation for Transformer models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T3" title="Table 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Comparison of Transformer performance with 2 attention heads</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T4" title="Table 3.4 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Comparison of Transformer performance with 8 attention heads</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T1" title="Table 4.1 ‣ 4.5.1 Sources for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Extracts from the <span class="ltx_text ltx_font_italic">gaHealth</span> corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T2" title="Table 4.2 ‣ 4.5.1 Sources for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Sources used in corpus development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T3" title="Table 4.3 ‣ 4.5.4 Transformer Architecture ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Hyperparameter optimisation for Transformer models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T4" title="Table 4.4 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA training, validation and test datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T5" title="Table 4.5 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN training, validation and test datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T6" title="Table 4.6 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA <span class="ltx_text ltx_font_italic">gaHealth</span> system compared with LoResMT2021 systems.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T7" title="Table 4.7 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN <span class="ltx_text ltx_font_italic">gaHealth</span> system compared with LoResMT2021 systems.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T1" title="Table 5.1 ‣ 5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.1</span> </span><span class="ltx_text" style="font-size:90%;">Transformer HPO using a random search approach</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T2" title="Table 5.2 ‣ Scalar Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>SQM levels explained</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T3" title="Table 5.3 ‣ Multidimensional Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.3</span> </span><span class="ltx_text" style="font-size:90%;">Description of error categories within the core MQM framework</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T4" title="Table 5.4 ‣ Inter-Annotator Agreement ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Inter-annotator agreement using Cohen values</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T5" title="Table 5.5 ‣ Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.5</span> </span><span class="ltx_text" style="font-size:90%;">RNN performance on DGT dataset of 52k Lines</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T6" title="Table 5.6 ‣ Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.6</span> </span><span class="ltx_text" style="font-size:90%;">Transformer performance on 52k DGT dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T7" title="Table 5.7 ‣ 5.6.3 Human Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Total errors found by each annotator using the MQM metric.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T8" title="Table 5.8 ‣ 5.6.3 Human Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8 </span>Transformer and RNN approach compared</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T9" title="Table 5.9 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.9</span> </span><span class="ltx_text" style="font-size:90%;">Random samples of human reference translations from the test dataset.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T10" title="Table 5.10 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.10 </span>Transformer model compared with Google Translate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T11" title="Table 5.11 ‣ Interpreting Meaning ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.11</span> </span><span class="ltx_text" style="font-size:90%;">Linguistic analysis of system outputs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T12" title="Table 5.12 ‣ Performance of RNN Approach Relative to Transformer Approach ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">5.12</span> </span><span class="ltx_text" style="font-size:90%;">Transformer approach compared to the RNN approach</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T1" title="Table 7.1 ‣ 7.5.6 NMT Tools ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Key features differentiating adaptNMT from Joey NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T2" title="Table 7.2 ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Hyperparameter optimisation for Transformer models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T3" title="Table 7.3 ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA training, validation and test dataset distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T4" title="Table 7.4 ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN training, validation and test dataset distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T5" title="Table 7.5 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA <span class="ltx_text ltx_font_italic">gaHealth</span> system compared with LoResMT 2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T6" title="Table 7.6 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN <span class="ltx_text ltx_font_italic">gaHealth</span> systems compared with LoResMT2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T7" title="Table 7.7 ‣ 7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>Stochastic differences between EN<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T8" title="Table 7.8 ‣ 7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.8 </span>Stochastic differences between GA<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Hyperparameter Optimisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T2" title="Table 8.2 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA: adaptMLLM systems compared with LoResMT2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T3" title="Table 8.3 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>GA<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN: adaptMLLM systems compared with LoResMT2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T4" title="Table 8.4 ‣ Translation in the EN↔MR directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>MR: adaptMLLM systems compared with LoResMT2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T5" title="Table 8.5 ‣ Translation in the EN↔MR directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5 </span>MR<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>EN: adaptMLLM systems compared with LoResMT2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T6" title="Table 8.6 ‣ Scalar Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.6</span> </span><span class="ltx_text" style="font-size:90%;">SQM levels explained </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="font-size:90%;">[</span><span class="ltx_ref">40</span><span class="ltx_text" style="font-size:90%;">]</span></cite><span class="ltx_text" style="font-size:90%;">.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T7" title="Table 8.7 ‣ Scalar Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7 </span>Annotator SQM scores for adaptMLLM systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T8" title="Table 8.8 ‣ Multidimensional Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.8</span> </span><span class="ltx_text" style="font-size:90%;">Description of error categories within the core MQM framework</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T9" title="Table 8.9 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.9</span> </span><span class="ltx_text" style="font-size:90%;">System errors found by each annotator using the MQM metric</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T10" title="Table 8.10 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.10</span> </span><span class="ltx_text" style="font-size:90%;">Fine-grained analysis with concatenated errors across both annotators</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T11" title="Table 8.11 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.11 </span>Inter-annotator agreement using Cohen values</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T12" title="Table 8.12 ‣ 8.8.4 Environmental Impact ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.12 </span>Energy consumption during MLLM fine-tuning experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T13" title="Table 8.13 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.13 </span>EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA test dataset of LoResMT2021: samples of human reference translations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T14" title="Table 8.14 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.14 </span>EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>GA fine-tuned MLLM model compared with Google Translate.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T15" title="Table 8.15 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.15 </span>Samples of human reference translations from EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>MR LoResMT2021</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T16" title="Table 8.16 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.16 </span>EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">{\rightarrow}</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>MR fine-tuned MLLM model compared with Google Translate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T17" title="Table 8.17 ‣ 8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.17</span> </span><span class="ltx_text" style="font-size:90%;">Linguistic analysis of EN</span><math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo mathsize="90%" stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">GA system output</span></span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Chx5.p2">
<p class="ltx_p" id="Chx5.p2.1"><span class="ltx_text ltx_font_bold" id="Chx5.p2.1.1" style="font-size:144%;">Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures<span class="ltx_text ltx_font_medium" id="Chx5.p2.1.1.1"></span></span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Chx5.p2.2"><span class="ltx_text ltx_font_bold" id="Chx5.p2.2.1" style="font-size:120%;">Séamus Lankford<span class="ltx_text ltx_font_medium" id="Chx5.p2.2.1.1"></span></span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Chx5.p2.3"><span class="ltx_text ltx_font_bold" id="Chx5.p2.3.1" style="font-size:120%;">Abstract<span class="ltx_text ltx_font_medium" id="Chx5.p2.3.1.1"></span></span></p>
</div>
<div class="ltx_para" id="Chx5.p3">
<p class="ltx_p" id="Chx5.p3.2">In the current machine translation (MT) landscape, the Transformer architecture stands as the gold standard, especially for high-resource language pairs. This research delves into its efficacy for low-resource language pairs including both the English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p3.1.m1.1"><semantics id="Chx5.p3.1.m1.1a"><mo id="Chx5.p3.1.m1.1.1" stretchy="false" xref="Chx5.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p3.1.m1.1b"><ci id="Chx5.p3.1.m1.1.1.cmml" xref="Chx5.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p3.1.m1.1d">↔</annotation></semantics></math>Irish and English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p3.2.m2.1"><semantics id="Chx5.p3.2.m2.1a"><mo id="Chx5.p3.2.m2.1.1" stretchy="false" xref="Chx5.p3.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p3.2.m2.1b"><ci id="Chx5.p3.2.m2.1.1.cmml" xref="Chx5.p3.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p3.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p3.2.m2.1d">↔</annotation></semantics></math>Marathi language pairs. Notably, the study identifies the optimal hyperparameters and subword model type to significantly improve the translation quality of Transformer models for low-resource language pairs.</p>
</div>
<div class="ltx_para" id="Chx5.p4">
<p class="ltx_p" id="Chx5.p4.1">The scarcity of parallel datasets for low-resource languages can hinder MT development. To address this, we developed gaHealth, the first bilingual corpus of health data for the Irish language. Focusing on the health domain, models developed using this in-domain dataset exhibited very significant improvements in BLEU score when compared with models from the LoResMT2021 Shared Task. A subsequent human evaluation using the multidimensional quality metrics error taxonomy showcased the superior performance of the Transformer system in reducing both accuracy and fluency errors compared to an RNN-based counterpart.</p>
</div>
<div class="ltx_para" id="Chx5.p5">
<p class="ltx_p" id="Chx5.p5.2">Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source applications streamlined for the development, fine-tuning, and deployment of neural machine translation models. These tools considerably simplify the setup and evaluation process, making MT more accessible to both developers and translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes eco-friendly natural language processing research by highlighting the environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM demonstrated advancements in translation performance for two low-resource language pairs: English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p5.1.m1.1"><semantics id="Chx5.p5.1.m1.1a"><mo id="Chx5.p5.1.m1.1.1" stretchy="false" xref="Chx5.p5.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p5.1.m1.1b"><ci id="Chx5.p5.1.m1.1.1.cmml" xref="Chx5.p5.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p5.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p5.1.m1.1d">↔</annotation></semantics></math>Irish and English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p5.2.m2.1"><semantics id="Chx5.p5.2.m2.1a"><mo id="Chx5.p5.2.m2.1.1" stretchy="false" xref="Chx5.p5.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p5.2.m2.1b"><ci id="Chx5.p5.2.m2.1.1.cmml" xref="Chx5.p5.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p5.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p5.2.m2.1d">↔</annotation></semantics></math>Marathi, compared to baselines from the LoResMT2021 Shared Task.

</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Chx5.p6">
<p class="ltx_p" id="Chx5.p6.1"><span class="ltx_text ltx_font_bold" id="Chx5.p6.1.1" style="font-size:144%;">Feabhas a chur ar Mheaisínaistriúchán Néarach Teangacha Ísealacmhainne: Forbairt Chorpais, Meastóireacht Dhaonna agus Ailtireacht IS Inmhínithe
<span class="ltx_text ltx_font_medium" id="Chx5.p6.1.1.1"></span></span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Chx5.p6.2"><span class="ltx_text ltx_font_bold" id="Chx5.p6.2.1" style="font-size:120%;">Séamus Lankford<span class="ltx_text ltx_font_medium" id="Chx5.p6.2.1.1"></span></span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Chx5.p6.3"><span class="ltx_text ltx_font_bold" id="Chx5.p6.3.1" style="font-size:120%;">Achoimre<span class="ltx_text ltx_font_medium" id="Chx5.p6.3.1.1"></span></span></p>
</div>
<div class="ltx_para" id="Chx5.p7">
<p class="ltx_p" id="Chx5.p7.2">Sa tírdhreach meaisínaistriúcháin (MA) reatha, seasann ailtireacht an Trasfhoirmeora mar an caighdeán is airde, go háirithe do phéirí teanga ardacmhainne. Déanann an taighde seo tochailt isteach ar a éifeachtúlacht le haghaidh péirí teanga nach bhfuil mórán acmhainní acu lena n-áirítear péirí teanga Béarla<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p7.1.m1.1"><semantics id="Chx5.p7.1.m1.1a"><mo id="Chx5.p7.1.m1.1.1" stretchy="false" xref="Chx5.p7.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p7.1.m1.1b"><ci id="Chx5.p7.1.m1.1.1.cmml" xref="Chx5.p7.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p7.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p7.1.m1.1d">↔</annotation></semantics></math>Gaeilge agus Béarla<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p7.2.m2.1"><semantics id="Chx5.p7.2.m2.1a"><mo id="Chx5.p7.2.m2.1.1" stretchy="false" xref="Chx5.p7.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p7.2.m2.1b"><ci id="Chx5.p7.2.m2.1.1.cmml" xref="Chx5.p7.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p7.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p7.2.m2.1d">↔</annotation></semantics></math>Maratais. Go suntasach, aithníonn an staidéar na hipear-pharaiméadair agus an cineál múnla fho-fhocal chun feabhas suntasach a chur ar chaighdeán aistriúcháin mhúnlaí an Trasfhoirmeora le haghaidh péirí teanga ísealacmhainne.</p>
</div>
<div class="ltx_para" id="Chx5.p8">
<p class="ltx_p" id="Chx5.p8.1">Féadtar leis an easnamh de thacair shonraí chomhthreomhara constaic a chur roimh fhorbairt MA. Chun aghaidh a thabhairt air seo, d’fhorbraíomar gaHealth, an chéad chorpas sláinte dátheangach do theanga na Gaeilge.
Ag díriú ar an bhfearann sláinte, léirigh múnlaí a forbraíodh ag baint úsáid as an tacar sonraí seo feabhsuithe suntasacha i scór BLEU nuair a chuirtear iad i gcomparáid le múnlaí ón Tasc Roinnte LoResMT2021.
</p>
</div>
<div class="ltx_para" id="Chx5.p9">
<p class="ltx_p" id="Chx5.p9.1">Léirigh meastóireacht dhaonna ina dhiaidh sin a bhain úsáid as meádracha ilghnéitheacha cáilíochta earráide tascsanmaíochta ardfheidhmíocht chóras an Trasfhoirmeora maidir le hearráidí cruinnis agus líofachta araon a laghdú i gcomparáid le macasamhail RNN-bunaithe.</p>
</div>
<div class="ltx_para" id="Chx5.p10">
<p class="ltx_p" id="Chx5.p10.1">Chomh maith leis sin, tugann an tráchtas seo adaptNMT agus adaptMLLM isteach, dhá fheidhm fhoinse-oscailte sruthaithe d’fhorbairt, mionchoigeartú agus úsáid mhúnlaí meaisínaistriúcháin néaraigh. Simplíonn na huirlisí an leagan amach agus an próiseas meastóireachta go mór, rud a dhéanann MA níos inrochtana le haghaidh forbróirí agus aistritheoirí araon.</p>
</div>
<div class="ltx_para" id="Chx5.p11">
<p class="ltx_p" id="Chx5.p11.2">Rud atá le tabhairt faoi ndeara ná go gcuireann adaptNMT, atá bunaithe i ngnáthóg an OpenNMT, taighde próiseála teanga nádúrtha éiceabháiche chun cinn trí achar timpeallachta an mhúnla fhorbartha a léiriú. Léirigh mionchoigeartú MMLManna ag adaptMLLM forbairtí i leith feidhmíochta aistriúcháin le haghaidh dhá phéire teanga ísealacmhainne: Béarla<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p11.1.m1.1"><semantics id="Chx5.p11.1.m1.1a"><mo id="Chx5.p11.1.m1.1.1" stretchy="false" xref="Chx5.p11.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p11.1.m1.1b"><ci id="Chx5.p11.1.m1.1.1.cmml" xref="Chx5.p11.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p11.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p11.1.m1.1d">↔</annotation></semantics></math>Gaeilge agus Béarla<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Chx5.p11.2.m2.1"><semantics id="Chx5.p11.2.m2.1a"><mo id="Chx5.p11.2.m2.1.1" stretchy="false" xref="Chx5.p11.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Chx5.p11.2.m2.1b"><ci id="Chx5.p11.2.m2.1.1.cmml" xref="Chx5.p11.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Chx5.p11.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Chx5.p11.2.m2.1d">↔</annotation></semantics></math>Maratais, i gcomparáid le bonnlínte ón Tasc Roinnte LoRESMT2021.</p>
</div>
</section>
<section class="ltx_chapter" id="Ch1">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 1 </span>Introduction</h2>
<div class="ltx_para" id="Ch1.p1">
<p class="ltx_p" id="Ch1.p1.1">The digital age has ushered in a plethora of innovations in machine learning (ML) and natural language processing (NLP), delivering solutions to difficult problems. Machine translation (MT) is among the prominent areas that have experienced a significant transformation over the years. While MT has made commendable strides in various language pairs, challenges remain in deploying these advanced models for low-resource languages. One such low-resource language pair that presents unique challenges is the English-to-Irish pair (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.p1.1.m1.1"><semantics id="Ch1.p1.1.m1.1a"><mo id="Ch1.p1.1.m1.1.1" stretchy="false" xref="Ch1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.p1.1.m1.1b"><ci id="Ch1.p1.1.m1.1.1.cmml" xref="Ch1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.p1.1.m1.1d">↔</annotation></semantics></math>GA).</p>
</div>
<div class="ltx_para" id="Ch1.p2">
<p class="ltx_p" id="Ch1.p2.1">The EU flags the importance of digital services being available in all languages to ensure a level playing field and full access to services for citizens, companies and governments (EU Digital Single Market 2018). This has been reinforced by a 2018 EU Parliament decision endorsing language equality in the digital age<span class="ltx_note ltx_role_footnote" id="Ch1.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.europarl.europa.eu/doceo/document/A-8-2018-0228_EN.html" title="">https://www.europarl.europa.eu/doceo/document/A-8-2018-0228_EN.html</a></span></span></span> and is a particularly acute issue for Ireland in fostering and protecting the Irish language as the European Commission derogation of Irish as an official EU language expired at the end of 2021.</p>
</div>
<div class="ltx_para" id="Ch1.p3">
<p class="ltx_p" id="Ch1.p3.1">A major part of this research involves developing applications and methods to address the challenges of low-resource language technology while also addressing the problem of data scarcity affecting deep learning (DL) for digital engagement. Harnessing the full potential of neural machine translation (NMT) for low-resource languages like EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.p3.1.m1.1"><semantics id="Ch1.p3.1.m1.1a"><mo id="Ch1.p3.1.m1.1.1" stretchy="false" xref="Ch1.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.p3.1.m1.1b"><ci id="Ch1.p3.1.m1.1.1.cmml" xref="Ch1.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.p3.1.m1.1d">↔</annotation></semantics></math>GA is multifaceted. This research helps deliver parity in support for Irish and other less-resourced languages.</p>
</div>
<section class="ltx_section" id="Ch1.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.1 </span>Neural Networks: an overview</h3>
<div class="ltx_para" id="Ch1.S1.p1">
<p class="ltx_p" id="Ch1.S1.p1.1">To grasp the concept of a neural network (NN), we can refer to the simplified diagram in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.F1" title="Figure 1.1 ‣ 1.1 Neural Networks: an overview ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_tag">1.1</span></a>. While real-world NNs used for translating one language to another are considerably more complex, this basic representation helps offer a general insight into NMT. The input layer depicts the source language words, whereas the output layer showcases potential translations suggested by the NN. These interconnections have specific weights, which might be initially set at random.</p>
</div>
<div class="ltx_para" id="Ch1.S1.p2">
<p class="ltx_p" id="Ch1.S1.p2.1">When the network proposes a translation, its accuracy is gauged by comparing it to a known human expert translation. If there is a difference (or error) between the predicted translation and the reference, as perhaps assessed by BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, this error is fed back into the NN using back-propagation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx97" title="">97</a>]</cite>. This process adjusts the connection weights and retrains the network in a subsequent cycle, involving a complete forward and backward pass through the NN.</p>
</div>
<div class="ltx_para" id="Ch1.S1.p3">
<p class="ltx_p" id="Ch1.S1.p3.1">Ideally, the network’s translation suggestions should come closer to the human reference with each iteration, indicating improved weights. If errors persist, the NN adjusts its weights, re-evaluates, and the cycle continues until no improvements in the BLEU score are observed. After repeatedly refining the network using a comprehensive set of examples from the training dataset, the final model is set. This optimised NN can then be introduced to new sentences it has not encountered before, marking the commencement of the actual translation process.</p>
</div>
<figure class="ltx_figure" id="Ch1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1781" id="Ch1.F1.g1" src="extracted/5444776/Images/neural_net_overview.png" width="1695"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch1.F1.2.1.1" style="font-size:90%;">Figure 1.1</span>: </span><span class="ltx_text" id="Ch1.F1.3.2" style="font-size:90%;">Simplified representation of a feed-forward NN with 3 layers. A matrix of weights, i-w1 to i-w6 connects the input and hidden layers whereas a separate matrix h-w1 to h-w6 connects the hidden and output layers.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch1.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.2 </span>Hyperparameters and Subword Models</h3>
<div class="ltx_para" id="Ch1.S2.p1">
<p class="ltx_p" id="Ch1.S2.p1.1">In the context of ML, <span class="ltx_text ltx_font_italic" id="Ch1.S2.p1.1.1">hyperparameters</span> are distinct from the typical model <span class="ltx_text ltx_font_italic" id="Ch1.S2.p1.1.2">parameters</span>. While model parameters are adjusted during training (e.g., weights and biases in a NN), hyperparameters are set before training begins. Common examples of hyperparameters include the learning rate, which governs how much model parameters are adjusted during training, batch size, which determines the number of data points considered in each iteration, and epochs, specifying how many times the algorithm will run through the entire training dataset. Regularisation parameters, which can help in adding penalties on the magnitude of parameters to prevent overfitting, are another set of important hyperparameters.
</p>
</div>
<div class="ltx_para" id="Ch1.S2.p2">
<p class="ltx_p" id="Ch1.S2.p2.1">The significance of hyperparameters cannot be overstated. Their proper selection and tuning can dramatically influence the performance of the model. A suitable learning rate, for instance, can be the difference between a model that converges efficiently and one that does not converge at all. In scenarios with limited data, such as with low-resource languages, hyperparameter choices become even more critical. It is paramount to extract as much value as possible from the scarce data available, and well-tuned hyperparameters facilitate that.</p>
</div>
<div class="ltx_para" id="Ch1.S2.p3">
<p class="ltx_p" id="Ch1.S2.p3.1">Languages are diverse and rich, with some exhibiting significant morphological variations. This variability becomes a challenge when training language models, especially for languages that are under-represented in terms of digitally available data. Subword models offer a solution by breaking words into smaller, more manageable units. These units can range from individual characters to larger subword chunks. For example, the word “unhappiness” could be segmented into “un-”, “-happi-”, and “-ness” using a subword tokenization approach.</p>
</div>
<div class="ltx_para" id="Ch1.S2.p4">
<p class="ltx_p" id="Ch1.S2.p4.1">For low-resource languages, the use of subword models brings multiple advantages. Firstly, it helps in managing the vocabulary size. Instead of handling every possible word form, the model deals with a more limited set of subword units which makes training of models faster. This approach also provides a mechanism for understanding and generating words the model has not explicitly seen during training. Instead of relegating unfamiliar words to a generic “unknown” category, the model can represent and process them using its known subword units. Furthermore, in the realm of transfer learning, where knowledge is transferred from data-rich languages to low-resource ones, subword models perform well. The shared subword units across languages make knowledge transfer more seamless.</p>
</div>
<div class="ltx_para" id="Ch1.S2.p5">
<p class="ltx_p" id="Ch1.S2.p5.1">Both hyperparameters and subword models play pivotal roles in language modelling, particularly for languages with limited data. While hyperparameters ensure the efficient and effective training of models, subword models offer a practical approach to tokenization that accounts for the morphological richness and data scarcity inherent in many of the world’s languages.</p>
</div>
</section>
<section class="ltx_section" id="Ch1.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.3 </span>Neural Machine Translation</h3>
<div class="ltx_para" id="Ch1.S3.p1">
<p class="ltx_p" id="Ch1.S3.p1.1">NMT has achieved impressive performance for many language pairs, especially those with very large amounts of parallel data available (e.g., English to French). However, for low-resource languages, which are languages with limited available parallel data, there are challenges:</p>
</div>
<div class="ltx_para" id="Ch1.S3.p2">
<ul class="ltx_itemize" id="Ch1.S3.I1">
<li class="ltx_item" id="Ch1.S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i1.p1">
<p class="ltx_p" id="Ch1.S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i1.p1.1.1">Data Scarcity</span>: The primary challenge is the lack of substantial parallel corpora. Neural models, especially the DL models used for NMT, require large amounts of data to train effectively and generalise well to unseen inputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx121" title="">121</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx109" title="">109</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i2.p1">
<p class="ltx_p" id="Ch1.S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i2.p1.1.1">Noisy Training Data</span>: To overcome the data scarcity problem, sometimes researchers use web-crawled data or other less reliable sources to augment the training set. Such data can introduce noise due to incorrect alignments or low-quality translations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx87" title="">87</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i3.p1">
<p class="ltx_p" id="Ch1.S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i3.p1.1.1">Overfitting</span>: Due to the limited data, NMT models can overfit easily, which means they might memorise the training data rather than generalising from it. As a result, their performance on unseen data might be suboptimal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx18" title="">18</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i4.p1">
<p class="ltx_p" id="Ch1.S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i4.p1.1.1">Morphological Complexity</span>: Many low-resource languages are morphological rich, meaning they have a large number of word forms due to inflection, derivation, or compounding. This complexity can pose challenges for NMT models which might not see enough examples of each morphological variant in the training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx102" title="">102</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i5.p1">
<p class="ltx_p" id="Ch1.S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i5.p1.1.1">Limited Pre-trained Models or Embeddings</span>: For popular languages, there are often pre-trained models or word embeddings available that can be fine-tuned for specific tasks. For many low-resource languages, these might not exist, forcing researchers to start from scratch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx96" title="">96</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i6.p1">
<p class="ltx_p" id="Ch1.S3.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i6.p1.1.1">Domain Mismatch</span>: The available data for low-resource languages might be restricted to specific domains (e.g., religious texts or legal documents). Training on such a restricted domain can limit the extent to which a model can generalise to other domains.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i7.p1">
<p class="ltx_p" id="Ch1.S3.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i7.p1.1.1">Evaluation Challenges</span>: Even if a model is trained for a low-resource language, evaluating its performance can be problematic due to the lack of standard benchmarks, reference translations, or even native speakers who can assess the translations’ quality.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i8.p1">
<p class="ltx_p" id="Ch1.S3.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i8.p1.1.1">Cultural and Contextual Nuances</span>: All languages convey cultural and contextual nuances. With limited data, the model might miss these subtleties, leading to translations that might be technically correct but culturally or contextually inappropriate or odd.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch1.S3.p3">
<p class="ltx_p" id="Ch1.S3.p3.1">To address these challenges, researchers are exploring various methods, such as using transfer learning, building multilingual models, applying data augmentation techniques and developing new corpora. Translating low-resource languages with high accuracy remains an active area of research. A core objective of our NMT work is to address some of the challenges outlined above by posing and answering the research questions in the next section. An in-depth discussion of NMT architecture and the mathematical first principles governing its operation are covered in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5" title="7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a>.
</p>
</div>
</section>
<section class="ltx_section" id="Ch1.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.4 </span>Research Questions</h3>
<div class="ltx_para" id="Ch1.S4.p1">
<p class="ltx_p" id="Ch1.S4.p1.1">Having introduced the concept of NMT in the context of low-resource languages, we will now discuss the specific research questions this PhD thesis seeks to address.</p>
</div>
<div class="ltx_para" id="Ch1.S4.p2">
<ul class="ltx_itemize" id="Ch1.S4.I1">
<li class="ltx_item" id="Ch1.S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I1.i1.p1">
<p class="ltx_p" id="Ch1.S4.I1.i1.p1.1">RQ<math alttext="{}_{1}" class="ltx_Math" display="inline" id="Ch1.S4.I1.i1.p1.1.m1.1"><semantics id="Ch1.S4.I1.i1.p1.1.m1.1a"><msub id="Ch1.S4.I1.i1.p1.1.m1.1.1" xref="Ch1.S4.I1.i1.p1.1.m1.1.1.cmml"><mi id="Ch1.S4.I1.i1.p1.1.m1.1.1a" xref="Ch1.S4.I1.i1.p1.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.I1.i1.p1.1.m1.1.1.1" xref="Ch1.S4.I1.i1.p1.1.m1.1.1.1.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.I1.i1.p1.1.m1.1b"><apply id="Ch1.S4.I1.i1.p1.1.m1.1.1.cmml" xref="Ch1.S4.I1.i1.p1.1.m1.1.1"><cn id="Ch1.S4.I1.i1.p1.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.I1.i1.p1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.I1.i1.p1.1.m1.1c">{}_{1}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.I1.i1.p1.1.m1.1d">start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT</annotation></semantics></math> - How can hyperparameters and subword models be optimised for Transformer-based MT in low-resource language settings?</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I1.i2.p1">
<p class="ltx_p" id="Ch1.S4.I1.i2.p1.1">RQ<math alttext="{}_{2}" class="ltx_Math" display="inline" id="Ch1.S4.I1.i2.p1.1.m1.1"><semantics id="Ch1.S4.I1.i2.p1.1.m1.1a"><msub id="Ch1.S4.I1.i2.p1.1.m1.1.1" xref="Ch1.S4.I1.i2.p1.1.m1.1.1.cmml"><mi id="Ch1.S4.I1.i2.p1.1.m1.1.1a" xref="Ch1.S4.I1.i2.p1.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.I1.i2.p1.1.m1.1.1.1" xref="Ch1.S4.I1.i2.p1.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.I1.i2.p1.1.m1.1b"><apply id="Ch1.S4.I1.i2.p1.1.m1.1.1.cmml" xref="Ch1.S4.I1.i2.p1.1.m1.1.1"><cn id="Ch1.S4.I1.i2.p1.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.I1.i2.p1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.I1.i2.p1.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.I1.i2.p1.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> - What is the benefit and impact of using in-domain datasets in improving the performance of MT models for low-resource language pairs, and how can these datasets be effectively developed and utilised?</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I1.i3.p1">
<p class="ltx_p" id="Ch1.S4.I1.i3.p1.1">RQ<math alttext="{}_{3}" class="ltx_Math" display="inline" id="Ch1.S4.I1.i3.p1.1.m1.1"><semantics id="Ch1.S4.I1.i3.p1.1.m1.1a"><msub id="Ch1.S4.I1.i3.p1.1.m1.1.1" xref="Ch1.S4.I1.i3.p1.1.m1.1.1.cmml"><mi id="Ch1.S4.I1.i3.p1.1.m1.1.1a" xref="Ch1.S4.I1.i3.p1.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.I1.i3.p1.1.m1.1.1.1" xref="Ch1.S4.I1.i3.p1.1.m1.1.1.1.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.I1.i3.p1.1.m1.1b"><apply id="Ch1.S4.I1.i3.p1.1.m1.1.1.cmml" xref="Ch1.S4.I1.i3.p1.1.m1.1.1"><cn id="Ch1.S4.I1.i3.p1.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.I1.i3.p1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.I1.i3.p1.1.m1.1c">{}_{3}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.I1.i3.p1.1.m1.1d">start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT</annotation></semantics></math> - How do NMT systems, specifically Transformer and recurrent neural network (RNN) based models, differ in terms of accuracy and fluency errors when evaluated using a human evaluation technique such as the Multidimensional Quality Metrics (MQM) error taxonomy?</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I1.i4.p1">
<p class="ltx_p" id="Ch1.S4.I1.i4.p1.1">RQ<math alttext="{}_{4}" class="ltx_Math" display="inline" id="Ch1.S4.I1.i4.p1.1.m1.1"><semantics id="Ch1.S4.I1.i4.p1.1.m1.1a"><msub id="Ch1.S4.I1.i4.p1.1.m1.1.1" xref="Ch1.S4.I1.i4.p1.1.m1.1.1.cmml"><mi id="Ch1.S4.I1.i4.p1.1.m1.1.1a" xref="Ch1.S4.I1.i4.p1.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.I1.i4.p1.1.m1.1.1.1" xref="Ch1.S4.I1.i4.p1.1.m1.1.1.1.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.I1.i4.p1.1.m1.1b"><apply id="Ch1.S4.I1.i4.p1.1.m1.1.1.cmml" xref="Ch1.S4.I1.i4.p1.1.m1.1.1"><cn id="Ch1.S4.I1.i4.p1.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.I1.i4.p1.1.m1.1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.I1.i4.p1.1.m1.1c">{}_{4}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.I1.i4.p1.1.m1.1d">start_FLOATSUBSCRIPT 4 end_FLOATSUBSCRIPT</annotation></semantics></math> - How can the process of NMT development, evaluation, and deployment be streamlined for both developers and translators, while also considering environmental sustainability?</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsubsection" id="Ch1.S4.SS0.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">RQ<math alttext="{}_{1}" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx1.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx1.1.m1.1b"><msub id="Ch1.S4.SS0.SSSx1.1.m1.1.1" xref="Ch1.S4.SS0.SSSx1.1.m1.1.1.cmml"><mi id="Ch1.S4.SS0.SSSx1.1.m1.1.1b" xref="Ch1.S4.SS0.SSSx1.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.SS0.SSSx1.1.m1.1.1.1" xref="Ch1.S4.SS0.SSSx1.1.m1.1.1.1.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx1.1.m1.1c"><apply id="Ch1.S4.SS0.SSSx1.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx1.1.m1.1.1"><cn id="Ch1.S4.SS0.SSSx1.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.SS0.SSSx1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx1.1.m1.1d">{}_{1}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx1.1.m1.1e">start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT</annotation></semantics></math> How can hyperparameters and subword models be optimised for Transformer-based MT in low-resource language settings?</h5>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx1.p1">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx1.p1.2">The effectiveness of fine-tuning the Transformer model for the low-resource EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx1.p1.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx1.p1.1.m1.1a"><mo id="Ch1.S4.SS0.SSSx1.p1.1.m1.1.1" stretchy="false" xref="Ch1.S4.SS0.SSSx1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx1.p1.1.m1.1b"><ci id="Ch1.S4.SS0.SSSx1.p1.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx1.p1.1.m1.1d">↔</annotation></semantics></math>GA language pair was investigated in the paper submitted to the MT Summit conference in 2021. Hyperparameter optimisation (HPO) and the impact of different subword models on translation performance were evaluated. The observations made as part of this study were used to choose the hyperparameters for developing an EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx1.p1.2.m2.1"><semantics id="Ch1.S4.SS0.SSSx1.p1.2.m2.1a"><mo id="Ch1.S4.SS0.SSSx1.p1.2.m2.1.1" stretchy="false" xref="Ch1.S4.SS0.SSSx1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx1.p1.2.m2.1b"><ci id="Ch1.S4.SS0.SSSx1.p1.2.m2.1.1.cmml" xref="Ch1.S4.SS0.SSSx1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx1.p1.2.m2.1d">→</annotation></semantics></math>GA model entered into the LoResMT2021 Shared Task<span class="ltx_note ltx_role_footnote" id="Ch1.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://machinetranslate.org/loresmt2021" title="">https://machinetranslate.org/loresmt2021</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>]</cite>. The focus of the shared task was to develop MT models for translating Covid-related data. The experimental findings from this shared task were the basis of a separate paper. This paper highlighted the role of careful selection of Transformer hyperparameters and demonstrated that choosing a 16k BPE SentencePiece submodel yielded high-performing translation models in a low-resource setting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx1.p2">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx1.p2.1">The research question was also explored as part of the
human evaluation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx1.p2.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx1.p2.1.m1.1a"><mo id="Ch1.S4.SS0.SSSx1.p2.1.m1.1.1" stretchy="false" xref="Ch1.S4.SS0.SSSx1.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx1.p2.1.m1.1b"><ci id="Ch1.S4.SS0.SSSx1.p2.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx1.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx1.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx1.p2.1.m1.1d">↔</annotation></semantics></math>GA Transformer-Based NMT where the impact of modifying hyperparameter settings and regularisation techniques on the performance of NMT models was evaluated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx66" title="">66</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx1.p3">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx1.p3.1">The theme is further addressed in Chapters 7 and 8 which cover the research papers on open-source architectures for fine-tuning NMT models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx68" title="">68</a>]</cite> and MLLM models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx67" title="">67</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch1.S4.SS0.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">RQ<math alttext="{}_{2}" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx2.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx2.1.m1.1b"><msub id="Ch1.S4.SS0.SSSx2.1.m1.1.1" xref="Ch1.S4.SS0.SSSx2.1.m1.1.1.cmml"><mi id="Ch1.S4.SS0.SSSx2.1.m1.1.1b" xref="Ch1.S4.SS0.SSSx2.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.SS0.SSSx2.1.m1.1.1.1" xref="Ch1.S4.SS0.SSSx2.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx2.1.m1.1c"><apply id="Ch1.S4.SS0.SSSx2.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx2.1.m1.1.1"><cn id="Ch1.S4.SS0.SSSx2.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.SS0.SSSx2.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx2.1.m1.1d">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx2.1.m1.1e">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> - What is the impact of using small in-domain datasets in improving the performance of MT models for low-resource language pairs, and how can these datasets be effectively developed and utilised?</h5>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p1">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p1.1">The fine-tuning experiments conducted as part of the winning LoResMT2021 Shared Task entry demonstrate that augmenting in-domain data, even by relatively modest amounts, for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx2.p1.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx2.p1.1.m1.1a"><mo id="Ch1.S4.SS0.SSSx2.p1.1.m1.1.1" stretchy="false" xref="Ch1.S4.SS0.SSSx2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx2.p1.1.m1.1b"><ci id="Ch1.S4.SS0.SSSx2.p1.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx2.p1.1.m1.1d">↔</annotation></semantics></math>GA translations in the Covid domain leads to much better performance when compared with other techniques.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p2">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p2.1">Given the success of the initial approach of using augmented data for fine-tuning Covid models, a separate study was conducted that resulted in the development of an in-domain health dataset. The development of the gaHealth corpus<span class="ltx_note ltx_role_footnote" id="Ch1.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seamusl/gaHealth" title="">https://github.com/seamusl/gaHealth</a></span></span></span> showcases the process and benefits of focusing on specific domains for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p3">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p3.1">To build a bilingual corpus of health data, we selected multiple sources of professionally translated documents from within the Irish government, all of which are publicly available. In particular, the bilingual strategy statements and annual reports of the Irish Department of Health since 2010 were chosen.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p4">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p4.1">Furthermore, a dataset of Covid-related data, developed for a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>]</cite>, was incorporated into a larger health dataset. This amalgamated corpus, gaHealth, consists of 16,201 lines of parallel text.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p5">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p5.1">The main contribution of this work is to present an ongoing translation project that aims to build a parallel corpus, gaHealth, of health data for the Irish language by fully utilising freely available parallel documents.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p6">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p6.1">Due to the issues encountered during the conversion of PDF documents, we developed guidelines for researchers developing low-resource corpora to aid in the conversion process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx63" title="">63</a>]</cite>. In addition to developing the gaHealth corpus, we trained and evaluated translation models for in-domain health data.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx2.p7">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx2.p7.1">There is no such corpus available according to the best of our knowledge, so gaHealth has become a useful resource in the NLP community, especially for those working with the Irish language domain. Addressing this research question has helped to increase the resources available for a low-resource language. The outcome of the work was published at the LREC 2022 conference in Marseille, France.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch1.S4.SS0.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">RQ<math alttext="{}_{3}" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx3.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx3.1.m1.1b"><msub id="Ch1.S4.SS0.SSSx3.1.m1.1.1" xref="Ch1.S4.SS0.SSSx3.1.m1.1.1.cmml"><mi id="Ch1.S4.SS0.SSSx3.1.m1.1.1b" xref="Ch1.S4.SS0.SSSx3.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.SS0.SSSx3.1.m1.1.1.1" xref="Ch1.S4.SS0.SSSx3.1.m1.1.1.1.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx3.1.m1.1c"><apply id="Ch1.S4.SS0.SSSx3.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx3.1.m1.1.1"><cn id="Ch1.S4.SS0.SSSx3.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.SS0.SSSx3.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx3.1.m1.1d">{}_{3}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx3.1.m1.1e">start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT</annotation></semantics></math> - How do MT systems, specifically Transformer and RNN-based models, differ in terms of accuracy and fluency errors when evaluated using a human evaluation technique such as the Multidimensional Quality Metrics (MQM) error taxonomy?</h5>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx3.p1">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx3.p1.1">In addressing this research question, a quantitative fine-grained manual evaluation was conducted which compared the performance of MT systems. Using the MQM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx75" title="">75</a>]</cite> error taxonomy, a human evaluation of the error types generated by an RNN-based system and a Transformer-based system was conducted. Furthermore, Scalar Quality Metrics (SQM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite> were also used so a combined approach helped end users to understand translation quality from a human, rather than a solely automatic perspective.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx3.p2">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx3.p2.1">An integral part of the human evaluation was to highlight the linguistic weaknesses associated with certain MT architectures which were discovered as part of the linguistic observations. Our findings show the best-performing Transformer system significantly reduces both accuracy and fluency errors when compared with an RNN-based model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch1.S4.SS0.SSSx4">
<h5 class="ltx_title ltx_title_subsubsection">RQ<math alttext="{}_{4}" class="ltx_Math" display="inline" id="Ch1.S4.SS0.SSSx4.1.m1.1"><semantics id="Ch1.S4.SS0.SSSx4.1.m1.1b"><msub id="Ch1.S4.SS0.SSSx4.1.m1.1.1" xref="Ch1.S4.SS0.SSSx4.1.m1.1.1.cmml"><mi id="Ch1.S4.SS0.SSSx4.1.m1.1.1b" xref="Ch1.S4.SS0.SSSx4.1.m1.1.1.cmml"></mi><mn id="Ch1.S4.SS0.SSSx4.1.m1.1.1.1" xref="Ch1.S4.SS0.SSSx4.1.m1.1.1.1.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS0.SSSx4.1.m1.1c"><apply id="Ch1.S4.SS0.SSSx4.1.m1.1.1.cmml" xref="Ch1.S4.SS0.SSSx4.1.m1.1.1"><cn id="Ch1.S4.SS0.SSSx4.1.m1.1.1.1.cmml" type="integer" xref="Ch1.S4.SS0.SSSx4.1.m1.1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS0.SSSx4.1.m1.1d">{}_{4}</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS0.SSSx4.1.m1.1e">start_FLOATSUBSCRIPT 4 end_FLOATSUBSCRIPT</annotation></semantics></math> - How can the process of NMT development, evaluation, and deployment be streamlined for both developers and translators, while also considering environmental sustainability?</h5>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx4.p1">
<p class="ltx_p" id="Ch1.S4.SS0.SSSx4.p1.1">To address this research question, two separate open-source tools were developed and journal papers describing them in detail were written <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx67" title="">67</a>]</cite>. The manner in which the features of each tool addressed this research question is outlined below.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS0.SSSx4.p2">
<ol class="ltx_enumerate" id="Ch1.S4.I2">
<li class="ltx_item" id="Ch1.S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Ch1.S4.I2.i1.p1">
<p class="ltx_p" id="Ch1.S4.I2.i1.p1.1">Streamlining for both developers and translators:</p>
</div>
<div class="ltx_para" id="Ch1.S4.I2.i1.p2">
<ul class="ltx_itemize" id="Ch1.S4.I2.i1.I1">
<li class="ltx_item" id="Ch1.S4.I2.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I2.i1.I1.i1.p1">
<p class="ltx_p" id="Ch1.S4.I2.i1.I1.i1.p1.1">The applications are designed for both technical and non-technical users in the field of MT.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I2.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I2.i1.I1.i2.p1">
<p class="ltx_p" id="Ch1.S4.I2.i1.I1.i2.p1.1">The setup of the development environment and the creation of training, validation, and test splits are simplified.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I2.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I2.i1.I1.i3.p1">
<p class="ltx_p" id="Ch1.S4.I2.i1.I1.i3.p1.1">HPO is made user-friendly through an intuitive interface.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ch1.S4.I2.i2.p1">
<p class="ltx_p" id="Ch1.S4.I2.i2.p1.1">Evaluation and Deployment:</p>
<ul class="ltx_itemize" id="Ch1.S4.I2.i2.I1">
<li class="ltx_item" id="Ch1.S4.I2.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I2.i2.I1.i1.p1">
<p class="ltx_p" id="Ch1.S4.I2.i2.I1.i1.p1.1">Models can be evaluated using BLEU, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I2.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I2.i2.I1.i2.p1">
<p class="ltx_p" id="Ch1.S4.I2.i2.I1.i2.p1.1">Deployment is facilitated as a translation service within the application itself.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ch1.S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ch1.S4.I2.i3.p1">
<p class="ltx_p" id="Ch1.S4.I2.i3.p1.1">Environmental Sustainability:</p>
</div>
<div class="ltx_para" id="Ch1.S4.I2.i3.p2">
<ul class="ltx_itemize" id="Ch1.S4.I2.i3.I1">
<li class="ltx_item" id="Ch1.S4.I2.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S4.I2.i3.I1.i1.p1">
<p class="ltx_p" id="Ch1.S4.I2.i3.I1.i1.p1.1">The application contains a green report that flags power consumption and kgCO<sub class="ltx_sub" id="Ch1.S4.I2.i3.I1.i1.p1.1.1">2</sub> emissions generated during model development. The inclusion of this feature creates an awareness of the environmental cost amongst developers and translators. This helps in addressing the environmental sustainability aspect of the research question.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="Ch1.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.5 </span>Thesis Outline</h3>
<div class="ltx_para" id="Ch1.S5.p1">
<p class="ltx_p" id="Ch1.S5.p1.1">The outline of the thesis provides an overview of the papers, the principal motivation for the work and the key research contributions. The adopted approach dedicates a chapter to each published paper which focuses on the outlined research questions. At the start of each chapter, a context section outlines the motivation for the research work and illustrates the flow in how the research questions were addressed. The timeline of the paper publications and their mapping to chapters within the thesis is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch1.F2" title="Figure 1.2 ‣ 1.6 Research Contributions ‣ Chapter 1 Introduction"><span class="ltx_text ltx_ref_tag">1.2</span></a>.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p2">
<p class="ltx_p" id="Ch1.S5.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p2.1.1">Chapter 2: Transformers for Low-Resource Languages:</span> This chapter examines the effectiveness of the Transformer model, particularly in MT scenarios where limited training data is available, such as with the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p2.1.m1.1"><semantics id="Ch1.S5.p2.1.m1.1a"><mo id="Ch1.S5.p2.1.m1.1.1" stretchy="false" xref="Ch1.S5.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p2.1.m1.1b"><ci id="Ch1.S5.p2.1.m1.1.1.cmml" xref="Ch1.S5.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p2.1.m1.1d">↔</annotation></semantics></math>GA language pair. By testing model configuration parameters, the study found significant performance improvements. Notably, the choice of subword model was crucial, with SentencePiece models using both unigram and BPE methods being examined. Overall, this paper demonstrates that Transformers can be efficient for a low-resource language pair.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p3">
<p class="ltx_p" id="Ch1.S5.p3.3"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p3.3.1">Chapter 3: Machine Translation in the Covid domain:</span> an EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p3.1.m1.1"><semantics id="Ch1.S5.p3.1.m1.1a"><mo id="Ch1.S5.p3.1.m1.1.1" stretchy="false" xref="Ch1.S5.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p3.1.m1.1b"><ci id="Ch1.S5.p3.1.m1.1.1.cmml" xref="Ch1.S5.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p3.1.m1.1d">→</annotation></semantics></math>GA case study for LoResMT2021 Shared Task. Focused on the specific challenge of translating EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p3.2.m2.1"><semantics id="Ch1.S5.p3.2.m2.1a"><mo id="Ch1.S5.p3.2.m2.1.1" stretchy="false" xref="Ch1.S5.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p3.2.m2.1b"><ci id="Ch1.S5.p3.2.m2.1.1.cmml" xref="Ch1.S5.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p3.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p3.2.m2.1d">→</annotation></semantics></math>GA Covid-related data, this chapter describes the paper which explored different domain adaptation techniques. The study also introduced a unique EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p3.3.m3.1"><semantics id="Ch1.S5.p3.3.m3.1a"><mo id="Ch1.S5.p3.3.m3.1.1" stretchy="false" xref="Ch1.S5.p3.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p3.3.m3.1b"><ci id="Ch1.S5.p3.3.m3.1.1.cmml" xref="Ch1.S5.p3.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p3.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p3.3.m3.1d">↔</annotation></semantics></math>GA dataset concerning health and education in the Covid context. Remarkably, just by adding 5k lines to an 8k in-domain baseline dataset, the BLEU score improved by a massive 27 points, with the Transformer architecture delivering the best performance.
</p>
</div>
<div class="ltx_para" id="Ch1.S5.p4">
<p class="ltx_p" id="Ch1.S5.p4.2"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p4.1.1">Chapter 4: gaHealth: An EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p4.1.1.m1.1"><semantics id="Ch1.S5.p4.1.1.m1.1a"><mo id="Ch1.S5.p4.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch1.S5.p4.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p4.1.1.m1.1b"><ci id="Ch1.S5.p4.1.1.m1.1.1.cmml" xref="Ch1.S5.p4.1.1.m1.1.1">normal-↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p4.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p4.1.1.m1.1d">↔</annotation></semantics></math>GA Bilingual Corpus of Health Data:</span> Highlighting the challenges and potential of MT for low-resource languages, this chapter discusses the motivation for gaHealth, a bilingual corpus geared towards health data in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p4.2.m1.1"><semantics id="Ch1.S5.p4.2.m1.1a"><mo id="Ch1.S5.p4.2.m1.1.1" stretchy="false" xref="Ch1.S5.p4.2.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p4.2.m1.1b"><ci id="Ch1.S5.p4.2.m1.1.1.cmml" xref="Ch1.S5.p4.2.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p4.2.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p4.2.m1.1d">↔</annotation></semantics></math>GA language pair. The research showcased how using a specific in-domain dataset, in this case for health, could significantly improve translation performance. By leveraging the gaHealth corpus, BLEU scores improved by a substantial 22.2 points when stacked against models from the LoResMT2021 Shared Task. The paper also shared linguistic guidelines for developing such datasets.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p5">
<p class="ltx_p" id="Ch1.S5.p5.3"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p5.1.1">Chapter 5: Human Evaluation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p5.1.1.m1.1"><semantics id="Ch1.S5.p5.1.1.m1.1a"><mo id="Ch1.S5.p5.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch1.S5.p5.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p5.1.1.m1.1b"><ci id="Ch1.S5.p5.1.1.m1.1.1.cmml" xref="Ch1.S5.p5.1.1.m1.1.1">normal-↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p5.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p5.1.1.m1.1d">↔</annotation></semantics></math>GA Transformer-Based NMT:</span> The previous work focused on automatic evaluation whereas the ultimate validation of the research effort can only be determined by a human analysis of the MT output. Therefore the chapter focuses on human evaluation of Transformer-based NMT for the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p5.2.m1.1"><semantics id="Ch1.S5.p5.2.m1.1a"><mo id="Ch1.S5.p5.2.m1.1.1" stretchy="false" xref="Ch1.S5.p5.2.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p5.2.m1.1b"><ci id="Ch1.S5.p5.2.m1.1.1.cmml" xref="Ch1.S5.p5.2.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p5.2.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p5.2.m1.1d">↔</annotation></semantics></math>GA pair. The paper found that Transformer-optimised models significantly outperformed RNN models. A fine-grained manual evaluation using the MQM error taxonomy identified that the Transformer models considerably cut down on both accuracy and fluency errors. The linguistic observations noted by our translators also highlight the typical errors which occur in EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p5.3.m2.1"><semantics id="Ch1.S5.p5.3.m2.1a"><mo id="Ch1.S5.p5.3.m2.1.1" stretchy="false" xref="Ch1.S5.p5.3.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p5.3.m2.1b"><ci id="Ch1.S5.p5.3.m2.1.1.cmml" xref="Ch1.S5.p5.3.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p5.3.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p5.3.m2.1d">↔</annotation></semantics></math>GA MT. Suggestions on how these errors could potentially be mitigated are outlined in the future work section of Chapter 9.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p6">
<p class="ltx_p" id="Ch1.S5.p6.1"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p6.1.1">Chapter 6: Design of an Open-Source Architecture for Neural Machine Translation:</span> The paper presented at the inaugural CrowdMT workshop in Tampere is discussed in this chapter. As a workshop paper, the research objective was to present a high-level view of the adaptNMT architecture. A more in-depth analysis of the tool is provided in the LREV journal paper which is covered in Chapter 7.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p7">
<p class="ltx_p" id="Ch1.S5.p7.1"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p7.1.1">Chapter 7: adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation:</span> This chapter includes the paper which introduced adaptNMT, an integrated open-source application designed for both newcomers and experts in MT. It simplifies various processes, from setting up the environment to model training, while also enabling hyperparameter customisation through a user-friendly interface. The application integrates graphing for training progress visualisation and uses SentencePiece for subword segmentation. One useful feature is the green report that brings to light the environmental cost associated with model development in terms of power consumption and kgCO<sub class="ltx_sub" id="Ch1.S5.p7.1.2">2</sub> emissions, thus encouraging researchers to choose an alternative greener infrastructure.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p8">
<p class="ltx_p" id="Ch1.S5.p8.2"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p8.2.1">Chapter 8: adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with integrated LLM playgrounds:</span> Continuing the theme of open-source tools, this chapter describes the paper on adaptMLLM which describes the tool designed to fine-tune MLLMs. This application is user-friendly, streamlining the process of setting up the development environment and customising hyperparameters. Tested on two low-resource language pairs, EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p8.1.m1.1"><semantics id="Ch1.S5.p8.1.m1.1a"><mo id="Ch1.S5.p8.1.m1.1.1" stretchy="false" xref="Ch1.S5.p8.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p8.1.m1.1b"><ci id="Ch1.S5.p8.1.m1.1.1.cmml" xref="Ch1.S5.p8.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p8.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p8.1.m1.1d">↔</annotation></semantics></math>GA and English-Marathi (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S5.p8.2.m2.1"><semantics id="Ch1.S5.p8.2.m2.1a"><mo id="Ch1.S5.p8.2.m2.1.1" stretchy="false" xref="Ch1.S5.p8.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S5.p8.2.m2.1b"><ci id="Ch1.S5.p8.2.m2.1.1.cmml" xref="Ch1.S5.p8.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S5.p8.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S5.p8.2.m2.1d">↔</annotation></semantics></math>MR), it demonstrated significant performance improvements over the bespoke NMT system which won the LoResMT2021 Shared Task.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p9">
<p class="ltx_p" id="Ch1.S5.p9.1"><span class="ltx_text ltx_font_bold" id="Ch1.S5.p9.1.1">Chapter 9: Conclusion:</span> The concluding chapter revisits the research questions and highlights the research contributions before finally discussing the lessons learnt as well as avenues for future research. In the future work section, roadmaps are laid out for future development work on the adaptNMT and adaptMLLM applications. While the performance of MT systems developed through adaptMLLM is higher, it also demands an infrastructure with a higher specification. Therefore, it is anticipated there will be separate roles for both applications going forward.</p>
</div>
</section>
<section class="ltx_section" id="Ch1.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.6 </span>Research Contributions</h3>
<div class="ltx_para" id="Ch1.S6.p1">
<p class="ltx_p" id="Ch1.S6.p1.1">The transformative potential of the Transformer model in MT is undeniable, as its adoption has heralded state-of-the-art (SOTA) results for many language pairs. However, as illustrated in our paper <span class="ltx_text ltx_font_italic" id="Ch1.S6.p1.1.1">“Transformers for Low-Resource Languages - is féidir linn!”</span>,<span class="ltx_note ltx_role_footnote" id="Ch1.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The title of the paper was inspired by Obama’s 2011 presidential visit to Ireland where he adapted his campaign slogan <span class="ltx_text ltx_font_italic" id="Ch1.footnote4.1">“Yes we can!”</span> to its Gaelic equivalent: <span class="ltx_text ltx_font_italic" id="Ch1.footnote4.2">“Is Féidir Linn!”</span>.</span></span></span> when faced with limited training data, even advanced NMT models like the Transformer may fall short of expectations. This limitation underscores the necessity to fine-tune models and optimise hyperparameters for better translation performance, particularly for low-resource language pairs like EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S6.p1.1.m1.1"><semantics id="Ch1.S6.p1.1.m1.1a"><mo id="Ch1.S6.p1.1.m1.1.1" stretchy="false" xref="Ch1.S6.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S6.p1.1.m1.1b"><ci id="Ch1.S6.p1.1.m1.1.1.cmml" xref="Ch1.S6.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S6.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S6.p1.1.m1.1d">↔</annotation></semantics></math>GA. The pivotal role of subword models, especially in determining MT translation performance in low-resource scenarios was emphasised. In addressing RQ1, the Transformer architecture was optimised yielding impressive gains in BLEU scores and other performance metrics, thus bridging the gap posed by insufficient amounts of training data.</p>
</div>
<div class="ltx_para" id="Ch1.S6.p2">
<p class="ltx_p" id="Ch1.S6.p2.1">The relevance and potential of domain-specific translation cannot be overstated, especially in the current global landscape marked by unpredictable challenges. The recent Covid-19 pandemic highlighted the urgent need for accurate translation within the health domain, especially for languages that lack large resources. In the paper, <span class="ltx_text ltx_font_italic" id="Ch1.S6.p2.1.1">“MT in the Covid domain: an EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S6.p2.1.1.m1.1"><semantics id="Ch1.S6.p2.1.1.m1.1a"><mo id="Ch1.S6.p2.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch1.S6.p2.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S6.p2.1.1.m1.1b"><ci id="Ch1.S6.p2.1.1.m1.1.1.cmml" xref="Ch1.S6.p2.1.1.m1.1.1">normal-↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S6.p2.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S6.p2.1.1.m1.1d">↔</annotation></semantics></math>GA case study for LoResMT2021 Shared Task”</span>, cf. Chapter 3, the efficacy of domain adaptation techniques was explored to address RQ2. Through this research, the significance of developing specialised datasets for domain-specific translation was highlighted. Augmenting in-domain data with a modest amount of domain-specific data yielded MT models which obtained 1st place in the LoResMT2021 Shared Task. Such findings emphasise the potential that lies in domain-focused MT models and drive advancements in domain-specific translation.</p>
</div>
<div class="ltx_para" id="Ch1.S6.p3">
<p class="ltx_p" id="Ch1.S6.p3.1">The idea of domain specificity was further developed by creating a corpus specifically for the health sector. As highlighted in the paper <span class="ltx_text ltx_font_italic" id="Ch1.S6.p3.1.1">“gaHealth: An English–Irish Bilingual Corpus of Health Data”</span>, cf. Chapter 4, there is a marked absence of parallel datasets tailored for low-resource languages. While the rush to build larger, more encompassing datasets is understandable, the merits of focused, in-domain datasets have often been overshadowed. The development of an in-domain corpus and subsequent results from training an MT system with the gaHealth corpus demonstrate the tangible benefits of deploying in-domain datasets for translation tasks. Moreover, in addressing RQ2, this study has provided a valuable resource for the wider NLP community.</p>
</div>
<div class="ltx_para" id="Ch1.S6.p4">
<p class="ltx_p" id="Ch1.S6.p4.1">In addressing the research question posed by RQ3, a detailed human evaluation was carried out to validate the translation outputs of RNN and Transformer models for a low-resource language pair. The approach taken and the findings are described in the paper, ‘<span class="ltx_text ltx_font_italic" id="Ch1.S6.p4.1.1">‘Human Evaluation of English–Irish Transformer-Based NMT”</span>, cf. Chapter 5. We utilised the MQM error framework to investigate the types of errors produced by both RNN and Transformer-based systems. Native Irish speakers were collaborated with for this assessment, and one of the research contributions was to establish how a human evaluation could be conducted with limited resources. As such, we adopted a simplified MQM error taxonomy, pairing it with an SQM. By engaging two native speakers and analysing 25 reference translations, we were able to validate the outputs from both the RNN and Transformer models. Our results indicate the top-performing Transformer system notably minimises errors in both accuracy and fluency when compared with its RNN counterpart. A significant takeaway from our study is the linguistic insights shared by our translators, which are elaborated on in the related paper (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8.SS3" title="5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.8.3</span></a>).</p>
</div>
<div class="ltx_para" id="Ch1.S6.p5">
<p class="ltx_p" id="Ch1.S6.p5.1">These practices were combined into a new framework which was subsequently applied when evaluating the output from MLLMs in our later work. Furthermore, in a separate contribution, this human evaluation component was integrated into the adaptNMT and adaptMLLM applications effectively bridging RQ3 with RQ4.</p>
</div>
<div class="ltx_para" id="Ch1.S6.p6">
<p class="ltx_p" id="Ch1.S6.p6.1">The research question identified by RQ4 was addressed in the subsequent journal paper, <span class="ltx_text ltx_font_italic" id="Ch1.S6.p6.1.1">“adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation”</span>, cf. Chapter 7. The application introduces an open-source application designed to simplify the NMT development process. adaptNMT is a representation of the increasing demand for tools that are intuitive, powerful, and user-friendly. One of the key features of this tool is its commitment to sustainable NLP research, evident from its green report, which calculates the environmental costs of model training. By highlighting the environmental cost of training models, users are encouraged to think about the carbon footprint of their research activities and are consequently incentivised to use a greener infrastructure or pre-trained models, where possible. Furthermore, adaptNMT’s role in the broader NLP community is underlined by its open-source nature, inviting collaborative enhancement from researchers worldwide.</p>
</div>
<div class="ltx_para" id="Ch1.S6.p7">
<p class="ltx_p" id="Ch1.S6.p7.1">Expanding on the foundation set by adaptNMT, RQ4 was further developed in the paper <span class="ltx_text ltx_font_italic" id="Ch1.S6.p7.1.1">“adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with integrated LLM playgrounds”</span>, cf. Chapter 8. Recognising the transformative potential of MLLMs and LLMs, the paper showcases the very significant gains in translation quality achieved by fine-tuning MLLMs and LLMs for low-resource language pairs. The adaptMLLM tool democratises cutting-edge NLP technologies, demonstrating that SOTA results can be achieved without the backing of colossal research infrastructures. Moreover, its open-source nature mirrors the ethos of adaptNMT, advocating for community-driven advancements in the field.</p>
</div>
<figure class="ltx_figure" id="Ch1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="825" id="Ch1.F2.g1" src="extracted/5444776/Images/PhD-timeline.png" width="628"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch1.F2.2.1.1" style="font-size:90%;">Figure 1.2</span>: </span><span class="ltx_text" id="Ch1.F2.3.2" style="font-size:90%;">Timeline of publications and mapping to thesis chapters. The illustration highlights the initial set of papers on automated machine learning (AutoML), followed by the MT papers and culminating in a book on the topic of MT and automation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx82" title="">82</a>]</cite>. </span></figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch1.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.7 </span>Publications</h3>
<div class="ltx_para" id="Ch1.S7.p1">
<p class="ltx_p" id="Ch1.S7.p1.1">My PhD research has culminated in the publication of eleven peer-reviewed papers, and presentations both virtual and physical at conferences in Dublin, Kyoto, Macau, Orlando and Tampere. Best presentation awards were received at the ACM conferences in Kyoto (2020)<span class="ltx_note ltx_role_footnote" id="Ch1.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.adip.org/2020.html" title="">http://www.adip.org/2020.html</a></span></span></span> and Macau (2021).<span class="ltx_note ltx_role_footnote" id="Ch1.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.aibc.org/2021.html" title="">http://www.aibc.org/2021.html</a></span></span></span> Furthermore, I developed a translation system for LoResMT2021 which obtained first place in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch1.S7.p1.1.m1.1"><semantics id="Ch1.S7.p1.1.m1.1a"><mo id="Ch1.S7.p1.1.m1.1.1" stretchy="false" xref="Ch1.S7.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch1.S7.p1.1.m1.1b"><ci id="Ch1.S7.p1.1.m1.1.1.cmml" xref="Ch1.S7.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S7.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S7.p1.1.m1.1d">→</annotation></semantics></math>GA shared task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S7.p2">
<p class="ltx_p" id="Ch1.S7.p2.1">All of the recent work, comprising seven publications, focuses on advancements in NMT and the development of user-friendly tools. These MT-related papers have been included as individual chapters in this thesis. The full set of peer-reviewed MT publications is outlined below.</p>
</div>
<div class="ltx_para" id="Ch1.S7.p3">
<ol class="ltx_enumerate" id="Ch1.S7.I1">
<li class="ltx_item" id="Ch1.S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Ch1.S7.I1.i1.p1">
<p class="ltx_p" id="Ch1.S7.I1.i1.p1.1">Lankford, S., Afli, H. and Way, A., 2023. adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with integrated LLM playgrounds. In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i1.p1.1.1">Information 14.12</span>, issn: 2078-2489, doi: 10.3390/info14120638, url: https://www.mdpi.com/2078-2489/14/12/638.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ch1.S7.I1.i2.p1">
<p class="ltx_p" id="Ch1.S7.I1.i2.p1.1">Lankford, S., Afli, H. and Way, A., 2023. adaptNMT: an open-source, language-agnostic development environment for neural machine translation. In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i2.p1.1.1">Language Resources and Evaluation</span>, pp.1-26. 
<br class="ltx_break"/>url: https://doi.org/10.1007/s10579-023-09671-2</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ch1.S7.I1.i3.p1">
<p class="ltx_p" id="Ch1.S7.I1.i3.p1.1">Lankford, S., Afli, H. and Way, A., 2023. Design of an Open-Source Architecture for Neural Machine Translation. In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i3.p1.1.1">Proceedings of the 1st Workshop on Open Community-Driven Machine Translation</span>. Tampere, Finland: European Association for Machine Translation, pp. 15–20. 
<br class="ltx_break"/>url: https://aclanthology.org/2023.crowdmt-1.2.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Ch1.S7.I1.i4.p1">
<p class="ltx_p" id="Ch1.S7.I1.i4.p1.1">Lankford, S., Afli, H. and Way, A., 2022. Human Evaluation of English-Irish Transformer-Based NMT. In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i4.p1.1.1">Information 13.7</span>, issn: 2078-2489. doi: 10.3390/info13070309. url: https://www.mdpi.com/2078-2489/13/7/309.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S7.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Ch1.S7.I1.i5.p1">
<p class="ltx_p" id="Ch1.S7.I1.i5.p1.1">Lankford, S., Afli, H., Ní Loinsigh Ó., Way, A., 2022. “gaHealth: An English–Irish Bilingual Corpus of Health Data”. In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i5.p1.1.1">Proceedings of the Thirteenth Language Resources and Evaluation Conference</span>. Marseille, France: European Language Resources Association, pp. 6753–6758. url: https://aclanthology. org/2022.lrec-1.727.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S7.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Ch1.S7.I1.i6.p1">
<p class="ltx_p" id="Ch1.S7.I1.i6.p1.1">Lankford, S., Afli, H., and Way, A. 2021. “Transformers for Low-Resource Languages: Is Féidir Linn!” In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i6.p1.1.1">Proceedings of Machine Translation Summit XVIII: Research Track</span>. Virtual: Association for Machine Translation in the Americas, pp. 48–60. url: https://aclanthology.org/2021.mtsummit-research.5.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S7.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Ch1.S7.I1.i7.p1">
<p class="ltx_p" id="Ch1.S7.I1.i7.p1.1">Lankford, S., Afli, H., and Way, A. 2021. “Machine Translation in the Covid domain: an EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch1.S7.I1.i7.p1.1.m1.1"><semantics id="Ch1.S7.I1.i7.p1.1.m1.1a"><mo id="Ch1.S7.I1.i7.p1.1.m1.1.1" stretchy="false" xref="Ch1.S7.I1.i7.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch1.S7.I1.i7.p1.1.m1.1b"><ci id="Ch1.S7.I1.i7.p1.1.m1.1.1.cmml" xref="Ch1.S7.I1.i7.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S7.I1.i7.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch1.S7.I1.i7.p1.1.m1.1d">↔</annotation></semantics></math>GA case study for LoResMT 2021”. In: <span class="ltx_text ltx_font_italic" id="Ch1.S7.I1.i7.p1.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</span>. Virtual: Association for Machine Translation in the Americas, pp. 144–150. url: https://aclanthology.org/2021.mtsummit-loresmt.15</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch2">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 2 </span>Transformers for Low-Resource Languages</h2>
<section class="ltx_section" id="Ch2.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.1 </span>Context</h3>
<div class="ltx_para" id="Ch2.S1.p1">
<p class="ltx_p" id="Ch2.S1.p1.1">The motivation for this research stems from the recognition that Transformer models are now SOTA in the MT arena, offering remarkable performance on well-resourced language pairs. However, such models often struggle when dealing with low-resource language pairs due to the scarcity of training data. This limitation has led to relatively few experiments and advancements in using Transformers for such language pairs. As a result, my motivation was to address RQ1 by exploring and optimising Transformer models specifically for translating low-resource languages such as the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S1.p1.1.m1.1"><semantics id="Ch2.S1.p1.1.m1.1a"><mo id="Ch2.S1.p1.1.m1.1.1" stretchy="false" xref="Ch2.S1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S1.p1.1.m1.1b"><ci id="Ch2.S1.p1.1.m1.1.1.cmml" xref="Ch2.S1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S1.p1.1.m1.1d">↔</annotation></semantics></math>GA language pair.</p>
</div>
<div class="ltx_para" id="Ch2.S1.p2">
<p class="ltx_p" id="Ch2.S1.p2.1">The key motivations and objectives of this research can be summarised as follows:</p>
<ul class="ltx_itemize" id="Ch2.S1.I1">
<li class="ltx_item" id="Ch2.S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S1.I1.i1.p1">
<p class="ltx_p" id="Ch2.S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.I1.i1.p1.1.1">Addressing Low-Resource Challenges</span>: The research seeks to tackle the challenges posed by low-resource language pairs, where conventional NMT models tend to perform poorly due to insufficient training data. The motivation is to bridge the performance gap and enhance translation quality for underrepresented language pairs.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S1.I1.i2.p1">
<p class="ltx_p" id="Ch2.S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.I1.i2.p1.1.1">Hyperparameter Optimisation</span>: The study focuses on hyperparameter optimisation for Transformer models. This is motivated by the understanding that the performance of MT models is heavily influenced by the choices made in configuring the model’s hyperparameters. The goal is to identify the most effective set of hyperparameters for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S1.I1.i2.p1.1.m1.1"><semantics id="Ch2.S1.I1.i2.p1.1.m1.1a"><mo id="Ch2.S1.I1.i2.p1.1.m1.1.1" stretchy="false" xref="Ch2.S1.I1.i2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S1.I1.i2.p1.1.m1.1b"><ci id="Ch2.S1.I1.i2.p1.1.m1.1.1.cmml" xref="Ch2.S1.I1.i2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S1.I1.i2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S1.I1.i2.p1.1.m1.1d">↔</annotation></semantics></math>GA translation.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S1.I1.i3.p1">
<p class="ltx_p" id="Ch2.S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.I1.i3.p1.1.1">Subword Model Selection</span>: Another key motivation is to determine the optimal subword model for low-resource translation. This choice is critical as it impacts the model’s ability to handle the linguistic nuances of the target language. The research evaluates different subword models, including SentencePiece models with unigram and BPE approaches.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S1.I1.i4.p1">
<p class="ltx_p" id="Ch2.S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.I1.i4.p1.1.1">Model Architecture Exploration</span>: Various aspects of the Transformer architecture were investigated to enhance translation performance. This included modifying the number of layers, testing regularisation techniques, and evaluating the ideal number of attention heads. The goal is to identify the best architectural configurations for the given language pair.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S1.I1.i5.p1">
<p class="ltx_p" id="Ch2.S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.I1.i5.p1.1.1">Benchmarking and Comparison</span>: To assess the effectiveness of the optimised Transformer models, the research benchmarks them against existing translation systems, including Google Translate. This comparison provides empirical evidence of the improvements achieved by their proposed approach.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S1.I1.i6.p1">
<p class="ltx_p" id="Ch2.S1.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.I1.i6.p1.1.1">Reducing Post-Editing Effort</span>: By improving translation quality, the research aims to reduce the post-editing effort required for low-resource language pairs. This has practical implications for making MT more accessible and cost-effective in scenarios where human expertise is required for quality control.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch2.S1.p3">
<p class="ltx_p" id="Ch2.S1.p3.1">In summary, the motivation for this paper is to address the challenges faced by low-resource language pairs in MT by optimising Transformer models. The study explores hyperparameter selection, subword modelling, and architectural modifications to achieve substantial performance improvements, ultimately making MT more effective and accessible for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S1.p3.1.m1.1"><semantics id="Ch2.S1.p3.1.m1.1a"><mo id="Ch2.S1.p3.1.m1.1.1" stretchy="false" xref="Ch2.S1.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S1.p3.1.m1.1b"><ci id="Ch2.S1.p3.1.m1.1.1.cmml" xref="Ch2.S1.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S1.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S1.p3.1.m1.1d">↔</annotation></semantics></math>GA translation.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch2.S1.p4">
<p class="ltx_p" id="Ch2.S1.p4.1"><span class="ltx_text ltx_font_bold" id="Ch2.S1.p4.1.1">Transformers for Low-Resource Languages: Is Féidir Linn!</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch2.S1.p4.2"><span class="ltx_text ltx_font_bold" id="Ch2.S1.p4.2.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch2.S1.p4.3"><span class="ltx_text ltx_font_bold" id="Ch2.S1.p4.3.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch2.S1.p4.4"><span class="ltx_text ltx_font_bold" id="Ch2.S1.p4.4.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch2.S1.p4.5">Proceedings of Machine Translation Summit XVIII</p>
<p class="ltx_p" id="Ch2.S1.p4.6">Research Track. Virtual: Association for Machine Translation in the Americas  August 16 - 20, 2021</p>
<p class="ltx_p" id="Ch2.S1.p4.7">Florida, USA</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch2.S1.p4.8">ADAPT Centre</p>
<p class="ltx_p" id="Ch2.S1.p4.9">Dublin City University</p>
<p class="ltx_p" id="Ch2.S1.p4.10">Ireland</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch2.S1.p4.11"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-research.5.pdf" title="">https://aclanthology.org/2021.mtsummit-research.5.pdf</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch2.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch2.S2.p1">
<p class="ltx_p" id="Ch2.S2.p1.1">The Transformer model is state-of-the-art in MT. However, in general, neural translation models often underperform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimisation of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate hyperparameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an in-domain 88k public admin corpus were used for evaluation. A Transformer-optimised model demonstrated a BLEU score improvement of 7.8 points when compared with a baseline RNN model. Improvements were observed across a range of metrics, including TER, indicating a substantially reduced post-editing effort for Transformer-optimised models with 16k BPE subword models. Benchmarked against Google Translate, our translation engines demonstrated significant improvements. The question of whether or not Transformers can be used effectively in a low-resource setting of English-Irish translation has been addressed. Is féidir linn - yes we can.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.3 </span>Introduction</h3>
<div class="ltx_para" id="Ch2.S3.p1">
<p class="ltx_p" id="Ch2.S3.p1.1">The advent of neural machine translation (NMT) has heralded an era of high-quality translations. However, these improvements have not been manifested in the translation of all languages. Large datasets are a prerequisite for high-quality NMT. This works well in the context of well-resourced languages where there is an abundance of data. In the context of low-resource languages which suffer from a sparsity of data, alternative approaches must be adopted.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p2">
<p class="ltx_p" id="Ch2.S3.p2.1">An important part of this research involves developing applications and models to address the challenges of low-resource language technology. Such technology incorporates methods to address the data scarcity affecting deep learning for digital engagement of low-resource languages.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p3">
<p class="ltx_p" id="Ch2.S3.p3.1">It has been shown that an out-of-the-box NMT system, trained on English-Irish (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S3.p3.1.m1.1"><semantics id="Ch2.S3.p3.1.m1.1a"><mo id="Ch2.S3.p3.1.m1.1.1" stretchy="false" xref="Ch2.S3.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S3.p3.1.m1.1b"><ci id="Ch2.S3.p3.1.m1.1.1.cmml" xref="Ch2.S3.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S3.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S3.p3.1.m1.1d">↔</annotation></semantics></math>GA) data, achieves a lower translation quality compared with using a tailored SMT system (Dowling et al, 2018). It is in this context that further research is required in the development of NMT for low-resource languages and the Irish language in particular.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p4">
<p class="ltx_p" id="Ch2.S3.p4.1">Most research on choosing subword models has focused on high-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx43" title="">43</a>]</cite>. In the context of developing models for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S3.p4.1.m1.1"><semantics id="Ch2.S3.p4.1.m1.1a"><mo id="Ch2.S3.p4.1.m1.1.1" stretchy="false" xref="Ch2.S3.p4.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S3.p4.1.m1.1b"><ci id="Ch2.S3.p4.1.m1.1.1.cmml" xref="Ch2.S3.p4.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S3.p4.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S3.p4.1.m1.1d">↔</annotation></semantics></math>GA translation, there are no clear recommendations on the choice of subword model types. One of the objectives of this study is to identify which type of subword model performs best in this low-resource scenario.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.4 </span>Background</h3>
<div class="ltx_para" id="Ch2.S4.p1">
<p class="ltx_p" id="Ch2.S4.p1.1">Native speakers of low-resource languages are often excluded from useful content since, more often than not, online content is not available to them in their language of choice. Such a digital divide and the resulting social exclusion experienced by second-language speakers, such as refugees living in developed countries, has been well documented in the research literature  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S4.p2">
<p class="ltx_p" id="Ch2.S4.p2.1">Research on machine translation (MT) in low-resource scenarios directly addresses this challenge of exclusion via pivot languages  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx72" title="">72</a>]</cite>, and indirectly, via domain adaptation of models  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx42" title="">42</a>]</cite>. Breakthrough performance improvements in the area of MT have been achieved through research efforts focusing on NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx28" title="">28</a>]</cite>. Consequently, state-of-the-art (SOTA) performance has been attained on multiple language pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx21" title="">21</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="Ch2.S4.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4.1 </span>Irish Language</h4>
<div class="ltx_para" id="Ch2.S4.SS1.p1">
<p class="ltx_p" id="Ch2.S4.SS1.p1.1">The Irish language is a primary example of such a low-resource language that will benefit from this research. NMT involving Transformer model development will improve the performance in specific domains of low-resource languages. Such research will address the end of the Irish language derogation in the European Commission in 2021<span class="ltx_note ltx_role_footnote" id="Ch2.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://amtaweb.org/wp-content/uploads/2020/11/MT-in-EU-Overview-with-Voiceover-Andy-Way-KEYNOTE-K1.pdf" title="">http://amtaweb.org/wp-content/uploads/2020/11/MT-in-EU-Overview-with-Voiceover-Andy-Way-KEYNOTE-K1.pdf</a></span></span></span> helping to deliver parity in support for Irish in online digital engagement.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S4.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4.2 </span>Hyperparameter Optimisation</h4>
<div class="ltx_para" id="Ch2.S4.SS2.p1">
<p class="ltx_p" id="Ch2.S4.SS2.p1.1">Hyperparameters are employed to customise machine learning models such as translation models. It has been shown that machine learning performance may be improved through HPO rather than just using default settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx98" title="">98</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S4.SS2.p2">
<p class="ltx_p" id="Ch2.S4.SS2.p2.1">The principle methods of HPO are grid search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx81" title="">81</a>]</cite> and random search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx16" title="">16</a>]</cite>]. Grid search is an exhaustive technique which evaluates all hyperparameter permutations. However, as the number of features grows, the amount of data permutations grows exponentially making optimisation expensive in the context of developing long-running translation models.</p>
</div>
<div class="ltx_para" id="Ch2.S4.SS2.p3">
<p class="ltx_p" id="Ch2.S4.SS2.p3.1">An effective, and less computationally intensive, alternative is to use random search which samples random configurations.</p>
</div>
<section class="ltx_subsubsection" id="Ch2.S4.SS2.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Recurrent Neural Networks</h5>
<div class="ltx_para" id="Ch2.S4.SS2.SSSx1.p1">
<p class="ltx_p" id="Ch2.S4.SS2.SSSx1.p1.1">Recurrent neural networks (RNNs) are often used for the tasks of natural language processing, speech recognition and MT. RNN models enable previous outputs to be used as inputs while having hidden states. In the context of MT, such neural networks were ideal due to their ability to process inputs of any length. Furthermore, the model sizes do not necessarily increase with the size of its input. Commonly used variants of RNN include bidirectional (BRNN) and deep (DRNN) architectures. However, the problem of vanishing gradients coupled with the development of attention-based algorithms often leads to Transformer models performing better than RNNs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch2.S4.SS2.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Transformer</h5>
<div class="ltx_para" id="Ch2.S4.SS2.SSSx2.p1">
<p class="ltx_p" id="Ch2.S4.SS2.SSSx2.p1.1">The greatest improvements have been demonstrated when either the RNN or the CNN architecture is abandoned completely and replaced with an attention mechanism creating a much simpler and faster architecture known as Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite>.
Transformer models use attention to focus on previously generated tokens. The approach allows models to develop a long memory which is particularly useful in the domain of language translation. Performance improvements to both RNN and CNN approaches may be achieved through the introduction of such attention layers in the translation architecture.</p>
</div>
<div class="ltx_para" id="Ch2.S4.SS2.SSSx2.p2">
<p class="ltx_p" id="Ch2.S4.SS2.SSSx2.p2.1">Experiments in MT tasks show such models are better in quality due to greater parallelisation while requiring significantly less time to train.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch2.S4.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4.3 </span>Subword Models</h4>
<div class="ltx_para" id="Ch2.S4.SS3.p1">
<p class="ltx_p" id="Ch2.S4.SS3.p1.1">Translation, by its nature, requires an open vocabulary and the use of subword models aims to address the fixed vocabulary problem associated with NMT. Rare and unknown words are encoded as sequences of subword units. By adapting the original Byte Pair Encoding (BPE) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx41" title="">41</a>]</cite>, the use of BPE submodels can improve translation performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx60" title="">60</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S4.SS3.p2">
<p class="ltx_p" id="Ch2.S4.SS3.p2.1">Designed for NMT, SentencePiece, is a language-independent subword tokenizer that provides an open-source C++ and a Python implementation for subword units. An attractive feature of the tokenizer is that SentencePiece trains subword models directly from raw sentences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="Ch2.S4.SS3.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Byte Pair Encoding compared with Unigram</h5>
<div class="ltx_para" id="Ch2.S4.SS3.SSSx1.p1">
<p class="ltx_p" id="Ch2.S4.SS3.SSSx1.p1.1">BPE and unigram language models are similar in that both encode text using fewer bits but each uses a different data compression principle (dictionary vs. entropy). In principle, we would expect the same benefits with the unigram language model as with BPE. However, unigram models are often more flexible since they are probabilistic models that output multiple segmentations with their probabilities.</p>
</div>
<figure class="ltx_figure" id="Ch2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch2.F1.g1" src="mtsummit2021.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F1.2.1.1" style="font-size:90%;">Figure 2.1</span>: </span><span class="ltx_text" id="Ch2.F1.3.2" style="font-size:90%;">Proposed approach of Transformers for low-resource languages</span></figcaption>
</figure>
<figure class="ltx_table ltx_align_center" id="Ch2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T1.2.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch2.T1.2.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.2.2.1">Learning rate</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.2.2.2">0.1, 0.01, 0.001, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.2.2.2.1">2</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.3.3.1">Batch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.3.3.2">1024, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.3.3.2.1">2048</span>, 4096, 8192</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.4.4.1">Attention heads</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.4.4.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T1.2.4.4.2.1">2</span>, 4, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.4.4.2.2">8</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.5.5.1">Number of layers</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.5.5.2">5, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.5.5.2.1">6</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.6.6.1">Feed-forward dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.6.6.2"><span class="ltx_text ltx_font_bold" id="Ch2.T1.2.6.6.2.1">2048</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.7.7.1">Embedding dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.7.7.2">128, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.7.7.2.1">256</span>, 512</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.8.8.1">Label smoothing</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.8.8.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T1.2.8.8.2.1">0.1</span>, 0.3</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.9.9.1">Dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.9.9.2">0.1, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.9.9.2.1">0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T1.2.10.10.1">Attention dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.2.10.10.2"><span class="ltx_text ltx_font_bold" id="Ch2.T1.2.10.10.2.1">0.1</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.2.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch2.T1.2.11.11.1">Average Decay</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch2.T1.2.11.11.2">0, <span class="ltx_text ltx_font_bold" id="Ch2.T1.2.11.11.2.1">0.0001</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T1.3.1.1" style="font-size:90%;">Table 2.1</span>: </span><span class="ltx_text" id="Ch2.T1.4.2" style="font-size:90%;">Hyperparameter optimisation for Transformer models. Optimal hyperparameters are highlighted in bold. The highest performing model trained on the 55k DGT corpus uses 2 attention heads whereas the best model trained with the larger 88k PA dataset uses 8 attention heads.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="Ch2.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.5 </span>Proposed Approach</h3>
<div class="ltx_para" id="Ch2.S5.p1">
<p class="ltx_p" id="Ch2.S5.p1.1">HPO of RNN models in low-resource settings has previously demonstrated considerable performance improvements. The extent to which such optimisation techniques may be applied to Transformer models in similar low-resource scenarios is evaluated as part of this study. Evaluations included modifying the number of attention heads, the number of layers and experimenting with regularisation techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and the vocabulary size are evaluated.</p>
</div>
<div class="ltx_para" id="Ch2.S5.p2">
<p class="ltx_p" id="Ch2.S5.p2.1">To test the effectiveness of our approaches, optimisation was carried out on two EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S5.p2.1.m1.1"><semantics id="Ch2.S5.p2.1.m1.1a"><mo id="Ch2.S5.p2.1.m1.1.1" stretchy="false" xref="Ch2.S5.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S5.p2.1.m1.1b"><ci id="Ch2.S5.p2.1.m1.1.1.cmml" xref="Ch2.S5.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S5.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S5.p2.1.m1.1d">↔</annotation></semantics></math>GA parallel datasets: a general corpus of 52k lines from the Directorate General for Translation (DGT) and an in-domain corpus of 88k lines of Public Administration (PA) data. With DGT, the test set used 1.3k lines and the development set comprised 2.6k lines. In the case of the PA dataset, there were 1.5k lines of test data and 3k lines of validation. All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece subword model. The impact of using separate source and target subword models was not explored.</p>
</div>
<div class="ltx_para" id="Ch2.S5.p3">
<p class="ltx_p" id="Ch2.S5.p3.1">The approach adopted is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F1" title="Figure 2.1 ‣ Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.1</span></a>. Two baseline architectures, RNN and Transformer, are evaluated. On evaluating the hyperparameter choices for Transformer models, the values outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T1" title="Table 2.1 ‣ Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.1</span></a> were tested using a random search approach. A range of values for each hyperparameter was tested using short cycles of 5k training steps. Once an optimal value, within the sampled range was identified, it was locked in for tests on subsequent hyperparameters.</p>
</div>
<section class="ltx_subsection" id="Ch2.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5.1 </span>Architecture Tuning</h4>
<div class="ltx_para" id="Ch2.S5.SS1.p1">
<p class="ltx_p" id="Ch2.S5.SS1.p1.1">Given the long training times associated with NMT, it is difficult and costly to tune systems using a conventional grid search approach. Therefore a random search approach was adopted in the HPO of our transformer models.</p>
</div>
<div class="ltx_para" id="Ch2.S5.SS1.p2">
<p class="ltx_p" id="Ch2.S5.SS1.p2.1">With low-resource datasets, the use of smaller and fewer layers has previously been shown to improve performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx5" title="">5</a>]</cite>. Performance of low-resource NMT has also been demonstrated to improve in cases where shallow Transformer models are adopted  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx17" title="">17</a>]</cite>. Guided by these findings, configurations were tested which varied the number of neurons in each layer and modified the number of layers used in the Transformer architecture.</p>
</div>
<div class="ltx_para" id="Ch2.S5.SS1.p3">
<p class="ltx_p" id="Ch2.S5.SS1.p3.1">The impact of regularisation, by applying varying degrees of dropout to Transformer models, was evaluated. Configurations using smaller (0.1) and larger values (0.3) were applied to the output of each feed-forward layer.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5.2 </span>Subword Models</h4>
<div class="ltx_para" id="Ch2.S5.SS2.p1">
<p class="ltx_p" id="Ch2.S5.SS2.p1.1">It has become standard practice to incorporate word segmentation approaches, such as Byte-Pair-Encoding (BPE) when developing NMT models. Previous work shows that subword models may be particularly beneficial for low-resource languages since rare words are often a problem. Reducing the number of BPE merge operations resulted in substantial improvements of 5 BLEU points (Sennrich and Zhang 2019) when tested on RNN models.</p>
</div>
<div class="ltx_para" id="Ch2.S5.SS2.p2">
<p class="ltx_p" id="Ch2.S5.SS2.p2.1">In the context of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S5.SS2.p2.1.m1.1"><semantics id="Ch2.S5.SS2.p2.1.m1.1a"><mo id="Ch2.S5.SS2.p2.1.m1.1.1" stretchy="false" xref="Ch2.S5.SS2.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S5.SS2.p2.1.m1.1b"><ci id="Ch2.S5.SS2.p2.1.m1.1.1.cmml" xref="Ch2.S5.SS2.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S5.SS2.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S5.SS2.p2.1.m1.1d">↔</annotation></semantics></math>GA translation, there is no clear agreement as to what constituted the best approach. Consequently, as part of this study, subword regularisation techniques, involving BPE and unigram models were evaluated to determine the optimal hyperparameters for maximising translation performance. BPE models with varying vocabulary sizes of 4k, 8k, 16k and 32k were tested.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch2.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.6 </span>Empirical Evaluation</h3>
<section class="ltx_subsection" id="Ch2.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6.1 </span>Experimental Setup</h4>
<section class="ltx_subsubsection" id="Ch2.S6.SS1.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Datasets</h5>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx1.p1">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx1.p1.1">The performance of the Transformer and RNN approaches is evaluated on EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S6.SS1.SSSx1.p1.1.m1.1"><semantics id="Ch2.S6.SS1.SSSx1.p1.1.m1.1a"><mo id="Ch2.S6.SS1.SSSx1.p1.1.m1.1.1" stretchy="false" xref="Ch2.S6.SS1.SSSx1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S6.SS1.SSSx1.p1.1.m1.1b"><ci id="Ch2.S6.SS1.SSSx1.p1.1.m1.1.1.cmml" xref="Ch2.S6.SS1.SSSx1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S6.SS1.SSSx1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S6.SS1.SSSx1.p1.1.m1.1d">↔</annotation></semantics></math>GA parallel datasets. Two datasets were used in the evaluation of our models namely the publicly available DGT dataset which may be broadly categorised as generic and an in-domain dataset which focuses on public administration data.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx1.p2">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx1.p2.1">The DGT, and its Joint Research Centre, have made available all Translation Memory (TM; i.e. sentences and their professionally produced translations) which cover all official European Union languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx107" title="">107</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx1.p3">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx1.p3.1">Data provided by the Department of Tourism, Culture, Arts, Gaeltacht, Sport and Media in Ireland formed the majority of the data in the public administration dataset. This includes staff notices, annual reports, website content, press releases and official correspondence.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx1.p4">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx1.p4.1">Parallel texts from the Digital Corpus of the European Parliament (DCEP) and the DGT are included in the training data. Crawled data, from sites of a similar domain are included. Furthermore, a parallel corpus collected from Conradh na Gaeilge (CnaG), an Irish language organisation that promotes the Irish language, was included. The dataset was compiled as part of a previous study which carried out a preliminary comparison of SMT and NMT models for the Irish language  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx37" title="">37</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch2.S6.SS1.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Infrastructure</h5>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx2.p1">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx2.p1.1">Models were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16GB memory, a 256GB SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16GB graphic cards and up to 27GB of memory when available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx2.p2">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx2.p2.1">Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch2.S6.SS1.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Metrics</h5>
<div class="ltx_para" id="Ch2.S6.SS1.SSSx3.p1">
<p class="ltx_p" id="Ch2.S6.SS1.SSSx3.p1.1">As part of this study, several automated metrics were used to determine the translation quality. All models were trained and evaluated on both the DGT and PA datasets using the BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite> evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch2.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6.2 </span>Results</h4>
<section class="ltx_subsubsection" id="Ch2.S6.SS2.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Performance of subword models</h5>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx1.p1">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx1.p1.1">The impact on translation accuracy when choosing a subword model is highlighted in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T2" title="Table 2.2 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.2</span></a> - <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T5" title="Table 2.5 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.5</span></a>. In training both RNN and Transformer architectures, incorporating any submodel type led to improvements in model accuracy. This finding is evident when training either the smaller generic DGT dataset or the larger in-domain PA dataset.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx1.p2">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx1.p2.1">Using an RNN architecture on DGT, as illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T2" title="Table 2.2 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.2</span></a>, the best-performing model with a 32k unigram submodel, achieved a BLEU score 7.4% higher than the baseline. With the PA dataset using an RNN, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T3" title="Table 2.3 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.3</span></a>, the model with the best BLEU, TER and ChrF3 scores again used a unigram submodel.</p>
</div>
<figure class="ltx_table" id="Ch2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.3.4.1">Architecture</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch2.T2.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T2.1.1.1.m1.1"><semantics id="Ch2.T2.1.1.1.m1.1a"><mo id="Ch2.T2.1.1.1.m1.1.1" stretchy="false" xref="Ch2.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T2.1.1.1.m1.1b"><ci id="Ch2.T2.1.1.1.m1.1.1.cmml" xref="Ch2.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T2.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T2.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch2.T2.2.2.2.m1.1"><semantics id="Ch2.T2.2.2.2.m1.1a"><mo id="Ch2.T2.2.2.2.m1.1.1" stretchy="false" xref="Ch2.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch2.T2.2.2.2.m1.1b"><ci id="Ch2.T2.2.2.2.m1.1.1.cmml" xref="Ch2.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T2.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T2.3.3.3.m1.1"><semantics id="Ch2.T2.3.3.3.m1.1a"><mo id="Ch2.T2.3.3.3.m1.1.1" stretchy="false" xref="Ch2.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T2.3.3.3.m1.1b"><ci id="Ch2.T2.3.3.3.m1.1.1.cmml" xref="Ch2.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T2.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.3.5.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.3.3.6">
<table class="ltx_tabular ltx_align_middle" id="Ch2.T2.3.3.6.1">
<tr class="ltx_tr" id="Ch2.T2.3.3.6.1.1">
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.3.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.3.6.1.1.1.1">Runtime</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T2.3.3.6.1.2">
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.3.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.3.6.1.2.1.1">(hours)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T2.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.3.7.1">kgCO<sub class="ltx_sub" id="Ch2.T2.3.3.7.1.1">2</sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T2.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.1">dgt-rnn-base</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.2">52.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.3">0.42</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.4">0.71</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.5">75k</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.6">4.47</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.3.4.1.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T2.3.5.2">
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.1">dgt-rnn-bpe8k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.2">54.6</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.3">0.40</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.4">0.73</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.5">85k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.6">5.07</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.5.2.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T2.3.6.3">
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.1">dgt-rnn-bpe16k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.2">55.6</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.3">0.39</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.4">0.74</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.5">100k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.6">5.58</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.6.3.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T2.3.7.4">
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.1">dgt-rnn-bpe32k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.2">55.3</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.3">0.39</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.4">0.74</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.5">95k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.6">4.67</td>
<td class="ltx_td ltx_align_left" id="Ch2.T2.3.7.4.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T2.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.1">dgt-rnn-unigram</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.2"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.8.5.2.1">55.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.3"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.8.5.3.1">0.39</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.4"><span class="ltx_text ltx_font_bold" id="Ch2.T2.3.8.5.4.1">0.74</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.5">105k</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.6">5.07</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T2.3.8.5.7">0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T2.5.1.1" style="font-size:90%;">Table 2.2</span>: </span><span class="ltx_text" id="Ch2.T2.6.2" style="font-size:90%;">RNN performance on DGT dataset of 52k lines</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.3.4.1">Architecture</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch2.T3.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T3.1.1.1.m1.1"><semantics id="Ch2.T3.1.1.1.m1.1a"><mo id="Ch2.T3.1.1.1.m1.1.1" stretchy="false" xref="Ch2.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T3.1.1.1.m1.1b"><ci id="Ch2.T3.1.1.1.m1.1.1.cmml" xref="Ch2.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T3.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T3.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch2.T3.2.2.2.m1.1"><semantics id="Ch2.T3.2.2.2.m1.1a"><mo id="Ch2.T3.2.2.2.m1.1.1" stretchy="false" xref="Ch2.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch2.T3.2.2.2.m1.1b"><ci id="Ch2.T3.2.2.2.m1.1.1.cmml" xref="Ch2.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T3.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T3.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T3.3.3.3.m1.1"><semantics id="Ch2.T3.3.3.3.m1.1a"><mo id="Ch2.T3.3.3.3.m1.1.1" stretchy="false" xref="Ch2.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T3.3.3.3.m1.1b"><ci id="Ch2.T3.3.3.3.m1.1.1.cmml" xref="Ch2.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.3.5.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.3.3.6">
<table class="ltx_tabular ltx_align_middle" id="Ch2.T3.3.3.6.1">
<tr class="ltx_tr" id="Ch2.T3.3.3.6.1.1">
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.3.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.3.6.1.1.1.1">Runtime</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T3.3.3.6.1.2">
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.3.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.3.6.1.2.1.1">(hours)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.3.7.1">kgCO<sub class="ltx_sub" id="Ch2.T3.3.3.7.1.1">2</sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T3.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.1">pa-rnn-base</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.2">40.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.3">0.47</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.4">0.63</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.5">60k</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.6">2.13</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.3.4.1.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T3.3.5.2">
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.1">pa-rnn-bpe8k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.2">41.5</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.3">0.46</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.4">0.64</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.5">110k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.6">4.16</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.5.2.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T3.3.6.3">
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.1">pa-rnn-bpe16k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.2">41.5</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.3">0.46</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.4">0.64</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.5">105k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.6">3.78</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.6.3.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T3.3.7.4">
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.1">pa-rnn-bpe32k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.2">41.9</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.3">0.47</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.4">0.64</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.5">100k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.6">2.88</td>
<td class="ltx_td ltx_align_left" id="Ch2.T3.3.7.4.7">0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T3.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.1">pa-rnn-unigram</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.2"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.8.5.2.1">41.9</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.3"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.8.5.3.1">0.46</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="Ch2.T3.3.8.5.4.1">0.64</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.5">95k</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.6">2.75</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T3.3.8.5.7">0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T3.5.1.1" style="font-size:90%;">Table 2.3</span>: </span><span class="ltx_text" id="Ch2.T3.6.2" style="font-size:90%;">RNN performance on PA dataset of 88k lines</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx1.p3">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx1.p3.1">There are small improvements in BLEU scores when the RNN baseline is compared with models using a BPE submodel of either 8k, 16k or 32k words, as illustrated in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T2" title="Table 2.2 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T3" title="Table 2.3 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.3</span></a>. The maximum BLEU score improvement of 1.5 points (2.5%) is quite modest in the case of the public admin corpus. However, there are larger gains with the DGT corpus. A baseline RNN model, trained on DGT, achieved a BLEU score of 52.7 whereas the highest-performing BPE variant, using a 16k vocab, recorded an improvement of nearly 3 points with a score of 55.6.</p>
</div>
<figure class="ltx_table" id="Ch2.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.3.4.1">Architecture</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T4.1.1.1.m1.1"><semantics id="Ch2.T4.1.1.1.m1.1a"><mo id="Ch2.T4.1.1.1.m1.1.1" stretchy="false" xref="Ch2.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T4.1.1.1.m1.1b"><ci id="Ch2.T4.1.1.1.m1.1.1.cmml" xref="Ch2.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T4.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T4.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch2.T4.2.2.2.m1.1"><semantics id="Ch2.T4.2.2.2.m1.1a"><mo id="Ch2.T4.2.2.2.m1.1.1" stretchy="false" xref="Ch2.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch2.T4.2.2.2.m1.1b"><ci id="Ch2.T4.2.2.2.m1.1.1.cmml" xref="Ch2.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T4.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T4.3.3.3.m1.1"><semantics id="Ch2.T4.3.3.3.m1.1a"><mo id="Ch2.T4.3.3.3.m1.1.1" stretchy="false" xref="Ch2.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T4.3.3.3.m1.1b"><ci id="Ch2.T4.3.3.3.m1.1.1.cmml" xref="Ch2.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T4.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T4.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.3.5.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.3.3.6">
<table class="ltx_tabular ltx_align_middle" id="Ch2.T4.3.3.6.1">
<tr class="ltx_tr" id="Ch2.T4.3.3.6.1.1">
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.3.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.3.6.1.1.1.1">Runtime</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.3.3.6.1.2">
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.3.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.3.6.1.2.1.1">(hours)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T4.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.3.7.1">kgCO<sub class="ltx_sub" id="Ch2.T4.3.3.7.1.1">2</sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T4.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.1">dgt-trans-base</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.2">53.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.3">0.41</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.4">0.72</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.5">55k</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.6">14.43</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.3.4.1.7">0.81</td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.3.5.2">
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.1">dgt-trans-bpe8k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.2">59.5</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.3">0.34</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.4">0.77</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.5">200k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.6">24.48</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.5.2.7">1.38</td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.3.6.3">
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.1">dgt-trans-bpe16k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.2"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.6.3.2.1">60.5</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.3"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.6.3.3.1">0.33</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T4.3.6.3.4.1">0.78</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.5">180k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.6">26.90</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.6.3.7">1.52</td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.3.7.4">
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.1">dgt-trans-bpe32k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.2">59.3</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.3">0.35</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.4">0.77</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.5">100k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.6">18.03</td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.3.7.4.7">1.02</td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.1">dgt-trans-unigram</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.2">59.3</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.3">0.35</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.4">0.77</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.5">125k</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.6">21.95</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T4.3.8.5.7">1.24</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T4.5.1.1" style="font-size:90%;">Table 2.4</span>: </span><span class="ltx_text" id="Ch2.T4.6.2" style="font-size:90%;">Transformer performance on 52k DGT dataset. The highest-performing model uses 2 attention heads. All other models use 8 attention heads.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch2.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.3.4.1">Architecture</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T5.1.1.1.m1.1"><semantics id="Ch2.T5.1.1.1.m1.1a"><mo id="Ch2.T5.1.1.1.m1.1.1" stretchy="false" xref="Ch2.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T5.1.1.1.m1.1b"><ci id="Ch2.T5.1.1.1.m1.1.1.cmml" xref="Ch2.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T5.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T5.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch2.T5.2.2.2.m1.1"><semantics id="Ch2.T5.2.2.2.m1.1a"><mo id="Ch2.T5.2.2.2.m1.1.1" stretchy="false" xref="Ch2.T5.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch2.T5.2.2.2.m1.1b"><ci id="Ch2.T5.2.2.2.m1.1.1.cmml" xref="Ch2.T5.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T5.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T5.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T5.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T5.3.3.3.m1.1"><semantics id="Ch2.T5.3.3.3.m1.1a"><mo id="Ch2.T5.3.3.3.m1.1.1" stretchy="false" xref="Ch2.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T5.3.3.3.m1.1b"><ci id="Ch2.T5.3.3.3.m1.1.1.cmml" xref="Ch2.T5.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T5.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T5.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.3.5.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.3.3.6">
<table class="ltx_tabular ltx_align_middle" id="Ch2.T5.3.3.6.1">
<tr class="ltx_tr" id="Ch2.T5.3.3.6.1.1">
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.3.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.3.6.1.1.1.1">Runtime</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.3.3.6.1.2">
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.3.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.3.6.1.2.1.1">(hours)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T5.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.3.7.1">kgCO<sub class="ltx_sub" id="Ch2.T5.3.3.7.1.1">2</sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T5.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.1">pa-trans-base</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.2">44.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.3">0.44</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.4">0.66</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.5">20k</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.6">5.97</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.3.4.1.7">0.34</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.3.5.2">
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.1">pa-trans-bpe8k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.2">46.6</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.3"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.5.2.3.1">0.40</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.4">0.68</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.5">160k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.6">20.1</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.5.2.7">1.13</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.3.6.3">
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.1">pa-trans-bpe16k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.2"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.6.3.2.1">47.1</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.3">0.41</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T5.3.6.3.4.1">0.68</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.5">100k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.6">14.22</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.6.3.7">0.80</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.3.7.4">
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.1">pa-trans-bpe32k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.2">46.8</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.3">0.41</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.4">0.68</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.5">70k</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.6">12.7</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.3.7.4.7">0.72</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.1">pa-trans-unigram</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.2">46.6</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.3">0.42</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.4">0.68</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.5">75k</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.6">13.34</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch2.T5.3.8.5.7">0.75</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T5.5.1.1" style="font-size:90%;">Table 2.5</span>: </span><span class="ltx_text" id="Ch2.T5.6.2" style="font-size:90%;">Transformer performance on 88k PA dataset. All models use 8 attention heads.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx1.p4">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx1.p4.1">In the context of Transformer architectures, highlighted in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T4" title="Table 2.4 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.4</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T5" title="Table 2.5 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.5</span></a>, the use of subword models delivers significant performance improvements for both the DGT and public admin corpora. The performance gains for Transformer models are far greater than RNN models. Baseline DGT Transformer models achieve a BLEU score of 53.4 while a Transformer model, with a 16k BPE submodel, has a score of 60.5 representing a BLEU score improvement of 13% at 7.1 BLEU points.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx1.p5">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx1.p5.1">For translating into a morphologically rich language, such as Irish, the ChrF metric has proven successful in showing a strong correlation with human translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx106" title="">106</a>]</cite>. In the context of our experiments, it worked well in highlighting the performance differences between RNN and Transformer architectures.</p>
</div>
<figure class="ltx_figure" id="Ch2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch2.F2.g1" src="bleu_all.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F2.2.1.1" style="font-size:90%;">Figure 2.2</span>: </span><span class="ltx_text" id="Ch2.F2.3.2" style="font-size:90%;">BLEU performance for all model architectures</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch2.S6.SS2.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Transformer performance compared with RNN</h5>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx2.p1">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx2.p1.1">The performance of RNN models is contrasted with the Transformer approach in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F2" title="Figure 2.2 ‣ Performance of subword models ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.2</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F3" title="Figure 2.3 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.3</span></a>. Transformer models, as anticipated, outperform all their RNN counterparts. It is interesting to note the impact of choosing the optimal vocabulary size for BPE submodels. Both datasets demonstrate that choosing a BPE vocabulary of 16k words yields the highest performance.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS2.SSSx2.p2">
<p class="ltx_p" id="Ch2.S6.SS2.SSSx2.p2.1">Furthermore, the TER scores highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F3" title="Figure 2.3 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.3</span></a> reinforce the findings that using 16k BPE submodels on Transformer architectures leads to better translation performance. The TER score for the DGT Transformer 16k BPE model is significantly better (0.33) when compared with the baseline performance (0.41).</p>
</div>
<figure class="ltx_figure" id="Ch2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="283" id="Ch2.F3.g1" src="extracted/5444776/Images/ter_all_isfeidirlinn.png" width="471"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F3.2.1.1" style="font-size:90%;">Figure 2.3</span>: </span><span class="ltx_text" id="Ch2.F3.3.2" style="font-size:90%;">TER performance for all model architectures</span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch2.F4.g1" src="dgt-trans-base.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F4.2.1.1" style="font-size:90%;">Figure 2.4</span>: </span><span class="ltx_text" id="Ch2.F4.3.2" style="font-size:90%;">Training DGT Transformer baseline</span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch2.F5.g1" src="dgt-trans-16kbpe.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F5.2.1.1" style="font-size:90%;">Figure 2.5</span>: </span><span class="ltx_text" id="Ch2.F5.3.2" style="font-size:90%;">Training DGT Transformer 16k BPE</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="Ch2.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.7 </span>Environmental Impact</h3>
<div class="ltx_para" id="Ch2.S7.p1">
<p class="ltx_p" id="Ch2.S7.p1.1">Motivated by the findings of Stochastic Parrots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx15" title="">15</a>]</cite>, energy consumption during model development was tracked. Prototype model development used Colab Pro, which as part of Google Cloud is carbon neutral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>. However, longer running Transformer experiments were conducted on local servers using 324 gCO<sub class="ltx_sub" id="Ch2.S7.p1.1.1">2</sub> per kWh<span class="ltx_note ltx_role_footnote" id="Ch2.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx100" title="">100</a>]</cite>. The net result was just under 10 kgCO<sub class="ltx_sub" id="Ch2.S7.p1.1.2">2</sub> created for a full run of model development. Models developed during this study will be reused for ensemble experiments in future work.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.8 </span>Discussion</h3>
<div class="ltx_para" id="Ch2.S8.p1">
<p class="ltx_p" id="Ch2.S8.p1.1">Validation accuracy, and model perplexity, in developing the baseline and optimal models for the DGT corpus are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F4" title="Figure 2.4 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.4</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F5" title="Figure 2.5 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.5</span></a>. Rapid convergence was observed while training the baseline model such that little accuracy improvement occurred after 20k steps. Including a subword model led to much slower convergence and there were only marginal gains after 60k steps. Furthermore, it is observed that training the DGT model, with a 16k BPE submodel, boosted validation accuracy by over 8% compared with its baseline.</p>
</div>
<div class="ltx_para" id="Ch2.S8.p2">
<p class="ltx_p" id="Ch2.S8.p2.1">With regard to the key metric of perplexity, it is shown to rise after training for 15k steps in the baseline models. PPL was observed to rise at later stages, typically after 40k steps in models developed using subword models. Perplexity (PPL), shows how many different, equally probable words can be produced during translation. As a metric for translation performance, it is important to keep low scores so the number of alternative translations is reduced. Therefore, for future model development, it may be worthwhile to set PPL as an early stopping hyperparameter.</p>
</div>
<div class="ltx_para" id="Ch2.S8.p3">
<p class="ltx_p" id="Ch2.S8.p3.1">On examining the PPL graphs of Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F4" title="Figure 2.4 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.4</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.F5" title="Figure 2.5 ‣ Transformer performance compared with RNN ‣ 2.6.2 Results ‣ 2.6 Empirical Evaluation ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.5</span></a>, it is clear that a lower global minimum is achieved when the Transformer approach is used with a 16k BPE submodel. The PPL global minimum (2.7) is over 50% lower than the corresponding PPL for the Transformer base model (5.5). Such a finding illustrates that choosing an optimal submodel delivers significant performance gains.</p>
</div>
<div class="ltx_para" id="Ch2.S8.p4">
<p class="ltx_p" id="Ch2.S8.p4.1">Translation engine performance was benchmarked against Google Translate’s <span class="ltx_note ltx_role_footnote" id="Ch2.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://translate.google.com/</span></span></span> EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch2.S8.p4.1.m1.1"><semantics id="Ch2.S8.p4.1.m1.1a"><mo id="Ch2.S8.p4.1.m1.1.1" stretchy="false" xref="Ch2.S8.p4.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch2.S8.p4.1.m1.1b"><ci id="Ch2.S8.p4.1.m1.1.1.cmml" xref="Ch2.S8.p4.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S8.p4.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S8.p4.1.m1.1d">↔</annotation></semantics></math>GA translation service which is freely available on the internet. Four random samples were selected from the English source test file and are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T6" title="Table 2.6 ‣ 2.8 Discussion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.6</span></a>. Translation of these samples was carried out on the optimal DGT Transformer model and using Google Translate. Case insensitive, sentence level BLEU scores were recorded and are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T7" title="Table 2.7 ‣ 2.8 Discussion ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.7</span></a>. The results are encouraging and indicate well-performing translation models on the DGT dataset.</p>
</div>
<div class="ltx_para" id="Ch2.S8.p5">
<p class="ltx_p" id="Ch2.S8.p5.1">The optimal hyperparameters selected in this discovery process are identified in bold in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T1" title="Table 2.1 ‣ Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.1</span></a>. A higher initial learning rate of 2 coupled with an average decay of 0.0001 led to longer training times but more accurate models. Despite setting an early stopping hyperparameter, many of the Transformer builds continued for the full cycle of 200k steps over periods of 20+ hours.</p>
</div>
<div class="ltx_para" id="Ch2.S8.p6">
<p class="ltx_p" id="Ch2.S8.p6.1">Training transformer models with a reduced number of attention heads led to a marginal improvement in translation accuracy with a smaller corpus. Our best-performing model on a 55k DGT corpus, with 2 heads and a 16k BPE submodel, achieved a BLEU score of 60.5 and a TER score of 0.33. By comparison, using 8 heads with the same architecture and dataset yielded 60.3 for the BLEU and 0.34 for the TER. In the case of a larger 88k PA corpus, all transformer models using 8 heads performed better than equivalent models using just 2 heads.</p>
</div>
<figure class="ltx_table" id="Ch2.T6">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch2.T6.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T6.2.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch2.T6.2.1.1.1" style="width:196.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch2.T6.2.1.1.1.1">Source Language (English)</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch2.T6.2.1.1.2" style="width:196.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch2.T6.2.1.1.2.1">Reference Human Translation (Irish)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T6.2.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T6.2.2.1.1" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.2.1.1.1">A clear harmonised procedure, including the necessary criteria for disease–free status, should be established for that purpose.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T6.2.2.1.2" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.2.1.2.1">Ba cheart nós imeachta comhchuibhithe soiléir, lena n-áirítear na critéir is gá do stádas saor ó ghalar, a bhunú chun na críche sin.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T6.2.3.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T6.2.3.2.1" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.3.2.1.1">the mark is applied anew, as appropriate.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T6.2.3.2.2" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.3.2.2.1">déanfar an mharcáil arís, mar is iomchuí.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T6.2.4.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T6.2.4.3.1" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.4.3.1.1">If the court decides that a review is justified on any of the grounds set out in paragraph 1, the judgment given in the European Small Claims Procedure shall be null and void.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T6.2.4.3.2" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.4.3.2.1">Má chinneann an chúirt go bhfuil bonn cirt le hathbhreithniú de bharr aon cheann de na forais a leagtar amach i mír 1, beidh an breithiúnas a tugadh sa Nós Imeachta Eorpach um Éilimh Bheaga ar neamhní go hiomlán.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T6.2.5.4">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch2.T6.2.5.4.1" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.5.4.1.1">households where pet animals are kept;</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch2.T6.2.5.4.2" style="width:196.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T6.2.5.4.2.1">teaghlaigh ina gcoimeádtar peataí;</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T6.3.1.1" style="font-size:90%;">Table 2.6</span>: </span><span class="ltx_text" id="Ch2.T6.4.2" style="font-size:90%;">Samples of human reference translations</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch2.T7">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch2.T7.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T7.2.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch2.T7.2.2.3" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch2.T7.2.2.3.1">Transformer (16 kBPE)</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch2.T7.1.1.1" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T7.1.1.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T7.1.1.1.1.1.m1.1"><semantics id="Ch2.T7.1.1.1.1.1.m1.1a"><mo id="Ch2.T7.1.1.1.1.1.m1.1.1" stretchy="false" xref="Ch2.T7.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T7.1.1.1.1.1.m1.1b"><ci id="Ch2.T7.1.1.1.1.1.m1.1.1.cmml" xref="Ch2.T7.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T7.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T7.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></p>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch2.T7.2.2.4" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch2.T7.2.2.4.1">Google Translate</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch2.T7.2.2.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T7.2.2.2.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch2.T7.2.2.2.1.1.m1.1"><semantics id="Ch2.T7.2.2.2.1.1.m1.1a"><mo id="Ch2.T7.2.2.2.1.1.m1.1.1" stretchy="false" xref="Ch2.T7.2.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch2.T7.2.2.2.1.1.m1.1b"><ci id="Ch2.T7.2.2.2.1.1.m1.1.1.cmml" xref="Ch2.T7.2.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.T7.2.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.T7.2.2.2.1.1.m1.1d">↑</annotation></semantics></math></p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T7.2.3.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.3.1.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.3.1.1.1">Ba cheart nós imeachta soiléir comhchuibhithe, lena n-áirítear na critéir is gá maidir le stádas saor ó ghalair, a bhunú chun na críche sin.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.3.1.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.3.1.2.1">61.6</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.3.1.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.3.1.3.1">Ba cheart nós imeachta comhchuibhithe soiléir, lena n-áirítear na critéir riachtanacha maidir le stádas saor ó ghalair, a bhunú chun na críche sin.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.3.1.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.3.1.4.1">70.2</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T7.2.4.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.4.2.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.4.2.1.1">go gcuirtear an marc i bhfeidhme, de réir mar is iomchuí.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.4.2.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.4.2.2.1">21.4</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.4.2.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.4.2.3.1">cuirtear an marc i bhfeidhm as an nua, de réir mar is cuí.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.4.2.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.4.2.4.1">6.6</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T7.2.5.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.5.3.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.5.3.1.1">Má chinneann an chúirt go bhfuil bonn cirt le hathbhreithniú ar aon cheann de na forais a leagtar amach i mír 1, beidh an breithiúnas a thugtar sa Nós Imeachta Eorpach um Éilimh Bheaga ar neamhní.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.5.3.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.5.3.2.1">77.3</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.5.3.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.5.3.3.1">Má chinneann an chúirt go bhfuil údar le hathbhreithniú ar aon cheann de na forais atá leagtha amach i mír 1, beidh an breithiúnas a thugtar sa Nós Imeachta Eorpach um Éilimh Bheaga ar neamhní</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch2.T7.2.5.3.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.5.3.4.1">59.1</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch2.T7.2.6.4">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch2.T7.2.6.4.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.6.4.1.1">teaghlaigh ina gcoimeádtar peataí;</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch2.T7.2.6.4.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.6.4.2.1">100</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch2.T7.2.6.4.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.6.4.3.1">teaghlaigh ina gcoinnítear peataí;</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch2.T7.2.6.4.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T7.2.6.4.4.1">30.2</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch2.T7.4.1.1" style="font-size:90%;">Table 2.7</span>: </span><span class="ltx_text" id="Ch2.T7.5.2" style="font-size:90%;">Transformer model compared with Google Translate using random samples from the DGT corpus. Full evaluation of Google Translate on the DGT test set, with 1.3k lines, generated a BLEU score of 46.3 and a TER score of 0.44. Comparative scores on the test set using our Transformer model, with 2 attention heads and 16k BPE submodel realised 60.5 for BLEU and 0.33 for TER.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S8.p7">
<p class="ltx_p" id="Ch2.S8.p7.1">Standard Transformer hyperparameters for batch size (2048) and the number of encoder and decoder layers (6) were both observed to perform well on the DGT and PA corpora. Reducing hidden neurons to 256 and increasing regularisation dropout to 0.3 improved translation performance and were chosen when building all Transformer models.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S9">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.9 </span>Conclusion</h3>
<div class="ltx_para" id="Ch2.S9.p1">
<p class="ltx_p" id="Ch2.S9.p1.1">In our paper, we demonstrated that a random search approach to hyperparameter optimisation leads to the development of high-performing translation models.</p>
</div>
<div class="ltx_para" id="Ch2.S9.p2">
<p class="ltx_p" id="Ch2.S9.p2.1">We have shown that choosing subword models, in our low-resource scenarios, is an important driver for the performance of MT engines. Moreover, the choice of vocabulary size leads to varying degrees of performance. Within the context of low-resource EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch2.S9.p2.1.m1.1"><semantics id="Ch2.S9.p2.1.m1.1a"><mo id="Ch2.S9.p2.1.m1.1.1" stretchy="false" xref="Ch2.S9.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch2.S9.p2.1.m1.1b"><ci id="Ch2.S9.p2.1.m1.1.1.cmml" xref="Ch2.S9.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S9.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch2.S9.p2.1.m1.1d">→</annotation></semantics></math>GA translation, we achieved optimal performance, on a 55k generic corpus and an 88k in-domain corpus, when a Transformer architecture with a 16k BPE submodel was used.
The importance of selecting hyperparameters in training low-resource Transformer models was also demonstrated. By reducing the number of hidden layer neurons and increasing dropout, our models performed significantly better than baseline models and Google Translate.</p>
</div>
<div class="ltx_para" id="Ch2.S9.p3">
<p class="ltx_p" id="Ch2.S9.p3.1">Performance improvement of our optimised Transformer models, with subword segmentation, was observed across all key indicators namely a higher validation accuracy, a PPL achieved at a lower global minimum, a lower post-editing effort and a higher translation accuracy.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch3">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 3 </span>MT in the Covid domain: Shared Task for LoResMT2021</h2>
<section class="ltx_section" id="Ch3.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.1 </span>Context</h3>
<figure class="ltx_figure" id="Ch3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch3.F1.g1" src="mtsummit2023-adaptnmt-enga.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch3.F1.2.1.1" style="font-size:90%;">Figure 3.1</span>: </span><span class="ltx_text" id="Ch3.F1.3.2" style="font-size:90%;">Proposed approach for LoResMT2021 Shared Task</span></figcaption>
</figure>
<div class="ltx_para" id="Ch3.S1.p1">
<p class="ltx_p" id="Ch3.S1.p1.1">In refining the Transformer model with a custom dataset, we investigated the research objectives outlined in RQ1 and RQ2. A significant contribution of this study was showcasing that hyperparameters specified in <span class="ltx_text ltx_font_italic" id="Ch3.S1.p1.1.1">“Transformers for Low-Resource Languages - is féidir linn!”</span> could be employed to excel in a shared task targeting low-resource languages. Therefore, the hyperparameters identified in the HPO experiments of Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch2.T1" title="Table 2.1 ‣ Byte Pair Encoding compared with Unigram ‣ 2.4.3 Subword Models ‣ 2.4 Background ‣ Chapter 2 Transformers for Low-Resource Languages"><span class="ltx_text ltx_ref_tag">2.1</span></a> were employed. Another key contribution was to demonstrate the notable improvement in translation quality when the shared task dataset was augmented by a compact bespoke in-domain dataset.</p>
</div>
<div class="ltx_para" id="Ch3.S1.p2">
<p class="ltx_p" id="Ch3.S1.p2.1">The approach I adopted in developing the model for the LoResMT2021 Shared Task is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F1" title="Figure 3.1 ‣ 3.1 Context ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.1</span></a>. As the winning entry in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S1.p2.1.m1.1"><semantics id="Ch3.S1.p2.1.m1.1a"><mo id="Ch3.S1.p2.1.m1.1.1" stretchy="false" xref="Ch3.S1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S1.p2.1.m1.1b"><ci id="Ch3.S1.p2.1.m1.1.1.cmml" xref="Ch3.S1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S1.p2.1.m1.1d">→</annotation></semantics></math>GA direction, the system achieved 36.0 BLEU, 0.531 TER and 0.6 ChrF. By combining the baseline LoResMT2021 5k line dataset with a bespoke 8k line Covid dataset, an amalgamated Covid-specific corpus of 13k lines was created. Using this dataset, a Transformer model was trained from scratch. Our system won by a wide margin with the second-placed team achieving a 25.8 BLEU.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch3.S1.p3">
<p class="ltx_p" id="Ch3.S1.p3.2"><span class="ltx_text ltx_font_bold" id="Ch3.S1.p3.2.1">Machine Translation in the Covid domain: an English-Irish case study - LoResMT2021</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch3.S1.p3.3"><span class="ltx_text ltx_font_bold" id="Ch3.S1.p3.3.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch3.S1.p3.4"><span class="ltx_text ltx_font_bold" id="Ch3.S1.p3.4.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch3.S1.p3.5"><span class="ltx_text ltx_font_bold" id="Ch3.S1.p3.5.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch3.S1.p3.6">Proceedings of the 4th Workshop on Technologies for</p>
<p class="ltx_p" id="Ch3.S1.p3.7">MT of Low-Resource Languages (LoResMT2021)</p>
<p class="ltx_p" id="Ch3.S1.p3.8">Virtual: Association for Machine Translation in the Americas</p>
<p class="ltx_p" id="Ch3.S1.p3.9">August 16 - 20, 2021</p>
<p class="ltx_p" id="Ch3.S1.p3.10">Florida, USA</p>
<p class="ltx_p" id="Ch3.S1.p3.1">1st place in EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S1.p3.1.m1.1"><semantics id="Ch3.S1.p3.1.m1.1a"><mo id="Ch3.S1.p3.1.m1.1.1" stretchy="false" xref="Ch3.S1.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S1.p3.1.m1.1b"><ci id="Ch3.S1.p3.1.m1.1.1.cmml" xref="Ch3.S1.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S1.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S1.p3.1.m1.1d">→</annotation></semantics></math>GA Shared Task</p>
<p class="ltx_p" id="Ch3.S1.p3.11">ADAPT Centre</p>
<p class="ltx_p" id="Ch3.S1.p3.12">Dublin City University</p>
<p class="ltx_p" id="Ch3.S1.p3.13">Ireland</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch3.S1.p3.14"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-loresmt.15.pdf" title="">https://aclanthology.org/2021.mtsummit-loresmt.15.pdf</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch3.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch3.S2.p1">
<p class="ltx_p" id="Ch3.S2.p1.1">Translation models for the specific domain of translating Covid data from English to Irish were developed. Domain adaptation techniques, using a Covid-adapted generic 55k corpus from the Directorate General of Translation, were applied. Fine-tuning, mixed fine-tuning and combined dataset approaches were compared with models trained on an extended in-domain dataset. As part of this study, an English-Irish dataset of Covid related data, from the Health and Education domains, was developed. The highest-performing model used a Transformer architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.3 </span>Introduction</h3>
<div class="ltx_para" id="Ch3.S3.p1">
<p class="ltx_p" id="Ch3.S3.p1.1">Neural machine translation (NMT) has routinely outperformed statistical machine translation (SMT) when large parallel datasets are available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx119" title="">119</a>]</cite>. Furthermore, Transformer-based approaches have demonstrated impressive results in moderate low-resource scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>.
NMT involving Transformer model development will improve the performance in specific domains of low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx5" title="">5</a>]</cite>. However, the benefits of NMT are less clear when using very low-resource machine translation (MT) on in-domain datasets of less than 10k lines.
</p>
</div>
<div class="ltx_para" id="Ch3.S3.p2">
<p class="ltx_p" id="Ch3.S3.p2.1">The Irish language is a primary example of a low-resource language that will benefit from such research. This paper reports the results of the MT system developed for the English–to-Irish (EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S3.p2.1.m1.1"><semantics id="Ch3.S3.p2.1.m1.1a"><mo id="Ch3.S3.p2.1.m1.1.1" stretchy="false" xref="Ch3.S3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S3.p2.1.m1.1b"><ci id="Ch3.S3.p2.1.m1.1.1.cmml" xref="Ch3.S3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S3.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S3.p2.1.m1.1d">→</annotation></semantics></math>GA) Shared Task at LoResMT2021 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite>. Relevant work is presented in the background section followed by an overview of the proposed approach. The empirical findings are outlined in the results section. Finally, the key findings are presented and discussed.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.4 </span>Background</h3>
<section class="ltx_subsection" id="Ch3.S4.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.1 </span>Transformer</h4>
<div class="ltx_para" id="Ch3.S4.SS1.p1">
<p class="ltx_p" id="Ch3.S4.SS1.p1.1">A novel architecture called Transformer was introduced in the paper ‘Attention Is All You Need’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite>. Transformer is an architecture for transforming one sequence into another with the help of an encoder and decoder without relying on recurrent neural networks (RNNs).</p>
</div>
<div class="ltx_para" id="Ch3.S4.SS1.p2">
<p class="ltx_p" id="Ch3.S4.SS1.p2.1">Transformer models use attention to focus on previously generated tokens. This approach allows models to develop a long memory which is particularly useful in the domain of language translation.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch3.S4.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.2 </span>Domain Adaptation</h4>
<div class="ltx_para" id="Ch3.S4.SS2.p1">
<p class="ltx_p" id="Ch3.S4.SS2.p1.1">Domain adaptation is a proven approach to addressing the paucity of data in low-resource settings. Fine-tuning an out-of-domain model by further training with in-domain data is effective in improving the performance of translation models (Freitag and Al-Onaizan, 2016; Sennrich et al., 2016). With this approach, an NMT model is initially trained using a large out-of-domain corpus. Once fully converged, the out-of-domain model is further trained by fine-tuning its parameters with a low-resource in-domain corpus.</p>
</div>
<div class="ltx_para" id="Ch3.S4.SS2.p2">
<p class="ltx_p" id="Ch3.S4.SS2.p2.1">A modification to this approach is known as mixed fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx29" title="">29</a>]</cite>. With this technique, an NMT model is trained on out-of-domain data until fully converged. This serves as a base model which is further trained using the combined in-domain and out-of-domain datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch3.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.5 </span>Proposed Approach</h3>
<figure class="ltx_figure" id="Ch3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch3.F2.g1" src="is_feidir_linn_workshop.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch3.F2.2.1.1" style="font-size:90%;">Figure 3.2</span>: </span><span class="ltx_text" id="Ch3.F2.3.2" style="font-size:90%;">Proposed approach of MT in the Covid domain. Optimal hyperparameters are applied to Transformer models which are trained using one of several possible approaches. The training dataset composition is determined by the chosen approach. Models are subsequently evaluated using a suite of metrics.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch3.S5.p1">
<p class="ltx_p" id="Ch3.S5.p1.1">Hyperparameter optimisation of RNN models in low-resource settings has previously demonstrated considerable performance improvements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx104" title="">104</a>]</cite>. The extent to which such optimisation techniques may be applied to Transformer models in similar low-resource scenarios was evaluated in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>. Evaluations included modifying the number of attention heads, the number of layers and experimenting with regularisation techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and the vocabulary size were evaluated.</p>
</div>
<div class="ltx_para" id="Ch3.S5.p2">
<p class="ltx_p" id="Ch3.S5.p2.1">To test the effectiveness of our approaches, models were trained using three EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S5.p2.1.m1.1"><semantics id="Ch3.S5.p2.1.m1.1a"><mo id="Ch3.S5.p2.1.m1.1.1" stretchy="false" xref="Ch3.S5.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S5.p2.1.m1.1b"><ci id="Ch3.S5.p2.1.m1.1.1.cmml" xref="Ch3.S5.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S5.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S5.p2.1.m1.1d">→</annotation></semantics></math>GA parallel datasets: a general corpus of 52k lines from the Directorate General for Translation (DGT) and two in-domain corpora of Covid data (8k and 5k lines). All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite> subword model. The impact of using separate source and target subword models was not explored.</p>
</div>
<div class="ltx_para" id="Ch3.S5.p3">
<p class="ltx_p" id="Ch3.S5.p3.1">The approach adopted is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F2" title="Figure 3.2 ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.2</span></a> and the datasets used in evaluating this approach are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T1" title="Table 3.1 ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.1</span></a>. All models were developed using a Transformer architecture.</p>
</div>
<figure class="ltx_table" id="Ch3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch3.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch3.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.1.1.1.1">Approach</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.1.1.2.1">Source</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.1.1.3.1">Lines</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch3.T1.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.2.2.1.1">Covid baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.2.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.2.2.1.3">8k</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.3.2">
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.3.2.1">Covid extended</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.3.2.2">Baseline + Covid_DCU</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.3.2.3">13k</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.4.3">
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.4.3.1">Out-of-domain</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.4.3.2">DGT</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.4.3.3">52k</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.5.4">
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.5.4.1">Fine-tuned</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.5.4.2">Baseline + Covid_DCU + DGT</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.5.4.3">65k</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.6.5">
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.6.5.1">Mixed fine-tuned</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.6.5.2">Baseline + Covid_DCU + DGT</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.2.6.5.3">65k</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.7.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch3.T1.2.7.6.1">Combined domains</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch3.T1.2.7.6.2">Baseline + Covid_DCU + DGT</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch3.T1.2.7.6.3">65k</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch3.T1.3.1.1" style="font-size:90%;">Table 3.1</span>: </span><span class="ltx_text" id="Ch3.T1.4.2" style="font-size:90%;">Datasets used in proposed approach for MT in Covid domain</span></figcaption>
</figure>
<section class="ltx_subsection" id="Ch3.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5.1 </span>Architecture Tuning</h4>
<div class="ltx_para" id="Ch3.S5.SS1.p1">
<p class="ltx_p" id="Ch3.S5.SS1.p1.1">Long training times associated with NMT make it costly to tune systems using conventional grid search approaches. A previous study identified the hyperparameters required for optimal performance  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>. Reducing the number of hidden layer neurons and increasing dropout led to significantly better performance. Furthermore, within the context of low-resource EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S5.SS1.p1.1.m1.1"><semantics id="Ch3.S5.SS1.p1.1.m1.1a"><mo id="Ch3.S5.SS1.p1.1.m1.1.1" stretchy="false" xref="Ch3.S5.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S5.SS1.p1.1.m1.1b"><ci id="Ch3.S5.SS1.p1.1.m1.1.1.cmml" xref="Ch3.S5.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S5.SS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S5.SS1.p1.1.m1.1d">→</annotation></semantics></math>GA translation, using a 16k BPE submodel resulted in the highest-performing models. The Transformer hyperparameters, chosen in line with these findings, are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T2" title="Table 3.2 ‣ 3.5.1 Architecture Tuning ‣ 3.5 Proposed Approach ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<figure class="ltx_table ltx_align_center" id="Ch3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch3.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch3.T2.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch3.T2.2.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch3.T2.2.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.2.2.1">Learning rate</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.2.2.2">0.1, 0.01, 0.001, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.2.2.2.1">2</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.3.3.1">Batch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.3.3.2">1024, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.3.3.2.1">2048</span>, 4096, 8192</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.4.4.1">Attention heads</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.4.4.2">
<span class="ltx_text ltx_font_bold" id="Ch3.T2.2.4.4.2.1">2</span>, 4, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.4.4.2.2">8</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.5.5.1">Number of layers</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.5.5.2">5, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.5.5.2.1">6</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.6.6.1">Feed-forward dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.6.6.2"><span class="ltx_text ltx_font_bold" id="Ch3.T2.2.6.6.2.1">2048</span></td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.7.7.1">Embedding dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.7.7.2">128, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.7.7.2.1">256</span>, 512</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.8.8.1">Label smoothing</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.8.8.2">
<span class="ltx_text ltx_font_bold" id="Ch3.T2.2.8.8.2.1">0.1</span>, 0.3</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.9.9.1">Dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.9.9.2">0.1, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.9.9.2.1">0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.2.10.10.1">Attention dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T2.2.10.10.2"><span class="ltx_text ltx_font_bold" id="Ch3.T2.2.10.10.2.1">0.1</span></td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.2.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch3.T2.2.11.11.1">Average Decay</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch3.T2.2.11.11.2">0, <span class="ltx_text ltx_font_bold" id="Ch3.T2.2.11.11.2.1">0.0001</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch3.T2.3.1.1" style="font-size:90%;">Table 3.2</span>: </span><span class="ltx_text" id="Ch3.T2.4.2" style="font-size:90%;">Hyperparameter optimisation for Transformer models. Optimal parameters are highlighted in bold. The highest-performing model trained on the 55k DGT corpus uses 2 attention heads  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Ch3.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.6 </span>Empirical Evaluation</h3>
<section class="ltx_subsection" id="Ch3.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6.1 </span>Experimental Setup</h4>
<section class="ltx_subsubsection" id="Ch3.S6.SS1.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Datasets</h5>
<div class="ltx_para" id="Ch3.S6.SS1.SSSx1.p1">
<p class="ltx_p" id="Ch3.S6.SS1.SSSx1.p1.1">The performance of the Transformer approach is evaluated on EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S6.SS1.SSSx1.p1.1.m1.1"><semantics id="Ch3.S6.SS1.SSSx1.p1.1.m1.1a"><mo id="Ch3.S6.SS1.SSSx1.p1.1.m1.1.1" stretchy="false" xref="Ch3.S6.SS1.SSSx1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S6.SS1.SSSx1.p1.1.m1.1b"><ci id="Ch3.S6.SS1.SSSx1.p1.1.m1.1.1.cmml" xref="Ch3.S6.SS1.SSSx1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S6.SS1.SSSx1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S6.SS1.SSSx1.p1.1.m1.1d">→</annotation></semantics></math>GA parallel datasets in the Covid domain. Three datasets were used in the evaluation of our models. These consisted of a baseline Covid dataset (8k) provided by MT Summit 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite>, an in-domain Covid dataset (5k) developed at DCU and a publicly available out-of-domain dataset (52k) provided by DGT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx107" title="">107</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch3.S6.SS1.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Infrastructure</h5>
<div class="ltx_para" id="Ch3.S6.SS1.SSSx2.p1">
<p class="ltx_p" id="Ch3.S6.SS1.SSSx2.p1.1">Models were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16GB memory, a 256GB SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16GB graphic cards and up to 27GB of memory when available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch3.S6.SS1.SSSx2.p2">
<p class="ltx_p" id="Ch3.S6.SS1.SSSx2.p2.1">Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch3.S6.SS1.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Metrics</h5>
<div class="ltx_para" id="Ch3.S6.SS1.SSSx3.p1">
<p class="ltx_p" id="Ch3.S6.SS1.SSSx3.p1.1">Automated metrics were used to determine the translation quality. All models were trained and evaluated using the BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite> evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch3.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6.2 </span>Results</h4>
<div class="ltx_para" id="Ch3.S6.SS2.p1">
<p class="ltx_p" id="Ch3.S6.SS2.p1.1">Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarised in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T3" title="Table 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.3</span></a> and in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.T4" title="Table 3.4 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.4</span></a>. Clearly, in the context of our low-resource experiments, it can be seen there is little performance difference using Transformer architectures with a differing number of attention heads. The largest difference occurs when using a fine-tuned approach (2.1 BLEU points). However, the difference between a 2-head and an 8-head approach is less than 1 BLEU point for all other models. The highest-performing approach uses the extended Covid dataset (13k) which is a combination of the MT summit Covid baseline and a custom DCU Covid dataset. This Transformer model, with 2 heads, performs well across all key translation metrics (BLEU: 36.0, TER: 0.63 and ChrF3: 0.32).</p>
</div>
<figure class="ltx_figure" id="Ch3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch3.F2.sf1.g1" src="lores_bleu.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch3.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="Ch3.F2.sf1.3.2" style="font-size:90%;">BLEU</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch3.F2.sf2.g1" src="lores_ter.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch3.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="Ch3.F2.sf2.3.2" style="font-size:90%;">TER</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch3.F3.2.1.1" style="font-size:90%;">Figure 3.3</span>: </span><span class="ltx_text" id="Ch3.F3.3.2" style="font-size:90%;">Translation performance using Transformers with 2 heads</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch3.T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch3.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch3.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch3.T3.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch3.T3.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch3.T3.3.3.5">
<span class="ltx_text ltx_font_bold" id="Ch3.T3.3.3.5.1">Heads</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T3.3.3.6">
<span class="ltx_text ltx_font_bold" id="Ch3.T3.3.3.6.1">Lines</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T3.3.3.7">
<span class="ltx_text ltx_font_bold" id="Ch3.T3.3.3.7.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T3.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch3.T3.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch3.T3.1.1.1.m1.1"><semantics id="Ch3.T3.1.1.1.m1.1a"><mo id="Ch3.T3.1.1.1.m1.1.1" stretchy="false" xref="Ch3.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch3.T3.1.1.1.m1.1b"><ci id="Ch3.T3.1.1.1.m1.1.1.cmml" xref="Ch3.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.T3.1.1.1.m1.1d">↑</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T3.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch3.T3.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch3.T3.2.2.2.m1.1"><semantics id="Ch3.T3.2.2.2.m1.1a"><mo id="Ch3.T3.2.2.2.m1.1.1" stretchy="false" xref="Ch3.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch3.T3.2.2.2.m1.1b"><ci id="Ch3.T3.2.2.2.m1.1.1.cmml" xref="Ch3.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.T3.2.2.2.m1.1d">↓</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T3.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch3.T3.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch3.T3.3.3.3.m1.1"><semantics id="Ch3.T3.3.3.3.m1.1a"><mo id="Ch3.T3.3.3.3.m1.1.1" stretchy="false" xref="Ch3.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch3.T3.3.3.3.m1.1b"><ci id="Ch3.T3.3.3.3.m1.1.1.cmml" xref="Ch3.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.T3.3.3.3.m1.1d">↑</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch3.T3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T3.3.4.1.1">Covid baseline</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch3.T3.3.4.1.2">2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T3.3.4.1.3">8k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T3.3.4.1.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T3.3.4.1.5">9.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T3.3.4.1.6">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T3.3.4.1.7">0.32</td>
</tr>
<tr class="ltx_tr" id="Ch3.T3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T3.3.5.2.1">Covid extended</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T3.3.5.2.2">2</th>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.5.2.3">13k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.5.2.4">35k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.5.2.5">36.0</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.5.2.6">0.63</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.5.2.7">0.54</td>
</tr>
<tr class="ltx_tr" id="Ch3.T3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T3.3.6.3.1">Out-of-domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T3.3.6.3.2">2</th>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.6.3.3">52k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.6.3.4">200k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.6.3.5">13.9</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.6.3.6">0.80</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.6.3.7">0.41</td>
</tr>
<tr class="ltx_tr" id="Ch3.T3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T3.3.7.4.1">Fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T3.3.7.4.2">2</th>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.7.4.3">65k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.7.4.4">35k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.7.4.5">22.9</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.7.4.6">0.64</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.7.4.7">0.42</td>
</tr>
<tr class="ltx_tr" id="Ch3.T3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T3.3.8.5.1">Mixed fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T3.3.8.5.2">2</th>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.8.5.3">65k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.8.5.4">35k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.8.5.5">18.2</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.8.5.6">0.71</td>
<td class="ltx_td ltx_align_center" id="Ch3.T3.3.8.5.7">0.42</td>
</tr>
<tr class="ltx_tr" id="Ch3.T3.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch3.T3.3.9.6.1">Combined domains</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="Ch3.T3.3.9.6.2">2</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T3.3.9.6.3">65k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T3.3.9.6.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T3.3.9.6.5">32.2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T3.3.9.6.6">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T3.3.9.6.7">0.55</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch3.T3.5.1.1" style="font-size:90%;">Table 3.3</span>: </span><span class="ltx_text" id="Ch3.T3.6.2" style="font-size:90%;">Comparison of Transformer performance with 2 attention heads</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch3.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch3.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch3.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch3.T4.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch3.T4.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch3.T4.3.3.5">
<span class="ltx_text ltx_font_bold" id="Ch3.T4.3.3.5.1">Heads</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T4.3.3.6">
<span class="ltx_text ltx_font_bold" id="Ch3.T4.3.3.6.1">Lines</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T4.3.3.7">
<span class="ltx_text ltx_font_bold" id="Ch3.T4.3.3.7.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T4.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch3.T4.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch3.T4.1.1.1.m1.1"><semantics id="Ch3.T4.1.1.1.m1.1a"><mo id="Ch3.T4.1.1.1.m1.1.1" stretchy="false" xref="Ch3.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch3.T4.1.1.1.m1.1b"><ci id="Ch3.T4.1.1.1.m1.1.1.cmml" xref="Ch3.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T4.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.T4.1.1.1.m1.1d">↑</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T4.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch3.T4.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch3.T4.2.2.2.m1.1"><semantics id="Ch3.T4.2.2.2.m1.1a"><mo id="Ch3.T4.2.2.2.m1.1.1" stretchy="false" xref="Ch3.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch3.T4.2.2.2.m1.1b"><ci id="Ch3.T4.2.2.2.m1.1.1.cmml" xref="Ch3.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.T4.2.2.2.m1.1d">↓</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch3.T4.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch3.T4.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch3.T4.3.3.3.m1.1"><semantics id="Ch3.T4.3.3.3.m1.1a"><mo id="Ch3.T4.3.3.3.m1.1.1" stretchy="false" xref="Ch3.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch3.T4.3.3.3.m1.1b"><ci id="Ch3.T4.3.3.3.m1.1.1.cmml" xref="Ch3.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T4.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.T4.3.3.3.m1.1d">↑</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch3.T4.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch3.T4.3.4.1.1">Covid baseline</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch3.T4.3.4.1.2">8</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T4.3.4.1.3">8k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T4.3.4.1.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T4.3.4.1.5">9.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T4.3.4.1.6">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T4.3.4.1.7">0.33</td>
</tr>
<tr class="ltx_tr" id="Ch3.T4.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T4.3.5.2.1">Covid extended</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T4.3.5.2.2">8</th>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.5.2.3">13k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.5.2.4">35k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.5.2.5">35.7</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.5.2.6">0.61</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.5.2.7">0.55</td>
</tr>
<tr class="ltx_tr" id="Ch3.T4.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T4.3.6.3.1">Out-of-domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T4.3.6.3.2">8</th>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.6.3.3">52k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.6.3.4">200k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.6.3.5">13.0</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.6.3.6">0.80</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.6.3.7">0.40</td>
</tr>
<tr class="ltx_tr" id="Ch3.T4.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T4.3.7.4.1">Fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T4.3.7.4.2">8</th>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.7.4.3">65k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.7.4.4">35k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.7.4.5">25.0</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.7.4.6">0.63</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.7.4.7">0.43</td>
</tr>
<tr class="ltx_tr" id="Ch3.T4.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch3.T4.3.8.5.1">Mixed fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T4.3.8.5.2">8</th>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.8.5.3">65k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.8.5.4">35k</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.8.5.5">18.0</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.8.5.6">0.71</td>
<td class="ltx_td ltx_align_center" id="Ch3.T4.3.8.5.7">0.42</td>
</tr>
<tr class="ltx_tr" id="Ch3.T4.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch3.T4.3.9.6.1">Combined domains</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="Ch3.T4.3.9.6.2">8</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T4.3.9.6.3">65k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T4.3.9.6.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T4.3.9.6.5">32.8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T4.3.9.6.6">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch3.T4.3.9.6.7">0.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch3.T4.5.1.1" style="font-size:90%;">Table 3.4</span>: </span><span class="ltx_text" id="Ch3.T4.6.2" style="font-size:90%;">Comparison of Transformer performance with 8 attention heads</span></figcaption>
</figure>
<div class="ltx_para" id="Ch3.S6.SS2.p2">
<p class="ltx_p" id="Ch3.S6.SS2.p2.1">The worst-performing model uses the Covid baseline which is not surprising given that only 8k lines are available. The performance of the higher resourced models (out-of-domain, fine-tuned, mixed fine-tuned and combined domains) all lag that of the Covid extended model. In particular, the out-of-domain model, using the DGT dataset, performs very poorly with a BLEU score of just 13.9 on a Transformer model with 2 heads.</p>
</div>
<div class="ltx_para" id="Ch3.S6.SS2.p3">
<p class="ltx_p" id="Ch3.S6.SS2.p3.1">The BLEU and TER scores for all approaches are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F2.sf1" title="3.2(a) ‣ Figure 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.2(a)</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch3.F2.sf2" title="3.2(b) ‣ Figure 3.3 ‣ 3.6.2 Results ‣ 3.6 Empirical Evaluation ‣ Chapter 3 MT in the Covid domain: Shared Task for LoResMT2021"><span class="ltx_text ltx_ref_tag">3.2(b)</span></a>. As expected, there is a high level of inverse correlation between BLEU and TER. Well-performing models, with high BLEU scores, also required little post-editing effort as indicated by their lower TER scores.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch3.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.7 </span>Discussion</h3>
<div class="ltx_para" id="Ch3.S7.p1">
<p class="ltx_p" id="Ch3.S7.p1.1">Standard Transformer parameters identified in a previous study were observed to perform well <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>. Reducing hidden neurons to 256 and increasing regularisation dropout to 0.3 improved translation performance and these hyperparameters were chosen when building all Transformer models. Furthermore, a batch size of 2048 and using 6 layers for the encoder and decoder were chosen throughout.</p>
</div>
<div class="ltx_para" id="Ch3.S7.p2">
<p class="ltx_p" id="Ch3.S7.p2.1">The results demonstrate that translation performance for specific domains is driven by the amount of data which is available for that specific domain. It is noteworthy that an in-domain dataset of 13k lines (Covid extended), trained for just 35k steps outperformed by 22.1 BLEU points the corresponding out-of-domain 52k dataset (DGT) which was trained for 200k steps.
</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.8 </span>Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch3.S8.p1">
<p class="ltx_p" id="Ch3.S8.p1.1">In our paper, we demonstrated that a high-performing in-domain translation model can be built with a dataset of 13k lines. Developing a small in-domain dataset, of just 5k lines, improved the BLEU score by 27 points when models were trained with the combined Covid baseline and custom Covid dataset.</p>
</div>
<div class="ltx_para" id="Ch3.S8.p2">
<p class="ltx_p" id="Ch3.S8.p2.1">Following on from our previous work, careful selection of Transformer hyperparameters, and using a 16k BPE SentencePiece submodel, enabled rapid development of high-performing translation models in a low-resource setting.</p>
</div>
<div class="ltx_para" id="Ch3.S8.p3">
<p class="ltx_p" id="Ch3.S8.p3.1">Within the context of our research in low-resource EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch3.S8.p3.1.m1.1"><semantics id="Ch3.S8.p3.1.m1.1a"><mo id="Ch3.S8.p3.1.m1.1.1" stretchy="false" xref="Ch3.S8.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch3.S8.p3.1.m1.1b"><ci id="Ch3.S8.p3.1.m1.1.1.cmml" xref="Ch3.S8.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S8.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S8.p3.1.m1.1d">→</annotation></semantics></math>GA translation, we have shown that augmenting in-domain data, by a small amount, performed better than approaches which incorporate fine-tuning, mixed fine-tuning or the combination of domains.</p>
</div>
<div class="ltx_para" id="Ch3.S8.p4">
<p class="ltx_p" id="Ch3.S8.p4.1">As part of our future work, we plan to develop EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch3.S8.p4.1.m1.1"><semantics id="Ch3.S8.p4.1.m1.1a"><mo id="Ch3.S8.p4.1.m1.1.1" stretchy="false" xref="Ch3.S8.p4.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch3.S8.p4.1.m1.1b"><ci id="Ch3.S8.p4.1.m1.1.1.cmml" xref="Ch3.S8.p4.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S8.p4.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch3.S8.p4.1.m1.1d">↔</annotation></semantics></math>GA MT models trained on a dataset derived from the health domain (cf. Chapter 4). Domain adaptation, through fine-tuning such models with the Covid extended dataset may further improve Covid MT performance.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch4">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 4 </span><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch4.1.1">gaHealth<span class="ltx_text ltx_font_upright" id="Ch4.1.1.1">: EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.1.1.1.m1.1"><semantics id="Ch4.1.1.1.m1.1b"><mo id="Ch4.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch4.1.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.1.1.1.m1.1c"><ci id="Ch4.1.1.1.m1.1.1.cmml" xref="Ch4.1.1.1.m1.1.1">normal-↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.1.1.1.m1.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.1.1.1.m1.1e">↔</annotation></semantics></math>GA Bilingual Corpus of Health Data</span></span>
</h2>
<section class="ltx_section" id="Ch4.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.1 </span>Context</h3>
<div class="ltx_para" id="Ch4.S1.p1">
<p class="ltx_p" id="Ch4.S1.p1.1">While MT has made significant strides in high-resource language pairs, there is a noticeable scarcity of parallel data for low-resource languages, particularly in specialised domains like health. An example of a high-resource language pair that has near-perfect MT in a specific domain can be seen in the English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S1.p1.1.m1.1"><semantics id="Ch4.S1.p1.1.m1.1a"><mo id="Ch4.S1.p1.1.m1.1.1" stretchy="false" xref="Ch4.S1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S1.p1.1.m1.1b"><ci id="Ch4.S1.p1.1.m1.1.1.cmml" xref="Ch4.S1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S1.p1.1.m1.1d">↔</annotation></semantics></math>French pair. Weather forecasting in Canada, which is provided both in English and French, has led to one of the best MT systems ever produced, namely the METEO system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx111" title="">111</a>]</cite>. Having been perfected over decades, the only errors made by the system are due to erroneous human input.</p>
</div>
<div class="ltx_para" id="Ch4.S1.p2">
<p class="ltx_p" id="Ch4.S1.p2.1">With regard to low-resource languages, the existing efforts often prioritise quantity over domain specificity. The focus of this research is to address, in part, this imbalance and the key motivations for this research are driven by RQ2. A separate contribution from the research was the development of a set of guidelines for the pre-processing, alignment and validation of a corpus for a low-resource language pair. An important motivating factor for answering this research question is that key information first comes out in “major” languages before it does in less well-resourced languages. Consequently, people who would prefer to access information in their preferred language are forced to operate in a language they otherwise would not choose. In particular, this paper deals with several aspects of the research question which are highlighted below:</p>
</div>
<div class="ltx_para" id="Ch4.S1.p3">
<ul class="ltx_itemize" id="Ch4.S1.I1">
<li class="ltx_item" id="Ch4.S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i1.p1">
<p class="ltx_p" id="Ch4.S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch4.S1.I1.i1.p1.1.1">Addressing Low-Resource Language Challenges</span>: The scarcity of parallel data for low-resource languages hinders the development of effective MT models. This scarcity is particularly acute in specialised domains like health. The study aims to alleviate this issue by creating a focused, in-domain dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i2.p1">
<p class="ltx_p" id="Ch4.S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch4.S1.I1.i2.p1.1.1">Importance of In-Domain Data</span>: The benefits of using in-domain data for training MT models are highlighted. In contrast to generic datasets, in-domain datasets are tailored to a specific domain (in this case, health), which can lead to more accurate and contextually relevant translations for that domain.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i3.p1">
<p class="ltx_p" id="Ch4.S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch4.S1.I1.i3.p1.1.1">Development of gaHealth Corpus</span>: The research introduces the gaHealth corpus, which is the first bilingual corpus of health data for the Irish language. This corpus is designed to be a valuable resource for the NLP community, especially those working within the Irish language domain. The importance of open access is recognised and the gaHealth corpus has been shared online, inviting further research and contributions from the wider community.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i4.p1">
<p class="ltx_p" id="Ch4.S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch4.S1.I1.i4.p1.1.1">Empirical Demonstration of Benefits</span>: The study provides empirical evidence to support the use of in-domain data, showing significant improvements in translation performance. The comparison with top-performing models from a shared task (LoResMT2021) highlights the substantial gains achieved through the use of the gaHealth corpus.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i5.p1">
<p class="ltx_p" id="Ch4.S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch4.S1.I1.i5.p1.1.1">Guidelines for Corpus Development</span>: Recognising the challenges encountered during the conversion of PDF documents, guidelines were established to facilitate the conversion process. This contribution not only aids in the development of the gaHealth corpus but also provides valuable insights for similar projects.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i6.p1">
<p class="ltx_p" id="Ch4.S1.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch4.S1.I1.i6.p1.1.1">Future Directions</span>: The research outlines a clear roadmap for future work. This includes expanding the gaHealth corpus as more Irish language documents become available, refining the models, and conducting a deep linguistic investigation to understand the nuances of model performance. Additionally, it is planned to extend the effort to other key domains such as Education and Finance.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch4.S1.p4">
<p class="ltx_p" id="Ch4.S1.p4.1">In summary, the research addresses a critical gap in the availability of specialised, in-domain data for low-resource languages, particularly in the context of health-related content for the Irish language. By creating the gaHealth corpus and demonstrating its effectiveness in improving translation models, the study provides a valuable resource for the NLP community and lays the foundation for future advancements in this domain. More importantly, it has contributed to high-quality MT systems which can assist Irish-language speakers in accessing health information in their preferred language.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch4.S1.p5">
<p class="ltx_p" id="Ch4.S1.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch4.S1.p5.1.1">gaHealth<span class="ltx_text ltx_font_upright" id="Ch4.S1.p5.1.1.1">: An EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S1.p5.1.1.1.m1.1"><semantics id="Ch4.S1.p5.1.1.1.m1.1a"><mo id="Ch4.S1.p5.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch4.S1.p5.1.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S1.p5.1.1.1.m1.1b"><ci id="Ch4.S1.p5.1.1.1.m1.1.1.cmml" xref="Ch4.S1.p5.1.1.1.m1.1.1">normal-↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S1.p5.1.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S1.p5.1.1.1.m1.1d">↔</annotation></semantics></math>GA Bilingual Corpus of Health Data</span></span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch4.S1.p5.2"><span class="ltx_text ltx_font_bold" id="Ch4.S1.p5.2.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch4.S1.p5.3"><span class="ltx_text ltx_font_bold" id="Ch4.S1.p5.3.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch4.S1.p5.4"><span class="ltx_text ltx_font_bold" id="Ch4.S1.p5.4.1">Órla Ní Loinsigh</span></p>
<p class="ltx_p" id="Ch4.S1.p5.5"><span class="ltx_text ltx_font_bold" id="Ch4.S1.p5.5.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch4.S1.p5.6">Proceedings of the Thirteenth Language Resources and Evaluation Conference</p>
<p class="ltx_p" id="Ch4.S1.p5.7">European Language Resources Association</p>
<p class="ltx_p" id="Ch4.S1.p5.8">20-25 June 2022</p>
<p class="ltx_p" id="Ch4.S1.p5.9">Marseille, France</p>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch4.S1.p5.10">ADAPT Centre</p>
<p class="ltx_p" id="Ch4.S1.p5.11">Dublin City University</p>
<p class="ltx_p" id="Ch4.S1.p5.12">Ireland</p>
<br class="ltx_break"/>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch4.S1.p5.13"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.lrec-1.727.pdf" title="">https://aclanthology.org/2022.lrec-1.727.pdf</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch4.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch4.S2.p1">
<p class="ltx_p" id="Ch4.S2.p1.1">Machine translation is a mature technology for many high-resource language pairs. However, in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English-to-Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the <span class="ltx_text ltx_font_italic" id="Ch4.S2.p1.1.1">gaHealth</span> corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared with top-performing models from the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for developing <span class="ltx_text ltx_font_italic" id="Ch4.S2.p1.1.2">gaHealth</span>, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of low-resource data sets. <span class="ltx_text ltx_font_italic" id="Ch4.S2.p1.1.3">gaHealth</span> is now freely available online and is ready to be explored for further research.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.3 </span>Introduction</h3>
<div class="ltx_para" id="Ch4.S3.p1">
<p class="ltx_p" id="Ch4.S3.p1.1">Improvements in performance in natural language processing (NLP) tasks are typically to be seen when deep learning models are used. However, deep learning requires large amounts of data for model training. Consequently, the availability of large amounts of textual data has become fundamental to the success of NLP applications, such as language modelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx24" title="">24</a>]</cite> and neural machine translation (NMT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx101" title="">101</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch4.S3.p2">
<p class="ltx_p" id="Ch4.S3.p2.1">A popular method of developing such corpora for machine translation (MT) tasks is to crawl and parse bilingual web pages to general parallel corpora. However, given the nature of low-resource languages, there are often insufficient websites available in both the languages of study. Accordingly, a lack of web content typically hinders the development of NLP applications for low-resource languages.</p>
</div>
<div class="ltx_para" id="Ch4.S3.p3">
<p class="ltx_p" id="Ch4.S3.p3.2">The motivation underpinning our present work comes from the challenges we faced in developing high-performing MT models in low-resource settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx2" title="">2</a>]</cite>. In this work, we developed the first bilingual corpus of health data for the English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S3.p3.1.m1.1"><semantics id="Ch4.S3.p3.1.m1.1a"><mo id="Ch4.S3.p3.1.m1.1.1" stretchy="false" xref="Ch4.S3.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S3.p3.1.m1.1b"><ci id="Ch4.S3.p3.1.m1.1.1.cmml" xref="Ch4.S3.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S3.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S3.p3.1.m1.1d">↔</annotation></semantics></math>Irish (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S3.p3.2.m2.1"><semantics id="Ch4.S3.p3.2.m2.1a"><mo id="Ch4.S3.p3.2.m2.1.1" stretchy="false" xref="Ch4.S3.p3.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S3.p3.2.m2.1b"><ci id="Ch4.S3.p3.2.m2.1.1.cmml" xref="Ch4.S3.p3.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S3.p3.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S3.p3.2.m2.1d">↔</annotation></semantics></math>GA) language pair. A procedure was created to extract, clean and select appropriate sentences to build a bilingual corpus. In addition, we built a high-performing MT model for translating in-domain health data.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.4 </span>Related work</h3>
<section class="ltx_subsection" id="Ch4.S4.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4.1 </span>Transformer</h4>
<div class="ltx_para" id="Ch4.S4.SS1.p1">
<p class="ltx_p" id="Ch4.S4.SS1.p1.1">Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite> is an architecture for transforming an input sequence into an output sequence via an encoder and decoder without relying on recurrent neural networks. Transformer models use attention to focus on previously generated tokens. This approach allows models to develop a long memory which is particularly useful in the domain of language translation.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch4.S4.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4.2 </span>Transformer Hyperparameter Optimisation</h4>
<div class="ltx_para" id="Ch4.S4.SS2.p1">
<p class="ltx_p" id="Ch4.S4.SS2.p1.1">Hyperparameter optimisation of Transformer models in translating the low-resource English-Irish (EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S4.SS2.p1.1.m1.1"><semantics id="Ch4.S4.SS2.p1.1.m1.1a"><mo id="Ch4.S4.SS2.p1.1.m1.1.1" stretchy="false" xref="Ch4.S4.SS2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.SS2.p1.1.m1.1b"><ci id="Ch4.S4.SS2.p1.1.m1.1.1.cmml" xref="Ch4.S4.SS2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.SS2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.SS2.p1.1.m1.1d">→</annotation></semantics></math>GA) language pair has been evaluated in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>.
Carefully selecting the appropriate subword model has been shown to be an important driver of translation performance. A Transformer architecture, using a 16k BPE SentencePiece subword model, demonstrated optimal performance.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch4.S4.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4.3 </span>Neural MT</h4>
<div class="ltx_para" id="Ch4.S4.SS3.p1">
<p class="ltx_p" id="Ch4.S4.SS3.p1.1">Using large bilingual corpora, NMT approaches require the training of neural networks to learn a statistical model for MT. The technique has demonstrated state-of-the-art translation performance on many benchmarks. However, one of the key factors in enabling the development of high-performing NMT models is the availability of large amounts of parallel data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx104" title="">104</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch4.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.5 </span>Proposed Approach</h3>
<section class="ltx_subsection" id="Ch4.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.1 </span>Sources for <span class="ltx_text ltx_font_italic" id="Ch4.S5.SS1.1.1">gaHealth</span> Development</h4>
<div class="ltx_para" id="Ch4.S5.SS1.p1">
<p class="ltx_p" id="Ch4.S5.SS1.p1.1">To build a bilingual corpus of health data, we selected multiple sources of professionally translated documents from within the Irish government, all of which are publicly available. In particular, the bilingual strategy statements and annual reports of the Irish Department of Health since 2010 were chosen.</p>
</div>
<div class="ltx_para" id="Ch4.S5.SS1.p2">
<p class="ltx_p" id="Ch4.S5.SS1.p2.1">Furthermore, a dataset of Covid-related data, developed for a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>]</cite>, was incorporated into a larger health dataset. Given the pace at which Covid-19 data was being published, translated Irish website content often lagged behind the English-language counterpart. Website snapshots taken by the WayBack Machine<span class="ltx_note ltx_role_footnote" id="Ch4.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://archive.org/web/" title="">https://archive.org/web/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx6" title="">6</a>]</cite> proved particularly useful in creating good parallel data from unaligned parallel websites. Extracts from the corpus are illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T1" title="Table 4.1 ‣ 4.5.1 Sources for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch4.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T1.2.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch4.T1.2.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch4.T1.2.1.1.1.1">Type</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch4.T1.2.1.1.2" style="width:341.4pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch4.T1.2.1.1.2.1">Sentence</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T1.2.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch4.T1.2.2.1.1" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.2.1.1.1">EN-1</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch4.T1.2.2.1.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.2.1.2.1">The Programme for Government makes strong commitments to strengthen community-based care, including primary care and social care, and sees this as fundamental to advancing Sláintecare reforms.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.2.3.2">
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.3.2.1" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.3.2.1.1">GA-1</p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.3.2.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.3.2.2.1">Tá gealltanais láidre sa Chlár Rialtais maidir le cúram pobalbhunaithe a neartú, cúram príomhúil agus cúram sóisialta san áireamh, agus breathnaítear air sin mar chuid bhunriachtanach d’athchóirithe Sláintecare a chur ar aghaidh.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.2.4.3">
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.4.3.1" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.4.3.1.1">EN-2</p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.4.3.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.4.3.2.1">The Hepatitis C Compensation Tribunal (Amendment) Act 2006 established a statutory scheme to address insurance difficulties experienced by persons infected with Hepatitis C and HIV through the administration within the State of blood and blood products.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.2.5.4">
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.5.4.1" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.5.4.1.1">GA-2</p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.5.4.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.5.4.2.1">Faoin Acht um Binse Cúitimh i ndáil le Heipitíteas C (Leasú) 2006, bunaíodh scéim reachtúil chun dul i ngleic le deacrachtaí árachais a bhí ag daoine a bhí ionfhabhtaithe le Heipitíteas C agus VEID trí fhuil agus táirgí fola a tugadh dóibh laistigh den Stát.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.2.6.5">
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.6.5.1" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.6.5.1.1">EN-3</p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch4.T1.2.6.5.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.6.5.2.1">To provide virological evidence on the presence and extent of undetected community transmission of covid-19 and monitor positivity rates among individuals presenting ill or acute respiratory tract infections to primary care.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.2.7.6">
<td class="ltx_td ltx_align_justify ltx_border_b" id="Ch4.T1.2.7.6.1" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.7.6.1.1">GA-3</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b" id="Ch4.T1.2.7.6.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch4.T1.2.7.6.2.1">Fianaise víreolaíoch a sholáthar maidir le láithreacht agus méid an tarchuir pobail anaithnid de covid-19 agus monatóireacht a dhéanamh ar rátaí dearfacha i measc daoine aonair a bhfuil ionfhabhtuithe ili nó conaire riospráide géarmhíochaine orthu chuig cúram príomhúil.</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T1.4.1.1" style="font-size:90%;">Table 4.1</span>: </span><span class="ltx_text" id="Ch4.T1.5.2" style="font-size:90%;">Extracts from the <span class="ltx_text ltx_font_italic" id="Ch4.T1.5.2.1">gaHealth</span> corpus are illustrated in this table. The sentences, EN-1 / GA-1, are drawn from strategy statements, EN-2 / GA-2 are taken from annual reports whereas EN-3 / GA-3 are from Covid sources.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S5.SS1.p3">
<p class="ltx_p" id="Ch4.S5.SS1.p3.1">This amalgamated corpus, <span class="ltx_text ltx_font_italic" id="Ch4.S5.SS1.p3.1.1">gaHealth</span>, consists of 16,201 lines of parallel text files. The combined English and Irish vocabulary size is 19,269 unique words. The constituent elements of the dataset, prior to applying the toolchain, are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T2" title="Table 4.2 ‣ 4.5.1 Sources for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T2.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch4.T2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T2.2.1.1.1.1">Documents</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch4.T2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T2.2.1.1.2.1">Source</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch4.T2.2.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T2.2.1.1.3.1">Lines</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T2.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.2.2.1.1">Strategy Statement 2020</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.2.2.1.2">HSE</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.2.2.1.3">3k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.3.2">
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.3.2.1">Strategy Statement 2017</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.3.2.2">HSE</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.3.2.3">2.5k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.4.3">
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.4.3.1">Strategy Statement 2015</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.4.3.2">HSE</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.4.3.3">3k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.5.4">
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.5.4.1">Annual Report 2020</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.5.4.2">HSE</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.5.4.3">2k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.6.5">
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.6.5.1">Annual Report 2019</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.6.5.2">HSE</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.6.5.3">2k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.7.6">
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.7.6.1">Annual Report 2017</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.7.6.2">HSE</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.7.6.3">2k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.8.7">
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.8.7.1">Website (Covid)</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.8.7.2">Citizen’s Advice</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.2.8.7.3">4k</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.2.9.8">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch4.T2.2.9.8.1">Publications (Covid)</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch4.T2.2.9.8.2">HSE</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch4.T2.2.9.8.3">4k</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T2.3.1.1" style="font-size:90%;">Table 4.2</span>: </span><span class="ltx_text" id="Ch4.T2.4.2" style="font-size:90%;">Sources used in corpus development</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch4.S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.2 </span>Toolchain used for <span class="ltx_text ltx_font_italic" id="Ch4.S5.SS2.1.1">gaHealth</span> Development</h4>
<div class="ltx_para" id="Ch4.S5.SS2.p1">
<p class="ltx_p" id="Ch4.S5.SS2.p1.1">The HSE PDF and Word documents were pre-processed using a toolchain currently under development as part of the Irish Language Resource Infrastructure project (ILRI), funded by the Department of the Gaeltacht. This toolchain has been written to accept primarily data that originates in public administration organisations, i.e. relatively formal text for which the translation quality is assumed to be high, the structure and formatting to be reasonably consistent, and the potential for noise to be low.</p>
</div>
<div class="ltx_para" id="Ch4.S5.SS2.p2">
<p class="ltx_p" id="Ch4.S5.SS2.p2.1">The source material consisted of a combination of twelve input files: six in English (all PDF) and six in Irish (five PDF, one Word); all PDFs had a text layer. They ranged from relatively short (30 to 40 A4 pages) to more substantial (ca. 200 pages). PDFs in particular can be problematic for creating high-quality corpora for a variety of reasons. In other words, while the quality of the input content in this case can be said to be high, the quality of the input medium is low.</p>
</div>
<div class="ltx_para" id="Ch4.S5.SS2.p3">
<p class="ltx_p" id="Ch4.S5.SS2.p3.1">The process used for developing the corpus is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F1" title="Figure 4.1 ‣ 4.5.2 Toolchain used for gaHealth Development ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.1</span></a>. The toolchain consists of a set of components run in sequence over a set of input documents, to convert it from raw content to a sentence-aligned corpus. Several of the components listed below have different implementations depending on the source type and intended output.</p>
</div>
<figure class="ltx_figure" id="Ch4.F1"><img alt="Refer to caption" class="ltx_graphics" id="Ch4.F1.g1" src="corpus_dev.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F1.2.1.1" style="font-size:90%;">Figure 4.1</span>: </span><span class="ltx_text" id="Ch4.F1.3.2" style="font-size:90%;">Corpus development process. In developing the corpus, the key steps of data collection, pre-processing, alignment and validation were followed. The role of the toolchain at various stages is highlighted.</span></figcaption>
</figure>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">Text extractors</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px1.p1.1">These are wrappers around external components that extract text based on input type; for the <span class="ltx_text ltx_font_italic" id="Ch4.S5.SS2.SSS0.Px1.p1.1.1">gaHealth</span> file types, wrappers were written for LibreOffice and pdftotext.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">Unicode normalizer</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px2.p1.1">This is used to achieve Unicode equivalence, and optionally to substitute certain (e.g. corrupted) characters.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px3">
<h6 class="ltx_title ltx_title_paragraph">Language detector</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px3.p1.1">In this toolchain, the language detector is a wrapper around langdetect, itself a port from language-detection by Nakatani Shuyo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx83" title="">83</a>]</cite>. The wrapper was written to allow this to be run conveniently either on a string or on an entire file.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px4">
<h6 class="ltx_title ltx_title_paragraph">Sentence splitters</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px4.p1.1">These are custom-written components to reconstruct sentence boundary information. For editable file types like plain text and Word, this process is relatively straightforward. However, PDFs present particular challenges in this regard. Along with ordering issues, the absence of sentence boundary information is one of the biggest reasons why it is so difficult to construct a high-quality corpus from PDFs. A custom sentence splitter was used to determine sentence boundaries from text extracted from PDFs specifically, primarily using capitalisation and language-specific lists of abbreviations to determine where sentences should be broken.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px5">
<h6 class="ltx_title ltx_title_paragraph">Document aligner</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px5.p1.1">This aligns sets of files whose languages have been identified. A wrapper was written around an external component called FaDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx73" title="">73</a>]</cite> to adapt it to the toolchain. As FaDA always names an alignment for each input file, sometimes even mapping two different files to the same one, it was necessary to put some selection logic in here, as well as a mechanism for determining when there is no appropriate mapping. Constraints may be put on the relative size of the files to accept an alignment, and the process may be re-run multiple times, with previously rejected files being run again.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px6">
<h6 class="ltx_title ltx_title_paragraph">Sentence aligner</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px6.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px6.p1.1">This aligns pairs of files at the sentence level. This was a wrapper around the external component hunalign <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx45" title="">45</a>]</cite>. No special parameters were used, as default settings produced results of high quality.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch4.S5.SS2.SSS0.Px7">
<h6 class="ltx_title ltx_title_paragraph">Text cleaners</h6>
<div class="ltx_para" id="Ch4.S5.SS2.SSS0.Px7.p1">
<p class="ltx_p" id="Ch4.S5.SS2.SSS0.Px7.p1.1">These remove sentence pairs that are believed to be incorrect alignments, such as empty segments and those with obviously mismatched content.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch4.S5.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.3 </span>Guidelines</h4>
<div class="ltx_para" id="Ch4.S5.SS3.p1">
<p class="ltx_p" id="Ch4.S5.SS3.p1.1">With the above considerations in mind, the following set of rules was decided upon when processing the <span class="ltx_text ltx_font_italic" id="Ch4.S5.SS3.p1.1.1">gaHealth</span> dataset. Many of these could be specified as parameters to the toolchain, while others were hard-coded into the system.</p>
</div>
<div class="ltx_para" id="Ch4.S5.SS3.p2">
<ol class="ltx_enumerate" id="Ch4.S5.I1">
<li class="ltx_item" id="Ch4.S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Ch4.S5.I1.i1.p1">
<p class="ltx_p" id="Ch4.S5.I1.i1.p1.1">Unicode standard: normalise all characters to Unicode UTF-8 NFC. Remove any byte order marks.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ch4.S5.I1.i2.p1">
<p class="ltx_p" id="Ch4.S5.I1.i2.p1.1">Whitespacing and capitalisation: merge sequences of whitespace characters into a single space. Do not perform tokenization or truecasing.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ch4.S5.I1.i3.p1">
<p class="ltx_p" id="Ch4.S5.I1.i3.p1.1">File language detection: scan the first 50 lines, and then every 100th line.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Ch4.S5.I1.i4.p1">
<p class="ltx_p" id="Ch4.S5.I1.i4.p1.1">Document alignment: assume that specific patterns like a line beginning with a single letter in parentheses or a number followed by a full stop indicate a sentence break from the previous line. Ensure each document is 0.75-1.33 times the size of the document it is being aligned with. Run for a maximum of three iterations.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Ch4.S5.I1.i5.p1">
<p class="ltx_p" id="Ch4.S5.I1.i5.p1.1">Sentence alignment: allow one-to-many alignments.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Ch4.S5.I1.i6.p1">
<p class="ltx_p" id="Ch4.S5.I1.i6.p1.1">Cleaning: remove any pairs where source or target:</p>
<ul class="ltx_itemize" id="Ch4.S5.I1.i6.I1">
<li class="ltx_item" id="Ch4.S5.I1.i6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S5.I1.i6.I1.i1.p1">
<p class="ltx_p" id="Ch4.S5.I1.i6.I1.i1.p1.1">is empty</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S5.I1.i6.I1.i2.p1">
<p class="ltx_p" id="Ch4.S5.I1.i6.I1.i2.p1.1">contains no non-alphabetical characters</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S5.I1.i6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S5.I1.i6.I1.i3.p1">
<p class="ltx_p" id="Ch4.S5.I1.i6.I1.i3.p1.1">is of an incorrect language. This will remove most untranslated segments. The language is only to be detected for segments that have at least 40 characters</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="Ch4.S5.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.4 </span>Transformer Architecture</h4>
<div class="ltx_para" id="Ch4.S5.SS4.p1">
<p class="ltx_p" id="Ch4.S5.SS4.p1.2">All EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S5.SS4.p1.1.m1.1"><semantics id="Ch4.S5.SS4.p1.1.m1.1a"><mo id="Ch4.S5.SS4.p1.1.m1.1.1" stretchy="false" xref="Ch4.S5.SS4.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S5.SS4.p1.1.m1.1b"><ci id="Ch4.S5.SS4.p1.1.m1.1.1.cmml" xref="Ch4.S5.SS4.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S5.SS4.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S5.SS4.p1.1.m1.1d">↔</annotation></semantics></math>GA models, trained with the <span class="ltx_text ltx_font_italic" id="Ch4.S5.SS4.p1.2.1">gaHealth</span> corpus, were developed using a Transformer architecture. A reduction in the number of hidden layer neurons and increasing dropout significantly improved performance. Furthermore, within the context of low-resource EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S5.SS4.p1.2.m2.1"><semantics id="Ch4.S5.SS4.p1.2.m2.1a"><mo id="Ch4.S5.SS4.p1.2.m2.1.1" stretchy="false" xref="Ch4.S5.SS4.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S5.SS4.p1.2.m2.1b"><ci id="Ch4.S5.SS4.p1.2.m2.1.1.cmml" xref="Ch4.S5.SS4.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S5.SS4.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S5.SS4.p1.2.m2.1d">→</annotation></semantics></math>GA translation, using a 16k BPE submodel resulted in the highest-performing models. Optimal hyperparameters were selected in line with these findings from our previous and are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T3" title="Table 4.3 ‣ 4.5.4 Transformer Architecture ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<figure class="ltx_table ltx_align_center" id="Ch4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch4.T3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T3.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T3.2.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.2.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.2.2.1">Learning rate</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.2.2.2">0.1, 0.01, 0.001, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.2.2.2.1">2</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.3.3.1">Batch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.3.3.2">1024, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.3.3.2.1">2048</span>, 4096, 8192</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.4.4.1">Attention heads</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.4.4.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T3.2.4.4.2.1">2</span>, 4, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.4.4.2.2">8</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.5.5.1">Number of layers</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.5.5.2">5, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.5.5.2.1">6</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.6.6.1">Feed-forward dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.6.6.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.2.6.6.2.1">2048</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.7.7.1">Embedding dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.7.7.2">128, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.7.7.2.1">256</span>, 512</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.8.8.1">Label smoothing</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.8.8.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T3.2.8.8.2.1">0.1</span>, 0.3</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.9.9.1">Dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.9.9.2">0.1, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.9.9.2.1">0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.2.10.10.1">Attention dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T3.2.10.10.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.2.10.10.2.1">0.1</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.2.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch4.T3.2.11.11.1">Average Decay</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch4.T3.2.11.11.2">0, <span class="ltx_text ltx_font_bold" id="Ch4.T3.2.11.11.2.1">0.0001</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T3.3.1.1" style="font-size:90%;">Table 4.3</span>: </span><span class="ltx_text" id="Ch4.T3.4.2" style="font-size:90%;">Hyperparameter optimisation for Transformer models. Optimal parameters are highlighted in bold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Ch4.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.6 </span>Empirical Evaluation</h3>
<div class="ltx_para" id="Ch4.S6.p1">
<p class="ltx_p" id="Ch4.S6.p1.1">In addition to developing the <span class="ltx_text ltx_font_italic" id="Ch4.S6.p1.1.1">gaHealth</span> corpus, the effectiveness of the dataset was evaluated by training models for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S6.p1.1.m1.1"><semantics id="Ch4.S6.p1.1.m1.1a"><mo id="Ch4.S6.p1.1.m1.1.1" stretchy="false" xref="Ch4.S6.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.p1.1.m1.1b"><ci id="Ch4.S6.p1.1.m1.1.1.cmml" xref="Ch4.S6.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.p1.1.m1.1d">↔</annotation></semantics></math>GA translation in the Health domain. All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite> subword model. The impact of using separate source and target subword models was not explored.
</p>
</div>
<div class="ltx_para" id="Ch4.S6.p2">
<p class="ltx_p" id="Ch4.S6.p2.2">To benchmark the performance of our models, the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.p2.1.m1.1"><semantics id="Ch4.S6.p2.1.m1.1a"><mo id="Ch4.S6.p2.1.m1.1.1" stretchy="false" xref="Ch4.S6.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.p2.1.m1.1b"><ci id="Ch4.S6.p2.1.m1.1.1.cmml" xref="Ch4.S6.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.p2.1.m1.1d">→</annotation></semantics></math>GA and GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.p2.2.m2.1"><semantics id="Ch4.S6.p2.2.m2.1a"><mo id="Ch4.S6.p2.2.m2.1.1" stretchy="false" xref="Ch4.S6.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.p2.2.m2.1b"><ci id="Ch4.S6.p2.2.m2.1.1.cmml" xref="Ch4.S6.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.p2.2.m2.1d">→</annotation></semantics></math>EN test datasets from the LoResMT2021 Shared Task  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite> were used. These test datasets enabled the evaluation of the <span class="ltx_text ltx_font_italic" id="Ch4.S6.p2.2.1">gaHealth</span> corpus, and associated models since this shared task focused on an application of the health domain i.e. the translation of Covid-related data. Furthermore, using a shared task test dataset enables the comparison of gaHealth models’ performance with models entered by other teams.</p>
</div>
<div class="ltx_para" id="Ch4.S6.p3">
<p class="ltx_p" id="Ch4.S6.p3.1">The results from the IIITT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx92" title="">92</a>]</cite> and UCF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx27" title="">27</a>]</cite> teams are included in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T6" title="Table 4.6 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T7" title="Table 4.7 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.7</span></a> so the performance of the <span class="ltx_text ltx_font_italic" id="Ch4.S6.p3.1.1">gaHealth</span> models can be easily compared with the findings of LoResMT2021. IIITT fine-tuned an Opus MT model from Helsinki NLP on the training dataset. UCF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx27" title="">27</a>]</cite> used transfer learning, unigram and subword segmentation methods for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S6.p3.1.m1.1"><semantics id="Ch4.S6.p3.1.m1.1a"><mo id="Ch4.S6.p3.1.m1.1.1" stretchy="false" xref="Ch4.S6.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.p3.1.m1.1b"><ci id="Ch4.S6.p3.1.m1.1.1.cmml" xref="Ch4.S6.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.p3.1.m1.1d">↔</annotation></semantics></math>GA translation.</p>
</div>
<section class="ltx_subsection" id="Ch4.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6.1 </span>Infrastructure</h4>
<div class="ltx_para" id="Ch4.S6.SS1.p1">
<p class="ltx_p" id="Ch4.S6.SS1.p1.1">Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16GB graphic cards and up to 27GB of memory when available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx19" title="">19</a>]</cite>. Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch4.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6.2 </span>Metrics</h4>
<div class="ltx_para" id="Ch4.S6.SS2.p1">
<p class="ltx_p" id="Ch4.S6.SS2.p1.1">Automated metrics were used to determine the translation quality. All models were trained and evaluated using the BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite> evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped after 40k training steps or once an early stopping criteria of no improvement in validation accuracy for four consecutive iterations was recorded.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch4.S6.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6.3 </span>Results: Automatic Evaluation</h4>
<div class="ltx_para" id="Ch4.S6.SS3.p1">
<p class="ltx_p" id="Ch4.S6.SS3.p1.2">The hyperparameters used for developing the models are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T3" title="Table 4.3 ‣ 4.5.4 Transformer Architecture ‣ 4.5 Proposed Approach ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.3</span></a>. The details of the training, validation and test sets used by our NMT models are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T4" title="Table 4.4 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.4</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T5" title="Table 4.5 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.5</span></a>. In all cases, 502 lines were used from the LoResMT2021 validation dataset whereas the test dataset used 502 lines for EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.SS3.p1.1.m1.1"><semantics id="Ch4.S6.SS3.p1.1.m1.1a"><mo id="Ch4.S6.SS3.p1.1.m1.1.1" stretchy="false" xref="Ch4.S6.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.SS3.p1.1.m1.1b"><ci id="Ch4.S6.SS3.p1.1.m1.1.1.cmml" xref="Ch4.S6.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.SS3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.SS3.p1.1.m1.1d">→</annotation></semantics></math>GA translation and 250 lines for GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.SS3.p1.2.m2.1"><semantics id="Ch4.S6.SS3.p1.2.m2.1a"><mo id="Ch4.S6.SS3.p1.2.m2.1.1" stretchy="false" xref="Ch4.S6.SS3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.SS3.p1.2.m2.1b"><ci id="Ch4.S6.SS3.p1.2.m2.1.1.cmml" xref="Ch4.S6.SS3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.SS3.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.SS3.p1.2.m2.1d">→</annotation></semantics></math>EN translation. Both were independent health-specific Covid test sets which were provided by LoResMT2021. There was one exception, due to a data overlap between test and training data, a reduced test set was used when testing the <span class="ltx_text ltx_font_italic" id="Ch4.S6.SS3.p1.2.1">gaHealth</span> en2ga* system.</p>
</div>
<figure class="ltx_table" id="Ch4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch4.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T4.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch4.T4.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T4.4.1.1.1.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T4.4.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T4.4.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T4.4.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T4.4.1.1.3.1">Train</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T4.4.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T4.4.1.1.4.1">Dev</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T4.4.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T4.4.1.1.5.1">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T4.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T4.4.2.1.1">adapt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.4.2.1.2">covid_extended</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.4.2.1.3">13k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.4.2.1.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.4.2.1.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.4.3.2">
<td class="ltx_td ltx_align_left" id="Ch4.T4.4.3.2.1">adapt</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.3.2.2">combined_domains</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.3.2.3">65k</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.3.2.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.3.2.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.4.4.3">
<td class="ltx_td ltx_align_left" id="Ch4.T4.4.4.3.1">IIITT</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.4.3.2">en2ga-b</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.4.3.3">8k</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.4.3.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.4.3.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.4.5.4">
<td class="ltx_td ltx_align_left" id="Ch4.T4.4.5.4.1">UCF</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.5.4.2">en2ga-a</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.5.4.3">8k</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.5.4.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.5.4.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.4.6.5">
<td class="ltx_td ltx_align_left" id="Ch4.T4.4.6.5.1"><span class="ltx_text ltx_font_italic" id="Ch4.T4.4.6.5.1.1">gaHealth</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.6.5.2">en2ga</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.6.5.3">24k</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.6.5.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.4.6.5.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.4.7.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch4.T4.4.7.6.1"><span class="ltx_text ltx_font_italic" id="Ch4.T4.4.7.6.1.1">gaHealth</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T4.4.7.6.2">en2ga*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T4.4.7.6.3">24k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T4.4.7.6.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T4.4.7.6.5">338</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T4.5.2.1" style="font-size:90%;">Table 4.4</span>: </span><span class="ltx_text" id="Ch4.T4.2.1" style="font-size:90%;">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.T4.2.1.m1.1"><semantics id="Ch4.T4.2.1.m1.1b"><mo id="Ch4.T4.2.1.m1.1.1" stretchy="false" xref="Ch4.T4.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.T4.2.1.m1.1c"><ci id="Ch4.T4.2.1.m1.1.1.cmml" xref="Ch4.T4.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T4.2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T4.2.1.m1.1e">→</annotation></semantics></math>GA training, validation and test dataset distributions. The baseline <span class="ltx_text ltx_font_italic" id="Ch4.T4.2.1.1">gaHealth</span> system was augmented with an 8k Covid dataset provided by LoResMT2021. A smaller test set was used when evaluating <span class="ltx_text ltx_font_italic" id="Ch4.T4.2.1.2">gaHealth</span> en2ga* due to an overlap with the training data. An alternative approach of removing the overlap from the <span class="ltx_text ltx_font_italic" id="Ch4.T4.2.1.3">gaHealth</span> corpus, before training, was also carried out to produce the <span class="ltx_text ltx_font_italic" id="Ch4.T4.2.1.4">gaHealth</span> en2ga system.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch4.T5.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T5.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch4.T5.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T5.4.1.1.1.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T5.4.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.4.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T5.4.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.4.1.1.3.1">Train</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T5.4.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.4.1.1.4.1">Dev</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T5.4.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.4.1.1.5.1">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T5.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T5.4.2.1.1">IIITT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.4.2.1.2">ga2en-b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.4.2.1.3">8k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.4.2.1.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.4.2.1.5">250</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.4.3.2">
<td class="ltx_td ltx_align_left" id="Ch4.T5.4.3.2.1">UCF</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.4.3.2.2">ga2en-b</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.4.3.2.3">8k</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.4.3.2.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.4.3.2.5">250</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.4.4.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch4.T5.4.4.3.1"><span class="ltx_text ltx_font_italic" id="Ch4.T5.4.4.3.1.1">gaHealth</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T5.4.4.3.2">ga2en</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T5.4.4.3.3">24k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T5.4.4.3.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T5.4.4.3.5">250</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T5.5.2.1" style="font-size:90%;">Table 4.5</span>: </span><span class="ltx_text" id="Ch4.T5.2.1" style="font-size:90%;">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.T5.2.1.m1.1"><semantics id="Ch4.T5.2.1.m1.1b"><mo id="Ch4.T5.2.1.m1.1.1" stretchy="false" xref="Ch4.T5.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.T5.2.1.m1.1c"><ci id="Ch4.T5.2.1.m1.1.1.cmml" xref="Ch4.T5.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T5.2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T5.2.1.m1.1e">→</annotation></semantics></math>EN training, validation and test dataset distributions. The baseline <span class="ltx_text ltx_font_italic" id="Ch4.T5.2.1.1">gaHealth</span> system was augmented with an 8k Covid dataset provided by LoResMT2021. All overlaps were removed from the <span class="ltx_text ltx_font_italic" id="Ch4.T5.2.1.2">gaHealth</span> corpus prior to training the <span class="ltx_text ltx_font_italic" id="Ch4.T5.2.1.3">gaHealth</span> ga2en model.</span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch4.F2.g1" src="en-ga_acc.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch4.F2.g2" src="en-ga_ppl.png"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F2.5.2.1" style="font-size:90%;">Figure 4.2</span>: </span><span class="ltx_text ltx_font_italic" id="Ch4.F2.2.1" style="font-size:90%;">gaHealth<span class="ltx_text ltx_font_upright" id="Ch4.F2.2.1.1"> en2ga* system: training EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.F2.2.1.1.m1.1"><semantics id="Ch4.F2.2.1.1.m1.1b"><mo id="Ch4.F2.2.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch4.F2.2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.F2.2.1.1.m1.1c"><ci id="Ch4.F2.2.1.1.m1.1.1.cmml" xref="Ch4.F2.2.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.F2.2.1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.F2.2.1.1.m1.1e">→</annotation></semantics></math>GA model with combined 16k gaHealth corpus and 8k LoResMT2021 covid corpus achieving a max validation accuracy of 38.5% and perplexity of 111 after 40k steps. BLEU score: <span class="ltx_text ltx_font_bold" id="Ch4.F2.2.1.1.1">37.6</span>. </span></span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch4.F3.g1" src="en-ga_acc_covid.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch4.F3.g2" src="en-ga_ppl_covid.png"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F3.4.2.1" style="font-size:90%;">Figure 4.3</span>: </span><span class="ltx_text" id="Ch4.F3.2.1" style="font-size:90%;"> adapt covid_extended system: training EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.F3.2.1.m1.1"><semantics id="Ch4.F3.2.1.m1.1b"><mo id="Ch4.F3.2.1.m1.1.1" stretchy="false" xref="Ch4.F3.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.F3.2.1.m1.1c"><ci id="Ch4.F3.2.1.m1.1.1.cmml" xref="Ch4.F3.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.F3.2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.F3.2.1.m1.1e">→</annotation></semantics></math>GA model with 8k LoResMT2021 covid corpus achieving a max validation accuracy of 30.0% and perplexity of 354 after 30k steps. BLEU score: <span class="ltx_text ltx_font_bold" id="Ch4.F3.2.1.1">36.0</span>.</span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch4.F4.g1" src="ga-en_acc.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch4.F4.g2" src="ga-en_ppl.png"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F4.5.2.1" style="font-size:90%;">Figure 4.4</span>: </span><span class="ltx_text ltx_font_italic" id="Ch4.F4.2.1" style="font-size:90%;">gaHealth<span class="ltx_text ltx_font_upright" id="Ch4.F4.2.1.1"> ga2en system: training GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.F4.2.1.1.m1.1"><semantics id="Ch4.F4.2.1.1.m1.1b"><mo id="Ch4.F4.2.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch4.F4.2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.F4.2.1.1.m1.1c"><ci id="Ch4.F4.2.1.1.m1.1.1.cmml" xref="Ch4.F4.2.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.F4.2.1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.F4.2.1.1.m1.1e">→</annotation></semantics></math>EN model with combined 16k gaHealth corpus and 8k LoResMT2021 Covid corpus achieving a max validation accuracy of 39.5% and perplexity of 116 after 40k steps. BLEU score: <span class="ltx_text ltx_font_bold" id="Ch4.F4.2.1.1.1">57.6.</span></span></span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S6.SS3.p2">
<p class="ltx_p" id="Ch4.S6.SS3.p2.1">Experimental results achieved using a Transformer architecture, are summarised in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T6" title="Table 4.6 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.6</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T7" title="Table 4.7 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.7</span></a>. In the LoResMT2021 Shared Task, the highest-performing EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.SS3.p2.1.m1.1"><semantics id="Ch4.S6.SS3.p2.1.m1.1a"><mo id="Ch4.S6.SS3.p2.1.m1.1.1" stretchy="false" xref="Ch4.S6.SS3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.SS3.p2.1.m1.1b"><ci id="Ch4.S6.SS3.p2.1.m1.1.1.cmml" xref="Ch4.S6.SS3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.SS3.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.SS3.p2.1.m1.1d">→</annotation></semantics></math>GA system was submitted by the ADAPT team. The system uses an extended Covid dataset (13k, which is a combination of the MT summit Covid baseline and a custom DCU Covid dataset. This Transformer model, with 2 heads, performs well across all key translation metrics (BLEU: 36.0, TER: 0.531 and ChrF3: 0.6).</p>
</div>
<figure class="ltx_table" id="Ch4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch4.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch4.T6.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T6.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T6.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch4.T6.1.1.1.m1.1"><semantics id="Ch4.T6.1.1.1.m1.1a"><mo id="Ch4.T6.1.1.1.m1.1.1" stretchy="false" xref="Ch4.T6.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch4.T6.1.1.1.m1.1b"><ci id="Ch4.T6.1.1.1.m1.1.1.cmml" xref="Ch4.T6.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T6.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T6.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T6.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T6.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch4.T6.2.2.2.m1.1"><semantics id="Ch4.T6.2.2.2.m1.1a"><mo id="Ch4.T6.2.2.2.m1.1.1" stretchy="false" xref="Ch4.T6.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch4.T6.2.2.2.m1.1b"><ci id="Ch4.T6.2.2.2.m1.1.1.cmml" xref="Ch4.T6.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T6.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T6.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T6.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch4.T6.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch4.T6.3.3.3.m1.1"><semantics id="Ch4.T6.3.3.3.m1.1a"><mo id="Ch4.T6.3.3.3.m1.1.1" stretchy="false" xref="Ch4.T6.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch4.T6.3.3.3.m1.1b"><ci id="Ch4.T6.3.3.3.m1.1.1.cmml" xref="Ch4.T6.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T6.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T6.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T6.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T6.3.4.1.1">UCF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.3.4.1.2">en2ga-b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.3.4.1.3">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.3.4.1.4">0.756</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.3.4.1.5">0.37</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T6.3.5.2.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.5.2.2">en2ga-b</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.5.2.3">25.8</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.5.2.4">0.629</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.5.2.5">0.53</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T6.3.6.3.1">adapt</th>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.6.3.2">combined</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.6.3.3">32.8</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.6.3.4">0.590</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.6.3.5">0.57</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T6.3.7.4.1"><span class="ltx_text ltx_font_italic" id="Ch4.T6.3.7.4.1.1">gaHealth</span></th>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.7.4.2">en2ga</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.7.4.3">33.3</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.7.4.4">0.604</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.7.4.5">0.56</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T6.3.8.5.1">adapt</th>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.8.5.2">covid_extended</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.8.5.3">36.0</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.8.5.4">0.531</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.3.8.5.5">0.60</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch4.T6.3.9.6.1"><span class="ltx_text ltx_font_italic" id="Ch4.T6.3.9.6.1.1">gaHealth</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T6.3.9.6.2">en2ga*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T6.3.9.6.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.3.9.6.3.1">37.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T6.3.9.6.4">0.577</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T6.3.9.6.5">0.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T6.8.2.1" style="font-size:90%;">Table 4.6</span>: </span><span class="ltx_text" id="Ch4.T6.5.1" style="font-size:90%;">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.T6.5.1.m1.1"><semantics id="Ch4.T6.5.1.m1.1b"><mo id="Ch4.T6.5.1.m1.1.1" stretchy="false" xref="Ch4.T6.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.T6.5.1.m1.1c"><ci id="Ch4.T6.5.1.m1.1.1.cmml" xref="Ch4.T6.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T6.5.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T6.5.1.m1.1e">→</annotation></semantics></math>GA <span class="ltx_text ltx_font_italic" id="Ch4.T6.5.1.1">gaHealth</span> system compared with LoResMT2021 systems.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S6.SS3.p3">
<p class="ltx_p" id="Ch4.S6.SS3.p3.1">Validation accuracy, and model perplexity, in developing the <span class="ltx_text ltx_font_italic" id="Ch4.S6.SS3.p3.1.1">gaHealth</span> models are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F2" title="Figure 4.2 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.2</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F4" title="Figure 4.4 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.4</span></a> whereas Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.F3" title="Figure 4.3 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.3</span></a> illustrates model training on just the covid_extended dataset. Rapid convergence was observed while training the <span class="ltx_text ltx_font_italic" id="Ch4.S6.SS3.p3.1.2">gaHealth</span> models such that little accuracy improvement occurs after 30k steps. Only marginal gains were achieved after this point and it declined in the case of the system trained using the covid_extended dataset.</p>
</div>
<div class="ltx_para" id="Ch4.S6.SS3.p4">
<p class="ltx_p" id="Ch4.S6.SS3.p4.1">Perplexity (PPL) shows how many different, equally probable words can be produced during translation. As a metric for translation performance, it is important to keep low scores so the number of alternative translations is reduced.</p>
</div>
<div class="ltx_para" id="Ch4.S6.SS3.p5">
<p class="ltx_p" id="Ch4.S6.SS3.p5.1">Of the models developed by the ADAPT team, the worst-performing model uses a larger 65k dataset. This is not surprising given the dataset is from a generic domain of which only 20% is health-related. The performance of this higher-resourced 65k line model lags the augmented <span class="ltx_text ltx_font_italic" id="Ch4.S6.SS3.p5.1.1">gaHealth</span> model which was developed using just 24k lines.</p>
</div>
<figure class="ltx_table" id="Ch4.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch4.T7.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch4.T7.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch4.T7.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T7.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch4.T7.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T7.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch4.T7.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch4.T7.1.1.1.m1.1"><semantics id="Ch4.T7.1.1.1.m1.1a"><mo id="Ch4.T7.1.1.1.m1.1.1" stretchy="false" xref="Ch4.T7.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch4.T7.1.1.1.m1.1b"><ci id="Ch4.T7.1.1.1.m1.1.1.cmml" xref="Ch4.T7.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T7.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T7.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T7.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T7.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch4.T7.2.2.2.m1.1"><semantics id="Ch4.T7.2.2.2.m1.1a"><mo id="Ch4.T7.2.2.2.m1.1.1" stretchy="false" xref="Ch4.T7.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch4.T7.2.2.2.m1.1b"><ci id="Ch4.T7.2.2.2.m1.1.1.cmml" xref="Ch4.T7.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T7.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T7.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch4.T7.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch4.T7.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch4.T7.3.3.3.m1.1"><semantics id="Ch4.T7.3.3.3.m1.1a"><mo id="Ch4.T7.3.3.3.m1.1.1" stretchy="false" xref="Ch4.T7.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch4.T7.3.3.3.m1.1b"><ci id="Ch4.T7.3.3.3.m1.1.1.cmml" xref="Ch4.T7.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T7.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T7.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T7.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T7.3.4.1.1">UCF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.3.4.1.2">ga2en-b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.3.4.1.3">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.3.4.1.4">0.711</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.3.4.1.5">0.45</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T7.3.5.2.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T7.3.5.2.2">ga2en-b</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.3.5.2.3">34.6</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.3.5.2.4">0.586</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.3.5.2.5">0.61</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch4.T7.3.6.3.1"><span class="ltx_text ltx_font_italic" id="Ch4.T7.3.6.3.1.1">gaHealth</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T7.3.6.3.2">ga2en</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T7.3.6.3.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.3.6.3.3.1">57.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T7.3.6.3.4">0.385</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch4.T7.3.6.3.5">0.71</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch4.T7.8.2.1" style="font-size:90%;">Table 4.7</span>: </span><span class="ltx_text" id="Ch4.T7.5.1" style="font-size:90%;">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.T7.5.1.m1.1"><semantics id="Ch4.T7.5.1.m1.1b"><mo id="Ch4.T7.5.1.m1.1.1" stretchy="false" xref="Ch4.T7.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.T7.5.1.m1.1c"><ci id="Ch4.T7.5.1.m1.1.1.cmml" xref="Ch4.T7.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T7.5.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.T7.5.1.m1.1e">→</annotation></semantics></math>EN <span class="ltx_text ltx_font_italic" id="Ch4.T7.5.1.1">gaHealth</span> system compared with LoResMT2021 systems.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S6.SS3.p6">
<p class="ltx_p" id="Ch4.S6.SS3.p6.2">For GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.SS3.p6.1.m1.1"><semantics id="Ch4.S6.SS3.p6.1.m1.1a"><mo id="Ch4.S6.SS3.p6.1.m1.1.1" stretchy="false" xref="Ch4.S6.SS3.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.SS3.p6.1.m1.1b"><ci id="Ch4.S6.SS3.p6.1.m1.1.1.cmml" xref="Ch4.S6.SS3.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.SS3.p6.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.SS3.p6.1.m1.1d">→</annotation></semantics></math>EN translation, the best-performing model for the LoResMT2021 Shared Task was developed by IIITT with a BLEU of 34.6, a TER of 0.586 and ChrF3: 0.6. This effectively serves as the baseline by which our GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S6.SS3.p6.2.m2.1"><semantics id="Ch4.S6.SS3.p6.2.m2.1a"><mo id="Ch4.S6.SS3.p6.2.m2.1.1" stretchy="false" xref="Ch4.S6.SS3.p6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S6.SS3.p6.2.m2.1b"><ci id="Ch4.S6.SS3.p6.2.m2.1.1.cmml" xref="Ch4.S6.SS3.p6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S6.SS3.p6.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S6.SS3.p6.2.m2.1d">→</annotation></semantics></math>EN model, developed using the <span class="ltx_text ltx_font_italic" id="Ch4.S6.SS3.p6.2.1">gaHealth</span> corpus, can be benchmarked. The performance of the <span class="ltx_text ltx_font_italic" id="Ch4.S6.SS3.p6.2.2">gaHealth</span> model offers an improvement across all metrics with a BLEU score of 57.6, a TER of 0.385 and a CHrF3 result of 0.71. In particular, the 40% improvement in BLEU score is very significant.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch4.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.7 </span>Discussion</h3>
<div class="ltx_para" id="Ch4.S7.p1">
<p class="ltx_p" id="Ch4.S7.p1.2">Although the main objective of this work is to develop the first bilingual corpus of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S7.p1.1.m1.1"><semantics id="Ch4.S7.p1.1.m1.1a"><mo id="Ch4.S7.p1.1.m1.1.1" stretchy="false" xref="Ch4.S7.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S7.p1.1.m1.1b"><ci id="Ch4.S7.p1.1.m1.1.1.cmml" xref="Ch4.S7.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S7.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S7.p1.1.m1.1d">↔</annotation></semantics></math>GA data, we conduct initial experiments on the effectiveness of such datasets in training MT models. We have used our <span class="ltx_text ltx_font_italic" id="Ch4.S7.p1.2.1">gaHealth</span> dataset to train an MT model on test data from the LoResMT2021 Shared task to evaluate how the system performs translating EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch4.S7.p1.2.m2.1"><semantics id="Ch4.S7.p1.2.m2.1a"><mo id="Ch4.S7.p1.2.m2.1.1" stretchy="false" xref="Ch4.S7.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch4.S7.p1.2.m2.1b"><ci id="Ch4.S7.p1.2.m2.1.1.cmml" xref="Ch4.S7.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S7.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S7.p1.2.m2.1d">↔</annotation></semantics></math>GA health data. Our systems, developed using the <span class="ltx_text ltx_font_italic" id="Ch4.S7.p1.2.2">gaHealth</span> corpus achieved significantly higher scores.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.8 </span>Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch4.S8.p1">
<p class="ltx_p" id="Ch4.S8.p1.1">The main contribution of this work is to present an ongoing translation project that aims at building the first ever parallel corpus of health data for the Irish language – <span class="ltx_text ltx_font_italic" id="Ch4.S8.p1.1.1">gaHealth</span> – by fully utilising freely available parallel documents.</p>
</div>
<div class="ltx_para" id="Ch4.S8.p2">
<p class="ltx_p" id="Ch4.S8.p2.1">Due to the issues encountered during the conversion of PDF documents, we developed guidelines to aid in the conversion process. In addition to developing the <span class="ltx_text ltx_font_italic" id="Ch4.S8.p2.1.1">gaHealth</span> corpus, we trained and evaluated translation models for in-domain health data.</p>
</div>
<div class="ltx_para" id="Ch4.S8.p3">
<p class="ltx_p" id="Ch4.S8.p3.2">In our experiments, the models achieved a BLEU score of 37.6 (Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T6" title="Table 4.6 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.6</span></a>) for translating EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S8.p3.1.m1.1"><semantics id="Ch4.S8.p3.1.m1.1a"><mo id="Ch4.S8.p3.1.m1.1.1" stretchy="false" xref="Ch4.S8.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S8.p3.1.m1.1b"><ci id="Ch4.S8.p3.1.m1.1.1.cmml" xref="Ch4.S8.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S8.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S8.p3.1.m1.1d">→</annotation></semantics></math>GA test data and 57.6 (Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch4.T7" title="Table 4.7 ‣ 4.6.3 Results: Automatic Evaluation ‣ 4.6 Empirical Evaluation ‣ Chapter 4 gaHealth: EN↔GA Bilingual Corpus of Health Data"><span class="ltx_text ltx_ref_tag">4.7</span></a>) for GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S8.p3.2.m2.1"><semantics id="Ch4.S8.p3.2.m2.1a"><mo id="Ch4.S8.p3.2.m2.1.1" stretchy="false" xref="Ch4.S8.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S8.p3.2.m2.1b"><ci id="Ch4.S8.p3.2.m2.1.1.cmml" xref="Ch4.S8.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S8.p3.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S8.p3.2.m2.1d">→</annotation></semantics></math>EN translation, which is encouraging performance given this is the beginning of our work on <span class="ltx_text ltx_font_italic" id="Ch4.S8.p3.2.1">gaHealth</span>. There is no such corpus available according to the best of our knowledge, so <span class="ltx_text ltx_font_italic" id="Ch4.S8.p3.2.2">gaHealth</span> will become a useful resource in the NLP community, especially for those working with the Irish language domain.</p>
</div>
<div class="ltx_para" id="Ch4.S8.p4">
<p class="ltx_p" id="Ch4.S8.p4.1">For future work, we intend to extend the corpus, as more Irish language documents become available. Upon extension, we will refine our models. One important aspect which needs further investigation is to understand why the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch4.S8.p4.1.m1.1"><semantics id="Ch4.S8.p4.1.m1.1a"><mo id="Ch4.S8.p4.1.m1.1.1" stretchy="false" xref="Ch4.S8.p4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch4.S8.p4.1.m1.1b"><ci id="Ch4.S8.p4.1.m1.1.1.cmml" xref="Ch4.S8.p4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S8.p4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch4.S8.p4.1.m1.1d">→</annotation></semantics></math>GA model (<span class="ltx_text ltx_font_italic" id="Ch4.S8.p4.1.1">gaHealth</span> en2ga), tested with the full test set, performed worse than the model (<span class="ltx_text ltx_font_italic" id="Ch4.S8.p4.1.2">gaHealth</span> en2ga*) which was tested with the reduced test set. A deep linguistic investigation involving a sentence-level BLEU analysis will be conducted as part of a future study.</p>
</div>
<div class="ltx_para" id="Ch4.S8.p5">
<p class="ltx_p" id="Ch4.S8.p5.1">In addition, we aim to build in-domain datasets for other key domains such as Education and Finance. We will also apply deep learning techniques to further refine our in-domain models. We have released the <span class="ltx_text ltx_font_italic" id="Ch4.S8.p5.1.1">gaHealth</span> corpus online<span class="ltx_note ltx_role_footnote" id="Ch4.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seamusl/gaHealth" title="">https://github.com/seamusl/gaHealth</a></span></span></span> to facilitate further research on this data set.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch5">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 5 </span>Human Evaluation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.1.m1.1"><semantics id="Ch5.1.m1.1b"><mo id="Ch5.1.m1.1.1" stretchy="false" xref="Ch5.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.1.m1.1c"><ci id="Ch5.1.m1.1.1.cmml" xref="Ch5.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.1.m1.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.1.m1.1e">↔</annotation></semantics></math>GA Transformer-Based NMT</h2>
<section class="ltx_section" id="Ch5.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.1 </span>Context</h3>
<div class="ltx_para" id="Ch5.S1.p1">
<p class="ltx_p" id="Ch5.S1.p1.1">In addressing RQ3, we employed a rigorous quantitative human evaluation of the translation outputs of RNN and Transformer models for a specific low-resource language pair. These methods and results are detailed in the paper <span class="ltx_text ltx_font_italic" id="Ch5.S1.p1.1.1">“Human evaluation of English–Irish Transformer-based NMT”</span>. Even though a different, human-centric approach was adopted, there is continuity with our previous research efforts. In particular, we sought a human validation of Transformer HPO for low-resource languages whereas in our previous work, <span class="ltx_text ltx_font_italic" id="Ch5.S1.p1.1.2">“Transformers for low-resource languages - is féidir linn!”</span>, the focus was on automatic metrics.</p>
</div>
<div class="ltx_para" id="Ch5.S1.p2">
<p class="ltx_p" id="Ch5.S1.p2.1">For our assessment, we employed an MQM error taxonomy complemented by an SQM to pinpoint the error types in both RNN and Transformer systems. Collaborating with native Irish speakers, our study devised a method for human evaluation under limited resources. Engaging native speakers to assess reference translations, we verified the outputs of both models. Our data reveals the Transformer system substantially reduces errors in accuracy and fluency. A pivotal aspect of our research is the linguistic feedback from our annotators.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch5.S1.p3">
<p class="ltx_p" id="Ch5.S1.p3.1"><span class="ltx_text ltx_font_bold" id="Ch5.S1.p3.1.1">Human Evaluation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S1.p3.1.1.m1.1"><semantics id="Ch5.S1.p3.1.1.m1.1a"><mo id="Ch5.S1.p3.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch5.S1.p3.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.p3.1.1.m1.1b"><ci id="Ch5.S1.p3.1.1.m1.1.1.cmml" xref="Ch5.S1.p3.1.1.m1.1.1">normal-↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.p3.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.p3.1.1.m1.1d">↔</annotation></semantics></math>GA Transformer-Based NMT</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch5.S1.p3.2"><span class="ltx_text ltx_font_bold" id="Ch5.S1.p3.2.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch5.S1.p3.3"><span class="ltx_text ltx_font_bold" id="Ch5.S1.p3.3.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch5.S1.p3.4"><span class="ltx_text ltx_font_bold" id="Ch5.S1.p3.4.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch5.S1.p3.5">Information 13.7, MDPI</p>
<p class="ltx_p" id="Ch5.S1.p3.6">January 10th, 2023</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch5.S1.p3.7">ADAPT Centre</p>
<p class="ltx_p" id="Ch5.S1.p3.8">Dublin City University</p>
<p class="ltx_p" id="Ch5.S1.p3.9">Ireland</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch5.S1.p3.10"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2078-2489/13/7/309" title="">https://www.mdpi.com/2078-2489/13/7/309</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch5.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch5.S2.p1">
<p class="ltx_p" id="Ch5.S2.p1.1">In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based neural machine translation for the low-resource English-Irish language pair. SentencePiece models using both byte pair encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimised model with a 16k BPE subword model. Compared with a baseline recurrent neural network (RNN) model, a Transformer-optimised model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine translation systems. Using the multidimensional quality metrics error taxonomy, a human evaluation of the error types generated by an RNN-based system and a Transformer-based system was explored. Our findings show the best-performing Transformer system significantly reduces both accuracy and fluency errors when compared with an RNN-based model.</p>
</div>
</section>
<section class="ltx_section" id="Ch5.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.3 </span>Introduction</h3>
<div class="ltx_para" id="Ch5.S3.p1">
<p class="ltx_p" id="Ch5.S3.p1.1">A new era of high-quality translations has been heralded with the advent of neural machine translation (NMT). Given that large datasets are a prerequisite for high-quality NMT, these improvements are not always evident in the translation of low-resource languages. In the context of such languages, which suffer from a sparsity of data, alternative approaches must be adopted.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p2">
<p class="ltx_p" id="Ch5.S3.p2.1">Developing applications and models to address the challenges of low-resource language technology is an important part of this research. This technology incorporates new methods, which reduce the impact that data scarcity has on the digital engagement of low-resource languages. One approach is to use a mechanism that helps NMT systems to learn from unlabelled data using dual-learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p3">
<p class="ltx_p" id="Ch5.S3.p3.2">Out-of-the-box NMT systems, trained on English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S3.p3.1.m1.1"><semantics id="Ch5.S3.p3.1.m1.1a"><mo id="Ch5.S3.p3.1.m1.1.1" stretchy="false" xref="Ch5.S3.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S3.p3.1.m1.1b"><ci id="Ch5.S3.p3.1.m1.1.1.cmml" xref="Ch5.S3.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S3.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S3.p3.1.m1.1d">↔</annotation></semantics></math>Irish (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S3.p3.2.m2.1"><semantics id="Ch5.S3.p3.2.m2.1a"><mo id="Ch5.S3.p3.2.m2.1.1" stretchy="false" xref="Ch5.S3.p3.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S3.p3.2.m2.1b"><ci id="Ch5.S3.p3.2.m2.1.1.cmml" xref="Ch5.S3.p3.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S3.p3.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S3.p3.2.m2.1d">↔</annotation></semantics></math>GA) data, have been shown to achieve a lower translation quality compared with using a tailored SMT system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx37" title="">37</a>]</cite>. It is in this context that further research is required in the development of NMT for low-resource languages, and the Irish language in particular.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p4">
<p class="ltx_p" id="Ch5.S3.p4.1">Most research on the choice of subword models has focused on high-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx43" title="">43</a>]</cite>. Translation, by its nature, requires an open vocabulary and the use of subword models aims to address the fixed-vocabulary problem associated with NMT. Rare and unknown words are encoded as sequences of subword units. By adapting the original BPE algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx41" title="">41</a>]</cite>, the use of BPE subword models can improve translation performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx60" title="">60</a>]</cite>. In the context of developing models for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S3.p4.1.m1.1"><semantics id="Ch5.S3.p4.1.m1.1a"><mo id="Ch5.S3.p4.1.m1.1.1" stretchy="false" xref="Ch5.S3.p4.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S3.p4.1.m1.1b"><ci id="Ch5.S3.p4.1.m1.1.1.cmml" xref="Ch5.S3.p4.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S3.p4.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S3.p4.1.m1.1d">↔</annotation></semantics></math>GA translation, there were no clear recommendations on the choice of subword model types. Character-based models were briefly explored due to their simplicity and reduced memory requirements. However, they were not considered suitable given that most single characters do not carry meaning in the English and Irish languages. Therefore, one of the objectives of our research is to identify which type of subword model performs best in this low-resource scenario.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p5">
<p class="ltx_p" id="Ch5.S3.p5.1">An important goal of this study is to extend our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite> by providing a human evaluation and comparison of English to Irish (EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch5.S3.p5.1.m1.1"><semantics id="Ch5.S3.p5.1.m1.1a"><mo id="Ch5.S3.p5.1.m1.1.1" stretchy="false" xref="Ch5.S3.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch5.S3.p5.1.m1.1b"><ci id="Ch5.S3.p5.1.m1.1.1.cmml" xref="Ch5.S3.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S3.p5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S3.p5.1.m1.1d">→</annotation></semantics></math>GA) machine translation (MT) on systems that use either a baseline RNN architecture or a subword-model optimised Transformer model.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p6">
<p class="ltx_p" id="Ch5.S3.p6.1">This paper describes the context in which our research was conducted and provides a background of the types of available architecture in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S4" title="5.4 Background ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.4</span></a>. A detailed overview of our approach is outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S5" title="5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.5</span></a>, where we provide details of the data and parameters used in our NMT systems. The empirical results, using both automatic metrics and a human evaluation, are presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S6" title="5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.6</span></a>. Finally, our findings are discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S8" title="5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.8</span></a> and the possibilities for future work are outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.S9" title="5.9 Conclusions and Future Work ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.9</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="Ch5.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.4 </span>Background</h3>
<div class="ltx_para" id="Ch5.S4.p1">
<p class="ltx_p" id="Ch5.S4.p1.1">Native speakers of low-resource languages are often excluded from useful content since, more often than not, online content is not available to them in their language of choice. This digital divide experienced by second-language speakers has been well-documented in the research literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch5.S4.p2">
<p class="ltx_p" id="Ch5.S4.p2.1">Research on MT in low-resource scenarios seeks to directly address this challenge of exclusion via pivot languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx72" title="">72</a>]</cite>, and indirectly, via domain adaptation of models  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx42" title="">42</a>]</cite>. Consequently, research efforts focusing on NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx28" title="">28</a>]</cite> have resulted in a state-of-the-art (SOTA) performance being attained for multiple language pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx21" title="">21</a>]</cite>. The Irish language is a primary example of a low-resource language that will benefit from this research. NMT involving Transformer model development will improve performance in specific domains of low-resource languages.</p>
</div>
<section class="ltx_subsection" id="Ch5.S4.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4.1 </span>Hyperparameter Optimisation</h4>
<div class="ltx_para" id="Ch5.S4.SS1.p1">
<p class="ltx_p" id="Ch5.S4.SS1.p1.1">Hyperparameters are employed to customise machine learning models such as translation models. It has been shown that machine learning performance may be improved through hyperparameter optimisation (HPO) rather than just using default settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx98" title="">98</a>]</cite>. The principal methods of HPO are grid search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx81" title="">81</a>]</cite> and random search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx16" title="">16</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="Ch5.S4.SS1.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">RNN</h5>
<div class="ltx_para" id="Ch5.S4.SS1.SSSx1.p1">
<p class="ltx_p" id="Ch5.S4.SS1.SSSx1.p1.1">The tasks of natural language processing (NLP), speech recognition and MT are often performed by RNNs. This architecture enables previous outputs to be used as inputs while having hidden states. In the context of MT, such neural networks were ideal due to their ability to process inputs of any length. Furthermore, the model sizes do not necessarily increase with the input size. Commonly used variants of RNN include bidirectional (BRNN) and deep (DRNN) architectures. However, the problem of vanishing gradients coupled with the development of attention-based algorithms often leads to Transformer models performing better than RNNs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S4.SS1.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Transformer</h5>
<div class="ltx_para" id="Ch5.S4.SS1.SSSx2.p1">
<p class="ltx_p" id="Ch5.S4.SS1.SSSx2.p1.1">The greatest improvements have been demonstrated when either the RNN or the CNN architecture is abandoned completely and replaced with an attention mechanism creating a much simpler and faster architecture known as Transformer. Experiments in MT tasks show such models are better in quality due to greater parallelisation while requiring significantly less time to train <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch5.S4.SS1.SSSx2.p2">
<p class="ltx_p" id="Ch5.S4.SS1.SSSx2.p2.1">Transformer models use attention to focus on previously generated tokens. The approach allows for models to develop a long memory, which is particularly useful in the domain of language translation. Performance improvements to both RNN and CNN approaches may be achieved through the introduction of such attention layers in the translation architecture.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch5.S4.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4.2 </span>SentencePiece</h4>
<div class="ltx_para" id="Ch5.S4.SS2.p1">
<p class="ltx_p" id="Ch5.S4.SS2.p1.1">Designed for NMT, SentencePiece is a language-independent subword tokenizer that provides an open-source C++ and a Python implementation for subword units. An attractive feature of the tokenizer is that SentencePiece directly trains subword models from raw sentences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch5.S4.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4.3 </span>Human Evaluation</h4>
<div class="ltx_para" id="Ch5.S4.SS3.p1">
<p class="ltx_p" id="Ch5.S4.SS3.p1.1">Human evaluation, within NLP and MT, is a topic of growing importance, which often has a dedicated research track or workshop at major conferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx14" title="">14</a>]</cite>. This focus has resulted in many publications in the area of human evaluation that relate to MT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx26" title="">26</a>]</cite> and it has particularly benefited the evaluation of low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx50" title="">50</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch5.S4.SS3.p2">
<p class="ltx_p" id="Ch5.S4.SS3.p2.1">The best practice for the human evaluation of MT has been published in the form of a series of recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx70" title="">70</a>]</cite>. As part of our research, we adopted these recommendations, which are in line with similar human evaluation studies of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S4.SS3.p2.1.m1.1"><semantics id="Ch5.S4.SS3.p2.1.m1.1a"><mo id="Ch5.S4.SS3.p2.1.m1.1.1" stretchy="false" xref="Ch5.S4.SS3.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S4.SS3.p2.1.m1.1b"><ci id="Ch5.S4.SS3.p2.1.m1.1.1.cmml" xref="Ch5.S4.SS3.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S4.SS3.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S4.SS3.p2.1.m1.1d">↔</annotation></semantics></math>GA MT at the ADAPT centre <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx38" title="">38</a>]</cite>. Specifically, these recommendations encourage the use of professional translators, evaluation at the document level and assessments of both fluency and accuracy. Original source texts were also used in the training and test data.</p>
</div>
<div class="ltx_para" id="Ch5.S4.SS3.p3">
<p class="ltx_p" id="Ch5.S4.SS3.p3.1">These recommendations have been complemented by a fine-grained human analysis, which uses both a Scalar Quality Metric (SQM) and Multidimensional Quality Metrics (MQM).</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch5.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.5 </span>Proposed Approach</h3>
<div class="ltx_para" id="Ch5.S5.p1">
<p class="ltx_p" id="Ch5.S5.p1.1">Considerable performance improvements have been achieved using the HPO of RNN models in low-resource settings. One of the key research questions, evaluated as part of this study, is to identify the extent to which such optimisation techniques may be applied to low-resource Transformer models. Evaluations included modifying the number of attention heads, changing the number of layers and experimenting with regularisation techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and vocabulary size is evaluated. Furthermore, previous research focuses on using an automatic evaluation of performance, whereas we propose combining a human evaluation approach with automatic metrics.</p>
</div>
<div class="ltx_para" id="Ch5.S5.p2">
<p class="ltx_p" id="Ch5.S5.p2.1">To test the effectiveness of our approach, optimisation was carried out on an EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S5.p2.1.m1.1"><semantics id="Ch5.S5.p2.1.m1.1a"><mo id="Ch5.S5.p2.1.m1.1.1" stretchy="false" xref="Ch5.S5.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S5.p2.1.m1.1b"><ci id="Ch5.S5.p2.1.m1.1.1.cmml" xref="Ch5.S5.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S5.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S5.p2.1.m1.1d">↔</annotation></semantics></math>GA parallel dataset: a general corpus of 52k lines from the Directorate General for Translation (DGT). With DGT, the test set used 1.3k lines and the development set comprised 2.6k lines. All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece subword model. The adopted approach is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F1" title="Figure 5.1 ‣ 5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<section class="ltx_subsection" id="Ch5.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5.1 </span>Architecture Tuning</h4>
<div class="ltx_para" id="Ch5.S5.SS1.p1">
<p class="ltx_p" id="Ch5.S5.SS1.p1.1">It is difficult and costly to tune systems using a conventional grid search approach given the long training times associated with NMT. Therefore, we adopted a random search approach in the HPO of our Transformer models.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS1.p2">
<p class="ltx_p" id="Ch5.S5.SS1.p2.1">Using smaller and fewer layers with low-resource datasets has previously been shown to improve performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx5" title="">5</a>]</cite>. Furthermore, the use of shallow Transformer models has been demonstrated to improve the translation performance of low-resource NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx17" title="">17</a>]</cite>. Guided by these findings, configurations were tested, which varied the number of neurons in each layer and modified the number of layers used in the Transformer architecture.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS1.p3">
<p class="ltx_p" id="Ch5.S5.SS1.p3.1">Varying degrees of dropout were applied to Transformer models to evaluate the impact of regularisation. Configurations using smaller (0.1) and larger values (0.3) were applied to the output of each feed-forward layer.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch5.S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5.2 </span>Subword Models</h4>
<div class="ltx_para" id="Ch5.S5.SS2.p1">
<p class="ltx_p" id="Ch5.S5.SS2.p1.1">Incorporating a word segmentation approach, such as BPE, is now standard practice when developing NMT models. Subword models are particularly beneficial for low-resource languages since rare words are often a problem. In the context of EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch5.S5.SS2.p1.1.m1.1"><semantics id="Ch5.S5.SS2.p1.1.m1.1a"><mo id="Ch5.S5.SS2.p1.1.m1.1.1" stretchy="false" xref="Ch5.S5.SS2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch5.S5.SS2.p1.1.m1.1b"><ci id="Ch5.S5.SS2.p1.1.m1.1.1.cmml" xref="Ch5.S5.SS2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S5.SS2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S5.SS2.p1.1.m1.1d">→</annotation></semantics></math>GA translation, there is no clear agreement as to what constitutes the best approach. Consequently, subword regularisation techniques involving BPE and unigram models were evaluated as part of this study to determine the optimal parameters for maximising translation performance. BPE models with varying vocabulary sizes of 4k, 8k, 16k and 32k were evaluated.
</p>
</div>
<figure class="ltx_figure" id="Ch5.F1">
<br class="ltx_break ltx_centering"/>
<br class="ltx_break ltx_centering"/><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch5.F1.g1" src="frontiers.png"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F1.2.1.1" style="font-size:90%;">Figure 5.1</span>: </span><span class="ltx_text" id="Ch5.F1.3.2" style="font-size:90%;">Proposed approach to evaluate the baseline architectures of RNN and Transformer models. Using a random search approach, the values outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T1" title="Table 5.1 ‣ 5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.1</span></a> were tested to determine the optimal hyperparameters. Short cycles of 5k training steps were applied to test a range of values for each parameter. Once an optimal value was identified within the sampled range, it was locked in for tests on subsequent parameters. A fine-grained human evaluation was conducted on the output from the DGT dataset and its results were compared with an automatic evaluation.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch5.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.1: </span>Transformer HPO using a random search approach. The optimal hyperparameters are highlighted in bold. The best-performing model used two attention heads and was trained on a 55k DGT corpus.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T1.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T1.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch5.T1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.1.1.1.1" style="font-size:90%;">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch5.T1.4.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.1.1.2.1" style="font-size:90%;">Values</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch5.T1.4.2.2.1"><span class="ltx_text" id="Ch5.T1.4.2.2.1.1" style="font-size:90%;">Learning rate</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch5.T1.4.2.2.2">
<span class="ltx_text" id="Ch5.T1.4.2.2.2.1" style="font-size:90%;">0.1, 0.01, 0.001, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.2.2.2.2" style="font-size:90%;">2</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.3.3.1"><span class="ltx_text" id="Ch5.T1.4.3.3.1.1" style="font-size:90%;">Batch size</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.3.3.2">
<span class="ltx_text" id="Ch5.T1.4.3.3.2.1" style="font-size:90%;">1024, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.3.3.2.2" style="font-size:90%;">2048</span><span class="ltx_text" id="Ch5.T1.4.3.3.2.3" style="font-size:90%;">, 4096, 8192</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.4.4.1"><span class="ltx_text" id="Ch5.T1.4.4.4.1.1" style="font-size:90%;">Attention heads</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.4.4.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T1.4.4.4.2.1" style="font-size:90%;">2</span><span class="ltx_text" id="Ch5.T1.4.4.4.2.2" style="font-size:90%;">, 4, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.4.4.2.3" style="font-size:90%;">8</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.5.5.1"><span class="ltx_text" id="Ch5.T1.4.5.5.1.1" style="font-size:90%;">Number of layers</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.5.5.2">
<span class="ltx_text" id="Ch5.T1.4.5.5.2.1" style="font-size:90%;">5, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.5.5.2.2" style="font-size:90%;">6</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.6.6.1"><span class="ltx_text" id="Ch5.T1.4.6.6.1.1" style="font-size:90%;">Feed-forward dimension</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.6.6.2"><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.6.6.2.1" style="font-size:90%;">2048</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.7.7.1"><span class="ltx_text" id="Ch5.T1.4.7.7.1.1" style="font-size:90%;">Embedding dimension</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.7.7.2">
<span class="ltx_text" id="Ch5.T1.4.7.7.2.1" style="font-size:90%;">128, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.7.7.2.2" style="font-size:90%;">256</span><span class="ltx_text" id="Ch5.T1.4.7.7.2.3" style="font-size:90%;">, 512</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.8.8.1"><span class="ltx_text" id="Ch5.T1.4.8.8.1.1" style="font-size:90%;">Label smoothing</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.8.8.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T1.4.8.8.2.1" style="font-size:90%;">0.1</span><span class="ltx_text" id="Ch5.T1.4.8.8.2.2" style="font-size:90%;">, 0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.9.9.1"><span class="ltx_text" id="Ch5.T1.4.9.9.1.1" style="font-size:90%;">Dropout</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.9.9.2">
<span class="ltx_text" id="Ch5.T1.4.9.9.2.1" style="font-size:90%;">0.1, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.9.9.2.2" style="font-size:90%;">0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T1.4.10.10.1"><span class="ltx_text" id="Ch5.T1.4.10.10.1.1" style="font-size:90%;">Attention dropout</span></th>
<td class="ltx_td ltx_align_left" id="Ch5.T1.4.10.10.2"><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.10.10.2.1" style="font-size:90%;">0.1</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch5.T1.4.11.11.1"><span class="ltx_text" id="Ch5.T1.4.11.11.1.1" style="font-size:90%;">Average Decay</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch5.T1.4.11.11.2">
<span class="ltx_text" id="Ch5.T1.4.11.11.2.1" style="font-size:90%;">0, </span><span class="ltx_text ltx_font_bold" id="Ch5.T1.4.11.11.2.2" style="font-size:90%;">0.0001</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="Ch5.S5.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5.3 </span>Human Evaluation of NMT</h4>
<div class="ltx_para" id="Ch5.S5.SS3.p1">
<p class="ltx_p" id="Ch5.S5.SS3.p1.1">Morphological-rich languages, such as Irish, have a high degree of inflection and free word order that gives rise to specific translation issues when translating from English. Grammatical categories such as gender or case inflections in nouns are often difficult to reliably generate in an Irish translation.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.p2">
<p class="ltx_p" id="Ch5.S5.SS3.p2.1">One of the goals of this research is to explore how an NMT system handles these issues compared with an RNN approach. Existing research suggests NMT systems should improve these linguistic aspects. NMT, with its use of subword models, implicitly addresses the problem in an unsupervised manner, without understanding the actual formal rules of grammatical categories.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.p3">
<p class="ltx_p" id="Ch5.S5.SS3.p3.1">Previous human evaluation studies that evaluate EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S5.SS3.p3.1.m1.1"><semantics id="Ch5.S5.SS3.p3.1.m1.1a"><mo id="Ch5.S5.SS3.p3.1.m1.1.1" stretchy="false" xref="Ch5.S5.SS3.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S5.SS3.p3.1.m1.1b"><ci id="Ch5.S5.SS3.p3.1.m1.1.1.cmml" xref="Ch5.S5.SS3.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S5.SS3.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S5.SS3.p3.1.m1.1d">↔</annotation></semantics></math>GA MT performance have focused on the differences between an SMT and an NMT approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx37" title="">37</a>]</cite>. In the context of our research, human evaluation was conducted on purely NMT methods, which included RNN and Transformer approaches. Furthermore, our study is differentiated by using both SQM and MQM as our human evaluation metrics.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.p4">
<p class="ltx_p" id="Ch5.S5.SS3.p4.1">It is clear from our earlier experimental findings, based solely on automatic evaluation metrics, that a Transformer approach leads to significant improvements compared to traditional RNN systems. However, as with most automatic scoring methods, these simply provide an overall score for each system but do not indicate the exact nature of the linguistic problems that may be encountered in translation. Therefore, it can be said that automatic evaluation does not address the question of the linguistic or grammatical quality of the target output. Nuances, such as how gender or cases are handled, are not covered by this approach.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.p5">
<p class="ltx_p" id="Ch5.S5.SS3.p5.1">To achieve a deeper understanding of the linguistic errors created by our RNN and Transformer systems, a fine-grained human evaluation was conducted. The outputs from these systems were systematically analysed and compared in a manual error analysis. This approach captures the nature of the translation errors for each of the evaluated systems. The output from this study forms the basis of future work, which will help to improve the translation quality of our models. The annotation framework, the overall annotation process and the inter-annotator agreement are discussed below and broadly follow the approach adopted by other fine-grained human evaluation studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx57" title="">57</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="Ch5.S5.SS3.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Scalar Quality Metrics</h5>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx1.p1">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx1.p1.1">SQM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite> adapts the WMT shared-task settings to collect segment-level scalar ratings with a document context. SQM uses a scale from 0 to 6 for translation quality assessment. This is a modification of the WMT approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx77" title="">77</a>]</cite>, which uses a range from 0 to 100.
</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx1.p2">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx1.p2.1">With this evaluation approach, annotators must select a rating from 0 through 6 when presented with the source and target sentences. The SQM quality levels for 0, 2, 4 and 6 are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T2" title="Table 5.2 ‣ Scalar Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.2</span></a>. Annotators may also choose intermediate levels of 1, 3 and 5 in cases where the translations do not exactly match the core SQM levels.</p>
</div>
<figure class="ltx_table" id="Ch5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T2.2.1.1" style="font-size:90%;">Table 5.2</span>: </span><span class="ltx_text" id="Ch5.T2.3.2" style="font-size:90%;">SQM levels explained <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T2.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch5.T2.4.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.1.1.1">SQM Level</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T2.4.1.1.2" style="width:341.4pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T2.4.1.1.2.1">Details of Quality</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T2.4.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch5.T2.4.2.1.1"><span class="ltx_text" id="Ch5.T2.4.2.1.1.1">6</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T2.4.2.1.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T2.4.2.1.2.1">Perfect Meaning and Grammar: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable). The grammar is also correct.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch5.T2.4.3.2.1"><span class="ltx_text" id="Ch5.T2.4.3.2.1.1">4</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T2.4.3.2.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T2.4.3.2.2.1">Most Meaning Preserved and Few Grammar Mistakes: The translation retains most of the meaning of the source. This may contain some grammar mistakes or minor contextual inconsistencies.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch5.T2.4.4.3.1"><span class="ltx_text" id="Ch5.T2.4.4.3.1.1">2</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T2.4.4.3.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T2.4.4.3.2.1">Some Meaning Preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow due to fundamental errors. Grammar may be poor.</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch5.T2.4.5.4.1" rowspan="2"><span class="ltx_text" id="Ch5.T2.4.5.4.1.1">0</span></th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T2.4.5.4.2" style="width:341.4pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T2.4.5.4.2.1">Nonsense/No meaning preserved: Nearly all information is lost between the translation and source. Grammar is irrelevant.</p>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch5.S5.SS3.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Multidimensional Quality Metrics</h5>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx2.p1">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx2.p1.1">As part of QTLaunchpad project<span class="ltx_note ltx_role_footnote" id="Ch5.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.qt21.eu/" title="">https://www.qt21.eu/</a></span></span></span> the MQM framework<span class="ltx_note ltx_role_footnote" id="Ch5.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://themqm.org/the-mqm-full-typology/" title="">https://themqm.org/the-mqm-full-typology/</a></span></span></span> was developed to provide a framework of how manual evaluation could be performed via a detailed error analysis. A single metric for all uses is not imposed. Instead, a comprehensive catalogue of quality issue types, with standardised names and definitions, is provided. This catalogue may be customised for specific tasks. In addition to forming a reliable methodology for quality assessment, it also allows for us to specify which error tags were relevant to our task.
</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx2.p2">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx2.p2.1">To adapt the generic MQM framework for our context, we followed the official guidelines for scientific research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx74" title="">74</a>]</cite>. The details of our customisation of MQM are discussed below.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx2.p3">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx2.p3.1">A large variety of tags, on several annotation layers, are proposed within the original MQM guidelines. However, this full MQM tagset is too detailed for a specific annotation task. Therefore, when evaluating our MT output, the smaller default set of evaluation categories, specified in the core tagset, was used. These standard top-level categories of accuracy and fluency, which are proposed by the MQM guidelines, are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F2" title="Figure 5.2 ‣ Multidimensional Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.2</span></a>. A special non-translation error was used to tag an entire sentence, which was too badly translated to allow for the identification of individual errors.</p>
</div>
<figure class="ltx_figure" id="Ch5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch5.F2.g1" src="mqm_core.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F2.2.1.1" style="font-size:90%;">Figure 5.2</span>: </span><span class="ltx_text" id="Ch5.F2.3.2" style="font-size:90%;">The core set of error categories proposed by the MQM guidelines.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx2.p4">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx2.p4.1">Error severities are specified as either major or minor errors and are assigned independently of category. These correspond to actual translation and grammatical errors or smaller imperfections, respectively.
The recommended default weights <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx74" title="">74</a>]</cite> were used, which allocate a weight of 1 to minor errors whereas major errors are assigned a weight of 10. Furthermore, the non-translation category was allocated a weight of 25, an approach which is in line with the best practice established in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx2.p5">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx2.p5.1">The annotators were instructed to identify all errors within each sentence of the translated output for both systems. The error categories used by the annotators are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T3" title="Table 5.3 ‣ Multidimensional Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch5.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.3: </span>Description of error categories within the core MQM framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch5.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T3.4.1.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.1.1.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T3.4.1.1.1.1" style="font-size:90%;">Category</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.1.1.2" style="width:93.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T3.4.1.1.2.1" style="font-size:90%;">Sub-Category</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.1.1.3" style="width:220.5pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T3.4.1.1.3.1" style="font-size:90%;">Description</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.2.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.2.2.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T3.4.2.2.1.1" style="font-size:90%;">Non-translation</span></td>
<td class="ltx_td ltx_border_t" id="Ch5.T3.4.2.2.2" style="width:93.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.2.2.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.2.2.3.1"><span class="ltx_text" id="Ch5.T3.4.2.2.3.1.1" style="font-size:90%;">Impossible to reliably characterise the 5 most severe errors.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.3.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.3.3.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T3.4.3.3.1.1" style="font-size:90%;">Accuracy</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.3.3.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.3.3.2.1"><span class="ltx_text" id="Ch5.T3.4.3.3.2.1.1" style="font-size:90%;">Addition</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.3.3.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.3.3.3.1"><span class="ltx_text" id="Ch5.T3.4.3.3.3.1.1" style="font-size:90%;">Translation includes information not present in the source.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.4.4">
<td class="ltx_td" id="Ch5.T3.4.4.4.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.4.4.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.4.4.2.1"><span class="ltx_text" id="Ch5.T3.4.4.4.2.1.1" style="font-size:90%;">Omission</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.4.4.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.4.4.3.1"><span class="ltx_text" id="Ch5.T3.4.4.4.3.1.1" style="font-size:90%;">Translation is missing content from the source.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.5.5">
<td class="ltx_td" id="Ch5.T3.4.5.5.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.5.5.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.5.5.2.1"><span class="ltx_text" id="Ch5.T3.4.5.5.2.1.1" style="font-size:90%;">Mistranslation</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.5.5.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.5.5.3.1"><span class="ltx_text" id="Ch5.T3.4.5.5.3.1.1" style="font-size:90%;">Translation does not accurately represent the source.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.6.6">
<td class="ltx_td" id="Ch5.T3.4.6.6.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.6.6.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.6.6.2.1"><span class="ltx_text" id="Ch5.T3.4.6.6.2.1.1" style="font-size:90%;">Untranslated text</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.6.6.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.6.6.3.1"><span class="ltx_text" id="Ch5.T3.4.6.6.3.1.1" style="font-size:90%;">Source text has been left untranslated.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.7.7">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.7.7.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T3.4.7.7.1.1" style="font-size:90%;">Fluency</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.7.7.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.7.7.2.1"><span class="ltx_text" id="Ch5.T3.4.7.7.2.1.1" style="font-size:90%;">Punctuation</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T3.4.7.7.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.7.7.3.1"><span class="ltx_text" id="Ch5.T3.4.7.7.3.1.1" style="font-size:90%;">Incorrect punctuation</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.8.8">
<td class="ltx_td" id="Ch5.T3.4.8.8.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.8.8.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.8.8.2.1"><span class="ltx_text" id="Ch5.T3.4.8.8.2.1.1" style="font-size:90%;">Spelling</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.8.8.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.8.8.3.1"><span class="ltx_text" id="Ch5.T3.4.8.8.3.1.1" style="font-size:90%;">Incorrect spelling or capitalisation.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.9.9">
<td class="ltx_td" id="Ch5.T3.4.9.9.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.9.9.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.9.9.2.1"><span class="ltx_text" id="Ch5.T3.4.9.9.2.1.1" style="font-size:90%;">Grammar</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.9.9.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.9.9.3.1"><span class="ltx_text" id="Ch5.T3.4.9.9.3.1.1" style="font-size:90%;">Problems with grammar, other than orthography.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.10.10">
<td class="ltx_td" id="Ch5.T3.4.10.10.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.10.10.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.10.10.2.1"><span class="ltx_text" id="Ch5.T3.4.10.10.2.1.1" style="font-size:90%;">Register</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.10.10.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.10.10.3.1"><span class="ltx_text" id="Ch5.T3.4.10.10.3.1.1" style="font-size:90%;">Wrong grammatical register (e.g., inappropriately informal pronouns).</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.11.11">
<td class="ltx_td" id="Ch5.T3.4.11.11.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.11.11.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.11.11.2.1"><span class="ltx_text" id="Ch5.T3.4.11.11.2.1.1" style="font-size:90%;">Inconsistency</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch5.T3.4.11.11.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.11.11.3.1"><span class="ltx_text" id="Ch5.T3.4.11.11.3.1.1" style="font-size:90%;">Internal inconsistency (not related to terminology).</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T3.4.12.12">
<td class="ltx_td ltx_border_b" id="Ch5.T3.4.12.12.1" style="width:76.8pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_b" id="Ch5.T3.4.12.12.2" style="width:93.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.12.12.2.1"><span class="ltx_text" id="Ch5.T3.4.12.12.2.1.1" style="font-size:90%;">Character encoding</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b" id="Ch5.T3.4.12.12.3" style="width:220.5pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T3.4.12.12.3.1"><span class="ltx_text" id="Ch5.T3.4.12.12.3.1.1" style="font-size:90%;">Characters are garbled due to incorrect encoding.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch5.S5.SS3.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Annotation Setup</h5>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx3.p1">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx3.p1.1">Annotations were carried out using the simpler SQM approach and a more detailed, fine-grained MQM approach. The hierarchical taxonomy of our MQM implementation is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F2" title="Figure 5.2 ‣ Multidimensional Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.2</span></a>, whereas the SQM categories are summarised in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T2" title="Table 5.2 ‣ Scalar Quality Metrics ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx3.p2">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx3.p2.1">Two annotators with similar backgrounds were used for the annotation of outputs from an RNN system and a Transformer system. Both annotators are native speakers of Irish and neither had prior experience with MQM. Prior to annotation, they were thoroughly familiarised with the process and the official MQM annotation guidelines. These guidelines offer detailed instructions for annotation within the MQM framework.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx3.p3">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx3.p3.1">Both annotators have been very involved in the education sector for decades. One of the annotators has edited numerous English-language and Irish-language books during her career as a university lecturer. The second annotator has a PhD in Irish-language place names. In addition, he has written numerous books in both English and Irish. Given their experience and strong language backgrounds, they were well-equipped to handle the task at hand.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx3.p4">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx3.p4.1">Using a test set of 20 randomly selected sentences, the annotators were presented with the English source text, an Irish reference translation and the two unannotated system outputs: one generated using an RNN model and the other created using a Transformer model. Potential bias was removed by using blind annotation such that annotators did not know which model the translation output came from. The annotators worked independently of each other but were occasionally in contact to discuss the process and how to approach difficult sentences.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx3.p5">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx3.p5.1">Translations from the RNN and the Transformer system were annotated by both annotators, meaning that each system translated the same 20 sentences and each annotator annotated the resulting 40 translated sentences (20 source sentences for 2 MT systems), producing a total of 80 annotated sentences. The annotated dataset is publicly available on GitHub.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seamusl/isfeidirlinn" title="">https://github.com/seamusl/isfeidirlinn</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx3.p6">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx3.p6.1">Once the annotation data were extracted, each annotator analysed the output to determine the performance of each system for each error category.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S5.SS3.SSSx4">
<h5 class="ltx_title ltx_title_subsubsection">Inter-Annotator Agreement</h5>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx4.p1">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx4.p1.1">Low inter-annotator agreement (IAA) scores are a common problem experienced when using manual MT evaluation approaches such as MQM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx25" title="">25</a>]</cite>.To determine the validity of the findings of our research, it is important to check the level of agreement between our annotators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch5.S5.SS3.SSSx4.p2">
<p class="ltx_p" id="Ch5.S5.SS3.SSSx4.p2.3">Cohen’s <math alttext="kappa" class="ltx_Math" display="inline" id="Ch5.S5.SS3.SSSx4.p2.1.m1.1"><semantics id="Ch5.S5.SS3.SSSx4.p2.1.m1.1a"><mrow id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.cmml"><mi id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.2" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.2.cmml">k</mi><mo id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.3" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.3.cmml">a</mi><mo id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1a" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.4" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.4.cmml">p</mi><mo id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1b" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.5" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.5.cmml">p</mi><mo id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1c" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.6" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch5.S5.SS3.SSSx4.p2.1.m1.1b"><apply id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1"><times id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.1"></times><ci id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.2.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.2">𝑘</ci><ci id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.3.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.3">𝑎</ci><ci id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.4.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.4">𝑝</ci><ci id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.5.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.5">𝑝</ci><ci id="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.6.cmml" xref="Ch5.S5.SS3.SSSx4.p2.1.m1.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S5.SS3.SSSx4.p2.1.m1.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch5.S5.SS3.SSSx4.p2.1.m1.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> (<math alttext="k" class="ltx_Math" display="inline" id="Ch5.S5.SS3.SSSx4.p2.2.m2.1"><semantics id="Ch5.S5.SS3.SSSx4.p2.2.m2.1a"><mi id="Ch5.S5.SS3.SSSx4.p2.2.m2.1.1" xref="Ch5.S5.SS3.SSSx4.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch5.S5.SS3.SSSx4.p2.2.m2.1b"><ci id="Ch5.S5.SS3.SSSx4.p2.2.m2.1.1.cmml" xref="Ch5.S5.SS3.SSSx4.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S5.SS3.SSSx4.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch5.S5.SS3.SSSx4.p2.2.m2.1d">italic_k</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx30" title="">30</a>]</cite> was used to determine inter-annotator agreement. The agreement was calculated based on the annotations of each system, with the agreement being observed at the sentence level. With this approach, the differences in agreement across systems were explored and we also gained a high-level view of overall agreement between the annotators. Furthermore, Cohen’s <math alttext="kappa" class="ltx_Math" display="inline" id="Ch5.S5.SS3.SSSx4.p2.3.m3.1"><semantics id="Ch5.S5.SS3.SSSx4.p2.3.m3.1a"><mrow id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.cmml"><mi id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.2" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.2.cmml">k</mi><mo id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.3" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.3.cmml">a</mi><mo id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1a" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.4" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.4.cmml">p</mi><mo id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1b" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.5" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.5.cmml">p</mi><mo id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1c" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.6" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch5.S5.SS3.SSSx4.p2.3.m3.1b"><apply id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1"><times id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.1"></times><ci id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.2.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.2">𝑘</ci><ci id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.3.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.3">𝑎</ci><ci id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.4.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.4">𝑝</ci><ci id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.5.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.5">𝑝</ci><ci id="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.6.cmml" xref="Ch5.S5.SS3.SSSx4.p2.3.m3.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S5.SS3.SSSx4.p2.3.m3.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch5.S5.SS3.SSSx4.p2.3.m3.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> was calculated separately for every error type and the findings are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T4" title="Table 5.4 ‣ Inter-Annotator Agreement ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T4.2.1.1" style="font-size:90%;">Table 5.4</span>: </span><span class="ltx_text" id="Ch5.T4.3.2" style="font-size:90%;">Inter-annotator agreement using Cohen values</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T4.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch5.T4.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch5.T4.4.1.1.1.1">Error type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T4.4.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T4.4.1.1.2.1">RNN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T4.4.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch5.T4.4.1.1.3.1">NMT</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T4.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch5.T4.4.2.1.1">Non-translation</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch5.T4.4.2.1.2">1.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch5.T4.4.2.1.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T4.4.3.2.1">Accuracy</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.3.2.2">1.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.3.2.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.4.3.1">Addition</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.4.3.2">1.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.4.3.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.5.4.1">Omission</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.5.4.2">1.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.5.4.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.6.5.1">Mistranslation</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.6.5.2">-0.071</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.6.5.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.7.6.1">Untranslated text</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.7.6.2">0.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.7.6.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T4.4.8.7.1">Fluency</th>
<td class="ltx_td" id="Ch5.T4.4.8.7.2"></td>
<td class="ltx_td" id="Ch5.T4.4.8.7.3"></td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.9.8.1">Punctuation</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.9.8.2">0.651</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.9.8.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.10.9.1">Spelling</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.10.9.2">0.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.10.9.3">0.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.11.10.1">Grammar</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.11.10.2">0.867</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.11.10.3">0.895</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.12.11.1">Register</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.12.11.2">1.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.12.11.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch5.T4.4.13.12.1">Inconsistency</th>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.13.12.2">1.0</td>
<td class="ltx_td ltx_align_left" id="Ch5.T4.4.13.12.3">1.0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T4.4.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="Ch5.T4.4.14.13.1">Character Encoding</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch5.T4.4.14.13.2">1.0</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch5.T4.4.14.13.3">1.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="Ch5.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.6 </span>Empirical Evaluation</h3>
<section class="ltx_subsection" id="Ch5.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6.1 </span>Experimental Setup</h4>
<section class="ltx_subsubsection" id="Ch5.S6.SS1.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Datasets</h5>
<div class="ltx_para" id="Ch5.S6.SS1.SSSx1.p1">
<p class="ltx_p" id="Ch5.S6.SS1.SSSx1.p1.1">The performance of the Transformer and RNN approaches is evaluated on a publicly available EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S6.SS1.SSSx1.p1.1.m1.1"><semantics id="Ch5.S6.SS1.SSSx1.p1.1.m1.1a"><mo id="Ch5.S6.SS1.SSSx1.p1.1.m1.1.1" stretchy="false" xref="Ch5.S6.SS1.SSSx1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S6.SS1.SSSx1.p1.1.m1.1b"><ci id="Ch5.S6.SS1.SSSx1.p1.1.m1.1.1.cmml" xref="Ch5.S6.SS1.SSSx1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S6.SS1.SSSx1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S6.SS1.SSSx1.p1.1.m1.1d">↔</annotation></semantics></math>GA parallel dataset from the Directorate General for Translation (DGT).<span class="ltx_note ltx_role_footnote" id="Ch5.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ec.europa.eu/info/departments/translation" title="">https://ec.europa.eu/info/departments/translation</a></span></span></span> The Joint Research Centre of the DGT has made all its translation memory (i.e. sentences and their professionally produced translations) available, which covers the official European Union languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx107" title="">107</a>]</cite>. Included in the training data are parallel texts from the Digital Corpus of the European Parliament (DCEP) and the DGT. Crawled data, from sites of a similar domain, are also incorporated. This dataset is broadly categorised as generic and is publicly available.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S6.SS1.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Infrastructure</h5>
<div class="ltx_para" id="Ch5.S6.SS1.SSSx2.p1">
<p class="ltx_p" id="Ch5.S6.SS1.SSSx2.p1.1">Model development was conducted using local workstations, each of which was built with an AMD Ryzen 7 2700X processor, 16GB memory, a 256SSD and an NVIDIA GeForce GTX 1080 Ti.</p>
</div>
<div class="ltx_para" id="Ch5.S6.SS1.SSSx2.p2">
<p class="ltx_p" id="Ch5.S6.SS1.SSSx2.p2.1">In addition, a Google Colab Pro subscription enabled rapid prototype development and created zero-emission models. The available computing power of the Google Cloud was much higher than our local infrastructure and provided servers with 16GB graphic cards (NVIDIA Tesla P100 PCIe) and up to 27GB of memory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx19" title="">19</a>]</cite>. Larger Transformer models were built on local infrastructure since long builds timed out on Colab due to Google restrictions. The Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite>, was used to train all MT models.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S6.SS1.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Metrics</h5>
<div class="ltx_para" id="Ch5.S6.SS1.SSSx3.p1">
<p class="ltx_p" id="Ch5.S6.SS1.SSSx3.p1.1">The performance of all models was evaluated using the automated metrics of BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite>. Case-insensitive BLEU scores are reported at the corpus level.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch5.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6.2 </span>Automatic Evaluation Results</h4>
<section class="ltx_subsubsection" id="Ch5.S6.SS2.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Performance of Subword Models</h5>
<div class="ltx_para" id="Ch5.S6.SS2.SSSx1.p1">
<p class="ltx_p" id="Ch5.S6.SS2.SSSx1.p1.1">The impact that choice of subword model has on translation is highlighted in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T5" title="Table 5.5 ‣ Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T6" title="Table 5.6 ‣ Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.6</span></a>.
Incorporating any subword model type led to improvements in model accuracy when training both RNN and Transformer architectures.</p>
</div>
<figure class="ltx_table" id="Ch5.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.5: </span>RNN performance on DGT dataset of 52k Lines. There were zero carbon emissions in building these models since smaller RNN models were trained on Google Colab servers, which are carbon-neutral.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch5.T5.3.3.4.1" style="font-size:90%;">Architecture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch5.T5.1.1.1.1" style="font-size:90%;">BLEU</span><span class="ltx_text" id="Ch5.T5.1.1.1.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T5.1.1.1.m1.1"><semantics id="Ch5.T5.1.1.1.m1.1a"><mo id="Ch5.T5.1.1.1.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T5.1.1.1.m1.1b"><ci id="Ch5.T5.1.1.1.m1.1.1.cmml" xref="Ch5.T5.1.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T5.1.1.1.m1.1d">bold_↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T5.2.2.2.1" style="font-size:90%;">TER</span><span class="ltx_text" id="Ch5.T5.2.2.2.2" style="font-size:90%;"> </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch5.T5.2.2.2.m1.1"><semantics id="Ch5.T5.2.2.2.m1.1a"><mo id="Ch5.T5.2.2.2.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T5.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch5.T5.2.2.2.m1.1b"><ci id="Ch5.T5.2.2.2.m1.1.1.cmml" xref="Ch5.T5.2.2.2.m1.1.1">bold-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T5.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T5.2.2.2.m1.1d">bold_↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch5.T5.3.3.3.1" style="font-size:90%;">ChrF3</span><span class="ltx_text" id="Ch5.T5.3.3.3.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T5.3.3.3.m1.1"><semantics id="Ch5.T5.3.3.3.m1.1a"><mo id="Ch5.T5.3.3.3.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T5.3.3.3.m1.1b"><ci id="Ch5.T5.3.3.3.m1.1.1.cmml" xref="Ch5.T5.3.3.3.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T5.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T5.3.3.3.m1.1d">bold_↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch5.T5.3.3.5.1" style="font-size:90%;">Steps</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.3.3.6"><span class="ltx_text ltx_font_bold" id="Ch5.T5.3.3.6.1" style="font-size:90%;">Runtime (h)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T5.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch5.T5.3.3.7.1" style="font-size:90%;">kgCO<sub class="ltx_sub" id="Ch5.T5.3.3.7.1.1">2</sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T5.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch5.T5.3.4.1.1"><span class="ltx_text" id="Ch5.T5.3.4.1.1.1" style="font-size:90%;">dgt-rnn-base</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T5.3.4.1.2"><span class="ltx_text" id="Ch5.T5.3.4.1.2.1" style="font-size:90%;">52.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T5.3.4.1.3"><span class="ltx_text" id="Ch5.T5.3.4.1.3.1" style="font-size:90%;">0.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T5.3.4.1.4"><span class="ltx_text" id="Ch5.T5.3.4.1.4.1" style="font-size:90%;">0.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T5.3.4.1.5"><span class="ltx_text" id="Ch5.T5.3.4.1.5.1" style="font-size:90%;">75k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T5.3.4.1.6"><span class="ltx_text" id="Ch5.T5.3.4.1.6.1" style="font-size:90%;">4.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T5.3.4.1.7"><span class="ltx_text" id="Ch5.T5.3.4.1.7.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T5.3.5.2">
<td class="ltx_td ltx_align_left" id="Ch5.T5.3.5.2.1"><span class="ltx_text" id="Ch5.T5.3.5.2.1.1" style="font-size:90%;">dgt-rnn-bpe8k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.5.2.2"><span class="ltx_text" id="Ch5.T5.3.5.2.2.1" style="font-size:90%;">54.6</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.5.2.3"><span class="ltx_text" id="Ch5.T5.3.5.2.3.1" style="font-size:90%;">0.40</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.5.2.4"><span class="ltx_text" id="Ch5.T5.3.5.2.4.1" style="font-size:90%;">0.73</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.5.2.5"><span class="ltx_text" id="Ch5.T5.3.5.2.5.1" style="font-size:90%;">85k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.5.2.6"><span class="ltx_text" id="Ch5.T5.3.5.2.6.1" style="font-size:90%;">5.07</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.5.2.7"><span class="ltx_text" id="Ch5.T5.3.5.2.7.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T5.3.6.3">
<td class="ltx_td ltx_align_left" id="Ch5.T5.3.6.3.1"><span class="ltx_text" id="Ch5.T5.3.6.3.1.1" style="font-size:90%;">dgt-rnn-bpe16k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.6.3.2"><span class="ltx_text" id="Ch5.T5.3.6.3.2.1" style="font-size:90%;">55.6</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.6.3.3"><span class="ltx_text" id="Ch5.T5.3.6.3.3.1" style="font-size:90%;">0.39</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.6.3.4"><span class="ltx_text" id="Ch5.T5.3.6.3.4.1" style="font-size:90%;">0.74</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.6.3.5"><span class="ltx_text" id="Ch5.T5.3.6.3.5.1" style="font-size:90%;">100k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.6.3.6"><span class="ltx_text" id="Ch5.T5.3.6.3.6.1" style="font-size:90%;">5.58</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.6.3.7"><span class="ltx_text" id="Ch5.T5.3.6.3.7.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T5.3.7.4">
<td class="ltx_td ltx_align_left" id="Ch5.T5.3.7.4.1"><span class="ltx_text" id="Ch5.T5.3.7.4.1.1" style="font-size:90%;">dgt-rnn-bpe32k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.7.4.2"><span class="ltx_text" id="Ch5.T5.3.7.4.2.1" style="font-size:90%;">55.3</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.7.4.3"><span class="ltx_text" id="Ch5.T5.3.7.4.3.1" style="font-size:90%;">0.39</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.7.4.4"><span class="ltx_text" id="Ch5.T5.3.7.4.4.1" style="font-size:90%;">0.74</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.7.4.5"><span class="ltx_text" id="Ch5.T5.3.7.4.5.1" style="font-size:90%;">95k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.7.4.6"><span class="ltx_text" id="Ch5.T5.3.7.4.6.1" style="font-size:90%;">4.67</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T5.3.7.4.7"><span class="ltx_text" id="Ch5.T5.3.7.4.7.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T5.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch5.T5.3.8.5.1"><span class="ltx_text" id="Ch5.T5.3.8.5.1.1" style="font-size:90%;">dgt-rnn-unigram</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T5.3.8.5.2"><span class="ltx_text" id="Ch5.T5.3.8.5.2.1" style="font-size:90%;">55.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T5.3.8.5.3"><span class="ltx_text" id="Ch5.T5.3.8.5.3.1" style="font-size:90%;">0.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T5.3.8.5.4"><span class="ltx_text" id="Ch5.T5.3.8.5.4.1" style="font-size:90%;">0.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T5.3.8.5.5"><span class="ltx_text" id="Ch5.T5.3.8.5.5.1" style="font-size:90%;">105k</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T5.3.8.5.6"><span class="ltx_text" id="Ch5.T5.3.8.5.6.1" style="font-size:90%;">5.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T5.3.8.5.7"><span class="ltx_text" id="Ch5.T5.3.8.5.7.1" style="font-size:90%;">0</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Ch5.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.6: </span>Transformer performance on 52k DGT dataset. The highest-performing model uses 2 attention heads. All other models use 8 attention heads. Transformer models were long-running builds, which had to be carried out on local servers.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch5.T6.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch5.T6.3.3.4.1" style="font-size:90%;">Architecture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T6.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch5.T6.1.1.1.1" style="font-size:90%;">BLEU</span><span class="ltx_text" id="Ch5.T6.1.1.1.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T6.1.1.1.m1.1"><semantics id="Ch5.T6.1.1.1.m1.1a"><mo id="Ch5.T6.1.1.1.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T6.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T6.1.1.1.m1.1b"><ci id="Ch5.T6.1.1.1.m1.1.1.cmml" xref="Ch5.T6.1.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T6.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T6.1.1.1.m1.1d">bold_↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T6.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T6.2.2.2.1" style="font-size:90%;">TER</span><span class="ltx_text" id="Ch5.T6.2.2.2.2" style="font-size:90%;"> </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch5.T6.2.2.2.m1.1"><semantics id="Ch5.T6.2.2.2.m1.1a"><mo id="Ch5.T6.2.2.2.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T6.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch5.T6.2.2.2.m1.1b"><ci id="Ch5.T6.2.2.2.m1.1.1.cmml" xref="Ch5.T6.2.2.2.m1.1.1">bold-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T6.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T6.2.2.2.m1.1d">bold_↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T6.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch5.T6.3.3.3.1" style="font-size:90%;">ChrF3</span><span class="ltx_text" id="Ch5.T6.3.3.3.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T6.3.3.3.m1.1"><semantics id="Ch5.T6.3.3.3.m1.1a"><mo id="Ch5.T6.3.3.3.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T6.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T6.3.3.3.m1.1b"><ci id="Ch5.T6.3.3.3.m1.1.1.cmml" xref="Ch5.T6.3.3.3.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T6.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T6.3.3.3.m1.1d">bold_↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T6.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch5.T6.3.3.5.1" style="font-size:90%;">Steps</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T6.3.3.6"><span class="ltx_text ltx_font_bold" id="Ch5.T6.3.3.6.1" style="font-size:90%;">Runtime (h)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T6.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch5.T6.3.3.7.1" style="font-size:90%;">kgCO<sub class="ltx_sub" id="Ch5.T6.3.3.7.1.1">2</sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T6.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch5.T6.3.4.1.1"><span class="ltx_text" id="Ch5.T6.3.4.1.1.1" style="font-size:90%;">dgt-trans-base</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T6.3.4.1.2"><span class="ltx_text" id="Ch5.T6.3.4.1.2.1" style="font-size:90%;">53.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T6.3.4.1.3"><span class="ltx_text" id="Ch5.T6.3.4.1.3.1" style="font-size:90%;">0.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T6.3.4.1.4"><span class="ltx_text" id="Ch5.T6.3.4.1.4.1" style="font-size:90%;">0.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T6.3.4.1.5"><span class="ltx_text" id="Ch5.T6.3.4.1.5.1" style="font-size:90%;">55k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T6.3.4.1.6"><span class="ltx_text" id="Ch5.T6.3.4.1.6.1" style="font-size:90%;">14.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T6.3.4.1.7"><span class="ltx_text" id="Ch5.T6.3.4.1.7.1" style="font-size:90%;">0.81</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T6.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T6.3.5.2.1"><span class="ltx_text" id="Ch5.T6.3.5.2.1.1" style="font-size:90%;">dgt-trans-bpe8k</span></th>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.5.2.2"><span class="ltx_text" id="Ch5.T6.3.5.2.2.1" style="font-size:90%;">59.5</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.5.2.3"><span class="ltx_text" id="Ch5.T6.3.5.2.3.1" style="font-size:90%;">0.34</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.5.2.4"><span class="ltx_text" id="Ch5.T6.3.5.2.4.1" style="font-size:90%;">0.77</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.5.2.5"><span class="ltx_text" id="Ch5.T6.3.5.2.5.1" style="font-size:90%;">200k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.5.2.6"><span class="ltx_text" id="Ch5.T6.3.5.2.6.1" style="font-size:90%;">24.48</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.5.2.7"><span class="ltx_text" id="Ch5.T6.3.5.2.7.1" style="font-size:90%;">1.38</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T6.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T6.3.6.3.1"><span class="ltx_text" id="Ch5.T6.3.6.3.1.1" style="font-size:90%;">dgt-trans-bpe16k</span></th>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.6.3.2"><span class="ltx_text" id="Ch5.T6.3.6.3.2.1" style="font-size:90%;">60.5</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.6.3.3"><span class="ltx_text" id="Ch5.T6.3.6.3.3.1" style="font-size:90%;">0.33</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.6.3.4"><span class="ltx_text" id="Ch5.T6.3.6.3.4.1" style="font-size:90%;">0.78</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.6.3.5"><span class="ltx_text" id="Ch5.T6.3.6.3.5.1" style="font-size:90%;">180k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.6.3.6"><span class="ltx_text" id="Ch5.T6.3.6.3.6.1" style="font-size:90%;">26.90</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.6.3.7"><span class="ltx_text" id="Ch5.T6.3.6.3.7.1" style="font-size:90%;">1.52</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T6.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch5.T6.3.7.4.1"><span class="ltx_text" id="Ch5.T6.3.7.4.1.1" style="font-size:90%;">dgt-trans-bpe32k</span></th>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.7.4.2"><span class="ltx_text" id="Ch5.T6.3.7.4.2.1" style="font-size:90%;">59.3</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.7.4.3"><span class="ltx_text" id="Ch5.T6.3.7.4.3.1" style="font-size:90%;">0.35</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.7.4.4"><span class="ltx_text" id="Ch5.T6.3.7.4.4.1" style="font-size:90%;">0.77</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.7.4.5"><span class="ltx_text" id="Ch5.T6.3.7.4.5.1" style="font-size:90%;">100k</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.7.4.6"><span class="ltx_text" id="Ch5.T6.3.7.4.6.1" style="font-size:90%;">18.03</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T6.3.7.4.7"><span class="ltx_text" id="Ch5.T6.3.7.4.7.1" style="font-size:90%;">1.02</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T6.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch5.T6.3.8.5.1"><span class="ltx_text" id="Ch5.T6.3.8.5.1.1" style="font-size:90%;">dgt-trans-unigram</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T6.3.8.5.2"><span class="ltx_text" id="Ch5.T6.3.8.5.2.1" style="font-size:90%;">59.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T6.3.8.5.3"><span class="ltx_text" id="Ch5.T6.3.8.5.3.1" style="font-size:90%;">0.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T6.3.8.5.4"><span class="ltx_text" id="Ch5.T6.3.8.5.4.1" style="font-size:90%;">0.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T6.3.8.5.5"><span class="ltx_text" id="Ch5.T6.3.8.5.5.1" style="font-size:90%;">125k</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T6.3.8.5.6"><span class="ltx_text" id="Ch5.T6.3.8.5.6.1" style="font-size:90%;">21.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T6.3.8.5.7"><span class="ltx_text" id="Ch5.T6.3.8.5.7.1" style="font-size:90%;">1.24</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ch5.S6.SS2.SSSx1.p2">
<p class="ltx_p" id="Ch5.S6.SS2.SSSx1.p2.1">A baseline RNN model, illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T5" title="Table 5.5 ‣ Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.5</span></a>, achieved a BLEU score of 52.7, whereas the highest-performing BPE variant, using a 16k vocabulary, recorded an improvement of nearly three points, with a score of 55.6.</p>
</div>
<div class="ltx_para" id="Ch5.S6.SS2.SSSx1.p3">
<p class="ltx_p" id="Ch5.S6.SS2.SSSx1.p3.1">In the context of Transformer architectures, highlighted in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T6" title="Table 5.6 ‣ Performance of Subword Models ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.6</span></a>, the use of subword models delivers significant performance improvements. The performance gains for Transformer models are much higher than the improvements recorded by the RNN models. A baseline Transformer model achieves a BLEU score of 53.4, whereas a Transformer model, with a 16k BPE subword model, has a score of 60.5, representing a BLEU score improvement of 13% at 7.1 BLEU points.</p>
</div>
<div class="ltx_para" id="Ch5.S6.SS2.SSSx1.p4">
<p class="ltx_p" id="Ch5.S6.SS2.SSSx1.p4.1">For translating into a morphological-rich language, such as Irish, the ChrF metric has proven successful in showing a strong correlation with human translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx106" title="">106</a>]</cite>. In the context of our experiments, this worked well in highlighting the performance differences between RNN and Transformer architectures.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S6.SS2.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Transformer Performance Compared with RNN</h5>
<div class="ltx_para" id="Ch5.S6.SS2.SSSx2.p1">
<p class="ltx_p" id="Ch5.S6.SS2.SSSx2.p1.1">The performance of RNN models is contrasted with the Transformer approach in Figures <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F3" title="Figure 5.3 ‣ Transformer Performance Compared with RNN ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F4" title="Figure 5.4 ‣ Transformer Performance Compared with RNN ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.4</span></a>. Transformer models, as anticipated, outperformed all their RNN counterparts. It is interesting to note the impact of choosing the optimal vocabulary size for BPE subword models. Choosing a BPE vocabulary of 16k words yields the highest performance.</p>
</div>
<div class="ltx_para" id="Ch5.S6.SS2.SSSx2.p2">
<p class="ltx_p" id="Ch5.S6.SS2.SSSx2.p2.1">Furthermore, the TER scores highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F4" title="Figure 5.4 ‣ Transformer Performance Compared with RNN ‣ 5.6.2 Automatic Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.4</span></a> reinforce the findings that using 16k BPE subword models on Transformer architectures leads to a better translation performance. The TER score for the 16k BPE Transformer model is significantly better (0.33) when compared with the baseline performance (0.41).</p>
</div>
<figure class="ltx_figure" id="Ch5.F3"><img alt="Refer to caption" class="ltx_graphics" id="Ch5.F3.g1" src="bleu_all.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F3.2.1.1" style="font-size:90%;">Figure 5.3</span>: </span><span class="ltx_text" id="Ch5.F3.3.2" style="font-size:90%;">BLEU performance for all model architectures. The use of a BPE subword model improved translation performance in all cases. The best-performing model was built using a 16k BPE subword model on a Transformer architecture. </span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="355" id="Ch5.F4.g1" src="extracted/5444776/Images/ter_all_he.png" width="589"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F4.2.1.1" style="font-size:90%;">Figure 5.4</span>: </span><span class="ltx_text" id="Ch5.F4.3.2" style="font-size:90%;">TER performance for all model architectures. The highest-performing model uses a 16k BPE subword model on a Transformer architecture. In all instances, incorporating a subword model improves TER.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="Ch5.S6.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6.3 </span>Human Evaluation Results</h4>
<div class="ltx_para" id="Ch5.S6.SS3.p1">
<p class="ltx_p" id="Ch5.S6.SS3.p1.1">The aggregate total of errors found by annotators for each system is highlighted in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T7" title="Table 5.7 ‣ 5.6.3 Human Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.7</span></a>. Looking at the aggregate data alone, it is evident that both annotators have judged that the RNN system contains more errors and that the NMT system contains fewer errors.</p>
</div>
<figure class="ltx_table" id="Ch5.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T7.2.1.1" style="font-size:90%;">Table 5.7</span>: </span><span class="ltx_text" id="Ch5.T7.3.2" style="font-size:90%;">Total errors found by each annotator using the MQM metric.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T7.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T7.4.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="Ch5.T7.4.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Ch5.T7.4.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T7.4.1.1.2.1">Annotator 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Ch5.T7.4.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch5.T7.4.1.1.3.1">Annotator 2</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T7.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch5.T7.4.2.2.1"><span class="ltx_text ltx_font_bold" id="Ch5.T7.4.2.2.1.1">System</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T7.4.2.2.2">RNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T7.4.2.2.3">Transformer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T7.4.2.2.4">RNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T7.4.2.2.5">Transformer</td>
</tr>
<tr class="ltx_tr" id="Ch5.T7.4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch5.T7.4.3.3.1"><span class="ltx_text ltx_font_bold" id="Ch5.T7.4.3.3.1.1">Total Errors</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch5.T7.4.3.3.2">41</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch5.T7.4.3.3.3">23</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch5.T7.4.3.3.4">43</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch5.T7.4.3.3.5">23</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ch5.S6.SS3.p2">
<p class="ltx_p" id="Ch5.S6.SS3.p2.1">While such a high-level view is instructive in determining which system is better, it lacks the granularity required to pinpoint the linguistic aspects of how these translations can be improved. To achieve a deeper insight, a fine-grained analysis of the error types was conducted, the results of which are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T8" title="Table 5.8 ‣ 5.6.3 Human Evaluation Results ‣ 5.6 Empirical Evaluation ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.8</span></a>. Categorised by error type, the sum of error tags by each annotator for each system is outlined.
</p>
</div>
<figure class="ltx_table" id="Ch5.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T8.2.1.1" style="font-size:90%;">Table 5.8</span>: </span><span class="ltx_text" id="Ch5.T8.3.2" style="font-size:90%;">Transformer and RNN approach compared using concatenated annotation data across both annotators. In all MQM error categories, the Transformer architecture performs better, apart from a tie in the omission category.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T8.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T8.4.1.1">
<td class="ltx_td ltx_border_t" id="Ch5.T8.4.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T8.4.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T8.4.1.1.2.1">RNN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T8.4.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch5.T8.4.1.1.3.1">NMT</span></th>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch5.T8.4.2.2.1"><span class="ltx_text ltx_font_bold" id="Ch5.T8.4.2.2.1.1">Error Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T8.4.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch5.T8.4.2.2.2.1">Error</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T8.4.2.2.3">
<span class="ltx_text ltx_font_bold" id="Ch5.T8.4.2.2.3.1">Error</span></th>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch5.T8.4.3.3.1">Non-translation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T8.4.3.3.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T8.4.3.3.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.4.4">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.4.4.1"><span class="ltx_text ltx_font_bold" id="Ch5.T8.4.4.4.1.1">Accuracy</span></td>
<td class="ltx_td" id="Ch5.T8.4.4.4.2"></td>
<td class="ltx_td" id="Ch5.T8.4.4.4.3"></td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.5.5">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.5.5.1">Addition</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.5.5.2">10</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.5.5.3">4</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.6.6">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.6.6.1">Omission</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.6.6.2">12</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.6.6.3">12</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.7.7">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.7.7.1">Mistranslation</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.7.7.2">26</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.7.7.3">14</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.8.8">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.8.8.1">Untranslated text</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.8.8.2">4</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.8.8.3">1</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.9.9">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.9.9.1"><span class="ltx_text ltx_font_bold" id="Ch5.T8.4.9.9.1.1">Fluency</span></td>
<td class="ltx_td" id="Ch5.T8.4.9.9.2"></td>
<td class="ltx_td" id="Ch5.T8.4.9.9.3"></td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.10.10">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.10.10.1">Punctuation</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.10.10.2">5</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.10.10.3">4</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.11.11">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.11.11.1">Spelling</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.11.11.2">1</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.11.11.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.12.12">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.12.12.1">Grammar</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.12.12.2">20</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.12.12.3">11</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.13.13">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.13.13.1">Register</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.13.13.2">2</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.13.13.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.14.14">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.14.14.1">Inconsistency</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.14.14.2">2</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.14.14.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.15.15">
<td class="ltx_td ltx_align_left" id="Ch5.T8.4.15.15.1">Character Encoding</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.15.15.2">0</td>
<td class="ltx_td ltx_align_center" id="Ch5.T8.4.15.15.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch5.T8.4.16.16">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch5.T8.4.16.16.1">
<span class="ltx_text ltx_font_bold" id="Ch5.T8.4.16.16.1.1">Total errors</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch5.T8.4.16.16.2">82</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch5.T8.4.16.16.3">46</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="Ch5.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.7 </span>Environmental Impact</h3>
<div class="ltx_para" id="Ch5.S7.p1">
<p class="ltx_p" id="Ch5.S7.p1.1">The environmental impact of all aspects of computing has received increased research interest in recent times. Much of this effort has concentrated on NMT’s carbon footprint <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx15" title="">15</a>]</cite>. To assess the environmental impact of our NMT models, we tracked energy consumption during their development.</p>
</div>
<div class="ltx_para" id="Ch5.S7.p2">
<p class="ltx_p" id="Ch5.S7.p2.1">Prototype model development was carried out using Google Colab which is a carbon-neutral platform <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>. However, longer running Transformer experiments were conducted on local servers using 324 gCO<sub class="ltx_sub" id="Ch5.S7.p2.1.1">2</sub> per kWh<span class="ltx_note ltx_role_footnote" id="Ch5.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf" title="">https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx100" title="">100</a>]</cite>. The net result was just under 10 kgCO<sub class="ltx_sub" id="Ch5.S7.p2.1.2">2</sub>, created for a full run of model development. Models developed during this study will be reused for ensemble experiments in future work.</p>
</div>
<div class="ltx_para" id="Ch5.S7.p3">
<p class="ltx_p" id="Ch5.S7.p3.1">The environmental costs of our model development were tracked to serve as a benchmark for future work. Awareness of such costs will impose a discipline on our work, such that we opt for carbon-neutral cloud providers. In cases where models are developed on local infrastructure, this will encourage the use of more efficient GPUs and the utilisation of techniques that result in faster builds.</p>
</div>
</section>
<section class="ltx_section" id="Ch5.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.8 </span>Discussion</h3>
<div class="ltx_para" id="Ch5.S8.p1">
<p class="ltx_p" id="Ch5.S8.p1.1">Validation accuracy and model perplexity (PPL) in developing the baseline and optimal Transformer models are illustrated in Figures <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F5" title="Figure 5.5 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F6" title="Figure 5.6 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.6</span></a>. Training a Transformer model with a 16k BPE subword model boosted the validation accuracy by over 8% compared to its baseline.</p>
</div>
<figure class="ltx_figure" id="Ch5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch5.F5.g1" src="dgt-trans-base.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F5.2.1.1" style="font-size:90%;">Figure 5.5</span>: </span><span class="ltx_text" id="Ch5.F5.3.2" style="font-size:90%;">Transformer baseline.</span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch5.F6.g1" src="dgt-trans-16kbpe.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F6.2.1.1" style="font-size:90%;">Figure 5.6</span>: </span><span class="ltx_text" id="Ch5.F6.3.2" style="font-size:90%;">Transformer 16k BPE subword model</span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S8.p2">
<p class="ltx_p" id="Ch5.S8.p2.1">Rapid convergence was observed while training the baseline model such that little accuracy improvement occurred after 20k steps. Including a subword model led to slower converging models, with only marginal gains recorded after 60k steps. Examining Figures <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F5" title="Figure 5.5 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.F6" title="Figure 5.6 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.6</span></a>, we can see that PPL achieves a lower global minimum when the Transformer approach is used with a 16k BPE subword model. The PPL global minimum (2.7) is over 50% lower than the corresponding PPL for the Transformer base model (5.5). This finding illustrates that choosing an optimal subword model delivers significant performance gains.</p>
</div>
<div class="ltx_para" id="Ch5.S8.p3">
<p class="ltx_p" id="Ch5.S8.p3.1">Translation engine performance, at the corpus level, was benchmarked against Google Translate’s<span class="ltx_note ltx_role_footnote" id="Ch5.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com/</a></span></span></span> EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch5.S8.p3.1.m1.1"><semantics id="Ch5.S8.p3.1.m1.1a"><mo id="Ch5.S8.p3.1.m1.1.1" stretchy="false" xref="Ch5.S8.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch5.S8.p3.1.m1.1b"><ci id="Ch5.S8.p3.1.m1.1.1.cmml" xref="Ch5.S8.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S8.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S8.p3.1.m1.1d">↔</annotation></semantics></math>GA translation service, which is freely available on the internet. Four random samples were selected from the English source test file and are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T9" title="Table 5.9 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.9</span></a>. Translation of these samples was carried out on the optimal Transformer model and using Google Translate. Case-insensitive, sentence-level BLEU scores were recorded and are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T10" title="Table 5.10 ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.10</span></a>. It must be acknowledged that this comparison is not entirely valid given that Google does not have access to our training data, nor do we have unlimited access to the Google Cloud infrastructure. Nonetheless, the results are encouraging and indicate a good performance by our translation models on the DGT dataset.</p>
</div>
<figure class="ltx_table" id="Ch5.T9">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.9: </span>Random samples of human reference translations from the test dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T9.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T9.4.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T9.4.1.1.1" style="width:204.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T9.4.1.1.1.1" style="font-size:90%;">Source Language (English)</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T9.4.1.1.2" style="width:204.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T9.4.1.1.2.1" style="font-size:90%;">Reference Human Translation (Irish)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T9.4.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T9.4.2.1.1" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.2.1.1.1"><span class="ltx_text" id="Ch5.T9.4.2.1.1.1.1" style="font-size:90%;">A clear harmonised procedure, including the necessary criteria for disease–free status, should be established for that purpose.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T9.4.2.1.2" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.2.1.2.1"><span class="ltx_text" id="Ch5.T9.4.2.1.2.1.1" style="font-size:90%;">Ba cheart nós imeachta comhchuibhithe soiléir, lena n-áirítear na critéir is gá do stádas saor ó ghalar, a bhunú chun na críche sin.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T9.4.3.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T9.4.3.2.1" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.3.2.1.1"><span class="ltx_text" id="Ch5.T9.4.3.2.1.1.1" style="font-size:90%;">the mark is applied anew, as appropriate.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T9.4.3.2.2" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.3.2.2.1"><span class="ltx_text" id="Ch5.T9.4.3.2.2.1.1" style="font-size:90%;">déanfar an mharcáil arís, mar is iomchuí.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T9.4.4.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T9.4.4.3.1" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.4.3.1.1"><span class="ltx_text" id="Ch5.T9.4.4.3.1.1.1" style="font-size:90%;">If the court decides that a review is justified on any of the grounds set out in paragraph 1, the judgment given in the European Small Claims Procedure shall be null and void.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T9.4.4.3.2" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.4.3.2.1"><span class="ltx_text" id="Ch5.T9.4.4.3.2.1.1" style="font-size:90%;">Má chinneann an chúirt go bhfuil bonn cirt le hathbhreithniú de bharr aon cheann de na forais a leagtar amach i mír 1, beidh an breithiúnas a tugadh sa Nós Imeachta Eorpach um Éilimh Bheaga ar neamhní go hiomlán.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T9.4.5.4">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T9.4.5.4.1" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.5.4.1.1"><span class="ltx_text" id="Ch5.T9.4.5.4.1.1.1" style="font-size:90%;">households where pet animals are kept;</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T9.4.5.4.2" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T9.4.5.4.2.1"><span class="ltx_text" id="Ch5.T9.4.5.4.2.1.1" style="font-size:90%;">teaghlaigh ina gcoimeádtar peataí;</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Ch5.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T10.4.1.1" style="font-size:90%;">Table 5.10</span>: </span><span class="ltx_text" id="Ch5.T10.5.2" style="font-size:90%;">Transformer model compared with Google Translate using random samples from the DGT corpus. Full evaluation of Google Translate’s engines on the DGT test set, with 1.3k lines, generated a BLEU score of 46.3 and a TER score of 0.44. Comparative scores on the test set using our Transformer model, with 2 attention heads and 16k BPE subword model realised 60.5 for BLEU and 0.33 for TER.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T10.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T10.2.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T10.2.2.3" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T10.2.2.3.1" style="font-size:90%;">Transformer (16k BPE)</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T10.1.1.1" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch5.T10.1.1.1.1.1.1" style="font-size:90%;">BLEU</span><span class="ltx_text" id="Ch5.T10.1.1.1.1.1.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T10.1.1.1.1.1.m1.1"><semantics id="Ch5.T10.1.1.1.1.1.m1.1a"><mo id="Ch5.T10.1.1.1.1.1.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T10.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T10.1.1.1.1.1.m1.1b"><ci id="Ch5.T10.1.1.1.1.1.m1.1.1.cmml" xref="Ch5.T10.1.1.1.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T10.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T10.1.1.1.1.1.m1.1d">bold_↑</annotation></semantics></math></p>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T10.2.2.4" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T10.2.2.4.1" style="font-size:90%;">Google Translate</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T10.2.2.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="Ch5.T10.2.2.2.1.1.1" style="font-size:90%;">BLEU</span><span class="ltx_text" id="Ch5.T10.2.2.2.1.1.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T10.2.2.2.1.1.m1.1"><semantics id="Ch5.T10.2.2.2.1.1.m1.1a"><mo id="Ch5.T10.2.2.2.1.1.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T10.2.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T10.2.2.2.1.1.m1.1b"><ci id="Ch5.T10.2.2.2.1.1.m1.1.1.cmml" xref="Ch5.T10.2.2.2.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T10.2.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T10.2.2.2.1.1.m1.1d">bold_↑</annotation></semantics></math></p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T10.2.3.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.3.1.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.3.1.1.1"><span class="ltx_text" id="Ch5.T10.2.3.1.1.1.1" style="font-size:90%;">Ba cheart nós imeachta soiléir comhchuibhithe, lena n-áirítear na critéir is gá maidir le stádas saor ó ghalair, a bhunú chun na críche sin.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.3.1.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.3.1.2.1"><span class="ltx_text" id="Ch5.T10.2.3.1.2.1.1" style="font-size:90%;">61.6</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.3.1.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.3.1.3.1"><span class="ltx_text" id="Ch5.T10.2.3.1.3.1.1" style="font-size:90%;">Ba cheart nós imeachta comhchuibhithe soiléir, lena n-áirítear na critéir riachtanacha maidir le stádas saor ó ghalair, a bhunú chun na críche sin.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.3.1.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.3.1.4.1"><span class="ltx_text" id="Ch5.T10.2.3.1.4.1.1" style="font-size:90%;">70.2</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T10.2.4.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.4.2.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.4.2.1.1"><span class="ltx_text" id="Ch5.T10.2.4.2.1.1.1" style="font-size:90%;">go gcuirtear an marc i bhfeidhme, de réir mar is iomchuí.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.4.2.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.4.2.2.1"><span class="ltx_text" id="Ch5.T10.2.4.2.2.1.1" style="font-size:90%;">21.4</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.4.2.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.4.2.3.1"><span class="ltx_text" id="Ch5.T10.2.4.2.3.1.1" style="font-size:90%;">cuirtear an marc i bhfeidhm as an nua, de réir mar is cuí.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.4.2.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.4.2.4.1"><span class="ltx_text" id="Ch5.T10.2.4.2.4.1.1" style="font-size:90%;">6.6</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T10.2.5.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.5.3.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.5.3.1.1"><span class="ltx_text" id="Ch5.T10.2.5.3.1.1.1" style="font-size:90%;">Má chinneann an chúirt go bhfuil bonn cirt le hathbhreithniú ar aon cheann de na forais a leagtar amach i mír 1, beidh an breithiúnas a thugtar sa Nós Imeachta Eorpach um Éilimh Bheaga ar neamhní.</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.5.3.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.5.3.2.1"><span class="ltx_text" id="Ch5.T10.2.5.3.2.1.1" style="font-size:90%;">77.3</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.5.3.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.5.3.3.1"><span class="ltx_text" id="Ch5.T10.2.5.3.3.1.1" style="font-size:90%;">Má chinneann an chúirt go bhfuil údar le hathbhreithniú ar aon cheann de na forais atá leagtha amach i mír 1, beidh an breithiúnas a thugtar sa Nós Imeachta Eorpach um Éilimh Bheaga ar neamhní</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T10.2.5.3.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.5.3.4.1"><span class="ltx_text" id="Ch5.T10.2.5.3.4.1.1" style="font-size:90%;">59.1</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T10.2.6.4">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T10.2.6.4.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.6.4.1.1"><span class="ltx_text" id="Ch5.T10.2.6.4.1.1.1" style="font-size:90%;">teaghlaigh ina gcoimeádtar peataí;</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T10.2.6.4.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.6.4.2.1"><span class="ltx_text" id="Ch5.T10.2.6.4.2.1.1" style="font-size:90%;">100</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T10.2.6.4.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.6.4.3.1"><span class="ltx_text" id="Ch5.T10.2.6.4.3.1.1" style="font-size:90%;">teaghlaigh ina gcoinnítear peataí;</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T10.2.6.4.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T10.2.6.4.4.1"><span class="ltx_text" id="Ch5.T10.2.6.4.4.1.1" style="font-size:90%;">30.2</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ch5.S8.p4">
<p class="ltx_p" id="Ch5.S8.p4.1">The optimal hyperparameters selected in this discovery process are identified in bold in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T1" title="Table 5.1 ‣ 5.5.2 Subword Models ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.1</span></a>. A higher initial learning rate of 2 coupled with an average decay of 0.0001 led to longer training times but more accurate models. Despite setting an early stopping parameter, many of the Transformer builds continued for the full cycle of 200k steps over periods of 20+ hours.</p>
</div>
<div class="ltx_para" id="Ch5.S8.p5">
<p class="ltx_p" id="Ch5.S8.p5.1">Training Transformer models with a reduced number of attention heads led to a marginal improvement in translation accuracy with a smaller corpus. Our best-performing model achieved a BLEU score of 60.5 and a TER score of 0.33 with 2 heads and a 16k BPE subword model. By comparison, using 8 heads with the same architecture and dataset yielded 60.3 for BLEU and 0.34 in terms of TER.</p>
</div>
<div class="ltx_para" id="Ch5.S8.p6">
<p class="ltx_p" id="Ch5.S8.p6.1">Transformer models developed, using state-of-the-art techniques, were evaluated as part of the LoResMT2021 Shared Task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite>. Models developed using our approach, as outlined above, were entered into the competition, and the highest-performing EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch5.S8.p6.1.m1.1"><semantics id="Ch5.S8.p6.1.m1.1a"><mo id="Ch5.S8.p6.1.m1.1.1" stretchy="false" xref="Ch5.S8.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch5.S8.p6.1.m1.1b"><ci id="Ch5.S8.p6.1.m1.1.1.cmml" xref="Ch5.S8.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S8.p6.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S8.p6.1.m1.1d">→</annotation></semantics></math>GA direction system was submitted by our team (ADAPT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="Ch5.S8.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8.1 </span>Inter-Annotator Reliability</h4>
<div class="ltx_para" id="Ch5.S8.SS1.p1">
<p class="ltx_p" id="Ch5.S8.SS1.p1.2">In Cohen’s original article <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx30" title="">30</a>]</cite>, the interpretation of specific <math alttext="k" class="ltx_Math" display="inline" id="Ch5.S8.SS1.p1.1.m1.1"><semantics id="Ch5.S8.SS1.p1.1.m1.1a"><mi id="Ch5.S8.SS1.p1.1.m1.1.1" xref="Ch5.S8.SS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch5.S8.SS1.p1.1.m1.1b"><ci id="Ch5.S8.SS1.p1.1.m1.1.1.cmml" xref="Ch5.S8.SS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S8.SS1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch5.S8.SS1.p1.1.m1.1d">italic_k</annotation></semantics></math> scores is clearly outlined. There is no agreement with values <math alttext="\leq" class="ltx_Math" display="inline" id="Ch5.S8.SS1.p1.2.m2.1"><semantics id="Ch5.S8.SS1.p1.2.m2.1a"><mo id="Ch5.S8.SS1.p1.2.m2.1.1" xref="Ch5.S8.SS1.p1.2.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="Ch5.S8.SS1.p1.2.m2.1b"><leq id="Ch5.S8.SS1.p1.2.m2.1.1.cmml" xref="Ch5.S8.SS1.p1.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S8.SS1.p1.2.m2.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="Ch5.S8.SS1.p1.2.m2.1d">≤</annotation></semantics></math> 0, none to slight agreement when scores are in the range of 0.01–0.20, fair agreement is represented by 0.21–0.40, 0.41–0.60 is moderate agreement, 0.61–0.80 is substantial agreement, and 0.81–1.00 is almost perfect agreement.</p>
</div>
<div class="ltx_para" id="Ch5.S8.SS1.p2">
<p class="ltx_p" id="Ch5.S8.SS1.p2.1">The literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx79" title="">79</a>]</cite> recommends a minimum of 80% agreement for good inter-annotator agreement. As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T4" title="Table 5.4 ‣ Inter-Annotator Agreement ‣ 5.5.3 Human Evaluation of NMT ‣ 5.5 Proposed Approach ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.4</span></a>, there is almost perfect agreement between the annotators when evaluating output from the NMT models. In the case of the RNN outputs, there is disagreement in the mistranslation category but agreement in all other categories. Given these scores, we have a high degree of confidence in our human evaluation of both the RNN and NMT outputs.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch5.S8.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8.2 </span>Performance of Models Relative to Google</h4>
<div class="ltx_para" id="Ch5.S8.SS2.p1">
<p class="ltx_p" id="Ch5.S8.SS2.p1.1">Using standard Transformer parameters, such as a batch size of 2048 and setting the number of encoder and decoder layers to 6, were observed to perform well. Increasing the regularisation dropout to 0.3 and reducing hidden neurons to 256 improved translation performance. Consequently, these values were selected when building all Transformer models.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch5.S8.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8.3 </span>Linguistic Observations</h4>
<div class="ltx_para" id="Ch5.S8.SS3.p1">
<p class="ltx_p" id="Ch5.S8.SS3.p1.1">A linguistic analysis of the outputs from the Transformer-optimised model is illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T11" title="Table 5.11 ‣ Interpreting Meaning ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.11</span></a>. The English language source sentences and their Irish language translations are presented. Sentences have been selected from the fine-grained human evaluation since they highlight some of the key error types that are encountered. The analysis focuses on the shortcomings of our model outputs, which fall into the following categories: interpretative meaning, core grammatical errors and commonly used irregular verbs. Finally, using the human evaluation metrics of SQM and MQM, the performance of an RNN approach is contrasted with that of the Transformer approach.</p>
</div>
<section class="ltx_subsubsection" id="Ch5.S8.SS3.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Interpreting Meaning</h5>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx1.p1">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx1.p1.1">The generic Irish verb “déan” (to do or to make) is used to express more precise concepts such as “to conduct”, “to put into effect” or “to carry out”. Both the RNN and Transformer systems make use of “déan” in a generic way, but they fail to capture the refinement of the concept expressed in each of these meanings. An example of this problem is illustrated in GA-1 in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T11" title="Table 5.11 ‣ Interpreting Meaning ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.11</span></a>. In this context, a more natural and intuitive translation to capture the expression “to conduct” would be to substitute “a dhéanamh” with “a sheoladh”.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx1.p2">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx1.p2.1">A similar lack of refinement from both systems is also found in the usage of other words. For example, “cuid” (part) is used to translate “operative part” in GA-2. However, a more precise interpretation would be the usage of “gné”, leading to the correct translation “gné oibríochtúil” i.e., “operative part”.</p>
</div>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx1.p3">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx1.p3.1">Another example where the translation models failed to correctly interpret the true sense of an English source word into a corresponding Irish translation can be seen in GA-3. The Irish verb “Mainnigh” meaning “to default” would not be used in the context of the source text in EN-3. Using the Irish verb “teip”, meaning “to fail”, is the correct translation of the idea “fails to meet the performance requirements”: “má theipeann an t-oibreoir na ceanglais feidhmíochta a chomhlíonadh.” This error was observed in both the RNN and Transformer model outputs.</p>
</div>
<figure class="ltx_table" id="Ch5.T11">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.11: </span>Linguistic analysis of system outputs. Sources of errors are flagged in blue and in red.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch5.T11.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T11.4.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T11.4.1.1.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.1.1.1.1" style="font-size:90%;">Type</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch5.T11.4.1.1.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.1.1.2.1" style="font-size:90%;">Sentence</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T11.4.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.2.1.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.2.1.1.1" style="font-size:90%;">EN-1</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.2.1.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.2.1.2.1"><span class="ltx_text" id="Ch5.T11.4.2.1.2.1.1" style="font-size:90%;">The lead supervisory authority may request at any time other supervisory authorities concerned to provide mutual assistance pursuant to Article 61 and </span><span class="ltx_text" id="Ch5.T11.4.2.1.2.1.2" style="font-size:90%;color:#0000FF;">may conduct</span><span class="ltx_text" id="Ch5.T11.4.2.1.2.1.3" style="font-size:90%;"> joint operations pursuant to Article 62, in particular for carrying out investigations or for monitoring the implementation of a measure concerning a controller or processor established in another Member State.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.3.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.3.2.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.3.2.1.1" style="font-size:90%;">GA-1</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.3.2.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.3.2.2.1"><span class="ltx_text" id="Ch5.T11.4.3.2.2.1.1" style="font-size:90%;">Féadfaidh an príomhúdarás maoirseachta iarraidh, tráth ar bith, ar bith eile lena mbaineann cúnamh frithpháirteach a chur ar fáil de bhun Airteagal 61 agus féadfaidh sé oibríochtaí comhpháirteacha a dhéanamh de bhun Airteagal 62, go háirithe maidir le himscrúduithe a dhéanamh nó maidir le faireachán </span><span class="ltx_text" id="Ch5.T11.4.3.2.2.1.2" style="font-size:90%;color:#FF0000;">a dhéanamh</span><span class="ltx_text" id="Ch5.T11.4.3.2.2.1.3" style="font-size:90%;"> ar chur chun feidhme beart i ndáil le rialaitheoir nó próiseálaí atá bunaithe i mBallstát eile.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.4.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.4.3.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.4.3.1.1" style="font-size:90%;">EN-2</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.4.3.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.4.3.2.1"><span class="ltx_text" id="Ch5.T11.4.4.3.2.1.1" style="font-size:90%;">The Office shall mention the judgment in the Register and shall take the necessary measures to comply with its operative </span><span class="ltx_text" id="Ch5.T11.4.4.3.2.1.2" style="font-size:90%;color:#0000FF;">part</span><span class="ltx_text" id="Ch5.T11.4.4.3.2.1.3" style="font-size:90%;">.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.5.4">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.5.4.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.5.4.1.1" style="font-size:90%;">GA-2</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.5.4.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.5.4.2.1"><span class="ltx_text" id="Ch5.T11.4.5.4.2.1.1" style="font-size:90%;">Luafaidh an Oifig an breithiúnas sa Chlár agus glacfaidh sí na bearta is gá chun cloí lena </span><span class="ltx_text" id="Ch5.T11.4.5.4.2.1.2" style="font-size:90%;color:#FF0000;">chuid</span><span class="ltx_text" id="Ch5.T11.4.5.4.2.1.3" style="font-size:90%;"> oibríochtúil.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.6.5">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.6.5.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.6.5.1.1" style="font-size:90%;">EN-3</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.6.5.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.6.5.2.1"><span class="ltx_text" id="Ch5.T11.4.6.5.2.1.1" style="font-size:90%;">The competent authority may at any time wholly or partially suspend or terminate the contract awarded under this provision if the operator </span><span class="ltx_text" id="Ch5.T11.4.6.5.2.1.2" style="font-size:90%;color:#0000FF;">fails</span><span class="ltx_text" id="Ch5.T11.4.6.5.2.1.3" style="font-size:90%;"> to meet the performance requirements.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.7.6">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.7.6.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.7.6.1.1" style="font-size:90%;">GA-3</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.7.6.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.7.6.2.1"><span class="ltx_text" id="Ch5.T11.4.7.6.2.1.1" style="font-size:90%;">Féadfaidh an t-údarás inniúil an conradh a dámhadh faoin bhforáil seo a chur ar fionraí nó a fhoirceannadh go hiomlán nó go páirteach </span><span class="ltx_text" id="Ch5.T11.4.7.6.2.1.2" style="font-size:90%;color:#FF0000;">má mhainníonn</span><span class="ltx_text" id="Ch5.T11.4.7.6.2.1.3" style="font-size:90%;"> an t-oibreoir na ceanglais feidhmíochta a chomhlíonadh.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.8.7">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.8.7.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.8.7.1.1" style="font-size:90%;">EN-4</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.8.7.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.8.7.2.1"><span class="ltx_text" id="Ch5.T11.4.8.7.2.1.1" style="font-size:90%;">This Directive shall enter into force on the day following that of its </span><span class="ltx_text" id="Ch5.T11.4.8.7.2.1.2" style="font-size:90%;color:#0000FF;">publication</span><span class="ltx_text" id="Ch5.T11.4.8.7.2.1.3" style="font-size:90%;"> in the Official Journal of the European Union.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.9.8">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.9.8.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.9.8.1.1" style="font-size:90%;">GA-4</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.9.8.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.9.8.2.1"><span class="ltx_text" id="Ch5.T11.4.9.8.2.1.1" style="font-size:90%;">Tiocfaidh an Treoir seo i bhfeidhm an lá tar éis lá </span><span class="ltx_text" id="Ch5.T11.4.9.8.2.1.2" style="font-size:90%;color:#FF0000;">a fhoilsithe</span><span class="ltx_text" id="Ch5.T11.4.9.8.2.1.3" style="font-size:90%;"> in Iris Oifigiúil an Aontais Eorpaigh.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.10.9">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.10.9.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.10.9.1.1" style="font-size:90%;">EN-5</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch5.T11.4.10.9.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.10.9.2.1"><span class="ltx_text" id="Ch5.T11.4.10.9.2.1.1" style="font-size:90%;">Such special measures are interim in nature, and </span><span class="ltx_text" id="Ch5.T11.4.10.9.2.1.2" style="font-size:90%;color:#0000FF;">shall not be</span><span class="ltx_text" id="Ch5.T11.4.10.9.2.1.3" style="font-size:90%;"> subject to the conditions set out in Article 7(1) and (2).</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch5.T11.4.11.10">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T11.4.11.10.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch5.T11.4.11.10.1.1" style="font-size:90%;">GA-5</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch5.T11.4.11.10.2" style="width:298.8pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch5.T11.4.11.10.2.1"><span class="ltx_text" id="Ch5.T11.4.11.10.2.1.1" style="font-size:90%;">Tá bearta speisialta den sórt sin eatramhach, agus </span><span class="ltx_text" id="Ch5.T11.4.11.10.2.1.2" style="font-size:90%;color:#FF0000;">ní bheidh said</span><span class="ltx_text" id="Ch5.T11.4.11.10.2.1.3" style="font-size:90%;"> faoi réir na gcoinníollacha a leagtar amach in Airteagal 7(1) agus (2) iad.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch5.S8.SS3.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Core Grammatical Errors</h5>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx2.p1">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx2.p1.1">Grammatical mistakes in the form of the misuse of lenitions (e.g., GA-4), incorrect pronouns (e.g., GA-5) and register errors (e.g., GA-5) were observed in both translation architectures. However, as is evident from both the automatic and MQM evaluations, there were far fewer errors with the Transformer model. Evidence of this can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T11" title="Table 5.11 ‣ Interpreting Meaning ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.11</span></a>. In the case of GA-4, the RNN model included the lenition in “a foilsithe”, whereas the Transformer model correctly removed “h”. The correct use of the feminine noun “treoir” requires the removal of “h” in “fhoilsithe”.</p>
</div>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx2.p2">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx2.p2.1">The misuse of pronouns was observed in the RNN translation model and, to a lesser degree, in the Transformer model. In the case of GA-5, the RNN’s incorrect use of the pronoun “ní bheidh siad” (they will not) is illustrated, whereas the Transformer approach used the correct form “ní bheidh sé” (he will not).</p>
</div>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx2.p3">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx2.p3.1">Within the same sentence, GA-5, there is also evidence of a register error. In the English source text EN-5, the use of “shall not be subject to” expresses a stipulation. This is not registered in the Irish translation of “ní bheidh said”, which simply, and less forcefully, means “they will not”. This incorrect use of register was observed with both the RNN and the Transformer approaches. A more formal and closer interpretation of the English source would be the use of the imperative mode: “ná bídís” (let it not be).</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S8.SS3.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Commonly-Used Irregular Verbs</h5>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx3.p1">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx3.p1.1">One of the main inadequacies observed in both the RNN and Transformer systems is a lack of refinement of verbal usage, particularly when using the verbs “déan” (to do or to make ) and “bí” (to be). As in many languages, the fact that these are possibly the two most universally used verbs in Irish further exacerbates the problem. An illustration of this problem can be seen in the output GA-1, which highlights the incorrect usage of “déan”. Similarly, GA-5 demonstrates how the system misinterprets the usage of the verb “bí”, e.g., “ní bheidh said”.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch5.S8.SS3.SSSx4">
<h5 class="ltx_title ltx_title_subsubsection">Performance of RNN Approach Relative to Transformer Approach</h5>
<div class="ltx_para" id="Ch5.S8.SS3.SSSx4.p1">
<p class="ltx_p" id="Ch5.S8.SS3.SSSx4.p1.1">There is a strong correlation between automatic and human evaluation of the translation systems that we developed. The automatic BLEU scores are contrasted with the human evaluation scores for both the RNN and Transformer models in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch5.T12" title="Table 5.12 ‣ Performance of RNN Approach Relative to Transformer Approach ‣ 5.8.3 Linguistic Observations ‣ 5.8 Discussion ‣ Chapter 5 Human Evaluation of EN↔GA Transformer-Based NMT"><span class="ltx_text ltx_ref_tag">5.12</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch5.T12">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5.12: </span>Transformer approach compared to the RNN approach across all metrics for the DGT dataset. The results from our human evaluation, using SQM and MQM metrics, validate the BLEU automatic evaluation results.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch5.T12.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T12.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch5.T12.3.3.4" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text ltx_font_bold" id="Ch5.T12.3.3.4.1" style="font-size:90%;">Approach</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T12.1.1.1" style="padding-left:30.9pt;padding-right:30.9pt;">
<span class="ltx_text ltx_font_bold" id="Ch5.T12.1.1.1.1" style="font-size:90%;">BLEU</span><span class="ltx_text" id="Ch5.T12.1.1.1.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T12.1.1.1.m1.1"><semantics id="Ch5.T12.1.1.1.m1.1a"><mo id="Ch5.T12.1.1.1.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T12.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T12.1.1.1.m1.1b"><ci id="Ch5.T12.1.1.1.m1.1.1.cmml" xref="Ch5.T12.1.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T12.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T12.1.1.1.m1.1d">bold_↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T12.2.2.2" style="padding-left:30.9pt;padding-right:30.9pt;">
<span class="ltx_text ltx_font_bold" id="Ch5.T12.2.2.2.1" style="font-size:90%;">SQM</span><span class="ltx_text" id="Ch5.T12.2.2.2.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T12.2.2.2.m1.1"><semantics id="Ch5.T12.2.2.2.m1.1a"><mo id="Ch5.T12.2.2.2.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T12.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T12.2.2.2.m1.1b"><ci id="Ch5.T12.2.2.2.m1.1.1.cmml" xref="Ch5.T12.2.2.2.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T12.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T12.2.2.2.m1.1d">bold_↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch5.T12.3.3.3" style="padding-left:30.9pt;padding-right:30.9pt;">
<span class="ltx_text ltx_font_bold" id="Ch5.T12.3.3.3.1" style="font-size:90%;">MQM</span><span class="ltx_text" id="Ch5.T12.3.3.3.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch5.T12.3.3.3.m1.1"><semantics id="Ch5.T12.3.3.3.m1.1a"><mo id="Ch5.T12.3.3.3.m1.1.1" mathsize="90%" mathvariant="bold" stretchy="false" xref="Ch5.T12.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch5.T12.3.3.3.m1.1b"><ci id="Ch5.T12.3.3.3.m1.1.1.cmml" xref="Ch5.T12.3.3.3.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T12.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.T12.3.3.3.m1.1d">bold_↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T12.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch5.T12.3.4.1.1" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text ltx_font_bold" id="Ch5.T12.3.4.1.1.1" style="font-size:90%;">Transformer</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T12.3.4.1.2" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text" id="Ch5.T12.3.4.1.2.1" style="font-size:90%;">60.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T12.3.4.1.3" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text" id="Ch5.T12.3.4.1.3.1" style="font-size:90%;">4.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T12.3.4.1.4" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text" id="Ch5.T12.3.4.1.4.1" style="font-size:90%;">77.92</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T12.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch5.T12.3.5.2.1" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text ltx_font_bold" id="Ch5.T12.3.5.2.1.1" style="font-size:90%;">RNN</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T12.3.5.2.2" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text" id="Ch5.T12.3.5.2.2.1" style="font-size:90%;">52.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T12.3.5.2.3" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text" id="Ch5.T12.3.5.2.3.1" style="font-size:90%;">3.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch5.T12.3.5.2.4" style="padding-left:30.9pt;padding-right:30.9pt;"><span class="ltx_text" id="Ch5.T12.3.5.2.4.1" style="font-size:90%;">43.05</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="Ch5.S8.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8.4 </span>Limitations of the Study</h4>
<div class="ltx_para" id="Ch5.S8.SS4.p1">
<p class="ltx_p" id="Ch5.S8.SS4.p1.1">Certain aspects of this study could be further developed, given more time and resources. Although there is a high inter-annotator agreement, it would help to have more annotators. In addition, the human evaluation of a greater number of lines, coupled with a more detailed MQM taxonomy, may provide greater insight into the MT outputs. This would help in uncovering other aspects, such as how gender is handled by the MT models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch5.S9">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.9 </span>Conclusions and Future Work</h3>
<div class="ltx_para" id="Ch5.S9.p1">
<p class="ltx_p" id="Ch5.S9.p1.2">With this research, we have presented the first human evaluation study that compares the output of EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch5.S9.p1.1.m1.1"><semantics id="Ch5.S9.p1.1.m1.1a"><mo id="Ch5.S9.p1.1.m1.1.1" stretchy="false" xref="Ch5.S9.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch5.S9.p1.1.m1.1b"><ci id="Ch5.S9.p1.1.m1.1.1.cmml" xref="Ch5.S9.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S9.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S9.p1.1.m1.1d">→</annotation></semantics></math>GA RNN systems with that of Transformer-based EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch5.S9.p1.2.m2.1"><semantics id="Ch5.S9.p1.2.m2.1a"><mo id="Ch5.S9.p1.2.m2.1.1" stretchy="false" xref="Ch5.S9.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch5.S9.p1.2.m2.1b"><ci id="Ch5.S9.p1.2.m2.1.1.cmml" xref="Ch5.S9.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S9.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S9.p1.2.m2.1d">→</annotation></semantics></math>GA systems. Automatic metrics were shown to differentiate the systems and highlighted that Transformer models are superior to RNN models. In our paper, we demonstrated that a random search approach to HPO enabled the development of high-performing translation models. We have shown there is a high level of correlation between a human evaluation and an automatic approach. Both the automatic metrics and our human evaluation demonstrated that the Transformer-based system is the most accurate.
</p>
</div>
<div class="ltx_para" id="Ch5.S9.p2">
<p class="ltx_p" id="Ch5.S9.p2.1">The importance of selecting hyperparameters when training low-resource Transformer models was also demonstrated. By increasing dropout and reducing the number of hidden-layer neurons, our models performed significantly better than Google Translate and our baseline models.</p>
</div>
<div class="ltx_para" id="Ch5.S9.p3">
<p class="ltx_p" id="Ch5.S9.p3.1">We have demonstrated that choosing the correct subword models is an important performance driver for low-resource MT. Within the context of low-resource EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch5.S9.p3.1.m1.1"><semantics id="Ch5.S9.p3.1.m1.1a"><mo id="Ch5.S9.p3.1.m1.1.1" stretchy="false" xref="Ch5.S9.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch5.S9.p3.1.m1.1b"><ci id="Ch5.S9.p3.1.m1.1.1.cmml" xref="Ch5.S9.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S9.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch5.S9.p3.1.m1.1d">→</annotation></semantics></math>GA translations, we achieved optimal performance on a 55k generic corpus when a Transformer architecture with a 16k BPE subword model was used. Improvements in the performance of our optimised Transformer models were observed across all key indicators, namely, PPL was achieved at a lower global minimum, with a lower post-editing effort and a higher translation accuracy.</p>
</div>
<div class="ltx_para" id="Ch5.S9.p4">
<p class="ltx_p" id="Ch5.S9.p4.1">As part of future work, steps can be taken to deal with the inadequacies highlighted in our linguistic analysis. The issue of misusing common irregular verbs could be addressed by fine-tuning our models with a dataset specifically tailored for that purpose. Similarly, fine-tuning after the careful selection of training data would also reduce the register errors encountered in our linguistic analysis. As it is difficult to train systems for all eventualities, using post-editing tools would be the best approach to correcting core grammatical errors involving pronouns, lenitions and lemmatization.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p5">
<p class="ltx_p" id="Ch5.S9.p5.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p5.1.1">Author Contributions:</span> All authors have read and agreed to the published version of the manuscript.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p6">
<p class="ltx_p" id="Ch5.S9.p6.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p6.1.1">Funding:</span> This work was supported by ADAPT, which is funded under the SFI Research Centres Programme (Grant 13/RC/2016) and is co-funded by the European Regional Development Fund. This research was also funded by the Munster Technological University.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p7">
<p class="ltx_p" id="Ch5.S9.p7.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p7.1.1">Informed Consent Statement:</span> Informed consent was obtained from all subjects involved in the study.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p8">
<p class="ltx_p" id="Ch5.S9.p8.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p8.1.1">Data Availability Statement:</span> The data presented in this study are openly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seamusl/isfeidirlinn" title="">https://github.com/seamusl/isfeidirlinn</a></p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p9">
<p class="ltx_p" id="Ch5.S9.p9.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p9.1.1">Acknowledgments:</span> We would like to thank the annotators, Dr Éamon Lankford and Ms. Máirín Lankford for their meticulous work in annotating the system outputs.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p10">
<p class="ltx_p" id="Ch5.S9.p10.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p10.1.1">Conflicts of Interest:</span>  The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p11">
<p class="ltx_p" id="Ch5.S9.p11.1"><span class="ltx_text ltx_font_bold" id="Ch5.S9.p11.1.1">Glossary</span></p>
</div>
<div class="ltx_para ltx_noindent" id="Ch5.S9.p12">
<p class="ltx_p" id="Ch5.S9.p12.1">Irish terms referenced and used in this manuscript:
<br class="ltx_break"/>
<span class="ltx_tabular ltx_align_middle" id="Ch5.S9.p12.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="Ch5.S9.p12.1.1.1.1">
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.1.1.1">Déan</span>
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.1.1.2">To do or to make</span></span>
<span class="ltx_tr" id="Ch5.S9.p12.1.1.2.2">
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.2.2.1">Bí</span>
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.2.2.2">To be</span></span>
<span class="ltx_tr" id="Ch5.S9.p12.1.1.3.3">
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.3.3.1">Ná bídís</span>
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.3.3.2">Let it not be</span></span>
<span class="ltx_tr" id="Ch5.S9.p12.1.1.4.4">
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.4.4.1">Ní bheidh siad</span>
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.4.4.2">They will not</span></span>
<span class="ltx_tr" id="Ch5.S9.p12.1.1.5.5">
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.5.5.1">Ní bheidh sé</span>
<span class="ltx_td ltx_align_left" id="Ch5.S9.p12.1.1.5.5.2">He will not</span></span>
</span>
</span></p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch6">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 6 </span>Design of an Open-Source Architecture for NMT</h2>
<section class="ltx_section" id="Ch6.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.1 </span>Context</h3>
<div class="ltx_para" id="Ch6.S1.p1">
<p class="ltx_p" id="Ch6.S1.p1.1">With RQ4, the process of how NMT development, evaluation, and deployment could be streamlined for both developers and translators was considered. The purpose of this short paper for the inaugural CrowdMT workshop at Tampere was to showcase the adaptNMT application <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx69" title="">69</a>]</cite>. As a workshop paper, this chapter is effectively a compressed version of the longer and more comprehensive journal paper outlined in Chapter 7. Readers who wish for an in-depth discussion of the adaptNMT application are encouraged to go directly to Chapter 7, adaptNMT: Open-Source Neural Machine Translation.</p>
</div>
<div class="ltx_para" id="Ch6.S1.p2">
<p class="ltx_p" id="Ch6.S1.p2.1">By simplifying NMT model development, building and deploying NMT models can be streamlined. This approach recognises that newcomers to the field face challenges in setting up the development environment and creating the necessary data splits for training, validation, and testing. The community is invited to contribute new ideas and improvements, and a spirit of collaboration will be fostered among researchers. The research contribution is to provide an open-source approach which can lead to the continuous improvement and evolution of the adaptNMT project.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch6.S1.p3">
<p class="ltx_p" id="Ch6.S1.p3.1"><span class="ltx_text ltx_font_bold" id="Ch6.S1.p3.1.1">Design of an Open-Source Architecture for Neural Machine Translation</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch6.S1.p3.2"><span class="ltx_text ltx_font_bold" id="Ch6.S1.p3.2.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch6.S1.p3.3"><span class="ltx_text ltx_font_bold" id="Ch6.S1.p3.3.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch6.S1.p3.4"><span class="ltx_text ltx_font_bold" id="Ch6.S1.p3.4.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch6.S1.p3.5">crowdMT23: Workshop on Open Community-Driven Machine Translation</p>
<p class="ltx_p" id="Ch6.S1.p3.6">June 15, 2023</p>
<p class="ltx_p" id="Ch6.S1.p3.7">Tampere, Finland.</p>
<p class="ltx_p" id="Ch6.S1.p3.8">ADAPT Centre</p>
<p class="ltx_p" id="Ch6.S1.p3.9">Dublin City University</p>
<p class="ltx_p" id="Ch6.S1.p3.10">Ireland</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch6.S1.p3.11"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.crowdmt-1.2.pdf" title="">https://aclanthology.org/2023.crowdmt-1.2.pdf</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch6.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch6.S2.p1">
<p class="ltx_p" id="Ch6.S2.p1.1">adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of recurrent neural networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of training, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customisation. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO<sub class="ltx_sub" id="Ch6.S2.p1.1.1">2</sub> emissions generated during model development. The application is freely available.<span class="ltx_note ltx_role_footnote" id="Ch6.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://github.com/adaptNMT" title="">http://github.com/adaptNMT</a></span></span></span></p>
</div>
</section>
<section class="ltx_section" id="Ch6.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.3 </span>Credits</h3>
<div class="ltx_para" id="Ch6.S3.p1">
<p class="ltx_p" id="Ch6.S3.p1.1">This research is supported by Science Foundation Ireland through the ADAPT Centre (Grant 13/RC/2106) (www.adaptcentre.ie) at Dublin City University. This research was also funded by the Munster Technological University.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.4 </span>Introduction</h3>
<div class="ltx_para" id="Ch6.S4.p1">
<p class="ltx_p" id="Ch6.S4.p1.1">Explainable Artificial Intelligence (XAI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx12" title="">12</a>]</cite> aims to ensure the outcomes of AI solutions are easily comprehensible to humans. In light of this goal, adaptNMT has been developed to provide users with a form of explainable neural machine translation (XNMT). The typical neural machine translation (NMT) process comprises several independent stages, including setting up the environment, preparing the dataset, training subword models, setting the parameters and training the main models, evaluating and deploying them. By adopting a modular approach, this framework has established an effective NMT model development process that caters to both technical and non-technical practitioners in the field. To address the environmental impact of building and running large AI models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx52" title="">52</a>]</cite>, we have also produced a “green report” that calculates carbon emissions. While primarily intended as an information aid, this report will hopefully encourage the development of reusable and sustainable models.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p2">
<p class="ltx_p" id="Ch6.S4.p2.1">This research endeavours to create models and applications that address the challenges of language technology, which will be particularly beneficial for those new to the field of Machine Translation (MT) and those seeking to learn more about NMT.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p3">
<p class="ltx_p" id="Ch6.S4.p3.1">The application is built on OpenNMT<span class="ltx_note ltx_role_footnote" id="Ch6.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opennmt.net" title="">https://opennmt.net</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite> and thus inherits all of its features. Unlike many NMT toolkits, a command line interface is not used, and the interface is designed and fully implemented in Google Colab.<span class="ltx_note ltx_role_footnote" id="Ch6.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://colab.research.google.com" title="">https://colab.research.google.com</a></span></span></span> For both educational and research purposes, a cloud-hosted solution like Colab is often more user-friendly. Additionally, the training of models can be monitored and controlled on a mobile phone using Google Colab’s responsive design, which is useful for long-run builds. The adaptNMT framework also includes GUI controls that allow for the customisation of all crucial parameters needed for NMT model training.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p4">
<p class="ltx_p" id="Ch6.S4.p4.1">The application can be run in local mode to utilise existing infrastructure or hosted mode for rapid infrastructure scaling. A deploy function is also included to allow for the immediate deployment of trained models.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p5">
<p class="ltx_p" id="Ch6.S4.p5.1">This paper begins by presenting background information on NMT and NMT tools in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S5" title="6.5 Related Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_tag">6.5</span></a>, followed by a detailed description of the adaptNMT architecture and its key features in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S6" title="6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_tag">6.6</span></a>. The system is discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S7" title="6.7 Discussion ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_tag">6.7</span></a> before concluding with a discussion of future work in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.S8" title="6.8 Conclusion and Future Work ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_tag">6.8</span></a>. A more in-depth system description, coupled with an empirical evaluation
of models developed using the application, is outlined in a separate paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx68" title="">68</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.5 </span>Related Work</h3>
<section class="ltx_subsection" id="Ch6.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5.1 </span>NMT</h4>
<div class="ltx_para" id="Ch6.S5.SS1.p1">
<p class="ltx_p" id="Ch6.S5.SS1.p1.1">In addition to the ongoing research dedicated to developing state-of-the-art (SOTA) NMT models, comprehensive descriptions of this technology are readily available in the literature, making it accessible to individuals who are new to the field or have limited technical expertise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx116" title="">116</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch6.S5.SS1.p2">
<p class="ltx_p" id="Ch6.S5.SS1.p2.1">NMT has benefited from the availability of large parallel corpora, leading to the development of high-performing MT models. The field of MT has experienced significant advancements through the application of NMT, particularly after the introduction of the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite> architecture, which has resulted in SOTA performance across multiple language pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx66" title="">66</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch6.S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5.2 </span>NMT Tools</h4>
<div class="ltx_para" id="Ch6.S5.SS2.p1">
<p class="ltx_p" id="Ch6.S5.SS2.p1.1">In essence, adaptNMT is an IPython wrapper built on OpenNMT, enabling it to benefit from OpenNMT’s extensive feature set and continuous code maintenance. However, adaptNMT takes abstraction to a higher level than OpenNMT, with a greater focus on usability, particularly for newcomers. As a result, adaptNMT facilitates easy and fast deployment, offering features such as more pre-processing, as well as GUI control over model creation. Moreover, it incorporates green features in line with current research efforts towards smaller models with reduced carbon footprints, making it suitable for educational and research environments alike.</p>
</div>
<div class="ltx_para" id="Ch6.S5.SS2.p2">
<p class="ltx_p" id="Ch6.S5.SS2.p2.1">Other commonly used frameworks for developing NMT systems include FAIRSEQ<span class="ltx_note ltx_role_footnote" id="Ch6.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairseq" title="">https://github.com/facebookresearch/fairseq</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx87" title="">87</a>]</cite>, an open-source sequence modelling toolkit based on PyTorch that allows for training models for translation, summarisation, and language modelling. Marian<span class="ltx_note ltx_role_footnote" id="Ch6.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://marian-nmt.github.io" title="">https://marian-nmt.github.io</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx53" title="">53</a>]</cite>, on the other hand, is an NMT framework based on dynamic computation graphs and developed using C++. OpenNMT is an open-source NMT framework that has been widely adopted in the research community and covers the entire MT workflow from data preparation to live inference.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch6.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.6 </span>Architecture of adaptNMT</h3>
<div class="ltx_para" id="Ch6.S6.p1">
<p class="ltx_p" id="Ch6.S6.p1.1">After providing a general overview of NMT and NMT development systems, we introduce the adaptNMT tool, which enables users to configure the components of the NMT development process. The platform’s system architecture is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.F1" title="Figure 6.1 ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_tag">6.1</span></a>. The tool is built as an IPython notebook and leverages the Pytorch implementation of OpenNMT for training models. Additionally, SentencePiece is used to train subword models. Using a Jupyter notebook facilitates sharing the application with other members of the MT community, and the application’s setup is simplified since all necessary packages are downloaded dynamically as the application runs.</p>
</div>
<figure class="ltx_figure" id="Ch6.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch6.F1.g1" src="autoNMT_arch.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F1.2.1.1" style="font-size:90%;">Figure 6.1</span>: </span><span class="ltx_text" id="Ch6.F1.3.2" style="font-size:90%;">Proposed architecture for adaptNMT: a language-agnostic NMT development environment. The system is designed to run either in the cloud or using local infrastructure. Models are trained using parallel corpora. Visualisation and extensive logging enable real-time monitoring. Models are developed using vanilla RNN-based NMT, Transformer-based approaches or transfer learning using a fine-tuning approach. Translation and evaluation can be carried out using either single models or ensembles.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S6.p2">
<p class="ltx_p" id="Ch6.S6.p2.1">The system has two deployment options: running it locally or as a Colab instance via Google Cloud. To build translation models, the system requires parallel text corpora for both the source and target languages. A Tensorboard visualization allows for real-time monitoring of the model training process. At run time, users can select to use the system for either model building or translation services, or both. Additionally, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch6.F1" title="Figure 6.1 ‣ 6.6 Architecture of adaptNMT ‣ Chapter 6 Design of an Open-Source Architecture for NMT"><span class="ltx_text ltx_ref_tag">6.1</span></a>, the system enables the generation of an ensemble output during translation. Finally, trained models can be easily deployed to a pre-configured location.</p>
</div>
<section class="ltx_subsection" id="Ch6.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6.1 </span>adaptNMT</h4>
<div class="ltx_para" id="Ch6.S6.SS1.p1">
<p class="ltx_p" id="Ch6.S6.SS1.p1.1">The application may be run as an IPython Jupyter notebook or as a Google Colab application. Given the ease of integrating large Google Drive storage into Colab, the application has been used exclusively as a Google Colab application for our experiments.</p>
</div>
<section class="ltx_subsubsection" id="Ch6.S6.SS1.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Initialisation and Logging</h5>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx1.p1">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx1.p1.1">Initialisation enables connection to Google Drive to run experiments, automatic installation of Python, OpenNMT,<span class="ltx_note ltx_role_footnote" id="Ch6.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opennmt.net" title="">https://opennmt.net</a></span></span></span> SentencePiece,<span class="ltx_note ltx_role_footnote" id="Ch6.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/sentencepiece" title="">https://github.com/google/sentencepiece</a></span></span></span> Pytorch and other applications. The visualisation section enables real-time graphing of model development. All log files are stored and can be viewed to inspect training convergence, the model’s training and validation accuracy and changes in learning rates.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch6.S6.SS1.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Modes of Operation</h5>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx2.p1">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx2.p1.1">There are two modes of operation: local and cloud. In local mode, the application is run so that models are built using the user’s local GPU resources. The option to use cloud mode enables users to develop models using Google’s GPU clusters. For shorter training times, the unpaid Colab option is adequate. However, for a small monthly subscription, the Google Colab Pro option is worthwhile since users have access to improved GPU and compute resources. Furthermore, using Google Cloud may be considered as the “green option” since its platform uses 100% renewables <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch6.S6.SS1.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Customisation of Models</h5>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx3.p1">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx3.p1.1">The system has been developed to allow users to select variations to the underlying model architecture. A vanilla RNN or Transformer approach may be selected to develop the NMT model. The customisation mode enables users to specify the exact parameters required for the chosen approach. One of the features, AutoBuild, enables a user to build an NMT model in three simple steps: (i) upload source and target files, (ii) select RNN or Transformer, and (iii) click AutoBuild.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch6.S6.SS1.SSSx4">
<h5 class="ltx_title ltx_title_subsubsection">Use of Subword Segmentation</h5>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx4.p1">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx4.p1.1">In the NMT development process, users can specify the type of optimiser for learning and choose from different subword models. The subword model functionality allows for the selection of a subword model type and the choice of vocabulary size, currently offering either a SentencePiece unigram or a SentencePiece BPE model.</p>
</div>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx4.p2">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx4.p2.1">A user may upload a dataset which includes the train, validation and test splits for both source and target languages. In cases where a user has not already created the required splits for model training, single source and target files may be uploaded. Automated splitting of the uploaded dataset into the train, validation, and test files is then performed based on the user’s chosen split ratio.</p>
</div>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx4.p3">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx4.p3.1">Given that building NMT models typically demands long training times, an automatic notification feature is incorporated that informs the user by email when model training has been completed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch6.S6.SS1.SSSx5">
<h5 class="ltx_title ltx_title_subsubsection">Translation and Evaluation</h5>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx5.p1">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx5.p1.1">The application supports not only the training of models but also the translation and evaluation of model performance. For translation using pre-built models, users can specify the model name as a hyperparameter which is subsequently used to translate and evaluate the test files. The option for creating an ensemble output is also available, with users simply naming the models to be used in generating the ensemble output.</p>
</div>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx5.p2">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx5.p2.1">Once the system has been built, the user can select the model to be used for translating the test set. While human evaluation is often considered the most insightful approach for evaluating translation quality, it can be limited by factors such as availability, cost, and subjectivity. Thus, automatic evaluation metrics are frequently employed, particularly by developers monitoring the incremental progress of systems. A further discussion on the advantages and disadvantages of human and automatic evaluation is available in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx115" title="">115</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx5.p3">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx5.p3.1">Several automatic evaluation metrics provided by SacreBleu<span class="ltx_note ltx_role_footnote" id="Ch6.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mjpost/sacrebleu" title="">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx91" title="">91</a>]</cite> are used: BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite>. Translation quality can also be evaluated using Meteor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx34" title="">34</a>]</cite> and F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx80" title="">80</a>]</cite>. Note that BLEU, ChrF, Meteor and F1 are precision-based metrics, so higher scores are better, whereas TER is an error-based metric and lower scores indicate better translation quality. Evaluation options available include standard (truecase) and lowercase BLEU scores, a sentence-level BLEU score option, ChrF1 and ChrF3.</p>
</div>
<div class="ltx_para" id="Ch6.S6.SS1.SSSx5.p4">
<p class="ltx_p" id="Ch6.S6.SS1.SSSx5.p4.1">There are three levels of logging: model development logs for graphing, training console output and experimental results. A references section outlines resources which are relevant to developing, using and understanding adaptNMT. Validation during training is currently conducted using model accuracy and perplexity (PPL).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch6.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6.2 </span>Infrastructure</h4>
<div class="ltx_para" id="Ch6.S6.SS2.p1">
<p class="ltx_p" id="Ch6.S6.SS2.p1.1">Rapid prototype development is possible through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16GB graphic cards and up to 27GB of memory when available.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch6.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.7 </span>Discussion</h3>
<div class="ltx_para" id="Ch6.S7.p1">
<p class="ltx_p" id="Ch6.S7.p1.1">Numerous tools have been developed to assess the carbon footprint of NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx11" title="">11</a>]</cite>. The notion of sustainable NLP has also gained momentum as an independent research track, with high-profile conferences such as the <span class="ltx_text ltx_font_italic" id="Ch6.S7.p1.1.1">EACL 2021 Green and Sustainable NLP</span> track dedicating resources to this area.<span class="ltx_note ltx_role_footnote" id="Ch6.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://2021.eacl.org/news/green-and-sustainable-nlp" title="">https://2021.eacl.org/news/green-and-sustainable-nlp</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch6.S7.p2">
<p class="ltx_p" id="Ch6.S7.p2.1">Given these developments, we have incorporated a “green report” into adaptNMT that logs the kgCO<sub class="ltx_sub" id="Ch6.S7.p2.1.1">2</sub> generated during model development. This aligns with the industry’s increasing focus on quantifying the environmental impact of NLP. In fact, it has been demonstrated that high-performing MT systems can be developed with much lower carbon footprints, leading to significant energy cost savings for a real translation company <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx51" title="">51</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch6.S7.p3">
<p class="ltx_p" id="Ch6.S7.p3.1">The risks associated with relying on large language models (LLMs) have been well-documented in the literature. The discussion surrounding these models emphasises not only their environmental impact but also the inherent biases and dangers they pose for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx15" title="">15</a>]</cite>. It is important to note that smaller, in-domain datasets can yield high-performing NMT models, and the adaptNMT framework makes this approach easily accessible and understandable.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.8 </span>Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch6.S8.p1">
<p class="ltx_p" id="Ch6.S8.p1.1">We have introduced adaptNMT, an application that manages the entire NMT model development, evaluation, and deployment workflow.</p>
</div>
<div class="ltx_para" id="Ch6.S8.p2">
<p class="ltx_p" id="Ch6.S8.p2.1">As for future work, our development efforts will be directed towards incorporating new transfer learning methods and improving our ability to track environmental costs. We will integrate modern zero-shot and few-shot approaches, as seen in the GPT3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx23" title="">23</a>]</cite> and Facebook LASER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx7" title="">7</a>]</cite> frameworks. While the existing adaptNMT application is focused on customising NMT models, we will also develop a separate application, adaptMLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx67" title="">67</a>]</cite>, for fine-tuning multilingual language models (MLLMs) and LLMs. In particular, adaptMLLM will cater for low-resource language pairs covered by Meta’s NLLB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx32" title="">32</a>]</cite> models.</p>
</div>
<div class="ltx_para" id="Ch6.S8.p3">
<p class="ltx_p" id="Ch6.S8.p3.1">The green report integrated into the application represents our first implementation of a sustainable NLP feature within adaptNMT. We plan to enhance this feature by improving the user interface and providing recommendations on how to develop greener models. As an open-source project, we invite the community to contribute new ideas and improvements to the development of this feature.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch7">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 7 </span>adaptNMT: Open-Source Neural Machine Translation</h2>
<section class="ltx_section" id="Ch7.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.1 </span>Context</h3>
<div class="ltx_para" id="Ch7.S1.p1">
<p class="ltx_p" id="Ch7.S1.p1.1">The motivation for the research on adaptNMT is to address several key challenges in the field of NMT and NLP in general. Overall, the research behind adaptNMT addresses critical issues in NMT development and deployment, while also promoting sustainable and collaborative practices within the NLP community.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p2">
<p class="ltx_p" id="Ch7.S1.p2.1">The primary motivations for this research were initially raised in RQ4 and are outlined in more detail below:</p>
</div>
<div class="ltx_para" id="Ch7.S1.p3">
<ul class="ltx_itemize" id="Ch7.S1.I1">
<li class="ltx_item" id="Ch7.S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i1.p1">
<p class="ltx_p" id="Ch7.S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i1.p1.1.1">Streamlining Development and Deployment Processes</span>: The primary goal of adaptNMT is to simplify and streamline all aspects of developing and deploying RNN and Transformer neural translation models. This includes processes like setting up the development environment, creating data splits, and training the models.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i2.p1">
<p class="ltx_p" id="Ch7.S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i2.p1.1.1">Accessibility for Both Developers and Translators</span>: The application is designed to be user-friendly for individuals with varying levels of technical expertise. This inclusivity aims to make MT more accessible to a wider audience, including those who may not have extensive technical backgrounds.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i3.p1">
<p class="ltx_p" id="Ch7.S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i3.p1.1.1">Support for New Entrants in the Field</span>: adaptNMT is particularly valuable for newcomers to the field of MT. By simplifying the setup and training process, it lowers the barrier to entry, allowing more individuals to participate in NLP research and development.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i4.p1">
<p class="ltx_p" id="Ch7.S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i4.p1.1.1">Built Upon the OpenNMT Ecosystem</span>: By leveraging the widely adopted OpenNMT ecosystem, adaptNMT benefits from a strong foundation of established tools and resources in the NMT community.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i5.p1">
<p class="ltx_p" id="Ch7.S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i5.p1.1.1">Visualisation and Monitoring of Model Training</span>: The application provides visualisations to track the progress of model training. This feature aids researchers and developers in understanding how their models are evolving during the training process.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i6.p1">
<p class="ltx_p" id="Ch7.S1.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i6.p1.1.1">Utilisation of SentencePiece for Subword Segmentation Models</span>: SentencePiece is employed to create subword segmentation models. This helps in handling morphologically rich languages and improving the overall performance of the translation models.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i7.p1">
<p class="ltx_p" id="Ch7.S1.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i7.p1.1.1">Intuitive Hyperparameter Customisation</span>: The application offers an intuitive user interface (UI) for customising hyperparameters. This facilitates experimentation with different settings to optimise model performance.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i8.p1">
<p class="ltx_p" id="Ch7.S1.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i8.p1.1.1">Efficient Model Development with Single-Click Approach</span>: The one-click model development approach simplifies and accelerates the process of creating and fine-tuning translation models.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i9.p1">
<p class="ltx_p" id="Ch7.S1.I1.i9.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i9.p1.1.1">Comprehensive Model Evaluation and Deployment</span>: Models developed with adaptNMT can be rigorously evaluated using a range of metrics. Additionally, the application allows for easy deployment of these models as a translation service.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i10.p1">
<p class="ltx_p" id="Ch7.S1.I1.i10.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i10.p1.1.1">Environmental Considerations</span>: A strong emphasis has been placed on eco-friendly practices in NLP research. A “green report” feature that tracks the power consumption and carbon emissions generated has been implemented which promotes sustainable research practices.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i11.p1">
<p class="ltx_p" id="Ch7.S1.I1.i11.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i11.p1.1.1">Demonstrated Success in Shared Tasks</span>: The performance of adaptNMT was validated by generating an EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S1.I1.i11.p1.1.m1.1"><semantics id="Ch7.S1.I1.i11.p1.1.m1.1a"><mo id="Ch7.S1.I1.i11.p1.1.m1.1.1" stretchy="false" xref="Ch7.S1.I1.i11.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S1.I1.i11.p1.1.m1.1b"><ci id="Ch7.S1.I1.i11.p1.1.m1.1.1.cmml" xref="Ch7.S1.I1.i11.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.I1.i11.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.I1.i11.p1.1.m1.1d">→</annotation></semantics></math>GA translation model, which achieved 1st place in the LoResMT2021 shared task. This success demonstrates the practical effectiveness of the application.</p>
</div>
</li>
<li class="ltx_item" id="Ch7.S1.I1.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch7.S1.I1.i12.p1">
<p class="ltx_p" id="Ch7.S1.I1.i12.p1.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.I1.i12.p1.1.1">Expanding to Fine-Tuning Large Language Models</span>: While adaptNMT focuses on customising NMT models, a separate application, adaptMLLM, has been developed to fine-tune MLLMs and LLMs. This is particularly important for low-resource language pairs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch7.S1.p4">
<p class="ltx_p" id="Ch7.S1.p4.1"><span class="ltx_text ltx_font_bold" id="Ch7.S1.p4.1.1">adaptNMT: an open-source,
language-agnostic development environment for Neural MT </span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch7.S1.p4.2"><span class="ltx_text ltx_font_bold" id="Ch7.S1.p4.2.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch7.S1.p4.3"><span class="ltx_text ltx_font_bold" id="Ch7.S1.p4.3.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch7.S1.p4.4"><span class="ltx_text ltx_font_bold" id="Ch7.S1.p4.4.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch7.S1.p4.5">Journal of Language Resources Evaluation, Springer</p>
<p class="ltx_p" id="Ch7.S1.p4.6">March 1st 2023</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch7.S1.p4.7">ADAPT Centre</p>
<p class="ltx_p" id="Ch7.S1.p4.8">Dublin City University</p>
<p class="ltx_p" id="Ch7.S1.p4.9">Ireland</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch7.S1.p4.10"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.springer.com/article/10.1007/s10579-023-09671-2" title="">https://link.springer.com/article/10.1007/s10579-023-09671-2</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch7.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch7.S2.p1">
<p class="ltx_p" id="Ch7.S2.p1.1">adaptNMT streamlines all processes involved in the development and deployment of RNN and Transformer neural translation models. As an open-source application, it is designed for both technical and non-technical users who work in the field of MT. Built upon the widely-adopted OpenNMT ecosystem, the application is particularly useful for new entrants to the field since the setup of the development environment and creation of training, validation and test splits is greatly simplified. Graphing, embedded within the application, illustrates the progress of model training, and SentencePiece is used for creating subword segmentation models. Hyperparameter customisation is facilitated through an intuitive user interface, and a single-click model development approach has been implemented. Models developed by adaptNMT can be evaluated using a range of metrics and deployed as a translation service within the application. To support eco-friendly research in the NLP space, a green report also flags the power consumption and kgCO<sub class="ltx_sub" id="Ch7.S2.p1.1.1">2</sub> emissions generated during model development. The application is freely available.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://github.com/adaptNMT" title="">http://github.com/adaptNMT</a></span></span></span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch7.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.3 </span>Graphical abstract</h3>
<figure class="ltx_figure" id="Ch7.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="381" id="Ch7.F1.g1" src="extracted/5444776/Images/adaptNMT-graphical-abstract.png" width="608"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F1.2.1.1" style="font-size:90%;">Figure 7.1</span>: </span><span class="ltx_text" id="Ch7.F1.3.2" style="font-size:90%;">Graphical abstract summarising the adaptNMT system</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch7.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.4 </span>Introduction</h3>
<div class="ltx_para" id="Ch7.S4.p1">
<p class="ltx_p" id="Ch7.S4.p1.1">Explainable artificial intelligence (XAI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx12" title="">12</a>]</cite> seeks to ensure that the results of AI solutions are easily understood by humans. It is against this backdrop that adaptNMT has been developed to afford users a form of <span class="ltx_text ltx_font_italic" id="Ch7.S4.p1.1.1">explainable neural machine translation (XNMT)</span>. The stages involved in a typical neural machine translation (NMT) process are broken down into a series of independent steps including environment setup, dataset preparation, training of subword models, parameterising and training of main models, evaluation and deployment. This modular approach has created an effective NMT model development process for both technical and less technical practitioners in the field. Given the environmental impact of building and running large AI models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx52" title="">52</a>]</cite>, we also compute carbon emissions in a “green report”, primarily as an information aid, but hopefully as a way to encourage reusable and sustainable model development.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p2">
<p class="ltx_p" id="Ch7.S4.p2.1">An important part of this research involves developing applications and models to address the challenges of language technology. It is hoped that such work will be of particular benefit to newcomers to the field of machine translation (MT) and in particular to those who wish to learn more about NMT.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p3">
<p class="ltx_p" id="Ch7.S4.p3.1">To have a thorough understanding of how NMT models are trained, the individual components and the mathematical concepts underpinning both RNN- and Transformer-based models are explained and illustrated in this paper. The application is built upon OpenNMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite> and subsequently inherits all of its features. Unlike many NMT toolkits, a command line interface approach is not used. The interface is designed and fully implemented in Google Colab.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://colab.research.google.com" title="">https://colab.research.google.com</a></span></span></span> For an educational setting, and indeed for research practitioners, a Colab cloud-hosted<span class="ltx_note ltx_role_footnote" id="Ch7.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com" title="">https://cloud.google.com</a></span></span></span> solution is often more intuitive to use. Furthermore, the training of models can be viewed and controlled using the Google Colab responsive website which is ideal for builds with long run times. GUI controls, also implemented within adaptNMT, enable the customisation of all key parameters required when training NMT models.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p4">
<p class="ltx_p" id="Ch7.S4.p4.1">The application can be run in local mode enabling existing infrastructure to be utilised, or in hosted mode which allows for rapid scaling of the infrastructure. A deploy function allows for the immediate deployment of trained models.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p5">
<p class="ltx_p" id="Ch7.S4.p5.1">This paper is organised by initially presenting background information on NMT and related work on system-building environments in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5" title="7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a>. This is followed by a detailed description of the adaptNMT architecture and its key features in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6" title="7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a>. An empirical evaluation of models is carried out in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7" title="7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a>. The system is discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S8" title="7.8 Discussion ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a> before drawing conclusions and describing future work in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S9" title="7.9 Conclusion and Future Work ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.9</span></a>. For newcomers to the field, we suggest going straight to Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6" title="7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a> to examine the platform’s capabilities, and then discovering more about the various components and their statistical underpinning in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5" title="7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a>. This can be followed by the remaining sections in their logical sequence.</p>
</div>
</section>
<section class="ltx_section" id="Ch7.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.5 </span>Neural Networks for MT</h3>
<section class="ltx_subsection" id="Ch7.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.1 </span>Recurrent Neural Network Architecture</h4>
<div class="ltx_para" id="Ch7.S5.SS1.p1">
<p class="ltx_p" id="Ch7.S5.SS1.p1.1">Recurrent neural networks (RNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx5" title="">5</a>]</cite> are often used for the tasks of natural language processing (NLP), speech recognition and MT. RNNs, such as long short-term memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx48" title="">48</a>]</cite>, were designed to support sequences of input data. LSTM models use an encoder-decoder architecture which enables variable length input sequences to predict variable length output sequences. This architecture is the cornerstone of many complex sequence prediction problems such as speech recognition and MT.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS1.p2">
<p class="ltx_p" id="Ch7.S5.SS1.p2.1">RNN models enable previous outputs to be used as inputs through the use of hidden states. In the context of MT, such neural networks were ideal due to their ability to process inputs of any length. In the initial stages of NMT, the RNN encoder-decoder framework was adopted and variable-length source sentences were encoded as fixed-length vectors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx109" title="">109</a>]</cite>. An improvement upon the basic RNN approach was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx10" title="">10</a>]</cite> which enhanced translation performance of the basic encoder-decoder architecture by replacing fixed-length vectors with variable-length vectors. A bidirectional RNN was now employed to read input sentences in the forward direction to produce forward hidden states while also producing backward hidden states by reading input sentences in the reverse direction. This development enabled neural networks to more accurately process long sentences, which previously had served as bottlenecks to performance, given their tendency to ‘forget’ words in long input sequences which are ‘too far away’ from the current word being processed.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS1.p3">
<p class="ltx_p" id="Ch7.S5.SS1.p3.1">More importantly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx10" title="">10</a>]</cite> introduced the concept of ‘attention’ to the basic RNN architecture, similar in spirit and intention to ‘alignments’ in the forerunner to NMT, statistical MT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx84" title="">84</a>]</cite>. In attention-augmented NMT, the system could now pay special heed to the most relevant other source-sentence words and use them as contextual clues when considering how best to select the most appropriate target words for translationally ambiguous words in the same string.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.2 </span>Transformer Architecture</h4>
<div class="ltx_para" id="Ch7.S5.SS2.p1">
<p class="ltx_p" id="Ch7.S5.SS2.p1.1">Following the introduction of the attention mechanism, a natural line of investigation was to see whether attention could do most of the heavy lifting of translation by itself. Accordingly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite> proposed that “attention is all you need” in their ‘Transformer architecture’, which has achieved state-of-the-art (SOTA) performance on many NLP benchmarks by relying solely on an attention mechanism, removing recurrence and convolution while allowing the use of much simpler feed-forward neural networks.</p>
</div>
<figure class="ltx_figure" id="Ch7.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch7.F2.g1" src="transformer.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F2.2.1.1" style="font-size:90%;">Figure 7.2</span>: </span><span class="ltx_text" id="Ch7.F2.3.2" style="font-size:90%;">The Transformer architecture using an encoder-decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite>. The encoder maps an input sequence to the decoder. The decoder generates a new output by combining the encoder output with the decoder output from the previous step.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S5.SS2.p2">
<p class="ltx_p" id="Ch7.S5.SS2.p2.1">This approach follows an encoder-decoder structure and allows models to develop a long memory which is particularly useful in the area of language translation. The task of the encoder is to map an input sequence to a sequence of continuous representations, which is then passed to a decoder to generate an output sequence by using the output of the encoder together with the decoder output from the previous time step. Both the encoder and decoder each consist of a stack of 6 identical layers, whose structure is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F2" title="Figure 7.2 ‣ 7.5.2 Transformer Architecture ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.2</span></a>. In the encoder, each layer is composed of two sub-layers: a multi-head self-attention mechanism and a fully connected feed-forward network. In the case of the decoder, there are three sub-layers: one which takes the previous output of the decoder stack, another which implements a multi-head self-attention mechanism, and the final layer which implements a fully connected feed-forward network.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S5.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.3 </span>Attention</h4>
<div class="ltx_para" id="Ch7.S5.SS3.p1">
<p class="ltx_p" id="Ch7.S5.SS3.p1.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F3" title="Figure 7.3 ‣ 7.5.3 Attention ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.3</span></a>, the attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key, as shown in Equation <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E1" title="7.1 ‣ 7.5.3 Attention ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="Ch7.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch7.F3.g1" src="attention.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F3.2.1.1" style="font-size:90%;">Figure 7.3</span>: </span><span class="ltx_text" id="Ch7.F3.3.2" style="font-size:90%;">Multi-head attention in the decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite>. In the decoder, a multi-head layer receives queries from the previous decoder sublayer, and the keys and values from the encoder output. The decoder can now attend to all words in the input sequence.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S5.SS3.p2">
<p class="ltx_p" id="Ch7.S5.SS3.p2.1">The query, keys and values used as inputs to the attention mechanism are different projections of the same input sentence (‘self-attention’) and capture the relationships between the different words of the same sentence.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS3.p3">
<p class="ltx_p" id="Ch7.S5.SS3.p3.5">Both a scaled dot-product attention and a multi-head attention are used in the Transformer architecture. With scaled dot-product attention, a dot product is initially computed for each query <math alttext="q" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p3.1.m1.1"><semantics id="Ch7.S5.SS3.p3.1.m1.1a"><mi id="Ch7.S5.SS3.p3.1.m1.1.1" xref="Ch7.S5.SS3.p3.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p3.1.m1.1b"><ci id="Ch7.S5.SS3.p3.1.m1.1.1.cmml" xref="Ch7.S5.SS3.p3.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p3.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p3.1.m1.1d">italic_q</annotation></semantics></math> with all of the keys <math alttext="k" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p3.2.m2.1"><semantics id="Ch7.S5.SS3.p3.2.m2.1a"><mi id="Ch7.S5.SS3.p3.2.m2.1.1" xref="Ch7.S5.SS3.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p3.2.m2.1b"><ci id="Ch7.S5.SS3.p3.2.m2.1.1.cmml" xref="Ch7.S5.SS3.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p3.2.m2.1d">italic_k</annotation></semantics></math>. The input consists of queries and keys of dimension <math alttext="d_{k}" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p3.3.m3.1"><semantics id="Ch7.S5.SS3.p3.3.m3.1a"><msub id="Ch7.S5.SS3.p3.3.m3.1.1" xref="Ch7.S5.SS3.p3.3.m3.1.1.cmml"><mi id="Ch7.S5.SS3.p3.3.m3.1.1.2" xref="Ch7.S5.SS3.p3.3.m3.1.1.2.cmml">d</mi><mi id="Ch7.S5.SS3.p3.3.m3.1.1.3" xref="Ch7.S5.SS3.p3.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p3.3.m3.1b"><apply id="Ch7.S5.SS3.p3.3.m3.1.1.cmml" xref="Ch7.S5.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS3.p3.3.m3.1.1.1.cmml" xref="Ch7.S5.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="Ch7.S5.SS3.p3.3.m3.1.1.2.cmml" xref="Ch7.S5.SS3.p3.3.m3.1.1.2">𝑑</ci><ci id="Ch7.S5.SS3.p3.3.m3.1.1.3.cmml" xref="Ch7.S5.SS3.p3.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p3.3.m3.1c">d_{k}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p3.3.m3.1d">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. Subsequently, each result is divided by <math alttext="\sqrt{d_{k}}" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p3.4.m4.1"><semantics id="Ch7.S5.SS3.p3.4.m4.1a"><msqrt id="Ch7.S5.SS3.p3.4.m4.1.1" xref="Ch7.S5.SS3.p3.4.m4.1.1.cmml"><msub id="Ch7.S5.SS3.p3.4.m4.1.1.2" xref="Ch7.S5.SS3.p3.4.m4.1.1.2.cmml"><mi id="Ch7.S5.SS3.p3.4.m4.1.1.2.2" xref="Ch7.S5.SS3.p3.4.m4.1.1.2.2.cmml">d</mi><mi id="Ch7.S5.SS3.p3.4.m4.1.1.2.3" xref="Ch7.S5.SS3.p3.4.m4.1.1.2.3.cmml">k</mi></msub></msqrt><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p3.4.m4.1b"><apply id="Ch7.S5.SS3.p3.4.m4.1.1.cmml" xref="Ch7.S5.SS3.p3.4.m4.1.1"><root id="Ch7.S5.SS3.p3.4.m4.1.1a.cmml" xref="Ch7.S5.SS3.p3.4.m4.1.1"></root><apply id="Ch7.S5.SS3.p3.4.m4.1.1.2.cmml" xref="Ch7.S5.SS3.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="Ch7.S5.SS3.p3.4.m4.1.1.2.1.cmml" xref="Ch7.S5.SS3.p3.4.m4.1.1.2">subscript</csymbol><ci id="Ch7.S5.SS3.p3.4.m4.1.1.2.2.cmml" xref="Ch7.S5.SS3.p3.4.m4.1.1.2.2">𝑑</ci><ci id="Ch7.S5.SS3.p3.4.m4.1.1.2.3.cmml" xref="Ch7.S5.SS3.p3.4.m4.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p3.4.m4.1c">\sqrt{d_{k}}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p3.4.m4.1d">square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> and a Softmax function is applied. The process leads to the weights which are used to scale the values, <math alttext="v" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p3.5.m5.1"><semantics id="Ch7.S5.SS3.p3.5.m5.1a"><mi id="Ch7.S5.SS3.p3.5.m5.1.1" xref="Ch7.S5.SS3.p3.5.m5.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p3.5.m5.1b"><ci id="Ch7.S5.SS3.p3.5.m5.1.1.cmml" xref="Ch7.S5.SS3.p3.5.m5.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p3.5.m5.1c">v</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p3.5.m5.1d">italic_v</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS3.p4">
<p class="ltx_p" id="Ch7.S5.SS3.p4.1">The Softmax function allows us to perform multiclass classification which makes it a good choice in the final layer of neural network-based classifiers. The function forces the outputs of the neural network to a total sum to 1, which can be viewed as a probability distribution across multiple classes. Therefore, Softmax is the ideal choice as the output activation function, given that NMT is essentially a multiclass classification problem where the output classes represent the words within the vocabulary.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS3.p5">
<p class="ltx_p" id="Ch7.S5.SS3.p5.3">Computations performed by scaled dot-product attention can be efficiently applied to the entire set of queries simultaneously. To achieve this, the matrices, <math alttext="Q" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p5.1.m1.1"><semantics id="Ch7.S5.SS3.p5.1.m1.1a"><mi id="Ch7.S5.SS3.p5.1.m1.1.1" xref="Ch7.S5.SS3.p5.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p5.1.m1.1b"><ci id="Ch7.S5.SS3.p5.1.m1.1.1.cmml" xref="Ch7.S5.SS3.p5.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p5.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p5.1.m1.1d">italic_Q</annotation></semantics></math>, <math alttext="K" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p5.2.m2.1"><semantics id="Ch7.S5.SS3.p5.2.m2.1a"><mi id="Ch7.S5.SS3.p5.2.m2.1.1" xref="Ch7.S5.SS3.p5.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p5.2.m2.1b"><ci id="Ch7.S5.SS3.p5.2.m2.1.1.cmml" xref="Ch7.S5.SS3.p5.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p5.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p5.2.m2.1d">italic_K</annotation></semantics></math> and <math alttext="V" class="ltx_Math" display="inline" id="Ch7.S5.SS3.p5.3.m3.1"><semantics id="Ch7.S5.SS3.p5.3.m3.1a"><mi id="Ch7.S5.SS3.p5.3.m3.1.1" xref="Ch7.S5.SS3.p5.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS3.p5.3.m3.1b"><ci id="Ch7.S5.SS3.p5.3.m3.1.1.cmml" xref="Ch7.S5.SS3.p5.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS3.p5.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS3.p5.3.m3.1d">italic_V</annotation></semantics></math>, are supplied as inputs to the attention function:</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS3.p6">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="attention(Q,K,V)=softmax(QK^{T}/\sqrt{d_{k}})V" class="ltx_Math" display="block" id="Ch7.E1.m1.4"><semantics id="Ch7.E1.m1.4a"><mrow id="Ch7.E1.m1.4.4" xref="Ch7.E1.m1.4.4.cmml"><mrow id="Ch7.E1.m1.4.4.3" xref="Ch7.E1.m1.4.4.3.cmml"><mi id="Ch7.E1.m1.4.4.3.2" xref="Ch7.E1.m1.4.4.3.2.cmml">a</mi><mo id="Ch7.E1.m1.4.4.3.1" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.3" xref="Ch7.E1.m1.4.4.3.3.cmml">t</mi><mo id="Ch7.E1.m1.4.4.3.1a" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.4" xref="Ch7.E1.m1.4.4.3.4.cmml">t</mi><mo id="Ch7.E1.m1.4.4.3.1b" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.5" xref="Ch7.E1.m1.4.4.3.5.cmml">e</mi><mo id="Ch7.E1.m1.4.4.3.1c" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.6" xref="Ch7.E1.m1.4.4.3.6.cmml">n</mi><mo id="Ch7.E1.m1.4.4.3.1d" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.7" xref="Ch7.E1.m1.4.4.3.7.cmml">t</mi><mo id="Ch7.E1.m1.4.4.3.1e" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.8" xref="Ch7.E1.m1.4.4.3.8.cmml">i</mi><mo id="Ch7.E1.m1.4.4.3.1f" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.9" xref="Ch7.E1.m1.4.4.3.9.cmml">o</mi><mo id="Ch7.E1.m1.4.4.3.1g" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.3.10" xref="Ch7.E1.m1.4.4.3.10.cmml">n</mi><mo id="Ch7.E1.m1.4.4.3.1h" xref="Ch7.E1.m1.4.4.3.1.cmml">⁢</mo><mrow id="Ch7.E1.m1.4.4.3.11.2" xref="Ch7.E1.m1.4.4.3.11.1.cmml"><mo id="Ch7.E1.m1.4.4.3.11.2.1" stretchy="false" xref="Ch7.E1.m1.4.4.3.11.1.cmml">(</mo><mi id="Ch7.E1.m1.1.1" xref="Ch7.E1.m1.1.1.cmml">Q</mi><mo id="Ch7.E1.m1.4.4.3.11.2.2" xref="Ch7.E1.m1.4.4.3.11.1.cmml">,</mo><mi id="Ch7.E1.m1.2.2" xref="Ch7.E1.m1.2.2.cmml">K</mi><mo id="Ch7.E1.m1.4.4.3.11.2.3" xref="Ch7.E1.m1.4.4.3.11.1.cmml">,</mo><mi id="Ch7.E1.m1.3.3" xref="Ch7.E1.m1.3.3.cmml">V</mi><mo id="Ch7.E1.m1.4.4.3.11.2.4" stretchy="false" xref="Ch7.E1.m1.4.4.3.11.1.cmml">)</mo></mrow></mrow><mo id="Ch7.E1.m1.4.4.2" xref="Ch7.E1.m1.4.4.2.cmml">=</mo><mrow id="Ch7.E1.m1.4.4.1" xref="Ch7.E1.m1.4.4.1.cmml"><mi id="Ch7.E1.m1.4.4.1.3" xref="Ch7.E1.m1.4.4.1.3.cmml">s</mi><mo id="Ch7.E1.m1.4.4.1.2" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.4" xref="Ch7.E1.m1.4.4.1.4.cmml">o</mi><mo id="Ch7.E1.m1.4.4.1.2a" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.5" xref="Ch7.E1.m1.4.4.1.5.cmml">f</mi><mo id="Ch7.E1.m1.4.4.1.2b" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.6" xref="Ch7.E1.m1.4.4.1.6.cmml">t</mi><mo id="Ch7.E1.m1.4.4.1.2c" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.7" xref="Ch7.E1.m1.4.4.1.7.cmml">m</mi><mo id="Ch7.E1.m1.4.4.1.2d" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.8" xref="Ch7.E1.m1.4.4.1.8.cmml">a</mi><mo id="Ch7.E1.m1.4.4.1.2e" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.9" xref="Ch7.E1.m1.4.4.1.9.cmml">x</mi><mo id="Ch7.E1.m1.4.4.1.2f" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mrow id="Ch7.E1.m1.4.4.1.1.1" xref="Ch7.E1.m1.4.4.1.1.1.1.cmml"><mo id="Ch7.E1.m1.4.4.1.1.1.2" stretchy="false" xref="Ch7.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="Ch7.E1.m1.4.4.1.1.1.1" xref="Ch7.E1.m1.4.4.1.1.1.1.cmml"><mrow id="Ch7.E1.m1.4.4.1.1.1.1.2" xref="Ch7.E1.m1.4.4.1.1.1.1.2.cmml"><mi id="Ch7.E1.m1.4.4.1.1.1.1.2.2" xref="Ch7.E1.m1.4.4.1.1.1.1.2.2.cmml">Q</mi><mo id="Ch7.E1.m1.4.4.1.1.1.1.2.1" xref="Ch7.E1.m1.4.4.1.1.1.1.2.1.cmml">⁢</mo><msup id="Ch7.E1.m1.4.4.1.1.1.1.2.3" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3.cmml"><mi id="Ch7.E1.m1.4.4.1.1.1.1.2.3.2" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3.2.cmml">K</mi><mi id="Ch7.E1.m1.4.4.1.1.1.1.2.3.3" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="Ch7.E1.m1.4.4.1.1.1.1.1" xref="Ch7.E1.m1.4.4.1.1.1.1.1.cmml">/</mo><msqrt id="Ch7.E1.m1.4.4.1.1.1.1.3" xref="Ch7.E1.m1.4.4.1.1.1.1.3.cmml"><msub id="Ch7.E1.m1.4.4.1.1.1.1.3.2" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2.cmml"><mi id="Ch7.E1.m1.4.4.1.1.1.1.3.2.2" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2.2.cmml">d</mi><mi id="Ch7.E1.m1.4.4.1.1.1.1.3.2.3" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mrow><mo id="Ch7.E1.m1.4.4.1.1.1.3" stretchy="false" xref="Ch7.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo id="Ch7.E1.m1.4.4.1.2g" xref="Ch7.E1.m1.4.4.1.2.cmml">⁢</mo><mi id="Ch7.E1.m1.4.4.1.10" xref="Ch7.E1.m1.4.4.1.10.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E1.m1.4b"><apply id="Ch7.E1.m1.4.4.cmml" xref="Ch7.E1.m1.4.4"><eq id="Ch7.E1.m1.4.4.2.cmml" xref="Ch7.E1.m1.4.4.2"></eq><apply id="Ch7.E1.m1.4.4.3.cmml" xref="Ch7.E1.m1.4.4.3"><times id="Ch7.E1.m1.4.4.3.1.cmml" xref="Ch7.E1.m1.4.4.3.1"></times><ci id="Ch7.E1.m1.4.4.3.2.cmml" xref="Ch7.E1.m1.4.4.3.2">𝑎</ci><ci id="Ch7.E1.m1.4.4.3.3.cmml" xref="Ch7.E1.m1.4.4.3.3">𝑡</ci><ci id="Ch7.E1.m1.4.4.3.4.cmml" xref="Ch7.E1.m1.4.4.3.4">𝑡</ci><ci id="Ch7.E1.m1.4.4.3.5.cmml" xref="Ch7.E1.m1.4.4.3.5">𝑒</ci><ci id="Ch7.E1.m1.4.4.3.6.cmml" xref="Ch7.E1.m1.4.4.3.6">𝑛</ci><ci id="Ch7.E1.m1.4.4.3.7.cmml" xref="Ch7.E1.m1.4.4.3.7">𝑡</ci><ci id="Ch7.E1.m1.4.4.3.8.cmml" xref="Ch7.E1.m1.4.4.3.8">𝑖</ci><ci id="Ch7.E1.m1.4.4.3.9.cmml" xref="Ch7.E1.m1.4.4.3.9">𝑜</ci><ci id="Ch7.E1.m1.4.4.3.10.cmml" xref="Ch7.E1.m1.4.4.3.10">𝑛</ci><vector id="Ch7.E1.m1.4.4.3.11.1.cmml" xref="Ch7.E1.m1.4.4.3.11.2"><ci id="Ch7.E1.m1.1.1.cmml" xref="Ch7.E1.m1.1.1">𝑄</ci><ci id="Ch7.E1.m1.2.2.cmml" xref="Ch7.E1.m1.2.2">𝐾</ci><ci id="Ch7.E1.m1.3.3.cmml" xref="Ch7.E1.m1.3.3">𝑉</ci></vector></apply><apply id="Ch7.E1.m1.4.4.1.cmml" xref="Ch7.E1.m1.4.4.1"><times id="Ch7.E1.m1.4.4.1.2.cmml" xref="Ch7.E1.m1.4.4.1.2"></times><ci id="Ch7.E1.m1.4.4.1.3.cmml" xref="Ch7.E1.m1.4.4.1.3">𝑠</ci><ci id="Ch7.E1.m1.4.4.1.4.cmml" xref="Ch7.E1.m1.4.4.1.4">𝑜</ci><ci id="Ch7.E1.m1.4.4.1.5.cmml" xref="Ch7.E1.m1.4.4.1.5">𝑓</ci><ci id="Ch7.E1.m1.4.4.1.6.cmml" xref="Ch7.E1.m1.4.4.1.6">𝑡</ci><ci id="Ch7.E1.m1.4.4.1.7.cmml" xref="Ch7.E1.m1.4.4.1.7">𝑚</ci><ci id="Ch7.E1.m1.4.4.1.8.cmml" xref="Ch7.E1.m1.4.4.1.8">𝑎</ci><ci id="Ch7.E1.m1.4.4.1.9.cmml" xref="Ch7.E1.m1.4.4.1.9">𝑥</ci><apply id="Ch7.E1.m1.4.4.1.1.1.1.cmml" xref="Ch7.E1.m1.4.4.1.1.1"><divide id="Ch7.E1.m1.4.4.1.1.1.1.1.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.1"></divide><apply id="Ch7.E1.m1.4.4.1.1.1.1.2.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2"><times id="Ch7.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2.1"></times><ci id="Ch7.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2.2">𝑄</ci><apply id="Ch7.E1.m1.4.4.1.1.1.1.2.3.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3"><csymbol cd="ambiguous" id="Ch7.E1.m1.4.4.1.1.1.1.2.3.1.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3">superscript</csymbol><ci id="Ch7.E1.m1.4.4.1.1.1.1.2.3.2.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3.2">𝐾</ci><ci id="Ch7.E1.m1.4.4.1.1.1.1.2.3.3.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="Ch7.E1.m1.4.4.1.1.1.1.3.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.3"><root id="Ch7.E1.m1.4.4.1.1.1.1.3a.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.3"></root><apply id="Ch7.E1.m1.4.4.1.1.1.1.3.2.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Ch7.E1.m1.4.4.1.1.1.1.3.2.1.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2">subscript</csymbol><ci id="Ch7.E1.m1.4.4.1.1.1.1.3.2.2.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2.2">𝑑</ci><ci id="Ch7.E1.m1.4.4.1.1.1.1.3.2.3.cmml" xref="Ch7.E1.m1.4.4.1.1.1.1.3.2.3">𝑘</ci></apply></apply></apply><ci id="Ch7.E1.m1.4.4.1.10.cmml" xref="Ch7.E1.m1.4.4.1.10">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E1.m1.4c">attention(Q,K,V)=softmax(QK^{T}/\sqrt{d_{k}})V</annotation><annotation encoding="application/x-llamapun" id="Ch7.E1.m1.4d">italic_a italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n ( italic_Q , italic_K , italic_V ) = italic_s italic_o italic_f italic_t italic_m italic_a italic_x ( italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT / square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) italic_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S5.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.4 </span>NMT</h4>
<div class="ltx_para" id="Ch7.S5.SS4.p1">
<p class="ltx_p" id="Ch7.S5.SS4.p1.1">While much research effort concentrates on creating new SOTA NMT models, excellent descriptions of the technology are also available within the literature for those starting out in the field, or for those with a less technical background <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx116" title="">116</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.p2">
<p class="ltx_p" id="Ch7.S5.SS4.p2.1">The availability of large parallel corpora has enabled NMT to develop high-performing MT models. Breakthrough performance improvements in the area of MT have been achieved through research efforts focusing on NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx10" title="">10</a>]</cite> but the advent of the Transformer architecture has greatly improved MT performance. Consequently, SOTA performance has been attained on multiple language pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.p3">
<p class="ltx_p" id="Ch7.S5.SS4.p3.4">Similar to many deep-learning approaches, NMT development is underpinned by the mathematics of probability. At a fundamental level, the goal is to predict the probabilistic distribution <math alttext="P(y|x)" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p3.1.m1.1"><semantics id="Ch7.S5.SS4.p3.1.m1.1a"><mrow id="Ch7.S5.SS4.p3.1.m1.1.1" xref="Ch7.S5.SS4.p3.1.m1.1.1.cmml"><mi id="Ch7.S5.SS4.p3.1.m1.1.1.3" xref="Ch7.S5.SS4.p3.1.m1.1.1.3.cmml">P</mi><mo id="Ch7.S5.SS4.p3.1.m1.1.1.2" xref="Ch7.S5.SS4.p3.1.m1.1.1.2.cmml">⁢</mo><mrow id="Ch7.S5.SS4.p3.1.m1.1.1.1.1" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.cmml"><mo id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.2" stretchy="false" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.cmml"><mi id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.2" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.1" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.3" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.3" stretchy="false" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p3.1.m1.1b"><apply id="Ch7.S5.SS4.p3.1.m1.1.1.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1"><times id="Ch7.S5.SS4.p3.1.m1.1.1.2.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1.2"></times><ci id="Ch7.S5.SS4.p3.1.m1.1.1.3.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1.3">𝑃</ci><apply id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1"><csymbol cd="latexml" id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.p3.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p3.1.m1.1c">P(y|x)</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p3.1.m1.1d">italic_P ( italic_y | italic_x )</annotation></semantics></math> given a dataset <math alttext="D" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p3.2.m2.1"><semantics id="Ch7.S5.SS4.p3.2.m2.1a"><mi id="Ch7.S5.SS4.p3.2.m2.1.1" xref="Ch7.S5.SS4.p3.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p3.2.m2.1b"><ci id="Ch7.S5.SS4.p3.2.m2.1.1.cmml" xref="Ch7.S5.SS4.p3.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p3.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p3.2.m2.1d">italic_D</annotation></semantics></math>, where <math alttext="x" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p3.3.m3.1"><semantics id="Ch7.S5.SS4.p3.3.m3.1a"><mi id="Ch7.S5.SS4.p3.3.m3.1.1" xref="Ch7.S5.SS4.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p3.3.m3.1b"><ci id="Ch7.S5.SS4.p3.3.m3.1.1.cmml" xref="Ch7.S5.SS4.p3.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p3.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p3.3.m3.1d">italic_x</annotation></semantics></math> represents the source input sentence and <math alttext="y" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p3.4.m4.1"><semantics id="Ch7.S5.SS4.p3.4.m4.1a"><mi id="Ch7.S5.SS4.p3.4.m4.1.1" xref="Ch7.S5.SS4.p3.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p3.4.m4.1b"><ci id="Ch7.S5.SS4.p3.4.m4.1.1.cmml" xref="Ch7.S5.SS4.p3.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p3.4.m4.1c">y</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p3.4.m4.1d">italic_y</annotation></semantics></math> represents the target output sentence.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.p4">
<p class="ltx_p" id="Ch7.S5.SS4.p4.6">Supervised training of an NMT model develops the model weights by comparing the predicted <math alttext="P(y|x)" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p4.1.m1.1"><semantics id="Ch7.S5.SS4.p4.1.m1.1a"><mrow id="Ch7.S5.SS4.p4.1.m1.1.1" xref="Ch7.S5.SS4.p4.1.m1.1.1.cmml"><mi id="Ch7.S5.SS4.p4.1.m1.1.1.3" xref="Ch7.S5.SS4.p4.1.m1.1.1.3.cmml">P</mi><mo id="Ch7.S5.SS4.p4.1.m1.1.1.2" xref="Ch7.S5.SS4.p4.1.m1.1.1.2.cmml">⁢</mo><mrow id="Ch7.S5.SS4.p4.1.m1.1.1.1.1" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.cmml"><mo id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.2" stretchy="false" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.cmml"><mi id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.2" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.1" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.3" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.3" stretchy="false" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p4.1.m1.1b"><apply id="Ch7.S5.SS4.p4.1.m1.1.1.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1"><times id="Ch7.S5.SS4.p4.1.m1.1.1.2.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1.2"></times><ci id="Ch7.S5.SS4.p4.1.m1.1.1.3.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1.3">𝑃</ci><apply id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1"><csymbol cd="latexml" id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.p4.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p4.1.m1.1c">P(y|x)</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p4.1.m1.1d">italic_P ( italic_y | italic_x )</annotation></semantics></math> with the correct <math alttext="y" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p4.2.m2.1"><semantics id="Ch7.S5.SS4.p4.2.m2.1a"><mi id="Ch7.S5.SS4.p4.2.m2.1.1" xref="Ch7.S5.SS4.p4.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p4.2.m2.1b"><ci id="Ch7.S5.SS4.p4.2.m2.1.1.cmml" xref="Ch7.S5.SS4.p4.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p4.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p4.2.m2.1d">italic_y</annotation></semantics></math> sentences of the training dataset, <math alttext="D\textsubscript{Train}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p4.3.m3.1"><semantics id="Ch7.S5.SS4.p4.3.m3.1a"><mrow id="Ch7.S5.SS4.p4.3.m3.1.1" xref="Ch7.S5.SS4.p4.3.m3.1.1.cmml"><mi id="Ch7.S5.SS4.p4.3.m3.1.1.2" xref="Ch7.S5.SS4.p4.3.m3.1.1.2.cmml">D</mi><mo id="Ch7.S5.SS4.p4.3.m3.1.1.1" xref="Ch7.S5.SS4.p4.3.m3.1.1.1.cmml">⁢</mo><mtext id="Ch7.S5.SS4.p4.3.m3.1.1.3" xref="Ch7.S5.SS4.p4.3.m3.1.1.3b.cmml"><sub class="ltx_sub" id="Ch7.S5.SS4.p4.3.m3.1.1.3.1nest">Train</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p4.3.m3.1b"><apply id="Ch7.S5.SS4.p4.3.m3.1.1.cmml" xref="Ch7.S5.SS4.p4.3.m3.1.1"><times id="Ch7.S5.SS4.p4.3.m3.1.1.1.cmml" xref="Ch7.S5.SS4.p4.3.m3.1.1.1"></times><ci id="Ch7.S5.SS4.p4.3.m3.1.1.2.cmml" xref="Ch7.S5.SS4.p4.3.m3.1.1.2">𝐷</ci><ci id="Ch7.S5.SS4.p4.3.m3.1.1.3b.cmml" xref="Ch7.S5.SS4.p4.3.m3.1.1.3"><mtext id="Ch7.S5.SS4.p4.3.m3.1.1.3.cmml" xref="Ch7.S5.SS4.p4.3.m3.1.1.3"><sub class="ltx_sub" id="Ch7.S5.SS4.p4.3.m3.1.1.3.1anest">Train</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p4.3.m3.1c">D\textsubscript{Train}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p4.3.m3.1d">italic_D</annotation></semantics></math>. In evaluating the performance of an NMT model, automatic evaluation results are determined when the predicted <math alttext="P(y|x)" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p4.4.m4.1"><semantics id="Ch7.S5.SS4.p4.4.m4.1a"><mrow id="Ch7.S5.SS4.p4.4.m4.1.1" xref="Ch7.S5.SS4.p4.4.m4.1.1.cmml"><mi id="Ch7.S5.SS4.p4.4.m4.1.1.3" xref="Ch7.S5.SS4.p4.4.m4.1.1.3.cmml">P</mi><mo id="Ch7.S5.SS4.p4.4.m4.1.1.2" xref="Ch7.S5.SS4.p4.4.m4.1.1.2.cmml">⁢</mo><mrow id="Ch7.S5.SS4.p4.4.m4.1.1.1.1" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.cmml"><mo id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.2" stretchy="false" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.cmml"><mi id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.2" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.1" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.1.cmml">|</mo><mi id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.3" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.3" stretchy="false" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p4.4.m4.1b"><apply id="Ch7.S5.SS4.p4.4.m4.1.1.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1"><times id="Ch7.S5.SS4.p4.4.m4.1.1.2.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1.2"></times><ci id="Ch7.S5.SS4.p4.4.m4.1.1.3.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1.3">𝑃</ci><apply id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1"><csymbol cd="latexml" id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.1">conditional</csymbol><ci id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.2">𝑦</ci><ci id="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.p4.4.m4.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p4.4.m4.1c">P(y|x)</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p4.4.m4.1d">italic_P ( italic_y | italic_x )</annotation></semantics></math> sentences are compared with the correct <math alttext="y" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p4.5.m5.1"><semantics id="Ch7.S5.SS4.p4.5.m5.1a"><mi id="Ch7.S5.SS4.p4.5.m5.1.1" xref="Ch7.S5.SS4.p4.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p4.5.m5.1b"><ci id="Ch7.S5.SS4.p4.5.m5.1.1.cmml" xref="Ch7.S5.SS4.p4.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p4.5.m5.1c">y</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p4.5.m5.1d">italic_y</annotation></semantics></math> sentences of the test dataset, <math alttext="D\textsubscript{Test}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.p4.6.m6.1"><semantics id="Ch7.S5.SS4.p4.6.m6.1a"><mrow id="Ch7.S5.SS4.p4.6.m6.1.1" xref="Ch7.S5.SS4.p4.6.m6.1.1.cmml"><mi id="Ch7.S5.SS4.p4.6.m6.1.1.2" xref="Ch7.S5.SS4.p4.6.m6.1.1.2.cmml">D</mi><mo id="Ch7.S5.SS4.p4.6.m6.1.1.1" xref="Ch7.S5.SS4.p4.6.m6.1.1.1.cmml">⁢</mo><mtext id="Ch7.S5.SS4.p4.6.m6.1.1.3" xref="Ch7.S5.SS4.p4.6.m6.1.1.3b.cmml"><sub class="ltx_sub" id="Ch7.S5.SS4.p4.6.m6.1.1.3.1nest">Test</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.p4.6.m6.1b"><apply id="Ch7.S5.SS4.p4.6.m6.1.1.cmml" xref="Ch7.S5.SS4.p4.6.m6.1.1"><times id="Ch7.S5.SS4.p4.6.m6.1.1.1.cmml" xref="Ch7.S5.SS4.p4.6.m6.1.1.1"></times><ci id="Ch7.S5.SS4.p4.6.m6.1.1.2.cmml" xref="Ch7.S5.SS4.p4.6.m6.1.1.2">𝐷</ci><ci id="Ch7.S5.SS4.p4.6.m6.1.1.3b.cmml" xref="Ch7.S5.SS4.p4.6.m6.1.1.3"><mtext id="Ch7.S5.SS4.p4.6.m6.1.1.3.cmml" xref="Ch7.S5.SS4.p4.6.m6.1.1.3"><sub class="ltx_sub" id="Ch7.S5.SS4.p4.6.m6.1.1.3.1anest">Test</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.p4.6.m6.1c">D\textsubscript{Test}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.p4.6.m6.1d">italic_D</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.p5">
<p class="ltx_p" id="Ch7.S5.SS4.p5.1">In adopting a deep learning paradigm, MT inherits the mathematical first principles which are inherent to this approach. To understand these principles, the manner in which neural networks model a conditional distribution is outlined. Furthermore, the encoder-decoder mechanism used for training NMT models is presented in the modelling subsection, and model optimisation using training objectives is outlined in the learning subsection. Finally, the mathematics of how translated sentences are generated is explored in the inference subsection.</p>
</div>
<section class="ltx_subsubsection" id="Ch7.S5.SS4.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Modelling</h5>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p1">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p1.2">In NMT, sentence-level translation is modelled using input and output sentences as sequences. Using this approach, an NMT model implements a sequence-to-sequence model with a given source sentence, <math alttext="x=(x_{1},...,x_{s})" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p1.1.m1.3"><semantics id="Ch7.S5.SS4.SSSx1.p1.1.m1.3a"><mrow id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.cmml"><mi id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.4" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.4.cmml">x</mi><mo id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.3" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.3.cmml">=</mo><mrow id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.3.cmml"><mo id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.3" stretchy="false" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.3.cmml">(</mo><msub id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.2" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.3" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.4" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.3.cmml">,</mo><mi id="Ch7.S5.SS4.SSSx1.p1.1.m1.1.1" mathvariant="normal" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.1.1.cmml">…</mi><mo id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.5" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.cmml"><mi id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.2" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mi id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.3" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.3.cmml">s</mi></msub><mo id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.6" stretchy="false" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p1.1.m1.3b"><apply id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3"><eq id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.3"></eq><ci id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.4.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.4">𝑥</ci><vector id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2"><apply id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="Ch7.S5.SS4.SSSx1.p1.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.1.1">…</ci><apply id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.2.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.2">𝑥</ci><ci id="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.1.m1.3.3.2.2.2.3">𝑠</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p1.1.m1.3c">x=(x_{1},...,x_{s})</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p1.1.m1.3d">italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math> generating a target sentence <math alttext="y=(y_{1},...,y_{t})" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p1.2.m2.3"><semantics id="Ch7.S5.SS4.SSSx1.p1.2.m2.3a"><mrow id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.cmml"><mi id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.4" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.4.cmml">y</mi><mo id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.3" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.3.cmml">=</mo><mrow id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.3.cmml"><mo id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.3" stretchy="false" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.3.cmml">(</mo><msub id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.2" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.2.cmml">y</mi><mn id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.3" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.4" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.3.cmml">,</mo><mi id="Ch7.S5.SS4.SSSx1.p1.2.m2.1.1" mathvariant="normal" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.1.1.cmml">…</mi><mo id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.5" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.3.cmml">,</mo><msub id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.cmml"><mi id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.2" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.2.cmml">y</mi><mi id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.3" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.3.cmml">t</mi></msub><mo id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.6" stretchy="false" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p1.2.m2.3b"><apply id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3"><eq id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.3"></eq><ci id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.4.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.4">𝑦</ci><vector id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2"><apply id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.2">𝑦</ci><cn id="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="Ch7.S5.SS4.SSSx1.p1.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.1.1">…</ci><apply id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.1.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.2.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.2">𝑦</ci><ci id="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.3.cmml" xref="Ch7.S5.SS4.SSSx1.p1.2.m2.3.3.2.2.2.3">𝑡</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p1.2.m2.3c">y=(y_{1},...,y_{t})</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p1.2.m2.3d">italic_y = ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p2">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p2.2">In effect, such a sequence-to-sequence NMT model acts as a conditional language model. The decoder within the model predicts the next word of the target sentence <math alttext="y" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p2.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx1.p2.1.m1.1a"><mi id="Ch7.S5.SS4.SSSx1.p2.1.m1.1.1" xref="Ch7.S5.SS4.SSSx1.p2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p2.1.m1.1b"><ci id="Ch7.S5.SS4.SSSx1.p2.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p2.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p2.1.m1.1d">italic_y</annotation></semantics></math>, while such predictions are conditioned on the source sentence <math alttext="x" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p2.2.m2.1"><semantics id="Ch7.S5.SS4.SSSx1.p2.2.m2.1a"><mi id="Ch7.S5.SS4.SSSx1.p2.2.m2.1.1" xref="Ch7.S5.SS4.SSSx1.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p2.2.m2.1b"><ci id="Ch7.S5.SS4.SSSx1.p2.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p2.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p2.2.m2.1d">italic_x</annotation></semantics></math>.
</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p3">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p3.3">By applying the chain rule, a model’s prediction (i.e. translation <math alttext="y" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p3.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx1.p3.1.m1.1a"><mi id="Ch7.S5.SS4.SSSx1.p3.1.m1.1.1" xref="Ch7.S5.SS4.SSSx1.p3.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p3.1.m1.1b"><ci id="Ch7.S5.SS4.SSSx1.p3.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p3.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p3.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p3.1.m1.1d">italic_y</annotation></semantics></math> of length <math alttext="T" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p3.2.m2.1"><semantics id="Ch7.S5.SS4.SSSx1.p3.2.m2.1a"><mi id="Ch7.S5.SS4.SSSx1.p3.2.m2.1.1" xref="Ch7.S5.SS4.SSSx1.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p3.2.m2.1b"><ci id="Ch7.S5.SS4.SSSx1.p3.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p3.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p3.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p3.2.m2.1d">italic_T</annotation></semantics></math>) maximises the probability <math alttext="P(y|x)" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p3.3.m3.1"><semantics id="Ch7.S5.SS4.SSSx1.p3.3.m3.1a"><mrow id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.3" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.3.cmml">P</mi><mo id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.2" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.2.cmml">⁢</mo><mrow id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.cmml"><mo id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.2" stretchy="false" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.2" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.1.cmml">|</mo><mi id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.3" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.3" stretchy="false" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p3.3.m3.1b"><apply id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1"><times id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.2"></times><ci id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.3">𝑃</ci><apply id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1"><csymbol cd="latexml" id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.1">conditional</csymbol><ci id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.2">𝑦</ci><ci id="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p3.3.m3.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p3.3.m3.1c">P(y|x)</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p3.3.m3.1d">italic_P ( italic_y | italic_x )</annotation></semantics></math> identified in Equations <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E2" title="7.2 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E3" title="7.3 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.3</span></a>:</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p4">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(y|x)=P(y_{1}|x)P(y_{2}|y_{1},x)P(y_{3}|y_{1},y_{2},x)P(y_{T}|y_{1},...,y_{T-%
1},x)" class="ltx_Math" display="block" id="Ch7.E2.m1.9"><semantics id="Ch7.E2.m1.9a"><mrow id="Ch7.E2.m1.9.9" xref="Ch7.E2.m1.9.9.cmml"><mrow id="Ch7.E2.m1.5.5.1" xref="Ch7.E2.m1.5.5.1.cmml"><mi id="Ch7.E2.m1.5.5.1.3" xref="Ch7.E2.m1.5.5.1.3.cmml">P</mi><mo id="Ch7.E2.m1.5.5.1.2" xref="Ch7.E2.m1.5.5.1.2.cmml">⁢</mo><mrow id="Ch7.E2.m1.5.5.1.1.1" xref="Ch7.E2.m1.5.5.1.1.1.1.cmml"><mo id="Ch7.E2.m1.5.5.1.1.1.2" stretchy="false" xref="Ch7.E2.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="Ch7.E2.m1.5.5.1.1.1.1" xref="Ch7.E2.m1.5.5.1.1.1.1.cmml"><mi id="Ch7.E2.m1.5.5.1.1.1.1.2" xref="Ch7.E2.m1.5.5.1.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.E2.m1.5.5.1.1.1.1.1" xref="Ch7.E2.m1.5.5.1.1.1.1.1.cmml">|</mo><mi id="Ch7.E2.m1.5.5.1.1.1.1.3" xref="Ch7.E2.m1.5.5.1.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.E2.m1.5.5.1.1.1.3" stretchy="false" xref="Ch7.E2.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch7.E2.m1.9.9.6" xref="Ch7.E2.m1.9.9.6.cmml">=</mo><mrow id="Ch7.E2.m1.9.9.5" xref="Ch7.E2.m1.9.9.5.cmml"><mi id="Ch7.E2.m1.9.9.5.6" xref="Ch7.E2.m1.9.9.5.6.cmml">P</mi><mo id="Ch7.E2.m1.9.9.5.5" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mrow id="Ch7.E2.m1.6.6.2.1.1" xref="Ch7.E2.m1.6.6.2.1.1.1.cmml"><mo id="Ch7.E2.m1.6.6.2.1.1.2" stretchy="false" xref="Ch7.E2.m1.6.6.2.1.1.1.cmml">(</mo><mrow id="Ch7.E2.m1.6.6.2.1.1.1" xref="Ch7.E2.m1.6.6.2.1.1.1.cmml"><msub id="Ch7.E2.m1.6.6.2.1.1.1.2" xref="Ch7.E2.m1.6.6.2.1.1.1.2.cmml"><mi id="Ch7.E2.m1.6.6.2.1.1.1.2.2" xref="Ch7.E2.m1.6.6.2.1.1.1.2.2.cmml">y</mi><mn id="Ch7.E2.m1.6.6.2.1.1.1.2.3" xref="Ch7.E2.m1.6.6.2.1.1.1.2.3.cmml">1</mn></msub><mo fence="false" id="Ch7.E2.m1.6.6.2.1.1.1.1" xref="Ch7.E2.m1.6.6.2.1.1.1.1.cmml">|</mo><mi id="Ch7.E2.m1.6.6.2.1.1.1.3" xref="Ch7.E2.m1.6.6.2.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.E2.m1.6.6.2.1.1.3" stretchy="false" xref="Ch7.E2.m1.6.6.2.1.1.1.cmml">)</mo></mrow><mo id="Ch7.E2.m1.9.9.5.5a" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mi id="Ch7.E2.m1.9.9.5.7" xref="Ch7.E2.m1.9.9.5.7.cmml">P</mi><mo id="Ch7.E2.m1.9.9.5.5b" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mrow id="Ch7.E2.m1.7.7.3.2.1" xref="Ch7.E2.m1.7.7.3.2.1.1.cmml"><mo id="Ch7.E2.m1.7.7.3.2.1.2" stretchy="false" xref="Ch7.E2.m1.7.7.3.2.1.1.cmml">(</mo><mrow id="Ch7.E2.m1.7.7.3.2.1.1" xref="Ch7.E2.m1.7.7.3.2.1.1.cmml"><msub id="Ch7.E2.m1.7.7.3.2.1.1.3" xref="Ch7.E2.m1.7.7.3.2.1.1.3.cmml"><mi id="Ch7.E2.m1.7.7.3.2.1.1.3.2" xref="Ch7.E2.m1.7.7.3.2.1.1.3.2.cmml">y</mi><mn id="Ch7.E2.m1.7.7.3.2.1.1.3.3" xref="Ch7.E2.m1.7.7.3.2.1.1.3.3.cmml">2</mn></msub><mo fence="false" id="Ch7.E2.m1.7.7.3.2.1.1.2" xref="Ch7.E2.m1.7.7.3.2.1.1.2.cmml">|</mo><mrow id="Ch7.E2.m1.7.7.3.2.1.1.1.1" xref="Ch7.E2.m1.7.7.3.2.1.1.1.2.cmml"><msub id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.cmml"><mi id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.2" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.2.cmml">y</mi><mn id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.3" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Ch7.E2.m1.7.7.3.2.1.1.1.1.2" xref="Ch7.E2.m1.7.7.3.2.1.1.1.2.cmml">,</mo><mi id="Ch7.E2.m1.1.1" xref="Ch7.E2.m1.1.1.cmml">x</mi></mrow></mrow><mo id="Ch7.E2.m1.7.7.3.2.1.3" stretchy="false" xref="Ch7.E2.m1.7.7.3.2.1.1.cmml">)</mo></mrow><mo id="Ch7.E2.m1.9.9.5.5c" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mi id="Ch7.E2.m1.9.9.5.8" xref="Ch7.E2.m1.9.9.5.8.cmml">P</mi><mo id="Ch7.E2.m1.9.9.5.5d" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mrow id="Ch7.E2.m1.8.8.4.3.1" xref="Ch7.E2.m1.8.8.4.3.1.1.cmml"><mo id="Ch7.E2.m1.8.8.4.3.1.2" stretchy="false" xref="Ch7.E2.m1.8.8.4.3.1.1.cmml">(</mo><mrow id="Ch7.E2.m1.8.8.4.3.1.1" xref="Ch7.E2.m1.8.8.4.3.1.1.cmml"><msub id="Ch7.E2.m1.8.8.4.3.1.1.4" xref="Ch7.E2.m1.8.8.4.3.1.1.4.cmml"><mi id="Ch7.E2.m1.8.8.4.3.1.1.4.2" xref="Ch7.E2.m1.8.8.4.3.1.1.4.2.cmml">y</mi><mn id="Ch7.E2.m1.8.8.4.3.1.1.4.3" xref="Ch7.E2.m1.8.8.4.3.1.1.4.3.cmml">3</mn></msub><mo fence="false" id="Ch7.E2.m1.8.8.4.3.1.1.3" xref="Ch7.E2.m1.8.8.4.3.1.1.3.cmml">|</mo><mrow id="Ch7.E2.m1.8.8.4.3.1.1.2.2" xref="Ch7.E2.m1.8.8.4.3.1.1.2.3.cmml"><msub id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.cmml"><mi id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.2" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.2.cmml">y</mi><mn id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.3" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Ch7.E2.m1.8.8.4.3.1.1.2.2.3" xref="Ch7.E2.m1.8.8.4.3.1.1.2.3.cmml">,</mo><msub id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.cmml"><mi id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.2" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.3" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.3.cmml">2</mn></msub><mo id="Ch7.E2.m1.8.8.4.3.1.1.2.2.4" xref="Ch7.E2.m1.8.8.4.3.1.1.2.3.cmml">,</mo><mi id="Ch7.E2.m1.2.2" xref="Ch7.E2.m1.2.2.cmml">x</mi></mrow></mrow><mo id="Ch7.E2.m1.8.8.4.3.1.3" stretchy="false" xref="Ch7.E2.m1.8.8.4.3.1.1.cmml">)</mo></mrow><mo id="Ch7.E2.m1.9.9.5.5e" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mi id="Ch7.E2.m1.9.9.5.9" xref="Ch7.E2.m1.9.9.5.9.cmml">P</mi><mo id="Ch7.E2.m1.9.9.5.5f" xref="Ch7.E2.m1.9.9.5.5.cmml">⁢</mo><mrow id="Ch7.E2.m1.9.9.5.4.1" xref="Ch7.E2.m1.9.9.5.4.1.1.cmml"><mo id="Ch7.E2.m1.9.9.5.4.1.2" stretchy="false" xref="Ch7.E2.m1.9.9.5.4.1.1.cmml">(</mo><mrow id="Ch7.E2.m1.9.9.5.4.1.1" xref="Ch7.E2.m1.9.9.5.4.1.1.cmml"><msub id="Ch7.E2.m1.9.9.5.4.1.1.4" xref="Ch7.E2.m1.9.9.5.4.1.1.4.cmml"><mi id="Ch7.E2.m1.9.9.5.4.1.1.4.2" xref="Ch7.E2.m1.9.9.5.4.1.1.4.2.cmml">y</mi><mi id="Ch7.E2.m1.9.9.5.4.1.1.4.3" xref="Ch7.E2.m1.9.9.5.4.1.1.4.3.cmml">T</mi></msub><mo fence="false" id="Ch7.E2.m1.9.9.5.4.1.1.3" xref="Ch7.E2.m1.9.9.5.4.1.1.3.cmml">|</mo><mrow id="Ch7.E2.m1.9.9.5.4.1.1.2.2" xref="Ch7.E2.m1.9.9.5.4.1.1.2.3.cmml"><msub id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.cmml"><mi id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.2" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.2.cmml">y</mi><mn id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.3" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Ch7.E2.m1.9.9.5.4.1.1.2.2.3" xref="Ch7.E2.m1.9.9.5.4.1.1.2.3.cmml">,</mo><mi id="Ch7.E2.m1.3.3" mathvariant="normal" xref="Ch7.E2.m1.3.3.cmml">…</mi><mo id="Ch7.E2.m1.9.9.5.4.1.1.2.2.4" xref="Ch7.E2.m1.9.9.5.4.1.1.2.3.cmml">,</mo><msub id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.cmml"><mi id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.2" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.2.cmml">y</mi><mrow id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.cmml"><mi id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.2" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.2.cmml">T</mi><mo id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.1" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.1.cmml">−</mo><mn id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.3" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="Ch7.E2.m1.9.9.5.4.1.1.2.2.5" xref="Ch7.E2.m1.9.9.5.4.1.1.2.3.cmml">,</mo><mi id="Ch7.E2.m1.4.4" xref="Ch7.E2.m1.4.4.cmml">x</mi></mrow></mrow><mo id="Ch7.E2.m1.9.9.5.4.1.3" stretchy="false" xref="Ch7.E2.m1.9.9.5.4.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E2.m1.9b"><apply id="Ch7.E2.m1.9.9.cmml" xref="Ch7.E2.m1.9.9"><eq id="Ch7.E2.m1.9.9.6.cmml" xref="Ch7.E2.m1.9.9.6"></eq><apply id="Ch7.E2.m1.5.5.1.cmml" xref="Ch7.E2.m1.5.5.1"><times id="Ch7.E2.m1.5.5.1.2.cmml" xref="Ch7.E2.m1.5.5.1.2"></times><ci id="Ch7.E2.m1.5.5.1.3.cmml" xref="Ch7.E2.m1.5.5.1.3">𝑃</ci><apply id="Ch7.E2.m1.5.5.1.1.1.1.cmml" xref="Ch7.E2.m1.5.5.1.1.1"><csymbol cd="latexml" id="Ch7.E2.m1.5.5.1.1.1.1.1.cmml" xref="Ch7.E2.m1.5.5.1.1.1.1.1">conditional</csymbol><ci id="Ch7.E2.m1.5.5.1.1.1.1.2.cmml" xref="Ch7.E2.m1.5.5.1.1.1.1.2">𝑦</ci><ci id="Ch7.E2.m1.5.5.1.1.1.1.3.cmml" xref="Ch7.E2.m1.5.5.1.1.1.1.3">𝑥</ci></apply></apply><apply id="Ch7.E2.m1.9.9.5.cmml" xref="Ch7.E2.m1.9.9.5"><times id="Ch7.E2.m1.9.9.5.5.cmml" xref="Ch7.E2.m1.9.9.5.5"></times><ci id="Ch7.E2.m1.9.9.5.6.cmml" xref="Ch7.E2.m1.9.9.5.6">𝑃</ci><apply id="Ch7.E2.m1.6.6.2.1.1.1.cmml" xref="Ch7.E2.m1.6.6.2.1.1"><csymbol cd="latexml" id="Ch7.E2.m1.6.6.2.1.1.1.1.cmml" xref="Ch7.E2.m1.6.6.2.1.1.1.1">conditional</csymbol><apply id="Ch7.E2.m1.6.6.2.1.1.1.2.cmml" xref="Ch7.E2.m1.6.6.2.1.1.1.2"><csymbol cd="ambiguous" id="Ch7.E2.m1.6.6.2.1.1.1.2.1.cmml" xref="Ch7.E2.m1.6.6.2.1.1.1.2">subscript</csymbol><ci id="Ch7.E2.m1.6.6.2.1.1.1.2.2.cmml" xref="Ch7.E2.m1.6.6.2.1.1.1.2.2">𝑦</ci><cn id="Ch7.E2.m1.6.6.2.1.1.1.2.3.cmml" type="integer" xref="Ch7.E2.m1.6.6.2.1.1.1.2.3">1</cn></apply><ci id="Ch7.E2.m1.6.6.2.1.1.1.3.cmml" xref="Ch7.E2.m1.6.6.2.1.1.1.3">𝑥</ci></apply><ci id="Ch7.E2.m1.9.9.5.7.cmml" xref="Ch7.E2.m1.9.9.5.7">𝑃</ci><apply id="Ch7.E2.m1.7.7.3.2.1.1.cmml" xref="Ch7.E2.m1.7.7.3.2.1"><csymbol cd="latexml" id="Ch7.E2.m1.7.7.3.2.1.1.2.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.2">conditional</csymbol><apply id="Ch7.E2.m1.7.7.3.2.1.1.3.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.3"><csymbol cd="ambiguous" id="Ch7.E2.m1.7.7.3.2.1.1.3.1.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.3">subscript</csymbol><ci id="Ch7.E2.m1.7.7.3.2.1.1.3.2.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.3.2">𝑦</ci><cn id="Ch7.E2.m1.7.7.3.2.1.1.3.3.cmml" type="integer" xref="Ch7.E2.m1.7.7.3.2.1.1.3.3">2</cn></apply><list id="Ch7.E2.m1.7.7.3.2.1.1.1.2.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1"><apply id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.1.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.2.cmml" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.2">𝑦</ci><cn id="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E2.m1.7.7.3.2.1.1.1.1.1.3">1</cn></apply><ci id="Ch7.E2.m1.1.1.cmml" xref="Ch7.E2.m1.1.1">𝑥</ci></list></apply><ci id="Ch7.E2.m1.9.9.5.8.cmml" xref="Ch7.E2.m1.9.9.5.8">𝑃</ci><apply id="Ch7.E2.m1.8.8.4.3.1.1.cmml" xref="Ch7.E2.m1.8.8.4.3.1"><csymbol cd="latexml" id="Ch7.E2.m1.8.8.4.3.1.1.3.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.3">conditional</csymbol><apply id="Ch7.E2.m1.8.8.4.3.1.1.4.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.4"><csymbol cd="ambiguous" id="Ch7.E2.m1.8.8.4.3.1.1.4.1.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.4">subscript</csymbol><ci id="Ch7.E2.m1.8.8.4.3.1.1.4.2.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.4.2">𝑦</ci><cn id="Ch7.E2.m1.8.8.4.3.1.1.4.3.cmml" type="integer" xref="Ch7.E2.m1.8.8.4.3.1.1.4.3">3</cn></apply><list id="Ch7.E2.m1.8.8.4.3.1.1.2.3.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2"><apply id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.1.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.2.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.2">𝑦</ci><cn id="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E2.m1.8.8.4.3.1.1.1.1.1.3">1</cn></apply><apply id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2"><csymbol cd="ambiguous" id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.1.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2">subscript</csymbol><ci id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.2.cmml" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.2">𝑦</ci><cn id="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.3.cmml" type="integer" xref="Ch7.E2.m1.8.8.4.3.1.1.2.2.2.3">2</cn></apply><ci id="Ch7.E2.m1.2.2.cmml" xref="Ch7.E2.m1.2.2">𝑥</ci></list></apply><ci id="Ch7.E2.m1.9.9.5.9.cmml" xref="Ch7.E2.m1.9.9.5.9">𝑃</ci><apply id="Ch7.E2.m1.9.9.5.4.1.1.cmml" xref="Ch7.E2.m1.9.9.5.4.1"><csymbol cd="latexml" id="Ch7.E2.m1.9.9.5.4.1.1.3.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.3">conditional</csymbol><apply id="Ch7.E2.m1.9.9.5.4.1.1.4.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.4"><csymbol cd="ambiguous" id="Ch7.E2.m1.9.9.5.4.1.1.4.1.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.4">subscript</csymbol><ci id="Ch7.E2.m1.9.9.5.4.1.1.4.2.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.4.2">𝑦</ci><ci id="Ch7.E2.m1.9.9.5.4.1.1.4.3.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.4.3">𝑇</ci></apply><list id="Ch7.E2.m1.9.9.5.4.1.1.2.3.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2"><apply id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.1.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.2.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.2">𝑦</ci><cn id="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E2.m1.9.9.5.4.1.1.1.1.1.3">1</cn></apply><ci id="Ch7.E2.m1.3.3.cmml" xref="Ch7.E2.m1.3.3">…</ci><apply id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2"><csymbol cd="ambiguous" id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.1.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2">subscript</csymbol><ci id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.2.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.2">𝑦</ci><apply id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3"><minus id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.1.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.1"></minus><ci id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.2.cmml" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.2">𝑇</ci><cn id="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.3.cmml" type="integer" xref="Ch7.E2.m1.9.9.5.4.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="Ch7.E2.m1.4.4.cmml" xref="Ch7.E2.m1.4.4">𝑥</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E2.m1.9c">P(y|x)=P(y_{1}|x)P(y_{2}|y_{1},x)P(y_{3}|y_{1},y_{2},x)P(y_{T}|y_{1},...,y_{T-%
1},x)</annotation><annotation encoding="application/x-llamapun" id="Ch7.E2.m1.9d">italic_P ( italic_y | italic_x ) = italic_P ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_x ) italic_P ( italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x ) italic_P ( italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x ) italic_P ( italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT , italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p5">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(y|x)=\prod_{t=1}^{T}P(y_{t}|y_{1},...,y_{T-1},x)" class="ltx_Math" display="block" id="Ch7.E3.m1.4"><semantics id="Ch7.E3.m1.4a"><mrow id="Ch7.E3.m1.4.4" xref="Ch7.E3.m1.4.4.cmml"><mrow id="Ch7.E3.m1.3.3.1" xref="Ch7.E3.m1.3.3.1.cmml"><mi id="Ch7.E3.m1.3.3.1.3" xref="Ch7.E3.m1.3.3.1.3.cmml">P</mi><mo id="Ch7.E3.m1.3.3.1.2" xref="Ch7.E3.m1.3.3.1.2.cmml">⁢</mo><mrow id="Ch7.E3.m1.3.3.1.1.1" xref="Ch7.E3.m1.3.3.1.1.1.1.cmml"><mo id="Ch7.E3.m1.3.3.1.1.1.2" stretchy="false" xref="Ch7.E3.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="Ch7.E3.m1.3.3.1.1.1.1" xref="Ch7.E3.m1.3.3.1.1.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.1.1.2" xref="Ch7.E3.m1.3.3.1.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.E3.m1.3.3.1.1.1.1.1" xref="Ch7.E3.m1.3.3.1.1.1.1.1.cmml">|</mo><mi id="Ch7.E3.m1.3.3.1.1.1.1.3" xref="Ch7.E3.m1.3.3.1.1.1.1.3.cmml">x</mi></mrow><mo id="Ch7.E3.m1.3.3.1.1.1.3" stretchy="false" xref="Ch7.E3.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch7.E3.m1.4.4.3" rspace="0.111em" xref="Ch7.E3.m1.4.4.3.cmml">=</mo><mrow id="Ch7.E3.m1.4.4.2" xref="Ch7.E3.m1.4.4.2.cmml"><munderover id="Ch7.E3.m1.4.4.2.2" xref="Ch7.E3.m1.4.4.2.2.cmml"><mo id="Ch7.E3.m1.4.4.2.2.2.2" movablelimits="false" xref="Ch7.E3.m1.4.4.2.2.2.2.cmml">∏</mo><mrow id="Ch7.E3.m1.4.4.2.2.2.3" xref="Ch7.E3.m1.4.4.2.2.2.3.cmml"><mi id="Ch7.E3.m1.4.4.2.2.2.3.2" xref="Ch7.E3.m1.4.4.2.2.2.3.2.cmml">t</mi><mo id="Ch7.E3.m1.4.4.2.2.2.3.1" xref="Ch7.E3.m1.4.4.2.2.2.3.1.cmml">=</mo><mn id="Ch7.E3.m1.4.4.2.2.2.3.3" xref="Ch7.E3.m1.4.4.2.2.2.3.3.cmml">1</mn></mrow><mi id="Ch7.E3.m1.4.4.2.2.3" xref="Ch7.E3.m1.4.4.2.2.3.cmml">T</mi></munderover><mrow id="Ch7.E3.m1.4.4.2.1" xref="Ch7.E3.m1.4.4.2.1.cmml"><mi id="Ch7.E3.m1.4.4.2.1.3" xref="Ch7.E3.m1.4.4.2.1.3.cmml">P</mi><mo id="Ch7.E3.m1.4.4.2.1.2" xref="Ch7.E3.m1.4.4.2.1.2.cmml">⁢</mo><mrow id="Ch7.E3.m1.4.4.2.1.1.1" xref="Ch7.E3.m1.4.4.2.1.1.1.1.cmml"><mo id="Ch7.E3.m1.4.4.2.1.1.1.2" stretchy="false" xref="Ch7.E3.m1.4.4.2.1.1.1.1.cmml">(</mo><mrow id="Ch7.E3.m1.4.4.2.1.1.1.1" xref="Ch7.E3.m1.4.4.2.1.1.1.1.cmml"><msub id="Ch7.E3.m1.4.4.2.1.1.1.1.4" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4.cmml"><mi id="Ch7.E3.m1.4.4.2.1.1.1.1.4.2" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4.2.cmml">y</mi><mi id="Ch7.E3.m1.4.4.2.1.1.1.1.4.3" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4.3.cmml">t</mi></msub><mo fence="false" id="Ch7.E3.m1.4.4.2.1.1.1.1.3" xref="Ch7.E3.m1.4.4.2.1.1.1.1.3.cmml">|</mo><mrow id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.3.cmml"><msub id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.2" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.2.cmml">y</mi><mn id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.3" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.3" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.3.cmml">,</mo><mi id="Ch7.E3.m1.1.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.cmml">…</mi><mo id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.4" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.3.cmml">,</mo><msub id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.cmml"><mi id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.2" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.2.cmml">y</mi><mrow id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.cmml"><mi id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.2" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.2.cmml">T</mi><mo id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.1" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.3" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.5" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.3.cmml">,</mo><mi id="Ch7.E3.m1.2.2" xref="Ch7.E3.m1.2.2.cmml">x</mi></mrow></mrow><mo id="Ch7.E3.m1.4.4.2.1.1.1.3" stretchy="false" xref="Ch7.E3.m1.4.4.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E3.m1.4b"><apply id="Ch7.E3.m1.4.4.cmml" xref="Ch7.E3.m1.4.4"><eq id="Ch7.E3.m1.4.4.3.cmml" xref="Ch7.E3.m1.4.4.3"></eq><apply id="Ch7.E3.m1.3.3.1.cmml" xref="Ch7.E3.m1.3.3.1"><times id="Ch7.E3.m1.3.3.1.2.cmml" xref="Ch7.E3.m1.3.3.1.2"></times><ci id="Ch7.E3.m1.3.3.1.3.cmml" xref="Ch7.E3.m1.3.3.1.3">𝑃</ci><apply id="Ch7.E3.m1.3.3.1.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.1"><csymbol cd="latexml" id="Ch7.E3.m1.3.3.1.1.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1">conditional</csymbol><ci id="Ch7.E3.m1.3.3.1.1.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.2">𝑦</ci><ci id="Ch7.E3.m1.3.3.1.1.1.1.3.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.3">𝑥</ci></apply></apply><apply id="Ch7.E3.m1.4.4.2.cmml" xref="Ch7.E3.m1.4.4.2"><apply id="Ch7.E3.m1.4.4.2.2.cmml" xref="Ch7.E3.m1.4.4.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.4.4.2.2.1.cmml" xref="Ch7.E3.m1.4.4.2.2">superscript</csymbol><apply id="Ch7.E3.m1.4.4.2.2.2.cmml" xref="Ch7.E3.m1.4.4.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.4.4.2.2.2.1.cmml" xref="Ch7.E3.m1.4.4.2.2">subscript</csymbol><csymbol cd="latexml" id="Ch7.E3.m1.4.4.2.2.2.2.cmml" xref="Ch7.E3.m1.4.4.2.2.2.2">product</csymbol><apply id="Ch7.E3.m1.4.4.2.2.2.3.cmml" xref="Ch7.E3.m1.4.4.2.2.2.3"><eq id="Ch7.E3.m1.4.4.2.2.2.3.1.cmml" xref="Ch7.E3.m1.4.4.2.2.2.3.1"></eq><ci id="Ch7.E3.m1.4.4.2.2.2.3.2.cmml" xref="Ch7.E3.m1.4.4.2.2.2.3.2">𝑡</ci><cn id="Ch7.E3.m1.4.4.2.2.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.4.4.2.2.2.3.3">1</cn></apply></apply><ci id="Ch7.E3.m1.4.4.2.2.3.cmml" xref="Ch7.E3.m1.4.4.2.2.3">𝑇</ci></apply><apply id="Ch7.E3.m1.4.4.2.1.cmml" xref="Ch7.E3.m1.4.4.2.1"><times id="Ch7.E3.m1.4.4.2.1.2.cmml" xref="Ch7.E3.m1.4.4.2.1.2"></times><ci id="Ch7.E3.m1.4.4.2.1.3.cmml" xref="Ch7.E3.m1.4.4.2.1.3">𝑃</ci><apply id="Ch7.E3.m1.4.4.2.1.1.1.1.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1"><csymbol cd="latexml" id="Ch7.E3.m1.4.4.2.1.1.1.1.3.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.3">conditional</csymbol><apply id="Ch7.E3.m1.4.4.2.1.1.1.1.4.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4"><csymbol cd="ambiguous" id="Ch7.E3.m1.4.4.2.1.1.1.1.4.1.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4">subscript</csymbol><ci id="Ch7.E3.m1.4.4.2.1.1.1.1.4.2.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4.2">𝑦</ci><ci id="Ch7.E3.m1.4.4.2.1.1.1.1.4.3.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.4.3">𝑡</ci></apply><list id="Ch7.E3.m1.4.4.2.1.1.1.1.2.3.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2"><apply id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.2.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.2">𝑦</ci><cn id="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.4.4.2.1.1.1.1.1.1.1.3">1</cn></apply><ci id="Ch7.E3.m1.1.1.cmml" xref="Ch7.E3.m1.1.1">…</ci><apply id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.1.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.2.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.2">𝑦</ci><apply id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3"><minus id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.1.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.1"></minus><ci id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.2.cmml" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.2">𝑇</ci><cn id="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.4.4.2.1.1.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="Ch7.E3.m1.2.2.cmml" xref="Ch7.E3.m1.2.2">𝑥</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E3.m1.4c">P(y|x)=\prod_{t=1}^{T}P(y_{t}|y_{1},...,y_{T-1},x)</annotation><annotation encoding="application/x-llamapun" id="Ch7.E3.m1.4d">italic_P ( italic_y | italic_x ) = ∏ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_P ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT , italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p6">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p6.1">Prior to Transformer, encoder-decoder models that incorporate RNNs were the most common method of representing text sequences in NMT. RNNs are networks which accumulate information composed of similar units repeated over time. In NMT, a primary function of the RNN encoder is that it encodes text, i.e. it turns text into a numeric representation. Neurons within an RNN are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F4" title="Figure 7.4 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.4</span></a>.</p>
</div>
<figure class="ltx_figure" id="Ch7.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch7.F4.g1" src="neurons.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F4.6.5.1" style="font-size:90%;">Figure 7.4</span>: </span><span class="ltx_text" id="Ch7.F4.4.4" style="font-size:90%;">Neurons within an RNN. At the input side, the neuron’s input at time <math alttext="t" class="ltx_Math" display="inline" id="Ch7.F4.1.1.m1.1"><semantics id="Ch7.F4.1.1.m1.1b"><mi id="Ch7.F4.1.1.m1.1.1" xref="Ch7.F4.1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="Ch7.F4.1.1.m1.1c"><ci id="Ch7.F4.1.1.m1.1.1.cmml" xref="Ch7.F4.1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F4.1.1.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="Ch7.F4.1.1.m1.1e">italic_t</annotation></semantics></math> is a function of the encoded word (i.e. input vector <math alttext="x_{t}" class="ltx_Math" display="inline" id="Ch7.F4.2.2.m2.1"><semantics id="Ch7.F4.2.2.m2.1b"><msub id="Ch7.F4.2.2.m2.1.1" xref="Ch7.F4.2.2.m2.1.1.cmml"><mi id="Ch7.F4.2.2.m2.1.1.2" xref="Ch7.F4.2.2.m2.1.1.2.cmml">x</mi><mi id="Ch7.F4.2.2.m2.1.1.3" xref="Ch7.F4.2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.F4.2.2.m2.1c"><apply id="Ch7.F4.2.2.m2.1.1.cmml" xref="Ch7.F4.2.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.F4.2.2.m2.1.1.1.cmml" xref="Ch7.F4.2.2.m2.1.1">subscript</csymbol><ci id="Ch7.F4.2.2.m2.1.1.2.cmml" xref="Ch7.F4.2.2.m2.1.1.2">𝑥</ci><ci id="Ch7.F4.2.2.m2.1.1.3.cmml" xref="Ch7.F4.2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F4.2.2.m2.1d">x_{t}</annotation><annotation encoding="application/x-llamapun" id="Ch7.F4.2.2.m2.1e">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>) and a hidden state vector <math alttext="h_{t-1}" class="ltx_Math" display="inline" id="Ch7.F4.3.3.m3.1"><semantics id="Ch7.F4.3.3.m3.1b"><msub id="Ch7.F4.3.3.m3.1.1" xref="Ch7.F4.3.3.m3.1.1.cmml"><mi id="Ch7.F4.3.3.m3.1.1.2" xref="Ch7.F4.3.3.m3.1.1.2.cmml">h</mi><mrow id="Ch7.F4.3.3.m3.1.1.3" xref="Ch7.F4.3.3.m3.1.1.3.cmml"><mi id="Ch7.F4.3.3.m3.1.1.3.2" xref="Ch7.F4.3.3.m3.1.1.3.2.cmml">t</mi><mo id="Ch7.F4.3.3.m3.1.1.3.1" xref="Ch7.F4.3.3.m3.1.1.3.1.cmml">−</mo><mn id="Ch7.F4.3.3.m3.1.1.3.3" xref="Ch7.F4.3.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch7.F4.3.3.m3.1c"><apply id="Ch7.F4.3.3.m3.1.1.cmml" xref="Ch7.F4.3.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.F4.3.3.m3.1.1.1.cmml" xref="Ch7.F4.3.3.m3.1.1">subscript</csymbol><ci id="Ch7.F4.3.3.m3.1.1.2.cmml" xref="Ch7.F4.3.3.m3.1.1.2">ℎ</ci><apply id="Ch7.F4.3.3.m3.1.1.3.cmml" xref="Ch7.F4.3.3.m3.1.1.3"><minus id="Ch7.F4.3.3.m3.1.1.3.1.cmml" xref="Ch7.F4.3.3.m3.1.1.3.1"></minus><ci id="Ch7.F4.3.3.m3.1.1.3.2.cmml" xref="Ch7.F4.3.3.m3.1.1.3.2">𝑡</ci><cn id="Ch7.F4.3.3.m3.1.1.3.3.cmml" type="integer" xref="Ch7.F4.3.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F4.3.3.m3.1d">h_{t-1}</annotation><annotation encoding="application/x-llamapun" id="Ch7.F4.3.3.m3.1e">italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> which contains the previous sequence. The output generated by the neuron is represented by the vector <math alttext="O_{t}" class="ltx_Math" display="inline" id="Ch7.F4.4.4.m4.1"><semantics id="Ch7.F4.4.4.m4.1b"><msub id="Ch7.F4.4.4.m4.1.1" xref="Ch7.F4.4.4.m4.1.1.cmml"><mi id="Ch7.F4.4.4.m4.1.1.2" xref="Ch7.F4.4.4.m4.1.1.2.cmml">O</mi><mi id="Ch7.F4.4.4.m4.1.1.3" xref="Ch7.F4.4.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.F4.4.4.m4.1c"><apply id="Ch7.F4.4.4.m4.1.1.cmml" xref="Ch7.F4.4.4.m4.1.1"><csymbol cd="ambiguous" id="Ch7.F4.4.4.m4.1.1.1.cmml" xref="Ch7.F4.4.4.m4.1.1">subscript</csymbol><ci id="Ch7.F4.4.4.m4.1.1.2.cmml" xref="Ch7.F4.4.4.m4.1.1.2">𝑂</ci><ci id="Ch7.F4.4.4.m4.1.1.3.cmml" xref="Ch7.F4.4.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F4.4.4.m4.1d">O_{t}</annotation><annotation encoding="application/x-llamapun" id="Ch7.F4.4.4.m4.1e">italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p7">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p7.1">Decoders unfold the vector representing the sequence state and return text. An important distinction between an encoder and a decoder is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F5" title="Figure 7.5 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a>, where it can be seen that both the encoder hidden state and the output from the previous decoding state are required by the decoder.</p>
</div>
<figure class="ltx_figure" id="Ch7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch7.F5.g1" src="nmtarch.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F5.3.2.1" style="font-size:90%;">Figure 7.5</span>: </span><span class="ltx_text" id="Ch7.F5.1.1" style="font-size:90%;">Encoder-decoder architecture. The encoder encodes the entire input sequence into a fixed-length <span class="ltx_text ltx_font_italic" id="Ch7.F5.1.1.1">context vector</span>, <math alttext="c" class="ltx_Math" display="inline" id="Ch7.F5.1.1.m1.1"><semantics id="Ch7.F5.1.1.m1.1b"><mi id="Ch7.F5.1.1.m1.1.1" xref="Ch7.F5.1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Ch7.F5.1.1.m1.1c"><ci id="Ch7.F5.1.1.m1.1.1.cmml" xref="Ch7.F5.1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F5.1.1.m1.1d">c</annotation><annotation encoding="application/x-llamapun" id="Ch7.F5.1.1.m1.1e">italic_c</annotation></semantics></math>, by processing input time steps. The function of the decoder is to read this <span class="ltx_text ltx_font_italic" id="Ch7.F5.1.1.2">context vector</span> while stepping through output time steps.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p8">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p8.1">To kick-start the processing of the decoder, a special token <math alttext="&lt;start&gt;" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx1.p8.1.m1.1a"><mrow id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.2.cmml"><mo fence="true" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.2" rspace="0em" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.2" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.2.cmml">s</mi><mo id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.3" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.3.cmml">t</mi><mo id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1a" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.4" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.4.cmml">a</mi><mo id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1b" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.5" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.5.cmml">r</mi><mo id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1c" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.6" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.6.cmml">t</mi></mrow><mo fence="true" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.3" lspace="0em" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1b"><apply id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1"><csymbol cd="latexml" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.2.1.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.2">expectation</csymbol><apply id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1"><times id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.1"></times><ci id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.2">𝑠</ci><ci id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.3">𝑡</ci><ci id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.4.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.4">𝑎</ci><ci id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.5.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.5">𝑟</ci><ci id="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.6.cmml" xref="Ch7.S5.SS4.SSSx1.p8.1.m1.1.1.1.1.6">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1c">&lt;start&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p8.1.m1.1d">&lt; italic_s italic_t italic_a italic_r italic_t &gt;</annotation></semantics></math> is used since there is no previous output. The calculations carried out by the encoder are summarised in Equation <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E4" title="7.4 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.4</span></a>:</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p9">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h_{t}=RNN_{ENC}(x_{t},h_{t-1})" class="ltx_Math" display="block" id="Ch7.E4.m1.2"><semantics id="Ch7.E4.m1.2a"><mrow id="Ch7.E4.m1.2.2" xref="Ch7.E4.m1.2.2.cmml"><msub id="Ch7.E4.m1.2.2.4" xref="Ch7.E4.m1.2.2.4.cmml"><mi id="Ch7.E4.m1.2.2.4.2" xref="Ch7.E4.m1.2.2.4.2.cmml">h</mi><mi id="Ch7.E4.m1.2.2.4.3" xref="Ch7.E4.m1.2.2.4.3.cmml">t</mi></msub><mo id="Ch7.E4.m1.2.2.3" xref="Ch7.E4.m1.2.2.3.cmml">=</mo><mrow id="Ch7.E4.m1.2.2.2" xref="Ch7.E4.m1.2.2.2.cmml"><mi id="Ch7.E4.m1.2.2.2.4" xref="Ch7.E4.m1.2.2.2.4.cmml">R</mi><mo id="Ch7.E4.m1.2.2.2.3" xref="Ch7.E4.m1.2.2.2.3.cmml">⁢</mo><mi id="Ch7.E4.m1.2.2.2.5" xref="Ch7.E4.m1.2.2.2.5.cmml">N</mi><mo id="Ch7.E4.m1.2.2.2.3a" xref="Ch7.E4.m1.2.2.2.3.cmml">⁢</mo><msub id="Ch7.E4.m1.2.2.2.6" xref="Ch7.E4.m1.2.2.2.6.cmml"><mi id="Ch7.E4.m1.2.2.2.6.2" xref="Ch7.E4.m1.2.2.2.6.2.cmml">N</mi><mrow id="Ch7.E4.m1.2.2.2.6.3" xref="Ch7.E4.m1.2.2.2.6.3.cmml"><mi id="Ch7.E4.m1.2.2.2.6.3.2" xref="Ch7.E4.m1.2.2.2.6.3.2.cmml">E</mi><mo id="Ch7.E4.m1.2.2.2.6.3.1" xref="Ch7.E4.m1.2.2.2.6.3.1.cmml">⁢</mo><mi id="Ch7.E4.m1.2.2.2.6.3.3" xref="Ch7.E4.m1.2.2.2.6.3.3.cmml">N</mi><mo id="Ch7.E4.m1.2.2.2.6.3.1a" xref="Ch7.E4.m1.2.2.2.6.3.1.cmml">⁢</mo><mi id="Ch7.E4.m1.2.2.2.6.3.4" xref="Ch7.E4.m1.2.2.2.6.3.4.cmml">C</mi></mrow></msub><mo id="Ch7.E4.m1.2.2.2.3b" xref="Ch7.E4.m1.2.2.2.3.cmml">⁢</mo><mrow id="Ch7.E4.m1.2.2.2.2.2" xref="Ch7.E4.m1.2.2.2.2.3.cmml"><mo id="Ch7.E4.m1.2.2.2.2.2.3" stretchy="false" xref="Ch7.E4.m1.2.2.2.2.3.cmml">(</mo><msub id="Ch7.E4.m1.1.1.1.1.1.1" xref="Ch7.E4.m1.1.1.1.1.1.1.cmml"><mi id="Ch7.E4.m1.1.1.1.1.1.1.2" xref="Ch7.E4.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="Ch7.E4.m1.1.1.1.1.1.1.3" xref="Ch7.E4.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="Ch7.E4.m1.2.2.2.2.2.4" xref="Ch7.E4.m1.2.2.2.2.3.cmml">,</mo><msub id="Ch7.E4.m1.2.2.2.2.2.2" xref="Ch7.E4.m1.2.2.2.2.2.2.cmml"><mi id="Ch7.E4.m1.2.2.2.2.2.2.2" xref="Ch7.E4.m1.2.2.2.2.2.2.2.cmml">h</mi><mrow id="Ch7.E4.m1.2.2.2.2.2.2.3" xref="Ch7.E4.m1.2.2.2.2.2.2.3.cmml"><mi id="Ch7.E4.m1.2.2.2.2.2.2.3.2" xref="Ch7.E4.m1.2.2.2.2.2.2.3.2.cmml">t</mi><mo id="Ch7.E4.m1.2.2.2.2.2.2.3.1" xref="Ch7.E4.m1.2.2.2.2.2.2.3.1.cmml">−</mo><mn id="Ch7.E4.m1.2.2.2.2.2.2.3.3" xref="Ch7.E4.m1.2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="Ch7.E4.m1.2.2.2.2.2.5" stretchy="false" xref="Ch7.E4.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E4.m1.2b"><apply id="Ch7.E4.m1.2.2.cmml" xref="Ch7.E4.m1.2.2"><eq id="Ch7.E4.m1.2.2.3.cmml" xref="Ch7.E4.m1.2.2.3"></eq><apply id="Ch7.E4.m1.2.2.4.cmml" xref="Ch7.E4.m1.2.2.4"><csymbol cd="ambiguous" id="Ch7.E4.m1.2.2.4.1.cmml" xref="Ch7.E4.m1.2.2.4">subscript</csymbol><ci id="Ch7.E4.m1.2.2.4.2.cmml" xref="Ch7.E4.m1.2.2.4.2">ℎ</ci><ci id="Ch7.E4.m1.2.2.4.3.cmml" xref="Ch7.E4.m1.2.2.4.3">𝑡</ci></apply><apply id="Ch7.E4.m1.2.2.2.cmml" xref="Ch7.E4.m1.2.2.2"><times id="Ch7.E4.m1.2.2.2.3.cmml" xref="Ch7.E4.m1.2.2.2.3"></times><ci id="Ch7.E4.m1.2.2.2.4.cmml" xref="Ch7.E4.m1.2.2.2.4">𝑅</ci><ci id="Ch7.E4.m1.2.2.2.5.cmml" xref="Ch7.E4.m1.2.2.2.5">𝑁</ci><apply id="Ch7.E4.m1.2.2.2.6.cmml" xref="Ch7.E4.m1.2.2.2.6"><csymbol cd="ambiguous" id="Ch7.E4.m1.2.2.2.6.1.cmml" xref="Ch7.E4.m1.2.2.2.6">subscript</csymbol><ci id="Ch7.E4.m1.2.2.2.6.2.cmml" xref="Ch7.E4.m1.2.2.2.6.2">𝑁</ci><apply id="Ch7.E4.m1.2.2.2.6.3.cmml" xref="Ch7.E4.m1.2.2.2.6.3"><times id="Ch7.E4.m1.2.2.2.6.3.1.cmml" xref="Ch7.E4.m1.2.2.2.6.3.1"></times><ci id="Ch7.E4.m1.2.2.2.6.3.2.cmml" xref="Ch7.E4.m1.2.2.2.6.3.2">𝐸</ci><ci id="Ch7.E4.m1.2.2.2.6.3.3.cmml" xref="Ch7.E4.m1.2.2.2.6.3.3">𝑁</ci><ci id="Ch7.E4.m1.2.2.2.6.3.4.cmml" xref="Ch7.E4.m1.2.2.2.6.3.4">𝐶</ci></apply></apply><interval closure="open" id="Ch7.E4.m1.2.2.2.2.3.cmml" xref="Ch7.E4.m1.2.2.2.2.2"><apply id="Ch7.E4.m1.1.1.1.1.1.1.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E4.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E4.m1.1.1.1.1.1.1.2.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="Ch7.E4.m1.1.1.1.1.1.1.3.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="Ch7.E4.m1.2.2.2.2.2.2.cmml" xref="Ch7.E4.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="Ch7.E4.m1.2.2.2.2.2.2.1.cmml" xref="Ch7.E4.m1.2.2.2.2.2.2">subscript</csymbol><ci id="Ch7.E4.m1.2.2.2.2.2.2.2.cmml" xref="Ch7.E4.m1.2.2.2.2.2.2.2">ℎ</ci><apply id="Ch7.E4.m1.2.2.2.2.2.2.3.cmml" xref="Ch7.E4.m1.2.2.2.2.2.2.3"><minus id="Ch7.E4.m1.2.2.2.2.2.2.3.1.cmml" xref="Ch7.E4.m1.2.2.2.2.2.2.3.1"></minus><ci id="Ch7.E4.m1.2.2.2.2.2.2.3.2.cmml" xref="Ch7.E4.m1.2.2.2.2.2.2.3.2">𝑡</ci><cn id="Ch7.E4.m1.2.2.2.2.2.2.3.3.cmml" type="integer" xref="Ch7.E4.m1.2.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E4.m1.2c">h_{t}=RNN_{ENC}(x_{t},h_{t-1})</annotation><annotation encoding="application/x-llamapun" id="Ch7.E4.m1.2d">italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R italic_N italic_N start_POSTSUBSCRIPT italic_E italic_N italic_C end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4)</span></td>
</tr></tbody>
</table>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p9.4">The <math alttext="RNN_{ENC}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p9.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx1.p9.1.m1.1a"><mrow id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.2" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.2.cmml">R</mi><mo id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.3" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.3.cmml">N</mi><mo id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.1a" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.cmml"><mi id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.2" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.2.cmml">N</mi><mrow id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.cmml"><mi id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.2" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.2.cmml">E</mi><mo id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.1" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.3" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.3.cmml">N</mi><mo id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.1a" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.4" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.4.cmml">C</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p9.1.m1.1b"><apply id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1"><times id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.1"></times><ci id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.2">𝑅</ci><ci id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.3">𝑁</ci><apply id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.2.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.2">𝑁</ci><apply id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3"><times id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.1"></times><ci id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.2.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.2">𝐸</ci><ci id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.3">𝑁</ci><ci id="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.4.cmml" xref="Ch7.S5.SS4.SSSx1.p9.1.m1.1.1.4.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p9.1.m1.1c">RNN_{ENC}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p9.1.m1.1d">italic_R italic_N italic_N start_POSTSUBSCRIPT italic_E italic_N italic_C end_POSTSUBSCRIPT</annotation></semantics></math> function is iteratively applied over the input sequence to generate the final encoder state, <math alttext="h_{s}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p9.2.m2.1"><semantics id="Ch7.S5.SS4.SSSx1.p9.2.m2.1a"><msub id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.2" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.2.cmml">h</mi><mi id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.3" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p9.2.m2.1b"><apply id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.2">ℎ</ci><ci id="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p9.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p9.2.m2.1c">h_{s}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p9.2.m2.1d">italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> which is fed to the decoder. The complete source sentence is effectively represented by <math alttext="h_{s}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p9.3.m3.1"><semantics id="Ch7.S5.SS4.SSSx1.p9.3.m3.1a"><msub id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.2" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.2.cmml">h</mi><mi id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.3" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p9.3.m3.1b"><apply id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.2">ℎ</ci><ci id="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p9.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p9.3.m3.1c">h_{s}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p9.3.m3.1d">italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. The decoder within the model predicts the next word of the target sentence y, while such predictions are conditional on the source sentence <math alttext="x" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p9.4.m4.1"><semantics id="Ch7.S5.SS4.SSSx1.p9.4.m4.1a"><mi id="Ch7.S5.SS4.SSSx1.p9.4.m4.1.1" xref="Ch7.S5.SS4.SSSx1.p9.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p9.4.m4.1b"><ci id="Ch7.S5.SS4.SSSx1.p9.4.m4.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p9.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p9.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p9.4.m4.1d">italic_x</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p10">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx1.p10.3">The RNN decoder, <math alttext="RNN_{DEC}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p10.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx1.p10.1.m1.1a"><mrow id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.2" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.2.cmml">R</mi><mo id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.1" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.3" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.3.cmml">N</mi><mo id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.1a" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.2" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.2.cmml">N</mi><mrow id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.2" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.2.cmml">D</mi><mo id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.1" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.3" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.3.cmml">E</mi><mo id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.1a" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.4" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.4.cmml">C</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p10.1.m1.1b"><apply id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1"><times id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.1"></times><ci id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.2">𝑅</ci><ci id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.3">𝑁</ci><apply id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.2">𝑁</ci><apply id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3"><times id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.1"></times><ci id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.2">𝐷</ci><ci id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.3">𝐸</ci><ci id="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.4.cmml" xref="Ch7.S5.SS4.SSSx1.p10.1.m1.1.1.4.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p10.1.m1.1c">RNN_{DEC}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p10.1.m1.1d">italic_R italic_N italic_N start_POSTSUBSCRIPT italic_D italic_E italic_C end_POSTSUBSCRIPT</annotation></semantics></math>, creates a state vector <math alttext="s_{t}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p10.2.m2.1"><semantics id="Ch7.S5.SS4.SSSx1.p10.2.m2.1a"><msub id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.2" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.2.cmml">s</mi><mi id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.3" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p10.2.m2.1b"><apply id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.2">𝑠</ci><ci id="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx1.p10.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p10.2.m2.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p10.2.m2.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> by compressing the decoding history <math alttext="{y_{0},…,y_{t-1}}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx1.p10.3.m3.3"><semantics id="Ch7.S5.SS4.SSSx1.p10.3.m3.3a"><mrow id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.3.cmml"><msub id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.2" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.2.cmml">y</mi><mn id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.3" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.3" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.3.cmml">,</mo><mi id="Ch7.S5.SS4.SSSx1.p10.3.m3.1.1" mathvariant="normal" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.1.1.cmml">…</mi><mo id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.4" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.3.cmml">,</mo><msub id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.2" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.2.cmml">y</mi><mrow id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.cmml"><mi id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.2" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.2.cmml">t</mi><mo id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.1" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.1.cmml">−</mo><mn id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.3" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx1.p10.3.m3.3b"><list id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.3.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2"><apply id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.2">𝑦</ci><cn id="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.3.cmml" type="integer" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.2.2.1.1.3">0</cn></apply><ci id="Ch7.S5.SS4.SSSx1.p10.3.m3.1.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.1.1">…</ci><apply id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2">subscript</csymbol><ci id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.2">𝑦</ci><apply id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3"><minus id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.1.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.1"></minus><ci id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.2.cmml" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.2">𝑡</ci><cn id="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.3.cmml" type="integer" xref="Ch7.S5.SS4.SSSx1.p10.3.m3.3.3.2.2.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx1.p10.3.m3.3c">{y_{0},…,y_{t-1}}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx1.p10.3.m3.3d">italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> which is described in Equation <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E5" title="7.5 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a>. The distribution of target tokens is predicted by a classification layer which typically uses the Softmax activation function.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx1.p11">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s_{t}=RNN_{DEC}(y_{t-1},s_{t-1})" class="ltx_Math" display="block" id="Ch7.E5.m1.2"><semantics id="Ch7.E5.m1.2a"><mrow id="Ch7.E5.m1.2.2" xref="Ch7.E5.m1.2.2.cmml"><msub id="Ch7.E5.m1.2.2.4" xref="Ch7.E5.m1.2.2.4.cmml"><mi id="Ch7.E5.m1.2.2.4.2" xref="Ch7.E5.m1.2.2.4.2.cmml">s</mi><mi id="Ch7.E5.m1.2.2.4.3" xref="Ch7.E5.m1.2.2.4.3.cmml">t</mi></msub><mo id="Ch7.E5.m1.2.2.3" xref="Ch7.E5.m1.2.2.3.cmml">=</mo><mrow id="Ch7.E5.m1.2.2.2" xref="Ch7.E5.m1.2.2.2.cmml"><mi id="Ch7.E5.m1.2.2.2.4" xref="Ch7.E5.m1.2.2.2.4.cmml">R</mi><mo id="Ch7.E5.m1.2.2.2.3" xref="Ch7.E5.m1.2.2.2.3.cmml">⁢</mo><mi id="Ch7.E5.m1.2.2.2.5" xref="Ch7.E5.m1.2.2.2.5.cmml">N</mi><mo id="Ch7.E5.m1.2.2.2.3a" xref="Ch7.E5.m1.2.2.2.3.cmml">⁢</mo><msub id="Ch7.E5.m1.2.2.2.6" xref="Ch7.E5.m1.2.2.2.6.cmml"><mi id="Ch7.E5.m1.2.2.2.6.2" xref="Ch7.E5.m1.2.2.2.6.2.cmml">N</mi><mrow id="Ch7.E5.m1.2.2.2.6.3" xref="Ch7.E5.m1.2.2.2.6.3.cmml"><mi id="Ch7.E5.m1.2.2.2.6.3.2" xref="Ch7.E5.m1.2.2.2.6.3.2.cmml">D</mi><mo id="Ch7.E5.m1.2.2.2.6.3.1" xref="Ch7.E5.m1.2.2.2.6.3.1.cmml">⁢</mo><mi id="Ch7.E5.m1.2.2.2.6.3.3" xref="Ch7.E5.m1.2.2.2.6.3.3.cmml">E</mi><mo id="Ch7.E5.m1.2.2.2.6.3.1a" xref="Ch7.E5.m1.2.2.2.6.3.1.cmml">⁢</mo><mi id="Ch7.E5.m1.2.2.2.6.3.4" xref="Ch7.E5.m1.2.2.2.6.3.4.cmml">C</mi></mrow></msub><mo id="Ch7.E5.m1.2.2.2.3b" xref="Ch7.E5.m1.2.2.2.3.cmml">⁢</mo><mrow id="Ch7.E5.m1.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.3.cmml"><mo id="Ch7.E5.m1.2.2.2.2.2.3" stretchy="false" xref="Ch7.E5.m1.2.2.2.2.3.cmml">(</mo><msub id="Ch7.E5.m1.1.1.1.1.1.1" xref="Ch7.E5.m1.1.1.1.1.1.1.cmml"><mi id="Ch7.E5.m1.1.1.1.1.1.1.2" xref="Ch7.E5.m1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="Ch7.E5.m1.1.1.1.1.1.1.3" xref="Ch7.E5.m1.1.1.1.1.1.1.3.cmml"><mi id="Ch7.E5.m1.1.1.1.1.1.1.3.2" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="Ch7.E5.m1.1.1.1.1.1.1.3.1" xref="Ch7.E5.m1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="Ch7.E5.m1.1.1.1.1.1.1.3.3" xref="Ch7.E5.m1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="Ch7.E5.m1.2.2.2.2.2.4" xref="Ch7.E5.m1.2.2.2.2.3.cmml">,</mo><msub id="Ch7.E5.m1.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.cmml">s</mi><mrow id="Ch7.E5.m1.2.2.2.2.2.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.cmml">t</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.1" xref="Ch7.E5.m1.2.2.2.2.2.2.3.1.cmml">−</mo><mn id="Ch7.E5.m1.2.2.2.2.2.2.3.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="Ch7.E5.m1.2.2.2.2.2.5" stretchy="false" xref="Ch7.E5.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E5.m1.2b"><apply id="Ch7.E5.m1.2.2.cmml" xref="Ch7.E5.m1.2.2"><eq id="Ch7.E5.m1.2.2.3.cmml" xref="Ch7.E5.m1.2.2.3"></eq><apply id="Ch7.E5.m1.2.2.4.cmml" xref="Ch7.E5.m1.2.2.4"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.4.1.cmml" xref="Ch7.E5.m1.2.2.4">subscript</csymbol><ci id="Ch7.E5.m1.2.2.4.2.cmml" xref="Ch7.E5.m1.2.2.4.2">𝑠</ci><ci id="Ch7.E5.m1.2.2.4.3.cmml" xref="Ch7.E5.m1.2.2.4.3">𝑡</ci></apply><apply id="Ch7.E5.m1.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2"><times id="Ch7.E5.m1.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.3"></times><ci id="Ch7.E5.m1.2.2.2.4.cmml" xref="Ch7.E5.m1.2.2.2.4">𝑅</ci><ci id="Ch7.E5.m1.2.2.2.5.cmml" xref="Ch7.E5.m1.2.2.2.5">𝑁</ci><apply id="Ch7.E5.m1.2.2.2.6.cmml" xref="Ch7.E5.m1.2.2.2.6"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.2.6.1.cmml" xref="Ch7.E5.m1.2.2.2.6">subscript</csymbol><ci id="Ch7.E5.m1.2.2.2.6.2.cmml" xref="Ch7.E5.m1.2.2.2.6.2">𝑁</ci><apply id="Ch7.E5.m1.2.2.2.6.3.cmml" xref="Ch7.E5.m1.2.2.2.6.3"><times id="Ch7.E5.m1.2.2.2.6.3.1.cmml" xref="Ch7.E5.m1.2.2.2.6.3.1"></times><ci id="Ch7.E5.m1.2.2.2.6.3.2.cmml" xref="Ch7.E5.m1.2.2.2.6.3.2">𝐷</ci><ci id="Ch7.E5.m1.2.2.2.6.3.3.cmml" xref="Ch7.E5.m1.2.2.2.6.3.3">𝐸</ci><ci id="Ch7.E5.m1.2.2.2.6.3.4.cmml" xref="Ch7.E5.m1.2.2.2.6.3.4">𝐶</ci></apply></apply><interval closure="open" id="Ch7.E5.m1.2.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2"><apply id="Ch7.E5.m1.1.1.1.1.1.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E5.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E5.m1.1.1.1.1.1.1.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2">𝑦</ci><apply id="Ch7.E5.m1.1.1.1.1.1.1.3.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3"><minus id="Ch7.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3.1"></minus><ci id="Ch7.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2">𝑡</ci><cn id="Ch7.E5.m1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="Ch7.E5.m1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="Ch7.E5.m1.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.2.2.2.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2">subscript</csymbol><ci id="Ch7.E5.m1.2.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2">𝑠</ci><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3"><minus id="Ch7.E5.m1.2.2.2.2.2.2.3.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.1"></minus><ci id="Ch7.E5.m1.2.2.2.2.2.2.3.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2">𝑡</ci><cn id="Ch7.E5.m1.2.2.2.2.2.2.3.3.cmml" type="integer" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E5.m1.2c">s_{t}=RNN_{DEC}(y_{t-1},s_{t-1})</annotation><annotation encoding="application/x-llamapun" id="Ch7.E5.m1.2d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R italic_N italic_N start_POSTSUBSCRIPT italic_D italic_E italic_C end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="Ch7.S5.SS4.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Learning</h5>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx2.p1">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx2.p1.1">It is possible to optimise models using different types of training objectives, although maximum log-likelihood (MLE) is the most commonly used method. Given a set of training examples <math alttext="D=\{(x^{s},y^{s})\}_{s=1}^{S}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx2.p1.1.m1.1a"><mrow id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.3" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.3.cmml">D</mi><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.2" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.cmml"><mrow id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.2.cmml"><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.3.cmml"><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.3" stretchy="false" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">(</mo><msup id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.2" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml">s</mi></msup><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.4" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">,</mo><msup id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.2" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.3" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.3.cmml">s</mi></msup><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.5" stretchy="false" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.cmml"><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.2" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.2.cmml">s</mi><mo id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.1" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.3" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.3" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.3.cmml">S</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1b"><apply id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1"><eq id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.2"></eq><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.3">𝐷</ci><apply id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1">superscript</csymbol><apply id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1">subscript</csymbol><set id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1"><interval closure="open" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2"><apply id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.1.1.3">𝑠</ci></apply><apply id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.2">𝑦</ci><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.1.1.1.2.2.3">𝑠</ci></apply></interval></set><apply id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3"><eq id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.1.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.1"></eq><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.2.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.2">𝑠</ci><cn id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.3.cmml" xref="Ch7.S5.SS4.SSSx2.p1.1.m1.1.1.1.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1c">D=\{(x^{s},y^{s})\}_{s=1}^{S}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx2.p1.1.m1.1d">italic_D = { ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT</annotation></semantics></math>, the MLE is maximised according to Equations <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E6" title="7.6 ‣ Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E7" title="7.7 ‣ Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx2.p2">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\boldsymbol{\hat{\theta}}_{MLE}=\arg\max_{\theta}\{\mathcal{L}(\theta)\}" class="ltx_Math" display="block" id="Ch7.E6.m1.3"><semantics id="Ch7.E6.m1.3a"><mrow id="Ch7.E6.m1.3.3" xref="Ch7.E6.m1.3.3.cmml"><msub id="Ch7.E6.m1.3.3.4" xref="Ch7.E6.m1.3.3.4.cmml"><mover accent="true" id="Ch7.E6.m1.3.3.4.2" xref="Ch7.E6.m1.3.3.4.2.cmml"><mi id="Ch7.E6.m1.3.3.4.2.2" xref="Ch7.E6.m1.3.3.4.2.2.cmml">𝜽</mi><mo id="Ch7.E6.m1.3.3.4.2.1" mathvariant="bold" xref="Ch7.E6.m1.3.3.4.2.1.cmml">^</mo></mover><mrow id="Ch7.E6.m1.3.3.4.3" xref="Ch7.E6.m1.3.3.4.3.cmml"><mi id="Ch7.E6.m1.3.3.4.3.2" xref="Ch7.E6.m1.3.3.4.3.2.cmml">M</mi><mo id="Ch7.E6.m1.3.3.4.3.1" xref="Ch7.E6.m1.3.3.4.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.3.3.4.3.3" xref="Ch7.E6.m1.3.3.4.3.3.cmml">L</mi><mo id="Ch7.E6.m1.3.3.4.3.1a" xref="Ch7.E6.m1.3.3.4.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.3.3.4.3.4" xref="Ch7.E6.m1.3.3.4.3.4.cmml">E</mi></mrow></msub><mo id="Ch7.E6.m1.3.3.3" xref="Ch7.E6.m1.3.3.3.cmml">=</mo><mrow id="Ch7.E6.m1.3.3.2" xref="Ch7.E6.m1.3.3.2.cmml"><mi id="Ch7.E6.m1.3.3.2.3" xref="Ch7.E6.m1.3.3.2.3.cmml">arg</mi><mo id="Ch7.E6.m1.3.3.2a" lspace="0.167em" xref="Ch7.E6.m1.3.3.2.cmml">⁡</mo><mrow id="Ch7.E6.m1.3.3.2.2.2" xref="Ch7.E6.m1.3.3.2.2.3.cmml"><munder id="Ch7.E6.m1.2.2.1.1.1.1" xref="Ch7.E6.m1.2.2.1.1.1.1.cmml"><mi id="Ch7.E6.m1.2.2.1.1.1.1.2" xref="Ch7.E6.m1.2.2.1.1.1.1.2.cmml">max</mi><mi id="Ch7.E6.m1.2.2.1.1.1.1.3" xref="Ch7.E6.m1.2.2.1.1.1.1.3.cmml">θ</mi></munder><mo id="Ch7.E6.m1.3.3.2.2.2a" xref="Ch7.E6.m1.3.3.2.2.3.cmml">⁡</mo><mrow id="Ch7.E6.m1.3.3.2.2.2.2" xref="Ch7.E6.m1.3.3.2.2.3.cmml"><mo id="Ch7.E6.m1.3.3.2.2.2.2.2" stretchy="false" xref="Ch7.E6.m1.3.3.2.2.3.cmml">{</mo><mrow id="Ch7.E6.m1.3.3.2.2.2.2.1" xref="Ch7.E6.m1.3.3.2.2.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Ch7.E6.m1.3.3.2.2.2.2.1.2" xref="Ch7.E6.m1.3.3.2.2.2.2.1.2.cmml">ℒ</mi><mo id="Ch7.E6.m1.3.3.2.2.2.2.1.1" xref="Ch7.E6.m1.3.3.2.2.2.2.1.1.cmml">⁢</mo><mrow id="Ch7.E6.m1.3.3.2.2.2.2.1.3.2" xref="Ch7.E6.m1.3.3.2.2.2.2.1.cmml"><mo id="Ch7.E6.m1.3.3.2.2.2.2.1.3.2.1" stretchy="false" xref="Ch7.E6.m1.3.3.2.2.2.2.1.cmml">(</mo><mi id="Ch7.E6.m1.1.1" xref="Ch7.E6.m1.1.1.cmml">θ</mi><mo id="Ch7.E6.m1.3.3.2.2.2.2.1.3.2.2" stretchy="false" xref="Ch7.E6.m1.3.3.2.2.2.2.1.cmml">)</mo></mrow></mrow><mo id="Ch7.E6.m1.3.3.2.2.2.2.3" stretchy="false" xref="Ch7.E6.m1.3.3.2.2.3.cmml">}</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E6.m1.3b"><apply id="Ch7.E6.m1.3.3.cmml" xref="Ch7.E6.m1.3.3"><eq id="Ch7.E6.m1.3.3.3.cmml" xref="Ch7.E6.m1.3.3.3"></eq><apply id="Ch7.E6.m1.3.3.4.cmml" xref="Ch7.E6.m1.3.3.4"><csymbol cd="ambiguous" id="Ch7.E6.m1.3.3.4.1.cmml" xref="Ch7.E6.m1.3.3.4">subscript</csymbol><apply id="Ch7.E6.m1.3.3.4.2.cmml" xref="Ch7.E6.m1.3.3.4.2"><ci id="Ch7.E6.m1.3.3.4.2.1.cmml" xref="Ch7.E6.m1.3.3.4.2.1">bold-^</ci><ci id="Ch7.E6.m1.3.3.4.2.2.cmml" xref="Ch7.E6.m1.3.3.4.2.2">𝜽</ci></apply><apply id="Ch7.E6.m1.3.3.4.3.cmml" xref="Ch7.E6.m1.3.3.4.3"><times id="Ch7.E6.m1.3.3.4.3.1.cmml" xref="Ch7.E6.m1.3.3.4.3.1"></times><ci id="Ch7.E6.m1.3.3.4.3.2.cmml" xref="Ch7.E6.m1.3.3.4.3.2">𝑀</ci><ci id="Ch7.E6.m1.3.3.4.3.3.cmml" xref="Ch7.E6.m1.3.3.4.3.3">𝐿</ci><ci id="Ch7.E6.m1.3.3.4.3.4.cmml" xref="Ch7.E6.m1.3.3.4.3.4">𝐸</ci></apply></apply><apply id="Ch7.E6.m1.3.3.2.cmml" xref="Ch7.E6.m1.3.3.2"><arg id="Ch7.E6.m1.3.3.2.3.cmml" xref="Ch7.E6.m1.3.3.2.3"></arg><apply id="Ch7.E6.m1.3.3.2.2.3.cmml" xref="Ch7.E6.m1.3.3.2.2.2"><apply id="Ch7.E6.m1.2.2.1.1.1.1.cmml" xref="Ch7.E6.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E6.m1.2.2.1.1.1.1.1.cmml" xref="Ch7.E6.m1.2.2.1.1.1.1">subscript</csymbol><max id="Ch7.E6.m1.2.2.1.1.1.1.2.cmml" xref="Ch7.E6.m1.2.2.1.1.1.1.2"></max><ci id="Ch7.E6.m1.2.2.1.1.1.1.3.cmml" xref="Ch7.E6.m1.2.2.1.1.1.1.3">𝜃</ci></apply><apply id="Ch7.E6.m1.3.3.2.2.2.2.1.cmml" xref="Ch7.E6.m1.3.3.2.2.2.2.1"><times id="Ch7.E6.m1.3.3.2.2.2.2.1.1.cmml" xref="Ch7.E6.m1.3.3.2.2.2.2.1.1"></times><ci id="Ch7.E6.m1.3.3.2.2.2.2.1.2.cmml" xref="Ch7.E6.m1.3.3.2.2.2.2.1.2">ℒ</ci><ci id="Ch7.E6.m1.1.1.cmml" xref="Ch7.E6.m1.1.1">𝜃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E6.m1.3c">\boldsymbol{\hat{\theta}}_{MLE}=\arg\max_{\theta}\{\mathcal{L}(\theta)\}</annotation><annotation encoding="application/x-llamapun" id="Ch7.E6.m1.3d">overbold_^ start_ARG bold_italic_θ end_ARG start_POSTSUBSCRIPT italic_M italic_L italic_E end_POSTSUBSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT { caligraphic_L ( italic_θ ) }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx2.p3">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\theta)\ =\sum_{s=1}^{S}logP(y^{s}|x^{s});\theta)" class="ltx_math_unparsed" display="block" id="Ch7.E7.m1.2"><semantics id="Ch7.E7.m1.2a"><mrow id="Ch7.E7.m1.2b"><mi class="ltx_font_mathcaligraphic" id="Ch7.E7.m1.2.3">ℒ</mi><mrow id="Ch7.E7.m1.2.4"><mo id="Ch7.E7.m1.2.4.1" stretchy="false">(</mo><mi id="Ch7.E7.m1.1.1">θ</mi><mo id="Ch7.E7.m1.2.4.2" rspace="0.500em" stretchy="false">)</mo></mrow><mo id="Ch7.E7.m1.2.5" rspace="0.111em">=</mo><munderover id="Ch7.E7.m1.2.6"><mo id="Ch7.E7.m1.2.6.2.2" movablelimits="false">∑</mo><mrow id="Ch7.E7.m1.2.6.2.3"><mi id="Ch7.E7.m1.2.6.2.3.2">s</mi><mo id="Ch7.E7.m1.2.6.2.3.1">=</mo><mn id="Ch7.E7.m1.2.6.2.3.3">1</mn></mrow><mi id="Ch7.E7.m1.2.6.3">S</mi></munderover><mi id="Ch7.E7.m1.2.7">l</mi><mi id="Ch7.E7.m1.2.8">o</mi><mi id="Ch7.E7.m1.2.9">g</mi><mi id="Ch7.E7.m1.2.10">P</mi><mrow id="Ch7.E7.m1.2.11"><mo id="Ch7.E7.m1.2.11.1" stretchy="false">(</mo><msup id="Ch7.E7.m1.2.11.2"><mi id="Ch7.E7.m1.2.11.2.2">y</mi><mi id="Ch7.E7.m1.2.11.2.3">s</mi></msup><mo fence="false" id="Ch7.E7.m1.2.11.3" rspace="0.167em" stretchy="false">|</mo><msup id="Ch7.E7.m1.2.11.4"><mi id="Ch7.E7.m1.2.11.4.2">x</mi><mi id="Ch7.E7.m1.2.11.4.3">s</mi></msup><mo id="Ch7.E7.m1.2.11.5" stretchy="false">)</mo></mrow><mo id="Ch7.E7.m1.2.12">;</mo><mi id="Ch7.E7.m1.2.2">θ</mi><mo id="Ch7.E7.m1.2.13" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="Ch7.E7.m1.2c">\mathcal{L}(\theta)\ =\sum_{s=1}^{S}logP(y^{s}|x^{s});\theta)</annotation><annotation encoding="application/x-llamapun" id="Ch7.E7.m1.2d">caligraphic_L ( italic_θ ) = ∑ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT italic_l italic_o italic_g italic_P ( italic_y start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT | italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx2.p4">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx2.p4.3">The gradient of <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx2.p4.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx2.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch7.S5.SS4.SSSx2.p4.1.m1.1.1" xref="Ch7.S5.SS4.SSSx2.p4.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx2.p4.1.m1.1b"><ci id="Ch7.S5.SS4.SSSx2.p4.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p4.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx2.p4.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx2.p4.1.m1.1d">caligraphic_L</annotation></semantics></math> with respect to <math alttext="\theta" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx2.p4.2.m2.1"><semantics id="Ch7.S5.SS4.SSSx2.p4.2.m2.1a"><mi id="Ch7.S5.SS4.SSSx2.p4.2.m2.1.1" xref="Ch7.S5.SS4.SSSx2.p4.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx2.p4.2.m2.1b"><ci id="Ch7.S5.SS4.SSSx2.p4.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p4.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx2.p4.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx2.p4.2.m2.1d">italic_θ</annotation></semantics></math> is calculated using back-propagation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx97" title="">97</a>]</cite> as an automatic differentiation algorithm for calculating gradients of the neural network weights, where <math alttext="\theta" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx2.p4.3.m3.1"><semantics id="Ch7.S5.SS4.SSSx2.p4.3.m3.1a"><mi id="Ch7.S5.SS4.SSSx2.p4.3.m3.1.1" xref="Ch7.S5.SS4.SSSx2.p4.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx2.p4.3.m3.1b"><ci id="Ch7.S5.SS4.SSSx2.p4.3.m3.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p4.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx2.p4.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx2.p4.3.m3.1d">italic_θ</annotation></semantics></math> is the set of model parameters.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx2.p5">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx2.p5.1">Many NMT approaches implement Stochastic Gradient Descent (SGD) as the optimisation algorithm for minimising the loss of the predictive model with regard to the training data. For reasons of computational efficiency, SGD typically computes the loss function and gradients on a minibatch of the training set. The standard SGD optimiser updates parameters of an NMT model according to Equation <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E8" title="7.8 ‣ Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a>, where the learning rate is specified by <math alttext="\alpha" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx2.p5.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx2.p5.1.m1.1a"><mi id="Ch7.S5.SS4.SSSx2.p5.1.m1.1.1" xref="Ch7.S5.SS4.SSSx2.p5.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx2.p5.1.m1.1b"><ci id="Ch7.S5.SS4.SSSx2.p5.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx2.p5.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx2.p5.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx2.p5.1.m1.1d">italic_α</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx2.p6">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta\leftarrow\theta-\alpha\bigtriangledown\mathcal{L}(\theta)" class="ltx_Math" display="block" id="Ch7.E8.m1.1"><semantics id="Ch7.E8.m1.1a"><mrow id="Ch7.E8.m1.1.2" xref="Ch7.E8.m1.1.2.cmml"><mi id="Ch7.E8.m1.1.2.2" xref="Ch7.E8.m1.1.2.2.cmml">θ</mi><mo id="Ch7.E8.m1.1.2.1" stretchy="false" xref="Ch7.E8.m1.1.2.1.cmml">←</mo><mrow id="Ch7.E8.m1.1.2.3" xref="Ch7.E8.m1.1.2.3.cmml"><mrow id="Ch7.E8.m1.1.2.3.2" xref="Ch7.E8.m1.1.2.3.2.cmml"><mi id="Ch7.E8.m1.1.2.3.2.2" xref="Ch7.E8.m1.1.2.3.2.2.cmml">θ</mi><mo id="Ch7.E8.m1.1.2.3.2.1" xref="Ch7.E8.m1.1.2.3.2.1.cmml">−</mo><mi id="Ch7.E8.m1.1.2.3.2.3" xref="Ch7.E8.m1.1.2.3.2.3.cmml">α</mi></mrow><mo id="Ch7.E8.m1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="Ch7.E8.m1.1.2.3.1.cmml">▽</mo><mrow id="Ch7.E8.m1.1.2.3.3" xref="Ch7.E8.m1.1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Ch7.E8.m1.1.2.3.3.2" xref="Ch7.E8.m1.1.2.3.3.2.cmml">ℒ</mi><mo id="Ch7.E8.m1.1.2.3.3.1" xref="Ch7.E8.m1.1.2.3.3.1.cmml">⁢</mo><mrow id="Ch7.E8.m1.1.2.3.3.3.2" xref="Ch7.E8.m1.1.2.3.3.cmml"><mo id="Ch7.E8.m1.1.2.3.3.3.2.1" stretchy="false" xref="Ch7.E8.m1.1.2.3.3.cmml">(</mo><mi id="Ch7.E8.m1.1.1" xref="Ch7.E8.m1.1.1.cmml">θ</mi><mo id="Ch7.E8.m1.1.2.3.3.3.2.2" stretchy="false" xref="Ch7.E8.m1.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E8.m1.1b"><apply id="Ch7.E8.m1.1.2.cmml" xref="Ch7.E8.m1.1.2"><ci id="Ch7.E8.m1.1.2.1.cmml" xref="Ch7.E8.m1.1.2.1">←</ci><ci id="Ch7.E8.m1.1.2.2.cmml" xref="Ch7.E8.m1.1.2.2">𝜃</ci><apply id="Ch7.E8.m1.1.2.3.cmml" xref="Ch7.E8.m1.1.2.3"><ci id="Ch7.E8.m1.1.2.3.1.cmml" xref="Ch7.E8.m1.1.2.3.1">▽</ci><apply id="Ch7.E8.m1.1.2.3.2.cmml" xref="Ch7.E8.m1.1.2.3.2"><minus id="Ch7.E8.m1.1.2.3.2.1.cmml" xref="Ch7.E8.m1.1.2.3.2.1"></minus><ci id="Ch7.E8.m1.1.2.3.2.2.cmml" xref="Ch7.E8.m1.1.2.3.2.2">𝜃</ci><ci id="Ch7.E8.m1.1.2.3.2.3.cmml" xref="Ch7.E8.m1.1.2.3.2.3">𝛼</ci></apply><apply id="Ch7.E8.m1.1.2.3.3.cmml" xref="Ch7.E8.m1.1.2.3.3"><times id="Ch7.E8.m1.1.2.3.3.1.cmml" xref="Ch7.E8.m1.1.2.3.3.1"></times><ci id="Ch7.E8.m1.1.2.3.3.2.cmml" xref="Ch7.E8.m1.1.2.3.3.2">ℒ</ci><ci id="Ch7.E8.m1.1.1.cmml" xref="Ch7.E8.m1.1.1">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E8.m1.1c">\theta\leftarrow\theta-\alpha\bigtriangledown\mathcal{L}(\theta)</annotation><annotation encoding="application/x-llamapun" id="Ch7.E8.m1.1d">italic_θ ← italic_θ - italic_α ▽ caligraphic_L ( italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.8)</span></td>
</tr></tbody>
</table>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch7.S5.SS4.SSSx2.p6.1">There are several alternatives to using SGD for optimisation, among which the ADAM optimiser has proven popular due to a reduction in training times <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx55" title="">55</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="Ch7.F6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F6.2.1.1" style="font-size:90%;">Figure 7.6</span>: </span><span class="ltx_text" id="Ch7.F6.3.2" style="font-size:90%;">Beam Search Algorithm</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch7.F6.g1" src="beamsearch.png"/></div>
<div class="ltx_flex_cell">
<p class="ltx_p ltx_align_center" id="Ch7.F6.4"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx120" title="">120</a>]</cite></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F6.2.1.1" style="font-size:90%;">Figure 7.6</span>: </span><span class="ltx_text" id="Ch7.F6.3.2" style="font-size:90%;">Beam Search Algorithm</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch7.S5.SS4.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Inference</h5>
<div class="ltx_para" id="Ch7.S5.SS4.SSSx3.p1">
<p class="ltx_p" id="Ch7.S5.SS4.SSSx3.p1.3">In the context of NMT, inference should ideally find the target translated sentence <math alttext="y" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx3.p1.1.m1.1"><semantics id="Ch7.S5.SS4.SSSx3.p1.1.m1.1a"><mi id="Ch7.S5.SS4.SSSx3.p1.1.m1.1.1" xref="Ch7.S5.SS4.SSSx3.p1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx3.p1.1.m1.1b"><ci id="Ch7.S5.SS4.SSSx3.p1.1.m1.1.1.cmml" xref="Ch7.S5.SS4.SSSx3.p1.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx3.p1.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx3.p1.1.m1.1d">italic_y</annotation></semantics></math> from the source <math alttext="x" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx3.p1.2.m2.1"><semantics id="Ch7.S5.SS4.SSSx3.p1.2.m2.1a"><mi id="Ch7.S5.SS4.SSSx3.p1.2.m2.1.1" xref="Ch7.S5.SS4.SSSx3.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx3.p1.2.m2.1b"><ci id="Ch7.S5.SS4.SSSx3.p1.2.m2.1.1.cmml" xref="Ch7.S5.SS4.SSSx3.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx3.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx3.p1.2.m2.1d">italic_x</annotation></semantics></math> which maximises the model prediction <math alttext="P(y|x;\theta)" class="ltx_Math" display="inline" id="Ch7.S5.SS4.SSSx3.p1.3.m3.3"><semantics id="Ch7.S5.SS4.SSSx3.p1.3.m3.3a"><mrow id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.cmml"><mi id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.3" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.3.cmml">P</mi><mo id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.2" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.2.cmml">⁢</mo><mrow id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.cmml"><mo id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.2" stretchy="false" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.cmml">(</mo><mrow id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.cmml"><mi id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.2" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.2.cmml">y</mi><mo fence="false" id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.1" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.1.cmml">|</mo><mrow id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.3.2" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.3.1.cmml"><mi id="Ch7.S5.SS4.SSSx3.p1.3.m3.1.1" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.1.1.cmml">x</mi><mo id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.3.2.1" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.3.1.cmml">;</mo><mi id="Ch7.S5.SS4.SSSx3.p1.3.m3.2.2" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.2.2.cmml">θ</mi></mrow></mrow><mo id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.3" stretchy="false" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S5.SS4.SSSx3.p1.3.m3.3b"><apply id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3"><times id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.2.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.2"></times><ci id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.3.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.3">𝑃</ci><apply id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1"><csymbol cd="latexml" id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.1.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.1">conditional</csymbol><ci id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.2.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.2">𝑦</ci><list id="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.3.1.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.3.3.1.1.1.3.2"><ci id="Ch7.S5.SS4.SSSx3.p1.3.m3.1.1.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.1.1">𝑥</ci><ci id="Ch7.S5.SS4.SSSx3.p1.3.m3.2.2.cmml" xref="Ch7.S5.SS4.SSSx3.p1.3.m3.2.2">𝜃</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S5.SS4.SSSx3.p1.3.m3.3c">P(y|x;\theta)</annotation><annotation encoding="application/x-llamapun" id="Ch7.S5.SS4.SSSx3.p1.3.m3.3d">italic_P ( italic_y | italic_x ; italic_θ )</annotation></semantics></math>. However, in practice, it is often difficult to find the translation with the highest probability due to the impractically large search space. Accordingly, to find a good but not necessarily the very ‘best’ (i.e. that with the highest probability given the model) translation, NMT usually relies instead on local search algorithms such as greedy search or beam search (cf. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F6" title="Figure 7.6 ‣ Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a>). Translations are carried out by default using beam search, although the option exists to switch to greedy search if needed. This approach is consistent with many other NMT tools since beam search is a classic local search algorithm. Using a pre-defined beam width parameter K, the beam search algorithm keeps only the top-K possible translations as potential candidates. With each iteration, a new potential translation is formed by combining each candidate word with a new word. New candidate translations compete with each other using log probability values to obtain the new top-K most probable results. This process is continued until the end of the translation process, and the 1-best translation is output.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch7.S5.SS5">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.5 </span>Subword Models</h4>
<div class="ltx_para" id="Ch7.S5.SS5.p1">
<p class="ltx_p" id="Ch7.S5.SS5.p1.1">Translation by its very nature requires an open vocabulary, but restricted (e.g. 30k, 50k, or 70k) vocabularies are typically used for reasons of computational efficiency. However, the use of subword models aims to address this fixed vocabulary problem associated with NMT. The problem manifests itself in how previously unseen ‘out-of-vocabulary’ (OOV) words are handled. In such cases, a single ‘UNK’ (for ‘unknown’) token is used to ‘recognise’ the OOV word. Encoding rare and unknown words into sequences of subword units significantly reduces the problem and has thus given rise to a number of subword algorithms.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS5.p2">
<p class="ltx_p" id="Ch7.S5.SS5.p2.1">Optimally, this will be performed via morphological processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx89" title="">89</a>]</cite>, but good quality wide-coverage morphological analysers are not always available. Therefore it is common practice to use methods such as Byte Pair Encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx41" title="">41</a>]</cite> to break down rare and previously unseen words into subword models in order to significantly improve translation performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx60" title="">60</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS5.p3">
<p class="ltx_p" id="Ch7.S5.SS5.p3.1">Designed for NMT, SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite>, is a language-independent subword tokenizer that provides an open-source C++ and a Python implementation for subword units. An attractive feature of the tokenizer is that SentencePiece trains subword models directly from raw sentences.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S5.SS6">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.6 </span>NMT Tools</h4>
<div class="ltx_para" id="Ch7.S5.SS6.p1">
<p class="ltx_p" id="Ch7.S5.SS6.p1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx59" title="">59</a>]</cite> describe their Joey NMT platform<span class="ltx_note ltx_role_footnote" id="Ch7.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/joeynmt/joeynmt" title="">https://github.com/joeynmt/joeynmt</a></span></span></span> as a minimalist NMT toolkit, based on PyTorch, which is designed especially for newcomers to the field. Joey NMT provides many popular NMT features in a simple code base enabling novice users to easily adapt the system to their particular requirements. The toolkit supports both RNN and Transformer architectures.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS6.p2">
<p class="ltx_p" id="Ch7.S5.SS6.p2.1">Given that adaptNMT is essentially an IPython wrapper layered on top of OpenNMT, it inherits all of OpenNMT’s features and continues to benefit from the work which goes into developing and maintaining its code base. adaptNMT offers a higher level of abstraction over OpenNMT where the focus is much more on usability, especially for newcomers to the field. Accordingly, it provides for easy and rapid deployment by enabling new features such as greater pre-processing, as well as GUI control over model building. It also contains green features in line with the current research drive towards smaller models with lower carbon footprints (cf. Sections <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS4" title="7.7.4 Environmental Impact ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7.4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S8" title="7.8 Discussion ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a>). Such features make adaptNMT suitable for both educational and research environments. The key features differentiating adaptNMT from Joey NMT are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T1" title="Table 7.1 ‣ 7.5.6 NMT Tools ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.1</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch7.T1">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T1.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.1.1.1">adaptNMT is built upon OpenNMT and subsequently inherits all of its features.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.2.2.1">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.2.2.2.1.1">
<tr class="ltx_tr" id="Ch7.T1.2.2.2.1.1.1">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.2.2.1.1.1.1">The interface is designed and fully implemented in Google Colab.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.3.3.1">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.2.3.3.1.1">
<tr class="ltx_tr" id="Ch7.T1.2.3.3.1.1.1">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.3.3.1.1.1.1">Colab is easier to follow for practitioners since each step can be executed individually.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.3.3.1.1.2">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.3.3.1.1.2.1">The approach is ideal in education since the progression of the pipeline is demonstrated.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.4.4.1">Training of models can be viewed and controlled using Colab Android or Apple apps.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.5.5.1">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.2.5.5.1.1">
<tr class="ltx_tr" id="Ch7.T1.2.5.5.1.1.1">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.5.5.1.1.1.1">adaptNMT can be run in local mode enabling existing infrastructure to be utilised or in</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.5.5.1.1.2">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.5.5.1.1.2.1">hosted mode which allows rapid scaling of the infrastructure.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.6.6.1">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.2.6.6.1.1">
<tr class="ltx_tr" id="Ch7.T1.2.6.6.1.1.1">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.6.6.1.1.1.1">Colab Pro+ provides individual researchers, or even small teams, the capacity to build</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.6.6.1.1.2">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.6.6.1.1.2.1">large models on an excellent infrastructure with very little resources.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.7.7.1">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.2.7.7.1.1">
<tr class="ltx_tr" id="Ch7.T1.2.7.7.1.1.1">
<td class="ltx_td ltx_align_left" id="Ch7.T1.2.7.7.1.1.1.1">GUI controls can split a corpus into training, validation and test datasets.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.8.8.1">GUI controls are available for hyperparameter customisation in NMT training.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.9.9.1">A green report outlines the country-specific kgCO<sub class="ltx_sub" id="Ch7.T1.2.9.9.1.1">2</sub> generated when training a model.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.10.10.1">Autonotification notifies the user on the completion of training.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T1.2.11.11.1">A deploy function enables the immediate deployment of trained models.</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.12.12">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch7.T1.2.12.12.1">The functionality of serverNMT is not available within Joey NMT.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T1.3.1.1" style="font-size:90%;">Table 7.1</span>: </span><span class="ltx_text" id="Ch7.T1.4.2" style="font-size:90%;">Key features differentiating adaptNMT from Joey NMT</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S5.SS6.p3">
<p class="ltx_p" id="Ch7.S5.SS6.p3.1">Other popular frameworks for NMT system-building include FAIRSEQ<span class="ltx_note ltx_role_footnote" id="Ch7.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairseq" title="">https://github.com/facebookresearch/fairseq</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx87" title="">87</a>]</cite>, an open-source sequence modelling toolkit based on PyTorch, that enables researchers to train models for translation, summarisation and language modelling. Marian<span class="ltx_note ltx_role_footnote" id="Ch7.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://marian-nmt.github.io" title="">https://marian-nmt.github.io</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx53" title="">53</a>]</cite>, developed using C++, is an NMT framework based on dynamic computation graphs. OpenNMT<span class="ltx_note ltx_role_footnote" id="Ch7.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opennmt.net" title="">https://opennmt.net</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx56" title="">56</a>]</cite> is an open-source NMT framework that has been widely adopted in the research community. The toolkit covers the entire MT workflow from the preparation of data to live inference.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S5.SS7">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.7 </span>Hyperparameter Optimisation</h4>
<div class="ltx_para" id="Ch7.S5.SS7.p1">
<p class="ltx_p" id="Ch7.S5.SS7.p1.1">Hyperparameters are employed to customise machine learning models such as translation models. It has been shown that machine learning performance may be improved through hyperparameter optimisation (HPO) rather than just using default settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx98" title="">98</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S5.SS7.p2">
<p class="ltx_p" id="Ch7.S5.SS7.p2.1">The principal methods of HPO are grid search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx81" title="">81</a>]</cite> and random search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx16" title="">16</a>]</cite>. Grid search is an exhaustive technique which evaluates all parameter permutations. However, as the number of features grows, the amount of data permutations grows exponentially making optimisation expensive in the context of developing translation models which require long build times. Accordingly, an effective, less computationally intensive alternative is to use random search which samples random configurations.</p>
</div>
<figure class="ltx_figure" id="Ch7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch7.F7.g1" src="autoNMT_arch.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F7.2.1.1" style="font-size:90%;">Figure 7.7</span>: </span><span class="ltx_text" id="Ch7.F7.3.2" style="font-size:90%;">Proposed architecture for adaptNMT: a language-agnostic NMT development environment. The system is designed to run either in the cloud or using local infrastructure. Models are trained using parallel corpora. Visualisation and extensive logging enable real-time monitoring. Models are developed using vanilla RNN-based NMT, Transformer-based approaches or transfer learning using a fine-tuning approach. Translation and evaluation can be carried out using either single models or ensembles.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Ch7.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.6 </span>Architecture of adaptNMT</h3>
<div class="ltx_para" id="Ch7.S6.p1">
<p class="ltx_p" id="Ch7.S6.p1.1">Having described the individual components of RNN- and Transformer-based NMT systems, we now present the adaptNMT tool itself, in which these components can be configured by the user. A high-level view of the system architecture of the platform is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F7" title="Figure 7.7 ‣ 7.5.7 Hyperparameter Optimisation ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a>. Developed as an IPython notebook, the application uses the Pytorch implementation of <span class="ltx_text ltx_font_italic" id="Ch7.S6.p1.1.1">OpenNMT</span> for training models with SentencePiece used for training subword models. By using a Jupyter notebook, the application may be easily shared with others in the MT community. Furthermore, the difficulties involved in setting up the correct development environment have largely been removed since all required packages are downloaded on the fly as the application runs.</p>
</div>
<figure class="ltx_figure" id="Ch7.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="Ch7.F7.sf1"><img alt="Refer to caption" class="ltx_graphics" id="Ch7.F7.sf1.g1" src="autoNMT_app.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F7.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="Ch7.F7.sf1.4.2" style="font-size:80%;">Overview of adaptNMT. Key areas include initialisation, pre-processing, environment setup, visualisation, auto and custom NMT, training of subword model, training of main model, evaluation and deployment (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS1" title="7.6.1 adaptNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6.1</span></a>). </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="Ch7.F7.sf2"><img alt="Refer to caption" class="ltx_graphics" id="Ch7.F7.sf2.g1" src="serverNMT_app.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F7.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="Ch7.F7.sf2.4.2" style="font-size:80%;">Overview of serverNMT. Highlighted cells include initialisation, environment setup, Anvil server, API functions, translation, model building, adaptNMT and running the server (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S6.SS2" title="7.6.2 serverNMT ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6.2</span></a>).</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F8.2.1.1" style="font-size:90%;">Figure 7.8</span>: </span><span class="ltx_text" id="Ch7.F8.3.2" style="font-size:90%;">adaptNMT and serverNMT</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S6.p2">
<p class="ltx_p" id="Ch7.S6.p2.1">There are options to run the system on local infrastructure or to run it as a Colab instance using Google Cloud. Translation models are developed using parallel text corpora of the source and target languages. A Tensorboard visualisation provides a real-time graphical view of model training. The primary use cases for the system are model building and a translation service, one or both of which can be selected at run-time. As illustrated in the system diagram in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F7" title="Figure 7.7 ‣ 7.5.7 Hyperparameter Optimisation ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a>, generating an ensemble output while translating has also been facilitated. Models may also be deployed to a pre-configured location.</p>
</div>
<section class="ltx_subsection" id="Ch7.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.1 </span>adaptNMT</h4>
<div class="ltx_para" id="Ch7.S6.SS1.p1">
<p class="ltx_p" id="Ch7.S6.SS1.p1.1">The application may be run as an IPython Jupyter notebook or as a Google Colab application. Given the ease of integrating large Google Drive storage into Colab, the application has been used exclusively as a Google Colab application for our experiments, some of which are described in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7" title="7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a>. The key features of the notebook are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F8" title="Figure 7.8 ‣ 7.6 Architecture of adaptNMT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a>.
</p>
</div>
<section class="ltx_subsubsection" id="Ch7.S6.SS1.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Initialisation and logging</h5>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx1.p1">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx1.p1.1">Initialisation enables connection to Google Drive to run experiments, automatic installation of Python, OpenNMT, SentencePiece, Pytorch and other applications. The visualisation section enables real-time graphing of model development. All log files are stored and can be viewed to inspect training convergence, the model’s training and validation accuracy, changes in learning rates and cross-entropy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch7.S6.SS1.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Modes of operation</h5>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx2.p1">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx2.p1.1">There are two modes of operation: local and cloud. In local mode, the application is run so that models are built using the user’s local GPU resources. The option to use cloud mode enables users to develop models using Google’s GPU clusters. For shorter training times, the unpaid Colab option is adequate. However, for a small monthly subscription, the Google Colab Pro option is worthwhile since users have access to improved GPU and compute resources. Nevertheless, there are also environmental and running costs to consider (cf. Sections <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S7.SS4" title="7.7.4 Environmental Impact ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7.4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S8" title="7.8 Discussion ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a>), although the Google Cloud is run on a platform which uses 100% renewables <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>. It is also a very cost-effective option for those working in the domain of low-resource languages since developing smaller models requires shorter training times. However, users requiring long training times and very high compute resources will need to use their own hardware and run the application in local mode unless they have access to large budgets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch7.S6.SS1.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Customisation of models</h5>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx3.p1">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx3.p1.1">The system has been developed to allow users to select variations to the underlying model architecture. A vanilla RNN or Transformer approach may be selected to develop the NMT model. The customisation mode enables users to specify the exact parameters required for the chosen approach. One of the features, AutoBuild, enables a user to build an NMT model in three simple steps: (i) upload source and target files, (ii) select RNN or Transformer, and (iii) click AutoBuild.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch7.S6.SS1.SSSx4">
<h5 class="ltx_title ltx_title_subsubsection">Use of subword segmentation</h5>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx4.p1">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx4.p1.1">The type of optimiser to be used for learning can be specified, and users may also choose to employ different types of subword models when building the system. The subword model functionality allows the user to choose whether or not to use a subword model. Currently, the user specifies the vocabulary size and chooses either a SentencePiece unigram or a SentencePiece BPE subword model (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.S5.SS5" title="7.5.5 Subword Models ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5.5</span></a>).</p>
</div>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx4.p2">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx4.p2.1">A user may upload a dataset which includes the training, validation and test splits for both source and target languages. In cases where a user has not already created the required splits for model training, single source and target files may be uploaded. The splits needed to create the training, validation and test files are then automatically generated according to the user-specified split ratio. Given that building NMT models typically demands long training times, an automatic notification feature is incorporated that informs the user by email when model training has been completed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch7.S6.SS1.SSSx5">
<h5 class="ltx_title ltx_title_subsubsection">Translation and evaluation</h5>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx5.p1">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx5.p1.1">In addition to supporting the training of models, the application also allows for translation and evaluation of model performance. Translation using pre-built models is also parameterised. Users specify the name of the model as a hyperparameter which is then subsequently used to translate and evaluate the test files. The option for creating an ensemble output is also catered for, and users simply name the models which are to be used in generating the ensemble output.</p>
</div>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx5.p2">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx5.p2.1">Once the system has been built, the model to be used for translating the test set may be selected. To evaluate the quality of translation, humans usually provide the best insight, but they may not always be available, do not always agree, and are expensive to recruit for experiments. Accordingly, automatic evaluation metrics are typically used, especially by developers monitoring the incremental progress of systems (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx115" title="">115</a>]</cite> for more on the pros and cons of human and automatic evaluation).</p>
</div>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx5.p3">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx5.p3.1">Several automatic evaluation metrics provided by SacreBleu<span class="ltx_note ltx_role_footnote" id="Ch7.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mjpost/sacrebleu" title="">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx91" title="">91</a>]</cite> are used: BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite>. Translation quality can also be evaluated using Meteor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx34" title="">34</a>]</cite> and F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx80" title="">80</a>]</cite>. Note that BLEU, ChrF, Meteor and F1 are precision-based metrics, so higher scores are better, whereas TER is an error-based metric and lower scores indicate better translation quality. Evaluation options available include standard (truecase) and lowercase BLEU scores, a sentence-level BLEU score option, ChrF1 and ChrF3.</p>
</div>
<div class="ltx_para" id="Ch7.S6.SS1.SSSx5.p4">
<p class="ltx_p" id="Ch7.S6.SS1.SSSx5.p4.1">There are three levels of logging for model development, training and experimental results. A references section outlines resources which are relevant to developing, using and understanding adaptNMT. Validation during training is currently conducted using model accuracy and perplexity (PPL).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch7.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.2 </span>serverNMT</h4>
<div class="ltx_para" id="Ch7.S6.SS2.p1">
<p class="ltx_p" id="Ch7.S6.SS2.p1.1">A server application, serverNMT, was also developed and implemented as an IPython notebook. It can be configured to run either as a translation server or as a build server. A secure connection, implemented from serverNMT, can be made to websites hosting embedded web apps. At the core of serverNMT, there are two embedded Python web apps, one for translation services and another for developing models, both of which use the anvil.works platform.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://anvil.works" title="">https://anvil.works</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch7.S6.SS2.p2">
<p class="ltx_p" id="Ch7.S6.SS2.p2.1">As a build server, serverNMT enables a window to the underlying cloud infrastructure in which NMT models can be trained. A web app hosted on another system may connect to this infrastructure made available by serverNMT.</p>
</div>
<div class="ltx_para" id="Ch7.S6.SS2.p3">
<p class="ltx_p" id="Ch7.S6.SS2.p3.1">Using an Anvil server embedded within serverNMT, the application continuously waits for communication to web apps and effectively enables a cloud infrastructure for NMT. Written as a REST server, it acts as an API for serving previously built models and facilitates the integration of translation models with other systems.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch7.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.7 </span>Empirical Evaluation</h3>
<div class="ltx_para" id="Ch7.S7.p1">
<p class="ltx_p" id="Ch7.S7.p1.3">Having described the theoretical background and the tool itself, we now evaluate the effectiveness of the adaptNMT approach by training models for English to Irish (EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.p1.1.m1.1"><semantics id="Ch7.S7.p1.1.m1.1a"><mo id="Ch7.S7.p1.1.m1.1.1" stretchy="false" xref="Ch7.S7.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.p1.1.m1.1b"><ci id="Ch7.S7.p1.1.m1.1.1.cmml" xref="Ch7.S7.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.p1.1.m1.1d">→</annotation></semantics></math>GA) and Irish to English (GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.p1.2.m2.1"><semantics id="Ch7.S7.p1.2.m2.1a"><mo id="Ch7.S7.p1.2.m2.1.1" stretchy="false" xref="Ch7.S7.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.p1.2.m2.1b"><ci id="Ch7.S7.p1.2.m2.1.1.cmml" xref="Ch7.S7.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.p1.2.m2.1d">→</annotation></semantics></math>EN) translation in the health domain using the <span class="ltx_text ltx_font_italic" id="Ch7.S7.p1.3.1">gaHealth</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx63" title="">63</a>]</cite> corpus.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seamusl/gaHealth" title="">https://github.com/seamusl/gaHealth</a></span></span></span> All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece subword model. To benchmark the performance of our models, the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch7.S7.p1.3.m3.1"><semantics id="Ch7.S7.p1.3.m3.1a"><mo id="Ch7.S7.p1.3.m3.1.1" stretchy="false" xref="Ch7.S7.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.p1.3.m3.1b"><ci id="Ch7.S7.p1.3.m3.1.1.cmml" xref="Ch7.S7.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.p1.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.p1.3.m3.1d">↔</annotation></semantics></math>GA test datasets from the LoResMT2021 Shared Task<span class="ltx_note ltx_role_footnote" id="Ch7.footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/loresmt/loresmt-2021" title="">https://github.com/loresmt/loresmt-2021</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite> were used. These test datasets enabled the evaluation of the <span class="ltx_text ltx_font_italic" id="Ch7.S7.p1.3.2">gaHealth</span> models since the shared task focused on an application of the health domain, namely the translation of Covid-related data. Furthermore, using an official test dataset from a shared task enables the direct comparison of our models’ performance with models entered by other teams, as well as future implementations.</p>
</div>
<div class="ltx_para" id="Ch7.S7.p2">
<p class="ltx_p" id="Ch7.S7.p2.2">The hyperparameters used for developing the models are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T2" title="Table 7.2 ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.2</span></a>. The details of the training, validation and test sets used by our NMT models are outlined in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T3" title="Table 7.3 ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T4" title="Table 7.4 ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.4</span></a>. In all cases, 502 lines were used from the LoResMT2021 validation dataset whereas the test dataset used 502 lines for EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.p2.1.m1.1"><semantics id="Ch7.S7.p2.1.m1.1a"><mo id="Ch7.S7.p2.1.m1.1.1" stretchy="false" xref="Ch7.S7.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.p2.1.m1.1b"><ci id="Ch7.S7.p2.1.m1.1.1.cmml" xref="Ch7.S7.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.p2.1.m1.1d">→</annotation></semantics></math>GA translation and 250 lines for GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.p2.2.m2.1"><semantics id="Ch7.S7.p2.2.m2.1a"><mo id="Ch7.S7.p2.2.m2.1.1" stretchy="false" xref="Ch7.S7.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.p2.2.m2.1b"><ci id="Ch7.S7.p2.2.m2.1.1.cmml" xref="Ch7.S7.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.p2.2.m2.1d">→</annotation></semantics></math>EN translation. Both were independent health-specific Covid test sets which were provided by LoResMT2021. There was one exception; due to a data overlap between the test and train datasets, a reduced test set was used when testing the <span class="ltx_text ltx_font_italic" id="Ch7.S7.p2.2.1">gaHealth</span> en2ga* system.</p>
</div>
<div class="ltx_para" id="Ch7.S7.p3">
<p class="ltx_p" id="Ch7.S7.p3.1">The results from the IIITT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx92" title="">92</a>]</cite> and UCF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx27" title="">27</a>]</cite> teams are included in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T5" title="Table 7.5 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T6" title="Table 7.6 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a> so the performance of the <span class="ltx_text ltx_font_italic" id="Ch7.S7.p3.1.1">gaHealth</span> models can be easily compared with the findings of the participating LoResMT2021 systems. IIITT fine-tuned an Opus MT model<span class="ltx_note ltx_role_footnote" id="Ch7.footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Helsinki-NLP/Opus-MT" title="">https://github.com/Helsinki-NLP/Opus-MT</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx112" title="">112</a>]</cite> on the training dataset. UCF used transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx121" title="">121</a>]</cite>, unigram and subword segmentation methods for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch7.S7.p3.1.m1.1"><semantics id="Ch7.S7.p3.1.m1.1a"><mo id="Ch7.S7.p3.1.m1.1.1" stretchy="false" xref="Ch7.S7.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.p3.1.m1.1b"><ci id="Ch7.S7.p3.1.m1.1.1.cmml" xref="Ch7.S7.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.p3.1.m1.1d">↔</annotation></semantics></math>GA translation.</p>
</div>
<figure class="ltx_table ltx_align_center" id="Ch7.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T2.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch7.T2.2.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch7.T2.2.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.2.2.1">Learning rate</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.2.2.2">0.1, 0.01, 0.001, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.2.2.2.1">2</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.3.3.1">Batch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.3.3.2">1024, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.3.3.2.1">2048</span>, 4096, 8192</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.4.4.1">Attention heads</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.4.4.2">
<span class="ltx_text ltx_font_bold" id="Ch7.T2.2.4.4.2.1">2</span>, 4, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.4.4.2.2">8</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.5.5.1">Number of layers</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.5.5.2">5, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.5.5.2.1">6</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.6.6.1">Feed-forward dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.6.6.2"><span class="ltx_text ltx_font_bold" id="Ch7.T2.2.6.6.2.1">2048</span></td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.7.7.1">Embedding dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.7.7.2">128, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.7.7.2.1">256</span>, 512</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.8.8.1">Label smoothing</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.8.8.2">
<span class="ltx_text ltx_font_bold" id="Ch7.T2.2.8.8.2.1">0.1</span>, 0.3</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.9.9.1">Dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.9.9.2">0.1, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.9.9.2.1">0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T2.2.10.10.1">Attention dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T2.2.10.10.2"><span class="ltx_text ltx_font_bold" id="Ch7.T2.2.10.10.2.1">0.1</span></td>
</tr>
<tr class="ltx_tr" id="Ch7.T2.2.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch7.T2.2.11.11.1">Average Decay</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch7.T2.2.11.11.2">0, <span class="ltx_text ltx_font_bold" id="Ch7.T2.2.11.11.2.1">0.0001</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T2.3.1.1" style="font-size:90%;">Table 7.2</span>: </span><span class="ltx_text" id="Ch7.T2.4.2" style="font-size:90%;">Hyperparameter optimisation for Transformer models. Optimal parameters are highlighted in bold. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch7.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch7.T3.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch7.T3.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch7.T3.4.1.1.1.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T3.4.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch7.T3.4.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T3.4.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch7.T3.4.1.1.3.1">Train</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T3.4.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch7.T3.4.1.1.4.1">Validation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T3.4.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch7.T3.4.1.1.5.1">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T3.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T3.4.2.1.1">adapt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T3.4.2.1.2">covid_extended</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T3.4.2.1.3">13k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T3.4.2.1.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T3.4.2.1.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch7.T3.4.3.2">
<td class="ltx_td ltx_align_left" id="Ch7.T3.4.3.2.1">adapt</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.3.2.2">combined_domains</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.3.2.3">65k</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.3.2.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.3.2.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch7.T3.4.4.3">
<td class="ltx_td ltx_align_left" id="Ch7.T3.4.4.3.1">IIITT</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.4.3.2">en2ga-b</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.4.3.3">8k</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.4.3.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.4.3.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch7.T3.4.5.4">
<td class="ltx_td ltx_align_left" id="Ch7.T3.4.5.4.1">UCF</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.5.4.2">en2ga-a</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.5.4.3">8k</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.5.4.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.5.4.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch7.T3.4.6.5">
<td class="ltx_td ltx_align_left" id="Ch7.T3.4.6.5.1"><span class="ltx_text ltx_font_italic" id="Ch7.T3.4.6.5.1.1">gaHealth</span></td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.6.5.2">en2ga</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.6.5.3">24k</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.6.5.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch7.T3.4.6.5.5">500</td>
</tr>
<tr class="ltx_tr" id="Ch7.T3.4.7.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch7.T3.4.7.6.1"><span class="ltx_text ltx_font_italic" id="Ch7.T3.4.7.6.1.1">gaHealth</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T3.4.7.6.2">en2ga*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T3.4.7.6.3">24k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T3.4.7.6.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T3.4.7.6.5">338</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T3.5.2.1" style="font-size:90%;">Table 7.3</span>: </span><span class="ltx_text" id="Ch7.T3.2.1" style="font-size:90%;">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T3.2.1.m1.1"><semantics id="Ch7.T3.2.1.m1.1b"><mo id="Ch7.T3.2.1.m1.1.1" stretchy="false" xref="Ch7.T3.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T3.2.1.m1.1c"><ci id="Ch7.T3.2.1.m1.1.1.cmml" xref="Ch7.T3.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T3.2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T3.2.1.m1.1e">→</annotation></semantics></math>GA training, validation and test dataset distributions. The baseline <span class="ltx_text ltx_font_italic" id="Ch7.T3.2.1.1">gaHealth</span> system was augmented with an 8k Covid dataset provided by LoResMT2021.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch7.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch7.T4.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch7.T4.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch7.T4.4.1.1.1.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T4.4.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch7.T4.4.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T4.4.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch7.T4.4.1.1.3.1">Train</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T4.4.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch7.T4.4.1.1.4.1">Validation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T4.4.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch7.T4.4.1.1.5.1">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T4.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch7.T4.4.2.1.1">IIITT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T4.4.2.1.2">ga2en-b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T4.4.2.1.3">8k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T4.4.2.1.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T4.4.2.1.5">250</td>
</tr>
<tr class="ltx_tr" id="Ch7.T4.4.3.2">
<td class="ltx_td ltx_align_left" id="Ch7.T4.4.3.2.1">UCF</td>
<td class="ltx_td ltx_align_center" id="Ch7.T4.4.3.2.2">ga2en-b</td>
<td class="ltx_td ltx_align_center" id="Ch7.T4.4.3.2.3">8k</td>
<td class="ltx_td ltx_align_center" id="Ch7.T4.4.3.2.4">502</td>
<td class="ltx_td ltx_align_center" id="Ch7.T4.4.3.2.5">250</td>
</tr>
<tr class="ltx_tr" id="Ch7.T4.4.4.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch7.T4.4.4.3.1"><span class="ltx_text ltx_font_italic" id="Ch7.T4.4.4.3.1.1">gaHealth</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T4.4.4.3.2">ga2en</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T4.4.4.3.3">24k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T4.4.4.3.4">502</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T4.4.4.3.5">250</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T4.5.2.1" style="font-size:90%;">Table 7.4</span>: </span><span class="ltx_text" id="Ch7.T4.2.1" style="font-size:90%;">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T4.2.1.m1.1"><semantics id="Ch7.T4.2.1.m1.1b"><mo id="Ch7.T4.2.1.m1.1.1" stretchy="false" xref="Ch7.T4.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T4.2.1.m1.1c"><ci id="Ch7.T4.2.1.m1.1.1.cmml" xref="Ch7.T4.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T4.2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T4.2.1.m1.1e">→</annotation></semantics></math>EN training, validation and test dataset distributions. The baseline <span class="ltx_text ltx_font_italic" id="Ch7.T4.2.1.1">gaHealth</span> system was augmented with an 8k Covid dataset provided by LoResMT2021. All overlaps were removed from the <span class="ltx_text ltx_font_italic" id="Ch7.T4.2.1.2">gaHealth</span> corpus prior to training the <span class="ltx_text ltx_font_italic" id="Ch7.T4.2.1.3">gaHealth</span> ga2en model.</span></figcaption>
</figure>
<section class="ltx_subsection" id="Ch7.S7.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7.1 </span>Infrastructure</h4>
<div class="ltx_para" id="Ch7.S7.SS1.p1">
<p class="ltx_p" id="Ch7.S7.SS1.p1.1">Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16GB graphic cards and up to 27GB of memory when available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx19" title="">19</a>]</cite>. All <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS1.p1.1.1">gaHealth</span> MT models were trained using adaptNMT.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S7.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7.2 </span>Metrics</h4>
<div class="ltx_para" id="Ch7.S7.SS2.p1">
<p class="ltx_p" id="Ch7.S7.SS2.p1.1">Automated metrics were used to determine the translation quality. To compare against our previous work, the performance of models is measured using three evaluation metrics, namely BLEU, TER and ChrF. These metrics indicate the accuracy of the translations derived from our NMT systems.</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS2.p2">
<p class="ltx_p" id="Ch7.S7.SS2.p2.1">Case-insensitive BLEU scores at the corpus level are reported. Model training was stopped after 40k training steps or once an early stopping criterion of no improvement in validation accuracy for four consecutive iterations was recorded.</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS2.p3">
<p class="ltx_p" id="Ch7.S7.SS2.p3.1">PPL is often used to evaluate language models within NLP. It measures the effectiveness of a probability model in predicting a sample. As a metric for translation performance, it is important to keep low scores so that the number of alternative translations is reduced.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S7.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7.3 </span>Results: Automatic Evaluation</h4>
<div class="ltx_para" id="Ch7.S7.SS3.p1">
<p class="ltx_p" id="Ch7.S7.SS3.p1.2">The experimental results from LoResMT 2021 are summarised in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T5" title="Table 7.5 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T6" title="Table 7.6 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a>. In the LoResMT2021 Shared Task, the highest-performing EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS3.p1.1.m1.1"><semantics id="Ch7.S7.SS3.p1.1.m1.1a"><mo id="Ch7.S7.SS3.p1.1.m1.1.1" stretchy="false" xref="Ch7.S7.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS3.p1.1.m1.1b"><ci id="Ch7.S7.SS3.p1.1.m1.1.1.cmml" xref="Ch7.S7.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS3.p1.1.m1.1d">→</annotation></semantics></math>GA system was submitted by the ADAPT team <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>]</cite>. The system uses an extended Covid dataset, which is a combination of the 2021 MT Summit Covid baseline and a custom ADAPT Covid dataset. The model, developed within adaptNMT, uses a Transformer architecture with 2 heads. It performs well across all key translation metrics (BLEU: 36.0, TER: 0.531 and ChrF3: 0.6). The training of this EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS3.p1.2.m2.1"><semantics id="Ch7.S7.SS3.p1.2.m2.1a"><mo id="Ch7.S7.SS3.p1.2.m2.1.1" stretchy="false" xref="Ch7.S7.SS3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS3.p1.2.m2.1b"><ci id="Ch7.S7.SS3.p1.2.m2.1.1.cmml" xref="Ch7.S7.SS3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS3.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS3.p1.2.m2.1d">→</annotation></semantics></math>GA model is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F9" title="Figure 7.9 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.9</span></a>. The model achieved a maximum validation accuracy of 30.0% and perplexity of 354 after 30k steps.</p>
</div>
<figure class="ltx_figure" id="Ch7.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch7.F9.g1" src="en-ga_acc_covid.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch7.F9.g2" src="en-ga_ppl_covid.png"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F9.3.2.1" style="font-size:90%;">Figure 7.9</span>: </span><span class="ltx_text" id="Ch7.F9.1.1" style="font-size:90%;">adapt covid_extended system: training EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.F9.1.1.m1.1"><semantics id="Ch7.F9.1.1.m1.1b"><mo id="Ch7.F9.1.1.m1.1.1" stretchy="false" xref="Ch7.F9.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.F9.1.1.m1.1c"><ci id="Ch7.F9.1.1.m1.1.1.cmml" xref="Ch7.F9.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F9.1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.F9.1.1.m1.1e">→</annotation></semantics></math>GA model with 13k lines consisting of the ADAPT 5k corpus and an 8k LoResMT2021 Covid corpus. The graph on the left illustrates accuracy and the graph on the right demonstrates perplexity.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch7.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch7.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch7.T5.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch7.T5.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T5.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch7.T5.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T5.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch7.T5.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T5.1.1.1.m1.1"><semantics id="Ch7.T5.1.1.1.m1.1a"><mo id="Ch7.T5.1.1.1.m1.1.1" stretchy="false" xref="Ch7.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T5.1.1.1.m1.1b"><ci id="Ch7.T5.1.1.1.m1.1.1.cmml" xref="Ch7.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T5.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T5.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch7.T5.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch7.T5.2.2.2.m1.1"><semantics id="Ch7.T5.2.2.2.m1.1a"><mo id="Ch7.T5.2.2.2.m1.1.1" stretchy="false" xref="Ch7.T5.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch7.T5.2.2.2.m1.1b"><ci id="Ch7.T5.2.2.2.m1.1.1.cmml" xref="Ch7.T5.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T5.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T5.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T5.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch7.T5.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T5.3.3.3.m1.1"><semantics id="Ch7.T5.3.3.3.m1.1a"><mo id="Ch7.T5.3.3.3.m1.1.1" stretchy="false" xref="Ch7.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T5.3.3.3.m1.1b"><ci id="Ch7.T5.3.3.3.m1.1.1.cmml" xref="Ch7.T5.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T5.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T5.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T5.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T5.3.4.1.1">UCF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T5.3.4.1.2">en2ga-b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T5.3.4.1.3">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T5.3.4.1.4">0.756</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T5.3.4.1.5">0.37</td>
</tr>
<tr class="ltx_tr" id="Ch7.T5.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch7.T5.3.5.2.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.5.2.2">en2ga-b</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.5.2.3">25.8</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.5.2.4">0.629</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.5.2.5">0.53</td>
</tr>
<tr class="ltx_tr" id="Ch7.T5.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch7.T5.3.6.3.1">adapt</th>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.6.3.2">combined</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.6.3.3">32.8</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.6.3.4">0.590</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.6.3.5">0.57</td>
</tr>
<tr class="ltx_tr" id="Ch7.T5.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch7.T5.3.7.4.1"><span class="ltx_text ltx_font_italic" id="Ch7.T5.3.7.4.1.1">gaHealth</span></th>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.7.4.2">en2ga</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.7.4.3">33.3</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.7.4.4">0.604</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.7.4.5">0.56</td>
</tr>
<tr class="ltx_tr" id="Ch7.T5.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch7.T5.3.8.5.1">adapt</th>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.8.5.2">covid_extended</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.8.5.3">36.0</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.8.5.4">0.531</td>
<td class="ltx_td ltx_align_center" id="Ch7.T5.3.8.5.5">0.60</td>
</tr>
<tr class="ltx_tr" id="Ch7.T5.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch7.T5.3.9.6.1"><span class="ltx_text ltx_font_italic" id="Ch7.T5.3.9.6.1.1">gaHealth</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T5.3.9.6.2">en2ga*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T5.3.9.6.3"><span class="ltx_text ltx_font_bold" id="Ch7.T5.3.9.6.3.1">37.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T5.3.9.6.4">0.577</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T5.3.9.6.5">0.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T5.9.3.1" style="font-size:90%;">Table 7.5</span>: </span><span class="ltx_text" id="Ch7.T5.6.2" style="font-size:90%;">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T5.5.1.m1.1"><semantics id="Ch7.T5.5.1.m1.1b"><mo id="Ch7.T5.5.1.m1.1.1" stretchy="false" xref="Ch7.T5.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T5.5.1.m1.1c"><ci id="Ch7.T5.5.1.m1.1.1.cmml" xref="Ch7.T5.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T5.5.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T5.5.1.m1.1e">→</annotation></semantics></math>GA system compared with LoResMT 2021 EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T5.6.2.m2.1"><semantics id="Ch7.T5.6.2.m2.1b"><mo id="Ch7.T5.6.2.m2.1.1" stretchy="false" xref="Ch7.T5.6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T5.6.2.m2.1c"><ci id="Ch7.T5.6.2.m2.1.1.cmml" xref="Ch7.T5.6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T5.6.2.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T5.6.2.m2.1e">→</annotation></semantics></math>GA systems.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S7.SS3.p2">
<p class="ltx_p" id="Ch7.S7.SS3.p2.1">The results from the LoResMT2021 Shared Task were further improved by developing models using a bespoke health dataset, <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p2.1.1">gaHealth</span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T5" title="Table 7.5 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a> shows an improvement of 1.6 BLEU points, a relative improvement of almost 4.5%, although TER and ChrF3 scores are a little worse. Validation accuracy and PPL in training the <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p2.1.2">gaHealth</span> models with adaptNMT are illustrated in Figures <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F10" title="Figure 7.10 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.10</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F11" title="Figure 7.11 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.11</span></a>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F9" title="Figure 7.9 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.9</span></a> illustrates model training using the covid_extended dataset, also developed using adaptNMT. In training the <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p2.1.3">gaHealth</span> en2ga* system, as highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F10" title="Figure 7.10 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.10</span></a>, the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS3.p2.1.m1.1"><semantics id="Ch7.S7.SS3.p2.1.m1.1a"><mo id="Ch7.S7.SS3.p2.1.m1.1.1" stretchy="false" xref="Ch7.S7.SS3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS3.p2.1.m1.1b"><ci id="Ch7.S7.SS3.p2.1.m1.1.1.cmml" xref="Ch7.S7.SS3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS3.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS3.p2.1.m1.1d">→</annotation></semantics></math>GA model was trained with the combined 16k <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p2.1.4">gaHealth</span> and 8k LoResMT2021 corpora. The model’s validation accuracy of 38.5% and perplexity of 113 achieved a BLEU score of 37.6 when evaluated with the test data.</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS3.p3">
<p class="ltx_p" id="Ch7.S7.SS3.p3.1">The training of the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS3.p3.1.m1.1"><semantics id="Ch7.S7.SS3.p3.1.m1.1a"><mo id="Ch7.S7.SS3.p3.1.m1.1.1" stretchy="false" xref="Ch7.S7.SS3.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS3.p3.1.m1.1b"><ci id="Ch7.S7.SS3.p3.1.m1.1.1.cmml" xref="Ch7.S7.SS3.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS3.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS3.p3.1.m1.1d">→</annotation></semantics></math>EN <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p3.1.1">gaHealth</span> ga2en system with the combined 16k gaHealth corpus and 8k LoResMT2021 Covid corpus is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F11" title="Figure 7.11 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.11</span></a>. This model achieves a validation accuracy of 39.5% and perplexity of 116 which results in a BLEU score of 57.6. This is significantly better (by 20 BLEU points) than for the reverse direction, as it is well-known that translating into a morphological-rich language like Irish is always more difficult compared to when the same language acts as the source. This is confirmed by comparing the results for the UCF (13.5 vs. 21.3 BLEU) and IIITT (25.8 vs. 34.6) systems in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T5" title="Table 7.5 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T6" title="Table 7.6 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.6</span></a>.
</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS3.p4">
<p class="ltx_p" id="Ch7.S7.SS3.p4.1">Rapid convergence was observed while training the <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p4.1.1">gaHealth</span> models such that little accuracy improvement occurs after 30k steps, 10K fewer than for the reverse direction. Only marginal gains were achieved after this point and it declined in the case of the system trained using the covid_extended dataset, as the left-hand graph in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.F9" title="Figure 7.9 ‣ 7.7.3 Results: Automatic Evaluation ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.9</span></a> shows.</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS3.p5">
<p class="ltx_p" id="Ch7.S7.SS3.p5.1">Of the models developed by the ADAPT team, the worst-performing model uses a larger 65k dataset. This is not surprising given that the dataset is from a generic domain of which only 20% is health-related. The performance of this higher-resourced 65k line model lags behind the augmented <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p5.1.1">gaHealth</span> model which was developed using just 24k lines.</p>
</div>
<figure class="ltx_table" id="Ch7.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch7.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch7.T6.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch7.T6.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T6.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch7.T6.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T6.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch7.T6.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T6.1.1.1.m1.1"><semantics id="Ch7.T6.1.1.1.m1.1a"><mo id="Ch7.T6.1.1.1.m1.1.1" stretchy="false" xref="Ch7.T6.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T6.1.1.1.m1.1b"><ci id="Ch7.T6.1.1.1.m1.1.1.cmml" xref="Ch7.T6.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T6.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T6.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T6.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch7.T6.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch7.T6.2.2.2.m1.1"><semantics id="Ch7.T6.2.2.2.m1.1a"><mo id="Ch7.T6.2.2.2.m1.1.1" stretchy="false" xref="Ch7.T6.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch7.T6.2.2.2.m1.1b"><ci id="Ch7.T6.2.2.2.m1.1.1.cmml" xref="Ch7.T6.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T6.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T6.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T6.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch7.T6.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T6.3.3.3.m1.1"><semantics id="Ch7.T6.3.3.3.m1.1a"><mo id="Ch7.T6.3.3.3.m1.1.1" stretchy="false" xref="Ch7.T6.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T6.3.3.3.m1.1b"><ci id="Ch7.T6.3.3.3.m1.1.1.cmml" xref="Ch7.T6.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T6.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T6.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T6.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T6.3.4.1.1">UCF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T6.3.4.1.2">ga2en-b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T6.3.4.1.3">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T6.3.4.1.4">0.711</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T6.3.4.1.5">0.45</td>
</tr>
<tr class="ltx_tr" id="Ch7.T6.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch7.T6.3.5.2.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch7.T6.3.5.2.2">ga2en-b</td>
<td class="ltx_td ltx_align_center" id="Ch7.T6.3.5.2.3">34.6</td>
<td class="ltx_td ltx_align_center" id="Ch7.T6.3.5.2.4">0.586</td>
<td class="ltx_td ltx_align_center" id="Ch7.T6.3.5.2.5">0.61</td>
</tr>
<tr class="ltx_tr" id="Ch7.T6.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch7.T6.3.6.3.1"><span class="ltx_text ltx_font_italic" id="Ch7.T6.3.6.3.1.1">gaHealth</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T6.3.6.3.2">ga2en</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T6.3.6.3.3"><span class="ltx_text ltx_font_bold" id="Ch7.T6.3.6.3.3.1">57.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T6.3.6.3.4">0.385</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T6.3.6.3.5">0.71</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T6.9.3.1" style="font-size:90%;">Table 7.6</span>: </span><span class="ltx_text" id="Ch7.T6.6.2" style="font-size:90%;">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T6.5.1.m1.1"><semantics id="Ch7.T6.5.1.m1.1b"><mo id="Ch7.T6.5.1.m1.1.1" stretchy="false" xref="Ch7.T6.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T6.5.1.m1.1c"><ci id="Ch7.T6.5.1.m1.1.1.cmml" xref="Ch7.T6.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T6.5.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T6.5.1.m1.1e">→</annotation></semantics></math>EN <span class="ltx_text ltx_font_italic" id="Ch7.T6.6.2.1">gaHealth</span> systems compared with LoResMT2021 GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T6.6.2.m2.1"><semantics id="Ch7.T6.6.2.m2.1b"><mo id="Ch7.T6.6.2.m2.1.1" stretchy="false" xref="Ch7.T6.6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T6.6.2.m2.1c"><ci id="Ch7.T6.6.2.m2.1.1.cmml" xref="Ch7.T6.6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T6.6.2.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T6.6.2.m2.1e">→</annotation></semantics></math>EN systems</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S7.SS3.p6">
<p class="ltx_p" id="Ch7.S7.SS3.p6.2">For GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS3.p6.1.m1.1"><semantics id="Ch7.S7.SS3.p6.1.m1.1a"><mo id="Ch7.S7.SS3.p6.1.m1.1.1" stretchy="false" xref="Ch7.S7.SS3.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS3.p6.1.m1.1b"><ci id="Ch7.S7.SS3.p6.1.m1.1.1.cmml" xref="Ch7.S7.SS3.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS3.p6.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS3.p6.1.m1.1d">→</annotation></semantics></math>EN translation, the best-performing model for the LoResMT2021 Shared Task was developed by IIITT with a BLEU of 34.6, a TER of 0.586 and ChrF3 of 0.6. Accordingly, this serves as the baseline score by which our GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS3.p6.2.m2.1"><semantics id="Ch7.S7.SS3.p6.2.m2.1a"><mo id="Ch7.S7.SS3.p6.2.m2.1.1" stretchy="false" xref="Ch7.S7.SS3.p6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS3.p6.2.m2.1b"><ci id="Ch7.S7.SS3.p6.2.m2.1.1.cmml" xref="Ch7.S7.SS3.p6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS3.p6.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS3.p6.2.m2.1d">→</annotation></semantics></math>EN model, developed using the <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p6.2.1">gaHealth</span> corpus, can be benchmarked. The performance of the <span class="ltx_text ltx_font_italic" id="Ch7.S7.SS3.p6.2.2">gaHealth</span> model offers an improvement across all metrics with a BLEU score of 57.6, a TER of 0.385 and a ChrF3 result of 0.71. In particular, the 66% relative improvement in BLEU score against the IIITT system is very significant.</p>
</div>
<figure class="ltx_figure" id="Ch7.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch7.F10.g1" src="en-ga_acc.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch7.F10.g2" src="en-ga_ppl.png"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F10.5.2.1" style="font-size:90%;">Figure 7.10</span>: </span><span class="ltx_text ltx_font_italic" id="Ch7.F10.2.1" style="font-size:90%;">gaHealth<span class="ltx_text ltx_font_upright" id="Ch7.F10.2.1.1"> en2ga* system: training EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.F10.2.1.1.m1.1"><semantics id="Ch7.F10.2.1.1.m1.1b"><mo id="Ch7.F10.2.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch7.F10.2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.F10.2.1.1.m1.1c"><ci id="Ch7.F10.2.1.1.m1.1.1.cmml" xref="Ch7.F10.2.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F10.2.1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.F10.2.1.1.m1.1e">→</annotation></semantics></math>GA model with combined 16k gaHealth corpus and 8k LoResMT2021 Covid corpus. The graph on the left illustrates OpenNMT accuracy and the graph on the right demonstrates perplexity.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch7.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch7.F11.g1" src="ga-en_acc.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2" id="Ch7.F11.g2" src="ga-en_ppl.png"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F11.5.2.1" style="font-size:90%;">Figure 7.11</span>: </span><span class="ltx_text ltx_font_italic" id="Ch7.F11.2.1" style="font-size:90%;">gaHealth<span class="ltx_text ltx_font_upright" id="Ch7.F11.2.1.1"> ga2en system: training GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.F11.2.1.1.m1.1"><semantics id="Ch7.F11.2.1.1.m1.1b"><mo id="Ch7.F11.2.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch7.F11.2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.F11.2.1.1.m1.1c"><ci id="Ch7.F11.2.1.1.m1.1.1.cmml" xref="Ch7.F11.2.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F11.2.1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.F11.2.1.1.m1.1e">→</annotation></semantics></math>EN model with combined 16k gaHealth corpus and 8k LoResMT2021 Covid corpus. The graph on the left illustrates OpenNMT accuracy and the graph on the right demonstrates perplexity.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch7.S7.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7.4 </span>Environmental Impact</h4>
<div class="ltx_para" id="Ch7.S7.SS4.p1">
<p class="ltx_p" id="Ch7.S7.SS4.p1.1">We were motivated by the findings of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx108" title="">108</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx15" title="">15</a>]</cite> to track the energy consumption required to train our models. Prototype model development used Colab Pro, which as part of Google Cloud is carbon neutral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>. However, longer running Transformer experiments were conducted on local servers using 324 gCO<sub class="ltx_sub" id="Ch7.S7.SS4.p1.1.1">2</sub> per kWh<span class="ltx_note ltx_role_footnote" id="Ch7.footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf" title="">https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx100" title="">100</a>]</cite>. The net result was just under 10 kgCO<sub class="ltx_sub" id="Ch7.S7.SS4.p1.1.2">2</sub> created for a full run of model development. Models developed during this study will be reused for ensemble experiments in the future so that work will have a life beyond this paper.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S7.SS5">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7.5 </span>Stochastic Nuances</h4>
<div class="ltx_para" id="Ch7.S7.SS5.p1">
<p class="ltx_p" id="Ch7.S7.SS5.p1.1">To evaluate the translation performance of an IPython-based application such as adaptNMT, a comparison with a Python script version of the same application, myNMT.py, was conducted. We built EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS5.p1.1.m1.1"><semantics id="Ch7.S7.SS5.p1.1.m1.1a"><mo id="Ch7.S7.SS5.p1.1.m1.1.1" stretchy="false" xref="Ch7.S7.SS5.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS5.p1.1.m1.1b"><ci id="Ch7.S7.SS5.p1.1.m1.1.1.cmml" xref="Ch7.S7.SS5.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS5.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS5.p1.1.m1.1d">↔</annotation></semantics></math>GA translation models using this script. The models developed with adaptNMT were trained on Google Colab using a 12GB Tesla K80 GPU, whereas the myNMT models were trained on a local machine using a 12GB Gigabyte 3060 graphics card. The results from evaluating these models are presented in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T7" title="Table 7.7 ‣ 7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T8" title="Table 7.8 ‣ 7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS5.p2">
<p class="ltx_p" id="Ch7.S7.SS5.p2.2">Despite setting the same random seed, it is clear from Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T7" title="Table 7.7 ‣ 7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.T8" title="Table 7.8 ‣ 7.7.5 Stochastic Nuances ‣ 7.7 Empirical Evaluation ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a> that the translation performance of the adaptNMT models is better by 1.2 BLEU points (3.3% relative improvement) in EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS5.p2.1.m1.1"><semantics id="Ch7.S7.SS5.p2.1.m1.1a"><mo id="Ch7.S7.SS5.p2.1.m1.1.1" stretchy="false" xref="Ch7.S7.SS5.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS5.p2.1.m1.1b"><ci id="Ch7.S7.SS5.p2.1.m1.1.1.cmml" xref="Ch7.S7.SS5.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS5.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS5.p2.1.m1.1d">→</annotation></semantics></math>GA translation and 1.0 BLEU point (1.8% relative improvement) in GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S7.SS5.p2.2.m2.1"><semantics id="Ch7.S7.SS5.p2.2.m2.1a"><mo id="Ch7.S7.SS5.p2.2.m2.1.1" stretchy="false" xref="Ch7.S7.SS5.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S7.SS5.p2.2.m2.1b"><ci id="Ch7.S7.SS5.p2.2.m2.1.1.cmml" xref="Ch7.S7.SS5.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S7.SS5.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S7.SS5.p2.2.m2.1d">→</annotation></semantics></math>EN translation.</p>
</div>
<div class="ltx_para" id="Ch7.S7.SS5.p3">
<p class="ltx_p" id="Ch7.S7.SS5.p3.1">Given the stochastic nature of machine learning, training models on different systems can yield different results even with the same training, validation and test data. The performance differences can be attributed to the stochastic nature of the learning algorithm and evaluation procedure. Furthermore, the platforms had different underlying system architectures which is another source of stochastic error.</p>
</div>
<figure class="ltx_table" id="Ch7.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T7.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch7.T7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch7.T7.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch7.T7.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T7.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch7.T7.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T7.1.1.1.m1.1"><semantics id="Ch7.T7.1.1.1.m1.1a"><mo id="Ch7.T7.1.1.1.m1.1.1" stretchy="false" xref="Ch7.T7.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T7.1.1.1.m1.1b"><ci id="Ch7.T7.1.1.1.m1.1.1.cmml" xref="Ch7.T7.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T7.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T7.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T7.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch7.T7.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch7.T7.2.2.2.m1.1"><semantics id="Ch7.T7.2.2.2.m1.1a"><mo id="Ch7.T7.2.2.2.m1.1.1" stretchy="false" xref="Ch7.T7.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch7.T7.2.2.2.m1.1b"><ci id="Ch7.T7.2.2.2.m1.1.1.cmml" xref="Ch7.T7.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T7.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T7.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T7.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch7.T7.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T7.3.3.3.m1.1"><semantics id="Ch7.T7.3.3.3.m1.1a"><mo id="Ch7.T7.3.3.3.m1.1.1" stretchy="false" xref="Ch7.T7.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T7.3.3.3.m1.1b"><ci id="Ch7.T7.3.3.3.m1.1.1.cmml" xref="Ch7.T7.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T7.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T7.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T7.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T7.3.4.1.1">adaptNMT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T7.3.4.1.2">37.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T7.3.4.1.3">0.577</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T7.3.4.1.4">0.570</td>
</tr>
<tr class="ltx_tr" id="Ch7.T7.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch7.T7.3.5.2.1">myNMT</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T7.3.5.2.2">36.4</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T7.3.5.2.3">0.622</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T7.3.5.2.4">0.56</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T7.7.2.1" style="font-size:90%;">Table 7.7</span>: </span><span class="ltx_text" id="Ch7.T7.5.1" style="font-size:90%;">Stochastic differences between EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T7.5.1.m1.1"><semantics id="Ch7.T7.5.1.m1.1b"><mo id="Ch7.T7.5.1.m1.1.1" stretchy="false" xref="Ch7.T7.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T7.5.1.m1.1c"><ci id="Ch7.T7.5.1.m1.1.1.cmml" xref="Ch7.T7.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T7.5.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T7.5.1.m1.1e">→</annotation></semantics></math>GA systems</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch7.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch7.T8.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch7.T8.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch7.T8.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch7.T8.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T8.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch7.T8.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T8.1.1.1.m1.1"><semantics id="Ch7.T8.1.1.1.m1.1a"><mo id="Ch7.T8.1.1.1.m1.1.1" stretchy="false" xref="Ch7.T8.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T8.1.1.1.m1.1b"><ci id="Ch7.T8.1.1.1.m1.1.1.cmml" xref="Ch7.T8.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T8.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T8.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T8.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch7.T8.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch7.T8.2.2.2.m1.1"><semantics id="Ch7.T8.2.2.2.m1.1a"><mo id="Ch7.T8.2.2.2.m1.1.1" stretchy="false" xref="Ch7.T8.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch7.T8.2.2.2.m1.1b"><ci id="Ch7.T8.2.2.2.m1.1.1.cmml" xref="Ch7.T8.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T8.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T8.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch7.T8.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch7.T8.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch7.T8.3.3.3.m1.1"><semantics id="Ch7.T8.3.3.3.m1.1a"><mo id="Ch7.T8.3.3.3.m1.1.1" stretchy="false" xref="Ch7.T8.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch7.T8.3.3.3.m1.1b"><ci id="Ch7.T8.3.3.3.m1.1.1.cmml" xref="Ch7.T8.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T8.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T8.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch7.T8.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch7.T8.3.4.1.1">adaptNMT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T8.3.4.1.2">57.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T8.3.4.1.3">0.385</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T8.3.4.1.4">0.71</td>
</tr>
<tr class="ltx_tr" id="Ch7.T8.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch7.T8.3.5.2.1">myNMT</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T8.3.5.2.2">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T8.3.5.2.3">0.399</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch7.T8.3.5.2.4">0.703</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T8.7.2.1" style="font-size:90%;">Table 7.8</span>: </span><span class="ltx_text" id="Ch7.T8.5.1" style="font-size:90%;">Stochastic differences between GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.T8.5.1.m1.1"><semantics id="Ch7.T8.5.1.m1.1b"><mo id="Ch7.T8.5.1.m1.1.1" stretchy="false" xref="Ch7.T8.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.T8.5.1.m1.1c"><ci id="Ch7.T8.5.1.m1.1.1.cmml" xref="Ch7.T8.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T8.5.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.T8.5.1.m1.1e">→</annotation></semantics></math>EN systems</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Ch7.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.8 </span>Discussion</h3>
<div class="ltx_para" id="Ch7.S8.p1">
<p class="ltx_p" id="Ch7.S8.p1.1">The mathematical first principles governing NMT development were presented to demonstrate the mechanics of what happens during model training. Several parameters in Equations <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E2" title="7.2 ‣ Modelling ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.2</span></a> - <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch7.E8" title="7.8 ‣ Learning ‣ 7.5.4 NMT ‣ 7.5 Neural Networks for MT ‣ Chapter 7 adaptNMT: Open-Source Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7.8</span></a> are configurable within the adaptNMT application.</p>
</div>
<div class="ltx_para" id="Ch7.S8.p2">
<p class="ltx_p" id="Ch7.S8.p2.1">The environmental impact of technology, and the measurement of its effects, has gained a lot of prominence in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx47" title="">47</a>]</cite>. Indeed, this may be viewed as a natural response to truly massive NLP models which have been developed by large multinational corporations. In particular, HPO of NMT models can be particularly demanding if hyperparameter fine-tuning is conducted across a broad search space. As part of their work on NMT architectures, the Google Brain team required more than 250,000 GPU hours for NMT HPO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx22" title="">22</a>]</cite>. Training of these models was conducted using Tesla K40m and Tesla K80 GPUs with maximum power consumption between 235 W and 300 W, giving rise to potentially in excess of 60 MWh of energy usage. Even though the Google Cloud is carbon neutral, one must consider the opportunity cost of this energy usage.</p>
</div>
<div class="ltx_para" id="Ch7.S8.p3">
<p class="ltx_p" id="Ch7.S8.p3.1">A plethora of tools to evaluate the carbon footprint of NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx11" title="">11</a>]</cite> has subsequently been developed and the concept of sustainable NLP has become an important research track in its own right at many high-profile conferences such as the EACL 2021 <span class="ltx_text ltx_font_italic" id="Ch7.S8.p3.1.1">Green and Sustainable NLP</span> track.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://2021.eacl.org/news/green-and-sustainable-nlp" title="">https://2021.eacl.org/news/green-and-sustainable-nlp</a></span></span></span>
In light of such developments, a ‘green report’ was incorporated into adaptNMT whereby the kgCO<sub class="ltx_sub" id="Ch7.S8.p3.1.2">2</sub> generated during model development is logged. This is very much in line with the industry trend of quantifying the impact of NLP on the environment; indeed, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx51" title="">51</a>]</cite> have demonstrated that high-performing MT systems can be built with much lower footprints, which not only reduce emissions but also in the post-deployment phase deliver savings of almost 50% in energy costs for a real translation company.</p>
</div>
<div class="ltx_para" id="Ch7.S8.p4">
<p class="ltx_p" id="Ch7.S8.p4.2">To evaluate system performance in translating health data in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S8.p4.1.m1.1"><semantics id="Ch7.S8.p4.1.m1.1a"><mo id="Ch7.S8.p4.1.m1.1.1" stretchy="false" xref="Ch7.S8.p4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S8.p4.1.m1.1b"><ci id="Ch7.S8.p4.1.m1.1.1.cmml" xref="Ch7.S8.p4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S8.p4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S8.p4.1.m1.1d">→</annotation></semantics></math>GA direction, we used the adaptNMT application to develop an MT model for the LoResMT2021 Shared Task. The application was subsequently used to develop an MT model for translating in the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S8.p4.2.m2.1"><semantics id="Ch7.S8.p4.2.m2.1a"><mo id="Ch7.S8.p4.2.m2.1.1" stretchy="false" xref="Ch7.S8.p4.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S8.p4.2.m2.1b"><ci id="Ch7.S8.p4.2.m2.1.1.cmml" xref="Ch7.S8.p4.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S8.p4.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S8.p4.2.m2.1d">→</annotation></semantics></math>EN direction. In both cases, high-performing models achieving SOTA scores were achieved by using adaptNMT to develop Transformer models capable of generating high-quality output.</p>
</div>
<div class="ltx_para" id="Ch7.S8.p5">
<p class="ltx_p" id="Ch7.S8.p5.1">The danger of relying on increasingly large language models (LLMs) has been well-documented in the literature. Such discussion focuses not just on the environmental impact but also highlights the impact of in-built bias and the inherent risks that large models pose for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx15" title="">15</a>]</cite>. Using an easily-understood framework such as adaptNMT, the benefits of developing high-performing NMT models with smaller in-domain datasets should not be overlooked.</p>
</div>
</section>
<section class="ltx_section" id="Ch7.S9">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.9 </span>Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch7.S9.p1">
<p class="ltx_p" id="Ch7.S9.p1.2">We introduced adaptNMT, an application for NMT which manages the complete workflow of model development, evaluation and deployment. The performance of the application was demonstrated in the context of generating an EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch7.S9.p1.1.m1.1"><semantics id="Ch7.S9.p1.1.m1.1a"><mo id="Ch7.S9.p1.1.m1.1.1" stretchy="false" xref="Ch7.S9.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch7.S9.p1.1.m1.1b"><ci id="Ch7.S9.p1.1.m1.1.1.cmml" xref="Ch7.S9.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S9.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S9.p1.1.m1.1d">→</annotation></semantics></math>GA translation model which ranked 1st in the LoResMT2021 shared task and validated against a standalone reimplementation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch7.S9.p1.2.m2.1"><semantics id="Ch7.S9.p1.2.m2.1a"><mo id="Ch7.S9.p1.2.m2.1.1" stretchy="false" xref="Ch7.S9.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch7.S9.p1.2.m2.1b"><ci id="Ch7.S9.p1.2.m2.1.1.cmml" xref="Ch7.S9.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S9.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch7.S9.p1.2.m2.1d">↔</annotation></semantics></math>GA systems outside the tool, where no drop-off in performance was seen.</p>
</div>
<div class="ltx_para" id="Ch7.S9.p2">
<p class="ltx_p" id="Ch7.S9.p2.1">With regard to future work, development will focus more on tracking environmental costs and integrating new transfer learning methods. Modern zero-shot and few-shot approaches, adopted by GPT3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx23" title="">23</a>]</cite> and Facebook LASER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx7" title="">7</a>]</cite> frameworks, will be integrated. Whereas the existing adaptNMT application focuses on customising NMT models, a separate application adaptMLLM will be developed to fine-tune multilingual language models and LLMs, in particular those that focus on low-resource language pairs such as NLLB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S9.p3">
<p class="ltx_p" id="Ch7.S9.p3.1">The green report embedded within the application is our first implementation of a sustainable NLP feature within adaptNMT. It is planned to develop this feature further to include an improved user interface and user recommendations about how to develop greener models. As an open-source project, we hope the community will add to its development by contributing new ideas and improvements.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch8">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 8 </span>adaptMLLM: Fine-Tuning Multilingual Language Models</h2>
<section class="ltx_section" id="Ch8.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.1 </span>Context</h3>
<figure class="ltx_figure" id="Ch8.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="1009" id="Ch8.F1.g1" src="extracted/5444776/Images/mtsummit2023-adaptLLM-engaboth.png" width="1881"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch8.F1.4.2.1" style="font-size:90%;">Figure 8.1</span>: </span><span class="ltx_text" id="Ch8.F1.2.1" style="font-size:90%;">Fine-tuned MLLM approach of adaptMLLM for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.F1.2.1.m1.1"><semantics id="Ch8.F1.2.1.m1.1b"><mo id="Ch8.F1.2.1.m1.1.1" stretchy="false" xref="Ch8.F1.2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.F1.2.1.m1.1c"><ci id="Ch8.F1.2.1.m1.1.1.cmml" xref="Ch8.F1.2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.F1.2.1.m1.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.F1.2.1.m1.1e">↔</annotation></semantics></math>GA translation</span></figcaption>
</figure>
<div class="ltx_para" id="Ch8.S1.p1">
<p class="ltx_p" id="Ch8.S1.p1.4">As part of the research effort required to answer RQ4, two low-resource language pairs (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p1.1.m1.1"><semantics id="Ch8.S1.p1.1.m1.1a"><mo id="Ch8.S1.p1.1.m1.1.1" stretchy="false" xref="Ch8.S1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p1.1.m1.1b"><ci id="Ch8.S1.p1.1.m1.1.1.cmml" xref="Ch8.S1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p1.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p1.2.m2.1"><semantics id="Ch8.S1.p1.2.m2.1a"><mo id="Ch8.S1.p1.2.m2.1.1" stretchy="false" xref="Ch8.S1.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p1.2.m2.1b"><ci id="Ch8.S1.p1.2.m2.1.1.cmml" xref="Ch8.S1.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p1.2.m2.1d">↔</annotation></semantics></math>MR) were chosen for running fine-tuning experiments. The research contribution of this work resulted in the development of the adaptMLLM application which was used to carry out the fine-tuning experiments. The fine-tuned EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p1.3.m3.1"><semantics id="Ch8.S1.p1.3.m3.1a"><mo id="Ch8.S1.p1.3.m3.1.1" stretchy="false" xref="Ch8.S1.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p1.3.m3.1b"><ci id="Ch8.S1.p1.3.m3.1.1.cmml" xref="Ch8.S1.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p1.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p1.3.m3.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p1.4.m4.1"><semantics id="Ch8.S1.p1.4.m4.1a"><mo id="Ch8.S1.p1.4.m4.1.1" stretchy="false" xref="Ch8.S1.p1.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p1.4.m4.1b"><ci id="Ch8.S1.p1.4.m4.1.1.cmml" xref="Ch8.S1.p1.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p1.4.m4.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p1.4.m4.1d">↔</annotation></semantics></math>MR MLLM models demonstrated superior translation quality when compared to baselines from the LoResMT2021 Shared Task.</p>
</div>
<div class="ltx_para" id="Ch8.S1.p2">
<p class="ltx_p" id="Ch8.S1.p2.3">The first suite of tests was conducted on the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p2.1.m1.1"><semantics id="Ch8.S1.p2.1.m1.1a"><mo id="Ch8.S1.p2.1.m1.1.1" stretchy="false" xref="Ch8.S1.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p2.1.m1.1b"><ci id="Ch8.S1.p2.1.m1.1.1.cmml" xref="Ch8.S1.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p2.1.m1.1d">↔</annotation></semantics></math>GA pair and our approach in fine-tuning a pre-trained 3.3B parameter No Language Left Behind (NLLB) model is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F1" title="Figure 8.1 ‣ 8.1 Context ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>. With EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p2.2.m2.1"><semantics id="Ch8.S1.p2.2.m2.1a"><mo id="Ch8.S1.p2.2.m2.1.1" stretchy="false" xref="Ch8.S1.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p2.2.m2.1b"><ci id="Ch8.S1.p2.2.m2.1.1.cmml" xref="Ch8.S1.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p2.2.m2.1d">→</annotation></semantics></math>GA translation, the following scores were attained: 41.2 BLEU, 0.531 TER and 0.6 ChrF. In the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p2.3.m3.1"><semantics id="Ch8.S1.p2.3.m3.1a"><mo id="Ch8.S1.p2.3.m3.1.1" stretchy="false" xref="Ch8.S1.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p2.3.m3.1b"><ci id="Ch8.S1.p2.3.m3.1.1.cmml" xref="Ch8.S1.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p2.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p2.3.m3.1d">→</annotation></semantics></math>EN direction, the system achieved 75.1 BLEU, 0.385 TER and 0.71 ChrF which surpassed by a wide margin the winning scores of the shared task. An overview of the NLLB project is outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS2" title="8.5.2 Multilingual Language Models - NLLB ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5.2</span></a> and a more in-depth discussion of the NLLB architecture is covered in Appendix A.</p>
</div>
<div class="ltx_para" id="Ch8.S1.p3">
<p class="ltx_p" id="Ch8.S1.p3.3">The approach taken in fine-tuning a pre-trained 3.3B parameter NLLB model on the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p3.1.m1.1"><semantics id="Ch8.S1.p3.1.m1.1a"><mo id="Ch8.S1.p3.1.m1.1.1" stretchy="false" xref="Ch8.S1.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p3.1.m1.1b"><ci id="Ch8.S1.p3.1.m1.1.1.cmml" xref="Ch8.S1.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p3.1.m1.1d">↔</annotation></semantics></math>MR pair is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F2" title="Figure 8.2 ‣ 8.1 Context ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.2</span></a>. Again these scores improved upon all previous scores which were achieved in LoResMT2021. In the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p3.2.m2.1"><semantics id="Ch8.S1.p3.2.m2.1a"><mo id="Ch8.S1.p3.2.m2.1.1" stretchy="false" xref="Ch8.S1.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p3.2.m2.1b"><ci id="Ch8.S1.p3.2.m2.1.1.cmml" xref="Ch8.S1.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p3.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p3.2.m2.1d">→</annotation></semantics></math>MR direction, the following scores were attained: 26.4 BLEU, 0.56 TER and 0.608 ChrF. In the MR<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S1.p3.3.m3.1"><semantics id="Ch8.S1.p3.3.m3.1a"><mo id="Ch8.S1.p3.3.m3.1.1" stretchy="false" xref="Ch8.S1.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S1.p3.3.m3.1b"><ci id="Ch8.S1.p3.3.m3.1.1.cmml" xref="Ch8.S1.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S1.p3.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S1.p3.3.m3.1d">→</annotation></semantics></math>EN direction 52.6 BLEU, 0.409 TER and 0.704 ChrF were achieved.</p>
</div>
<figure class="ltx_figure" id="Ch8.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="951" id="Ch8.F2.g1" src="extracted/5444776/Images/mtsummit2023-adaptLLM-enmrboth.png" width="1851"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch8.F2.4.2.1" style="font-size:90%;">Figure 8.2</span>: </span><span class="ltx_text" id="Ch8.F2.2.1" style="font-size:90%;">
Fine-tuned MLLM approach of adaptMLLM for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.F2.2.1.m1.1"><semantics id="Ch8.F2.2.1.m1.1b"><mo id="Ch8.F2.2.1.m1.1.1" stretchy="false" xref="Ch8.F2.2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.F2.2.1.m1.1c"><ci id="Ch8.F2.2.1.m1.1.1.cmml" xref="Ch8.F2.2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.F2.2.1.m1.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.F2.2.1.m1.1e">↔</annotation></semantics></math>MR translation</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch8.S1.p4">
<p class="ltx_p" id="Ch8.S1.p4.1"><span class="ltx_text ltx_font_bold" id="Ch8.S1.p4.1.1">adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with integrated LLM playgrounds</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch8.S1.p4.2"><span class="ltx_text ltx_font_bold" id="Ch8.S1.p4.2.1">Séamus Lankford</span></p>
<p class="ltx_p" id="Ch8.S1.p4.3"><span class="ltx_text ltx_font_bold" id="Ch8.S1.p4.3.1">Haithem Afli</span></p>
<p class="ltx_p" id="Ch8.S1.p4.4"><span class="ltx_text ltx_font_bold" id="Ch8.S1.p4.4.1">Andy Way</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch8.S1.p4.5">Information, MDPI</p>
<p class="ltx_p" id="Ch8.S1.p4.6">November, 2023</p>
<p class="ltx_p" id="Ch8.S1.p4.7">ADAPT Centre</p>
<p class="ltx_p" id="Ch8.S1.p4.8">Dublin City University</p>
<p class="ltx_p" id="Ch8.S1.p4.9">Ireland</p>
<br class="ltx_break"/>
<p class="ltx_p" id="Ch8.S1.p4.10"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/info14120638" title="">https://doi.org/10.3390/info14120638</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch8.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.2 </span>Abstract</h3>
<div class="ltx_para" id="Ch8.S2.p1">
<p class="ltx_p" id="Ch8.S2.p1.7">The advent of multilingual language models (MLLMs) and large language models (LLMs) has spawned innovation in many areas of natural language processing. Despite the exciting potential of this technology, its impact on developing high-quality machine translation (MT) outputs for low-resource languages remains relatively under-explored. Furthermore, an open-source application, dedicated to both fine-tuning MLLMs and managing the complete MT workflow for low-resources languages, remains unavailable. We aim to address these imbalances through the development of adaptMLLM which streamlines all processes involved in the fine-tuning of MLLMs for MT. This open-source application is tailored for developers, translators, and users who are engaged in MT. It is particularly useful for newcomers to the field, as it significantly streamlines the configuration of the development environment. An intuitive interface allows for easy customisation of hyperparameters, and the application offers a range of metrics for model evaluation and the capability to deploy models as a translation service directly within the application. As a multilingual tool, we used adaptMLLM to fine-tune models for two low-resource language pairs: English to Irish (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.1.m1.1"><semantics id="Ch8.S2.p1.1.m1.1a"><mo id="Ch8.S2.p1.1.m1.1.1" stretchy="false" xref="Ch8.S2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.1.m1.1b"><ci id="Ch8.S2.p1.1.m1.1.1.cmml" xref="Ch8.S2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.1.m1.1d">↔</annotation></semantics></math>GA) and English to Marathi (EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.2.m2.1"><semantics id="Ch8.S2.p1.2.m2.1a"><mo id="Ch8.S2.p1.2.m2.1.1" stretchy="false" xref="Ch8.S2.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.2.m2.1b"><ci id="Ch8.S2.p1.2.m2.1.1.cmml" xref="Ch8.S2.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.2.m2.1d">↔</annotation></semantics></math>MR). Compared with baselines from the LoResMT2021 Shared Task, the adaptMLLM system demonstrated significant improvements. In the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.3.m3.1"><semantics id="Ch8.S2.p1.3.m3.1a"><mo id="Ch8.S2.p1.3.m3.1.1" stretchy="false" xref="Ch8.S2.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.3.m3.1b"><ci id="Ch8.S2.p1.3.m3.1.1.cmml" xref="Ch8.S2.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.3.m3.1d">→</annotation></semantics></math>GA direction, an improvement of 5.2 BLEU points was observed and an increase of 40.5 BLEU points was recorded in the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.4.m4.1"><semantics id="Ch8.S2.p1.4.m4.1a"><mo id="Ch8.S2.p1.4.m4.1.1" stretchy="false" xref="Ch8.S2.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.4.m4.1b"><ci id="Ch8.S2.p1.4.m4.1.1.cmml" xref="Ch8.S2.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.4.m4.1d">→</annotation></semantics></math>EN direction representing relative improvements of 14% and 117% respectively. Significant improvements in the translation performance of the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.5.m5.1"><semantics id="Ch8.S2.p1.5.m5.1a"><mo id="Ch8.S2.p1.5.m5.1.1" stretchy="false" xref="Ch8.S2.p1.5.m5.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.5.m5.1b"><ci id="Ch8.S2.p1.5.m5.1.1.cmml" xref="Ch8.S2.p1.5.m5.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.5.m5.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.5.m5.1d">↔</annotation></semantics></math>MR pair were also observed notably in the MR<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.6.m6.1"><semantics id="Ch8.S2.p1.6.m6.1a"><mo id="Ch8.S2.p1.6.m6.1.1" stretchy="false" xref="Ch8.S2.p1.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.6.m6.1b"><ci id="Ch8.S2.p1.6.m6.1.1.cmml" xref="Ch8.S2.p1.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.6.m6.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.6.m6.1d">→</annotation></semantics></math>EN direction with an increase of 21.3 BLEU points which corresponds to a relative improvement of 68%. Finally, a fine-grained human evaluation of the MLLM output on the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S2.p1.7.m7.1"><semantics id="Ch8.S2.p1.7.m7.1a"><mo id="Ch8.S2.p1.7.m7.1.1" stretchy="false" xref="Ch8.S2.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S2.p1.7.m7.1b"><ci id="Ch8.S2.p1.7.m7.1.1.cmml" xref="Ch8.S2.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S2.p1.7.m7.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S2.p1.7.m7.1d">→</annotation></semantics></math>GA pair was conducted using the Multidimensional Quality Metrics and Scalar Quality Metrics error taxonomies. The application and models are freely available.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://github.com/adaptNMT/adaptMLLM" title="">http://github.com/adaptNMT/adaptMLLM</a></span></span></span></p>
</div>
</section>
<section class="ltx_section" id="Ch8.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.3 </span>Graphical abstract</h3>
<figure class="ltx_figure" id="Ch8.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="362" id="Ch8.F3.g1" src="extracted/5444776/Images/adaptMLLM-graphical-abstract.png" width="608"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch8.F3.2.1.1" style="font-size:90%;">Figure 8.3</span>: </span><span class="ltx_text" id="Ch8.F3.3.2" style="font-size:90%;">Graphical abstract summarising the adaptMLLM system</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch8.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.4 </span>Introduction</h3>
<div class="ltx_para" id="Ch8.S4.p1">
<p class="ltx_p" id="Ch8.S4.p1.1">Large language models (LLMs), are AI models that use deep learning techniques to generate human-like text. These models are trained on vast amounts of text data, often using unsupervised learning, to learn the patterns and relationships within language. This results in models that can generate text which is often indistinguishable from text written by a human.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p2">
<p class="ltx_p" id="Ch8.S4.p2.1">The excitement surrounding LLMs stems from their potential to revolutionise many fields, from language translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx32" title="">32</a>]</cite> and content generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx23" title="">23</a>]</cite> to chatbots<span class="ltx_note ltx_role_footnote" id="Ch8.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a></span></span></span> and virtual assistants.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://genie.stanford.edu/" title="">https://genie.stanford.edu/</a></span></span></span> With their ability to understand natural language and generate complex responses, LLMs have the potential to enhance human communication and productivity in ways that were previously unimaginable. LLMs can also be used in creative applications, such as generating music<span class="ltx_note ltx_role_footnote" id="Ch8.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://soundraw.io/" title="">https://soundraw.io/</a></span></span></span> or art.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://labs.openai.com/" title="">https://labs.openai.com/</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch8.S4.p3">
<p class="ltx_p" id="Ch8.S4.p3.1">No Language Left Behind (NLLB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx32" title="">32</a>]</cite> represents a groundbreaking AI project in the area of multilingual language models (MLLMs). The project has released open-source models proficient in delivering high-quality translations across 200 languages and has enhanced translations for low-resource languages on platforms like Facebook and Instagram. The NLLB-200 model, integrated into the Wikimedia Foundation’s Content Translation tool, aids Wikipedia editors in translating content into their preferred languages. These editors can now more effectively translate articles from lesser-known languages, such as Luganda and Icelandic, enriching Wikipedia’s language diversity. The open-sourced nature of the NLLB-200 model also empowers the research community and Wikipedia editor groups to expand upon their findings.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p4">
<p class="ltx_p" id="Ch8.S4.p4.1">When building MLLMs and LLMs, the focus is on designing and training the model architecture. This involves selecting the appropriate neural network architecture and hyperparameters, as well as deciding on the training data and optimisation techniques to use.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p5">
<p class="ltx_p" id="Ch8.S4.p5.1">Tuning an MLLM or LLM, on the other hand, involves adjusting the parameters of the model to improve its performance on a specific task. In neural networks such as MLLMs and LLMs, the weights and biases are parameters that the network adjusts through training to minimise a cost function. This is done by training the model on a task-specific dataset and adjusting the model’s hyperparameters to optimise its performance. Tuning an MLLM can be a challenging task, as the model is often very complex and the training process can take a long time. Our paper concentrates on fine-tuning pre-built MLLMs to enhance machine translation (MT) with a particular focus on low-resource language pairs.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p6">
<p class="ltx_p" id="Ch8.S4.p6.1">The process of fine-tuning an MLLM involves several distinct stages which are broken down into individual steps. These steps include setting up the environment, preparing the dataset, parameterising and fine-tuning the chosen MLLM, and evaluating and deploying the model. This modular approach has proven to be effective in fine-tuning MLLMs and we have structured our adaptMLLM application to cater for both developers and translators. In light of the environmental impact of developing and running large AI models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx51" title="">51</a>]</cite>, we also calculate carbon emissions in a “green report”. It is envisaged that such a report will incentivise more responsible and sustainable model development.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p7">
<p class="ltx_p" id="Ch8.S4.p7.1">A significant aspect of our research involves creating applications and models to address language technology challenges. Similar to our previous work which focused on developing NMT models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx68" title="">68</a>]</cite>, we hope this paper will be particularly helpful for those new to MT wishing to learn more about fine-tuning MLLMs.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p8">
<p class="ltx_p" id="Ch8.S4.p8.1">Unlike many translation toolkits, our application does not use a command line interface. Instead, we have designed and fully implemented the interface in Google Colab,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://colab.research.google.com" title="">https://colab.research.google.com</a></span></span></span> a cloud-hosted solution<span class="ltx_note ltx_role_footnote" id="Ch8.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com" title="">https://cloud.google.com</a></span></span></span> that is more intuitive for both educational and research settings. Furthermore, our application provides graphical user interface (GUI) controls within adaptMLLM, enabling users to customise all key hyperparameters required for MLLMs.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p9">
<p class="ltx_p" id="Ch8.S4.p9.1">Our application is designed to operate as a platform as a service (PaaS) cloud computing application, allowing for quick and efficient scaling of the infrastructure. Additionally, the deploy function allows for immediate deployment of trained models.</p>
</div>
<div class="ltx_para" id="Ch8.S4.p10">
<p class="ltx_p" id="Ch8.S4.p10.1">This paper is organised by initially presenting related work and background information on MLLMs and LLMs in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5" title="8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5</span></a>. This is followed by a description of our datasets in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S6" title="8.6 Datasets ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.6</span></a>. The key features of the adaptMLLM architecture are discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7" title="8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.7</span></a> and an empirical evaluation of our trained models, including a human evaluation is carried out in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8" title="8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.8</span></a>. The system is discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9" title="8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.9</span></a> before drawing conclusions and describing future work in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S10" title="8.10 Conclusion and Future Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.10</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="Ch8.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.5 </span>Related Work</h3>
<section class="ltx_subsection" id="Ch8.S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5.1 </span>Transformer Architecture</h4>
<div class="ltx_para" id="Ch8.S5.SS1.p1">
<p class="ltx_p" id="Ch8.S5.SS1.p1.1">Following the introduction of the attention mechanism, a natural line of investigation was to see whether attention could do most of the heavy lifting of translation by itself. Accordingly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx114" title="">114</a>]</cite> proposed that “attention is all you need” in their Transformer architecture, which has achieved state-of-the-art (SOTA) performance on many natural language processing (NLP) benchmarks by relying solely on an attention mechanism, removing recurrence and convolution while allowing the use of much simpler feed-forward neural networks. In the context of our research, we have previously demonstrated that Transformer-based models deliver high-functioning models for the low-resource EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S5.SS1.p1.1.m1.1"><semantics id="Ch8.S5.SS1.p1.1.m1.1a"><mo id="Ch8.S5.SS1.p1.1.m1.1.1" stretchy="false" xref="Ch8.S5.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S5.SS1.p1.1.m1.1b"><ci id="Ch8.S5.SS1.p1.1.m1.1.1.cmml" xref="Ch8.S5.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S5.SS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S5.SS1.p1.1.m1.1d">→</annotation></semantics></math>GA language pair <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx65" title="">65</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS1.p2">
<p class="ltx_p" id="Ch8.S5.SS1.p2.1">The default Transformer architecture follows an encoder-decoder structure generating its output without relying on recurrence and convolutions. The task of the encoder is to map an input sequence to a sequence of continuous representations, which is then passed to a decoder. The decoder generates an output sequence by using the output of the encoder together with the decoder output from the previous time step.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5.2 </span>Multilingual Language Models - NLLB</h4>
<div class="ltx_para" id="Ch8.S5.SS2.p1">
<p class="ltx_p" id="Ch8.S5.SS2.p1.1">MT has become a significant area of research in AI to eliminate language barriers worldwide. However, the current focus is limited to a small number of languages, neglecting the vast majority of low-resource languages. To address this issue, the NLLB initiative was launched. This project aims to overcome the challenges of using MT for low-resource language translation by developing datasets and models that bridge the performance gap between low- and high-resource languages. The NLLB team has also created architectural and training enhancements tailored to support MT for low-resource languages. Their work is open-source,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairseq/tree/nllb" title="">https://github.com/facebookresearch/fairseq/tree/nllb</a></span></span></span> and many of their models serve as baselines for fine-tuning with adaptMLLM.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S5.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5.3 </span>Large Language Models</h4>
<div class="ltx_para" id="Ch8.S5.SS3.p1">
<p class="ltx_p" id="Ch8.S5.SS3.p1.1">The increasing availability of large datasets provides the raw material for LLM training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx117" title="">117</a>]</cite>
enabling performance improvement on NLP tasks which can learn from a wide variety of sources.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS3.p2">
<p class="ltx_p" id="Ch8.S5.SS3.p2.1">Another key factor in driving the ubiquity of LLMs has been the growth in computational power dedicated to the domain. As a consequence, more powerful computers now have the capability to train LLMs on massive datasets, which in turn has led to SOTA results on many common NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx35" title="">35</a>]</cite>. New training algorithms developed through advancement in AI research have further boosted LLM performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx71" title="">71</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS3.p3">
<p class="ltx_p" id="Ch8.S5.SS3.p3.1">LLMs have the potential to improve the use of technology across a wide range of domains, among which include medicine, education and computational linguistics. In education, LLMs may be used for personalised student learning experiences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx54" title="">54</a>]</cite> while in the medical domain, analysing large amounts of medical files can assist doctors in treating patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx49" title="">49</a>]</cite>.
Of particular interest to our research is how LLMs can be used within the realm of computational linguistics, more specifically in the field of MT.</p>
</div>
<section class="ltx_subsubsection" id="Ch8.S5.SS3.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">GPT-J</h5>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx1.p1">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx1.p1.1">Transformers are increasingly the architecture of choice for NLP problems, replacing recurrent neural networks (RNNs) such as long short-term memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx48" title="">48</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx1.p2">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx1.p2.1">GPT-J is an open-source implementation of a particular class of LLMs known as generative pre-trained transformer (GPT) models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx93" title="">93</a>]</cite>. GPT-J is a Transformer model trained using Wang’s Mesh Transformer JAX.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kingoflolz/mesh-transformer-jax" title="">https://github.com/kingoflolz/mesh-transformer-jax</a></span></span></span> GPT-J-6B<span class="ltx_note ltx_role_footnote" id="Ch8.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://6b.eleuther.ai" title="">https://6b.eleuther.ai</a></span></span></span> is an autoregressive language model, created by EleutherAI,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eleuther.ai" title="">https://www.eleuther.ai</a></span></span></span> with 6 billion trainable parameters. As an advanced alternative to OpenAI’s GPT-3, it performs very well on a wide array of NLP tasks such as chat, summarisation, and question answering.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch8.S5.SS3.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">GPT-4</h5>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx2.p1">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx2.p1.1">The primary distinction between GPT-3.5 and GPT-4<span class="ltx_note ltx_role_footnote" id="Ch8.footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/product/gpt-4" title="">https://openai.com/product/gpt-4</a></span></span></span> is that while the former is a text-to-text model, the latter is more of a data-to-text model, exhibiting the ability to perform tasks that its predecessor could not. For example, GPT-4 is capable of processing visual input as part of a prompt, such as images or web pages, and can even generate text that explains the humour in memes. Consequently, GPT-4 can be classified as a “multimodal model”. Furthermore, GPT-4 has a longer memory than its previous versions, with a short-term memory closer to 64,000 words, enabling it to maintain coherence during extended interactions. GPT-4 also enables users to select different personalities for the model’s responses.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx2.p2">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx2.p2.1">The number of parameters utilised in the training of GPT-4 has not been disclosed by OpenAI; however, other sources, such as AX Semantics,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.ax-semantics.com/" title="">https://en.ax-semantics.com/</a></span></span></span> have estimated the number to be around 100 trillion. AX Semantics maintains that such a number makes the language model more akin to the functioning of the human brain for language and logic.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.ax-semantics.com/blog/gpt-4-and-whats-different-from-gpt-3/" title="">https://en.ax-semantics.com/blog/gpt-4-and-whats-different-from-gpt-3/</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx2.p3">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx2.p3.1">Additionally, GPT-4 outperformed GPT-3.5 in various standardised tests, such as the LSAT, SAT, Uniform Bar Exam, and GRE, and was shown to be 82% less likely to respond when prompted inappropriately and 60% less likely to generate false information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx86" title="">86</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch8.S5.SS3.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">BARD</h5>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx3.p1">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx3.p1.1">BARD<span class="ltx_note ltx_role_footnote" id="Ch8.footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bard.google.com/" title="">https://bard.google.com/</a></span></span></span> utilises a lightweight version of the Language Model for Dialogue Applications (LaMDA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx110" title="">110</a>]</cite>, which is an AI engine developed by Google. BARD has two primary objectives: to ensure the accuracy of its responses and to integrate the benefits of AI into its everyday products. Google has a rich history of employing AI to improve the search experience for billions of users. Its earlier Transformer model, BERT,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google-research/bert" title="">https://github.com/google-research/bert</a></span></span></span> was a breakthrough in comprehending the intricacies of human language. The company has since introduced MUM,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/products/search/introducing-mum/" title="">https://blog.google/products/search/introducing-mum/</a></span></span></span> which is a thousand times more potent than BERT. Recent AI technologies like LaMDA, PaLM, Imagen, and MusicLM are building on these developments, creating new ways to interact with information from language and images to video and audio. Furthermore, in 2018, Google was one of the pioneering companies to release a set of AI principles.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google/principles/" title="">https://ai.google/principles/</a></span></span></span>
</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS3.SSSx3.p2">
<p class="ltx_p" id="Ch8.S5.SS3.SSSx3.p2.1">Apart from its products, Google aims to assist developers in innovating with AI by simplifying and scaling the benefits of these advances. In the future, the company intends to create a suite of tools and APIs that will make it easier to build innovative applications with BARD and more generally with its AI.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch8.S5.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5.4 </span>DeepSpeed</h4>
<div class="ltx_para" id="Ch8.S5.SS4.p1">
<p class="ltx_p" id="Ch8.S5.SS4.p1.1">The advent of DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx95" title="">95</a>]</cite>, a free software library from Microsoft, was a significant breakthrough for researchers looking to implement and fine-tune MLLMs and LLMs with limited resources. Large model training, in terms of scale, speed, and cost, is now achievable for most people. Additionally, DeepSpeed’s most recent Transformer kernel improvements enabled the DeepSpeed team to achieve SOTA performance, setting a new record for the fastest BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx35" title="">35</a>]</cite> pre-training.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS4.p2">
<p class="ltx_p" id="Ch8.S5.SS4.p2.1">For small teams, DeepSpeed’s Zero Redundancy Optimizer (ZeRO) is particularly advantageous, providing fresh memory optimisation for large-scale distributed deep learning. With minor changes to a PyTorch model, DeepSpeed can improve the speed and scale of model training.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S5.SS5">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5.5 </span>HuggingFace</h4>
<div class="ltx_para" id="Ch8.S5.SS5.p1">
<p class="ltx_p" id="Ch8.S5.SS5.p1.1">The Hugging Face Transformers library<span class="ltx_note ltx_role_footnote" id="Ch8.footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers" title="">https://github.com/huggingface/transformers</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx118" title="">118</a>]</cite> is an open-source software library that provides a wide range of pre-trained SOTA NLP models, including models for language modelling, question answering, text classification, and MT, among others.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS5.p2">
<p class="ltx_p" id="Ch8.S5.SS5.p2.1">The library is built on top of popular deep learning frameworks such as PyTorch<span class="ltx_note ltx_role_footnote" id="Ch8.footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pytorch/pytorch" title="">https://github.com/pytorch/pytorch</a></span></span></span> and TensorFlow,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tensorflow/tensorflow" title="">https://github.com/tensorflow/tensorflow</a></span></span></span> and it provides a simple and consistent API for accessing pre-trained models and fine-tuning them for downstream tasks. The library also includes a set of tools for data pre-processing, model evaluation, and visualisation, which make it easier for researchers and developers to experiment with different NLP models and tasks.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS5.p3">
<p class="ltx_p" id="Ch8.S5.SS5.p3.1">The Hugging Face Transformers library has become one of the most popular and widely used NLP libraries in the industry and the research community, and it has been adopted by many companies and organisations to build NLP applications and systems.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S5.SS6">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5.6 </span>Human Evaluation</h4>
<div class="ltx_para" id="Ch8.S5.SS6.p1">
<p class="ltx_p" id="Ch8.S5.SS6.p1.1">Within the fields of NLP and MT, human evaluation is increasingly recognised as critical, often meriting its own specialised research track or workshop at leading conferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx14" title="">14</a>]</cite>. This emphasis has spurred a wealth of studies focusing on human evaluation related to MT, proving especially valuable in assessing low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx50" title="">50</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS6.p2">
<p class="ltx_p" id="Ch8.S5.SS6.p2.1">A set of best practices for human evaluation in MT has emerged, detailed in a collection of suggested guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx70" title="">70</a>]</cite>. Our study incorporates these guidelines, aligning with comparable EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S5.SS6.p2.1.m1.1"><semantics id="Ch8.S5.SS6.p2.1.m1.1a"><mo id="Ch8.S5.SS6.p2.1.m1.1.1" stretchy="false" xref="Ch8.S5.SS6.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S5.SS6.p2.1.m1.1b"><ci id="Ch8.S5.SS6.p2.1.m1.1.1.cmml" xref="Ch8.S5.SS6.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S5.SS6.p2.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S5.SS6.p2.1.m1.1d">↔</annotation></semantics></math>GA studies at the ADAPT centre. To enhance these guidelines, a detailed human analysis was conducted, employing both the Scalar Quality Metric (SQM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite> and the Multidimensional Quality Metric (MQM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx75" title="">75</a>]</cite> for a nuanced assessment. SQM and MQM, are both widely used in industry and academia, to evaluate the quality of machine-generated text.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS6.p3">
<p class="ltx_p" id="Ch8.S5.SS6.p3.1">SQM is a simple, single-number metric that is used to measure the overall MT quality. It is often used when a quick evaluation of the quality of the text is required.</p>
</div>
<div class="ltx_para" id="Ch8.S5.SS6.p4">
<p class="ltx_p" id="Ch8.S5.SS6.p4.1">MQM, on the other hand, is a more complex metric that measures the quality of the text across multiple dimensions such as fluency, adequacy, and coherence, to name a few. It provides a more comprehensive evaluation of MT by measuring the quality of the text across different aspects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch8.S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.6 </span>Datasets</h3>
<section class="ltx_subsection" id="Ch8.S6.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.6.1 </span>Language Pairs</h4>
<div class="ltx_para" id="Ch8.S6.SS1.p1">
<p class="ltx_p" id="Ch8.S6.SS1.p1.2">To evaluate the translation performance of adaptMLLM in fine-tuning MLLMs for low-resource languages, we had to choose suitable language pairs. Furthermore, appropriate datasets upon which we could benchmark our performance also had to be sourced. The EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS1.p1.1.m1.1"><semantics id="Ch8.S6.SS1.p1.1.m1.1a"><mo id="Ch8.S6.SS1.p1.1.m1.1.1" stretchy="false" xref="Ch8.S6.SS1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS1.p1.1.m1.1b"><ci id="Ch8.S6.SS1.p1.1.m1.1.1.cmml" xref="Ch8.S6.SS1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS1.p1.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS1.p1.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS1.p1.2.m2.1"><semantics id="Ch8.S6.SS1.p1.2.m2.1a"><mo id="Ch8.S6.SS1.p1.2.m2.1.1" stretchy="false" xref="Ch8.S6.SS1.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS1.p1.2.m2.1b"><ci id="Ch8.S6.SS1.p1.2.m2.1.1.cmml" xref="Ch8.S6.SS1.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS1.p1.2.m2.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS1.p1.2.m2.1d">↔</annotation></semantics></math>MR language pairs were selected since they fulfilled the criteria of low-resource languages.</p>
</div>
<div class="ltx_para" id="Ch8.S6.SS1.p2">
<p class="ltx_p" id="Ch8.S6.SS1.p2.1">The Irish language, also known as Irish Gaelic, is the first official language of the Republic of Ireland and is also recognised as a minority language in Northern Ireland. According to the 2022 Irish census,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cso.ie/en/releasesandpublications/ep/p-cpsr/censusofpopulation2022-summaryresults/educationandirishlanguage/" title="">https://www.cso.ie/en/releasesandpublications/ep/p-cpsr/censusofpopulation2022-summaryresults/educationandirishlanguage/</a></span></span></span> 1.87 million people in the Republic of Ireland reported being able to speak Irish to some degree, which represents 40.4% of the population. Irish is also spoken by a small number of people in other countries, particularly in the United States, Canada, and Australia, as well as in Irish-speaking communities in other parts of the world. It is also one of the official languages of the European Union and a recognised minority language in Northern Ireland with an ISO code of “GA”.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.iso.org/" title="">https://www.iso.org/</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch8.S6.SS1.p3">
<p class="ltx_p" id="Ch8.S6.SS1.p3.1">The dominant language spoken in India’s Maharashtra state is Marathi with an ISO code of “MR”. It has over 83 million speakers and is a member of the Indo-Aryan language family. Despite being spoken by a significant number of people, Marathi is considered to be relatively under-resourced when compared to other languages used in the region.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S6.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.6.2 </span>Shared Task Datasets</h4>
<div class="ltx_para" id="Ch8.S6.SS2.p1">
<p class="ltx_p" id="Ch8.S6.SS2.p1.3">To benchmark the performance of our EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p1.1.m1.1"><semantics id="Ch8.S6.SS2.p1.1.m1.1a"><mo id="Ch8.S6.SS2.p1.1.m1.1.1" stretchy="false" xref="Ch8.S6.SS2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p1.1.m1.1b"><ci id="Ch8.S6.SS2.p1.1.m1.1.1.cmml" xref="Ch8.S6.SS2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p1.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p1.1.m1.1d">↔</annotation></semantics></math>GA models, trained using adaptMLLM, datasets from the LoResMT2021 Shared Task<span class="ltx_note ltx_role_footnote" id="Ch8.footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/loresmt/loresmt-2021" title="">https://github.com/loresmt/loresmt-2021</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite> were used. These datasets enabled the evaluation of adaptMLLM models since the shared task focused on low-resource languages which included both the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p1.2.m2.1"><semantics id="Ch8.S6.SS2.p1.2.m2.1a"><mo id="Ch8.S6.SS2.p1.2.m2.1.1" stretchy="false" xref="Ch8.S6.SS2.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p1.2.m2.1b"><ci id="Ch8.S6.SS2.p1.2.m2.1.1.cmml" xref="Ch8.S6.SS2.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p1.2.m2.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p1.2.m2.1d">↔</annotation></semantics></math>GA pair and the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p1.3.m3.1"><semantics id="Ch8.S6.SS2.p1.3.m3.1a"><mo id="Ch8.S6.SS2.p1.3.m3.1.1" stretchy="false" xref="Ch8.S6.SS2.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p1.3.m3.1b"><ci id="Ch8.S6.SS2.p1.3.m3.1.1.cmml" xref="Ch8.S6.SS2.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p1.3.m3.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p1.3.m3.1d">↔</annotation></semantics></math>MR pair. Furthermore, using official datasets from a shared task enables the direct comparison of our models’ performance with models entered by other teams.</p>
</div>
<div class="ltx_para" id="Ch8.S6.SS2.p2">
<p class="ltx_p" id="Ch8.S6.SS2.p2.2">Both datasets focused on the specific domain of translation of Covid-related data. A parallel corpus of EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p2.1.m1.1"><semantics id="Ch8.S6.SS2.p2.1.m1.1a"><mo id="Ch8.S6.SS2.p2.1.m1.1.1" stretchy="false" xref="Ch8.S6.SS2.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p2.1.m1.1b"><ci id="Ch8.S6.SS2.p2.1.m1.1.1.cmml" xref="Ch8.S6.SS2.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p2.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p2.1.m1.1d">↔</annotation></semantics></math>GA sentences concentrating on the Covid domain were mainly drawn from the Government of Ireland<span class="ltx_note ltx_role_footnote" id="Ch8.footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gov.ie/" title="">https://www.gov.ie/</a></span></span></span> and the Health Service Executive<span class="ltx_note ltx_role_footnote" id="Ch8.footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.hse.ie/" title="">https://www.hse.ie/</a></span></span></span> websites. EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p2.2.m2.1"><semantics id="Ch8.S6.SS2.p2.2.m2.1a"><mo id="Ch8.S6.SS2.p2.2.m2.1.1" stretchy="false" xref="Ch8.S6.SS2.p2.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p2.2.m2.1b"><ci id="Ch8.S6.SS2.p2.2.m2.1.1.cmml" xref="Ch8.S6.SS2.p2.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p2.2.m2.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p2.2.m2.1d">↔</annotation></semantics></math>MR parallel Covid sentences were extracted from the Government of India<span class="ltx_note ltx_role_footnote" id="Ch8.footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mygov.in/" title="">https://www.mygov.in/</a></span></span></span> website, BBC Marathi<span class="ltx_note ltx_role_footnote" id="Ch8.footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bbc.com/marathi" title="">https://www.bbc.com/marathi</a></span></span></span> and online newspapers. A detailed breakdown of all sources is available in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx85" title="">85</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S6.SS2.p3">
<p class="ltx_p" id="Ch8.S6.SS2.p3.5">The datasets from the shared task provided 502 Irish and 500 Marathi validation sentences whereas 250 GA<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p3.1.m1.1"><semantics id="Ch8.S6.SS2.p3.1.m1.1a"><mo id="Ch8.S6.SS2.p3.1.m1.1.1" stretchy="false" xref="Ch8.S6.SS2.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p3.1.m1.1b"><ci id="Ch8.S6.SS2.p3.1.m1.1.1.cmml" xref="Ch8.S6.SS2.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p3.1.m1.1c">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p3.1.m1.1d">→</annotation></semantics></math>EN, 500 EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p3.2.m2.1"><semantics id="Ch8.S6.SS2.p3.2.m2.1a"><mo id="Ch8.S6.SS2.p3.2.m2.1.1" stretchy="false" xref="Ch8.S6.SS2.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p3.2.m2.1b"><ci id="Ch8.S6.SS2.p3.2.m2.1.1.cmml" xref="Ch8.S6.SS2.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p3.2.m2.1c">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p3.2.m2.1d">→</annotation></semantics></math>GA, 500 EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p3.3.m3.1"><semantics id="Ch8.S6.SS2.p3.3.m3.1a"><mo id="Ch8.S6.SS2.p3.3.m3.1.1" stretchy="false" xref="Ch8.S6.SS2.p3.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p3.3.m3.1b"><ci id="Ch8.S6.SS2.p3.3.m3.1.1.cmml" xref="Ch8.S6.SS2.p3.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p3.3.m3.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p3.3.m3.1d">↔</annotation></semantics></math>MR sentences were made available in the test datasets i.e. exactly the same as our other experiments to allow direct comparison with previous work. Training data consisted of 20,933 lines of parallel data for the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p3.4.m4.1"><semantics id="Ch8.S6.SS2.p3.4.m4.1a"><mo id="Ch8.S6.SS2.p3.4.m4.1.1" stretchy="false" xref="Ch8.S6.SS2.p3.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p3.4.m4.1b"><ci id="Ch8.S6.SS2.p3.4.m4.1.1.cmml" xref="Ch8.S6.SS2.p3.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p3.4.m4.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p3.4.m4.1d">↔</annotation></semantics></math>MR language pair and 13,171 lines of parallel data were used to train the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S6.SS2.p3.5.m5.1"><semantics id="Ch8.S6.SS2.p3.5.m5.1a"><mo id="Ch8.S6.SS2.p3.5.m5.1.1" stretchy="false" xref="Ch8.S6.SS2.p3.5.m5.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S6.SS2.p3.5.m5.1b"><ci id="Ch8.S6.SS2.p3.5.m5.1.1.cmml" xref="Ch8.S6.SS2.p3.5.m5.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S6.SS2.p3.5.m5.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S6.SS2.p3.5.m5.1d">↔</annotation></semantics></math>GA models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch8.S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.7 </span>Approach</h3>
<div class="ltx_para" id="Ch8.S7.p1">
<p class="ltx_p" id="Ch8.S7.p1.1">After outlining the background that gave rise to the creation of MLLMs and LLMs, we now introduce the adaptMLLM tool. This tool allows users to customise these components to their liking. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F4" title="Figure 8.4 ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.4</span></a> offers a high-level overview of the platform’s system architecture.</p>
</div>
<figure class="ltx_figure" id="Ch8.F4"><img alt="Refer to caption" class="ltx_graphics" id="Ch8.F4.g1" src="adaptLLM_fullarch.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch8.F4.2.1.1" style="font-size:90%;">Figure 8.4</span>: </span><span class="ltx_text" id="Ch8.F4.3.2" style="font-size:90%;">Proposed architecture for adaptMLLM: a system for fine-tuning MLLMs</span></figcaption>
</figure>
<div class="ltx_para" id="Ch8.S7.p2">
<p class="ltx_p" id="Ch8.S7.p2.1">The application is designed as an IPython notebook and employs Pytorch for model training. The utilisation of a Jupyter notebook format facilitates easy sharing within the AI community. Additionally, the challenge of configuring the proper development environment is substantially reduced, as all necessary packages are automatically downloaded while the application is running.</p>
</div>
<figure class="ltx_figure" id="Ch8.F5">
<br class="ltx_break ltx_centering"/><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="Ch8.F5.g1" src="adaptLLM_app.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch8.F5.2.1.1" style="font-size:90%;">Figure 8.5</span>: </span><span class="ltx_text" id="Ch8.F5.3.2" style="font-size:90%;">Overview of adaptMLLM. Key areas include initialisation, a menu of operation modes, loading and pre-processing, MLLM fine-tuning, visualisation, deployment, a green report, MLLM translation and evaluation, LLM playgrounds and a human evaluation (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S7" title="8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.7</span></a>).</span></figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
<div class="ltx_para" id="Ch8.S7.p3">
<p class="ltx_p" id="Ch8.S7.p3.1">There are options to run the system for fine-tuning MLLMs, evaluating MLLM translation performance, testing LLM playgrounds and conducting a human evaluation of the translation performance. The application is run as a Colab instance on the Google Cloud. Translation models are developed using aligned text corpora from both the original and the target languages. Tensorboard offers a live graphical representation of the training process of the model. The system is primarily employed for training models and functioning as a translation service, either of which can be chosen at run-time.</p>
</div>
<div class="ltx_para" id="Ch8.S7.p4">
<p class="ltx_p" id="Ch8.S7.p4.1">The application is primarily run as a Google Colab application but may also be run as a Jupyter notebook. Given the ease of integrating Google Drive storage into Colab, we have used adaptMLLM exclusively as a Google Colab application for our experiments, some of which are described in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S8" title="8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.8</span></a>. Key features of the notebook are highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.F5" title="Figure 8.5 ‣ 8.7 Approach ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5</span></a>.</p>
</div>
<section class="ltx_subsection" id="Ch8.S7.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.1 </span>Initialisation and Pre-processing</h4>
<div class="ltx_para" id="Ch8.S7.SS1.p1">
<p class="ltx_p" id="Ch8.S7.SS1.p1.1">Initialisation enables connection to Google Drive to run experiments, automatic installation of Python, SentencePiece<span class="ltx_note ltx_role_footnote" id="Ch8.footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/sentencepiece" title="">https://github.com/google/sentencepiece</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx61" title="">61</a>]</cite>, Pytorch, HuggingFace Transformer’s library (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS5" title="8.5.5 HuggingFace ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5.5</span></a>) and other libraries.
</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS1.p2">
<p class="ltx_p" id="Ch8.S7.SS1.p2.1">The training, validation and test splits for both source and target languages may be uploaded by the users. In cases where a user has not already created the required splits for model training, single source and target files may be uploaded. The necessary splits to form the training, validation, and test files will be automatically created based on the split ratio specified by the user.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S7.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.2 </span>Modes of Operation</h4>
<div class="ltx_para" id="Ch8.S7.SS2.p1">
<p class="ltx_p" id="Ch8.S7.SS2.p1.1">There are several modes of operation, namely MLLM fine-tuning, evaluation of MLLM translation performance, experimentation with LLM playgrounds and a human evaluation of the translation output.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS2.p2">
<p class="ltx_p" id="Ch8.S7.SS2.p2.1">With MLLM fine-tuning, the application develops models using Google’s GPU-based cloud platform. For a monthly subscription, the Google Colab Pro+ is a prerequisite since fine-tuning demands access to high-end GPU and compute resources.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS2.p3">
<p class="ltx_p" id="Ch8.S7.SS2.p3.1">Apart from low-cost access to a high-specification infrastructure, model development on the Google Cloud is also recommended given the platform uses 100% renewables <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>. This has emerged as an economical choice for practitioners in the field of low-resource languages, as the creation of smaller models involves reduced training times.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S7.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.3 </span>Fine-tuning and Visualisation</h4>
<div class="ltx_para" id="Ch8.S7.SS3.p1">
<p class="ltx_p" id="Ch8.S7.SS3.p1.1">The system has been designed to enable users to choose variations of the base MLLM architecture. In the current release, users can choose to fine-tune the following baselines: i) NLLB-200-600M, ii) NLLB-200-1.3M, iii) NLLB-200-3.3B or iv) a user-specified baseline. The fine-tuning mode allows users to specify, using GUI controls, the exact hyperparameters required for the chosen approach.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS3.p2">
<p class="ltx_p" id="Ch8.S7.SS3.p2.1">The visualisation segment provides live graphing of model progression, allowing for the monitoring of model convergence. All log files are preserved and accessible for review to examine the training convergence, as well as to evaluate the model’s accuracy during the training and validation phases.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S7.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.4 </span>Deployment</h4>
<div class="ltx_para" id="Ch8.S7.SS4.p1">
<p class="ltx_p" id="Ch8.S7.SS4.p1.1">Gradio<span class="ltx_note ltx_role_footnote" id="Ch8.footnote30"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><span class="ltx_tag ltx_tag_note">30</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gradio.app/" title="">https://gradio.app/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx1" title="">1</a>]</cite> is an open-source Python library that enables the development of easy-to-use web applications for ML models. The library integrates with the most popular Python libraries, including Scikit-learn and PyTorch.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS4.p2">
<p class="ltx_p" id="Ch8.S7.SS4.p2.1">A key advantage is that it allows interaction with a web app developed for a Jupyter or Colab notebook. Consequently, it was selected as the library used for the deployment of our custom fine-tuned models.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S7.SS5">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.5 </span>Green Report</h4>
<div class="ltx_para" id="Ch8.S7.SS5.p1">
<p class="ltx_p" id="Ch8.S7.SS5.p1.1">In recent years, the ecological footprint of technology, along with the assessment of its impacts, has become increasingly prominent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx47" title="">47</a>]</cite>. Indeed, this may be viewed as a natural response to truly massive NLP models which have been developed by large multinational corporations with little apparent regard for their environmental impact.
</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS5.p2">
<p class="ltx_p" id="Ch8.S7.SS5.p2.1">Specifically, HPO for finely-tuned MLLMs can be especially demanding when the fine-tuning of hyperparameters spans a wide search space.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS5.p3">
<p class="ltx_p" id="Ch8.S7.SS5.p3.1">Consequently, a wide array of tools for assessing NLP’s carbon footprint has been created <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx11" title="">11</a>]</cite>, and the idea of sustainable NLP has emerged as a significant area of research. This has been recognised at numerous prestigious conferences, for instance, the Green and Sustainable NLP track at EACL 2021.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote31"><sup class="ltx_note_mark">31</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">31</sup><span class="ltx_tag ltx_tag_note">31</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://2021.eacl.org/news/green-and-sustainable-nlp" title="">https://2021.eacl.org/news/green-and-sustainable-nlp</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch8.S7.SS5.p4">
<p class="ltx_p" id="Ch8.S7.SS5.p4.1">Reflecting these advancements, adaptMLLM has integrated a “green report” feature that records the kgCO<sub class="ltx_sub" id="Ch8.S7.SS5.p4.1.1">2</sub> emitted during the development of the model. This aligns closely with the current industry movement towards measuring the environmental impact of NLP activities; indeed, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx51" title="">51</a>]</cite> have demonstrated that high-performing MT systems can be built with much lower footprints, which not only reduce emissions but also in the post-deployment phase deliver savings of almost 50% in energy costs for a real translation company.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S7.SS6">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.6 </span>MLLM Translation and Evaluation</h4>
<div class="ltx_para" id="Ch8.S7.SS6.p1">
<p class="ltx_p" id="Ch8.S7.SS6.p1.1">Besides facilitating model fine-tuning, the application also provides functionality for translation and assessing model performance. The use of pre-trained models for translation is also parameterised; users specify the model’s name as a hyperparameter, which is then used to perform translation and evaluation on the test files.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS6.p2">
<p class="ltx_p" id="Ch8.S7.SS6.p2.1">After building the system, users can select the model they wish to use for translation of the test set. While human judgement is often the most reliable for assessing translation quality, human evaluators are not always accessible, may have differing opinions, and can be costly to engage for experimental purposes. As a result, automatic evaluation metrics are commonly employed, particularly by developers who are tracking the step-by-step advancement of their systems (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx115" title="">115</a>]</cite> for more on the pros and cons of human and automatic evaluation).</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS6.p3">
<p class="ltx_p" id="Ch8.S7.SS6.p3.1">Several automatic evaluation metrics provided by SacreBleu<span class="ltx_note ltx_role_footnote" id="Ch8.footnote32"><sup class="ltx_note_mark">32</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">32</sup><span class="ltx_tag ltx_tag_note">32</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mjpost/sacrebleu" title="">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx91" title="">91</a>]</cite> are used: BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx88" title="">88</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx105" title="">105</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx90" title="">90</a>]</cite>. Translation quality can also be evaluated using Meteor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx34" title="">34</a>]</cite> and F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx80" title="">80</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS6.p4">
<p class="ltx_p" id="Ch8.S7.SS6.p4.1">It’s important to recognise that BLEU, ChrF, Meteor, and F1 are metrics based on precision, thus higher values signify better performance. On the other hand, TER is a metric based on errors, with lower values denoting superior translation quality. The available evaluation options include standard (truecase) and lowercase BLEU scores, along with sentence-level BLEU scoring, as well as ChrF1 and ChrF3.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS6.p5">
<p class="ltx_p" id="Ch8.S7.SS6.p5.1">Logging occurs at three tiers: a model development log for charting progress, an output log from the training console, and a log of the evaluation outcomes. Additionally, there is a references section that provides materials pertinent to the development, utilisation, and comprehension of adaptMLLM. Presently, validation throughout the training process is performed based on model loss.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch8.S7.SS7">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7.7 </span>LLM Playgrounds</h4>
<div class="ltx_para" id="Ch8.S7.SS7.p1">
<p class="ltx_p" id="Ch8.S7.SS7.p1.1">When OpenAI<span class="ltx_note ltx_role_footnote" id="Ch8.footnote33"><sup class="ltx_note_mark">33</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">33</sup><span class="ltx_tag ltx_tag_note">33</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/" title="">https://openai.com/</a></span></span></span> released a playground for its GPT-3 model, the community was quick to create demos. Given that OpenAI’s GPT-3 is proprietary, generating text using its API would incorporate a cost and involve sending data to the site. Ideally, we sought to host an open-source text generation model, and associated playground app in our environment.</p>
</div>
<div class="ltx_para" id="Ch8.S7.SS7.p2">
<p class="ltx_p" id="Ch8.S7.SS7.p2.1">In 2021, Eleuther AI created GPT-J, an open-source text generation model to rival GPT-3 and the model is freely available on the Hugging Face Model Hub allowing us to download variations of this model. In this spirit, we have developed our own fully customisable text generation playground using GPT-J. Using Gradio, a web interface that can interact with these GPT models was developed.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch8.S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.8 </span>Empirical Evaluation</h3>
<div class="ltx_para" id="Ch8.S8.p1">
<p class="ltx_p" id="Ch8.S8.p1.2">After outlining the theoretical framework and the tool itself, we proceed to assess the efficacy of the adaptMLLM methodology by training models for the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.p1.1.m1.1"><semantics id="Ch8.S8.p1.1.m1.1a"><mo id="Ch8.S8.p1.1.m1.1.1" stretchy="false" xref="Ch8.S8.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.p1.1.m1.1b"><ci id="Ch8.S8.p1.1.m1.1.1.cmml" xref="Ch8.S8.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.p1.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.p1.1.m1.1d">↔</annotation></semantics></math>GA and the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.p1.2.m2.1"><semantics id="Ch8.S8.p1.2.m2.1a"><mo id="Ch8.S8.p1.2.m2.1.1" stretchy="false" xref="Ch8.S8.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.p1.2.m2.1b"><ci id="Ch8.S8.p1.2.m2.1.1.cmml" xref="Ch8.S8.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.p1.2.m2.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.p1.2.m2.1d">↔</annotation></semantics></math>MR language pairs.</p>
</div>
<section class="ltx_subsection" id="Ch8.S8.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.8.1 </span>Infrastructure and Hyperparameters</h4>
<div class="ltx_para" id="Ch8.S8.SS1.p1">
<p class="ltx_p" id="Ch8.S8.SS1.p1.1">A Google Colab Pro+ subscription facilitated the rapid development of prototypes using NVIDIA 40GB GPU graphics cards (A100-SXM4-40GB) and compute resources of up to 89GB of system memory when available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx19" title="">19</a>]</cite>. All MT models were trained using the adaptMLLM application.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS1.p2">
<p class="ltx_p" id="Ch8.S8.SS1.p2.1">The DeepSpeed library (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS4" title="8.5.4 DeepSpeed ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5.4</span></a>) is a critical component in making the adaptMLLM system work since it enables our models to be loaded across both GPU and system memory. Without such a library, very significant compute resources would be required which would be prohibitively costly for our team to hire. The hyperparameters used for developing models for both language pairs are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch8.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch8.T1.2.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch8.T1.2.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T1.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T1.2.2.2.1">Epochs</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T1.2.2.2.2">1, 3, <span class="ltx_text ltx_font_bold" id="Ch8.T1.2.2.2.2.1">5</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T1.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T1.2.3.3.1">Batch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T1.2.3.3.2">8, 12, <span class="ltx_text ltx_font_bold" id="Ch8.T1.2.3.3.2.1">16</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T1.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T1.2.4.4.1">Gradient accumulation steps</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T1.2.4.4.2">2, 4, <span class="ltx_text ltx_font_bold" id="Ch8.T1.2.4.4.2.1">8</span>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T1.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T1.2.5.5.1">Learning rate</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T1.2.5.5.2">1e-5, <span class="ltx_text ltx_font_bold" id="Ch8.T1.2.5.5.2.1">3e-5</span>, 9e-5</td>
</tr>
<tr class="ltx_tr" id="Ch8.T1.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T1.2.6.6.1">Weight decay</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T1.2.6.6.2">0.01, <span class="ltx_text ltx_font_bold" id="Ch8.T1.2.6.6.2.1">0.1</span>, 1, 2</td>
</tr>
<tr class="ltx_tr" id="Ch8.T1.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch8.T1.2.7.7.1">Mixed precision</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch8.T1.2.7.7.2">False, <span class="ltx_text ltx_font_bold" id="Ch8.T1.2.7.7.2.1">True</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T1.3.1.1" style="font-size:90%;">Table 8.1</span>: </span><span class="ltx_text" id="Ch8.T1.4.2" style="font-size:90%;">HPO with optimal hyperparameters, within the search space, are highlighted in bold.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch8.S8.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.8.2 </span>Results: Automatic Evaluation</h4>
<div class="ltx_para" id="Ch8.S8.SS2.p1">
<p class="ltx_p" id="Ch8.S8.SS2.p1.1">To determine the quality of our translations, automated metrics were employed. For comparison with our prior studies, the performance of models was gauged using three evaluative metrics: BLEU, TER, and ChrF. These metrics reflect the precision of translations produced by our fine-tuned MLLM systems. We report case-insensitive BLEU scores at the corpus level.</p>
</div>
<section class="ltx_subsubsection" id="Ch8.S8.SS2.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Translation in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx1.1.m1.1b"><mo id="Ch8.S8.SS2.SSSx1.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.1.m1.1c"><ci id="Ch8.S8.SS2.SSSx1.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.1.m1.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.1.m1.1e">↔</annotation></semantics></math>GA directions</h5>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx1.p1">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx1.p1.1">The experimental results from the LoResMT2021 Shared Task in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.p1.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx1.p1.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx1.p1.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.p1.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx1.p1.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.p1.1.m1.1d">↔</annotation></semantics></math>GA directions are summarised in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T2" title="Table 8.2 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.2</span></a>–<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T3" title="Table 8.3 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.3</span></a> and are compared with our experimental findings which adaptMLLM achieved by fine-tuning a 3.3B parameter NLLB MLLM.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx1.p2">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx1.p2.1">The highest-performing EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.p2.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx1.p2.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx1.p2.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.p2.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx1.p2.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.p2.1.m1.1d">→</annotation></semantics></math>GA system in the LoResMT2021 Shared Task was submitted by the ADAPT team <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx64" title="">64</a>]</cite>. The model was developed with an in-house application, adaptNMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx68" title="">68</a>]</cite> using a Transformer architecture. It performed well across all key translation metrics (BLEU: 36.0, TER: 0.531 and ChrF3: 0.6).</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx1.p3">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx1.p3.1">Subsequently, these results were improved upon (BLEU: 37.6, TER: 0.577 and ChrF3: 0.57) by training a Transformer model on a bespoke health dataset, gaHealth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx63" title="">63</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="Ch8.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T2.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch8.T2.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T2.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch8.T2.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T2.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T2.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T2.1.1.1.m1.1"><semantics id="Ch8.T2.1.1.1.m1.1a"><mo id="Ch8.T2.1.1.1.m1.1.1" stretchy="false" xref="Ch8.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T2.1.1.1.m1.1b"><ci id="Ch8.T2.1.1.1.m1.1.1.cmml" xref="Ch8.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T2.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T2.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch8.T2.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch8.T2.2.2.2.m1.1"><semantics id="Ch8.T2.2.2.2.m1.1a"><mo id="Ch8.T2.2.2.2.m1.1.1" stretchy="false" xref="Ch8.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch8.T2.2.2.2.m1.1b"><ci id="Ch8.T2.2.2.2.m1.1.1.cmml" xref="Ch8.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T2.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch8.T2.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T2.3.3.3.m1.1"><semantics id="Ch8.T2.3.3.3.m1.1a"><mo id="Ch8.T2.3.3.3.m1.1.1" stretchy="false" xref="Ch8.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T2.3.3.3.m1.1b"><ci id="Ch8.T2.3.3.3.m1.1.1.cmml" xref="Ch8.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T2.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T2.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T2.3.4.1.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T2.3.4.1.2">en2ga-tuned</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T2.3.4.1.3"><span class="ltx_text ltx_font_bold" id="Ch8.T2.3.4.1.3.1">41.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T2.3.4.1.4"><span class="ltx_text ltx_font_bold" id="Ch8.T2.3.4.1.4.1">0.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T2.3.4.1.5"><span class="ltx_text ltx_font_bold" id="Ch8.T2.3.4.1.5.1">0.48</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T2.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T2.3.5.2.1">adapt</th>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.5.2.2">covid_extended</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.5.2.3">36.0</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.5.2.4">0.531</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.5.2.5">0.60</td>
</tr>
<tr class="ltx_tr" id="Ch8.T2.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T2.3.6.3.1">adapt</th>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.6.3.2">combined</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.6.3.3">32.8</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.6.3.4">0.590</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.6.3.5">0.57</td>
</tr>
<tr class="ltx_tr" id="Ch8.T2.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T2.3.7.4.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.7.4.2">en2ga-baseline</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.7.4.3">29.7</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.7.4.4">0.595</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.7.4.5">0.559</td>
</tr>
<tr class="ltx_tr" id="Ch8.T2.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T2.3.8.5.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.8.5.2">en2ga-b</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.8.5.3">25.8</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.8.5.4">0.629</td>
<td class="ltx_td ltx_align_center" id="Ch8.T2.3.8.5.5">0.53</td>
</tr>
<tr class="ltx_tr" id="Ch8.T2.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch8.T2.3.9.6.1">UCF</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T2.3.9.6.2">en2ga-b</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T2.3.9.6.3">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T2.3.9.6.4">0.756</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T2.3.9.6.5">0.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T2.7.2.1" style="font-size:90%;">Table 8.2</span>: </span><span class="ltx_text" id="Ch8.T2.5.1" style="font-size:90%;">EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T2.5.1.m1.1"><semantics id="Ch8.T2.5.1.m1.1b"><mo id="Ch8.T2.5.1.m1.1.1" stretchy="false" xref="Ch8.T2.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T2.5.1.m1.1c"><ci id="Ch8.T2.5.1.m1.1.1.cmml" xref="Ch8.T2.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T2.5.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T2.5.1.m1.1e">→</annotation></semantics></math>GA: adaptMLLM systems compared with LoResMT2021. The impact of fine-tuning the baseline NLLB model is evident with the BLEU score rising from 29.7 to 41.2 representing a 39% relative improvement. Models developed using adaptMLLM were trained using the optimal hyperparameters set out in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx1.p4">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx1.p4.1">By fine-tuning the NLLB MLLM, using the hyperparameters outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>, a significant improvement in translation performance was achieved. The adaptMLLM EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.p4.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx1.p4.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx1.p4.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.p4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.p4.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx1.p4.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.p4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.p4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.p4.1.m1.1d">→</annotation></semantics></math>GA en2ga system, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T2" title="Table 8.2 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.2</span></a>, achieves a BLEU score of 41.2 which is 5.2 BLEU points higher than our previous score which won the shared task in 2021. This represents a relative improvement of 14%.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx1.p5">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx1.p5.3">For translation in the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.p5.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx1.p5.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx1.p5.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.p5.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx1.p5.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.p5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.p5.1.m1.1d">→</annotation></semantics></math>EN direction, illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T3" title="Table 8.3 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.3</span></a>, the best-performing model for the LoResMT2021 Shared Task was developed by IIITT with a BLEU of 34.6, a TER of 0.586 and ChrF3 of 0.6. Accordingly, this serves as the baseline score by which we can benchmark our GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.p5.2.m2.1"><semantics id="Ch8.S8.SS2.SSSx1.p5.2.m2.1a"><mo id="Ch8.S8.SS2.SSSx1.p5.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.p5.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.p5.2.m2.1b"><ci id="Ch8.S8.SS2.SSSx1.p5.2.m2.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.p5.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.p5.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.p5.2.m2.1d">→</annotation></semantics></math>EN model, developed by fine-tuning a 3.3B parameter NLLB using adaptMLLM. Similar to the results achieved in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx1.p5.3.m3.1"><semantics id="Ch8.S8.SS2.SSSx1.p5.3.m3.1a"><mo id="Ch8.S8.SS2.SSSx1.p5.3.m3.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx1.p5.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx1.p5.3.m3.1b"><ci id="Ch8.S8.SS2.SSSx1.p5.3.m3.1.1.cmml" xref="Ch8.S8.SS2.SSSx1.p5.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx1.p5.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx1.p5.3.m3.1d">→</annotation></semantics></math>GA direction, significant translation performance was observed using this new method. The performance of the adaptMLLM model offers an improvement across all metrics with a BLEU score of 75.1, a TER of 0.385 and a ChrF3 result of 0.71. In particular, the 117% relative improvement in BLEU score against the IIITT system is very significant. The adaptMLLM model is a fine-tuned pre-trained NLLB 3.3B parameter MLLM whereas the IIITT model fine-tuned a smaller Opus MT model from Helsinki NLP. MLLMs and LLMs have already learned to represent natural language patterns and structures from large amounts of data, which can be adapted to specific tasks or domains by updating the model’s parameters with a smaller amount of annotated data. The effect of this approach is demonstrated in the substantially higher BLEU achieved by the adaptMLLM model relative to the IIITT model which was trained on a much smaller Opus model.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx1.p6">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx1.p6.1">The improvement in translation performance is real and not just a BLEU score anomaly given that large improvements were simultaneously observed across the BLEU, TER and CHRF metrics. More specifically, Meta’s nllb-200-3.3B model has a memory footprint of 17.58GB enabling 3.3 billion parameters to be trained compared to the Helsinki-NLP model, opus-mt-ga-en, which is just 295MB and has a correspondingly much smaller set of trainable parameters. Another aspect differentiating the adaptMLLM approach is the relatively broad hyperparameter search space compared to other teams outlined in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T2" title="Table 8.2 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.2</span></a>–<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T3" title="Table 8.3 ‣ Translation in the EN↔GA directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.3</span></a>. We experimented with the number of epochs, the batch size, the gradient accumulation steps, the learning rate, the weight decay and the type of precision used. The exact hyperparameters used are illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch8.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T3.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch8.T3.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T3.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch8.T3.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T3.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T3.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T3.1.1.1.m1.1"><semantics id="Ch8.T3.1.1.1.m1.1a"><mo id="Ch8.T3.1.1.1.m1.1.1" stretchy="false" xref="Ch8.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T3.1.1.1.m1.1b"><ci id="Ch8.T3.1.1.1.m1.1.1.cmml" xref="Ch8.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T3.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T3.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch8.T3.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch8.T3.2.2.2.m1.1"><semantics id="Ch8.T3.2.2.2.m1.1a"><mo id="Ch8.T3.2.2.2.m1.1.1" stretchy="false" xref="Ch8.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch8.T3.2.2.2.m1.1b"><ci id="Ch8.T3.2.2.2.m1.1.1.cmml" xref="Ch8.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T3.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T3.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch8.T3.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T3.3.3.3.m1.1"><semantics id="Ch8.T3.3.3.3.m1.1a"><mo id="Ch8.T3.3.3.3.m1.1.1" stretchy="false" xref="Ch8.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T3.3.3.3.m1.1b"><ci id="Ch8.T3.3.3.3.m1.1.1.cmml" xref="Ch8.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T3.3.4.1.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T3.3.4.1.2">ga2en-tuned</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T3.3.4.1.3"><span class="ltx_text ltx_font_bold" id="Ch8.T3.3.4.1.3.1">75.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T3.3.4.1.4"><span class="ltx_text ltx_font_bold" id="Ch8.T3.3.4.1.4.1">0.385</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T3.3.4.1.5"><span class="ltx_text ltx_font_bold" id="Ch8.T3.3.4.1.5.1">0.71</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T3.3.5.2.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.5.2.2">ga2en-baseline</td>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.5.2.3">47.8</td>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.5.2.4">0.442</td>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.5.2.5">0.692</td>
</tr>
<tr class="ltx_tr" id="Ch8.T3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T3.3.6.3.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.6.3.2">ga2en-b</td>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.6.3.3">34.6</td>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.6.3.4">0.586</td>
<td class="ltx_td ltx_align_center" id="Ch8.T3.3.6.3.5">0.61</td>
</tr>
<tr class="ltx_tr" id="Ch8.T3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch8.T3.3.7.4.1">UCF</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T3.3.7.4.2">ga2en-b</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T3.3.7.4.3">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T3.3.7.4.4">0.711</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T3.3.7.4.5">0.45</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T3.7.2.1" style="font-size:90%;">Table 8.3</span>: </span><span class="ltx_text" id="Ch8.T3.5.1" style="font-size:90%;">GA<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T3.5.1.m1.1"><semantics id="Ch8.T3.5.1.m1.1b"><mo id="Ch8.T3.5.1.m1.1.1" stretchy="false" xref="Ch8.T3.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T3.5.1.m1.1c"><ci id="Ch8.T3.5.1.m1.1.1.cmml" xref="Ch8.T3.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T3.5.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T3.5.1.m1.1e">→</annotation></semantics></math>EN: adaptMLLM systems compared with LoResMT2021. The impact of fine-tuning the baseline NLLB model is evident with the BLEU score rising from 47.8 to 75.1 representing a 57% relative improvement. Models developed using adaptMLLM were trained using the optimal hyperparameters set out in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch8.S8.SS2.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Translation in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx2.1.m1.1b"><mo id="Ch8.S8.SS2.SSSx2.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.1.m1.1c"><ci id="Ch8.S8.SS2.SSSx2.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.1.m1.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.1.m1.1e">↔</annotation></semantics></math>MR directions</h5>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx2.p1">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx2.p1.2">The experimental results from the LoResMT2021 Shared Task in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p1.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx2.p1.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx2.p1.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p1.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx2.p1.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p1.1.m1.1d">↔</annotation></semantics></math>MR directions are summarised in Tables <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T4" title="Table 8.4 ‣ Translation in the EN↔MR directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.4</span></a>–<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T5" title="Table 8.5 ‣ Translation in the EN↔MR directions ‣ 8.8.2 Results: Automatic Evaluation ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5</span></a> and are compared with our experimental findings in developing adaptMLLM. For the shared task, the highest-performing EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p1.2.m2.1"><semantics id="Ch8.S8.SS2.SSSx2.p1.2.m2.1a"><mo id="Ch8.S8.SS2.SSSx2.p1.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p1.2.m2.1b"><ci id="Ch8.S8.SS2.SSSx2.p1.2.m2.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p1.2.m2.1d">→</annotation></semantics></math>MR system was submitted by the IIITT team. Their model used a Transformer architecture and achieved a BLEU score of 34.6, a TER of 0.586 and ChrF3 of 0.61.</p>
</div>
<figure class="ltx_table" id="Ch8.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T4.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch8.T4.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T4.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch8.T4.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T4.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T4.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T4.1.1.1.m1.1"><semantics id="Ch8.T4.1.1.1.m1.1a"><mo id="Ch8.T4.1.1.1.m1.1.1" stretchy="false" xref="Ch8.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T4.1.1.1.m1.1b"><ci id="Ch8.T4.1.1.1.m1.1.1.cmml" xref="Ch8.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T4.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T4.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T4.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch8.T4.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch8.T4.2.2.2.m1.1"><semantics id="Ch8.T4.2.2.2.m1.1a"><mo id="Ch8.T4.2.2.2.m1.1.1" stretchy="false" xref="Ch8.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch8.T4.2.2.2.m1.1b"><ci id="Ch8.T4.2.2.2.m1.1.1.cmml" xref="Ch8.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T4.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T4.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch8.T4.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T4.3.3.3.m1.1"><semantics id="Ch8.T4.3.3.3.m1.1a"><mo id="Ch8.T4.3.3.3.m1.1.1" stretchy="false" xref="Ch8.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T4.3.3.3.m1.1b"><ci id="Ch8.T4.3.3.3.m1.1.1.cmml" xref="Ch8.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T4.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T4.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T4.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T4.3.4.1.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T4.3.4.1.2">en2mr-tuned</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T4.3.4.1.3"><span class="ltx_text ltx_font_bold" id="Ch8.T4.3.4.1.3.1">26.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T4.3.4.1.4"><span class="ltx_text ltx_font_bold" id="Ch8.T4.3.4.1.4.1">0.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T4.3.4.1.5"><span class="ltx_text ltx_font_bold" id="Ch8.T4.3.4.1.5.1">0.608</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T4.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T4.3.5.2.1">IIITT</th>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.5.2.2">en2mr-IndicTrans-b</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.5.2.3">24.2</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.5.2.4">0.59</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.5.2.5">0.597</td>
</tr>
<tr class="ltx_tr" id="Ch8.T4.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T4.3.6.3.1">oneNLP-IIITH</th>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.6.3.2">en2mr-Method2-c</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.6.3.3">22.2</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.6.3.4">0.56</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.6.3.5">0.746</td>
</tr>
<tr class="ltx_tr" id="Ch8.T4.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T4.3.7.4.1">oneNLP-IIITH</th>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.7.4.2">en2mr-Method3-c</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.7.4.3">22.0</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.7.4.4">0.56</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.7.4.5">0.753</td>
</tr>
<tr class="ltx_tr" id="Ch8.T4.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T4.3.8.5.1">oneNLP-IIITH</th>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.8.5.2">en2mr-Method1-c</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.8.5.3">21.5</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.8.5.4">0.56</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.8.5.5">0.746</td>
</tr>
<tr class="ltx_tr" id="Ch8.T4.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T4.3.9.6.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.9.6.2">en2mr-baseline</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.9.6.3">19.8</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.9.6.4">0.656</td>
<td class="ltx_td ltx_align_center" id="Ch8.T4.3.9.6.5">0.57</td>
</tr>
<tr class="ltx_tr" id="Ch8.T4.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch8.T4.3.10.7.1">adaptNMT</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T4.3.10.7.2">en2mr</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T4.3.10.7.3">13.7</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T4.3.10.7.4">0.778</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T4.3.10.7.5">0.393</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T4.7.2.1" style="font-size:90%;">Table 8.4</span>: </span><span class="ltx_text" id="Ch8.T4.5.1" style="font-size:90%;">EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T4.5.1.m1.1"><semantics id="Ch8.T4.5.1.m1.1b"><mo id="Ch8.T4.5.1.m1.1.1" stretchy="false" xref="Ch8.T4.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T4.5.1.m1.1c"><ci id="Ch8.T4.5.1.m1.1.1.cmml" xref="Ch8.T4.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T4.5.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T4.5.1.m1.1e">→</annotation></semantics></math>MR: adaptMLLM systems compared with LoResMT2021. The impact of fine-tuning the baseline NLLB model is evident with the BLEU score rising from 19.8 to 26.4 representing a 33% relative improvement. Models developed using adaptMLLM were trained using the optimal hyperparameters set out in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ch8.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T5.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch8.T5.3.3.4.1">Team</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T5.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch8.T5.3.3.5.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T5.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T5.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T5.1.1.1.m1.1"><semantics id="Ch8.T5.1.1.1.m1.1a"><mo id="Ch8.T5.1.1.1.m1.1.1" stretchy="false" xref="Ch8.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T5.1.1.1.m1.1b"><ci id="Ch8.T5.1.1.1.m1.1.1.cmml" xref="Ch8.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T5.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T5.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch8.T5.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch8.T5.2.2.2.m1.1"><semantics id="Ch8.T5.2.2.2.m1.1a"><mo id="Ch8.T5.2.2.2.m1.1.1" stretchy="false" xref="Ch8.T5.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch8.T5.2.2.2.m1.1b"><ci id="Ch8.T5.2.2.2.m1.1.1.cmml" xref="Ch8.T5.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T5.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T5.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T5.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch8.T5.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T5.3.3.3.m1.1"><semantics id="Ch8.T5.3.3.3.m1.1a"><mo id="Ch8.T5.3.3.3.m1.1.1" stretchy="false" xref="Ch8.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T5.3.3.3.m1.1b"><ci id="Ch8.T5.3.3.3.m1.1.1.cmml" xref="Ch8.T5.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T5.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T5.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T5.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T5.3.4.1.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T5.3.4.1.2">mr2en-tuned</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T5.3.4.1.3"><span class="ltx_text ltx_font_bold" id="Ch8.T5.3.4.1.3.1">52.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T5.3.4.1.4"><span class="ltx_text ltx_font_bold" id="Ch8.T5.3.4.1.4.1">0.409</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T5.3.4.1.5"><span class="ltx_text ltx_font_bold" id="Ch8.T5.3.4.1.5.1">0.704</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T5.3.5.2.1">adaptMLLM</th>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.5.2.2">mr2en-baseline</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.5.2.3">42.7</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.5.2.4">0.506</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.5.2.5">0.639</td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T5.3.6.3.1">oneNLP-IIITH</th>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.6.3.2">mr2en-Method3-c</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.6.3.3">31.3</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.6.3.4">0.58</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.6.3.5">0.646</td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T5.3.7.4.1">oneNLP-IIITH</th>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.7.4.2">mr2en-Method2-c</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.7.4.3">30.6</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.7.4.4">0.57</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.7.4.5">0.659</td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T5.3.8.5.1">oneNLP-IIITH</th>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.8.5.2">mr2en-Method1-c</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.8.5.3">20.7</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.8.5.4">0.48</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.8.5.5">0.735</td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T5.3.9.6.1">adaptNMT</th>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.9.6.2">mr2en</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.9.6.3">19.9</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.9.6.4">0.758</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.9.6.5">0.429</td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch8.T5.3.10.7.1">UCF</th>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.10.7.2">mr2en-UnigramSegmentation-b</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.10.7.3">7.7</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.10.7.4">0.24</td>
<td class="ltx_td ltx_align_center" id="Ch8.T5.3.10.7.5">0.833</td>
</tr>
<tr class="ltx_tr" id="Ch8.T5.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch8.T5.3.11.8.1">IIITT</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T5.3.11.8.2">mr2en-IndicTrans-b</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T5.3.11.8.3">5.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T5.3.11.8.4">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T5.3.11.8.5">1.002</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T5.7.2.1" style="font-size:90%;">Table 8.5</span>: </span><span class="ltx_text" id="Ch8.T5.5.1" style="font-size:90%;">MR<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T5.5.1.m1.1"><semantics id="Ch8.T5.5.1.m1.1b"><mo id="Ch8.T5.5.1.m1.1.1" stretchy="false" xref="Ch8.T5.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T5.5.1.m1.1c"><ci id="Ch8.T5.5.1.m1.1.1.cmml" xref="Ch8.T5.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T5.5.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T5.5.1.m1.1e">→</annotation></semantics></math>EN: adaptMLLM systems compared with LoResMT2021. The impact of fine-tuning the baseline NLLB model is evident with the BLEU score rising from 42.7 to 52.6 representing a 23% relative improvement. Models developed using adaptMLLM were trained using the optimal hyperparameters set out in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T1" title="Table 8.1 ‣ 8.8.1 Infrastructure and Hyperparameters ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx2.p2">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx2.p2.3">Again the approach taken by adaptMLLM in fine-tuning a 3.3.B parameter NLLB MLLM yielded the best performance compared with other systems entered for the shared task. The EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p2.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx2.p2.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx2.p2.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p2.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx2.p2.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p2.1.m1.1d">→</annotation></semantics></math>MR adaptMLLM en2mr system achieves the highest BLEU score of 26.4 compared with IIITT, the winning team in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p2.2.m2.1"><semantics id="Ch8.S8.SS2.SSSx2.p2.2.m2.1a"><mo id="Ch8.S8.SS2.SSSx2.p2.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p2.2.m2.1b"><ci id="Ch8.S8.SS2.SSSx2.p2.2.m2.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p2.2.m2.1d">→</annotation></semantics></math>MR Shared task. IIITT had a BLEU score of 24.2 which represents a relative improvement of 9% for the adaptMLLM system. The other key translation metrics of TER and ChrF3 were also improved upon indicating that the adaptMLLM system is the best approach in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p2.3.m3.1"><semantics id="Ch8.S8.SS2.SSSx2.p2.3.m3.1a"><mo id="Ch8.S8.SS2.SSSx2.p2.3.m3.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p2.3.m3.1b"><ci id="Ch8.S8.SS2.SSSx2.p2.3.m3.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p2.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p2.3.m3.1d">→</annotation></semantics></math>MR direction.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS2.SSSx2.p3">
<p class="ltx_p" id="Ch8.S8.SS2.SSSx2.p3.2">For translation in the MR<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p3.1.m1.1"><semantics id="Ch8.S8.SS2.SSSx2.p3.1.m1.1a"><mo id="Ch8.S8.SS2.SSSx2.p3.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p3.1.m1.1b"><ci id="Ch8.S8.SS2.SSSx2.p3.1.m1.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p3.1.m1.1c">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p3.1.m1.1d">→</annotation></semantics></math>EN direction, the best-performing model for the LoResMT2021 Shared Task was developed by oneNLP-IIITT with a BLEU score of 31.3, a TER of 0.58 and ChrF3 of 0.646. This serves as the baseline score by which our MR<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.SS2.SSSx2.p3.2.m2.1"><semantics id="Ch8.S8.SS2.SSSx2.p3.2.m2.1a"><mo id="Ch8.S8.SS2.SSSx2.p3.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS2.SSSx2.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS2.SSSx2.p3.2.m2.1b"><ci id="Ch8.S8.SS2.SSSx2.p3.2.m2.1.1.cmml" xref="Ch8.S8.SS2.SSSx2.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS2.SSSx2.p3.2.m2.1c">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS2.SSSx2.p3.2.m2.1d">→</annotation></semantics></math>EN model, developed using adaptMLLM, can be benchmarked. The performance of the adaptMLLM model offers a significant improvement across all metrics with a BLEU score of 52.6, a TER of 0.409 and a ChrF3 of 0.704. Again this represents a very strong relative improvement of 68% in BLEU compared with the winning team from the shared task.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch8.S8.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.8.3 </span>Human Evaluation Results</h4>
<div class="ltx_para" id="Ch8.S8.SS3.p1">
<p class="ltx_p" id="Ch8.S8.SS3.p1.1">Irish, characterised by its complex morphology, flexible sentence structure, and extensive inflection, presents unique challenges in translation from English. As a result, accurately producing grammatical aspects like gender or case inflections in nouns within Irish translations often proves to be a difficult task.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.p2">
<p class="ltx_p" id="Ch8.S8.SS3.p2.1">This research aims to investigate how a neural machine translation (NMT) system, like a fine-tuned NLLB model, manages these linguistic complexities. Current studies imply that fine-tuned MLLMs are likely to enhance these language features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx32" title="">32</a>]</cite>. MLLMs and LLMs tackle the issue indirectly through subword models in an unsupervised fashion, without grasping the explicit formal principles of grammatical categories.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.p3">
<p class="ltx_p" id="Ch8.S8.SS3.p3.3">Past human evaluation studies examining EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.p3.1.m1.1"><semantics id="Ch8.S8.SS3.p3.1.m1.1a"><mo id="Ch8.S8.SS3.p3.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.p3.1.m1.1b"><ci id="Ch8.S8.SS3.p3.1.m1.1.1.cmml" xref="Ch8.S8.SS3.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.p3.1.m1.1d">→</annotation></semantics></math>GA MT performance have centred on outputs from NMT systems that did not use pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx66" title="">66</a>]</cite>. In the context of this research, we now conduct a human evaluation of the output from our MLLM models. The work is further differentiated in that it examines the output in both the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.p3.2.m2.1"><semantics id="Ch8.S8.SS3.p3.2.m2.1a"><mo id="Ch8.S8.SS3.p3.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS3.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.p3.2.m2.1b"><ci id="Ch8.S8.SS3.p3.2.m2.1.1.cmml" xref="Ch8.S8.SS3.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.p3.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.p3.2.m2.1d">→</annotation></semantics></math>GA and GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.p3.3.m3.1"><semantics id="Ch8.S8.SS3.p3.3.m3.1a"><mo id="Ch8.S8.SS3.p3.3.m3.1.1" stretchy="false" xref="Ch8.S8.SS3.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.p3.3.m3.1b"><ci id="Ch8.S8.SS3.p3.3.m3.1.1.cmml" xref="Ch8.S8.SS3.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.p3.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.p3.3.m3.1d">→</annotation></semantics></math>EN directions. The approach taken in the previous study and our current work are similar in that we use SQM and MQM as our human evaluation metrics.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.p4">
<p class="ltx_p" id="Ch8.S8.SS3.p4.1">While automatic evaluation metrics show that a fine-tuned MLLM approach leads to significant improvements compared to building a Transformer model from scratch, it fails to address the issue of grammatical or linguistic quality in the translated output. Such an approach does not account for the subtleties of handling gender or cases in the target language. To gain a more comprehensive understanding of the linguistic errors produced by MLLM systems, a fine-grained human evaluation was conducted through a manual error analysis. This approach allowed for the identification and categorisation of specific translation errors associated with each of the evaluated systems, providing a foundation for future work aimed at improving the translation quality of the models.
</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.p5">
<p class="ltx_p" id="Ch8.S8.SS3.p5.1">We also describe the annotation framework, the overall annotation process, and the level of agreement among annotators, which broadly follows the approach taken by other fine-grained human evaluation studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx66" title="">66</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="Ch8.S8.SS3.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Scalar Quality Metrics</h5>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx1.p1">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx1.p1.1">The SQM framework modifies the WMT shared-task settings to acquire segment-level scalar ratings with document context. SQM assesses the quality of translations using a scale that ranges from 0 to 6, which is different from the WMT approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx77" title="">77</a>]</cite>, which employs a range of 0 to 100.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx1.p2">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx1.p2.1">When using this evaluation method, annotators are required to choose a rating ranging from 0 to 6 after being presented with the source and target sentences. Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T6" title="Table 8.6 ‣ Scalar Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.6</span></a> provides the SQM quality levels for ratings 0, 2, 4, and 6. In situations where the translations do not precisely align with the core SQM levels, annotators may select intermediate ratings of 1, 3, or 5.</p>
</div>
<figure class="ltx_table" id="Ch8.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8.6: </span>SQM levels explained <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T6.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T6.7.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T6.7.1.1.1" style="padding-left:23.2pt;padding-right:23.2pt;">
<span class="ltx_text ltx_font_bold" id="Ch8.T6.7.1.1.1.1" style="font-size:90%;">SQM Level</span><span class="ltx_text" id="Ch8.T6.7.1.1.1.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T6.7.1.1.2" style="width:284.5pt;padding-left:23.2pt;padding-right:23.2pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T6.7.1.1.2.1" style="font-size:90%;">Details of Quality</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T6.7.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch8.T6.7.2.1.1" style="padding-left:23.2pt;padding-right:23.2pt;"><span class="ltx_text" id="Ch8.T6.7.2.1.1.1" style="font-size:90%;">6</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T6.7.2.1.2" style="width:284.5pt;padding-left:23.2pt;padding-right:23.2pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T6.7.2.1.2.1"><span class="ltx_text" id="Ch8.T6.7.2.1.2.1.1" style="font-size:90%;">Perfect Meaning and Grammar: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable). The grammar is also correct.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T6.7.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch8.T6.7.3.2.1" style="padding-left:23.2pt;padding-right:23.2pt;"><span class="ltx_text" id="Ch8.T6.7.3.2.1.1" style="font-size:90%;">4</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T6.7.3.2.2" style="width:284.5pt;padding-left:23.2pt;padding-right:23.2pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T6.7.3.2.2.1"><span class="ltx_text" id="Ch8.T6.7.3.2.2.1.1" style="font-size:90%;">Most Meaning Preserved and Few Grammar Mistakes: The translation retains most of the meaning of the source. This may contain some grammar mistakes or minor contextual inconsistencies.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T6.7.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch8.T6.7.4.3.1" style="padding-left:23.2pt;padding-right:23.2pt;"><span class="ltx_text" id="Ch8.T6.7.4.3.1.1" style="font-size:90%;">2</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T6.7.4.3.2" style="width:284.5pt;padding-left:23.2pt;padding-right:23.2pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T6.7.4.3.2.1"><span class="ltx_text" id="Ch8.T6.7.4.3.2.1.1" style="font-size:90%;">Some Meaning Preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow due to fundamental errors. Grammar may be poor.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T6.7.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch8.T6.7.5.4.1" style="padding-left:23.2pt;padding-right:23.2pt;"><span class="ltx_text" id="Ch8.T6.7.5.4.1.1" style="font-size:90%;">0</span></th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T6.7.5.4.2" style="width:284.5pt;padding-left:23.2pt;padding-right:23.2pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T6.7.5.4.2.1"><span class="ltx_text" id="Ch8.T6.7.5.4.2.1.1" style="font-size:90%;">Nonsense/No meaning preserved: Nearly all information is lost between the translation and source. Grammar is irrelevant.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx1.p3">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx1.p3.3">The average annotator SQM scores arising from our human evaluation were compared with automatic metric scores recorded by adpatMLLM when evaluating the EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx1.p3.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx1.p3.1.m1.1a"><mo id="Ch8.S8.SS3.SSSx1.p3.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx1.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx1.p3.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx1.p3.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx1.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx1.p3.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx1.p3.1.m1.1d">↔</annotation></semantics></math>GA systems. These results, illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T7" title="Table 8.7 ‣ Scalar Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.7</span></a> indicate a high level of correlation between the automatic metrics and the SQM outputs of the human evaluation. Clearly, the system translating in the GA<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx1.p3.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx1.p3.2.m2.1a"><mo id="Ch8.S8.SS3.SSSx1.p3.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx1.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx1.p3.2.m2.1b"><ci id="Ch8.S8.SS3.SSSx1.p3.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx1.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx1.p3.2.m2.1c">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx1.p3.2.m2.1d">→</annotation></semantics></math>EN direction performs better, when evaluated using both automatic and human evaluation, than its counterpart when translating in the opposite direction. These results are consistent with our previous work which also shows better GA<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx1.p3.3.m3.1"><semantics id="Ch8.S8.SS3.SSSx1.p3.3.m3.1a"><mo id="Ch8.S8.SS3.SSSx1.p3.3.m3.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx1.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx1.p3.3.m3.1b"><ci id="Ch8.S8.SS3.SSSx1.p3.3.m3.1.1.cmml" xref="Ch8.S8.SS3.SSSx1.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx1.p3.3.m3.1c">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx1.p3.3.m3.1d">→</annotation></semantics></math>EN translation performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx68" title="">68</a>]</cite>. This performance difference is attributed to the morphological-rich nature of the Irish language which relies heavily on inflection, derivation and its case system.</p>
</div>
<figure class="ltx_table" id="Ch8.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T7.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T7.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T7.4.4.5"><span class="ltx_text ltx_font_bold" id="Ch8.T7.4.4.5.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T7.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T7.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T7.1.1.1.m1.1"><semantics id="Ch8.T7.1.1.1.m1.1a"><mo id="Ch8.T7.1.1.1.m1.1.1" stretchy="false" xref="Ch8.T7.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T7.1.1.1.m1.1b"><ci id="Ch8.T7.1.1.1.m1.1.1.cmml" xref="Ch8.T7.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T7.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T7.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T7.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch8.T7.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch8.T7.2.2.2.m1.1"><semantics id="Ch8.T7.2.2.2.m1.1a"><mo id="Ch8.T7.2.2.2.m1.1.1" stretchy="false" xref="Ch8.T7.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch8.T7.2.2.2.m1.1b"><ci id="Ch8.T7.2.2.2.m1.1.1.cmml" xref="Ch8.T7.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T7.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T7.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T7.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch8.T7.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T7.3.3.3.m1.1"><semantics id="Ch8.T7.3.3.3.m1.1a"><mo id="Ch8.T7.3.3.3.m1.1.1" stretchy="false" xref="Ch8.T7.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T7.3.3.3.m1.1b"><ci id="Ch8.T7.3.3.3.m1.1.1.cmml" xref="Ch8.T7.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T7.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T7.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T7.4.4.4">
<span class="ltx_text ltx_font_bold" id="Ch8.T7.4.4.4.1">SQM</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T7.4.4.4.m1.1"><semantics id="Ch8.T7.4.4.4.m1.1a"><mo id="Ch8.T7.4.4.4.m1.1.1" stretchy="false" xref="Ch8.T7.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T7.4.4.4.m1.1b"><ci id="Ch8.T7.4.4.4.m1.1.1.cmml" xref="Ch8.T7.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T7.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T7.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T7.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T7.4.5.1.1">adaptMLLM en2ga</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T7.4.5.1.2">41.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T7.4.5.1.3">0.51</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T7.4.5.1.4">0.48</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T7.4.5.1.5">4.38</td>
</tr>
<tr class="ltx_tr" id="Ch8.T7.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Ch8.T7.4.6.2.1">adaptMLLM ga2en</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T7.4.6.2.2">75.1</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T7.4.6.2.3">0.385</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T7.4.6.2.4">0.71</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T7.4.6.2.5">5.63</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T7.6.1.1" style="font-size:90%;">Table 8.7</span>: </span><span class="ltx_text" id="Ch8.T7.7.2" style="font-size:90%;">Average SQM scores for adaptMLLM systems compared with automatic metrics.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch8.S8.SS3.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Multidimensional Quality Metrics</h5>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx2.p1">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx2.p1.1">Within the QTLaunchpad project,<span class="ltx_note ltx_role_footnote" id="Ch8.footnote34"><sup class="ltx_note_mark">34</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">34</sup><span class="ltx_tag ltx_tag_note">34</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.qt21.eu" title="">https://www.qt21.eu</a></span></span></span> the development of the MQM framework<span class="ltx_note ltx_role_footnote" id="Ch8.footnote35"><sup class="ltx_note_mark">35</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">35</sup><span class="ltx_tag ltx_tag_note">35</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://themqm.org/the-mqm-full-typology/" title="">https://themqm.org/the-mqm-full-typology/</a></span></span></span> aimed to offer a structured approach to conducting manual evaluations through meticulous error analysis. This framework does not mandate a uniform metric for all applications; rather, it supplies an extensive list of potential quality issues, each with standardised names and definitions, which can be tailored to particular tasks. Beyond establishing a dependable method for quality evaluation, the MQM framework also enables us to identify and select error tags pertinent to our specific task.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx2.p2">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx2.p2.1">We customised the MQM framework to suit our context by following the official scientific research guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx74" title="">74</a>]</cite>. Our modifications to MQM are explained below.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx2.p3">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx2.p3.1">The original MQM guidelines propose a wide range of tags on different annotation layers. However, for our specific annotation task, this comprehensive tagset is too detailed. Hence, we evaluated our MT output using the smaller default set of evaluation categories outlined in the core tagset. These standard top-level categories, which include accuracy and fluency, are recommended by the MQM guidelines and are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T8" title="Table 8.8 ‣ Multidimensional Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.8</span></a>.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx2.p4">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx2.p4.1">We used a special non-translation error tag to label entire sentences that were so poorly translated that individual errors could not be identified. Error severities were designated as major or minor errors, and they were assigned independently of the category. These corresponded to actual translation or grammatical errors and minor imperfections, respectively. We used the default recommended weights <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx74" title="">74</a>]</cite> which assign a weight of 1 to minor errors, while major errors are given a weight of 10. Additionally, the non-translation category was assigned a weight of 25, which is consistent with best practices established in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx2.p5">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx2.p5.1">Our annotators were instructed to identify all errors in each sentence of the translated output using the error categories provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T8" title="Table 8.8 ‣ Multidimensional Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.8</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch8.T8">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8.8: </span>Description of error categories within the core MQM framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx40" title="">40</a>]</cite>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch8.T8.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T8.4.1.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.1.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T8.4.1.1.1.1" style="font-size:90%;">Category</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.1.1.2" style="width:99.6pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T8.4.1.1.2.1" style="font-size:90%;">Sub-Category</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.1.1.3" style="width:221.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T8.4.1.1.3.1" style="font-size:90%;">Description</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.2.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.2.2.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T8.4.2.2.1.1" style="font-size:90%;">Non-translation</span></td>
<td class="ltx_td ltx_border_t" id="Ch8.T8.4.2.2.2" style="width:99.6pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.2.2.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.2.2.3.1"><span class="ltx_text" id="Ch8.T8.4.2.2.3.1.1" style="font-size:90%;">Impossible to reliably characterise the 5 most severe errors.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.3.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.3.3.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T8.4.3.3.1.1" style="font-size:90%;">Accuracy</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.3.3.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.3.3.2.1"><span class="ltx_text" id="Ch8.T8.4.3.3.2.1.1" style="font-size:90%;">Addition</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.3.3.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.3.3.3.1"><span class="ltx_text" id="Ch8.T8.4.3.3.3.1.1" style="font-size:90%;">Translation includes information not present in the source.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.4.4">
<td class="ltx_td" id="Ch8.T8.4.4.4.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.4.4.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.4.4.2.1"><span class="ltx_text" id="Ch8.T8.4.4.4.2.1.1" style="font-size:90%;">Omission</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.4.4.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.4.4.3.1"><span class="ltx_text" id="Ch8.T8.4.4.4.3.1.1" style="font-size:90%;">Translation is missing content from the source.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.5.5">
<td class="ltx_td" id="Ch8.T8.4.5.5.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.5.5.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.5.5.2.1"><span class="ltx_text" id="Ch8.T8.4.5.5.2.1.1" style="font-size:90%;">Mistranslation</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.5.5.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.5.5.3.1"><span class="ltx_text" id="Ch8.T8.4.5.5.3.1.1" style="font-size:90%;">Translation does not accurately represent the source.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.6.6">
<td class="ltx_td" id="Ch8.T8.4.6.6.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.6.6.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.6.6.2.1"><span class="ltx_text" id="Ch8.T8.4.6.6.2.1.1" style="font-size:90%;">Untranslated text</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.6.6.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.6.6.3.1"><span class="ltx_text" id="Ch8.T8.4.6.6.3.1.1" style="font-size:90%;">Source text has been left untranslated.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.7.7">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.7.7.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T8.4.7.7.1.1" style="font-size:90%;">Fluency</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.7.7.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.7.7.2.1"><span class="ltx_text" id="Ch8.T8.4.7.7.2.1.1" style="font-size:90%;">Punctuation</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T8.4.7.7.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.7.7.3.1"><span class="ltx_text" id="Ch8.T8.4.7.7.3.1.1" style="font-size:90%;">Incorrect punctuation</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.8.8">
<td class="ltx_td" id="Ch8.T8.4.8.8.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.8.8.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.8.8.2.1"><span class="ltx_text" id="Ch8.T8.4.8.8.2.1.1" style="font-size:90%;">Spelling</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.8.8.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.8.8.3.1"><span class="ltx_text" id="Ch8.T8.4.8.8.3.1.1" style="font-size:90%;">Incorrect spelling or capitalisation.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.9.9">
<td class="ltx_td" id="Ch8.T8.4.9.9.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.9.9.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.9.9.2.1"><span class="ltx_text" id="Ch8.T8.4.9.9.2.1.1" style="font-size:90%;">Grammar</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.9.9.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.9.9.3.1"><span class="ltx_text" id="Ch8.T8.4.9.9.3.1.1" style="font-size:90%;">Problems with grammar, other than orthography.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.10.10">
<td class="ltx_td" id="Ch8.T8.4.10.10.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.10.10.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.10.10.2.1"><span class="ltx_text" id="Ch8.T8.4.10.10.2.1.1" style="font-size:90%;">Register</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.10.10.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.10.10.3.1"><span class="ltx_text" id="Ch8.T8.4.10.10.3.1.1" style="font-size:90%;">Wrong grammatical register (e.g., inappropriately informal pronouns).</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.11.11">
<td class="ltx_td" id="Ch8.T8.4.11.11.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.11.11.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.11.11.2.1"><span class="ltx_text" id="Ch8.T8.4.11.11.2.1.1" style="font-size:90%;">Inconsistency</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="Ch8.T8.4.11.11.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.11.11.3.1"><span class="ltx_text" id="Ch8.T8.4.11.11.3.1.1" style="font-size:90%;">Internal inconsistency (not related to terminology).</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T8.4.12.12">
<td class="ltx_td ltx_border_b" id="Ch8.T8.4.12.12.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_b" id="Ch8.T8.4.12.12.2" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.12.12.2.1"><span class="ltx_text" id="Ch8.T8.4.12.12.2.1.1" style="font-size:90%;">Character encoding</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b" id="Ch8.T8.4.12.12.3" style="width:221.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T8.4.12.12.3.1"><span class="ltx_text" id="Ch8.T8.4.12.12.3.1.1" style="font-size:90%;">Characters are garbled due to incorrect encoding.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch8.S8.SS3.SSSx3">
<h5 class="ltx_title ltx_title_subsubsection">Annotation Setup</h5>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx3.p1">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx3.p1.1">Annotations were carried out using a detailed, fine-grained MQM approach and a simpler SQM approach. The SQM categories are summarised in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T6" title="Table 8.6 ‣ Scalar Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.6</span></a> whereas the hierarchical taxonomy of our MQM implementation is outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T8" title="Table 8.8 ‣ Multidimensional Quality Metrics ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.8</span></a>.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx3.p2">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx3.p2.1">Working independently of one another, two annotators with similar backgrounds were selected for the annotation of fine-tuned EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx3.p2.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx3.p2.1.m1.1a"><mo id="Ch8.S8.SS3.SSSx3.p2.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx3.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx3.p2.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx3.p2.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx3.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx3.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx3.p2.1.m1.1d">↔</annotation></semantics></math>GA systems. Both annotators are fluent speakers of Irish and neither had prior experience with MQM. The annotators are postgraduate students of the Máistir Gairmiúil san Oideas (Postgraduate Masters in Education) at the University of Galway.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote36"><sup class="ltx_note_mark">36</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">36</sup><span class="ltx_tag ltx_tag_note">36</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://universityofgalway.ie" title="">https://universityofgalway.ie</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx3.p3">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx3.p3.1">Before starting the annotation process, they were extensively briefed on the process and the MQM annotation guidelines. These guidelines provide in-depth directions for carrying out annotation activities under the MQM framework.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx3.p4">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx3.p4.2">In conducting the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx3.p4.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx3.p4.1.m1.1a"><mo id="Ch8.S8.SS3.SSSx3.p4.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx3.p4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx3.p4.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx3.p4.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx3.p4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx3.p4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx3.p4.1.m1.1d">→</annotation></semantics></math>GA human evaluation of the translation output, we presented our annotators with a test set of 25 randomly selected sentences, which consisted of the English source text, an Irish reference translation and the unannotated fine-tuned MLLM EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx3.p4.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx3.p4.2.m2.1a"><mo id="Ch8.S8.SS3.SSSx3.p4.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx3.p4.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx3.p4.2.m2.1b"><ci id="Ch8.S8.SS3.SSSx3.p4.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx3.p4.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx3.p4.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx3.p4.2.m2.1d">→</annotation></semantics></math>GA system output.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx3.p5">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx3.p5.2">A similar approach was adopted for the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx3.p5.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx3.p5.1.m1.1a"><mo id="Ch8.S8.SS3.SSSx3.p5.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx3.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx3.p5.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx3.p5.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx3.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx3.p5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx3.p5.1.m1.1d">→</annotation></semantics></math>EN human evaluation where the annotator test set consisted of 25 randomly selected sentences, which consisted of the Irish source text, an English reference translation and the unannotated fine-tuned MLLM GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx3.p5.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx3.p5.2.m2.1a"><mo id="Ch8.S8.SS3.SSSx3.p5.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx3.p5.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx3.p5.2.m2.1b"><ci id="Ch8.S8.SS3.SSSx3.p5.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx3.p5.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx3.p5.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx3.p5.2.m2.1d">→</annotation></semantics></math>EN system output.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx3.p6">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx3.p6.1">After extracting the annotation data, the annotators individually examined the output to assess the performance of each system across the different error categories.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch8.S8.SS3.SSSx4">
<h5 class="ltx_title ltx_title_subsubsection">Inter-Annotator Agreement</h5>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx4.p1">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx4.p1.2">To ensure the validity of our research findings, it is essential to assess the degree of consensus among our annotators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx8" title="">8</a>]</cite>. Manual evaluation methods for MT, such as MQM, often result in low inter-annotator agreement (IAA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx25" title="">25</a>]</cite>. We computed inter-annotator agreement using Cohen’s <math alttext="kappa" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p1.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx4.p1.1.m1.1a"><mrow id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.2" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.2.cmml">k</mi><mo id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.3" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.3.cmml">a</mi><mo id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1a" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.4" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.4.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1b" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.5" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.5.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1c" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.6" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p1.1.m1.1b"><apply id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1"><times id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.2">𝑘</ci><ci id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.3">𝑎</ci><ci id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.4.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.4">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.5.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.5">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.6.cmml" xref="Ch8.S8.SS3.SSSx4.p1.1.m1.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p1.1.m1.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p1.1.m1.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> (<math alttext="k" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p1.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx4.p1.2.m2.1a"><mi id="Ch8.S8.SS3.SSSx4.p1.2.m2.1.1" xref="Ch8.S8.SS3.SSSx4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p1.2.m2.1b"><ci id="Ch8.S8.SS3.SSSx4.p1.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p1.2.m2.1d">italic_k</annotation></semantics></math>) coefficient <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx30" title="">30</a>]</cite>, a widely recognised metric in the field. The evaluation was performed at the sentence level for each system, and the agreement discrepancies across systems were examined. This approach also allowed us to obtain an overall view of the level of agreement between annotators.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx4.p2">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx4.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T9" title="Table 8.9 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.9</span></a> highlights the cumulative number of errors identified by the annotators for each system. Looking at the aggregate data alone, it is evident that both annotators have judged the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p2.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx4.p2.1.m1.1a"><mo id="Ch8.S8.SS3.SSSx4.p2.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx4.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p2.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx4.p2.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p2.1.m1.1d">→</annotation></semantics></math>GA system to contain significantly more errors which supports the findings of the automatic evaluation.</p>
</div>
<figure class="ltx_table" id="Ch8.T9">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8.9: </span>System errors found by each annotator using the MQM metric</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T9.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T9.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Ch8.T9.2.2.3"><span class="ltx_text ltx_font_bold" id="Ch8.T9.2.2.3.1" style="font-size:90%;">Num Errors</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T9.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch8.T9.1.1.1.1" style="font-size:90%;">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T9.1.1.1.1.m1.1"><semantics id="Ch8.T9.1.1.1.1.m1.1a"><mo id="Ch8.T9.1.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch8.T9.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T9.1.1.1.1.m1.1b"><ci id="Ch8.T9.1.1.1.1.m1.1.1.cmml" xref="Ch8.T9.1.1.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T9.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T9.1.1.1.1.m1.1d">→</annotation></semantics></math>GA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T9.2.2.2"><span class="ltx_text ltx_font_bold" id="Ch8.T9.2.2.2.1" style="font-size:90%;">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T9.2.2.2.1.m1.1"><semantics id="Ch8.T9.2.2.2.1.m1.1a"><mo id="Ch8.T9.2.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch8.T9.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T9.2.2.2.1.m1.1b"><ci id="Ch8.T9.2.2.2.1.m1.1.1.cmml" xref="Ch8.T9.2.2.2.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T9.2.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T9.2.2.2.1.m1.1d">→</annotation></semantics></math>EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T9.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch8.T9.2.3.1.1"><span class="ltx_text" id="Ch8.T9.2.3.1.1.1" style="font-size:90%;">Annotator 1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T9.2.3.1.2"><span class="ltx_text" id="Ch8.T9.2.3.1.2.1" style="font-size:90%;">53</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T9.2.3.1.3"><span class="ltx_text" id="Ch8.T9.2.3.1.3.1" style="font-size:90%;">7</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T9.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="Ch8.T9.2.4.2.1"><span class="ltx_text" id="Ch8.T9.2.4.2.1.1" style="font-size:90%;">Annotator 2</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch8.T9.2.4.2.2"><span class="ltx_text" id="Ch8.T9.2.4.2.2.1" style="font-size:90%;">82</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch8.T9.2.4.2.3"><span class="ltx_text" id="Ch8.T9.2.4.2.3.1" style="font-size:90%;">11</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx4.p3">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx4.p3.3">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T9" title="Table 8.9 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.9</span></a> provides a useful overview for evaluating which system performs better overall, but it does not offer the detailed analysis necessary to identify specific linguistic areas for improvement in the translations. For a more comprehensive understanding, we delved into a detailed examination of the types of errors present, with the findings presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T10" title="Table 8.10 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.10</span></a>. This table breaks down the total number of error tags noted by each annotator for each system, categorised by the type of error. The detailed analysis underscores how the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p3.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx4.p3.1.m1.1a"><mo id="Ch8.S8.SS3.SSSx4.p3.1.m1.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx4.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p3.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx4.p3.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p3.1.m1.1d">→</annotation></semantics></math>EN system outperforms the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p3.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx4.p3.2.m2.1a"><mo id="Ch8.S8.SS3.SSSx4.p3.2.m2.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx4.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p3.2.m2.1b"><ci id="Ch8.S8.SS3.SSSx4.p3.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p3.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p3.2.m2.1d">→</annotation></semantics></math>GA system. Notably, the GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p3.3.m3.1"><semantics id="Ch8.S8.SS3.SSSx4.p3.3.m3.1a"><mo id="Ch8.S8.SS3.SSSx4.p3.3.m3.1.1" stretchy="false" xref="Ch8.S8.SS3.SSSx4.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p3.3.m3.1b"><ci id="Ch8.S8.SS3.SSSx4.p3.3.m3.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p3.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p3.3.m3.1d">→</annotation></semantics></math>EN system’s translations display significantly greater fluency, as evidenced by just two errors recorded in this category.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx4.p4">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx4.p4.3">One way to measure inter-rater reliability is to use Cohen’s <math alttext="kappa" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p4.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx4.p4.1.m1.1a"><mrow id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.2" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.2.cmml">k</mi><mo id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.3" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.3.cmml">a</mi><mo id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1a" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.4" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.4.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1b" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.5" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.5.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1c" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.6" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p4.1.m1.1b"><apply id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1"><times id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.2">𝑘</ci><ci id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.3">𝑎</ci><ci id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.4.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.4">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.5.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.5">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.6.cmml" xref="Ch8.S8.SS3.SSSx4.p4.1.m1.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p4.1.m1.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p4.1.m1.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math>, which is a rigorous method. It determines the percentage of items that raters agree on while also taking into account the possibility of them agreeing on some items by chance. Cohen’s <math alttext="kappa" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p4.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx4.p4.2.m2.1a"><mrow id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.2" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.2.cmml">k</mi><mo id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.3" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.3.cmml">a</mi><mo id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1a" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.4" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.4.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1b" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.5" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.5.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1c" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.6" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p4.2.m2.1b"><apply id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1"><times id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.2">𝑘</ci><ci id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.3">𝑎</ci><ci id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.4.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.4">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.5.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.5">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.6.cmml" xref="Ch8.S8.SS3.SSSx4.p4.2.m2.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p4.2.m2.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p4.2.m2.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> was calculated separately for every error type and the findings are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T11" title="Table 8.11 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.11</span></a> and discussed in further detail later in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS2" title="8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.9.2</span></a>. To calculate Cohen’s <math alttext="kappa" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p4.3.m3.1"><semantics id="Ch8.S8.SS3.SSSx4.p4.3.m3.1a"><mrow id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.2" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.2.cmml">k</mi><mo id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.3" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.3.cmml">a</mi><mo id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1a" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.4" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.4.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1b" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.5" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.5.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1c" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.6" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p4.3.m3.1b"><apply id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1"><times id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.2">𝑘</ci><ci id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.3">𝑎</ci><ci id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.4.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.4">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.5.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.5">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.6.cmml" xref="Ch8.S8.SS3.SSSx4.p4.3.m3.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p4.3.m3.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p4.3.m3.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> the following formula is used:</p>
<table class="ltx_equation ltx_eqn_table" id="Ch8.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="k=(p\textsubscript{o}-p\textsubscript{e})/(1-p\textsubscript{e})" class="ltx_Math" display="block" id="Ch8.E1.m1.2"><semantics id="Ch8.E1.m1.2a"><mrow id="Ch8.E1.m1.2.2" xref="Ch8.E1.m1.2.2.cmml"><mi id="Ch8.E1.m1.2.2.4" xref="Ch8.E1.m1.2.2.4.cmml">k</mi><mo id="Ch8.E1.m1.2.2.3" xref="Ch8.E1.m1.2.2.3.cmml">=</mo><mrow id="Ch8.E1.m1.2.2.2" xref="Ch8.E1.m1.2.2.2.cmml"><mrow id="Ch8.E1.m1.1.1.1.1.1" xref="Ch8.E1.m1.1.1.1.1.1.1.cmml"><mo id="Ch8.E1.m1.1.1.1.1.1.2" stretchy="false" xref="Ch8.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch8.E1.m1.1.1.1.1.1.1" xref="Ch8.E1.m1.1.1.1.1.1.1.cmml"><mrow id="Ch8.E1.m1.1.1.1.1.1.1.2" xref="Ch8.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="Ch8.E1.m1.1.1.1.1.1.1.2.2" xref="Ch8.E1.m1.1.1.1.1.1.1.2.2.cmml">p</mi><mo id="Ch8.E1.m1.1.1.1.1.1.1.2.1" xref="Ch8.E1.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mtext id="Ch8.E1.m1.1.1.1.1.1.1.2.3" xref="Ch8.E1.m1.1.1.1.1.1.1.2.3b.cmml"><sub class="ltx_sub" id="Ch8.E1.m1.1.1.1.1.1.1.2.3.1nest">o</sub></mtext></mrow><mo id="Ch8.E1.m1.1.1.1.1.1.1.1" xref="Ch8.E1.m1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="Ch8.E1.m1.1.1.1.1.1.1.3" xref="Ch8.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="Ch8.E1.m1.1.1.1.1.1.1.3.2" xref="Ch8.E1.m1.1.1.1.1.1.1.3.2.cmml">p</mi><mo id="Ch8.E1.m1.1.1.1.1.1.1.3.1" xref="Ch8.E1.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mtext id="Ch8.E1.m1.1.1.1.1.1.1.3.3" xref="Ch8.E1.m1.1.1.1.1.1.1.3.3b.cmml"><sub class="ltx_sub" id="Ch8.E1.m1.1.1.1.1.1.1.3.3.1nest">e</sub></mtext></mrow></mrow><mo id="Ch8.E1.m1.1.1.1.1.1.3" stretchy="false" xref="Ch8.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="Ch8.E1.m1.2.2.2.3" xref="Ch8.E1.m1.2.2.2.3.cmml">/</mo><mrow id="Ch8.E1.m1.2.2.2.2.1" xref="Ch8.E1.m1.2.2.2.2.1.1.cmml"><mo id="Ch8.E1.m1.2.2.2.2.1.2" stretchy="false" xref="Ch8.E1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="Ch8.E1.m1.2.2.2.2.1.1" xref="Ch8.E1.m1.2.2.2.2.1.1.cmml"><mn id="Ch8.E1.m1.2.2.2.2.1.1.2" xref="Ch8.E1.m1.2.2.2.2.1.1.2.cmml">1</mn><mo id="Ch8.E1.m1.2.2.2.2.1.1.1" xref="Ch8.E1.m1.2.2.2.2.1.1.1.cmml">−</mo><mrow id="Ch8.E1.m1.2.2.2.2.1.1.3" xref="Ch8.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="Ch8.E1.m1.2.2.2.2.1.1.3.2" xref="Ch8.E1.m1.2.2.2.2.1.1.3.2.cmml">p</mi><mo id="Ch8.E1.m1.2.2.2.2.1.1.3.1" xref="Ch8.E1.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><mtext id="Ch8.E1.m1.2.2.2.2.1.1.3.3" xref="Ch8.E1.m1.2.2.2.2.1.1.3.3b.cmml"><sub class="ltx_sub" id="Ch8.E1.m1.2.2.2.2.1.1.3.3.1nest">e</sub></mtext></mrow></mrow><mo id="Ch8.E1.m1.2.2.2.2.1.3" stretchy="false" xref="Ch8.E1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch8.E1.m1.2b"><apply id="Ch8.E1.m1.2.2.cmml" xref="Ch8.E1.m1.2.2"><eq id="Ch8.E1.m1.2.2.3.cmml" xref="Ch8.E1.m1.2.2.3"></eq><ci id="Ch8.E1.m1.2.2.4.cmml" xref="Ch8.E1.m1.2.2.4">𝑘</ci><apply id="Ch8.E1.m1.2.2.2.cmml" xref="Ch8.E1.m1.2.2.2"><divide id="Ch8.E1.m1.2.2.2.3.cmml" xref="Ch8.E1.m1.2.2.2.3"></divide><apply id="Ch8.E1.m1.1.1.1.1.1.1.cmml" xref="Ch8.E1.m1.1.1.1.1.1"><minus id="Ch8.E1.m1.1.1.1.1.1.1.1.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.1"></minus><apply id="Ch8.E1.m1.1.1.1.1.1.1.2.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.2"><times id="Ch8.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.2.1"></times><ci id="Ch8.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.2.2">𝑝</ci><ci id="Ch8.E1.m1.1.1.1.1.1.1.2.3b.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.2.3"><mtext id="Ch8.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.2.3"><sub class="ltx_sub" id="Ch8.E1.m1.1.1.1.1.1.1.2.3.1anest">o</sub></mtext></ci></apply><apply id="Ch8.E1.m1.1.1.1.1.1.1.3.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.3"><times id="Ch8.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.3.1"></times><ci id="Ch8.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.3.2">𝑝</ci><ci id="Ch8.E1.m1.1.1.1.1.1.1.3.3b.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.3.3"><mtext id="Ch8.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="Ch8.E1.m1.1.1.1.1.1.1.3.3"><sub class="ltx_sub" id="Ch8.E1.m1.1.1.1.1.1.1.3.3.1anest">e</sub></mtext></ci></apply></apply><apply id="Ch8.E1.m1.2.2.2.2.1.1.cmml" xref="Ch8.E1.m1.2.2.2.2.1"><minus id="Ch8.E1.m1.2.2.2.2.1.1.1.cmml" xref="Ch8.E1.m1.2.2.2.2.1.1.1"></minus><cn id="Ch8.E1.m1.2.2.2.2.1.1.2.cmml" type="integer" xref="Ch8.E1.m1.2.2.2.2.1.1.2">1</cn><apply id="Ch8.E1.m1.2.2.2.2.1.1.3.cmml" xref="Ch8.E1.m1.2.2.2.2.1.1.3"><times id="Ch8.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="Ch8.E1.m1.2.2.2.2.1.1.3.1"></times><ci id="Ch8.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="Ch8.E1.m1.2.2.2.2.1.1.3.2">𝑝</ci><ci id="Ch8.E1.m1.2.2.2.2.1.1.3.3b.cmml" xref="Ch8.E1.m1.2.2.2.2.1.1.3.3"><mtext id="Ch8.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="Ch8.E1.m1.2.2.2.2.1.1.3.3"><sub class="ltx_sub" id="Ch8.E1.m1.2.2.2.2.1.1.3.3.1anest">e</sub></mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.E1.m1.2c">k=(p\textsubscript{o}-p\textsubscript{e})/(1-p\textsubscript{e})</annotation><annotation encoding="application/x-llamapun" id="Ch8.E1.m1.2d">italic_k = ( italic_p - italic_p ) / ( 1 - italic_p )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8.1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx4.p5">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx4.p5.1"><math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p5.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx4.p5.1.m1.1a"><mrow id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.2" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.1" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p5.1.m1.1b"><apply id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1"><times id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.2">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3b.cmml" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3"><mtext id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx4.p5.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p5.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p5.1.m1.1d">italic_p</annotation></semantics></math>: Relative observed agreement among raters</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx4.p6">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx4.p6.1"><math alttext="p\textsubscript{e}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx4.p6.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx4.p6.1.m1.1a"><mrow id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.2" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.1" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3.1nest">e</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx4.p6.1.m1.1b"><apply id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1"><times id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.2">𝑝</ci><ci id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3b.cmml" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3"><mtext id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx4.p6.1.m1.1.1.3.1anest">e</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx4.p6.1.m1.1c">p\textsubscript{e}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx4.p6.1.m1.1d">italic_p</annotation></semantics></math>: Hypothetical probability of chance agreement</p>
</div>
<figure class="ltx_table" id="Ch8.T10">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8.10: </span>Fine-grained analysis with concatenated errors across both annotators</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T10.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T10.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T10.2.2.3"><span class="ltx_text ltx_font_bold" id="Ch8.T10.2.2.3.1" style="font-size:90%;">Error Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T10.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T10.1.1.1.1" style="font-size:90%;">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T10.1.1.1.1.m1.1"><semantics id="Ch8.T10.1.1.1.1.m1.1a"><mo id="Ch8.T10.1.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch8.T10.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T10.1.1.1.1.m1.1b"><ci id="Ch8.T10.1.1.1.1.m1.1.1.cmml" xref="Ch8.T10.1.1.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T10.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T10.1.1.1.1.m1.1d">→</annotation></semantics></math>GA errors</span><span class="ltx_text" id="Ch8.T10.1.1.1.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T10.2.2.2"><span class="ltx_text ltx_font_bold" id="Ch8.T10.2.2.2.1" style="font-size:90%;">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T10.2.2.2.1.m1.1"><semantics id="Ch8.T10.2.2.2.1.m1.1a"><mo id="Ch8.T10.2.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch8.T10.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T10.2.2.2.1.m1.1b"><ci id="Ch8.T10.2.2.2.1.m1.1.1.cmml" xref="Ch8.T10.2.2.2.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T10.2.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T10.2.2.2.1.m1.1d">→</annotation></semantics></math>EN errors</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T10.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T10.2.3.1.1"><span class="ltx_text" id="Ch8.T10.2.3.1.1.1" style="font-size:90%;">Non-translation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T10.2.3.1.2"><span class="ltx_text" id="Ch8.T10.2.3.1.2.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T10.2.3.1.3"><span class="ltx_text" id="Ch8.T10.2.3.1.3.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.4.2">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.4.2.1"><span class="ltx_text ltx_font_bold" id="Ch8.T10.2.4.2.1.1" style="font-size:90%;">Accuracy</span></td>
<td class="ltx_td" id="Ch8.T10.2.4.2.2"></td>
<td class="ltx_td" id="Ch8.T10.2.4.2.3"></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.5.3">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.5.3.1"><span class="ltx_text" id="Ch8.T10.2.5.3.1.1" style="font-size:90%;">Addition</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.5.3.2"><span class="ltx_text" id="Ch8.T10.2.5.3.2.1" style="font-size:90%;">12</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.5.3.3"><span class="ltx_text" id="Ch8.T10.2.5.3.3.1" style="font-size:90%;">5</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.6.4">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.6.4.1"><span class="ltx_text" id="Ch8.T10.2.6.4.1.1" style="font-size:90%;">Omission</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.6.4.2"><span class="ltx_text" id="Ch8.T10.2.6.4.2.1" style="font-size:90%;">14</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.6.4.3"><span class="ltx_text" id="Ch8.T10.2.6.4.3.1" style="font-size:90%;">3</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.7.5">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.7.5.1"><span class="ltx_text" id="Ch8.T10.2.7.5.1.1" style="font-size:90%;">Mistranslation</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.7.5.2"><span class="ltx_text" id="Ch8.T10.2.7.5.2.1" style="font-size:90%;">41</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.7.5.3"><span class="ltx_text" id="Ch8.T10.2.7.5.3.1" style="font-size:90%;">6</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.8.6">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.8.6.1"><span class="ltx_text" id="Ch8.T10.2.8.6.1.1" style="font-size:90%;">Untranslated text</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.8.6.2"><span class="ltx_text" id="Ch8.T10.2.8.6.2.1" style="font-size:90%;">9</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.8.6.3"><span class="ltx_text" id="Ch8.T10.2.8.6.3.1" style="font-size:90%;">2</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.9.7">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.9.7.1"><span class="ltx_text ltx_font_bold" id="Ch8.T10.2.9.7.1.1" style="font-size:90%;">Fluency</span></td>
<td class="ltx_td" id="Ch8.T10.2.9.7.2"></td>
<td class="ltx_td" id="Ch8.T10.2.9.7.3"></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.10.8">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.10.8.1"><span class="ltx_text" id="Ch8.T10.2.10.8.1.1" style="font-size:90%;">Punctuation</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.10.8.2"><span class="ltx_text" id="Ch8.T10.2.10.8.2.1" style="font-size:90%;">10</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.10.8.3"><span class="ltx_text" id="Ch8.T10.2.10.8.3.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.11.9">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.11.9.1"><span class="ltx_text" id="Ch8.T10.2.11.9.1.1" style="font-size:90%;">Spelling</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.11.9.2"><span class="ltx_text" id="Ch8.T10.2.11.9.2.1" style="font-size:90%;">6</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.11.9.3"><span class="ltx_text" id="Ch8.T10.2.11.9.3.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.12.10">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.12.10.1"><span class="ltx_text" id="Ch8.T10.2.12.10.1.1" style="font-size:90%;">Grammar</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.12.10.2"><span class="ltx_text" id="Ch8.T10.2.12.10.2.1" style="font-size:90%;">27</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.12.10.3"><span class="ltx_text" id="Ch8.T10.2.12.10.3.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.13.11">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.13.11.1"><span class="ltx_text" id="Ch8.T10.2.13.11.1.1" style="font-size:90%;">Register</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.13.11.2"><span class="ltx_text" id="Ch8.T10.2.13.11.2.1" style="font-size:90%;">19</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.13.11.3"><span class="ltx_text" id="Ch8.T10.2.13.11.3.1" style="font-size:90%;">2</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.14.12">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.14.12.1"><span class="ltx_text" id="Ch8.T10.2.14.12.1.1" style="font-size:90%;">Inconsistency</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.14.12.2"><span class="ltx_text" id="Ch8.T10.2.14.12.2.1" style="font-size:90%;">6</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.14.12.3"><span class="ltx_text" id="Ch8.T10.2.14.12.3.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.15.13">
<td class="ltx_td ltx_align_left" id="Ch8.T10.2.15.13.1"><span class="ltx_text" id="Ch8.T10.2.15.13.1.1" style="font-size:90%;">Character Encoding</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.15.13.2"><span class="ltx_text" id="Ch8.T10.2.15.13.2.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ch8.T10.2.15.13.3"><span class="ltx_text" id="Ch8.T10.2.15.13.3.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T10.2.16.14">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Ch8.T10.2.16.14.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T10.2.16.14.1.1" style="font-size:90%;">Total errors</span><span class="ltx_text" id="Ch8.T10.2.16.14.1.2" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch8.T10.2.16.14.2"><span class="ltx_text" id="Ch8.T10.2.16.14.2.1" style="font-size:90%;">135</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="Ch8.T10.2.16.14.3"><span class="ltx_text" id="Ch8.T10.2.16.14.3.1" style="font-size:90%;">18</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Ch8.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T11.12.2.1" style="font-size:90%;">Table 8.11</span>: </span><span class="ltx_text" id="Ch8.T11.1.1" style="font-size:90%;">Inter-annotator agreement using Cohen values. Perfect observed agreement is indicated by <math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.T11.1.1.m1.1"><semantics id="Ch8.T11.1.1.m1.1b"><mrow id="Ch8.T11.1.1.m1.1.1" xref="Ch8.T11.1.1.m1.1.1.cmml"><mi id="Ch8.T11.1.1.m1.1.1.2" xref="Ch8.T11.1.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.1.1.m1.1.1.1" xref="Ch8.T11.1.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.1.1.m1.1.1.3" xref="Ch8.T11.1.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.1.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.1.1.m1.1c"><apply id="Ch8.T11.1.1.m1.1.1.cmml" xref="Ch8.T11.1.1.m1.1.1"><times id="Ch8.T11.1.1.m1.1.1.1.cmml" xref="Ch8.T11.1.1.m1.1.1.1"></times><ci id="Ch8.T11.1.1.m1.1.1.2.cmml" xref="Ch8.T11.1.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.1.1.m1.1.1.3b.cmml" xref="Ch8.T11.1.1.m1.1.1.3"><mtext id="Ch8.T11.1.1.m1.1.1.3.cmml" xref="Ch8.T11.1.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.1.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.1.1.m1.1d">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.1.1.m1.1e">italic_p</annotation></semantics></math> = 1.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T11.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T11.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T11.3.2.3"><span class="ltx_text ltx_font_bold" id="Ch8.T11.3.2.3.1">Error type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T11.2.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T11.2.1.1.1">EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T11.2.1.1.1.m1.1"><semantics id="Ch8.T11.2.1.1.1.m1.1a"><mo id="Ch8.T11.2.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch8.T11.2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T11.2.1.1.1.m1.1b"><ci id="Ch8.T11.2.1.1.1.m1.1.1.cmml" xref="Ch8.T11.2.1.1.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.2.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.2.1.1.1.m1.1d">→</annotation></semantics></math>GA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ch8.T11.3.2.2"><span class="ltx_text ltx_font_bold" id="Ch8.T11.3.2.2.1">GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T11.3.2.2.1.m1.1"><semantics id="Ch8.T11.3.2.2.1.m1.1a"><mo id="Ch8.T11.3.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="Ch8.T11.3.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T11.3.2.2.1.m1.1b"><ci id="Ch8.T11.3.2.2.1.m1.1.1.cmml" xref="Ch8.T11.3.2.2.1.m1.1.1">normal-→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.3.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.3.2.2.1.m1.1d">→</annotation></semantics></math>EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T11.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T11.5.4.3">Non-translation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T11.4.3.1">
<math alttext="p\textsubscript{a}" class="ltx_Math" display="inline" id="Ch8.T11.4.3.1.m1.1"><semantics id="Ch8.T11.4.3.1.m1.1a"><mrow id="Ch8.T11.4.3.1.m1.1.1" xref="Ch8.T11.4.3.1.m1.1.1.cmml"><mi id="Ch8.T11.4.3.1.m1.1.1.2" xref="Ch8.T11.4.3.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.4.3.1.m1.1.1.1" xref="Ch8.T11.4.3.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.4.3.1.m1.1.1.3" xref="Ch8.T11.4.3.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.4.3.1.m1.1.1.3.1nest">a</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.4.3.1.m1.1b"><apply id="Ch8.T11.4.3.1.m1.1.1.cmml" xref="Ch8.T11.4.3.1.m1.1.1"><times id="Ch8.T11.4.3.1.m1.1.1.1.cmml" xref="Ch8.T11.4.3.1.m1.1.1.1"></times><ci id="Ch8.T11.4.3.1.m1.1.1.2.cmml" xref="Ch8.T11.4.3.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.4.3.1.m1.1.1.3b.cmml" xref="Ch8.T11.4.3.1.m1.1.1.3"><mtext id="Ch8.T11.4.3.1.m1.1.1.3.cmml" xref="Ch8.T11.4.3.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.4.3.1.m1.1.1.3.1anest">a</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.4.3.1.m1.1c">p\textsubscript{a}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.4.3.1.m1.1d">italic_p</annotation></semantics></math>=1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch8.T11.5.4.2">
<math alttext="p\textsubscript{a}" class="ltx_Math" display="inline" id="Ch8.T11.5.4.2.m1.1"><semantics id="Ch8.T11.5.4.2.m1.1a"><mrow id="Ch8.T11.5.4.2.m1.1.1" xref="Ch8.T11.5.4.2.m1.1.1.cmml"><mi id="Ch8.T11.5.4.2.m1.1.1.2" xref="Ch8.T11.5.4.2.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.5.4.2.m1.1.1.1" xref="Ch8.T11.5.4.2.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.5.4.2.m1.1.1.3" xref="Ch8.T11.5.4.2.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.5.4.2.m1.1.1.3.1nest">a</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.5.4.2.m1.1b"><apply id="Ch8.T11.5.4.2.m1.1.1.cmml" xref="Ch8.T11.5.4.2.m1.1.1"><times id="Ch8.T11.5.4.2.m1.1.1.1.cmml" xref="Ch8.T11.5.4.2.m1.1.1.1"></times><ci id="Ch8.T11.5.4.2.m1.1.1.2.cmml" xref="Ch8.T11.5.4.2.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.5.4.2.m1.1.1.3b.cmml" xref="Ch8.T11.5.4.2.m1.1.1.3"><mtext id="Ch8.T11.5.4.2.m1.1.1.3.cmml" xref="Ch8.T11.5.4.2.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.5.4.2.m1.1.1.3.1anest">a</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.5.4.2.m1.1c">p\textsubscript{a}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.5.4.2.m1.1d">italic_p</annotation></semantics></math>=1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.10.1">
<td class="ltx_td ltx_align_left" id="Ch8.T11.10.10.1.1">Accuracy</td>
<td class="ltx_td" id="Ch8.T11.10.10.1.2"></td>
<td class="ltx_td" id="Ch8.T11.10.10.1.3"></td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.11.2">
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.11.2.1">Addition</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.11.2.2">0.24</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.11.2.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.12.3">
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.12.3.1">Omission</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.12.3.2">0.31</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.12.3.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.13.4">
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.13.4.1">Mistranslation</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.13.4.2">0.32</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.13.4.3">-0.11</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.14.5">
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.14.5.1">Untranslated text</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.14.5.2">0.07</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.14.5.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.15.6">
<td class="ltx_td ltx_align_left" id="Ch8.T11.10.15.6.1">Fluency</td>
<td class="ltx_td" id="Ch8.T11.10.15.6.2"></td>
<td class="ltx_td" id="Ch8.T11.10.15.6.3"></td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.6.5">
<td class="ltx_td ltx_align_center" id="Ch8.T11.6.5.2">Punctuation</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.6.5.3">1</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.6.5.1">
<math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.T11.6.5.1.m1.1"><semantics id="Ch8.T11.6.5.1.m1.1a"><mrow id="Ch8.T11.6.5.1.m1.1.1" xref="Ch8.T11.6.5.1.m1.1.1.cmml"><mi id="Ch8.T11.6.5.1.m1.1.1.2" xref="Ch8.T11.6.5.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.6.5.1.m1.1.1.1" xref="Ch8.T11.6.5.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.6.5.1.m1.1.1.3" xref="Ch8.T11.6.5.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.6.5.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.6.5.1.m1.1b"><apply id="Ch8.T11.6.5.1.m1.1.1.cmml" xref="Ch8.T11.6.5.1.m1.1.1"><times id="Ch8.T11.6.5.1.m1.1.1.1.cmml" xref="Ch8.T11.6.5.1.m1.1.1.1"></times><ci id="Ch8.T11.6.5.1.m1.1.1.2.cmml" xref="Ch8.T11.6.5.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.6.5.1.m1.1.1.3b.cmml" xref="Ch8.T11.6.5.1.m1.1.1.3"><mtext id="Ch8.T11.6.5.1.m1.1.1.3.cmml" xref="Ch8.T11.6.5.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.6.5.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.6.5.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.6.5.1.m1.1d">italic_p</annotation></semantics></math>=1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.7.6">
<td class="ltx_td ltx_align_center" id="Ch8.T11.7.6.2">Spelling</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.7.6.3">0.24</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.7.6.1">
<math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.T11.7.6.1.m1.1"><semantics id="Ch8.T11.7.6.1.m1.1a"><mrow id="Ch8.T11.7.6.1.m1.1.1" xref="Ch8.T11.7.6.1.m1.1.1.cmml"><mi id="Ch8.T11.7.6.1.m1.1.1.2" xref="Ch8.T11.7.6.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.7.6.1.m1.1.1.1" xref="Ch8.T11.7.6.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.7.6.1.m1.1.1.3" xref="Ch8.T11.7.6.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.7.6.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.7.6.1.m1.1b"><apply id="Ch8.T11.7.6.1.m1.1.1.cmml" xref="Ch8.T11.7.6.1.m1.1.1"><times id="Ch8.T11.7.6.1.m1.1.1.1.cmml" xref="Ch8.T11.7.6.1.m1.1.1.1"></times><ci id="Ch8.T11.7.6.1.m1.1.1.2.cmml" xref="Ch8.T11.7.6.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.7.6.1.m1.1.1.3b.cmml" xref="Ch8.T11.7.6.1.m1.1.1.3"><mtext id="Ch8.T11.7.6.1.m1.1.1.3.cmml" xref="Ch8.T11.7.6.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.7.6.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.7.6.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.7.6.1.m1.1d">italic_p</annotation></semantics></math>=1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.8.7">
<td class="ltx_td ltx_align_center" id="Ch8.T11.8.7.2">Grammar</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.8.7.3">0.59</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.8.7.1">
<math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.T11.8.7.1.m1.1"><semantics id="Ch8.T11.8.7.1.m1.1a"><mrow id="Ch8.T11.8.7.1.m1.1.1" xref="Ch8.T11.8.7.1.m1.1.1.cmml"><mi id="Ch8.T11.8.7.1.m1.1.1.2" xref="Ch8.T11.8.7.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.8.7.1.m1.1.1.1" xref="Ch8.T11.8.7.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.8.7.1.m1.1.1.3" xref="Ch8.T11.8.7.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.8.7.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.8.7.1.m1.1b"><apply id="Ch8.T11.8.7.1.m1.1.1.cmml" xref="Ch8.T11.8.7.1.m1.1.1"><times id="Ch8.T11.8.7.1.m1.1.1.1.cmml" xref="Ch8.T11.8.7.1.m1.1.1.1"></times><ci id="Ch8.T11.8.7.1.m1.1.1.2.cmml" xref="Ch8.T11.8.7.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.8.7.1.m1.1.1.3b.cmml" xref="Ch8.T11.8.7.1.m1.1.1.3"><mtext id="Ch8.T11.8.7.1.m1.1.1.3.cmml" xref="Ch8.T11.8.7.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.8.7.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.8.7.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.8.7.1.m1.1d">italic_p</annotation></semantics></math>=1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.16.7">
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.16.7.1">Register</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.16.7.2">-0.07</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.10.16.7.3">0</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.9.8">
<td class="ltx_td ltx_align_center" id="Ch8.T11.9.8.2">Inconsistency</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.9.8.3">0.34</td>
<td class="ltx_td ltx_align_center" id="Ch8.T11.9.8.1">
<math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.T11.9.8.1.m1.1"><semantics id="Ch8.T11.9.8.1.m1.1a"><mrow id="Ch8.T11.9.8.1.m1.1.1" xref="Ch8.T11.9.8.1.m1.1.1.cmml"><mi id="Ch8.T11.9.8.1.m1.1.1.2" xref="Ch8.T11.9.8.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.9.8.1.m1.1.1.1" xref="Ch8.T11.9.8.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.9.8.1.m1.1.1.3" xref="Ch8.T11.9.8.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.9.8.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.9.8.1.m1.1b"><apply id="Ch8.T11.9.8.1.m1.1.1.cmml" xref="Ch8.T11.9.8.1.m1.1.1"><times id="Ch8.T11.9.8.1.m1.1.1.1.cmml" xref="Ch8.T11.9.8.1.m1.1.1.1"></times><ci id="Ch8.T11.9.8.1.m1.1.1.2.cmml" xref="Ch8.T11.9.8.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.9.8.1.m1.1.1.3b.cmml" xref="Ch8.T11.9.8.1.m1.1.1.3"><mtext id="Ch8.T11.9.8.1.m1.1.1.3.cmml" xref="Ch8.T11.9.8.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.9.8.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.9.8.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.9.8.1.m1.1d">italic_p</annotation></semantics></math>=1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T11.10.9">
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T11.10.9.2">Character Encoding</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T11.10.9.1">
<math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.T11.10.9.1.m1.1"><semantics id="Ch8.T11.10.9.1.m1.1a"><mrow id="Ch8.T11.10.9.1.m1.1.1" xref="Ch8.T11.10.9.1.m1.1.1.cmml"><mi id="Ch8.T11.10.9.1.m1.1.1.2" xref="Ch8.T11.10.9.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.T11.10.9.1.m1.1.1.1" xref="Ch8.T11.10.9.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.T11.10.9.1.m1.1.1.3" xref="Ch8.T11.10.9.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.T11.10.9.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.T11.10.9.1.m1.1b"><apply id="Ch8.T11.10.9.1.m1.1.1.cmml" xref="Ch8.T11.10.9.1.m1.1.1"><times id="Ch8.T11.10.9.1.m1.1.1.1.cmml" xref="Ch8.T11.10.9.1.m1.1.1.1"></times><ci id="Ch8.T11.10.9.1.m1.1.1.2.cmml" xref="Ch8.T11.10.9.1.m1.1.1.2">𝑝</ci><ci id="Ch8.T11.10.9.1.m1.1.1.3b.cmml" xref="Ch8.T11.10.9.1.m1.1.1.3"><mtext id="Ch8.T11.10.9.1.m1.1.1.3.cmml" xref="Ch8.T11.10.9.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.T11.10.9.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T11.10.9.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T11.10.9.1.m1.1d">italic_p</annotation></semantics></math>=1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch8.T11.10.9.3">1.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="Ch8.S8.SS3.SSSx5">
<h5 class="ltx_title ltx_title_subsubsection">Inter-Annotator Reliability</h5>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx5.p1">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx5.p1.3">In Cohen’s seminal paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx30" title="">30</a>]</cite>, he precisely defines the interpretation of various <math alttext="k" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p1.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx5.p1.1.m1.1a"><mi id="Ch8.S8.SS3.SSSx5.p1.1.m1.1.1" xref="Ch8.S8.SS3.SSSx5.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p1.1.m1.1b"><ci id="Ch8.S8.SS3.SSSx5.p1.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p1.1.m1.1d">italic_k</annotation></semantics></math> scores. Scores <math alttext="\leq" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p1.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx5.p1.2.m2.1a"><mo id="Ch8.S8.SS3.SSSx5.p1.2.m2.1.1" xref="Ch8.S8.SS3.SSSx5.p1.2.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p1.2.m2.1b"><leq id="Ch8.S8.SS3.SSSx5.p1.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p1.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p1.2.m2.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p1.2.m2.1d">≤</annotation></semantics></math> 0 indicate no agreement, scores from 0.01 to 0.20 suggest none to slight agreement, scores from 0.21 to 0.40 denote fair agreement, scores from 0.41 to 0.60 reflect moderate agreement, scores from 0.61 to 0.80 correspond to substantial agreement, and scores from 0.81 to 1.00 represent almost perfect agreement. The <math alttext="kappa" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p1.3.m3.1"><semantics id="Ch8.S8.SS3.SSSx5.p1.3.m3.1a"><mrow id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.2" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.2.cmml">k</mi><mo id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.3" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.3.cmml">a</mi><mo id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1a" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.4" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.4.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1b" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.5" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.5.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1c" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.6" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p1.3.m3.1b"><apply id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1"><times id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.2">𝑘</ci><ci id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.3">𝑎</ci><ci id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.4.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.4">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.5.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.5">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.6.cmml" xref="Ch8.S8.SS3.SSSx5.p1.3.m3.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p1.3.m3.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p1.3.m3.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> values of each error type are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T11" title="Table 8.11 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.11</span></a>.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx5.p2">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx5.p2.5">Many chance-adjusted indices of inter-rater reliability estimate agreement using a distribution-based approach. A problem arises when there is only one observed response category, resulting in a score of NaN (Not a Number). This occurs when the observed agreement, <math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p2.1.m1.1"><semantics id="Ch8.S8.SS3.SSSx5.p2.1.m1.1a"><mrow id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.2" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.2.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.1" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.1.cmml">⁢</mo><mtext id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p2.1.m1.1b"><apply id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1"><times id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.2">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3b.cmml" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3"><mtext id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.1.m1.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p2.1.m1.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p2.1.m1.1d">italic_p</annotation></semantics></math> and the chance agreement, <math alttext="p\textsubscript{e}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p2.2.m2.1"><semantics id="Ch8.S8.SS3.SSSx5.p2.2.m2.1a"><mrow id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.2" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.2.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.1" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.1.cmml">⁢</mo><mtext id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3.1nest">e</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p2.2.m2.1b"><apply id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1"><times id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.2">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3b.cmml" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3"><mtext id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.2.m2.1.1.3.1anest">e</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p2.2.m2.1c">p\textsubscript{e}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p2.2.m2.1d">italic_p</annotation></semantics></math> are both 1, which cannot be computed as seen in Equation <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.E1" title="8.1 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.1</span></a>. In such cases, it is better to report <math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p2.3.m3.1"><semantics id="Ch8.S8.SS3.SSSx5.p2.3.m3.1a"><mrow id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.2" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.2.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.1" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.1.cmml">⁢</mo><mtext id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p2.3.m3.1b"><apply id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1"><times id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.2">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3b.cmml" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3"><mtext id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.3.m3.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p2.3.m3.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p2.3.m3.1d">italic_p</annotation></semantics></math> instead of <math alttext="kappa" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p2.4.m4.1"><semantics id="Ch8.S8.SS3.SSSx5.p2.4.m4.1a"><mrow id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.2" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.2.cmml">k</mi><mo id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.3" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.3.cmml">a</mi><mo id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1a" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.4" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.4.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1b" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.5" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.5.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1c" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.6" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p2.4.m4.1b"><apply id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1"><times id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.2">𝑘</ci><ci id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.3">𝑎</ci><ci id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.4.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.4">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.5.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.5">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.6.cmml" xref="Ch8.S8.SS3.SSSx5.p2.4.m4.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p2.4.m4.1c">kappa</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p2.4.m4.1d">italic_k italic_a italic_p italic_p italic_a</annotation></semantics></math> since there is perfect observed agreement i.e. <math alttext="p\textsubscript{o}" class="ltx_Math" display="inline" id="Ch8.S8.SS3.SSSx5.p2.5.m5.1"><semantics id="Ch8.S8.SS3.SSSx5.p2.5.m5.1a"><mrow id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.cmml"><mi id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.2" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.2.cmml">p</mi><mo id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.1" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.1.cmml">⁢</mo><mtext id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3b.cmml"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3.1nest">o</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="Ch8.S8.SS3.SSSx5.p2.5.m5.1b"><apply id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1"><times id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.1.cmml" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.1"></times><ci id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.2.cmml" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.2">𝑝</ci><ci id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3b.cmml" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3"><mtext id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3.cmml" xref="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3"><sub class="ltx_sub" id="Ch8.S8.SS3.SSSx5.p2.5.m5.1.1.3.1anest">o</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S8.SS3.SSSx5.p2.5.m5.1c">p\textsubscript{o}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S8.SS3.SSSx5.p2.5.m5.1d">italic_p</annotation></semantics></math> = 1.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS3.SSSx5.p3">
<p class="ltx_p" id="Ch8.S8.SS3.SSSx5.p3.1">Illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T11" title="Table 8.11 ‣ Inter-Annotator Agreement ‣ 8.8.3 Human Evaluation Results ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.11</span></a>, we observe a high level of agreement overall. There is either fair agreement, or perfect observed agreement, in 16 out of 22 sub-categories. Given these scores, we have a high degree of confidence in the human evaluation of the fine-tuned MLLM outputs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch8.S8.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.8.4 </span>Environmental Impact</h4>
<div class="ltx_para" id="Ch8.S8.SS4.p1">
<p class="ltx_p" id="Ch8.S8.SS4.p1.1">Motivated by research which examines the environmental impact of NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx15" title="">15</a>]</cite>, we monitored the energy and carbon emissions required to train our models.</p>
</div>
<div class="ltx_para" id="Ch8.S8.SS4.p2">
<p class="ltx_p" id="Ch8.S8.SS4.p2.1">Model development was carried out using Colab Pro+, which as part of Google Cloud is carbon neutral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx62" title="">62</a>]</cite>. All fine-tuning experiments of MLLMs were conducted on Google Cloud servers and consequently were emission-free.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote37"><sup class="ltx_note_mark">37</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">37</sup><span class="ltx_tag ltx_tag_note">37</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/sustainability/region-carbon" title="">https://cloud.google.com/sustainability/region-carbon</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch8.S8.SS4.p3">
<p class="ltx_p" id="Ch8.S8.SS4.p3.1">In terms of energy consumption, the total power draw for each experimental run is outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T12" title="Table 8.12 ‣ 8.8.4 Environmental Impact ‣ 8.8 Empirical Evaluation ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.12</span></a>. As part of our Google Colab subscription, NVIDIA a100-sxm4-40gb graphics cards were used which have a max power consumption of 400W. The calculations are based on the graphics card running at 80% max power during model training.</p>
</div>
<figure class="ltx_table" id="Ch8.T12">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T12.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T12.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch8.T12.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch8.T12.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T12.1.1.1.m1.1"><semantics id="Ch8.T12.1.1.1.m1.1a"><mo id="Ch8.T12.1.1.1.m1.1.1" stretchy="false" xref="Ch8.T12.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T12.1.1.1.m1.1b"><ci id="Ch8.T12.1.1.1.m1.1.1.cmml" xref="Ch8.T12.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T12.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T12.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch8.T12.2.2.2.1">TER</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="Ch8.T12.2.2.2.m1.1"><semantics id="Ch8.T12.2.2.2.m1.1a"><mo id="Ch8.T12.2.2.2.m1.1.1" stretchy="false" xref="Ch8.T12.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Ch8.T12.2.2.2.m1.1b"><ci id="Ch8.T12.2.2.2.m1.1.1.cmml" xref="Ch8.T12.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T12.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T12.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.3.3.3">
<span class="ltx_text ltx_font_bold" id="Ch8.T12.3.3.3.1">ChrF3</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T12.3.3.3.m1.1"><semantics id="Ch8.T12.3.3.3.m1.1a"><mo id="Ch8.T12.3.3.3.m1.1.1" stretchy="false" xref="Ch8.T12.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T12.3.3.3.m1.1b"><ci id="Ch8.T12.3.3.3.m1.1.1.cmml" xref="Ch8.T12.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T12.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T12.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch8.T12.3.3.5.1">Lines</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.3.3.6">
<table class="ltx_tabular ltx_align_middle" id="Ch8.T12.3.3.6.1">
<tr class="ltx_tr" id="Ch8.T12.3.3.6.1.1">
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.3.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch8.T12.3.3.6.1.1.1.1">Runtime</span></td>
</tr>
<tr class="ltx_tr" id="Ch8.T12.3.3.6.1.2">
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.3.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch8.T12.3.3.6.1.2.1.1">(hours)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch8.T12.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch8.T12.3.3.7.1">kWh</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T12.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.1">adaptMLLM en2ga</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.2">41.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.3">0.51</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.4">0.48</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.5">13k</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.6">3.51</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch8.T12.3.4.1.7">1.1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T12.3.5.2">
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.1">adaptMLLM ga2en</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.2">75.1</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.3">0.385</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.4">0.71</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.5">13k</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.6">3.41</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.5.2.7">1.1</td>
</tr>
<tr class="ltx_tr" id="Ch8.T12.3.6.3">
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.1">adaptMLLM en2mr</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.2">26.4</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.3">0.56</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.4">0.608</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.5">21k</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.6">5.49</td>
<td class="ltx_td ltx_align_left" id="Ch8.T12.3.6.3.7">1.8</td>
</tr>
<tr class="ltx_tr" id="Ch8.T12.3.7.4">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.1">adaptMLLM mr2en</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.2">52.6</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.3">0.409</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.4">0.74</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.5">21k</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.6">5.43</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch8.T12.3.7.4.7">1.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T12.5.1.1" style="font-size:90%;">Table 8.12</span>: </span><span class="ltx_text" id="Ch8.T12.6.2" style="font-size:90%;">Energy consumption during MLLM fine-tuning experiments. All experiments were carried out on Google Cloud with 0 kgCO<sub class="ltx_sub" id="Ch8.T12.6.2.1">2</sub> emissions.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Ch8.S9">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.9 </span>Discussion</h3>
<div class="ltx_para" id="Ch8.S9.p1">
<p class="ltx_p" id="Ch8.S9.p1.1">We used the adaptMLLM application to create MT models with datasets from the LoResMT2021 Shared Task to assess system efficiency when translating in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S9.p1.1.m1.1"><semantics id="Ch8.S9.p1.1.m1.1a"><mo id="Ch8.S9.p1.1.m1.1.1" stretchy="false" xref="Ch8.S9.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.p1.1.m1.1b"><ci id="Ch8.S9.p1.1.m1.1.1.cmml" xref="Ch8.S9.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.p1.1.m1.1d">↔</annotation></semantics></math>GA directions.</p>
</div>
<div class="ltx_para" id="Ch8.S9.p2">
<p class="ltx_p" id="Ch8.S9.p2.1">High-performing models achieving SOTA scores were developed by fine-tuning the NLLB MLLM pre-trained models with adaptMLLM. Using an easily-understood framework such as adaptMLLM, the benefits of developing high-performing fine-tuned models with small in-domain datasets are thus clear.</p>
</div>
<section class="ltx_subsection" id="Ch8.S9.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.9.1 </span>Performance of adaptMLLM Relative to Google Translate</h4>
<div class="ltx_para" id="Ch8.S9.SS1.p1">
<p class="ltx_p" id="Ch8.S9.SS1.p1.1">Translation engine performance, at the corpus level, was benchmarked against Google Translate’s <span class="ltx_note ltx_role_footnote" id="Ch8.footnote38"><sup class="ltx_note_mark">38</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">38</sup><span class="ltx_tag ltx_tag_note">38</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com" title="">https://translate.google.com</a></span></span></span> EN<math alttext="{\leftrightarrow}" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p1.1.m1.1"><semantics id="Ch8.S9.SS1.p1.1.m1.1a"><mo id="Ch8.S9.SS1.p1.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p1.1.m1.1b"><ci id="Ch8.S9.SS1.p1.1.m1.1.1.cmml" xref="Ch8.S9.SS1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p1.1.m1.1c">{\leftrightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p1.1.m1.1d">↔</annotation></semantics></math>GA translation service, which is freely available on the internet.</p>
</div>
<div class="ltx_para" id="Ch8.S9.SS1.p2">
<p class="ltx_p" id="Ch8.S9.SS1.p2.2">A full evaluation of Google Translate’s engines on the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p2.1.m1.1"><semantics id="Ch8.S9.SS1.p2.1.m1.1a"><mo id="Ch8.S9.SS1.p2.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p2.1.m1.1b"><ci id="Ch8.S9.SS1.p2.1.m1.1.1.cmml" xref="Ch8.S9.SS1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p2.1.m1.1d">→</annotation></semantics></math>GA test set generated a BLEU score of 38.7, a TER score of 0.493 and a ChrF3 of 0.633. The comparative scores on the test set using our fine-tuned MLLM realised 41.2 for BLEU, 0.489 for TER and 0.653 for ChrF3. Therefore, in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p2.2.m2.1"><semantics id="Ch8.S9.SS1.p2.2.m2.1a"><mo id="Ch8.S9.SS1.p2.2.m2.1.1" stretchy="false" xref="Ch8.S9.SS1.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p2.2.m2.1b"><ci id="Ch8.S9.SS1.p2.2.m2.1.1.cmml" xref="Ch8.S9.SS1.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p2.2.m2.1d">→</annotation></semantics></math>GA direction, the adaptMLLM system demonstrates a relative BLEU score improvement of 6.5% compared to Google Translate.</p>
</div>
<div class="ltx_para" id="Ch8.S9.SS1.p3">
<p class="ltx_p" id="Ch8.S9.SS1.p3.1">The translation output from our fine-tuned MLLMs was also compared with Google Translate using random samples from the LoResMT2021 EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p3.1.m1.1"><semantics id="Ch8.S9.SS1.p3.1.m1.1a"><mo id="Ch8.S9.SS1.p3.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS1.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p3.1.m1.1b"><ci id="Ch8.S9.SS1.p3.1.m1.1.1.cmml" xref="Ch8.S9.SS1.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p3.1.m1.1d">→</annotation></semantics></math>GA corpus. Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T13" title="Table 8.13 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.13</span></a> highlights random samples which were picked from the English source test file. A perfect match, with a BLEU of 100, was recorded in one instance which is unusual. However, this may occur on occasion with the translation of short sentences. Any duplicates between training and test data were removed before fine-tuning but the possibility exists of the test sentence forming part of the original training of the NLLB model.</p>
</div>
<div class="ltx_para" id="Ch8.S9.SS1.p4">
<p class="ltx_p" id="Ch8.S9.SS1.p4.1">Translation of these samples was independently carried out on the optimal fine-tuned MLLM model and also using Google Translate. Case-insensitive, sentence-level BLEU scores were recorded and are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T14" title="Table 8.14 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.14</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch8.T13">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T13.4.2.1" style="font-size:90%;">Table 8.13</span>: </span><span class="ltx_text" id="Ch8.T13.2.1" style="font-size:90%;">EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T13.2.1.m1.1"><semantics id="Ch8.T13.2.1.m1.1b"><mo id="Ch8.T13.2.1.m1.1.1" stretchy="false" xref="Ch8.T13.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T13.2.1.m1.1c"><ci id="Ch8.T13.2.1.m1.1.1.cmml" xref="Ch8.T13.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T13.2.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T13.2.1.m1.1e">→</annotation></semantics></math>GA test dataset of LoResMT2021: samples of human reference translations.</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch8.T13.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T13.5.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T13.5.1.1.1" style="width:204.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T13.5.1.1.1.1">Source Language (English)</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T13.5.1.1.2" style="width:204.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T13.5.1.1.2.1">Human Translation (Irish)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T13.5.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T13.5.2.1.1" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T13.5.2.1.1.1">Temporary Covid-19 Wage Subsidy Scheme</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T13.5.2.1.2" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T13.5.2.1.2.1">Scéim Fóirdheontais Shealadaigh Pá Covid-19</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T13.5.3.2">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T13.5.3.2.1" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T13.5.3.2.1.1">how Covid-19 spreads and its symptoms</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T13.5.3.2.2" style="width:204.9pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T13.5.3.2.2.1">conas a scaipeann Covid-19 agus na siomptóim a bhaineann leis</p>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Ch8.T14">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T14.6.2.1" style="font-size:90%;">Table 8.14</span>: </span><span class="ltx_text" id="Ch8.T14.2.1" style="font-size:90%;">EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T14.2.1.m1.1"><semantics id="Ch8.T14.2.1.m1.1b"><mo id="Ch8.T14.2.1.m1.1.1" stretchy="false" xref="Ch8.T14.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T14.2.1.m1.1c"><ci id="Ch8.T14.2.1.m1.1.1.cmml" xref="Ch8.T14.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T14.2.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T14.2.1.m1.1e">→</annotation></semantics></math>GA fine-tuned MLLM model compared with Google Translate. </span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch8.T14.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T14.4.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T14.4.2.3" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T14.4.2.3.1">fine-tuned MLLM</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T14.3.1.1" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch8.T14.3.1.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T14.3.1.1.1.1.m1.1"><semantics id="Ch8.T14.3.1.1.1.1.m1.1a"><mo id="Ch8.T14.3.1.1.1.1.m1.1.1" mathvariant="bold" stretchy="false" xref="Ch8.T14.3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T14.3.1.1.1.1.m1.1b"><ci id="Ch8.T14.3.1.1.1.1.m1.1.1.cmml" xref="Ch8.T14.3.1.1.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T14.3.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T14.3.1.1.1.1.m1.1d">bold_↑</annotation></semantics></math></p>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T14.4.2.4" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T14.4.2.4.1">Google Translate</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T14.4.2.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.2.2.1.1"><span class="ltx_text ltx_font_bold" id="Ch8.T14.4.2.2.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch8.T14.4.2.2.1.1.m1.1"><semantics id="Ch8.T14.4.2.2.1.1.m1.1a"><mo id="Ch8.T14.4.2.2.1.1.m1.1.1" mathvariant="bold" stretchy="false" xref="Ch8.T14.4.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch8.T14.4.2.2.1.1.m1.1b"><ci id="Ch8.T14.4.2.2.1.1.m1.1.1.cmml" xref="Ch8.T14.4.2.2.1.1.m1.1.1">bold-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T14.4.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T14.4.2.2.1.1.m1.1d">bold_↑</annotation></semantics></math></p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T14.4.3.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T14.4.3.1.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.3.1.1.1">Scéim Fóirdheontais Pá Sealadach Covid-19</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T14.4.3.1.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.3.1.2.1">25.4</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T14.4.3.1.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.3.1.3.1">Scéim Fóirdheontais Pá Shealadach Covid-19</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T14.4.3.1.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.3.1.4.1">25.4</p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T14.4.4.2">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T14.4.4.2.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.4.2.1.1">conas a scaipeann Covid-19 agus na siomptóim a bhaineann leis</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T14.4.4.2.2" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.4.2.2.1">100</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T14.4.4.2.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.4.2.3.1">conas a scaipeann Covid-19 agus na hairíonna a bhaineann leis</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T14.4.4.2.4" style="width:49.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T14.4.4.2.4.1">65.8</p>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ch8.S9.SS1.p5">
<p class="ltx_p" id="Ch8.S9.SS1.p5.3">The translation output from our fine-tuned MLLMs was also compared with Google Translate using random samples from the LoResMT2021 EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p5.1.m1.1"><semantics id="Ch8.S9.SS1.p5.1.m1.1a"><mo id="Ch8.S9.SS1.p5.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS1.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p5.1.m1.1b"><ci id="Ch8.S9.SS1.p5.1.m1.1.1.cmml" xref="Ch8.S9.SS1.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p5.1.m1.1d">→</annotation></semantics></math>MR corpus. A full evaluation of Google Translate’s engines on the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p5.2.m2.1"><semantics id="Ch8.S9.SS1.p5.2.m2.1a"><mo id="Ch8.S9.SS1.p5.2.m2.1.1" stretchy="false" xref="Ch8.S9.SS1.p5.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p5.2.m2.1b"><ci id="Ch8.S9.SS1.p5.2.m2.1.1.cmml" xref="Ch8.S9.SS1.p5.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p5.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p5.2.m2.1d">→</annotation></semantics></math>MR test set, with 500 lines, generated a BLEU score of 25.9, a TER score of 0.566 and a ChrF3 of 0.601. The comparative scores on the test set using our fine-tuned MLLM realised 26.4 for BLEU, 0.565 for TER and 0.608 for ChrF3. Therefore, in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p5.3.m3.1"><semantics id="Ch8.S9.SS1.p5.3.m3.1a"><mo id="Ch8.S9.SS1.p5.3.m3.1.1" stretchy="false" xref="Ch8.S9.SS1.p5.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p5.3.m3.1b"><ci id="Ch8.S9.SS1.p5.3.m3.1.1.cmml" xref="Ch8.S9.SS1.p5.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p5.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p5.3.m3.1d">→</annotation></semantics></math>MR direction, the adaptMLLM system demonstrates a relative BLEU score improvement of 1.9% compared to Google Translate.</p>
</div>
<div class="ltx_para" id="Ch8.S9.SS1.p6">
<p class="ltx_p" id="Ch8.S9.SS1.p6.1">Samples from the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS1.p6.1.m1.1"><semantics id="Ch8.S9.SS1.p6.1.m1.1a"><mo id="Ch8.S9.SS1.p6.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS1.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS1.p6.1.m1.1b"><ci id="Ch8.S9.SS1.p6.1.m1.1.1.cmml" xref="Ch8.S9.SS1.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS1.p6.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS1.p6.1.m1.1d">→</annotation></semantics></math>MR test set, along with the corresponding human translation are illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T15" title="Table 8.15 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.15</span></a>. The performance of these individual samples from the MLLM output and the Google Translation output is compared in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T16" title="Table 8.16 ‣ 8.9.1 Performance of adaptMLLM Relative to Google Translate ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.16</span></a>.
The results are encouraging and indicate a good performance by our translation models on the datasets from LoResMT2021.</p>
</div>
<figure class="ltx_table" id="Ch8.T15">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T15.4.2.1" style="font-size:90%;">Table 8.15</span>: </span><span class="ltx_text" id="Ch8.T15.2.1" style="font-size:90%;">Samples of human reference translations from EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T15.2.1.m1.1"><semantics id="Ch8.T15.2.1.m1.1b"><mo id="Ch8.T15.2.1.m1.1.1" stretchy="false" xref="Ch8.T15.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T15.2.1.m1.1c"><ci id="Ch8.T15.2.1.m1.1.1.cmml" xref="Ch8.T15.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T15.2.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T15.2.1.m1.1e">→</annotation></semantics></math>MR LoResMT2021</span></figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="147" id="Ch8.T15.g1" src="extracted/5444776/Images/translations_en2mr.png" width="598"/>
</figure>
<figure class="ltx_table" id="Ch8.T16">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch8.T16.4.2.1" style="font-size:90%;">Table 8.16</span>: </span><span class="ltx_text" id="Ch8.T16.2.1" style="font-size:90%;">EN<math alttext="{\rightarrow}" class="ltx_Math" display="inline" id="Ch8.T16.2.1.m1.1"><semantics id="Ch8.T16.2.1.m1.1b"><mo id="Ch8.T16.2.1.m1.1.1" stretchy="false" xref="Ch8.T16.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T16.2.1.m1.1c"><ci id="Ch8.T16.2.1.m1.1.1.cmml" xref="Ch8.T16.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T16.2.1.m1.1d">{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="Ch8.T16.2.1.m1.1e">→</annotation></semantics></math>MR fine-tuned MLLM model compared with Google Translate. MR phrases are back-translated to EN and highlighted immediately below each MR sentence pair.</span></figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="307" id="Ch8.T16.g1" src="extracted/5444776/Images/en2mr_gt.png" width="598"/>
</figure>
</section>
<section class="ltx_subsection" id="Ch8.S9.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.9.2 </span>Linguistic Observations</h4>
<div class="ltx_para" id="Ch8.S9.SS2.p1">
<p class="ltx_p" id="Ch8.S9.SS2.p1.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.T17" title="Table 8.17 ‣ 8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.17</span></a> provides a linguistic analysis of the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS2.p1.1.m1.1"><semantics id="Ch8.S9.SS2.p1.1.m1.1a"><mo id="Ch8.S9.SS2.p1.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS2.p1.1.m1.1b"><ci id="Ch8.S9.SS2.p1.1.m1.1.1.cmml" xref="Ch8.S9.SS2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS2.p1.1.m1.1d">→</annotation></semantics></math>GA MLLM outputs, showcasing the source sentences alongside their corresponding translations. These sentences were chosen specifically for this detailed human evaluation since they underscore the principal types of errors observed. The approach adopted is similar to the analysis taken in our previous human evaluation of EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS2.p1.2.m2.1"><semantics id="Ch8.S9.SS2.p1.2.m2.1a"><mo id="Ch8.S9.SS2.p1.2.m2.1.1" stretchy="false" xref="Ch8.S9.SS2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS2.p1.2.m2.1b"><ci id="Ch8.S9.SS2.p1.2.m2.1.1.cmml" xref="Ch8.S9.SS2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS2.p1.2.m2.1d">→</annotation></semantics></math>GA translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx66" title="">66</a>]</cite> in that it focuses on model output errors which fall into the categories of ‘interpreting meaning’ and ‘core grammatical errors’.</p>
</div>
<figure class="ltx_table" id="Ch8.T17">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8.17: </span>Linguistic analysis of EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.T17.2.m1.1"><semantics id="Ch8.T17.2.m1.1b"><mo id="Ch8.T17.2.m1.1.1" stretchy="false" xref="Ch8.T17.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.T17.2.m1.1c"><ci id="Ch8.T17.2.m1.1.1.cmml" xref="Ch8.T17.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.T17.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.T17.2.m1.1e">→</annotation></semantics></math>GA system output. Errors in the target translation are flagged in red with the corresponding source highlighted in blue.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch8.T17.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch8.T17.7.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T17.7.1.1.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.1.1.1.1" style="font-size:90%;">Type</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="Ch8.T17.7.1.1.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.1.1.2.1" style="font-size:90%;">Sentence</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch8.T17.7.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.2.1.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.2.1.1.1" style="font-size:90%;">EN-1</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.2.1.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.2.1.2.1"><span class="ltx_text" id="Ch8.T17.7.2.1.2.1.1" style="font-size:90%;color:#0000FF;">Covid-19 information</span><span class="ltx_text" id="Ch8.T17.7.2.1.2.1.2" style="font-size:90%;"> and advice for taxpayers and agents</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.3.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.3.2.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.3.2.1.1" style="font-size:90%;">GA-1</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.3.2.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.3.2.2.1"><span class="ltx_text" id="Ch8.T17.7.3.2.2.1.1" style="font-size:90%;">Eolas agus </span><span class="ltx_text" id="Ch8.T17.7.3.2.2.1.2" style="font-size:90%;color:#FF0000;">comhairle Covid-19</span><span class="ltx_text" id="Ch8.T17.7.3.2.2.1.3" style="font-size:90%;"> díocóirí cánach agus dionadaithe</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.4.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.4.3.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.4.3.1.1" style="font-size:90%;">EN-2</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.4.3.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.4.3.2.1"><span class="ltx_text" id="Ch8.T17.7.4.3.2.1.1" style="font-size:90%;color:#0000FF;">We understand</span><span class="ltx_text" id="Ch8.T17.7.4.3.2.1.2" style="font-size:90%;"> the unprecedented situation facing taxpayers as a result of the Covid-19 pandemic.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.5.4">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.5.4.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.5.4.1.1" style="font-size:90%;">GA-2</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.5.4.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.5.4.2.1"><span class="ltx_text" id="Ch8.T17.7.5.4.2.1.1" style="font-size:90%;color:#FF0000;">Tuigeann muid</span><span class="ltx_text" id="Ch8.T17.7.5.4.2.1.2" style="font-size:90%;"> an cás gan fasach atá roimh cháiníocóirí mar thoradh ar an bpaindéim Covid-19.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.6.5">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.6.5.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.6.5.1.1" style="font-size:90%;">EN-3</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.6.5.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.6.5.2.1"><span class="ltx_text" id="Ch8.T17.7.6.5.2.1.1" style="font-size:90%;">Further information on the Employment Wage Subsidy Scheme (EWSS) is available from the </span><span class="ltx_text" id="Ch8.T17.7.6.5.2.1.2" style="font-size:90%;color:#0000FF;">Employing people section</span><span class="ltx_text" id="Ch8.T17.7.6.5.2.1.3" style="font-size:90%;"> on this website.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.7.6">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.7.6.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.7.6.1.1" style="font-size:90%;">GA-3</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.7.6.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.7.6.2.1"><span class="ltx_text" id="Ch8.T17.7.7.6.2.1.1" style="font-size:90%;">Tá tuilleadh faisnéise ar Scéim Fóirdheontais Pá Fostaíochta (EWSS) ar fáil ón </span><span class="ltx_text" id="Ch8.T17.7.7.6.2.1.2" style="font-size:90%;color:#FF0000;">gcuid Fostaithe</span><span class="ltx_text" id="Ch8.T17.7.7.6.2.1.3" style="font-size:90%;"> ar an láithreán gréasáin seo.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.8.7">
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.8.7.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.8.7.1.1" style="font-size:90%;">EN-4</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Ch8.T17.7.8.7.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.8.7.2.1"><span class="ltx_text" id="Ch8.T17.7.8.7.2.1.1" style="font-size:90%;">Information for </span><span class="ltx_text" id="Ch8.T17.7.8.7.2.1.2" style="font-size:90%;color:#0000FF;">employers</span><span class="ltx_text" id="Ch8.T17.7.8.7.2.1.3" style="font-size:90%;"> on the Temporary Covid-19 Wage Subsidy Scheme is available from the Employing people section on this website.</span></p>
</td>
</tr>
<tr class="ltx_tr" id="Ch8.T17.7.9.8">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T17.7.9.8.1" style="width:56.9pt;padding-left:13.8pt;padding-right:13.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch8.T17.7.9.8.1.1" style="font-size:90%;">GA-4</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="Ch8.T17.7.9.8.2" style="width:284.5pt;padding-left:13.8pt;padding-right:13.8pt;">
<p class="ltx_p ltx_align_top" id="Ch8.T17.7.9.8.2.1"><span class="ltx_text" id="Ch8.T17.7.9.8.2.1.1" style="font-size:90%;">Tá faisnéis </span><span class="ltx_text" id="Ch8.T17.7.9.8.2.1.2" style="font-size:90%;color:#FF0000;">dfhostóirí</span><span class="ltx_text" id="Ch8.T17.7.9.8.2.1.3" style="font-size:90%;"> ar an Scéim Fóirdheontais Pá Sealadach Covid-19 ar fáil ón gcuid Fostaithe ar an láithreán gréasáin seo.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="Ch8.S9.SS2.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Interpreting Meaning</h5>
<div class="ltx_para" id="Ch8.S9.SS2.SSSx1.p1">
<p class="ltx_p" id="Ch8.S9.SS2.SSSx1.p1.1">When examining the relationship of one noun to another noun, it should not necessarily be directly translated from English to Irish. This is illustrated in EN-1 where “COVID-19 information and advice” refers to the information and advice that is related to COVID. However, the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS2.SSSx1.p1.1.m1.1"><semantics id="Ch8.S9.SS2.SSSx1.p1.1.m1.1a"><mo id="Ch8.S9.SS2.SSSx1.p1.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS2.SSSx1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS2.SSSx1.p1.1.m1.1b"><ci id="Ch8.S9.SS2.SSSx1.p1.1.m1.1.1.cmml" xref="Ch8.S9.SS2.SSSx1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS2.SSSx1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS2.SSSx1.p1.1.m1.1d">→</annotation></semantics></math>GA system translates this to “Comhairle COVID-19” which effectively means “COVID-19’s information and advice”, i.e. COVID-19 is treated as a possessive noun which is incorrect.</p>
</div>
<div class="ltx_para" id="Ch8.S9.SS2.SSSx1.p2">
<p class="ltx_p" id="Ch8.S9.SS2.SSSx1.p2.1">At times the translated output does not reflect the context in which particular words should be used. An example of this can be seen in the translation of the words “Employing people section” in EN-3 which was interpreted by the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch8.S9.SS2.SSSx1.p2.1.m1.1"><semantics id="Ch8.S9.SS2.SSSx1.p2.1.m1.1a"><mo id="Ch8.S9.SS2.SSSx1.p2.1.m1.1.1" stretchy="false" xref="Ch8.S9.SS2.SSSx1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch8.S9.SS2.SSSx1.p2.1.m1.1b"><ci id="Ch8.S9.SS2.SSSx1.p2.1.m1.1.1.cmml" xref="Ch8.S9.SS2.SSSx1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S9.SS2.SSSx1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S9.SS2.SSSx1.p2.1.m1.1d">→</annotation></semantics></math>GA system as “gcuid Fostaithe”. In this English source sentence, the meaning focuses on a section related to a website and the correct translation would be “rannán Daoine a Fhostú”. This is outlined in more detail on the reference website, Fóclóir.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote39"><sup class="ltx_note_mark">39</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">39</sup><span class="ltx_tag ltx_tag_note">39</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.focloir.ie" title="">https://www.focloir.ie</a></span></span></span> It is interesting to note that Google Translate correctly interprets this meaning in its translation of the sentence.</p>
</div>
<div class="ltx_para" id="Ch8.S9.SS2.SSSx1.p3">
<p class="ltx_p" id="Ch8.S9.SS2.SSSx1.p3.1">Given the nature of the source text, one word frequently encountered was “Information”. The word was accurately translated to “faisnéis” over the text, but it is important to note this word is not widely used in the Irish language. We recommend using the word “eolas” (knowledge) since it is a more natural and intuitive translation.<span class="ltx_note ltx_role_footnote" id="Ch8.footnote40"><sup class="ltx_note_mark">40</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">40</sup><span class="ltx_tag ltx_tag_note">40</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.teanglann.ie/en/fgb/eolas" title="">https://www.teanglann.ie/en/fgb/eolas</a></span></span></span></p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch8.S9.SS2.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Core Grammatical Errors</h5>
<div class="ltx_para" id="Ch8.S9.SS2.SSSx2.p1">
<p class="ltx_p" id="Ch8.S9.SS2.SSSx2.p1.1">Common mistakes which were encountered throughout the texts involved the use of the apostrophe. Most of these mistakes were flagged as minor errors but in some cases, a missing apostrophe conveyed an entirely different meaning. An example of this can be seen in EN-4 and GA-4 where “information for employers” has been translated to “faisnéis dfhostóirí” which means “employers’ information”. By simply correcting this to “faisnéis d’fhostóirí”, the correct meaning would have been preserved.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="Ch8.S10">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.10 </span>Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch8.S10.p1">
<p class="ltx_p" id="Ch8.S10.p1.2">We presented adaptMLLM, a comprehensive application designed for fine-tuning MLLMs and which handles the entire process of model development, evaluation, and deployment. The performance of the application was showcased through the creation of EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p1.1.m1.1"><semantics id="Ch8.S10.p1.1.m1.1a"><mo id="Ch8.S10.p1.1.m1.1.1" stretchy="false" xref="Ch8.S10.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p1.1.m1.1b"><ci id="Ch8.S10.p1.1.m1.1.1.cmml" xref="Ch8.S10.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p1.1.m1.1d">↔</annotation></semantics></math>GA translation models, which exhibited substantial improvements over the top-ranked models from the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p1.2.m2.1"><semantics id="Ch8.S10.p1.2.m2.1a"><mo id="Ch8.S10.p1.2.m2.1.1" stretchy="false" xref="Ch8.S10.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p1.2.m2.1b"><ci id="Ch8.S10.p1.2.m2.1.1.cmml" xref="Ch8.S10.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p1.2.m2.1d">↔</annotation></semantics></math>GA LoResMT2021 Shared Tasks.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p2">
<p class="ltx_p" id="Ch8.S10.p2.1">To further validate this work, a fine-grained human evaluation was conducted by annotators on the translation output in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p2.1.m1.1"><semantics id="Ch8.S10.p2.1.m1.1a"><mo id="Ch8.S10.p2.1.m1.1.1" stretchy="false" xref="Ch8.S10.p2.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p2.1.m1.1b"><ci id="Ch8.S10.p2.1.m1.1.1.cmml" xref="Ch8.S10.p2.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p2.1.m1.1d">↔</annotation></semantics></math>GA directions and the findings are outlined in our linguistic observations (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S9.SS2" title="8.9.2 Linguistic Observations ‣ 8.9 Discussion ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.9.2</span></a>).</p>
</div>
<div class="ltx_para" id="Ch8.S10.p3">
<p class="ltx_p" id="Ch8.S10.p3.3">As a multilingual tool, systems derived from adaptMLLM were also compared with the winning entries from the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p3.1.m1.1"><semantics id="Ch8.S10.p3.1.m1.1a"><mo id="Ch8.S10.p3.1.m1.1.1" stretchy="false" xref="Ch8.S10.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p3.1.m1.1b"><ci id="Ch8.S10.p3.1.m1.1.1.cmml" xref="Ch8.S10.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p3.1.m1.1d">↔</annotation></semantics></math>MR LoResMT2021 Shared Tasks. Fine-tuning 3.3B parameter NLLB models, using adaptMLLM demonstrated that our models for the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p3.2.m2.1"><semantics id="Ch8.S10.p3.2.m2.1a"><mo id="Ch8.S10.p3.2.m2.1.1" stretchy="false" xref="Ch8.S10.p3.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p3.2.m2.1b"><ci id="Ch8.S10.p3.2.m2.1.1.cmml" xref="Ch8.S10.p3.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p3.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p3.2.m2.1d">↔</annotation></semantics></math>MR language pair performed significantly better across all translation metrics when compared with the winning entries in the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p3.3.m3.1"><semantics id="Ch8.S10.p3.3.m3.1a"><mo id="Ch8.S10.p3.3.m3.1.1" stretchy="false" xref="Ch8.S10.p3.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p3.3.m3.1b"><ci id="Ch8.S10.p3.3.m3.1.1.cmml" xref="Ch8.S10.p3.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p3.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p3.3.m3.1d">↔</annotation></semantics></math>MR LoResMT2021 Shared Tasks.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p4">
<p class="ltx_p" id="Ch8.S10.p4.4">The performance of our translation models developed for this study was compared with the output generated by Google Translate on both the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p4.1.m1.1"><semantics id="Ch8.S10.p4.1.m1.1a"><mo id="Ch8.S10.p4.1.m1.1.1" stretchy="false" xref="Ch8.S10.p4.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p4.1.m1.1b"><ci id="Ch8.S10.p4.1.m1.1.1.cmml" xref="Ch8.S10.p4.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p4.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p4.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p4.2.m2.1"><semantics id="Ch8.S10.p4.2.m2.1a"><mo id="Ch8.S10.p4.2.m2.1.1" stretchy="false" xref="Ch8.S10.p4.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p4.2.m2.1b"><ci id="Ch8.S10.p4.2.m2.1.1.cmml" xref="Ch8.S10.p4.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p4.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p4.2.m2.1d">↔</annotation></semantics></math>MR language pairs. In all language directions, the performance of the adaptMLLM models was better than that of Google Translate demonstrating a new SOTA in low-resource MT of the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p4.3.m3.1"><semantics id="Ch8.S10.p4.3.m3.1a"><mo id="Ch8.S10.p4.3.m3.1.1" stretchy="false" xref="Ch8.S10.p4.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p4.3.m3.1b"><ci id="Ch8.S10.p4.3.m3.1.1.cmml" xref="Ch8.S10.p4.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p4.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p4.3.m3.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p4.4.m4.1"><semantics id="Ch8.S10.p4.4.m4.1a"><mo id="Ch8.S10.p4.4.m4.1.1" stretchy="false" xref="Ch8.S10.p4.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p4.4.m4.1b"><ci id="Ch8.S10.p4.4.m4.1.1.cmml" xref="Ch8.S10.p4.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p4.4.m4.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p4.4.m4.1d">↔</annotation></semantics></math>MR language pairs.
</p>
</div>
<div class="ltx_para" id="Ch8.S10.p5">
<p class="ltx_p" id="Ch8.S10.p5.1">In terms of future work, there is much which can be done to extend our research. There are several avenues which we plan on exploring further. Firstly we would like to establish the effects of fine-tuning larger MLLMs, such as the 54B parameter NLLB network, on our existing datasets. It is anticipated this will most likely improve our results and will also establish the trend in which increasingly larger MLLMs drive MT performance. The availability of the MTU and DCU GPU clusters within the ADAPT centre, coupled with the DeepSpeed library, provides the platform upon which this can be achieved.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p6">
<p class="ltx_p" id="Ch8.S10.p6.1">At this juncture, we have just scratched the surface of the MT performance enhancements which are possible through hyperparameter optimisation. Using a random search approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx16" title="">16</a>]</cite>, we will extend our search space by examining a greater number of hyperparameters and a larger range of associated values.
Against this backdrop, it will be possible to apply adaptMLLM to new shared tasks and WMT competitions. This will also address another goal of our future work which is to apply our approach to other low-resource language pairs.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p7">
<p class="ltx_p" id="Ch8.S10.p7.1">Furthermore, integration of GPT-3, GPT-4 and BARD (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S5.SS3" title="8.5.3 Large Language Models ‣ 8.5 Related Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.5.3</span></a>) playgrounds into adaptMLLM, in addition to fine-tuning these LLMs will be explored as part of future work.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p8">
<p class="ltx_p" id="Ch8.S10.p8.1">Once the preserve of large research teams with very significant compute infrastructure, our approach has shown much smaller research teams can fine-tune MLLMs on modest budgets. In doing so, we have succeeded in developing SOTA results for two low-resource language pairs. As an open-source initiative, we look forward to the community contributing to its advancement with the addition of fresh concepts and feature enhancements.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p9">
<p class="ltx_p" id="Ch8.S10.p9.2">We have shown in the context of our low-resource EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p9.1.m1.1"><semantics id="Ch8.S10.p9.1.m1.1a"><mo id="Ch8.S10.p9.1.m1.1.1" stretchy="false" xref="Ch8.S10.p9.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p9.1.m1.1b"><ci id="Ch8.S10.p9.1.m1.1.1.cmml" xref="Ch8.S10.p9.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p9.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p9.1.m1.1d">↔</annotation></semantics></math>MR and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch8.S10.p9.2.m2.1"><semantics id="Ch8.S10.p9.2.m2.1a"><mo id="Ch8.S10.p9.2.m2.1.1" stretchy="false" xref="Ch8.S10.p9.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch8.S10.p9.2.m2.1b"><ci id="Ch8.S10.p9.2.m2.1.1.cmml" xref="Ch8.S10.p9.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch8.S10.p9.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch8.S10.p9.2.m2.1d">↔</annotation></semantics></math>GA pairs that fine-tuning a pre-trained MLLM such as NLLB is a more efficient and effective approach than training a bespoke Transformer model from scratch.</p>
</div>
<div class="ltx_para" id="Ch8.S10.p10">
<p class="ltx_p" id="Ch8.S10.p10.1">In addition to improved performance, fine-tuning MLLMs saves both time and computational resources. Consequently, given the right infrastructure, we recommend using such an approach when developing MT systems for low-resource pairs in the future.</p>
</div>
</section>
<section class="ltx_section" id="Ch8.S11">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8.11 </span>Limitations of the Study</h3>
<div class="ltx_para" id="Ch8.S11.p1">
<p class="ltx_p" id="Ch8.S11.p1.1">With additional resources, some elements of this research could be expanded upon. While there is a satisfactory level of agreement between annotators, the inclusion of a larger pool of annotators would be beneficial. Moreover, evaluating a more extensive selection of lines with a finer classification of the MQM taxonomy could yield a deeper understanding of the MT outputs.</p>
</div>
<div class="ltx_para" id="Ch8.S11.p2">
<p class="ltx_p" id="Ch8.S11.p2.1">Whereas fine-tuning the baseline NLLB models highlighted a demonstrable improvement in translation quality using automatic metrics, a corresponding human evaluation of the baseline NLLB outputs was not conducted. As part of our future work, it is planned to conduct such an evaluation.</p>
</div>
<div class="ltx_para" id="Ch8.S11.p3">
<p class="ltx_p" id="Ch8.S11.p3.1">The focus of the study primarily centred on fine-tuning the NLLB base model since it was the most likely candidate for success in producing high-quality MT output for low-resource languages. Other LLMs, such as GPT-J, should also be investigated for fine-tuning experiments.</p>
</div>
<div class="ltx_para" id="Ch8.S11.p4">
<p class="ltx_p" id="Ch8.S11.p4.1">With more hardware resources, and a larger research team, the impact of even larger models such as NLLB-54B would have been explored. It is planned to address these limitations in our future work (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#Ch8.S10" title="8.10 Conclusion and Future Work ‣ Chapter 8 adaptMLLM: Fine-Tuning Multilingual Language Models"><span class="ltx_text ltx_ref_tag">8.10</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch8.S11.p5">
<p class="ltx_p" id="Ch8.S11.p5.1"><span class="ltx_text ltx_font_bold" id="Ch8.S11.p5.1.1">Author Contributions:</span> Writing—original draft, S.L.; Writing—review &amp; editing, H.A. and A.W. All authors have read and agreed to the published version of the manuscript.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch8.S11.p6">
<p class="ltx_p" id="Ch8.S11.p6.1"><span class="ltx_text ltx_font_bold" id="Ch8.S11.p6.1.1">Funding:</span> This research is supported by Science Foundation Ireland through ADAPT Centre (Grant 13/RC/2106) at Dublin City University. This research was also funded by the Staff Doctorate Scheme at the Munster Technological University.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch8.S11.p7">
<p class="ltx_p" id="Ch8.S11.p7.1"><span class="ltx_text ltx_font_bold" id="Ch8.S11.p7.1.1">Institutional Review Board Statement:</span> In the “Related Work” section of this paper, we discuss academic papers published at conferences and in academic journals. We ensure that all data used in our analysis were obtained legally and ethically. With regard to licensing for our application, adaptMLLM, it is covered by the Creative Commons Attribution 4.0 International License. We recognise the importance of responsible and ethical conduct in AI research and will continue to prioritise these values in our work.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch8.S11.p8">
<p class="ltx_p" id="Ch8.S11.p8.1"><span class="ltx_text ltx_font_bold" id="Ch8.S11.p8.1.1">Data Availability Statement:</span> The data presented in this study are openly available and can be found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/adaptNMT/adaptMLLM/" title="">https://github.com/adaptNMT/adaptMLLM/</a></p>
</div>
<div class="ltx_para ltx_noindent" id="Ch8.S11.p9">
<p class="ltx_p" id="Ch8.S11.p9.1"><span class="ltx_text ltx_font_bold" id="Ch8.S11.p9.1.1">Acknowledgments:</span>  We also thank our anonymous reviewers for their comments, and our annotators Darragh Lankford and Muireann Ní Chorcora for their meticulous work in annotating the system outputs.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch8.S11.p10">
<p class="ltx_p" id="Ch8.S11.p10.1"><span class="ltx_text ltx_font_bold" id="Ch8.S11.p10.1.1">Conflicts of Interest:</span>  The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch9">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 9 </span>Conclusion</h2>
<div class="ltx_para" id="Ch9.p1">
<p class="ltx_p" id="Ch9.p1.1">The main research contributions are summarised and the initial research questions laid out in Chapter 1 are reflected upon in this conclusion chapter. The lessons learnt are subsequently discussed and finally, roadmaps for future work are outlined.</p>
</div>
<div class="ltx_para" id="Ch9.p2">
<p class="ltx_p" id="Ch9.p2.1">This research has contributed to the field of NMT by thoroughly evaluating hyperparameter settings, proposing an innovative framework for the human evaluation of low-resource languages, and developing the adaptable and user-friendly tools of adaptNMT and adaptMLLM. The research findings provide valuable insights into improving translation performance for low-resource languages and lay the groundwork for further advancements in MT.</p>
</div>
<section class="ltx_section" id="Ch9.S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9.1 </span>Research contributions</h3>
<section class="ltx_subsection" id="Ch9.S1.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1.1 </span>HPO for Low-resource Languages</h4>
<div class="ltx_para" id="Ch9.S1.SS1.p1">
<p class="ltx_p" id="Ch9.S1.SS1.p1.1">A comprehensive evaluation of hyperparameter settings and model variations in the context of Transformer-based NMT for low-resource language pairs was conducted as a means of addressing RQ1. The study explored the impact of modifications in the number of layers, regularisation techniques, attention heads, subword model types, and vocabulary size, aiming to identify optimal settings for improving translation performance. The research demonstrated that HPO significantly enhances the translation quality of Transformer models, and recommendations for optimal hyperparameter settings for our low-resource models were proposed. The beneficial impact of subword models, such as BPE and unigram models, on translation performance for low-resource languages was also highlighted. However, it must be highlighted that HPO of the corresponding RNN models was not conducted as part of these experiments. Future work in this area should ensure the HPO of RNN models if such models are to be used as a baseline for comparison purposes.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch9.S1.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1.2 </span>Corpus Development and Guidelines</h4>
<div class="ltx_para" id="Ch9.S1.SS2.p1">
<p class="ltx_p" id="Ch9.S1.SS2.p1.1">The research question identified in RQ2 addresses a significant gap in NMT: while translation technology is advanced for high-resource languages, it lags in some domains for low-resource languages due to the scarcity of parallel datasets. Most efforts for such languages are concentrated on constructing large, generic datasets without focusing on the potential advantages of specialised, in-domain datasets. The impact of domain-specific data on MT quality was examined by formulating a health-focused dataset for the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S1.SS2.p1.1.m1.1"><semantics id="Ch9.S1.SS2.p1.1.m1.1a"><mo id="Ch9.S1.SS2.p1.1.m1.1.1" stretchy="false" xref="Ch9.S1.SS2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S1.SS2.p1.1.m1.1b"><ci id="Ch9.S1.SS2.p1.1.m1.1.1.cmml" xref="Ch9.S1.SS2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S1.SS2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S1.SS2.p1.1.m1.1d">↔</annotation></semantics></math>GA language pair, a notably low-resource combination. This work resulted in the development of gaHealth, a bilingual health data corpus tailored for the Irish language and also created linguistic guidelines instrumental for its development. Another contribution arising from RQ2 was the experimental findings from using the gaHealth dataset. By making these results, and the gaHealth corpus, openly accessible online, the study furnishes empirical evidence underscoring the utility of domain-centric datasets while laying a foundation for future explorations of in-domain corpora.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch9.S1.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1.3 </span>Human Evaluation of Low-resource Languages</h4>
<div class="ltx_para" id="Ch9.S1.SS3.p1">
<p class="ltx_p" id="Ch9.S1.SS3.p1.1">With RQ3, we systematically compared the outputs of RNN and Transformer systems, analysing and categorising the nature of translation errors produced by each system. A human evaluation based on the MQM error taxonomy demonstrated that Transformer-based NMT systems substantially reduce both accuracy and fluency errors compared to RNN-based models. A key research contribution was the development of a modified MQM framework which was subsequently incorporated into the tools developed in response to answering RQ4. This analysis serves as a foundation for future work to enhance translation quality in NMT models for low-resource language pairs.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch9.S1.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1.4 </span>Explainable AI Architectures</h4>
<div class="ltx_para" id="Ch9.S1.SS4.p1">
<p class="ltx_p" id="Ch9.S1.SS4.p1.1">Explainable AI (XAI) refers to AI systems that are designed to be transparent and understandable by humans. Furthermore, XAI should also make the decision-making processes of AI systems clear so that users can understand how and why a particular decision or prediction was made.</p>
</div>
<div class="ltx_para" id="Ch9.S1.SS4.p2">
<p class="ltx_p" id="Ch9.S1.SS4.p2.1">In the context of my research, one of the core objectives of XAI has been achieved, namely the delivery of transparent and easily understandable applications. Further work is required to address how particular translations are arrived at. As part of our future work, it should be relatively easy to expose the final SoftMax layer within the neural models so that probabilities associated with each potential translation are displayed. Of course, for a truly explainable architecture, it is important to go deeper into the network where gradient or attention-based methods could be used to find the most salient words in the input when determining system outputs.</p>
</div>
<div class="ltx_para" id="Ch9.S1.SS4.p3">
<p class="ltx_p" id="Ch9.S1.SS4.p3.1">The key research contribution arising from answering RQ4 was to develop a user-friendly and flexible tool (adaptNMT) for training and deploying NMT models. The adaptNMT application simplifies the entire development and deployment process, offering a streamlined approach for developers, translators and end users. Notable features include simplified setup and customisation with evaluation and deployment capabilities. A significant contribution was the introduction of a green report feature which promotes eco-friendly research by monitoring power consumption and kgCO<sub class="ltx_sub" id="Ch9.S1.SS4.p3.1.1">2</sub> emissions.</p>
</div>
<div class="ltx_para" id="Ch9.S1.SS4.p4">
<p class="ltx_p" id="Ch9.S1.SS4.p4.1">In particular, adaptNMT offers a real-time graphical view of model training via TensorBoard visualisation, enabling users to easily monitor their model’s training progress. It also incorporates SentencePiece to establish subword segmentation models, though users have the flexibility to select other subword model types during system development if they choose. Moreover, adaptNMT is versatile in its deployment; users can run it on local setups or utilise it as a Colab instance on Google Cloud, with the latter offering enhanced GPU and computing resources through the Colab Pro subscription.
</p>
</div>
<div class="ltx_para" id="Ch9.S1.SS4.p5">
<p class="ltx_p" id="Ch9.S1.SS4.p5.2">The research contributions of RQ4 were further refined with the development of a separate application to concentrate solely on MLLMs and LLMs including NLLB and GPT-J. Open-sourced as a standalone tool, adaptMLLM was evaluated for the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S1.SS4.p5.1.m1.1"><semantics id="Ch9.S1.SS4.p5.1.m1.1a"><mo id="Ch9.S1.SS4.p5.1.m1.1.1" stretchy="false" xref="Ch9.S1.SS4.p5.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S1.SS4.p5.1.m1.1b"><ci id="Ch9.S1.SS4.p5.1.m1.1.1.cmml" xref="Ch9.S1.SS4.p5.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S1.SS4.p5.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S1.SS4.p5.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S1.SS4.p5.2.m2.1"><semantics id="Ch9.S1.SS4.p5.2.m2.1a"><mo id="Ch9.S1.SS4.p5.2.m2.1.1" stretchy="false" xref="Ch9.S1.SS4.p5.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S1.SS4.p5.2.m2.1b"><ci id="Ch9.S1.SS4.p5.2.m2.1.1.cmml" xref="Ch9.S1.SS4.p5.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S1.SS4.p5.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S1.SS4.p5.2.m2.1d">↔</annotation></semantics></math>MR language pairs. The evaluation showcased significant improvements in translation performance through fine-tuning a 3.3B parameter NLLB MLLM using the adaptMLLM approach. Relative improvements ranging from 9% to 117% were achieved compared to baseline models submitted in the LoResMT2021 Shared Task.</p>
</div>
<div class="ltx_para" id="Ch9.S1.SS4.p6">
<p class="ltx_p" id="Ch9.S1.SS4.p6.1">The tools developed through research on RQ4 are both open-source applications that streamline all processes involved in the development and deployment of NMT models. Furthermore, models can be easily evaluated with various metrics and seamlessly deployed as an in-app translation service. The applications provide an intuitive interface for hyperparameter customisation, allowing users to fine-tune parameters specific to their chosen methodology.</p>
</div>
<div class="ltx_para" id="Ch9.S1.SS4.p7">
<p class="ltx_p" id="Ch9.S1.SS4.p7.1">The study also emphasised the vital role played by the DeepSpeed library in enabling efficient model training and prototype development to be conducted on a moderate infrastructure. Fine-tuning of MLLMs which had traditionally been the preserve of large tech corporations has effectively been democratised by this library in that large MLLMs and LLMs can be stored and trained on combined GPU and system RAM.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch9.S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9.2 </span>Research Impact</h3>
<div class="ltx_para" id="Ch9.S2.p1">
<p class="ltx_p" id="Ch9.S2.p1.3">The motivation for this research was primarily driven by a desire to use AI technology to improve MT for low-resource languages, particularly the EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S2.p1.1.m1.1"><semantics id="Ch9.S2.p1.1.m1.1a"><mo id="Ch9.S2.p1.1.m1.1.1" stretchy="false" xref="Ch9.S2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S2.p1.1.m1.1b"><ci id="Ch9.S2.p1.1.m1.1.1.cmml" xref="Ch9.S2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S2.p1.1.m1.1d">↔</annotation></semantics></math>GA language pair. The skills developed as part of an MSc. in AI were used to run fine-tuning experiments and make recommendations for optimal Transformer settings for low-resource languages. To facilitate such experimental runs, architectures were developed to provide standardised approaches which were repeatable. The tools developed as part of this work were used to train MT systems for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S2.p1.2.m2.1"><semantics id="Ch9.S2.p1.2.m2.1a"><mo id="Ch9.S2.p1.2.m2.1.1" stretchy="false" xref="Ch9.S2.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S2.p1.2.m2.1b"><ci id="Ch9.S2.p1.2.m2.1.1.cmml" xref="Ch9.S2.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S2.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S2.p1.2.m2.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S2.p1.3.m3.1"><semantics id="Ch9.S2.p1.3.m3.1a"><mo id="Ch9.S2.p1.3.m3.1.1" stretchy="false" xref="Ch9.S2.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S2.p1.3.m3.1b"><ci id="Ch9.S2.p1.3.m3.1.1.cmml" xref="Ch9.S2.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S2.p1.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S2.p1.3.m3.1d">↔</annotation></semantics></math>MR translations. These architectures have subsequently been made available to the community as open-source tools. We hope that such tools will be extended by others in ways which we have not foreseen, and also in ways which we have anticipated through the roadmaps set out below.</p>
</div>
<div class="ltx_para" id="Ch9.S2.p2">
<p class="ltx_p" id="Ch9.S2.p2.1">The impact on MT quality of fine-tuning with a small, high-quality in-domain corpus was clearly demonstrated. These results should provide the motivation for developing similar corpora in other domains using the guidelines laid out as part of our work.</p>
</div>
<div class="ltx_para" id="Ch9.S2.p3">
<p class="ltx_p" id="Ch9.S2.p3.1">While automatic evaluation has an important role to play in validating MT systems, humans are the ultimate arbiters who validate the quality of such systems. In recognising this, a framework has been put in place to assist practitioners in carrying out human validation experiments. This approach has been used to successfully evaluate the quality of the output from both XAI architectures which were developed. It is anticipated that our approach will be useful for human evaluation of other low-resource models.</p>
</div>
</section>
<section class="ltx_section" id="Ch9.S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9.3 </span>Lessons Learnt</h3>
<div class="ltx_para" id="Ch9.S3.p1">
<p class="ltx_p" id="Ch9.S3.p1.1">NMT models often struggle when dealing with language pairs that have limited training data. However, HPO of the Transformer model can lead to significant performance enhancements, especially in these low-resource scenarios. The choice of the correct subword model emerges as a pivotal factor driving translation performance.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p2">
<p class="ltx_p" id="Ch9.S3.p2.1">Further improvements can be achieved by optimising certain aspects of the model, such as the number of attention heads, the number of layers, and the employment of various regularisation techniques. Interestingly, using smaller and fewer layers within the Transformer model was found to be beneficial for performance when working with low-resource datasets.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p3">
<p class="ltx_p" id="Ch9.S3.p3.2">For the HPO of Transformer models in such settings, a random search approach is favoured. This preference helps reduce the costs and extended training durations typically associated with the grid search method. The research underscored the potential of Transformers, proving they can be effectively harnessed for low-resource translation tasks, such as EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p3.1.m1.1"><semantics id="Ch9.S3.p3.1.m1.1a"><mo id="Ch9.S3.p3.1.m1.1.1" stretchy="false" xref="Ch9.S3.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p3.1.m1.1b"><ci id="Ch9.S3.p3.1.m1.1.1.cmml" xref="Ch9.S3.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p3.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p3.2.m2.1"><semantics id="Ch9.S3.p3.2.m2.1a"><mo id="Ch9.S3.p3.2.m2.1.1" stretchy="false" xref="Ch9.S3.p3.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p3.2.m2.1b"><ci id="Ch9.S3.p3.2.m2.1.1.cmml" xref="Ch9.S3.p3.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p3.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p3.2.m2.1d">↔</annotation></semantics></math>MR translations.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p4">
<p class="ltx_p" id="Ch9.S3.p4.1">The custom development of an in-domain corpus is a tedious and time-consuming process. However, we have learnt that even a small in-domain dataset can have a very significant impact on the translation quality of MT systems. This was clearly illustrated in the development of a Covid corpus for the LoResMT2021 Shared Task and the subsequent development of gaHealth. Adhering to a pre-defined approach as laid out in our paper for LREC 2022 helped to accelerate both the development time and quality of the corpus.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p5">
<p class="ltx_p" id="Ch9.S3.p5.3">The linguistic quality of NMT systems was assessed by comparing the performance of RNN and Transformer systems using SQM and MQM as human evaluation metrics. The proposed approach in this study combines human evaluation with automatic metrics. From our human evaluation research, several key insights emerged regarding MT, particularly concerning the comparison between RNN- and Transformer-based EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p5.1.m1.1"><semantics id="Ch9.S3.p5.1.m1.1a"><mo id="Ch9.S3.p5.1.m1.1.1" stretchy="false" xref="Ch9.S3.p5.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p5.1.m1.1b"><ci id="Ch9.S3.p5.1.m1.1.1.cmml" xref="Ch9.S3.p5.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p5.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p5.1.m1.1d">↔</annotation></semantics></math>GA systems. This study presents a human evaluation that compares the output from EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p5.2.m2.1"><semantics id="Ch9.S3.p5.2.m2.1a"><mo id="Ch9.S3.p5.2.m2.1.1" stretchy="false" xref="Ch9.S3.p5.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p5.2.m2.1b"><ci id="Ch9.S3.p5.2.m2.1.1.cmml" xref="Ch9.S3.p5.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p5.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p5.2.m2.1d">↔</annotation></semantics></math>GA RNN systems against Transformer-based EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p5.3.m3.1"><semantics id="Ch9.S3.p5.3.m3.1a"><mo id="Ch9.S3.p5.3.m3.1.1" stretchy="false" xref="Ch9.S3.p5.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p5.3.m3.1b"><ci id="Ch9.S3.p5.3.m3.1.1.cmml" xref="Ch9.S3.p5.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p5.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p5.3.m3.1d">↔</annotation></semantics></math>GA models. The findings show a substantial correlation between human evaluation and automated methods. Both these evaluation methods converged on the finding that Transformer-based models displayed significantly higher accuracy. As part of the human evaluation, the misuse of common irregular verbs was flagged as a common problem but that could perhaps be rectified by refining our models with datasets meticulously curated for this particular challenge. Likewise, the strategic selection of training data for model fine-tuning could help reduce the register errors highlighted in our linguistic analysis.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p6">
<p class="ltx_p" id="Ch9.S3.p6.2">Several lessons were learnt from the empirical evaluation of the adaptMLLM approach, which was used to train models for two language pairs namely EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p6.1.m1.1"><semantics id="Ch9.S3.p6.1.m1.1a"><mo id="Ch9.S3.p6.1.m1.1.1" stretchy="false" xref="Ch9.S3.p6.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p6.1.m1.1b"><ci id="Ch9.S3.p6.1.m1.1.1.cmml" xref="Ch9.S3.p6.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p6.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p6.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p6.2.m2.1"><semantics id="Ch9.S3.p6.2.m2.1a"><mo id="Ch9.S3.p6.2.m2.1.1" stretchy="false" xref="Ch9.S3.p6.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p6.2.m2.1b"><ci id="Ch9.S3.p6.2.m2.1.1.cmml" xref="Ch9.S3.p6.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p6.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p6.2.m2.1d">↔</annotation></semantics></math>MR.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p7">
<p class="ltx_p" id="Ch9.S3.p7.1">Infrastructure and hyperparameters play a foundational role in the adaptMLLM system. The availability of the Google Colab Pro+ subscription, supplied with NVIDIA 40GB GPU graphics cards and access to up to 89GB of system memory, proved invaluable. This resource allowed for rapid prototype development, which was further amplified by the DeepSpeed library. This library is integral to the adaptMLLM system, facilitating the loading of models across both GPU and system memory, effectively keeping computational costs in check.</p>
</div>
<div class="ltx_para" id="Ch9.S3.p8">
<p class="ltx_p" id="Ch9.S3.p8.4">Fine-tuning results were particularly promising. When using adaptMLLM to refine the NLLB MLLM, there was a marked improvement in translation performance. Specifically, for EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p8.1.m1.1"><semantics id="Ch9.S3.p8.1.m1.1a"><mo id="Ch9.S3.p8.1.m1.1.1" stretchy="false" xref="Ch9.S3.p8.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p8.1.m1.1b"><ci id="Ch9.S3.p8.1.m1.1.1.cmml" xref="Ch9.S3.p8.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p8.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p8.1.m1.1d">→</annotation></semantics></math>GA, adaptMLLM demonstrated a relative enhancement of 14.4% over the top-performing system in the LoResMT2021 Shared Task. The GA<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p8.2.m2.1"><semantics id="Ch9.S3.p8.2.m2.1a"><mo id="Ch9.S3.p8.2.m2.1.1" stretchy="false" xref="Ch9.S3.p8.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p8.2.m2.1b"><ci id="Ch9.S3.p8.2.m2.1.1.cmml" xref="Ch9.S3.p8.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p8.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p8.2.m2.1d">→</annotation></semantics></math>EN counterpart displayed a more significant improvement still, with a relative improvement of 117% in BLEU score compared to the task’s best model. Likewise, for translation in the EN<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p8.3.m3.1"><semantics id="Ch9.S3.p8.3.m3.1a"><mo id="Ch9.S3.p8.3.m3.1.1" stretchy="false" xref="Ch9.S3.p8.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p8.3.m3.1b"><ci id="Ch9.S3.p8.3.m3.1.1.cmml" xref="Ch9.S3.p8.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p8.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p8.3.m3.1d">→</annotation></semantics></math>MR direction, the adaptMLLM system surpassed the winning team’s results in the shared task by 9%. In the reverse direction, MR<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Ch9.S3.p8.4.m4.1"><semantics id="Ch9.S3.p8.4.m4.1a"><mo id="Ch9.S3.p8.4.m4.1.1" stretchy="false" xref="Ch9.S3.p8.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Ch9.S3.p8.4.m4.1b"><ci id="Ch9.S3.p8.4.m4.1.1.cmml" xref="Ch9.S3.p8.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S3.p8.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S3.p8.4.m4.1d">→</annotation></semantics></math>EN, the gain was a relative improvement of 68% in the BLEU score.</p>
</div>
</section>
<section class="ltx_section" id="Ch9.S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9.4 </span>Future Work</h3>
<section class="ltx_subsection" id="Ch9.S4.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4.1 </span>Hyperparameter Tuning</h4>
<div class="ltx_para" id="Ch9.S4.SS1.p1">
<p class="ltx_p" id="Ch9.S4.SS1.p1.1">An initial evaluation of the impact of varying the number of attention heads, the number of layers, and experimentation with other regularisation techniques, such as label smoothing, has been conducted. Widening the hyperparameter search space should lead to marginal improvements in the performance of Transformer models in low-resource scenarios. In addition, the performance of the optimised Transformer model should be evaluated on other low-resource language pairs.</p>
</div>
<div class="ltx_para" id="Ch9.S4.SS1.p2">
<p class="ltx_p" id="Ch9.S4.SS1.p2.1">The effects of using different training data, such as news articles or social media posts, on the performance of Transformer models in low-resource settings could be evaluated. This exploration would provide insights into the effectiveness of using different types of data for training.</p>
</div>
<div class="ltx_para" id="Ch9.S4.SS1.p3">
<p class="ltx_p" id="Ch9.S4.SS1.p3.1">The performance of the optimised Transformer model could be compared with other SOTA models, such as convolutional neural networks. This comparison will identify the strengths and weaknesses of the Transformer model in low-resource scenarios.</p>
</div>
<div class="ltx_para" id="Ch9.S4.SS1.p4">
<p class="ltx_p" id="Ch9.S4.SS1.p4.1">While some transfer learning techniques have been incorporated into the adaptNMT and adaptMLLM applications, others will be explored to improve the performance of the Transformer model in low-resource scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch9.S4.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4.2 </span>Corpus Development</h4>
<div class="ltx_para" id="Ch9.S4.SS2.p1">
<p class="ltx_p" id="Ch9.S4.SS2.p1.1">The substantial impact that a small in-domain health dataset can have on MT quality has been observed, and empirical evidence has been provided through RQ2. This underscores the importance of further development of in-domain datasets. As part of the background work for gaHealth, the availability of parallel data from other domains such as Finance, Education and Agriculture was explored. The departments of the Irish government publish much of their work, including strategy statements and annual reports, bilingually in English and Irish. Although such reports are not organised as parallel text corpora, the application of the guidelines developed by RQ2 could be applied in the creation of a new set of concise in-domain corpora. The availability of an array of in-domain corpora could be a platform for developing a high-performing MT ensemble for EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S4.SS2.p1.1.m1.1"><semantics id="Ch9.S4.SS2.p1.1.m1.1a"><mo id="Ch9.S4.SS2.p1.1.m1.1.1" stretchy="false" xref="Ch9.S4.SS2.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S4.SS2.p1.1.m1.1b"><ci id="Ch9.S4.SS2.p1.1.m1.1.1.cmml" xref="Ch9.S4.SS2.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S4.SS2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S4.SS2.p1.1.m1.1d">↔</annotation></semantics></math>GA systems.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch9.S4.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4.3 </span>Human Evaluation</h4>
<div class="ltx_para" id="Ch9.S4.SS3.p1">
<p class="ltx_p" id="Ch9.S4.SS3.p1.1">Further human evaluation studies should be conducted to gain a deeper understanding of the linguistic errors generated by NMT models and to develop strategies to address these errors. Techniques to incorporate external linguistic resources, such as morphological analysers or dictionaries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx89" title="">89</a>]</cite>, into NMT models could be developed which would potentially improve MT performance. The proposed approach could be applied to other low-resource language pairs to test how well it generalises.</p>
</div>
<div class="ltx_para" id="Ch9.S4.SS3.p2">
<p class="ltx_p" id="Ch9.S4.SS3.p2.1">The outputs from the human evaluation aspect of the study have helped in understanding the quality and the failings of our translation of our models. In our human evaluation, we propose combining a human evaluation approach with automatic metrics to test the effectiveness of the models in creating high-quality MT outputs. The research aims to explore how NMT systems handle translation issues compared to RNN approaches in morphologically rich languages.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch9.S4.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4.4 </span>Explainable AI Architectures</h4>
<section class="ltx_subsubsection" id="Ch9.S4.SS4.SSSx1">
<h5 class="ltx_title ltx_title_subsubsection">Roadmap for adaptNMT</h5>
<div class="ltx_para" id="Ch9.S4.SS4.SSSx1.p1">
<p class="ltx_p" id="Ch9.S4.SS4.SSSx1.p1.1">The application could be expanded to incorporate other NMT architectures, such as the recently introduced non-autoregressive models. Other subword segmentation models, beyond the currently implemented SentencePiece models, should be integrated into the adaptNMT system.</p>
</div>
<div class="ltx_para" id="Ch9.S4.SS4.SSSx1.p2">
<p class="ltx_p" id="Ch9.S4.SS4.SSSx1.p2.1">The following enhancements to the adaptNMT architecture should be incorporated as part of a future research roadmap:</p>
<ul class="ltx_itemize" id="Ch9.S4.I1">
<li class="ltx_item" id="Ch9.S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i1.p1">
<p class="ltx_p" id="Ch9.S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i1.p1.1.1">Enhancement of the green report feature</span> to provide more detailed information on the environmental impact of NMT model development and deployment, and suggestions for eco-friendly practices. Currently, the green report flags the power consumption and kgCO<sub class="ltx_sub" id="Ch9.S4.I1.i1.p1.1.2">2</sub> emissions generated during model development, but additional green features such as a recommender function that suggests energy-efficient model architectures or automatic selection of eco-friendly compute resources could be explored.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i2.p1">
<p class="ltx_p" id="Ch9.S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i2.p1.1.1">Development of a user community</span> for sharing models, datasets, and best practices, which could be facilitated through the application’s Colab notebook format.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i3.p1">
<p class="ltx_p" id="Ch9.S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i3.p1.1.1">Improvement of the automatic notification</span> feature to include more detailed information on model training convergence and accuracy, and suggestions for model refinement.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i4.p1">
<p class="ltx_p" id="Ch9.S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i4.p1.1.1">Integration with additional datasets and languages</span>: adaptNMT could be expanded to include a wider range of language pairs and training datasets, enabling the tool to be used for a broader range of applications.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i5.p1">
<p class="ltx_p" id="Ch9.S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i5.p1.1.1">Development of a mobile version</span>: The mobile web version of adaptNMT works similarly to other Colab notebooks running on mobile platforms, and users can build and deploy NMT models on the go. However, a dedicated application would improve its usability for mobile users.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i6.p1">
<p class="ltx_p" id="Ch9.S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i6.p1.1.1">Integration with other NLP tools</span>: Integrating adaptNMT with other NLP tools such as part-of-speech taggers, named entity recognition tools, and sentiment analysers could enhance the capabilities of the tool and enable more sophisticated NMT models to be built. Such integration would also facilitate more advanced pre-processing and post-processing of NMT outputs.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i7.p1">
<p class="ltx_p" id="Ch9.S4.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i7.p1.1.1">Implementation of additional evaluation metrics</span>: While adaptNMT currently supports a range of evaluation metrics, additional metrics such as BLEU+ could be implemented to provide a more comprehensive evaluation of NMT models.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I1.i8.p1">
<p class="ltx_p" id="Ch9.S4.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I1.i8.p1.1.1">Integration with cloud-based ML platforms</span>: Integrating adaptNMT with cloud-based ML platforms such as Amazon SageMaker or Microsoft Azure could enable more powerful model development and deployment, as well as easier collaboration between researchers and teams.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="Ch9.S4.SS4.SSSx2">
<h5 class="ltx_title ltx_title_subsubsection">Roadmap for adaptMLLM</h5>
<div class="ltx_para" id="Ch9.S4.SS4.SSSx2.p1">
<p class="ltx_p" id="Ch9.S4.SS4.SSSx2.p1.1">To achieve better translation performance, further optimisation of the infrastructure and hyperparameters for adaptMLLM models using multiple language pairs should be considered. Additionally, the adaptMLLM approach can be evaluated on more language pairs to see if it generalises well to other languages. Overall, the feature roadmap for adaptMLLM should focus on making the application more user-friendly, versatile, and accessible to a wider audience. In particular, the following roadmap should be considered:</p>
<ul class="ltx_itemize" id="Ch9.S4.I2">
<li class="ltx_item" id="Ch9.S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i1.p1">
<p class="ltx_p" id="Ch9.S4.I2.i1.p1.2"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i1.p1.2.1">Greater multilingual support</span>: adaptMLLM is currently available for two language pairs, EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S4.I2.i1.p1.1.m1.1"><semantics id="Ch9.S4.I2.i1.p1.1.m1.1a"><mo id="Ch9.S4.I2.i1.p1.1.m1.1.1" stretchy="false" xref="Ch9.S4.I2.i1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S4.I2.i1.p1.1.m1.1b"><ci id="Ch9.S4.I2.i1.p1.1.m1.1.1.cmml" xref="Ch9.S4.I2.i1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S4.I2.i1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S4.I2.i1.p1.1.m1.1d">↔</annotation></semantics></math>GA and EN<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch9.S4.I2.i1.p1.2.m2.1"><semantics id="Ch9.S4.I2.i1.p1.2.m2.1a"><mo id="Ch9.S4.I2.i1.p1.2.m2.1.1" stretchy="false" xref="Ch9.S4.I2.i1.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch9.S4.I2.i1.p1.2.m2.1b"><ci id="Ch9.S4.I2.i1.p1.2.m2.1.1.cmml" xref="Ch9.S4.I2.i1.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch9.S4.I2.i1.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch9.S4.I2.i1.p1.2.m2.1d">↔</annotation></semantics></math>MR. To make the application more versatile, additional language pairs could be added in the future.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i2.p1">
<p class="ltx_p" id="Ch9.S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i2.p1.1.1">Fine-tuning with different pre-trained models</span>: Currently, adaptMLLM fine-tunes MLLMs using pre-trained models like NLLB. This could be extended to include the ability to fine-tune with other pre-trained models.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i3.p1">
<p class="ltx_p" id="Ch9.S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i3.p1.1.1">Interactive visualisation of model training</span>: While TensorBoard provides real-time graphical views of model training, an interactive visualisation could provide a more intuitive way to track the training progress.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i4.p1">
<p class="ltx_p" id="Ch9.S4.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i4.p1.1.1">Easy integration with other NLP libraries</span>: adaptMLLM could be made more user-friendly by providing easy integration with other popular NLP libraries like spaCy and NLTK.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i5.p1">
<p class="ltx_p" id="Ch9.S4.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i5.p1.1.1">Integration of different pre-processing techniques</span>: adaptMLLM could be enhanced by integrating different pre-processing techniques like data cleaning, normalisation, and augmentation to improve the quality of training data.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i6.p1">
<p class="ltx_p" id="Ch9.S4.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i6.p1.1.1">Automatic tuning of hyperparameters</span>: While adaptMLLM provides an intuitive user interface for hyperparameter customisation, it could be further enhanced by incorporating automatic tuning of hyperparameters.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i7.p1">
<p class="ltx_p" id="Ch9.S4.I2.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i7.p1.1.1">Support for transfer learning</span>: adaptMLLM could be enhanced by incorporating support for transfer learning, which could enable users to transfer knowledge from one language pair to another.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i8.p1">
<p class="ltx_p" id="Ch9.S4.I2.i8.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i8.p1.1.1">Generalising the adaptMLLM approach</span>: the adaptMLLM approach was developed specifically for low-resource MT. Future work could include exploring ways to generalise the approach for other NLP tasks.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i9.p1">
<p class="ltx_p" id="Ch9.S4.I2.i9.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i9.p1.1.1">Scaling the adaptMLLM approach</span>: the adaptMLLM approach was developed using a 3.3B parameter NLLB MLLM. Future work could involve scaling the adaptMLLM approach to bigger MLLMs.</p>
</div>
</li>
<li class="ltx_item" id="Ch9.S4.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch9.S4.I2.i10.p1">
<p class="ltx_p" id="Ch9.S4.I2.i10.p1.1"><span class="ltx_text ltx_font_bold" id="Ch9.S4.I2.i10.p1.1.1">Optimising the infrastructure and hyperparameters</span>: the infrastructure and hyperparameters used for developing models for the language pairs could be significantly enhanced. Future work will involve enhancing the training process by optimising both the infrastructure and hyperparameters used for developing models for the language pairs.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="Ch9.S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9.5 </span>Final Remarks</h3>
<div class="ltx_para" id="Ch9.S5.p1">
<p class="ltx_p" id="Ch9.S5.p1.1">The applications developed to improve the XAI of MT address a common problem, namely the streamlining of NMT development for developers and translators. However, each application’s approach and infrastructure requirements differ significantly. With adaptNMT, Transformer models can be built from scratch using a modest architecture with limited system memory and less powerful, cheaper GPUs.</p>
</div>
<div class="ltx_para" id="Ch9.S5.p2">
<p class="ltx_p" id="Ch9.S5.p2.1">In the case of adaptMLLM, the approach taken is to fine-tune large pre-trained MLLM models which is demanding both in terms of GPU and system memory requirements. In our research, we have concentrated on pre-trained models with 3.3 billion parameters but with the right infrastructure, there is the capacity to scale to 54 billion parameters.</p>
</div>
<div class="ltx_para" id="Ch9.S5.p3">
<p class="ltx_p" id="Ch9.S5.p3.1">Furthermore, both applications are built on very different MT engines. The OpenNMT framework is the underlying MT engine for adaptNMT whereas the HuggingFace platform is at the core of adaptMLLM. This has important implications for how both applications can be developed by the MT community and possibly integrated into other tools. Therefore, it is envisaged that both projects will coexist with each other.</p>
</div>
<div class="ltx_para" id="Ch9.S5.p4">
<p class="ltx_p" id="Ch9.S5.p4.1">The research contributions arising from this work will not solve all the problems encountered in developing MT systems for low-resource languages. However, the approach adopted may serve as a handbook for how low-resource MT systems can be developed. Through the use of HPO and XAI architectures, coupled with small custom-developed corpora and a straightforward human evaluation framework, SOTA in the field of low-resource MT can be achieved.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>No Language Left Behind</h2>
<figure class="ltx_figure" id="A1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="306" id="A1.F1.g1" src="extracted/5444776/Images/nllb_overview.png" width="608"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F1.2.1.1" style="font-size:90%;">Figure A.1</span>: </span><span class="ltx_text" id="A1.F1.3.2" style="font-size:90%;">Overview of the NLLB approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#bib.bibx32" title="">32</a>]</cite></span></figcaption>
</figure>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Meta employs several advanced techniques in its No Language Left Behind (NLLB) architecture including a mixture of experts (MoE), curriculum learning, self-supervised training, backtranslation and an NLLB Seed. The MoE approach efficiently scales NNs using ‘experts’ (sub-networks) which are specialised in different types of tasks or data. MoE enables the model to handle the diversity of languages more effectively. Each ‘expert’ can focus on specific language features or translation nuances, making the model powerful and efficient.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Curriculum learning involves training a model on tasks of increasing complexity. This step-by-step approach can help in better generalisation and handling of complex language translations. Furthermore, backtranslation is used to generate synthetic parallel sentences, which are particularly useful for training translation models in language pairs with scarce data.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">An NLLB-seed is a foundational dataset or pre-trained model used as the starting point for the NLLB project. The NLLB-seed facilitates transfer learning by providing a robust base model that can be fine-tuned or adjusted for specific languages or language pairs, especially those with limited training data. Using NLLB-Seed means that the model already has a significant understanding of linguistic patterns and structures, which it can then build upon.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">The NLLB project uses a diverse training dataset to support its goal of providing high-quality translations for a wide range of languages, including many that are underrepresented low-resource languages. The principal components of the training dataset are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01580v1#A1.F1" title="Figure A.1 ‣ Appendix A No Language Left Behind"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p" id="A1.p5.1">Publicly available datasets that are commonly found in MT and language research are used. These include datasets from WMT<span class="ltx_note ltx_role_footnote" id="A1.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://machinetranslate.org/wmt</span></span></span> (Workshop on Machine Translation), OPUS<span class="ltx_note ltx_role_footnote" id="A1.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/Helsinki-NLP/Opus-MT</span></span></span> (from the University of Helsinki), and other similar resources. Furthermore, specially curated datasets for low-resource languages have been created. This involves collecting texts from diverse sources, including books and websites which are then translated and included in the training dataset.</p>
</div>
<div class="ltx_para" id="A1.p6">
<p class="ltx_p" id="A1.p6.1">To further boost the quality of the datasets, the NLLB project has collaborated with language experts and native speakers, especially for low-resource languages. These collaborations help in verifying the quality of translations and in collecting authentic language data. Finally, the dataset also includes data from social media platforms and other online sources which is useful for capturing contemporary usage and evolving language trends.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Abubakar Abid et al.
</span>
<span class="ltx_bibblock">“Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild”, 2019
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1906.02569" title="">1906.02569 [cs.LG]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Haithem Afli, Sorcha Maguire and Andy Way
</span>
<span class="ltx_bibblock">“Sentiment translation for low resourced languages: experiments on Irish general election Tweets” Unpublished
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx2.1.1">18th International Conference on Computational Linguistics and Intelligent Text Processing</em>, 2017
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doras.dcu.ie/23370/" title="">https://doras.dcu.ie/23370/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Benyamin Ahmadnia and Bonnie J. Dorr
</span>
<span class="ltx_bibblock">“Augmenting Neural Machine Translation through Round-Trip Training Approach”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx3.1.1">Open Computer Science</em> <span class="ltx_text ltx_font_bold" id="bib.bibx3.2.2">9.1</span>, 2019, pp. 268–278
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/doi:10.1515/comp-2019-0019" title="">doi:10.1515/comp-2019-0019</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Khorshed Alam and Sophia Imran
</span>
<span class="ltx_bibblock">“The Digital Divide and Social Inclusion among Refugee Migrants: A Case in Regional Australia”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx4.1.1">Information Technology and People</em> <span class="ltx_text ltx_font_bold" id="bib.bibx4.2.2">28</span>, 2015
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1108/ITP-04-2014-0083" title="">10.1108/ITP-04-2014-0083</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Ali Araabi and Christof Monz
</span>
<span class="ltx_bibblock">“Optimizing Transformer for Low-Resource Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx5.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>
</span>
<span class="ltx_bibblock">Barcelona, Spain (Online): International Committee on Computational Linguistics, 2020, pp. 3429–3435
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2020.coling-main.304" title="">10.18653/v1/2020.coling-main.304</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Sanjay K. Arora, Yin Li, Jan Youtie and Philip Shapira
</span>
<span class="ltx_bibblock">“Using the wayback machine to mine websites in the social sciences: A methodological resource”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx6.1.1">Journal of the Association for Information Science and Technology</em> <span class="ltx_text ltx_font_bold" id="bib.bibx6.2.2">67.8</span>, 2016, pp. 1904–1915
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1002/asi.23503" title="">https://doi.org/10.1002/asi.23503</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Mikel Artetxe and Holger Schwenk
</span>
<span class="ltx_bibblock">“Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx7.1.1">Transactions of the Association for Computational Linguistics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx7.2.2">7</span>
</span>
<span class="ltx_bibblock">Cambridge, MA: MIT Press, 2019, pp. 597–610
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1162/tacl_a_00288" title="">10.1162/tacl˙a˙00288</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Ron Artstein
</span>
<span class="ltx_bibblock">“Inter-annotator Agreement”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx8.1.1">Handbook of Linguistic Annotation</em>
</span>
<span class="ltx_bibblock">Dordrecht: Springer Netherlands, 2017, pp. 297–313
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1007/978-94-024-0881-2_11" title="">10.1007/978-94-024-0881-2˙11</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Duygu Ataman and Marcello Federico
</span>
<span class="ltx_bibblock">“Compositional Representation of Morphologically-Rich Input for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx9.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>
</span>
<span class="ltx_bibblock">Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 305–311
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P18-2049" title="">10.18653/v1/P18-2049</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio
</span>
<span class="ltx_bibblock">“Neural Machine Translation by Jointly Learning to Align and Translate”, 2016
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1409.0473" title="">1409.0473 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Nesrine Bannour, Sahar Ghannay, Aurélie Névéol and Anne-Laure Ligozat
</span>
<span class="ltx_bibblock">“Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx11.1.1">Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</em>
</span>
<span class="ltx_bibblock">Virtual: Association for Computational Linguistics, 2021, pp. 11–21
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2021.sustainlp-1.2" title="">10.18653/v1/2021.sustainlp-1.2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Alejandro Barredo Arrieta et al.
</span>
<span class="ltx_bibblock">“Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx12.1.1">Information Fusion</em> <span class="ltx_text ltx_font_bold" id="bib.bibx12.2.2">58</span>, 2020, pp. 82–115
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1016/j.inffus.2019.12.012" title="">https://doi.org/10.1016/j.inffus.2019.12.012</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">María Do Campo Bayón and Pilar Sánchez-Gijón
</span>
<span class="ltx_bibblock">“Evaluating machine translation in a low-resource language combination: Spanish-Galician.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx13.1.1">Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks</em>
</span>
<span class="ltx_bibblock">Dublin, Ireland: European Association for Machine Translation, 2019, pp. 30–35
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-6705" title="">https://aclanthology.org/W19-6705</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Anya Belz et al.
</span>
<span class="ltx_bibblock">“Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)”
</span>
<span class="ltx_bibblock">Association for Computational Linguistics, 2021
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.humeval-1.0" title="">https://aclanthology.org/2021.humeval-1.0</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell
</span>
<span class="ltx_bibblock">“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx15.1.1">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT ’21
</span>
<span class="ltx_bibblock">Virtual Event, Canada: Association for Computing Machinery, 2021, pp. 610–623
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1145/3442188.3445922" title="">10.1145/3442188.3445922</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">James Bergstra and Yoshua Bengio
</span>
<span class="ltx_bibblock">“Random Search for Hyper-Parameter Optimization”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx16.1.1">Journal of Machine Learning Research</em> <span class="ltx_text ltx_font_bold" id="bib.bibx16.2.2">13.10</span>, 2012, pp. 281–305
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v13/bergstra12a.html" title="">http://jmlr.org/papers/v13/bergstra12a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Elan Van Biljon, Arnu Pretorius and Julia Kreutzer
</span>
<span class="ltx_bibblock">“On Optimal Transformer Depth for Low-Resource Language Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx17.1.1">1st AfricaNLP Workshop Proceedings, AfricaNLP@ICLR 2020, Virtual Conference, Formerly Addis Ababa Ethiopia, 26th April 2020</em>, 2020
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2004.04418" title="">https://arxiv.org/abs/2004.04418</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Christopher M. Bishop
</span>
<span class="ltx_bibblock">“Pattern Recognition and Machine Learning (Information Science and Statistics)”
</span>
<span class="ltx_bibblock">Berlin, Heidelberg: Springer-Verlag, 2006
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Ekaba Bisong
</span>
<span class="ltx_bibblock">“Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners”
</span>
<span class="ltx_bibblock">Berkeley, CA: Apress, 2019, pp. 59–64
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1007/978-1-4842-4470-8_7" title="">10.1007/978-1-4842-4470-8˙7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Ondřej Bojar et al.
</span>
<span class="ltx_bibblock">“Findings of the 2017 Conference on Machine Translation (WMT17)”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx20.1.1">Proceedings of the Second Conference on Machine Translation</em>
</span>
<span class="ltx_bibblock">Copenhagen, Denmark: Association for Computational Linguistics, 2017, pp. 169–214
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W17-4717" title="">10.18653/v1/W17-4717</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Ondřej Bojar et al.
</span>
<span class="ltx_bibblock">“Findings of the 2018 Conference on Machine Translation (WMT18)”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx21.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>
</span>
<span class="ltx_bibblock">Belgium, Brussels: Association for Computational Linguistics, 2018, pp. 272–303
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W18-6401" title="">10.18653/v1/W18-6401</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Denny Britz, Anna Goldie, Minh-Thang Luong and Quoc Le
</span>
<span class="ltx_bibblock">“Massive Exploration of Neural Machine Translation Architectures”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx22.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>
</span>
<span class="ltx_bibblock">Copenhagen, Denmark: Association for Computational Linguistics, 2017, pp. 1442–1451
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/D17-1151" title="">10.18653/v1/D17-1151</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Tom B. Brown et al.
</span>
<span class="ltx_bibblock">“Language Models Are Few-Shot Learners”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx23.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, NIPS’20
</span>
<span class="ltx_bibblock">Vancouver, BC, Canada: Curran Associates Inc., 2020
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/pdf/10.5555/3495724.3495883" title="">https://dl.acm.org/doi/pdf/10.5555/3495724.3495883</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Christian Buck, Kenneth Heafield and Bas Ooyen
</span>
<span class="ltx_bibblock">“N-gram Counts and Language Models from the Common Crawl”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx24.1.1">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</em>
</span>
<span class="ltx_bibblock">Reykjavik, Iceland: European Language Resources Association (ELRA), 2014, pp. 3579–3584
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf" title="">http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Chris Callison-Burch et al.
</span>
<span class="ltx_bibblock">“(Meta-) Evaluation of Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx25.1.1">Proceedings of the Second Workshop on Statistical Machine Translation</em>
</span>
<span class="ltx_bibblock">Prague, Czech Republic: Association for Computational Linguistics, 2007, pp. 136–158
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W07-0718" title="">https://aclanthology.org/W07-0718</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Sheila Castilho et al.
</span>
<span class="ltx_bibblock">“Is neural machine translation the new state of the art?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx26.1.1">The Prague Bulletin of Mathematical Linguistics</em>
</span>
<span class="ltx_bibblock">PBML, 2017
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/DOI:%2010.1515/pralin-2017-0013" title="">DOI: 10.1515/pralin-2017-0013</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">William Chen and Brett Fazio
</span>
<span class="ltx_bibblock">“The UCF Systems for the LoResMT 2021 Machine Translation Shared Task”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx27.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</em>
</span>
<span class="ltx_bibblock">Virtual: Association for Machine Translation in the Americas, 2021, pp. 129–133
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-loresmt.13" title="">https://aclanthology.org/2021.mtsummit-loresmt.13</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Kyunghyun Cho, Bart Merriënboer, Dzmitry Bahdanau and Yoshua Bengio
</span>
<span class="ltx_bibblock">“On the Properties of Neural Machine Translation: Encoder–Decoder Approaches”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx28.1.1">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</em>
</span>
<span class="ltx_bibblock">Doha, Qatar: Association for Computational Linguistics, 2014, pp. 103–111
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.3115/v1/W14-4012" title="">10.3115/v1/W14-4012</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Chenhui Chu, Raj Dabre and Sadao Kurohashi
</span>
<span class="ltx_bibblock">“An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx29.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>
</span>
<span class="ltx_bibblock">Vancouver, Canada: Association for Computational Linguistics, 2017, pp. 385–391
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P17-2061" title="">10.18653/v1/P17-2061</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Jacob Cohen
</span>
<span class="ltx_bibblock">“A Coefficient of Agreement for Nominal Scales”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx30.1.1">Educational and Psychological Measurement</em> <span class="ltx_text ltx_font_bold" id="bib.bibx30.2.2">20.1</span>, 1960, pp. 37–46
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1177/001316446002000104" title="">10.1177/001316446002000104</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Alexis Conneau et al.
</span>
<span class="ltx_bibblock">“Unsupervised Cross-lingual Representation Learning at Scale”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx31.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>
</span>
<span class="ltx_bibblock">Online: Association for Computational Linguistics, 2020, pp. 8440–8451
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2020.acl-main.747" title="">10.18653/v1/2020.acl-main.747</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Marta R Costa-jussà et al.
</span>
<span class="ltx_bibblock">“No Language Left Behind: Scaling Human-Centered Machine Translation”, 2022
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.48550/arXiv.2207.04672" title="">10.48550/arXiv.2207.04672</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Josep Crego et al.
</span>
<span class="ltx_bibblock">“SYSTRAN’s Pure Neural Machine Translation Systems”, 2016
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1610.05540" title="">1610.05540 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Michael Denkowski and Alon Lavie
</span>
<span class="ltx_bibblock">“Meteor Universal: Language Specific Translation Evaluation for Any Target Language”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx34.1.1">Proceedings of the Ninth Workshop on Statistical Machine Translation</em>
</span>
<span class="ltx_bibblock">Baltimore, Maryland, USA: Association for Computational Linguistics, 2014, pp. 376–380
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.3115/v1/W14-3348" title="">10.3115/v1/W14-3348</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova
</span>
<span class="ltx_bibblock">“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx35.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>
</span>
<span class="ltx_bibblock">Minneapolis, Minnesota: Association for Computational Linguistics, 2019, pp. 4171–4186
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/N19-1423" title="">10.18653/v1/N19-1423</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Shuoyang Ding, Adithya Renduchintala and Kevin Duh
</span>
<span class="ltx_bibblock">“A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx36.1.1">Proceedings of Machine Translation Summit XVII: Research Track</em>
</span>
<span class="ltx_bibblock">Dublin, Ireland: European Association for Machine Translation, 2019, pp. 204–213
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-6620" title="">https://aclanthology.org/W19-6620</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Meghan Dowling, Teresa Lynn, Alberto Poncelas and Andy Way
</span>
<span class="ltx_bibblock">“SMT versus NMT: Preliminary comparisons for Irish”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx37.1.1">Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018)</em>
</span>
<span class="ltx_bibblock">Boston, MA: Association for Machine Translation in the Americas, 2018, pp. 12–20
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W18-2202" title="">https://aclanthology.org/W18-2202</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Meghan Dowling et al.
</span>
<span class="ltx_bibblock">“A human evaluation of English-Irish statistical and neural machine translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx38.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>
</span>
<span class="ltx_bibblock">Lisboa, Portugal: European Association for Machine Translation, 2020, pp. 431–440
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.eamt-1.46" title="">https://aclanthology.org/2020.eamt-1.46</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Mikel L. Forcada
</span>
<span class="ltx_bibblock">“Making sense of neural machine translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx39.1.1">Translation Spaces</em> <span class="ltx_text ltx_font_bold" id="bib.bibx39.2.2">6.2</span>
</span>
<span class="ltx_bibblock">John Benjamins, 2017, pp. 291–309
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1075/ts.6.2.06for" title="">https://doi.org/10.1075/ts.6.2.06for</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Markus Freitag et al.
</span>
<span class="ltx_bibblock">“Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx40.1.1">Transactions of the Association for Computational Linguistics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx40.2.2">9</span>
</span>
<span class="ltx_bibblock">Cambridge, MA: MIT Press, 2021, pp. 1460–1474
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1162/tacl_a_00437" title="">10.1162/tacl˙a˙00437</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Philip Gage
</span>
<span class="ltx_bibblock">“A New Algorithm for Data Compression”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx41.1.1">C Users J.</em> <span class="ltx_text ltx_font_bold" id="bib.bibx41.2.2">12.2</span>
</span>
<span class="ltx_bibblock">USA: R &amp; D Publications, Inc., 1994, pp. 23–38
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Muhammad Ghifary et al.
</span>
<span class="ltx_bibblock">“Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx42.1.1">Computer Vision – ECCV 2016</em>
</span>
<span class="ltx_bibblock">Cham: Springer International Publishing, 2016, pp. 597–613
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1007/978-3-319-46493-0_36" title="">https://doi.org/10.1007/978-3-319-46493-0˙36</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Thamme Gowda and Jonathan May
</span>
<span class="ltx_bibblock">“Finding the Optimal Vocabulary Size for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx43.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>
</span>
<span class="ltx_bibblock">Online: Association for Computational Linguistics, 2020, pp. 3955–3964
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2020.findings-emnlp.352" title="">10.18653/v1/2020.findings-emnlp.352</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">David Gunning et al.
</span>
<span class="ltx_bibblock">“XAI—Explainable artificial intelligence”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx44.1.1">Science Robotics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx44.2.2">4.37</span>, 2019, pp. eaay7120
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1126/scirobotics.aay7120" title="">10.1126/scirobotics.aay7120</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">Péter Halácsy et al.
</span>
<span class="ltx_bibblock">“Parallel corpora for medium density languages”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx45.1.1">Recent Advances in Natural Language Processing IV</em>, 2007, pp. 247–258
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1075/cilt.292.32var" title="">10.1075/cilt.292.32var</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">Di He et al.
</span>
<span class="ltx_bibblock">“Dual Learning for Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx46.1.1">Proceedings of the 30th International Conference on Neural Information Processing Systems</em>, NIPS’16
</span>
<span class="ltx_bibblock">Barcelona, Spain: Curran Associates Inc., 2016, pp. 820–828
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/pdf/10.5555/3157096.3157188" title="">https://dl.acm.org/doi/pdf/10.5555/3157096.3157188</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">Peter Henderson et al.
</span>
<span class="ltx_bibblock">“Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx47.1.1">J. Mach. Learn. Res.</em> <span class="ltx_text ltx_font_bold" id="bib.bibx47.2.2">21.1</span>
</span>
<span class="ltx_bibblock">JMLR.org, 2020
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/pdf/10.5555/3455716.3455964" title="">https://dl.acm.org/doi/pdf/10.5555/3455716.3455964</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">Sepp Hochreiter and Jürgen Schmidhuber
</span>
<span class="ltx_bibblock">“Long Short-Term Memory”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx48.1.1">Neural Comput.</em> <span class="ltx_text ltx_font_bold" id="bib.bibx48.2.2">9.8</span>
</span>
<span class="ltx_bibblock">Cambridge, MA, USA: MIT Press, 1997, pp. 1735–1780
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1162/neco.1997.9.8.1735" title="">10.1162/neco.1997.9.8.1735</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">Linta Iftikhar
</span>
<span class="ltx_bibblock">“DocGPT: Impact of ChatGPT-3 on Health Services as a Virtual Doctor”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx49.1.1">EC Paediatrics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx49.2.2">12</span>, 2023, pp. 45–55
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">Aizhan Imankulova, Raj Dabre, Atsushi Fujita and Kenji Imamura
</span>
<span class="ltx_bibblock">“Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx50.1.1">Proceedings of Machine Translation Summit XVII: Research Track</em>
</span>
<span class="ltx_bibblock">Dublin, Ireland: European Association for Machine Translation, 2019, pp. 128–139
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-6613" title="">https://aclanthology.org/W19-6613</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">Wandri Jooste, Rejwanul Haque and Andy Way
</span>
<span class="ltx_bibblock">“Knowledge Distillation: A Method for Making Neural Machine Translation More Efficient”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx51.1.1">Information</em> <span class="ltx_text ltx_font_bold" id="bib.bibx51.2.2">13.2</span>, 2022
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.3390/info13020088" title="">10.3390/info13020088</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">Wandri Jooste, Andy Way, Rejwanul Haque and Riccardo Superbo
</span>
<span class="ltx_bibblock">“Knowledge Distillation for Sustainable Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx52.1.1">Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)</em>
</span>
<span class="ltx_bibblock">Orlando, USA: Association for Machine Translation in the Americas, 2022, pp. 221–230
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.amta-upg.16" title="">https://aclanthology.org/2022.amta-upg.16</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">Marcin Junczys-Dowmunt et al.
</span>
<span class="ltx_bibblock">“Marian: Fast Neural Machine Translation in C++”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx53.1.1">Proceedings of ACL 2018, System Demonstrations</em>
</span>
<span class="ltx_bibblock">Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 116–121
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P18-4020" title="">10.18653/v1/P18-4020</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">Enkelejda Kasneci et al.
</span>
<span class="ltx_bibblock">“ChatGPT for good? On opportunities and challenges of large language models for education”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx54.1.1">Learning and Individual Differences</em> <span class="ltx_text ltx_font_bold" id="bib.bibx54.2.2">103</span>, 2023, pp. 102274
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1016/j.lindif.2023.102274" title="">https://doi.org/10.1016/j.lindif.2023.102274</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">Diederik P. Kingma and Jimmy Ba
</span>
<span class="ltx_bibblock">“Adam: A Method for Stochastic Optimization”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx55.1.1">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, 2015
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1412.6980" title="">http://arxiv.org/abs/1412.6980</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">Guillaume Klein et al.
</span>
<span class="ltx_bibblock">“OpenNMT: Open-Source Toolkit for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx56.1.1">Proceedings of ACL 2017, System Demonstrations</em>
</span>
<span class="ltx_bibblock">Vancouver, Canada: Association for Computational Linguistics, 2017, pp. 67–72
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P17-4012" title="">https://aclanthology.org/P17-4012</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">Filip Klubička, Antonio Toral and Víctor M. Sánchez-Cartagena
</span>
<span class="ltx_bibblock">“Quantitative fine-grained human evaluation of machine translation systems: a case study on English to Croatian”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx57.1.1">Machine Translation</em> <span class="ltx_text ltx_font_bold" id="bib.bibx57.2.2">32.3</span>, 2018, pp. 195–215
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1007/s10590-018-9214-x" title="">10.1007/s10590-018-9214-x</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">Philipp Koehn and Rebecca Knowles
</span>
<span class="ltx_bibblock">“Six Challenges for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx58.1.1">Proceedings of the First Workshop on Neural Machine Translation</em>
</span>
<span class="ltx_bibblock">Vancouver: Association for Computational Linguistics, 2017, pp. 28–39
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W17-3204" title="">10.18653/v1/W17-3204</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">Julia Kreutzer, Jasmijn Bastings and Stefan Riezler
</span>
<span class="ltx_bibblock">“Joey NMT: A Minimalist NMT Toolkit for Novices”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx59.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</em>
</span>
<span class="ltx_bibblock">Hong Kong, China: Association for Computational Linguistics, 2019, pp. 109–114
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/D19-3019" title="">10.18653/v1/D19-3019</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">Taku Kudo
</span>
<span class="ltx_bibblock">“Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx60.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 66–75
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P18-1007" title="">10.18653/v1/P18-1007</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">Taku Kudo and John Richardson
</span>
<span class="ltx_bibblock">“SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx61.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>
</span>
<span class="ltx_bibblock">Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 66–71
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/D18-2012" title="">10.18653/v1/D18-2012</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt and Thomas Dandres
</span>
<span class="ltx_bibblock">“Quantifying the Carbon Emissions of Machine Learning”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx62.1.1">CoRR</em> <span class="ltx_text ltx_font_bold" id="bib.bibx62.2.2">abs/1910.09700</span>, 2019
</span>
<span class="ltx_bibblock">arXiv: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1910.09700" title="">http://arxiv.org/abs/1910.09700</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli, Órla Ní Loinsigh and Andy Way
</span>
<span class="ltx_bibblock">“gaHealth: An English–Irish Bilingual Corpus of Health Data”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx63.1.1">Proceedings of the Thirteenth Language Resources and Evaluation Conference</em>
</span>
<span class="ltx_bibblock">Marseille, France: European Language Resources Association, 2022, pp. 6753–6758
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.lrec-1.727" title="">https://aclanthology.org/2022.lrec-1.727</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli and Andy Way
</span>
<span class="ltx_bibblock">“Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx64.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</em>
</span>
<span class="ltx_bibblock">Virtual: Association for Machine Translation in the Americas, 2021, pp. 144–150
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-loresmt.15" title="">https://aclanthology.org/2021.mtsummit-loresmt.15</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli and Andy Way
</span>
<span class="ltx_bibblock">“Transformers for Low-Resource Languages: Is Féidir Linn!”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx65.1.1">Proceedings of Machine Translation Summit XVIII: Research Track</em>
</span>
<span class="ltx_bibblock">Virtual: Association for Machine Translation in the Americas, 2021, pp. 48–60
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-research.5" title="">https://aclanthology.org/2021.mtsummit-research.5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli and Andy Way
</span>
<span class="ltx_bibblock">“Human Evaluation of English-Irish Transformer-Based NMT”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx66.1.1">Information</em> <span class="ltx_text ltx_font_bold" id="bib.bibx66.2.2">13.7</span>, 2022
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.3390/info13070309" title="">10.3390/info13070309</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli and Andy Way
</span>
<span class="ltx_bibblock">“adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx67.1.1">Information</em> <span class="ltx_text ltx_font_bold" id="bib.bibx67.2.2">14.12</span>, 2023
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.3390/info14120638" title="">10.3390/info14120638</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli and Andy Way
</span>
<span class="ltx_bibblock">“adaptNMT: an open-source, language-agnostic development environment for neural machine translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx68.1.1">Language Resources and Evaluation</em>
</span>
<span class="ltx_bibblock">Springer, 2023
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/DOI:%2010.1007/s10579-023-09671-2" title="">DOI: 10.1007/s10579-023-09671-2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">Séamus Lankford, Haithem Afli and Andy Way
</span>
<span class="ltx_bibblock">“Design of an Open-Source Architecture for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx69.1.1">Proceedings of the 1st Workshop on Open Community-Driven Machine Translation</em>
</span>
<span class="ltx_bibblock">Tampere, Finland: European Association for Machine Translation, 2023, pp. 15–20
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.crowdmt-1.2" title="">https://aclanthology.org/2023.crowdmt-1.2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">Samuel Läubli et al.
</span>
<span class="ltx_bibblock">“A set of recommendations for assessing human–machine parity in language translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx70.1.1">Journal of Artificial Intelligence Research</em> <span class="ltx_text ltx_font_bold" id="bib.bibx70.2.2">67</span>, 2020, pp. 653–672
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1613/jair.1.11371" title="">https://doi.org/10.1613/jair.1.11371</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">Dmitry Lepikhin et al.
</span>
<span class="ltx_bibblock">“GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx71.1.1">International Conference on Learning Representations</em>, 2021
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=qrwe7XHTmYb" title="">https://openreview.net/forum?id=qrwe7XHTmYb</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">Chao-Hong Liu, Catarina Cruz Silva, Longyue Wang and Andy Way
</span>
<span class="ltx_bibblock">“Pivot Machine Translation Using Chinese as Pivot Language”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx72.1.1">Machine Translation</em>
</span>
<span class="ltx_bibblock">Singapore: Springer Singapore, 2019, pp. 74–85
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/https://doi.org/10.1007/978-981-13-3083-4_7" title="">https://doi.org/10.1007/978-981-13-3083-4˙7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">Pintu Lohar et al.
</span>
<span class="ltx_bibblock">“FaDA: fast document aligner using word embedding”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx73.1.1">Prague Bulletin of Mathematical Linguistics</em>
</span>
<span class="ltx_bibblock">PBML, 2016, pp. 169–179
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/DOI:%2010.1515/pralin-2016-0016" title="">DOI: 10.1515/pralin-2016-0016</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">Arle Lommel
</span>
<span class="ltx_bibblock">“Metrics for Translation Quality Assessment: A Case for Standardising Error Typologies”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx74.1.1">Translation Quality Assessment: From Principles to Practice</em>
</span>
<span class="ltx_bibblock">Cham: Springer International Publishing, 2018, pp. 109–127
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1007/978-3-319-91241-7_6" title="">10.1007/978-3-319-91241-7˙6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">Arle Lommel, Aljoscha Burchardt and Hans Uszkoreit
</span>
<span class="ltx_bibblock">“Multidimensional Quality Metrics (MQM): A Framework for Declaring and Describing Translation Quality Metrics”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx75.1.1">Tradumàtica: tecnologies de la traducció</em> <span class="ltx_text ltx_font_bold" id="bib.bibx75.2.2">0</span>, 2014, pp. 455–463
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.5565/rev/tradumatica.77" title="">10.5565/rev/tradumatica.77</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">Arle Lommel et al.
</span>
<span class="ltx_bibblock">“Using a new analytic measure for the annotation and analysis of MT errors on real data”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx76.1.1">Proceedings of the 17th Annual conference of the European Association for Machine Translation</em>
</span>
<span class="ltx_bibblock">Dubrovnik, Croatia: European Association for Machine Translation, 2014, pp. 165–172
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2014.eamt-1.38" title="">https://aclanthology.org/2014.eamt-1.38</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">Qingsong Ma, Yvette Graham, Shugen Wang and Qun Liu
</span>
<span class="ltx_bibblock">“Blend: a Novel Combined MT Metric Based on Direct Assessment — CASICT-DCU submission to WMT17 Metrics Task”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx77.1.1">Proceedings of the Second Conference on Machine Translation</em>
</span>
<span class="ltx_bibblock">Copenhagen, Denmark: Association for Computational Linguistics, 2017, pp. 598–603
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W17-4768" title="">10.18653/v1/W17-4768</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">Anne MacFarlane, Liam G. Glynn, Phillip I. Mosinkie and Andrew W. Murphy
</span>
<span class="ltx_bibblock">“Responses to language barriers in consultations with refugees and asylum seekers: a telephone survey of Irish general practitioners”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx78.1.1">BMC Family Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx78.2.2">9.1</span>, 2008, pp. 68
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1186/1471-2296-9-68" title="">10.1186/1471-2296-9-68</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">Mary L McHugh
</span>
<span class="ltx_bibblock">“Interrater reliability: the kappa statistic”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx79.1.1">Biochemia medica</em> <span class="ltx_text ltx_font_bold" id="bib.bibx79.2.2">22.3</span>
</span>
<span class="ltx_bibblock">Medicinska naklada, 2012, pp. 276–282
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.11613/BM.2012.031" title="">10.11613/BM.2012.031</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">I.Dan Melamed, Ryan Green and Joseph P. Turian
</span>
<span class="ltx_bibblock">“Precision and Recall of Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx80.1.1">Companion Volume of the Proceedings of HLT-NAACL 2003 - Short Papers</em>, 2003, pp. 61–63
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N03-2021" title="">https://aclanthology.org/N03-2021</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">Dauglas Montgomery and Cahyono St
</span>
<span class="ltx_bibblock">“Design and Analysis of Experiments, 9th Edition”
</span>
<span class="ltx_bibblock">New York: Wiley, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">Joss Moorkens, Séamus Lankford and Andy Way
</span>
<span class="ltx_bibblock">“Machine Translation and Automation An Introduction for Students, Translators, and Users.”
</span>
<span class="ltx_bibblock">London: Routledge, under review, 2024
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">Shuyo Nakatani
</span>
<span class="ltx_bibblock">“Language Detection Library for Java”, 2010
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/shuyo/language-detection" title="">https://github.com/shuyo/language-detection</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">Franz Josef Och and Hermann Ney
</span>
<span class="ltx_bibblock">“A Systematic Comparison of Various Statistical Alignment Models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx84.1.1">Computational Linguistics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx84.2.2">29.1</span>, 2003, pp. 19–51
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1162/089120103321337421" title="">10.1162/089120103321337421</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">Atul Kr. Ojha et al.
</span>
<span class="ltx_bibblock">“Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx85.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</em>
</span>
<span class="ltx_bibblock">Virtual: Association for Machine Translation in the Americas, 2021, pp. 114–123
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-loresmt.11" title="">https://aclanthology.org/2021.mtsummit-loresmt.11</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“GPT-4 Technical Report”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.08774" title="">2303.08774 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">Myle Ott et al.
</span>
<span class="ltx_bibblock">“fairseq: A Fast, Extensible Toolkit for Sequence Modeling”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx87.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</em>
</span>
<span class="ltx_bibblock">Minneapolis, Minnesota: Association for Computational Linguistics, 2019, pp. 48–53
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/N19-4009" title="">10.18653/v1/N19-4009</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu
</span>
<span class="ltx_bibblock">“Bleu: a Method for Automatic Evaluation of Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx88.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>
</span>
<span class="ltx_bibblock">Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, 2002, pp. 311–318
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.3115/1073083.1073135" title="">10.3115/1073083.1073135</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">Peyman Passban, Andy Way and Qun Liu
</span>
<span class="ltx_bibblock">“Tailoring Neural Architectures for Translating from Morphologically Rich Languages”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx89.1.1">Proceedings of the 27th International Conference on Computational Linguistics</em>
</span>
<span class="ltx_bibblock">Santa Fe, New Mexico, USA: Association for Computational Linguistics, 2018, pp. 3134–3145
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/C18-1265" title="">https://aclanthology.org/C18-1265</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">Maja Popović
</span>
<span class="ltx_bibblock">“chrF: character n-gram F-score for automatic MT evaluation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx90.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>
</span>
<span class="ltx_bibblock">Lisbon, Portugal: Association for Computational Linguistics, 2015, pp. 392–395
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W15-3049" title="">10.18653/v1/W15-3049</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">Matt Post
</span>
<span class="ltx_bibblock">“A Call for Clarity in Reporting BLEU Scores”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx91.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>
</span>
<span class="ltx_bibblock">Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 186–191
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W18-6319" title="">10.18653/v1/W18-6319</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">Karthik Puranik et al.
</span>
<span class="ltx_bibblock">“Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx92.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</em>
</span>
<span class="ltx_bibblock">Virtual: Association for Machine Translation in the Americas, 2021, pp. 134–143
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mtsummit-loresmt.14" title="">https://aclanthology.org/2021.mtsummit-loresmt.14</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever
</span>
<span class="ltx_bibblock">“Improving language understanding by generative pre-training”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx93.1.1">Technical report, OpenAI</em>
</span>
<span class="ltx_bibblock">OpenAI, 2018
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" title="">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">Alec Radford et al.
</span>
<span class="ltx_bibblock">“Language models are unsupervised multitask learners”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx94.1.1">OpenAI blog</em> <span class="ltx_text ltx_font_bold" id="bib.bibx94.2.2">1.8</span>, 2019, pp. 9
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase and Yuxiong He
</span>
<span class="ltx_bibblock">“DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx95.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD ’20
</span>
<span class="ltx_bibblock">Virtual Event, CA, USA: Association for Computing Machinery, 2020, pp. 3505–3506
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1145/3394486.3406703" title="">10.1145/3394486.3406703</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">Sebastian Ruder, Ivan Vulić and Anders Søgaard
</span>
<span class="ltx_bibblock">“A Survey of Cross-Lingual Word Embedding Models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx96.1.1">J. Artif. Int. Res.</em> <span class="ltx_text ltx_font_bold" id="bib.bibx96.2.2">65.1</span>
</span>
<span class="ltx_bibblock">El Segundo, CA, USA: AI Access Foundation, 2019, pp. 569–630
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1613/jair.1.11640" title="">10.1613/jair.1.11640</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">David E Rumelhart, Geoffrey E Hinton and Ronald J Williams
</span>
<span class="ltx_bibblock">“Learning representations by back-propagating errors”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx97.1.1">Nature</em> <span class="ltx_text ltx_font_bold" id="bib.bibx97.2.2">323.6088</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group, 1986, pp. 533–536
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1038/323533a0" title="">10.1038/323533a0</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">Samantha Sanders and Christophe Giraud-Carrier
</span>
<span class="ltx_bibblock">“Informing the Use of Hyperparameter Optimization Through Metalearning”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx98.1.1">2017 IEEE International Conference on Data Mining (ICDM)</em>, 2017, pp. 1051–1056
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/ICDM.2017.137" title="">10.1109/ICDM.2017.137</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">Holger Schwenk et al.
</span>
<span class="ltx_bibblock">“CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx99.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Online: Association for Computational Linguistics, 2021, pp. 6490–6500
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2021.acl-long.507" title="">10.18653/v1/2021.acl-long.507</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock"> SEAI
</span>
<span class="ltx_bibblock">“Sustainable Energy in Ireland”, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf" title="">https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf</a>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">Rico Sennrich, Barry Haddow and Alexandra Birch
</span>
<span class="ltx_bibblock">“Edinburgh Neural Machine Translation Systems for WMT 16”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx101.1.1">Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers</em>
</span>
<span class="ltx_bibblock">Berlin, Germany: Association for Computational Linguistics, 2016, pp. 371–376
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W16-2323" title="">10.18653/v1/W16-2323</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">Rico Sennrich, Barry Haddow and Alexandra Birch
</span>
<span class="ltx_bibblock">“Neural Machine Translation of Rare Words with Subword Units”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx102.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Berlin, Germany: Association for Computational Linguistics, 2016, pp. 1715–1725
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P16-1162" title="">10.18653/v1/P16-1162</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">Rico Sennrich, Barry Haddow and Alexandra Birch
</span>
<span class="ltx_bibblock">“Neural Machine Translation of Rare Words with Subword Units”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx103.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Berlin, Germany: Association for Computational Linguistics, 2016, pp. 1715–1725
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P16-1162" title="">10.18653/v1/P16-1162</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">Rico Sennrich and Biao Zhang
</span>
<span class="ltx_bibblock">“Revisiting Low-Resource Neural Machine Translation: A Case Study”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx104.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>
</span>
<span class="ltx_bibblock">Florence, Italy: Association for Computational Linguistics, 2019, pp. 211–221
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P19-1021" title="">10.18653/v1/P19-1021</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">Matthew Snover et al.
</span>
<span class="ltx_bibblock">“A Study of Translation Edit Rate with Targeted Human Annotation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx105.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</em>
</span>
<span class="ltx_bibblock">Cambridge, Massachusetts, USA: Association for Machine Translation in the Americas, 2006, pp. 223–231
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2006.amta-papers.25" title="">https://aclanthology.org/2006.amta-papers.25</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">Miloš Stanojević, Amir Kamran, Philipp Koehn and Ondřej Bojar
</span>
<span class="ltx_bibblock">“Results of the WMT15 Metrics Shared Task”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx106.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>
</span>
<span class="ltx_bibblock">Lisbon, Portugal: Association for Computational Linguistics, 2015, pp. 256–273
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W15-3031" title="">10.18653/v1/W15-3031</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">Ralf Steinberger et al.
</span>
<span class="ltx_bibblock">“DGT-TM: A freely available Translation Memory in 22 languages”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx107.1.1">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12)</em>
</span>
<span class="ltx_bibblock">Istanbul, Turkey: European Language Resources Association (ELRA), 2012, pp. 454–459
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/814_Paper.pdf" title="">http://www.lrec-conf.org/proceedings/lrec2012/pdf/814_Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">Emma Strubell, Ananya Ganesh and Andrew McCallum
</span>
<span class="ltx_bibblock">“Energy and Policy Considerations for Deep Learning in NLP”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx108.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>
</span>
<span class="ltx_bibblock">Florence, Italy: Association for Computational Linguistics, 2019, pp. 3645–3650
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/P19-1355" title="">10.18653/v1/P19-1355</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">Ilya Sutskever, Oriol Vinyals and Quoc V Le
</span>
<span class="ltx_bibblock">“Sequence to Sequence Learning with Neural Networks”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx109.1.1">Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bibx109.2.2">27</span>
</span>
<span class="ltx_bibblock">Curran Associates, Inc., 2014
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">Romal Thoppilan et al.
</span>
<span class="ltx_bibblock">“LaMDA: Language Models for Dialog Applications”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx110.1.1">CoRR</em> <span class="ltx_text ltx_font_bold" id="bib.bibx110.2.2">abs/2201.08239</span>, 2022
</span>
<span class="ltx_bibblock">arXiv: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.08239" title="">https://arxiv.org/abs/2201.08239</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">Benoit Thouin
</span>
<span class="ltx_bibblock">“The Meteo system”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx111.1.1">Translating and the Computer: Practical experience of machine translation</em>
</span>
<span class="ltx_bibblock">London, UK: Aslib, 1981
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/1981.tc-1.4" title="">https://aclanthology.org/1981.tc-1.4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">Jörg Tiedemann and Santhosh Thottingal
</span>
<span class="ltx_bibblock">“OPUS-MT – Building open translation services for the World”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx112.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>
</span>
<span class="ltx_bibblock">Lisboa, Portugal: European Association for Machine Translation, 2020, pp. 479–480
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.eamt-1.61" title="">https://aclanthology.org/2020.eamt-1.61</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">Antonio Toral, Sheila Castilho, Ke Hu and Andy Way
</span>
<span class="ltx_bibblock">“Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx113.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>
</span>
<span class="ltx_bibblock">Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 113–123
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/W18-6312" title="">10.18653/v1/W18-6312</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">Ashish Vaswani et al.
</span>
<span class="ltx_bibblock">“Attention is All you Need”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx114.1.1">Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bibx114.2.2">30</span>
</span>
<span class="ltx_bibblock">Curran Associates, Inc., 2017
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">Andy Way
</span>
<span class="ltx_bibblock">“Quality expectations of machine translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx115.1.1">Translation Quality Assessment: From Principles to Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx115.2.2">1</span>, Machine Translation: Technologies and Applications Series Volume
</span>
<span class="ltx_bibblock">Berlin/Heidelberg: Springer, 2018, pp. 159–178
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/DOI:10.1007/978-3-319-91241-7" title="">DOI:10.1007/978-3-319-91241-7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">Andy Way
</span>
<span class="ltx_bibblock">“Machine translation: where are we at today?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx116.1.1">The Bloomsbury Companion to Language Industry Studies</em>, Bloomsbury Companions
</span>
<span class="ltx_bibblock">NY, USA: Bloomsbury Academic Publishing, 2019
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doras.dcu.ie/24598/" title="">https://doras.dcu.ie/24598/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">Genta Indra Winata et al.
</span>
<span class="ltx_bibblock">“Language Models are Few-shot Multilingual Learners”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx117.1.1">Proceedings of the 1st Workshop on Multilingual Representation Learning</em>
</span>
<span class="ltx_bibblock">Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021, pp. 1–15
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2021.mrl-1.1" title="">10.18653/v1/2021.mrl-1.1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">Thomas Wolf et al.
</span>
<span class="ltx_bibblock">“Transformers: State-of-the-Art Natural Language Processing”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx118.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>
</span>
<span class="ltx_bibblock">Online: Association for Computational Linguistics, 2020, pp. 38–45
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2020.emnlp-demos.6" title="">10.18653/v1/2020.emnlp-demos.6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">Yonghui Wu et al.
</span>
<span class="ltx_bibblock">“Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx119.1.1">CoRR</em> <span class="ltx_text ltx_font_bold" id="bib.bibx119.2.2">abs/1609.08144</span>, 2016
</span>
<span class="ltx_bibblock">arXiv: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1609.08144" title="">http://arxiv.org/abs/1609.08144</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">Shuoheng Yang, Yuxin Wang and Xiaowen Chu
</span>
<span class="ltx_bibblock">“A Survey of Deep Learning Techniques for Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx120.1.1">CoRR</em> <span class="ltx_text ltx_font_bold" id="bib.bibx120.2.2">abs/2002.07526</span>, 2020
</span>
<span class="ltx_bibblock">arXiv: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2002.07526" title="">https://arxiv.org/abs/2002.07526</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">Barret Zoph, Deniz Yuret, Jonathan May and Kevin Knight
</span>
<span class="ltx_bibblock">“Transfer Learning for Low-Resource Neural Machine Translation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx121.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>
</span>
<span class="ltx_bibblock">Austin, Texas: Association for Computational Linguistics, 2016, pp. 1568–1575
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/D16-1163" title="">10.18653/v1/D16-1163</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Mar  3 17:57:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
