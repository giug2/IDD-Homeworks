<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.17338] A Survey on Blockchain-Based Federated Learning and Data Privacy</title><meta property="og:description" content="Federated learning is a decentralized machine learning paradigm that allows multiple clients to collaborate by leveraging local computational power and the model’s transmission. This method reduces the costs and privac…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey on Blockchain-Based Federated Learning and Data Privacy">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey on Blockchain-Based Federated Learning and Data Privacy">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.17338">

<!--Generated on Wed Feb 28 20:41:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  Data privacy,  Privacy-preserving,  Blockchain,  Industrial Internet of Things (IIoT),  Data Security,  Data-sharing platforms
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on Blockchain-Based Federated Learning and Data Privacy</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bipin Chhetri, Saroj Gopali, Rukayat Olapojoye, Samin Dehbashi and Akbar Siami Namin
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science, Texas Tech University</span> 
<br class="ltx_break">{bipin.chhetri, saroj.gopali, rolapojo, samin.dehbashi, akbar.namin}@ttu.edu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated learning is a decentralized machine learning paradigm that allows multiple clients to collaborate by leveraging local computational power and the model’s transmission. This method reduces the costs and privacy concerns associated with centralized machine learning methods while ensuring data privacy by distributing training data across heterogeneous devices. On the other hand, federated learning has the drawback of data leakage due to the lack of privacy-preserving mechanisms employed during storage, transfer, and sharing, thus posing significant risks to data owners and suppliers. Blockchain technology has emerged as a promising technology for offering secure data-sharing platforms in federated learning, especially in Industrial Internet of Things (IIoT) settings. This survey aims to compare the performance and security of various data privacy mechanisms adopted in blockchain-based federated learning architectures. We conduct a systematic review of existing literature on secure data-sharing platforms for federated learning provided by blockchain technology, providing an in-depth overview of blockchain-based federated learning, its essential components, and discussing its principles, and potential applications. The primary contribution of this survey paper is to identify critical research questions and propose potential directions for future research in blockchain-based federated learning.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, Data privacy, Privacy-preserving, Blockchain, Industrial Internet of Things (IIoT), Data Security, Data-sharing platforms

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The rapid development of the Industrial Internet of Things (IIoT) has resulted in a significant increase in data generated by connected devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
The current privacy and security measures for IIoT are outdated and require significant updates. In addition, some of these measures are still under development and testing with a myriad of vulnerabilities. As a result, new techniques and policies are urgently needed to secure data sharing across wireless networks and address security challenges in IIoT. To address these challenges, Monrat et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> proposes the use of blockchain technology as a secure data-sharing architecture and thus introducing the Blockchain technology as a decentralized and secure IoT revolution.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Rao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> note that user privacy laws in many regions worldwide that mandate technological companies handle user data with extra care. The conventional machine learning techniques have a significant limitation in that they require all data to be gathered in a single location, typically a data center. This approach poses a potential risk to user privacy and could violate data confidentiality laws that protect sensitive information. As a result, new machine-learning techniques that can preserve data privacy and confidentiality are needed to address these concerns. To address the limitations of conventional machine learning techniques, Cloud Service Providers (CSPs) have adopted a strategy of centralizing data storage. This approach helps to ensure data integrity, availability, privacy, and confidentiality. This methodology enables CSPs to manage and protect data more effectively, ensuring that it is secure and readily available to authorized users. By centralizing data storage and management, CSPs can also integrate advanced security measures and technologies to protect against cyber threats and safeguard sensitive information. Nevertheless, CSPs do not always deliver trustworthy data services to customers, and there are problems with cloud data storage such as data breaches, data theft, privacy concerns, and cloud data unavailability. Therefore, submitting raw data to a central server raises privacy and communication concerns for data owners, reducing the likelihood of uploading data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Bonawitz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> state that Federated Learning (FL) is an innovative approach to machine-learning that resolves problems associated with traditional methods. FL allows multiple parties to train a shared model without revealing their data. The training algorithms on data are distributed across several clients with no need for data samples to be exchanged. A centralized server manages the training process in the traditional FL approach including client management, global model maintenance, and gradient aggregation. The server sends the current model to nodes each round, updated with their data and gradients sent back. The gradients are aggregated and integrated into the model for the next round by the server. FL preserves data privacy by sharing gradient information instead of raw data. However, as Li et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> note, there is still a risk of sensitive information being exposed to a third party or the central server. Moreover, the conventional FL framework is vulnerable to malicious data and single points of failure, which can undermine its reliability.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Blockchain technology is an alternative to centralized methods in IoT and edge computing, overcoming their limitations. Blockchain’s decentralization enables Smart Contracts (SC) to function as a centralized server through blockchain transactions. This study focuses on the implementation of privacy-preserving machine learning methods based on blockchain and their practical applications. The main objective of this exploratory study is to understand how blockchain-based federated learning can enhance the training stage in machine learning algorithms. The focus is on addressing data privacy requirements while enabling practical applications. A review of existing literature on blockchain-based federated learning is conducted. The aim is to provide a comprehensive overview of the current state-of-the-art in this field. The research efforts are categorized and analyzed to identify open research questions and challenges that need to be addressed to advance this line of research. The contributions of this study are as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">An in-depth overview of blockchain-based federated learning along with its essential components, underlying principles, and potential applications are presented.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">A comprehensive systematic review of existing literature in the context of privacy-preserving in Blockchain is conducted.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">A number of critical open research questions and potential directions for future research in blockchain-based federated learning are proposed.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The paper is organized as follows: Section <a href="#S2" title="II Motivation ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref ltx_font_bold"><span class="ltx_text ltx_ref_tag">II</span></a> covers the motivation for the study, while Section <a href="#S3" title="III Technical Background ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref ltx_font_bold"><span class="ltx_text ltx_ref_tag">III</span></a> provides background information on Federated Learning, Blockchain, Blockchain-based Federated Learning, and Data Privacy. Recent studies on implementing Federated Learning using Blockchain are discussed in Section <a href="#S4" title="IV Blockchain-based Federated Learning ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref ltx_font_bold"><span class="ltx_text ltx_ref_tag">IV</span></a>. A discussion about the weaknesses of using blockchain technology in federated learning is explored in Sections <a href="#S5" title="V Data Privacy Challenges ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref ltx_font_bold"><span class="ltx_text ltx_ref_tag">V</span></a>. The future directions of the research are explored <a href="#S6" title="VI Future Lines of Research ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref ltx_font_bold"><span class="ltx_text ltx_ref_tag">VI</span></a>. The conclusion is presented in Section <a href="#S7" title="VII Conclusion ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref ltx_font_bold"><span class="ltx_text ltx_ref_tag">VII</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Motivation</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The primary motivation behind this study is the pressing issues surrounding data privacy and sharing. Security concerns regarding federated learning have been arisen. These concerns stem from the potential for malicious clients or central servers to attack the global model or access user privacy data.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> tackled the data privacy issue by implementing federated learning, where only the models and not the raw data are shared. This ensures the data’s efficiency and usefulness while maintaining privacy. A fully decentralized federated learning system is proposed. The system employs blockchain technology as the underlying architecture and the proof-of-work (PoW) consensus process. The decentralized system offers resistance to poisoning attacks. Incentives are provided and accuracy is enhanced through member selections.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Data leakage during storage, transmission, and sharing is a significant challenge faced by data owners and providers. This challenge is particularly prominent in the context of Industrial Internet of Things (IIoT) applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. To address attacks on global models or user privacy data, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> proposed the Blockchain-based Federated Learning framework with Committee consensus (BFLC), a decentralized, blockchain-based federated learning framework. However, the Committee consensus mechanism (CCM) used in the framework may result in a large amount of communication overhead between nodes. This could lead to slow training times and increased energy consumption.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposed a solution to data-sharing challenges by combining federated learning with permissioned blockchain. The permissioned blockchain in this system creates secure connections between end IoT devices. These connections are established through encrypted records maintained by supernodes, such as base stations and roadside units. This ensures data privacy and accessibility. The proposed architecture does not store raw data. It uses permissioned blockchain to access related data and controls data accessibility. This addresses storage constraints and privacy concerns.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">To facilitate collaborative learning and data protection, it is crucial to conduct research in blockchain-based federated learning and data privacy. In conventional machine learning scenarios, sharing a large dataset required for training a model is challenging. This is particularly true when the data is distributed across multiple organizations or individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Federated learning solves the problem of sharing a large dataset for training a model. It does so by allowing each party to train a local model on their data. The local model’s parameters are then shared with a central server, which aggregates these updates to produce a global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">To enhance the security and trustworthiness of this process. Blockchain technology can provide a tamper-proof and transparent ledger for recording the model updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. This way, participating parties can verify the integrity of the global model. This ensures equitable incorporation of their contributions. Furthermore, blockchain can be used to implement privacy-preserving mechanisms. These mechanisms, such as differential privacy, enable participating parties to share their model updates. The sharing of updates is accomplished without revealing sensitive information.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Technical Background</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section provides a discussion of the background of federated learning, blockchain technology, blockchain-based federated learning, and data privacy along with the fundamental concepts and principles of each area. Understanding the backgrounds of blockchain and federated learning is crucial. It allows us to appreciate the potential benefits and limitations of applying blockchain-based federated learning. This application can address data privacy concerns in collaborative learning scenarios.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Federated Learning (FL) is a decentralized approach to machine learning. FL allows clients to participate in training a machine learning model. Clients can contribute to training without uploading their data samples to a centralized data warehouse. This preserves the privacy of each data sample, as they remain with the clients. In FL, clients train a model locally on their private data samples. This local training happens in each iteration of model training. Clients use an initial global model provided by the aggregation server. After training, the model gradients or weights are generated and uploaded to a server for aggregation. The core workflow involved in traditional FL includes several steps. The first step is the selection of participating clients.
Clients then train their own local model. The local model weights are uploaded to the centralized server. Finally, the server aggregates the local model weights to obtain a global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Blockchain</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Blockchain is a decentralized and tamper-proof system used for permanently recording transactions. It consists of a network of nodes, transactions, a chain of blocks, and a shared ledger. The transactions are recorded and maintained by all the nodes in the network, providing benefits such as decentralization, immutability, transparency, and anonymity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Based on the level of access control, blockchain can be categorized as public, private, or consortium. In a public blockchain, any node can participate in the network without requiring permission. In contrast, nodes on a private blockchain require authorization to join the network and access the shared ledger. In a consortium blockchain, control is typically restricted to selected nodes. These selected nodes have the right to generate new blocks. This makes consortium blockchain a partially decentralized system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Blockchain-based Federated Learning</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The traditional approach to Federated Learning (FL) depends on a central server for model aggregation, which can be considered as a weak point in the system. To improve reliability, blockchain technology, which is decentralized, has been proposed as a solution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Blockchain-based Federated Learning (BCFL) integrates the decentralized property of blockchain with the distributed nature of FL to eliminate the threat of a single point of failure in the FL system’s aggregation server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Various BCFL architectures have been proposed, which can be categorized into three groups: (1) fully coupled BCFL, (2) flexible coupled BCFL, and (3) loosely coupled BCFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Smart Contracts (SC) have been used to implement the functions of FL aggregation in more recent approaches such as BLOCKFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and BAFFLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. These SCs are activated through blockchain transactions to facilitate the aggregation of local model updates from participating clients. In BLOCKFL, a two-phase consensus protocol is used to ensure the integrity of the aggregated model. BAFFLE employs a modified Byzantine fault-tolerant consensus algorithm. The algorithm is used to address the security and scalability issues faced in traditional FL systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Data Privacy</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In traditional FL, the data samples of each participating client are not exposed to each other or to the aggregation server. However, the local model updates sent over for aggregation are exposed to the server. Typically in a BCFL, the model updates from the participating clients are also uploaded to the blockchain network as raw data. These scenarios lead to data leakage and pose a threat to the system as malicious clients or attackers could exploit this vulnerability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed a blockchain-based collaborative system. The system, called BLADE-FL, is designed to share data across distributed multiple parties. The goal is to reduce the risk of data leakage. They ensured data privacy by incorporating differential privacy into federated learning. Other techniques, such as Homomorphic Encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and Secure Multiparty Computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, have been integrated to preserve privacy from end to end in a BCFL. These data privacy techniques are briefly discussed as follows:</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS1.5.1.1" class="ltx_text">III-D</span>1 </span>Differential Privacy (DP)</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.9" class="ltx_p">A mathematical definition of privacy that protects users’ privacy in published data by adding some randomly generated noises. Basically, the definition of DP is stated as for a random algorithm <math id="S3.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.SSS1.p1.1.m1.1a"><mi id="S3.SS4.SSS1.p1.1.m1.1.1" xref="S3.SS4.SSS1.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.1.m1.1b"><ci id="S3.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.1.m1.1c">A</annotation></semantics></math>, <math id="S3.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS4.SSS1.p1.2.m2.1a"><mi id="S3.SS4.SSS1.p1.2.m2.1.1" xref="S3.SS4.SSS1.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.2.m2.1b"><ci id="S3.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.2.m2.1c">Q</annotation></semantics></math> is the set of all possible outputs. If for any pair of the neighboring datasets <math id="S3.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS4.SSS1.p1.3.m3.1a"><mi id="S3.SS4.SSS1.p1.3.m3.1.1" xref="S3.SS4.SSS1.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.3.m3.1b"><ci id="S3.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.3.m3.1c">D</annotation></semantics></math> and <math id="S3.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.SS4.SSS1.p1.4.m4.1a"><msup id="S3.SS4.SSS1.p1.4.m4.1.1" xref="S3.SS4.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS4.SSS1.p1.4.m4.1.1.2" xref="S3.SS4.SSS1.p1.4.m4.1.1.2.cmml">D</mi><mo id="S3.SS4.SSS1.p1.4.m4.1.1.3" xref="S3.SS4.SSS1.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.4.m4.1b"><apply id="S3.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS4.SSS1.p1.4.m4.1.1.2">𝐷</ci><ci id="S3.SS4.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS4.SSS1.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.4.m4.1c">D^{\prime}</annotation></semantics></math>, any subset <math id="S3.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS4.SSS1.p1.5.m5.1a"><mi id="S3.SS4.SSS1.p1.5.m5.1.1" xref="S3.SS4.SSS1.p1.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.5.m5.1b"><ci id="S3.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS4.SSS1.p1.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.5.m5.1c">S</annotation></semantics></math> of <math id="S3.SS4.SSS1.p1.6.m6.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS4.SSS1.p1.6.m6.1a"><mi id="S3.SS4.SSS1.p1.6.m6.1.1" xref="S3.SS4.SSS1.p1.6.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.6.m6.1b"><ci id="S3.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.6.m6.1c">Q</annotation></semantics></math>, algorithm <math id="S3.SS4.SSS1.p1.7.m7.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.SSS1.p1.7.m7.1a"><mi id="S3.SS4.SSS1.p1.7.m7.1.1" xref="S3.SS4.SSS1.p1.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.7.m7.1b"><ci id="S3.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.7.m7.1c">A</annotation></semantics></math> satisfies.
The algorithm <math id="S3.SS4.SSS1.p1.8.m8.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.SSS1.p1.8.m8.1a"><mi id="S3.SS4.SSS1.p1.8.m8.1.1" xref="S3.SS4.SSS1.p1.8.m8.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.8.m8.1b"><ci id="S3.SS4.SSS1.p1.8.m8.1.1.cmml" xref="S3.SS4.SSS1.p1.8.m8.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.8.m8.1c">A</annotation></semantics></math> satisfies differential privacy, where <math id="S3.SS4.SSS1.p1.9.m9.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S3.SS4.SSS1.p1.9.m9.1a"><mi id="S3.SS4.SSS1.p1.9.m9.1.1" xref="S3.SS4.SSS1.p1.9.m9.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.9.m9.1b"><ci id="S3.SS4.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS4.SSS1.p1.9.m9.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.9.m9.1c">\varepsilon</annotation></semantics></math> is the privacy protection budget.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.5" class="ltx_Math" alttext="\operatorname{Pr}\left[A(D)\in S\right]\leq e^{\varepsilon}\operatorname{Pr}\left[A\left(D^{\prime}\right)\in S\right]" display="block"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.2.cmml"><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Pr</mi><mo id="S3.E1.m1.4.4.1.1a" xref="S3.E1.m1.4.4.1.2.cmml">⁡</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.2.cmml"><mo id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.2.cmml">[</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.2.2" xref="S3.E1.m1.4.4.1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.2.1" xref="S3.E1.m1.4.4.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.2.3.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.2.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">D</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.2.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">∈</mo><mi id="S3.E1.m1.4.4.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.3.cmml">S</mi></mrow><mo id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.2.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml">≤</mo><mrow id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml"><msup id="S3.E1.m1.5.5.2.3" xref="S3.E1.m1.5.5.2.3.cmml"><mi id="S3.E1.m1.5.5.2.3.2" xref="S3.E1.m1.5.5.2.3.2.cmml">e</mi><mi id="S3.E1.m1.5.5.2.3.3" xref="S3.E1.m1.5.5.2.3.3.cmml">ε</mi></msup><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.5.5.2.2" xref="S3.E1.m1.5.5.2.2.cmml">​</mo><mrow id="S3.E1.m1.5.5.2.1.1" xref="S3.E1.m1.5.5.2.1.2.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">Pr</mi><mo id="S3.E1.m1.5.5.2.1.1a" xref="S3.E1.m1.5.5.2.1.2.cmml">⁡</mo><mrow id="S3.E1.m1.5.5.2.1.1.1" xref="S3.E1.m1.5.5.2.1.2.cmml"><mo id="S3.E1.m1.5.5.2.1.1.1.2" xref="S3.E1.m1.5.5.2.1.2.cmml">[</mo><mrow id="S3.E1.m1.5.5.2.1.1.1.1" xref="S3.E1.m1.5.5.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.5.5.2.1.1.1.1.1" xref="S3.E1.m1.5.5.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.2.1.1.1.1.1.3" xref="S3.E1.m1.5.5.2.1.1.1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.1.1.1.1.1.2" xref="S3.E1.m1.5.5.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.2.cmml">D</mi><mo id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.2.1.1.1.1.2" xref="S3.E1.m1.5.5.2.1.1.1.1.2.cmml">∈</mo><mi id="S3.E1.m1.5.5.2.1.1.1.1.3" xref="S3.E1.m1.5.5.2.1.1.1.1.3.cmml">S</mi></mrow><mo id="S3.E1.m1.5.5.2.1.1.1.3" xref="S3.E1.m1.5.5.2.1.2.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><leq id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3"></leq><apply id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.1"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">Pr</ci><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1"><in id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"></in><apply id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2"><times id="S3.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.1"></times><ci id="S3.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.2">𝐴</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐷</ci></apply><ci id="S3.E1.m1.4.4.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3">𝑆</ci></apply></apply><apply id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"><times id="S3.E1.m1.5.5.2.2.cmml" xref="S3.E1.m1.5.5.2.2"></times><apply id="S3.E1.m1.5.5.2.3.cmml" xref="S3.E1.m1.5.5.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.3.1.cmml" xref="S3.E1.m1.5.5.2.3">superscript</csymbol><ci id="S3.E1.m1.5.5.2.3.2.cmml" xref="S3.E1.m1.5.5.2.3.2">𝑒</ci><ci id="S3.E1.m1.5.5.2.3.3.cmml" xref="S3.E1.m1.5.5.2.3.3">𝜀</ci></apply><apply id="S3.E1.m1.5.5.2.1.2.cmml" xref="S3.E1.m1.5.5.2.1.1"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">Pr</ci><apply id="S3.E1.m1.5.5.2.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1"><in id="S3.E1.m1.5.5.2.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.2"></in><apply id="S3.E1.m1.5.5.2.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1"><times id="S3.E1.m1.5.5.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1.2"></times><ci id="S3.E1.m1.5.5.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1.3">𝐴</ci><apply id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.2">𝐷</ci><ci id="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.1.1.1.1.3">′</ci></apply></apply><ci id="S3.E1.m1.5.5.2.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.2.1.1.1.1.3">𝑆</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\operatorname{Pr}\left[A(D)\in S\right]\leq e^{\varepsilon}\operatorname{Pr}\left[A\left(D^{\prime}\right)\in S\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.12" class="ltx_p">The above equation represents a probabilistic inequality. The inequality indicates that the probability of algorithm <math id="S3.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.SSS1.p2.1.m1.1a"><mi id="S3.SS4.SSS1.p2.1.m1.1.1" xref="S3.SS4.SSS1.p2.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.1.m1.1b"><ci id="S3.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.1.m1.1c">A</annotation></semantics></math> producing a result in set <math id="S3.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS4.SSS1.p2.2.m2.1a"><mi id="S3.SS4.SSS1.p2.2.m2.1.1" xref="S3.SS4.SSS1.p2.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.2.m2.1b"><ci id="S3.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p2.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.2.m2.1c">S</annotation></semantics></math> with dataset <math id="S3.SS4.SSS1.p2.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS4.SSS1.p2.3.m3.1a"><mi id="S3.SS4.SSS1.p2.3.m3.1.1" xref="S3.SS4.SSS1.p2.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.3.m3.1b"><ci id="S3.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p2.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.3.m3.1c">D</annotation></semantics></math> is less than or equal to <math id="S3.SS4.SSS1.p2.4.m4.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S3.SS4.SSS1.p2.4.m4.1a"><mi id="S3.SS4.SSS1.p2.4.m4.1.1" xref="S3.SS4.SSS1.p2.4.m4.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.4.m4.1b"><ci id="S3.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS4.SSS1.p2.4.m4.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.4.m4.1c">e</annotation></semantics></math> raised to the power of <math id="S3.SS4.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S3.SS4.SSS1.p2.5.m5.1a"><mi id="S3.SS4.SSS1.p2.5.m5.1.1" xref="S3.SS4.SSS1.p2.5.m5.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.5.m5.1b"><ci id="S3.SS4.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS4.SSS1.p2.5.m5.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.5.m5.1c">\varepsilon</annotation></semantics></math> times the probability of obtaining a result in set <math id="S3.SS4.SSS1.p2.6.m6.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS4.SSS1.p2.6.m6.1a"><mi id="S3.SS4.SSS1.p2.6.m6.1.1" xref="S3.SS4.SSS1.p2.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.6.m6.1b"><ci id="S3.SS4.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS4.SSS1.p2.6.m6.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.6.m6.1c">S</annotation></semantics></math> with a different dataset <math id="S3.SS4.SSS1.p2.7.m7.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.SS4.SSS1.p2.7.m7.1a"><msup id="S3.SS4.SSS1.p2.7.m7.1.1" xref="S3.SS4.SSS1.p2.7.m7.1.1.cmml"><mi id="S3.SS4.SSS1.p2.7.m7.1.1.2" xref="S3.SS4.SSS1.p2.7.m7.1.1.2.cmml">D</mi><mo id="S3.SS4.SSS1.p2.7.m7.1.1.3" xref="S3.SS4.SSS1.p2.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.7.m7.1b"><apply id="S3.SS4.SSS1.p2.7.m7.1.1.cmml" xref="S3.SS4.SSS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.7.m7.1.1.1.cmml" xref="S3.SS4.SSS1.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p2.7.m7.1.1.2.cmml" xref="S3.SS4.SSS1.p2.7.m7.1.1.2">𝐷</ci><ci id="S3.SS4.SSS1.p2.7.m7.1.1.3.cmml" xref="S3.SS4.SSS1.p2.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.7.m7.1c">D^{\prime}</annotation></semantics></math>. In simpler terms, using dataset <math id="S3.SS4.SSS1.p2.8.m8.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.SS4.SSS1.p2.8.m8.1a"><msup id="S3.SS4.SSS1.p2.8.m8.1.1" xref="S3.SS4.SSS1.p2.8.m8.1.1.cmml"><mi id="S3.SS4.SSS1.p2.8.m8.1.1.2" xref="S3.SS4.SSS1.p2.8.m8.1.1.2.cmml">D</mi><mo id="S3.SS4.SSS1.p2.8.m8.1.1.3" xref="S3.SS4.SSS1.p2.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.8.m8.1b"><apply id="S3.SS4.SSS1.p2.8.m8.1.1.cmml" xref="S3.SS4.SSS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.8.m8.1.1.1.cmml" xref="S3.SS4.SSS1.p2.8.m8.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p2.8.m8.1.1.2.cmml" xref="S3.SS4.SSS1.p2.8.m8.1.1.2">𝐷</ci><ci id="S3.SS4.SSS1.p2.8.m8.1.1.3.cmml" xref="S3.SS4.SSS1.p2.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.8.m8.1c">D^{\prime}</annotation></semantics></math> increases the likelihood of obtaining a result in set <math id="S3.SS4.SSS1.p2.9.m9.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS4.SSS1.p2.9.m9.1a"><mi id="S3.SS4.SSS1.p2.9.m9.1.1" xref="S3.SS4.SSS1.p2.9.m9.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.9.m9.1b"><ci id="S3.SS4.SSS1.p2.9.m9.1.1.cmml" xref="S3.SS4.SSS1.p2.9.m9.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.9.m9.1c">S</annotation></semantics></math> compared to dataset <math id="S3.SS4.SSS1.p2.10.m10.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS4.SSS1.p2.10.m10.1a"><mi id="S3.SS4.SSS1.p2.10.m10.1.1" xref="S3.SS4.SSS1.p2.10.m10.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.10.m10.1b"><ci id="S3.SS4.SSS1.p2.10.m10.1.1.cmml" xref="S3.SS4.SSS1.p2.10.m10.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.10.m10.1c">D</annotation></semantics></math>, by a factor of <math id="S3.SS4.SSS1.p2.11.m11.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S3.SS4.SSS1.p2.11.m11.1a"><mi id="S3.SS4.SSS1.p2.11.m11.1.1" xref="S3.SS4.SSS1.p2.11.m11.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.11.m11.1b"><ci id="S3.SS4.SSS1.p2.11.m11.1.1.cmml" xref="S3.SS4.SSS1.p2.11.m11.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.11.m11.1c">e</annotation></semantics></math> raised to the power of <math id="S3.SS4.SSS1.p2.12.m12.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S3.SS4.SSS1.p2.12.m12.1a"><mi id="S3.SS4.SSS1.p2.12.m12.1.1" xref="S3.SS4.SSS1.p2.12.m12.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.12.m12.1b"><ci id="S3.SS4.SSS1.p2.12.m12.1.1.cmml" xref="S3.SS4.SSS1.p2.12.m12.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.12.m12.1c">\varepsilon</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS2.5.1.1" class="ltx_text">III-D</span>2 </span>Homomorphic Encryption (HE)</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.4" class="ltx_p">This is a cryptography method that allows computation on encrypted data and provides encrypted results to the user without having to decrypt the encrypted data. Asides from being able to process encrypted data, HE also ensures that the privacy of data is preserved. Given encryption of messages (<math id="S3.SS4.SSS2.p1.1.m1.3" class="ltx_Math" alttext="m_{1},m_{2},m_{3},....m_{n}" display="inline"><semantics id="S3.SS4.SSS2.p1.1.m1.3a"><mrow id="S3.SS4.SSS2.p1.1.m1.3.3.2" xref="S3.SS4.SSS2.p1.1.m1.3.3.3.cmml"><mrow id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.4.cmml"><msub id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.2" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.2.cmml">m</mi><mn id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.3" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.4" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.4.cmml">,</mo><msub id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.2" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.2.cmml">m</mi><mn id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.3" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.5" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.4.cmml">,</mo><msub id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.2" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.2.cmml">m</mi><mn id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.3" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.3.cmml">3</mn></msub><mo id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.6" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.SSS2.p1.1.m1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.cmml">…</mi></mrow><mo lspace="0em" rspace="0.167em" id="S3.SS4.SSS2.p1.1.m1.3.3.2.3" xref="S3.SS4.SSS2.p1.1.m1.3.3.3a.cmml">.</mo><msub id="S3.SS4.SSS2.p1.1.m1.3.3.2.2" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.3.3.2.2.2" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2.2.cmml">m</mi><mi id="S3.SS4.SSS2.p1.1.m1.3.3.2.2.3" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.1.m1.3b"><apply id="S3.SS4.SSS2.p1.1.m1.3.3.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p1.1.m1.3.3.3a.cmml" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.3">formulae-sequence</csymbol><list id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.4.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3"><apply id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.2">𝑚</ci><cn type="integer" id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2">subscript</csymbol><ci id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.2">𝑚</ci><cn type="integer" id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.2.2.3">2</cn></apply><apply id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3">subscript</csymbol><ci id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.2">𝑚</ci><cn type="integer" id="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.2.2.1.1.3.3.3">3</cn></apply><ci id="S3.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1">…</ci></list><apply id="S3.SS4.SSS2.p1.1.m1.3.3.2.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p1.1.m1.3.3.2.2.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS4.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2.2">𝑚</ci><ci id="S3.SS4.SSS2.p1.1.m1.3.3.2.2.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.3.3.2.2.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.1.m1.3c">m_{1},m_{2},m_{3},....m_{n}</annotation></semantics></math>) as <math id="S3.SS4.SSS2.p1.2.m2.1" class="ltx_math_unparsed" alttext="E(m_{1}),E(m_{2}),E(m_{3})....,E(m_{n})" display="inline"><semantics id="S3.SS4.SSS2.p1.2.m2.1a"><mrow id="S3.SS4.SSS2.p1.2.m2.1b"><mi id="S3.SS4.SSS2.p1.2.m2.1.1">E</mi><mrow id="S3.SS4.SSS2.p1.2.m2.1.2"><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.2.1">(</mo><msub id="S3.SS4.SSS2.p1.2.m2.1.2.2"><mi id="S3.SS4.SSS2.p1.2.m2.1.2.2.2">m</mi><mn id="S3.SS4.SSS2.p1.2.m2.1.2.2.3">1</mn></msub><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.2.3">)</mo></mrow><mo id="S3.SS4.SSS2.p1.2.m2.1.3">,</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.4">E</mi><mrow id="S3.SS4.SSS2.p1.2.m2.1.5"><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.5.1">(</mo><msub id="S3.SS4.SSS2.p1.2.m2.1.5.2"><mi id="S3.SS4.SSS2.p1.2.m2.1.5.2.2">m</mi><mn id="S3.SS4.SSS2.p1.2.m2.1.5.2.3">2</mn></msub><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.5.3">)</mo></mrow><mo id="S3.SS4.SSS2.p1.2.m2.1.6">,</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.7">E</mi><mrow id="S3.SS4.SSS2.p1.2.m2.1.8"><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.8.1">(</mo><msub id="S3.SS4.SSS2.p1.2.m2.1.8.2"><mi id="S3.SS4.SSS2.p1.2.m2.1.8.2.2">m</mi><mn id="S3.SS4.SSS2.p1.2.m2.1.8.2.3">3</mn></msub><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.8.3">)</mo></mrow><mi mathvariant="normal" id="S3.SS4.SSS2.p1.2.m2.1.9">…</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.SSS2.p1.2.m2.1.10">.</mo><mo id="S3.SS4.SSS2.p1.2.m2.1.11">,</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.12">E</mi><mrow id="S3.SS4.SSS2.p1.2.m2.1.13"><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.13.1">(</mo><msub id="S3.SS4.SSS2.p1.2.m2.1.13.2"><mi id="S3.SS4.SSS2.p1.2.m2.1.13.2.2">m</mi><mi id="S3.SS4.SSS2.p1.2.m2.1.13.2.3">n</mi></msub><mo stretchy="false" id="S3.SS4.SSS2.p1.2.m2.1.13.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.2.m2.1c">E(m_{1}),E(m_{2}),E(m_{3})....,E(m_{n})</annotation></semantics></math>, a ciphertext that efficiently encrypts <math id="S3.SS4.SSS2.p1.3.m3.1" class="ltx_math_unparsed" alttext="f(m_{1},m_{2},m_{3}....m_{n})" display="inline"><semantics id="S3.SS4.SSS2.p1.3.m3.1a"><mrow id="S3.SS4.SSS2.p1.3.m3.1b"><mi id="S3.SS4.SSS2.p1.3.m3.1.1">f</mi><mrow id="S3.SS4.SSS2.p1.3.m3.1.2"><mo stretchy="false" id="S3.SS4.SSS2.p1.3.m3.1.2.1">(</mo><msub id="S3.SS4.SSS2.p1.3.m3.1.2.2"><mi id="S3.SS4.SSS2.p1.3.m3.1.2.2.2">m</mi><mn id="S3.SS4.SSS2.p1.3.m3.1.2.2.3">1</mn></msub><mo id="S3.SS4.SSS2.p1.3.m3.1.2.3">,</mo><msub id="S3.SS4.SSS2.p1.3.m3.1.2.4"><mi id="S3.SS4.SSS2.p1.3.m3.1.2.4.2">m</mi><mn id="S3.SS4.SSS2.p1.3.m3.1.2.4.3">2</mn></msub><mo id="S3.SS4.SSS2.p1.3.m3.1.2.5">,</mo><msub id="S3.SS4.SSS2.p1.3.m3.1.2.6"><mi id="S3.SS4.SSS2.p1.3.m3.1.2.6.2">m</mi><mn id="S3.SS4.SSS2.p1.3.m3.1.2.6.3">3</mn></msub><mi mathvariant="normal" id="S3.SS4.SSS2.p1.3.m3.1.2.7">…</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.SSS2.p1.3.m3.1.2.8">.</mo><msub id="S3.SS4.SSS2.p1.3.m3.1.2.9"><mi id="S3.SS4.SSS2.p1.3.m3.1.2.9.2">m</mi><mi id="S3.SS4.SSS2.p1.3.m3.1.2.9.3">n</mi></msub><mo stretchy="false" id="S3.SS4.SSS2.p1.3.m3.1.2.10">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.3.m3.1c">f(m_{1},m_{2},m_{3}....m_{n})</annotation></semantics></math> can be computed for any computable function <math id="S3.SS4.SSS2.p1.4.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS4.SSS2.p1.4.m4.1a"><mi id="S3.SS4.SSS2.p1.4.m4.1.1" xref="S3.SS4.SSS2.p1.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.4.m4.1b"><ci id="S3.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS2.p1.4.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.4.m4.1c">f</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS3.5.1.1" class="ltx_text">III-D</span>3 </span>Secure Multiparty Computation (MPC)</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.3" class="ltx_p">Is also a cryptography technique that enables different parties to carry out distributed tasks in a well-secure manner. In an MPC, a given number of parties, <math id="S3.SS4.SSS3.p1.1.m1.1" class="ltx_math_unparsed" alttext="P_{1},P_{2},P_{3},....,P_{N}" display="inline"><semantics id="S3.SS4.SSS3.p1.1.m1.1a"><mrow id="S3.SS4.SSS3.p1.1.m1.1b"><msub id="S3.SS4.SSS3.p1.1.m1.1.2"><mi id="S3.SS4.SSS3.p1.1.m1.1.2.2">P</mi><mn id="S3.SS4.SSS3.p1.1.m1.1.2.3">1</mn></msub><mo id="S3.SS4.SSS3.p1.1.m1.1.3">,</mo><msub id="S3.SS4.SSS3.p1.1.m1.1.4"><mi id="S3.SS4.SSS3.p1.1.m1.1.4.2">P</mi><mn id="S3.SS4.SSS3.p1.1.m1.1.4.3">2</mn></msub><mo id="S3.SS4.SSS3.p1.1.m1.1.5">,</mo><msub id="S3.SS4.SSS3.p1.1.m1.1.6"><mi id="S3.SS4.SSS3.p1.1.m1.1.6.2">P</mi><mn id="S3.SS4.SSS3.p1.1.m1.1.6.3">3</mn></msub><mo id="S3.SS4.SSS3.p1.1.m1.1.7">,</mo><mi mathvariant="normal" id="S3.SS4.SSS3.p1.1.m1.1.1">…</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.SSS3.p1.1.m1.1.8">.</mo><mo id="S3.SS4.SSS3.p1.1.m1.1.9">,</mo><msub id="S3.SS4.SSS3.p1.1.m1.1.10"><mi id="S3.SS4.SSS3.p1.1.m1.1.10.2">P</mi><mi id="S3.SS4.SSS3.p1.1.m1.1.10.3">N</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.1.m1.1c">P_{1},P_{2},P_{3},....,P_{N}</annotation></semantics></math> and each have private data, <math id="S3.SS4.SSS3.p1.2.m2.1" class="ltx_math_unparsed" alttext="D_{1},D_{2},D_{3},....,D_{N}" display="inline"><semantics id="S3.SS4.SSS3.p1.2.m2.1a"><mrow id="S3.SS4.SSS3.p1.2.m2.1b"><msub id="S3.SS4.SSS3.p1.2.m2.1.2"><mi id="S3.SS4.SSS3.p1.2.m2.1.2.2">D</mi><mn id="S3.SS4.SSS3.p1.2.m2.1.2.3">1</mn></msub><mo id="S3.SS4.SSS3.p1.2.m2.1.3">,</mo><msub id="S3.SS4.SSS3.p1.2.m2.1.4"><mi id="S3.SS4.SSS3.p1.2.m2.1.4.2">D</mi><mn id="S3.SS4.SSS3.p1.2.m2.1.4.3">2</mn></msub><mo id="S3.SS4.SSS3.p1.2.m2.1.5">,</mo><msub id="S3.SS4.SSS3.p1.2.m2.1.6"><mi id="S3.SS4.SSS3.p1.2.m2.1.6.2">D</mi><mn id="S3.SS4.SSS3.p1.2.m2.1.6.3">3</mn></msub><mo id="S3.SS4.SSS3.p1.2.m2.1.7">,</mo><mi mathvariant="normal" id="S3.SS4.SSS3.p1.2.m2.1.1">…</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.SSS3.p1.2.m2.1.8">.</mo><mo id="S3.SS4.SSS3.p1.2.m2.1.9">,</mo><msub id="S3.SS4.SSS3.p1.2.m2.1.10"><mi id="S3.SS4.SSS3.p1.2.m2.1.10.2">D</mi><mi id="S3.SS4.SSS3.p1.2.m2.1.10.3">N</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.2.m2.1c">D_{1},D_{2},D_{3},....,D_{N}</annotation></semantics></math> respectively. The participants individually compute the value of a public function on their private data as; <math id="S3.SS4.SSS3.p1.3.m3.1" class="ltx_math_unparsed" alttext="F(D_{1},D_{2},D_{3},....,D_{N})" display="inline"><semantics id="S3.SS4.SSS3.p1.3.m3.1a"><mrow id="S3.SS4.SSS3.p1.3.m3.1b"><mi id="S3.SS4.SSS3.p1.3.m3.1.2">F</mi><mrow id="S3.SS4.SSS3.p1.3.m3.1.3"><mo stretchy="false" id="S3.SS4.SSS3.p1.3.m3.1.3.1">(</mo><msub id="S3.SS4.SSS3.p1.3.m3.1.3.2"><mi id="S3.SS4.SSS3.p1.3.m3.1.3.2.2">D</mi><mn id="S3.SS4.SSS3.p1.3.m3.1.3.2.3">1</mn></msub><mo id="S3.SS4.SSS3.p1.3.m3.1.3.3">,</mo><msub id="S3.SS4.SSS3.p1.3.m3.1.3.4"><mi id="S3.SS4.SSS3.p1.3.m3.1.3.4.2">D</mi><mn id="S3.SS4.SSS3.p1.3.m3.1.3.4.3">2</mn></msub><mo id="S3.SS4.SSS3.p1.3.m3.1.3.5">,</mo><msub id="S3.SS4.SSS3.p1.3.m3.1.3.6"><mi id="S3.SS4.SSS3.p1.3.m3.1.3.6.2">D</mi><mn id="S3.SS4.SSS3.p1.3.m3.1.3.6.3">3</mn></msub><mo id="S3.SS4.SSS3.p1.3.m3.1.3.7">,</mo><mi mathvariant="normal" id="S3.SS4.SSS3.p1.3.m3.1.1">…</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.SSS3.p1.3.m3.1.3.8">.</mo><mo id="S3.SS4.SSS3.p1.3.m3.1.3.9">,</mo><msub id="S3.SS4.SSS3.p1.3.m3.1.3.10"><mi id="S3.SS4.SSS3.p1.3.m3.1.3.10.2">D</mi><mi id="S3.SS4.SSS3.p1.3.m3.1.3.10.3">N</mi></msub><mo stretchy="false" id="S3.SS4.SSS3.p1.3.m3.1.3.11">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.3.m3.1c">F(D_{1},D_{2},D_{3},....,D_{N})</annotation></semantics></math> while keeping their individual inputs secret.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Blockchain-based Federated Learning</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section reviews existing research studies related to the application of blockchain technology to federated learning.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Homomorphic encryption based</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> proposed a blockchain-based privacy-preserving federated learning (BPFL) model in the Internet of Vehicles (IoV). The main goal is to mitigate the privacy risk of poisoning attacks by participants and stealing sensitive data by aggregation servers. BPFL model consists of four sections: Client user, federated learning node (FL node), model aggregation node (MA node), virtual verification node (VV node), and certificate authority (CA). Using homomorphic encryption and developing Multi-Krum<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> lead to verifying and filtering local model changes. Multi-Krum uses the Krum function to calculate scores for each proposed vector, which helps identify reliable participants while excluding outliers in distributed machine learning. As a result, the system decreases runtime overhead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In another study, Miao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> provided privacy by designing a blockchain-based privacy-preserving byzantine-robust federated learning (PBFL) model. They make a trusted global model by checking cosine similarities to identify negative gradient and honest gradient vectors. Also, they applied Cheon-Kim-Kim-Song (CKKS) scheme, a fully homomorphic encryption (FHE) method to encrypt local gradients and provide privacy protection. However, their work is suitable for a balanced distribution of client data only and not in cases where the client data is Non-independent and identically distributed (Non-IID).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> approach to blockchain-based federated learning involves encrypting gradients using the BCP (Bresson-Catalano-Pointcheval) mechanism, which adds noise to each encrypted gradient. Then, all the updated gradients are collected to another blockchain. This blockchain could evaluate the malicious client if they provided a low-quality gradient. In terms of overhead, the algorithm does not add any extra overhead of encryption compared to previous works, but it reduces the time-consuming process. Also, the accuracy of the audit algorithm in the baseline is over 92%, and it decreased to 90% when faced with a poisoning attack (i.e., low gradients). This shows that the proposed model can recognize malicious owners. However, as the number of data owners increases, the behavior and audit chains may become overwhelming. This can lead to increased processing times and delays. This could limit the practical use of the proposed approach in large-scale federated learning scenarios.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">A novel technique was proposed by Alzubi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> using deep learning and blockchain paradigms to preserve the privacy of electronic health records. First, the health records are classified using CNN into normal and abnormal users. Next, a federated learning mechanism based on cryptography is integrated into the Blockchain system. The blockchain becomes responsible for keeping track of encrypted local models from the FL clients. The blockchain also ensures that the client’s contributions to the global model are verified before aggregation.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">An efficient and secure blockchain-based FL system paradigm (ESB-FL) was also developed by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In the introduced scheme, a new lightweight cryptography tool was proposed. The tool is based on a non-interaction designated decryptor function. The tool is used to encrypt each participant’s local model updates. ESB-FL can ensure the privacy protection of FL participants. ESB-FL can also efficiently preserve the global model’s accuracy. The approach achieves this with considerably low and effective communication costs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Differential Privacy-based Approaches</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> designed a blockchain-based federated learning model for home appliance manufacturers to develop their services and products. First, customers train a model using a collection of home appliance data. Then, they send the trained model to the blockchain to trace clients’ or manufacturers’ activities and prevent the probability of cyber threats. Finally, as a miner, one of the clients uploads the model to the blockchain. The authors recommended using differential privacy techniques on features to provide clients’ privacy by adding <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\varepsilon</annotation></semantics></math>-DP noise to features.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In another study, Qi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed a federated learning-based Traffic Flow Prediction (TFP) system. They have integrated GRU neural networks with blockchain and FL-based TFP schemes. Rather than directly sending individual data, the participating vehicles use their data to perform local model training and share local model updates, thus protecting privacy. Blockchain prevents the security risks of the central server and clients. This is achieved by replacing the central server with a group of trusted nodes. The nodes manage all the local model updates. In addition, they apply differential privacy by adding Gaussian noise to local model updates, thereby protecting the client’s data. The proposed model was compared with LSTM, stacked autoencoder (SAE), and SVM, in which the proposed model could accurately predict traffic flow better than the other models. Moreover, the proposed model effectively mitigates poisoning attacks since the accuracy of blockchain does not reduce even if the number of malicious clients increases. The proposed model faces a challenge in that the convergence rate of the SAE model is faster. This is because the SAE model does not need to complete a model aggregation step. The SAE model has a centralized learning paradigm, which contributes to its faster convergence rate.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Wan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposed a novel blockchain-based federated learning framework to avoid data falsification beyond 5G networks (B5G) enabling edge computing. They also added a differential privacy identifier to Wasserstein Generative Adversarial Network (WGAN)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to distinguish if synthetic data complies with differential privacy. Lastly, a time delay analysis was conducted on a single epoch of the proposed model, which was then used to determine the optimal rate for generating blockchain blocks. The trained local parameters of edge devices are regenerated by the WGAN generator and then assessed by DP-identifier and WGAN discriminator during the FL training process. With better data utility, this technique ensures that the resulting synthetic model parameters fulfill DP. Blockchain-enabled FL’s convergence latency has been seen to be quadratic to the block production rate. As a result, the experimental findings lead to an optimum block generation rate.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Shayan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> introduced Biscotti, a blockchain-based system for federated learning. It uses cryptography and blockchain technology to enable secure and private federated learning across multiple organizations. The system allows organizations to store and process data locally. The system also allows machine learning models to be trained across all participating organizations. Biscotti comprises four main components: blockchain ledger, consensus protocol, smart contracts, and off-chain storage. The system provides a variety of security measures such as data privacy and access control. The measures are put in place to ensure that data is secure and only accessible to authorized parties. Additionally, the system utilizes various techniques to facilitate efficient and secure data exchange, such as differential privacy and distributed data aggregation.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Salim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> proposed a differential privacy blockchain-based explainable FL (DP-BFL) architecture using Social Media 3.0 networks. This architecture allows internet-enabled devices to participate in training global models while preserving data privacy. After local training, participants upload their deferentially private local model updates to the blockchain system. These local updates are then evaluated and verified by the miners of the blockchain system. DP-BFL ensures that the privacy of the participants as well as a good performance of the global model, is achieved by mitigating the impact of malicious participants’ local updates.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposed a novel approach to block-chain-enabled adaptive asynchronous federated learning (FedTwin). The approach enables adaptive and asynchronous training in digital twin networks. The approach addresses the challenges of centralized processing, data falsification, privacy leakage, and lack of incentive mechanisms in digital twin networks. FedTwin uses a proof-of-federalism consensus algorithm for efficient and secure synchronization of digital twin networks (DTN), enabling a personalized incentive mechanism. The approach also uses privacy-preserving local digital twin (DT) training with falsification filtering. The approach uses adaptive asynchronous global aggregation of DTN with a roll-back mechanism. The authors evaluate the performance of FedTwin on a real-world dataset, which shows its superior performance for DTN.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Secure Multi-party Computation-based Approaches</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposed a collaborative architecture enabled by blockchain to share data among multiple parties. The architecture also minimizes the risk of data leakage and grants data owners greater control over access to shared data. By using federated learning to construct data models and share them instead of raw data, the authors transformed data sharing into a machine learning problem, thereby, enhancing the usage of computing resources and the effectiveness of the data-sharing system. To safeguard data privacy, the authors integrated differential privacy into federated learning. The effectiveness of the proposed model was evaluated for data categorization using benchmark, open real-world datasets. However, three potential threats were identified: data quality, data security, and data authority management. To address these threats, the authors integrated federated learning to achieve differential privacy. They employed a permissioned blockchain to eliminate centralized trust. They ensured the quality of shared data to prevent invalid sharing. They facilitated secure data management by allowing data providers to upload data only through permissioned blockchain.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> proposed BFLC, a Blockchain-based Federated Learning framework with Committee consensus. To address the integrating storage optimization, analysis of hostile node threats, and community node administration issues, FL is performed by participating nodes using blockchain. The blockchain maintains global models and local updates without the use of a centralized server. The authors employ a novel delegated consensus mechanism, which addresses the missions of gradient selection and block generation while accounting for the communication cost of FL. In the experiment, BFLC demonstrated higher accuracy compared to basis FL and stand-alone framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> in various malicious proportion settings. The author incorporated real-world datasets into the BFLC framework, enabling them to obtain global models that closely resemble the centralized training approach in federated learning. However, the need for a trusted blockchain system raises unexplored aspects of ensuring trustworthiness, which may pose challenges and require further investigation in order to address potential vulnerabilities and maintain the integrity of the BFLC framework.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposed a decentralized paradigm for big data-driven cognitive computing (D2C). This paradigm addresses issues such as unreliable performance, inefficiency, privacy leakage, and poisoning attacks. It does so by combining federated learning and blockchain. Their novel architecture uses the federated learning paradigm for massive D2C. This architecture significantly improves the manufacturing performance of Industry 4.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. It also overcomes privacy and performance problems associated with cognitive computing. To enhance performance, accuracy, and incentive mechanisms for Industry 4.0 automation, the authors integrate blockchain into federated learning, creating a D2C paradigm for the Industry 4.0 model. They develop an optimization model with a modified Markovian decision process to simulate a conflict with adversaries. The model increases accuracy and robustness against poisoning attacks.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Rehman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> presented a novel approach for secure and privacy-preserving federated learning.The proposed approach is based on blockchain technology, providing a distributed consensus mechanism for reputation-aware federated learning. This system allows the federated learning participants to securely and privately exchange information. It also ensures data privacy and integrity. The system uses a blockchain-based distributed ledger to provide a trustless and decentralized environment for federated learning. </p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">Arachchige et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposed PriMod-Chain, a new framework for trustworthy and privacy-preserving machine learning in Industrial IoT systems. To ensure privacy and trustworthiness, the framework combines smart contracts, blockchain, Federated Machine Learning (FedML), Differential Privacy (DP), and InterPlanetary File System (IPFS). The proposed framework was tested for feasibility, and the results for privacy, security, reliability, safety, and resilience were all positive. The authors suggested further research to reduce latency in order to improve efficiency. The PriMod-Chain protocol is presented as a viable solution for reliable privacy-preserving machine learning in Industrial IoT systems.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Overview of recent studies on Data Privacy in BCFL.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r">
<span id="S4.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.1.1.1.1.1" class="ltx_p"><span id="S4.T1.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Paper</span></span>
</span>
</th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r">
<span id="S4.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.1.1.2.1.1" class="ltx_p"><span id="S4.T1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Privacy Approach</span></span>
</span>
</th>
<th id="S4.T1.4.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r">
<span id="S4.T1.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.1.1.3.1.1" class="ltx_p"><span id="S4.T1.4.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Challenges</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.2.1" class="ltx_tr">
<td id="S4.T1.4.2.1.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.2.1.1.1.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.2.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.2.1.2.1.1" class="ltx_p">BPFL- A combination of Multi-Krum and homomorphic encryption</span>
</span>
</td>
<td id="S4.T1.4.2.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.2.1.3.1.1" class="ltx_p">Efficient model combination and complex homomorphic encryption management.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.3.2" class="ltx_tr">
<td id="S4.T1.4.3.2.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.3.2.1.1.1" class="ltx_p">Zhoa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.3.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.3.2.2.1.1" class="ltx_p">DP- Laplace noise</span>
</span>
</td>
<td id="S4.T1.4.3.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.3.2.3.1.1" class="ltx_p">Optimizing noise level and selecting privacy parameters.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.4.3" class="ltx_tr">
<td id="S4.T1.4.4.3.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.3.1.1.1" class="ltx_p">Miao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.4.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.3.2.1.1" class="ltx_p">PBFL- Fully homomorphic encryption and cosine similarity</span>
</span>
</td>
<td id="S4.T1.4.4.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.3.3.1.1" class="ltx_p">Scheme on a balanced distribution of client data and not on cases where the client data is non-IID</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.5.4" class="ltx_tr">
<td id="S4.T1.4.5.4.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.5.4.1.1.1" class="ltx_p">Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.5.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.5.4.2.1.1" class="ltx_p">Homomorphic encryption (BCP-based) for gradient</span>
</span>
</td>
<td id="S4.T1.4.5.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.5.4.3.1.1" class="ltx_p">Blockchain-based audit approach for encrypted gradients may have limited scalability due to the increased processing times and delay</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.6.5" class="ltx_tr">
<td id="S4.T1.4.6.5.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.6.5.1.1.1" class="ltx_p">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.6.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.6.5.2.1.1" class="ltx_p">BFLC- Blockchain-based Federated Learning framework with Committee consensus</span>
</span>
</td>
<td id="S4.T1.4.6.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.6.5.3.1.1" class="ltx_p">CCM approach has increased energy consumption due to a large amount of communication overhead involved during model updates between nodes</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.7.6" class="ltx_tr">
<td id="S4.T1.4.7.6.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.7.6.1.1.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.7.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.7.6.2.1.1" class="ltx_p">PBFL- Privacy-preserving data sharing the mechanism for distributed multiple parties</span>
</span>
</td>
<td id="S4.T1.4.7.6.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.7.6.3.1.1" class="ltx_p">Improving the utility of data models mapped from raw data is necessary</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.8.7" class="ltx_tr">
<td id="S4.T1.4.8.7.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.8.7.1.1.1" class="ltx_p">Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.8.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.8.7.2.1.1" class="ltx_p">BFL- Decentralized paradigm for big data-driven cognitive computing (D2C)</span>
</span>
</td>
<td id="S4.T1.4.8.7.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.8.7.3.1.1" class="ltx_p">Improves on Markov decision process (MDP) rather than addressing privacy issues with blockchain that is assumed tamper-proof</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.9.8" class="ltx_tr">
<td id="S4.T1.4.9.8.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.9.8.1.1.1" class="ltx_p">Wan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.9.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.9.8.2.1.1" class="ltx_p">BFL- Wasserstein generative adversarial network (WGAN)</span>
</span>
</td>
<td id="S4.T1.4.9.8.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.9.8.3.1.1" class="ltx_p">Need for efficient communication and computation methods, privacy and security concerns in federated learning, and the problem of non-iid data distribution in edge computing environments</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.10.9" class="ltx_tr">
<td id="S4.T1.4.10.9.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.10.9.1.1.1" class="ltx_p">Shayan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.10.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.10.9.2.1.1" class="ltx_p">Biscotti: a fully decentralized peer-to-peer (P2P) approach to multi-party ML</span>
</span>
</td>
<td id="S4.T1.4.10.9.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.10.9.3.1.1" class="ltx_p">Requiring large honest samples for Multi-Krum, limited scalability for large deep learning models, and vulnerability to privacy attacks.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.11.10" class="ltx_tr">
<td id="S4.T1.4.11.10.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.11.10.1.1.1" class="ltx_p">Qi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.11.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.11.10.2.1.1" class="ltx_p">BFL- Traffic Flow Prediction (TFP)</span>
</span>
</td>
<td id="S4.T1.4.11.10.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.11.10.3.1.1" class="ltx_p">Slower convergence rate due to its decentralized learning paradigm and model aggregation step, as opposed to the SAE model’s centralized learning.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.12.11" class="ltx_tr">
<td id="S4.T1.4.12.11.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.12.11.1.1.1" class="ltx_p">Ur Rehman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.12.11.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.12.11.2.1.1" class="ltx_p">BFL- Reputation-aware fine-grained</span>
</span>
</td>
<td id="S4.T1.4.12.11.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.12.11.3.1.1" class="ltx_p">A reputation-aware federated learning system that exchanges information securely and privately while maintaining data privacy and integrity.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.13.12" class="ltx_tr">
<td id="S4.T1.4.13.12.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.13.12.1.1.1" class="ltx_p">Arachchige et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.13.12.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.13.12.2.1.1" class="ltx_p">PriModChain - Differential privacy, Federated ML, Ethereum blockchain, and Smart contracts.</span>
</span>
</td>
<td id="S4.T1.4.13.12.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.13.12.3.1.1" class="ltx_p">Vulnerabilities and providing security recommendations.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.14.13" class="ltx_tr">
<td id="S4.T1.4.14.13.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.14.13.1.1.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.14.13.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.14.13.2.1.1" class="ltx_p">BEMA - Multiparty multiclass margin System initialization,
off-chain sample mining and on-chain mining.</span>
</span>
</td>
<td id="S4.T1.4.14.13.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.14.13.3.1.1" class="ltx_p">Lack of guaranteed robustness against Byzantine attacks.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.15.14" class="ltx_tr">
<td id="S4.T1.4.15.14.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.15.14.1.1.1" class="ltx_p">Alzubi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.15.14.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.15.14.2.1.1" class="ltx_p">Deep learning and Blockchain techniques for electronic health record privacy-preservation</span>
</span>
</td>
<td id="S4.T1.4.15.14.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.15.14.3.1.1" class="ltx_p">User classification, integration using cryptography, and client contribution verification prior to model aggregation.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.16.15" class="ltx_tr">
<td id="S4.T1.4.16.15.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.16.15.1.1.1" class="ltx_p">Salim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.16.15.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.16.15.2.1.1" class="ltx_p">DP-BFL - Differential Privacy blockchain-based explainable FL</span>
</span>
</td>
<td id="S4.T1.4.16.15.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.16.15.3.1.1" class="ltx_p">Ensuring participant privacy, maintaining global model performance, and mitigating the impact of malicious local updates.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.17.16" class="ltx_tr">
<td id="S4.T1.4.17.16.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.17.16.1.1.1" class="ltx_p">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.17.16.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.17.16.2.1.1" class="ltx_p">ESB-FL - Blockchain-based FL system paradigm using Cryptography</span>
</span>
</td>
<td id="S4.T1.4.17.16.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.17.16.3.1.1" class="ltx_p">To protect FL participants’ privacy, maintain global model accuracy with low communication costs.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.18.17" class="ltx_tr">
<td id="S4.T1.4.18.17.1" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.18.17.1.1.1" class="ltx_p">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.18.17.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.18.17.2.1.1" class="ltx_p">Privacy-Preserving permissioned Blockchain enabled FL with Multi-Party Computation and Fully Homomorphic Encryption</span>
</span>
</td>
<td id="S4.T1.4.18.17.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.18.17.3.1.1" class="ltx_p">Privacy protection of participants, anonymity, and secure model updates using multi-party computation and fully-homomorphic encryption.</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.19.18" class="ltx_tr">
<td id="S4.T1.4.19.18.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.4.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.19.18.1.1.1" class="ltx_p">Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span>
</span>
</td>
<td id="S4.T1.4.19.18.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.4.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.19.18.2.1.1" class="ltx_p">BFL - Digital twin networks (DTN)</span>
</span>
</td>
<td id="S4.T1.4.19.18.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.4.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.19.18.3.1.1" class="ltx_p">Challenges in digital twin networks include centralized processing, data falsification, privacy leakage, and lack of incentive mechanisms.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> introduced a blockchain-empowered decentralized, secure multiparty learning system called BEMA, where learning parties hold diverse local models. Their work suggests ”on-chain” and ”off-chain” mining strategies for defense against attacks. The proposed approach involves two steps. The first step is to identify data samples suitable for model calibration. On the other hand, the second step is used to calibrate particular local models based on the discovered samples. Then, these models are entered into the new blocks. Mainly, BEMA includes off-chain and system initialization, both on-chain mining and sample mining. During system startup, The operators (OPs) register their names (IDs) and model details on the link. The participating party can then register their IDs and chain model information. Once a miner uses a valid sample to update models on the chain, each party can broadcast it to the blockchain and earn certain system rewards.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed a blockchain-enabled secure federated learning system for distributed banks. This system combines multi-party computation (MPC) and the multi-key fully-homomorphic encryption(FHE) scheme. The local model updates from the participants are encrypted using the Multi-key FHE scheme and then signed with some pseudo-ID before sharing with others participating in the MPC. With this approach, the participants’ privacy protection and anonymity is conveniently achieved.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Data Privacy Challenges</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Several studies have proposed different solutions to implement federated learning using blockchain technology. Table <a href="#S4.T1" title="TABLE I ‣ IV-C Secure Multi-party Computation-based Approaches ‣ IV Blockchain-based Federated Learning ‣ A Survey on Blockchain-Based Federated Learning and Data Privacy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> lists these studies, which aim to address privacy challenges in federated learning. However, the approaches employed by these studies encounter some challenges such as efficient model combination, selecting privacy parameters, non-IID data distribution, scalability, and privacy attacks. Some proposed solutions include homomorphic encryption, differential privacy, reputation-aware federated learning, and digital twin networks. These approaches aim to maintain global model accuracy, protect participant privacy, and reduce the impact of malicious local updates. However, they also have limitations, such as increased energy consumption, communication overhead, and security vulnerabilities. Despite these challenges, data privacy and security concerns still arise. Although blockchain technology has effectively decentralized federated learning, it has some drawbacks.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Data Leakage</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The privacy of BCFL can be compromised by inference attacks even though the model updates are encrypted. Malicious users can still analyze the updates to deduce information. To address this issue, future works may explore the use of other FL structures or combine different techniques.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Model accuracy and Latency</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">BCFL models require maintaining and improving the accuracy and efficiency of the model. The mini-batching at the client during each training epoch and increasing multi-client parallelism to reach a target test-set accuracy framework can be adopted.
The learning performance of Federated Learning has not been discussed in details. It is necessary to verify the multi-key encryption protocol because of the way it secures the federated ML model data. The accuracy and latency of PrimodChain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> systems still need optimization.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Unexplored Complexity</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The work presented by Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> provides only theoretical analysis, and BEMA robustness against Byzantine attacks cannot be guaranteed. A Byzantine attack occurs when an attacker adheres to the system protocol but disseminates malicious information to innocent system participants, with the goal of diminishing system performance and manipulating or influencing the system’s output. More evaluation is still needed as the learning strategies and security concerns are not fully investigated. The existing research on multiparty learning has mainly focused on homogeneous local models. However, there is still a lack of research on multiparty learning over heterogeneous local models. This is despite the fact that such a scenario may be more practical and useful in real-world applications.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Incentive Mechanism Scheme</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The blockchain incentive mechanism plays a crucial role in motivating users to participate in consensus. Abandoning the token incentive could significantly decrease users’ motivation to participate if the incentive system of the agreement is not sufficient or perfect enough. The rewards are typically accessible when a new block is
either automatically generated or obtained by charging fees for transactions. Blockchain technology requires honest mining for the successful completion of mining the block. The significant role of incentive mechanisms schemes in blockchain federated learning is often overlooked in many studies.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.5.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.6.2" class="ltx_text ltx_font_italic">Scalability</span>
</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">The blockchain-based audit approach for encrypted gradients in federated learning provides privacy while assessing gradient quality. However, scalability can be a challenge due to the consensus requirement in blockchain technology. Adding new blocks necessitates agreement among all network nodes, resulting in time-consuming and expensive audits, particularly for large federated learning systems. Two approaches to mitigate scalability limitations are (1) off-chain computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which performs audits on a subset of nodes, and (2) compression, which reduces the size of encrypted gradients before blockchain storage. These techniques aim to improve efficiency without compromising quality. Continued research seeks to enhance scalability and make this approach more practical.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future Lines of Research</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The field of blockchain-based federated learning presents several promising avenues for future research. One such area is investigating incentive systems that can encourage data providers to participate in the federated learning process within blockchain networks. Incentive systems can help address challenges such as the lack of motivation for data providers to contribute to federated learning or the potential for free-riding on the contributions of others. The design and implementation of incentive mechanisms that provide adequate rewards to data providers without compromising the security and privacy of the system is a crucial research question.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Another promising direction for future research in blockchain-based federated learning is the development of privacy-preserving techniques. Although current systems use secure aggregation to encrypt model updates before transmitting them to the server, this approach may not be sufficient to protect sensitive data. Therefore, exploring novel techniques for data privacy protection in federated learning is necessary. Such techniques could include encryption methods such as homomorphic encryption, differential privacy, or secure multi-party computation. These techniques ensure that the data remains private, even from the server or other nodes, while allowing for effective aggregation and model updates.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Smart contracts, which are significant programs that run on the blockchain, can also play a significant role in blockchain-based federated learning. Smart contracts can automate and enforce the rules governing the training process, including data privacy and security protocols and incentives for participating nodes. Using smart contracts could enhance the transparency and fairness of the system, allowing for more efficient and secure federated learning.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Moreover, integrating machine learning techniques such as transfer learning and meta-learning can enhance the efficiency and effectiveness of the federated learning system. Transfer learning is a machine learning technique that allows the transfer of knowledge from one task to another. On the other hand, meta-learning is a learning approach that utilizes prior knowledge to achieve faster and more accurate learning. Incorporating these techniques into the federated learning system can reduce the required training data and improve the models’ accuracy.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Blockchain-based Federated Learning (FL) is an emerging approach that has garnered significant interest in improving the privacy and security of machine learning models. The decentralized and immutable nature of blockchain technology has the potential to replace traditional centralized methods, enhancing the privacy and efficiency of FL. Blockchain technology can protect against data breaches and malicious actors while enabling multiple parties to train models collaboratively without sharing their data. Moreover, data distribution among multiple parties without a centralized server can further reduce the risks of data breaches and enhance data privacy. The use of blockchain technology also ensures that data is securely stored and accurately tracked, enabling efficient and trustworthy collaborative learning. The integration of blockchain and FL holds immense potential in advancing the field of machine learning and improving its practical applications in various industries.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jafar A. Alzubi, Omar A. Alzubi, Ashish Singh, and Manikandan Ramachandran.

</span>
<span class="ltx_bibblock">Cloud-iiot-based electronic health record privacy-preserving by cnn
and blockchain-enabled federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 19(1):1080–1087,
2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Pathum Chamikara Mahawaga Arachchige, Peter Bertok, Ibrahim Khalil, Dongxi Liu,
Seyit Camtepe, and Mohammed Atiquzzaman.

</span>
<span class="ltx_bibblock">A trustworthy privacy preserving framework for machine learning in
industrial iot systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 16(9):6092–6102,
2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Martin Arjovsky, Soumith Chintala, and Léon Bottou.

</span>
<span class="ltx_bibblock">Wasserstein generative adversarial networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
214–223. PMLR, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Ateniese, S. Kamara, and J. Katz.

</span>
<span class="ltx_bibblock">Enhancing bitcoin security and performance with strong consistency
via collective signing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Conference on Financial Cryptography and Data
Security</span>, pages 136–150. Springer, Berlin, Heidelberg, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient
descent.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano
Mazzocchi, Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, 1:374–388, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Poornima M Chanal and Mahabaleshwar S Kakkasageri.

</span>
<span class="ltx_bibblock">Security and privacy in iot: a survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Wireless Personal Communications</span>, 115(2):1667–1693, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Biwen Chen, Honghong Zeng, Tao Xiang, Shangwei Guo, Tianwei Zhang, and Yang
Liu.

</span>
<span class="ltx_bibblock">Esb-fl: Efficient and secure blockchain-based federated learning with
fair payment.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Big Data</span>, pages 1–1, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ronald Cramer, Ivan Bjerre Damgård, et al.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Secure multiparty computation</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jacob Eberhardt and Jonathan Heiss.

</span>
<span class="ltx_bibblock">Off-chaining models and approaches to off-chain computations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2nd Workshop on Scalable and Resilient
Infrastructures for Distributed Ledgers</span>, pages 7–12, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Craig Gentry.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">A fully homomorphic encryption scheme</span>.

</span>
<span class="ltx_bibblock">Stanford university, 2009.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">Blockchained on-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Communications Letters</span>, 24(6):1279–1283, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael
Hoffmann.

</span>
<span class="ltx_bibblock">Industry 4.0.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Business &amp; information systems engineering</span>, 6:239–242, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jun Li, Yumeng Shao, Kang Wei, Ming Ding, Chuan Ma, Long Shi, Zhu Han, and
H Vincent Poor.

</span>
<span class="ltx_bibblock">Blockchain assisted decentralized federated learning (blade-fl):
Performance analysis and resource allocation.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>,
33(10):2401–2415, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50–60, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang Yan.

</span>
<span class="ltx_bibblock">A blockchain-based decentralized federated learning framework with
committee consensus.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Network</span>, 35(1):234–241, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jingwei Liu, Xinyu He, Rong Sun, Xiaojiang Du, and Mohsen Guizani.

</span>
<span class="ltx_bibblock">Privacy-preserving data sharing scheme with fl via mpc in financial
permissioned blockchain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ICC 2021 - IEEE International Conference on Communications</span>,
pages 1–6, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang.

</span>
<span class="ltx_bibblock">Blockchain and federated learning for privacy-preserved data sharing
in industrial iot.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 16(6):4177–4186,
2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang.

</span>
<span class="ltx_bibblock">Differentially private asynchronous federated learning for mobile
edge computing in urban informatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 16(3):2134–2143,
2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lichuan Ma, Qingqi Pei, Youyang Qu, Kefeng Fan, and Xin Lai.

</span>
<span class="ltx_bibblock">Decentralized privacy-preserving reputation management for mobile
crowdsensing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">International Conference on Security and Privacy in
Communication Systems</span>, pages 532–548. Springer, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, and S. Hampson.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.05629</span>, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yinbin Miao, Ziteng Liu, Hongwei Li, Kim-Kwang Raymond Choo, and Robert H Deng.

</span>
<span class="ltx_bibblock">Privacy-preserving byzantine-robust federated learning via blockchain
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
17:2848–2861, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yinbin Miao, Ziteng Liu, Hongwei Li, Kim-Kwang Raymond Choo, and Robert H Deng.

</span>
<span class="ltx_bibblock">Privacy-preserving byzantine-robust federated learning via blockchain
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
17:2848–2861, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ahmed Afif Monrat, Olov Schelén, and Karl Andersson.

</span>
<span class="ltx_bibblock">A survey of blockchain from the perspectives of applications,
challenges, and opportunities.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 7:117134–117151, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Yuanhang Qi, M Shamim Hossain, Jiangtian Nie, and Xuandi Li.

</span>
<span class="ltx_bibblock">Privacy-preserving blockchain-based federated learning for traffic
flow prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 117:328–337, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Youyang Qu, Longxiang Gao, Yong Xiang, Shigen Shen, and Shui Yu.

</span>
<span class="ltx_bibblock">Fedtwin: Blockchain-enabled adaptive asynchronous federated learning
for digital twin networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Network</span>, pages 1–8, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Youyang Qu, Shiva Raj Pokhrel, Sahil Garg, Longxiang Gao, and Yong Xiang.

</span>
<span class="ltx_bibblock">A blockchained federated learning framework for cognitive computing
in industry 4.0 networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 17(4):2964–2973,
2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Paritosh Ramanan and Kiyoshi Nakayama.

</span>
<span class="ltx_bibblock">Baffle: Blockchain based aggregator free federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Conference on Blockchain
(Blockchain)</span>, pages 72–81. IEEE, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
B Thirumala Rao et al.

</span>
<span class="ltx_bibblock">A study on data storage security issues in cloud computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Procedia Computer Science</span>, 92:128–135, 2016.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Sara Salim, Benjamin Turnbull, and Nour Moustafa.

</span>
<span class="ltx_bibblock">A blockchain-enabled explainable federated learning for securing
internet-of-things-based social media 3.0 networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Computational Social Systems</span>, pages 1–17,
2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Muhammad Shayan, Clement Fung, Chris JM Yoon, and Ivan Beschastnikh.

</span>
<span class="ltx_bibblock">Biscotti: A blockchain system for private and secure federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>,
32(7):1513–1525, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Emiliano Sisinni, Abusayeed Saifullah, Song Han, Ulf Jennehag, and Mikael
Gidlund.

</span>
<span class="ltx_bibblock">Industrial internet of things: Challenges, opportunities, and
directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">IEEE transactions on industrial informatics</span>, 14(11):4724–4734,
2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zhe Sun, Junping Wan, Lihua Yin, Zhiqiang Cao, Tianjie Luo, and Bin Wang.

</span>
<span class="ltx_bibblock">A blockchain-based audit approach for encrypted data in federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Digital Communications and Networks</span>, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Muhammad Habib ur Rehman, Khaled Salah, Ernesto Damiani, and Davor Svetinovic.

</span>
<span class="ltx_bibblock">Towards blockchain-based reputation-aware federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2020-IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS)</span>, pages 183–188. IEEE, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yichen Wan, Youyang Qu, Longxiang Gao, and Yong Xiang.

</span>
<span class="ltx_bibblock">Privacy-preserving blockchain-enabled federated learning for
b5g-driven edge computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Computer Networks</span>, 204:108671, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Naiyu Wang, Wenti Yang, Xiaodong Wang, Longfei Wu, Zhitao Guan, Xiaojiang Du,
and Mohsen Guizani.

</span>
<span class="ltx_bibblock">A blockchain based privacy-preserving federated learning scheme for
internet of vehicles.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Digital Communications and Networks</span>, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Qianlong Wang, Yifan Guo, Xufei Wang, Tianxi Ji, Lixing Yu, and Pan Li.

</span>
<span class="ltx_bibblock">Ai at the edge: Blockchain-empowered secure multiparty learning with
heterogeneous models.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 7(10):9600–9610, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Zhilin Wang and Qin Hu.

</span>
<span class="ltx_bibblock">Blockchain-based federated learning: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2110.02182</span>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin,
Tony QS Quek, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:3454–3469, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao, Linshan Jiang, Rui Tan, Dusit Niyato, Zengxiang Li,
Lingjuan Lyu, and Yingbo Liu.

</span>
<span class="ltx_bibblock">Privacy-preserving blockchain-based federated learning for iot
devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 8(3):1817–1829, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.17337" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.17338" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.17338">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.17338" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.17339" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 20:41:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
