<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.06067] MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data</title><meta property="og:description" content="Previous studies on federated learning (FL) often encounter performance degradation due to data heterogeneity among different clients. In light of the recent advances in multimodal large language models (MLLMs), such aâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.06067">

<!--Generated on Sat Oct  5 21:22:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning,  Multimodality,  Large Language Model">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianyi Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Duke University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Frank Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">Johns Hopkins University</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ang Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">University of Maryland</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xin GUO
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Lenovo Research</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pu Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Johns Hopkins University</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haiming Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">Lenovo Research</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiran Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Duke University</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hai Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id15.1.id1" class="ltx_text ltx_affiliation_institution">Duke University</span><span id="id16.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id17.id1" class="ltx_p">Previous studies on federated learning (FL) often encounter performance degradation due to data heterogeneity among different clients. In light of the recent advances in multimodal large language models (MLLMs), such as GPT-4v and LLaVA, which demonstrate their exceptional proficiency in multimodal tasks, such as image captioning and multimodal question answering. We introduce a novel federated learning framework, named Multimodal Large Language Model Assisted Federated Learning (MLLM-FL), which which employs powerful MLLMs at the server end to address the heterogeneous and long-tailed challenges. Owing to the advanced cross-modality representation capabilities and the extensive open-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing the extensive, yet previously underexploited, open-source data accessible from websites and powerful server-side computational resources. Hence, the MLLM-FL not only enhances the performance but also avoids increasing the risk of privacy leakage and the computational burden on local devices, distinguishing it from prior methodologies. Our framework has three key stages. Initially, prior to local training on local datasets of clients, we conduct global visual-text pretraining of the model. This pretraining is facilitated by utilizing the extensive open-source data available online, with the assistance of multimodal large language models. Subsequently, the pretrained model is distributed among various clients for local training. Finally, once the locally trained models are transmitted back to the server, a global alignment is carried out under the supervision of MLLMs to further enhance the performance. Experimental evaluations on established benchmarks, show that our framework delivers promising performance in the typical scenarios with data heterogeneity and long-tail distribution across different clients in FL.</p>
</div>
<div class="ltx_keywords">Federated Learning, Multimodality, Large Language Model
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span></span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>; ; </span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The surge in IoT devices has unlocked vast potential for leveraging edge-generated data in driving cooperative computing applications such as autonomous vehicles, video analytics, and recommendation systems. Traditionally, the centralized training process raises significant data privacy and security concerns due to the necessity of transferring local information. Federated learning (FL), as introduced by <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2017</a>)</cite>, offers a solution to these privacy challenges by enabling collaborative model training across numerous clients under the orchestration of a central server, without sharing the raw data. FL systems bring obvious advantages by involving clients downloading a global model, performing local updates using their data, and then sending these updates back to the server. The server aggregates these updates to enhance the global model, thereby preserving data privacy. Aside from the privacy considerations mentioned above, there are two fundamental acknowledgements about (cross-device) federated learning which have been widely recognized: data heterogeneity among clients and the limited and diverse computational resources on local devices <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2017</a>; Kairouz etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Mahlool and Abed, <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Data heterogeneity represents a significant challenge in federated learning. It largely stems from the fact that the data across participating clients are distributed independently, with each client having a different sample distribution. Due to the diversity in clientsâ€™ datasets. these datasets often exhibit a long-tailed distribution, leading to client models that are biased toward the more common classes <cite class="ltx_cite ltx_citemacro_citep">(Sahu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2018</a>; Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2020b</a>)</cite>. This discrepancy often results in a drop in model accuracy. Although several approaches have been proposed <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>; Huang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Feng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>; Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>; Jia etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; Yang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2021</a>; Hao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, the majority fail to strike an optimal balance between performance and mitigating two critical issues: 1. avoiding concerns of privacy leakage, and 2. preventing the imposition of extra computational loads on local edge devices. For instance, some contemporary methodologies necessitate the transmission of both gradients and parameters from local models to the server, which introduces substantial privacy risks. This is because attackers could potentially reverse-engineer the transmitted data to reconstruct client-specific images, as highlighted in various studies <cite class="ltx_cite ltx_citemacro_citep">(Geiping etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019</a>; Haim etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>. Alternatively, other approaches require the deployment of sizable models on local devices, which increases the memory and computational demands.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.06067/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="262" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The whole workflow of MLLM-FL The MLLM are utilized in the first stage Global Multimodal Pretraining and the third stage Global Alignment on the server side, to avoid extra computational load on devices.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In light of the current popularity and exceptional proficiency of multimodal large language models (MLLMs) in tasks involving multimodalities, such as image captioning and multimodal question answering <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib34" title="" class="ltx_ref">2022</a>; Team etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2023</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>; Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>, we introduce a three-stage framework, named Multimodal Large Language Model
sisted Federated Learning (MLLM-FL), which utilizes multimodal large language models (MLLMs) to the FL performance on heterogeneous and long-tailed data.
The adaptation of MLLMs in FL is supported by two main considerations. Firstly, beyond the heterogenous and long-tailed distribution of client datasets, there exists an abundance of open-sourced and legally available data on the internet that can be utilized for training. This implies the potential of employing MLLMs to annotate unstructured, unlabeled online data, thereby augmenting FL performance. Secondly, in contrast to the limited computational resources available on client devices, the server-side capabilities are significantly more robust. This disparity opens up the possibility of deploying additional, more powerful MLLM on the server side to provide assistance to the FL system.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison between our methods and other status quo approaches for addressing long-tailed distribution challenges in FL</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Method</th>
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Multimodal</td>
</tr>
<tr id="S1.T1.1.1.1.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Supervision</td>
</tr>
</table>
</td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.3.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">No Gradient</td>
</tr>
<tr id="S1.T1.1.1.1.3.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Upload for Privacy</td>
</tr>
</table>
</td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.4.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">No Additional</td>
</tr>
<tr id="S1.T1.1.1.1.4.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Computing Burden</td>
</tr>
<tr id="S1.T1.1.1.1.4.1.3" class="ltx_tr">
<td id="S1.T1.1.1.1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">on Devices</td>
</tr>
</table>
</td>
<td id="S1.T1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Compatibility</td>
</tr>
<tr id="S1.T1.1.2.2" class="ltx_tr">
<th id="S1.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CReFF <cite class="ltx_cite ltx_citemacro_citep">(Shang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>,</th>
<td id="S1.T1.1.2.2.2" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.1.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S1.T1.1.2.2.5" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S1.T1.1.3.3" class="ltx_tr">
<th id="S1.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CLIP2FL <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S1.T1.1.3.3.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S1.T1.1.3.3.3" class="ltx_td"></td>
<td id="S1.T1.1.3.3.4" class="ltx_td"></td>
<td id="S1.T1.1.3.3.5" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S1.T1.1.4.4" class="ltx_tr">
<th id="S1.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MLLM-FL (ours)</th>
<td id="S1.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">âœ“</td>
<td id="S1.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">âœ“</td>
<td id="S1.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">âœ“</td>
<td id="S1.T1.1.4.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">âœ“</td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our framework has three key stages. The initial stage, termed Global Multimodal Pretraining, first employs MLLMs to generate descriptions for unlabelled data collected from the internet. Then we develop a novel pretraining strategy, Dynamic Weighted Pretraining (DWP), which enables MLLMs to assist the compact FL models within the FL framework to conduct pertaining more efficiently on the open-sourced dataset. In the second stage, known as Federated Finetuning, we distribute the pretrained FL model to clients for local training on their datasets similar to with traditional FL approaches. This stage is highly flexible and compatible, allowing the integration of various previously designed FL methods. During the third stage, we perform Global Alignment on the server-side aggregated FL model under MLLM supervision. This process, similar to the idea of alignment in large language models, is aimed at further refining the modelâ€™s outputs to better align with task-specific requirements. Indeed, our framework is adaptable to a wide range of federated learning (FL) tasks. In this study, we specifically address the prevalent challenge of data heterogeneity in federated image classification tasks and the multimodal large language models we adopt here are the large vision-language models (LVLM). During the pretraining stage, thanks to the extensive open-vocabulary prior knowledge embedded in large vision-language models, these models are capable of generating detailed descriptions for complex images found on the internet. This pretraining process of our FL model with the assistance of LVLMs on a large dataset of text-images, enables our FL model to develop enhanced image representation capabilities to better counteract the effects of data heterogeneity inherent in FL environments. Furthermore, the global alignment stage can also be designed to mitigate the issue of long-tailed distributions, which tend to bias client-side models towards more frequently occurring classes. Our contribution can be summarized as follows.</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Firstly, we pioneer the integration of the widely recognized multimodal large language model (MLLM) as an auxiliary tool in federated learning, aiming to enhance the utilization of previously underexplored internet data resources and server computational capabilities. Leveraging the formidable cross-modality representation capabilities and the vast open-vocabulary prior knowledge inherent in MLLMs, we introduce novel methodologies to address the challenges posed by long-tail distributions and data heterogeneity. This also marks the first exploration of employing MLLMs to augment federated learning, establishing an innovative framework in the field.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">In comparison to prevailing state-of-the-art approaches that address data heterogeneity in federated learning (FL), our methodology not only enhances privacy protection further but also significantly reduces the computational burden on client devices.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our extensive experimental results show that MLLM-FL can effectively handle heterogeneity and class-distribution imbalance, consistently surpassing the performance of existing state-of-the-art federated learning methodologies across a variety of datasets.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Multimodal large language model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The introduction of GPT-4(Vision) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite> and Gemini <cite class="ltx_cite ltx_citemacro_citep">(Team etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite> have demonstrated remarkable abilities in Multimodal understanding and generation, sparking a research fervor on Multimodal large language model. This enthusiasm extends to a variety of tasks, including image-text comprehension <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>; Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>; video-text understanding <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>; Maaz etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>; and audio-text understanding <cite class="ltx_cite ltx_citemacro_citep">(Chu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. Among them, recent studies in image-text comprehension with large vision-language models (LVLM) <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>; Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite> have catalyzed notable advancements in harnessing the robust capabilities of large language models to tackle multimodal tasks effectively, such as crafting narratives from images and executing intricate reasoning tasks. Prominent instances include Visual ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>, which amalgamates diverse visual foundational models for intricate visual tasks and instructions, employing iterative feedback to synchronize visual and textual modalities. In a similar way, MM-REACT <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite> merges ChatGPT with visual models for multimodal undertakings, especially in the Visual Question Answering (VQA) framework. BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>, notable for its Q-former model, has shown encouraging outcomes in VQA tasks, both in zero-shot and fine-tuning settings. LLaMA-Adapter <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> enhances multimodal fine-tuning efficiency by integrating adaptation prompt vectors as adjustable parameters, showcasing versatility in multimodal contexts. MiniGPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>, derived from GPT-4 and incorporating elements from BLIP-2 and Vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, specializes in caption generation and model refinement through image-text pair fine-tuning. LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, leveraging GPT-4, focuses on a broad spectrum of instruction fine-tuning data, ranging from multi-turn QA to image descriptions, adopting a dual-stage fine-tuning approach that prioritizes language model loss while keeping the visual model static.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Federated Learning with Heterogeneous Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Current methodologies tackling the challenge of data heterogeneity fall into the following broad categories. Some approaches aim to simultaneously improve the models on both clients and server sides through optimization techniques. Key contributions in this area have been made by the work of <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>; Huang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Feng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, who have investigated various optimization methods. Other strategies focus on enhancing the stability of local models via knowledge transfer, a technique that is model-agnostic and has been explored in the research by <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2023</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>; Chen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, aiming to mitigate data heterogeneity by spreading local training knowledge throughout the whole FL framework. Additional methods, such as those proposed by <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Chen and Chao, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, concentrate on improving model aggregation on the server side to address data heterogeneity. Certain strategies also regulate the scheduling of client participation to avoid biasing the FL model towards classes that are more prevalent, as explored in the studies by <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2020</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite>. While these approaches have advanced the handling of data heterogeneity, they often do not fully address the specific issues related to long-tailed distributions in FL.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.06067/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="404" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The visulalization of our pretraining mechanism</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">A recent approach, CReFF <cite class="ltx_cite ltx_citemacro_citep">(Shang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, introduces a decoupling strategy to create balanced class-distribution federated features for the server model and to retrain the classifier with these features. Nonetheless, CReFF encounters two main limitations due to its reliance on generating federated features through client-side gradient information: 1) The one-to-many relationship between gradients and samples can result in the problem becoming ill-posed; 2) The absence of semantic guidance might lead to federated features that lack discriminative ability for their respective classes. The subsequent attempt, CLIP2FL <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>, seeks to overcome these drawbacks by integrating a multimodal model to direct the federated learning process. However, it still has its own drawbacks. Firstly, deploying the sizable CLIP model on devices increases memory and computational demands. Secondly, transmitting both the gradient and parameters of local models to the server, as necessitated by both CLIP4FL and CReFF, raises significant privacy concerns, as attackers could potentially reconstruct client images through reverse engineering <cite class="ltx_cite ltx_citemacro_citep">(Geiping etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019</a>; Haim etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In the conventional federated learning (FL) pipeline, the FL models are typically assigned to local clients for training on their heterogeneous datasets. These models are then sent back to the server for aggregation. This cycle continues repeatedly until the FL training concludes. To better align the above FL framework with practical requirements, we incorporate the following two additional stages. The first stage occurs before local training, involving pretraining on the server side, a strategy supported by previous work <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite> which found that pretraining can accelerate the convergence of FL training and mitigate the effects of data heterogeneity on convergence. The second stage takes place after aggregation, where the FL model may undergo further training to meet the broader requirements of FL companies, such as the performance and safety considerations we discuss later.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Drawing inspiration from recent developments in learning paradigms of natural language processing, we structure our work into three parts: global multimodal pretraining, federated local finetuning, and global alignment. We deploy multimodal approaches at the server side to assist both global multimodal pretraining and global alignment phases. In this section, we will introduce our comprehensive framework as follows: Section 3.1 will delve into the details of our global multimodal pretraining; Section 3.2 will cover federated local finetuning; Section 3.3 will discuss global alignment; and in Section 3.4, we will compare our method with previous approaches.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span> Global Multimodal Pretraining</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pretraining Dataset</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">As discussed in Section <a href="#S1" title="1. Introduction â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the wealth of open-source multimodal data, such as images and their captions, remains underutilized resources for pretraining FL models. Often, these datasets are noisy, unlabeled or contain elements that are too complex, making them unsuitable for straightforward pretraining of the compact FL models. However, given the current advanced capabilities in multimodal processing of MLLMs, we now have new, convenient methods to leverage such data for pretraining purposes. Utilizing GPT-4, akin to the approach used in LLaVA, we can transform complex image data collected from the internet into three main categories:</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Conversation: This category includes dialogues between an assistant and an individual seeking specific information about a photo. The assistantâ€™s responses simulate observing the image directly, answering a variety of questions about the visual content, such as identifying objects, counting them, describing actions, pinpointing locations, and noting their spatial relations.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Detailed Description: To gain a thorough understanding of an image, we formulated a series of questions designed to elicit detailed descriptions. Responses to these questions were generated using GPT-4, enriching our dataset with nuanced insights into the images.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Complex Reasoning: This category focuses on more sophisticated reasoning questions based on the content of the images. Answering these questions involves a detailed logical breakdown, reflecting a deep comprehension of the images and the ability to reason through them.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p3.1" class="ltx_p">Leveraging the aforementioned dataset formulations, we are equipped to facilitate the pretraining of FL models with the support of Multimodal Large Language Models.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pretraining Mechanism</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.11" class="ltx_p">Our pretraining mechanism draws inspiration from the structure of LLaVA, a highly effective and recent multimodal large language model. It consists of three key components: a visual encoder <math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">g</annotation></semantics></math>, which is a frozen pretrained CLIP model; a projection layer designed to align the features of the visual model with the text domain embeddings, where the projection layer is a trainable matrix <math id="S3.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">W</annotation></semantics></math>; and a part comprising a large language model (LLM), typically employing promising models such as Vicuna or LLaMA-2. The workflow of LLaVA proceeds as follows: For the data formats mentioned earlier, whether it be conversation, detailed description, or complex reasoning, the input includes a text modality instruction <math id="S3.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{X}_{\mathrm{q}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2">ğ—</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3">q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.1c">\mathbf{X}_{\mathrm{q}}</annotation></semantics></math> (e.g., â€Could you provide a detailed description of this image?â€) and an image <math id="S3.SS1.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{X}_{\mathrm{v}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2">ğ—</ci><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">\mathbf{X}_{\mathrm{v}}</annotation></semantics></math>. The instruction <math id="S3.SS1.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{X}_{\mathrm{q}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.2">ğ—</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.3">q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.1c">\mathbf{X}_{\mathrm{q}}</annotation></semantics></math> passes through an embedding layer to obtain the text embedding <math id="S3.SS1.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{H}_{q}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.cmml">ğ‡</mi><mi id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2">ğ‡</ci><ci id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.6.m6.1c">\mathbf{H}_{q}</annotation></semantics></math>. As for the image <math id="S3.SS1.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{X}_{\mathrm{v}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.7.m7.1a"><msub id="S3.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.2" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.2">ğ—</ci><ci id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.7.m7.1c">\mathbf{X}_{\mathrm{v}}</annotation></semantics></math>, it first goes through the visual encoder <math id="S3.SS1.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.8.m8.1a"><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.8.m8.1b"><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.8.m8.1c">g</annotation></semantics></math> to acquire the grid feature <math id="S3.SS1.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="\mathbf{Z}_{\mathrm{v}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.9.m9.1a"><msub id="S3.SS1.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2.cmml">ğ™</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.9.m9.1b"><apply id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2">ğ™</ci><ci id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.9.m9.1c">\mathbf{Z}_{\mathrm{v}}</annotation></semantics></math>, which then passes through the projection layer to obtain the visual embedding <math id="S3.SS1.SSS0.Px2.p1.10.m10.1" class="ltx_Math" alttext="\mathbf{H}_{v}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.10.m10.1a"><msub id="S3.SS1.SSS0.Px2.p1.10.m10.1.1" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.cmml">ğ‡</mi><mi id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2">ğ‡</ci><ci id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.10.m10.1c">\mathbf{H}_{v}</annotation></semantics></math>, aligned in dimension with <math id="S3.SS1.SSS0.Px2.p1.11.m11.1" class="ltx_Math" alttext="\mathbf{H}_{q}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.11.m11.1a"><msub id="S3.SS1.SSS0.Px2.p1.11.m11.1.1" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.cmml">ğ‡</mi><mi id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.11.m11.1b"><apply id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2">ğ‡</ci><ci id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.11.m11.1c">\mathbf{H}_{q}</annotation></semantics></math>:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="\mathbf{H}_{v}=\mathbf{W}\cdot\mathbf{Z}_{\mathrm{v}}\text{, where }\mathbf{Z}_{\mathrm{v}}=g\left(\mathbf{X}_{\mathrm{v}}\right)" display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">ğ‡</mi><mi id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml">v</mi></msub><mo id="S3.Ex1.m1.1.1.4" xref="S3.Ex1.m1.1.1.4.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.5" xref="S3.Ex1.m1.1.1.5.cmml"><mrow id="S3.Ex1.m1.1.1.5.2" xref="S3.Ex1.m1.1.1.5.2.cmml"><mi id="S3.Ex1.m1.1.1.5.2.2" xref="S3.Ex1.m1.1.1.5.2.2.cmml">ğ–</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.1.1.5.2.1" xref="S3.Ex1.m1.1.1.5.2.1.cmml">â‹…</mo><msub id="S3.Ex1.m1.1.1.5.2.3" xref="S3.Ex1.m1.1.1.5.2.3.cmml"><mi id="S3.Ex1.m1.1.1.5.2.3.2" xref="S3.Ex1.m1.1.1.5.2.3.2.cmml">ğ™</mi><mi mathvariant="normal" id="S3.Ex1.m1.1.1.5.2.3.3" xref="S3.Ex1.m1.1.1.5.2.3.3.cmml">v</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.5.1" xref="S3.Ex1.m1.1.1.5.1.cmml">â€‹</mo><mtext id="S3.Ex1.m1.1.1.5.3" xref="S3.Ex1.m1.1.1.5.3a.cmml">, whereÂ </mtext><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.5.1a" xref="S3.Ex1.m1.1.1.5.1.cmml">â€‹</mo><msub id="S3.Ex1.m1.1.1.5.4" xref="S3.Ex1.m1.1.1.5.4.cmml"><mi id="S3.Ex1.m1.1.1.5.4.2" xref="S3.Ex1.m1.1.1.5.4.2.cmml">ğ™</mi><mi mathvariant="normal" id="S3.Ex1.m1.1.1.5.4.3" xref="S3.Ex1.m1.1.1.5.4.3.cmml">v</mi></msub></mrow><mo id="S3.Ex1.m1.1.1.6" xref="S3.Ex1.m1.1.1.6.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><and id="S3.Ex1.m1.1.1a.cmml" xref="S3.Ex1.m1.1.1"></and><apply id="S3.Ex1.m1.1.1b.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.4"></eq><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">ğ‡</ci><ci id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3">ğ‘£</ci></apply><apply id="S3.Ex1.m1.1.1.5.cmml" xref="S3.Ex1.m1.1.1.5"><times id="S3.Ex1.m1.1.1.5.1.cmml" xref="S3.Ex1.m1.1.1.5.1"></times><apply id="S3.Ex1.m1.1.1.5.2.cmml" xref="S3.Ex1.m1.1.1.5.2"><ci id="S3.Ex1.m1.1.1.5.2.1.cmml" xref="S3.Ex1.m1.1.1.5.2.1">â‹…</ci><ci id="S3.Ex1.m1.1.1.5.2.2.cmml" xref="S3.Ex1.m1.1.1.5.2.2">ğ–</ci><apply id="S3.Ex1.m1.1.1.5.2.3.cmml" xref="S3.Ex1.m1.1.1.5.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.5.2.3.1.cmml" xref="S3.Ex1.m1.1.1.5.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.5.2.3.2.cmml" xref="S3.Ex1.m1.1.1.5.2.3.2">ğ™</ci><ci id="S3.Ex1.m1.1.1.5.2.3.3.cmml" xref="S3.Ex1.m1.1.1.5.2.3.3">v</ci></apply></apply><ci id="S3.Ex1.m1.1.1.5.3a.cmml" xref="S3.Ex1.m1.1.1.5.3"><mtext id="S3.Ex1.m1.1.1.5.3.cmml" xref="S3.Ex1.m1.1.1.5.3">, whereÂ </mtext></ci><apply id="S3.Ex1.m1.1.1.5.4.cmml" xref="S3.Ex1.m1.1.1.5.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.5.4.1.cmml" xref="S3.Ex1.m1.1.1.5.4">subscript</csymbol><ci id="S3.Ex1.m1.1.1.5.4.2.cmml" xref="S3.Ex1.m1.1.1.5.4.2">ğ™</ci><ci id="S3.Ex1.m1.1.1.5.4.3.cmml" xref="S3.Ex1.m1.1.1.5.4.3">v</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1c.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.6.cmml" xref="S3.Ex1.m1.1.1.6"></eq><share href="#S3.Ex1.m1.1.1.5.cmml" id="S3.Ex1.m1.1.1d.cmml" xref="S3.Ex1.m1.1.1"></share><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><times id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.2"></times><ci id="S3.Ex1.m1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.3">ğ‘”</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2">ğ—</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3">v</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathbf{H}_{v}=\mathbf{W}\cdot\mathbf{Z}_{\mathrm{v}}\text{, where }\mathbf{Z}_{\mathrm{v}}=g\left(\mathbf{X}_{\mathrm{v}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px2.p1.13" class="ltx_p">The text embedding <math id="S3.SS1.SSS0.Px2.p1.12.m1.1" class="ltx_Math" alttext="\mathbf{H}_{q}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.12.m1.1a"><msub id="S3.SS1.SSS0.Px2.p1.12.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.12.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1.2.cmml">ğ‡</mi><mi id="S3.SS1.SSS0.Px2.p1.12.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.12.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.12.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.12.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.12.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1.2">ğ‡</ci><ci id="S3.SS1.SSS0.Px2.p1.12.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.12.m1.1c">\mathbf{H}_{q}</annotation></semantics></math> and the visual embedding <math id="S3.SS1.SSS0.Px2.p1.13.m2.1" class="ltx_Math" alttext="\mathbf{H}_{v}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.13.m2.1a"><msub id="S3.SS1.SSS0.Px2.p1.13.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.13.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1.2.cmml">ğ‡</mi><mi id="S3.SS1.SSS0.Px2.p1.13.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.13.m2.1b"><apply id="S3.SS1.SSS0.Px2.p1.13.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.13.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.13.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1.2">ğ‡</ci><ci id="S3.SS1.SSS0.Px2.p1.13.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m2.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.13.m2.1c">\mathbf{H}_{v}</annotation></semantics></math> are concatenated and subsequently input into the LLM. The resulting output is the MLLMâ€™s response to the given inputs. Throughout the LLaVA training process, both the projector and the LLM are trainable, whereas the visual encoder and the CLIP model remain fixed and are not subject to training.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">Drawing from the LLaVA mechanism, our Global Multimodal Pretraining essentially integrates the compact FL model, designed for downstream image classification tasks, as a component within the LLaVA frameworkâ€™s visual encoder. The FL model is made trainable and is denoted as <math id="S3.SS1.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="g_{fl}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">g</mi><mrow id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2">ğ‘”</ci><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3"><times id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2">ğ‘“</ci><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.1.m1.1c">g_{fl}</annotation></semantics></math>. Inspried by the previous work in knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Hinton etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2015</a>; Mirzadeh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Jin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Park etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2023b</a>)</cite>, We have developed an approach termed Dynamic Weighted Distillation, which involves computing a weighted average of the visual features obtained from the FL model and those from the original visual encoder:</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.3" class="ltx_Math" alttext="\mathbf{Z}_{\mathrm{v}}=(1-\alpha)g\left(\mathbf{X}_{\mathrm{v}}\right)+\alpha g_{fl}\left(\mathbf{X}_{\mathrm{v}}\right)" display="block"><semantics id="S3.Ex2.m1.3a"><mrow id="S3.Ex2.m1.3.3" xref="S3.Ex2.m1.3.3.cmml"><msub id="S3.Ex2.m1.3.3.5" xref="S3.Ex2.m1.3.3.5.cmml"><mi id="S3.Ex2.m1.3.3.5.2" xref="S3.Ex2.m1.3.3.5.2.cmml">ğ™</mi><mi mathvariant="normal" id="S3.Ex2.m1.3.3.5.3" xref="S3.Ex2.m1.3.3.5.3.cmml">v</mi></msub><mo id="S3.Ex2.m1.3.3.4" xref="S3.Ex2.m1.3.3.4.cmml">=</mo><mrow id="S3.Ex2.m1.3.3.3" xref="S3.Ex2.m1.3.3.3.cmml"><mrow id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.cmml"><mrow id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.Ex2.m1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.Ex2.m1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.cmml">Î±</mi></mrow><mo stretchy="false" id="S3.Ex2.m1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.2.2.2.3" xref="S3.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S3.Ex2.m1.2.2.2.2.4" xref="S3.Ex2.m1.2.2.2.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.2.2.2.3a" xref="S3.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex2.m1.2.2.2.2.2.1" xref="S3.Ex2.m1.2.2.2.2.2.1.1.cmml"><mo id="S3.Ex2.m1.2.2.2.2.2.1.2" xref="S3.Ex2.m1.2.2.2.2.2.1.1.cmml">(</mo><msub id="S3.Ex2.m1.2.2.2.2.2.1.1" xref="S3.Ex2.m1.2.2.2.2.2.1.1.cmml"><mi id="S3.Ex2.m1.2.2.2.2.2.1.1.2" xref="S3.Ex2.m1.2.2.2.2.2.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.Ex2.m1.2.2.2.2.2.1.1.3" xref="S3.Ex2.m1.2.2.2.2.2.1.1.3.cmml">v</mi></msub><mo id="S3.Ex2.m1.2.2.2.2.2.1.3" xref="S3.Ex2.m1.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.3.3.3.4" xref="S3.Ex2.m1.3.3.3.4.cmml">+</mo><mrow id="S3.Ex2.m1.3.3.3.3" xref="S3.Ex2.m1.3.3.3.3.cmml"><mi id="S3.Ex2.m1.3.3.3.3.3" xref="S3.Ex2.m1.3.3.3.3.3.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.3.3.3.3.2" xref="S3.Ex2.m1.3.3.3.3.2.cmml">â€‹</mo><msub id="S3.Ex2.m1.3.3.3.3.4" xref="S3.Ex2.m1.3.3.3.3.4.cmml"><mi id="S3.Ex2.m1.3.3.3.3.4.2" xref="S3.Ex2.m1.3.3.3.3.4.2.cmml">g</mi><mrow id="S3.Ex2.m1.3.3.3.3.4.3" xref="S3.Ex2.m1.3.3.3.3.4.3.cmml"><mi id="S3.Ex2.m1.3.3.3.3.4.3.2" xref="S3.Ex2.m1.3.3.3.3.4.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.3.3.3.3.4.3.1" xref="S3.Ex2.m1.3.3.3.3.4.3.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.3.3.3.3.4.3.3" xref="S3.Ex2.m1.3.3.3.3.4.3.3.cmml">l</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.3.3.3.3.2a" xref="S3.Ex2.m1.3.3.3.3.2.cmml">â€‹</mo><mrow id="S3.Ex2.m1.3.3.3.3.1.1" xref="S3.Ex2.m1.3.3.3.3.1.1.1.cmml"><mo id="S3.Ex2.m1.3.3.3.3.1.1.2" xref="S3.Ex2.m1.3.3.3.3.1.1.1.cmml">(</mo><msub id="S3.Ex2.m1.3.3.3.3.1.1.1" xref="S3.Ex2.m1.3.3.3.3.1.1.1.cmml"><mi id="S3.Ex2.m1.3.3.3.3.1.1.1.2" xref="S3.Ex2.m1.3.3.3.3.1.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.Ex2.m1.3.3.3.3.1.1.1.3" xref="S3.Ex2.m1.3.3.3.3.1.1.1.3.cmml">v</mi></msub><mo id="S3.Ex2.m1.3.3.3.3.1.1.3" xref="S3.Ex2.m1.3.3.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.3b"><apply id="S3.Ex2.m1.3.3.cmml" xref="S3.Ex2.m1.3.3"><eq id="S3.Ex2.m1.3.3.4.cmml" xref="S3.Ex2.m1.3.3.4"></eq><apply id="S3.Ex2.m1.3.3.5.cmml" xref="S3.Ex2.m1.3.3.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.5.1.cmml" xref="S3.Ex2.m1.3.3.5">subscript</csymbol><ci id="S3.Ex2.m1.3.3.5.2.cmml" xref="S3.Ex2.m1.3.3.5.2">ğ™</ci><ci id="S3.Ex2.m1.3.3.5.3.cmml" xref="S3.Ex2.m1.3.3.5.3">v</ci></apply><apply id="S3.Ex2.m1.3.3.3.cmml" xref="S3.Ex2.m1.3.3.3"><plus id="S3.Ex2.m1.3.3.3.4.cmml" xref="S3.Ex2.m1.3.3.3.4"></plus><apply id="S3.Ex2.m1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2"><times id="S3.Ex2.m1.2.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.2.3"></times><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1"><minus id="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.Ex2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3">ğ›¼</ci></apply><ci id="S3.Ex2.m1.2.2.2.2.4.cmml" xref="S3.Ex2.m1.2.2.2.2.4">ğ‘”</ci><apply id="S3.Ex2.m1.2.2.2.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.2.2.2.1.1.1.cmml" xref="S3.Ex2.m1.2.2.2.2.2.1">subscript</csymbol><ci id="S3.Ex2.m1.2.2.2.2.2.1.1.2.cmml" xref="S3.Ex2.m1.2.2.2.2.2.1.1.2">ğ—</ci><ci id="S3.Ex2.m1.2.2.2.2.2.1.1.3.cmml" xref="S3.Ex2.m1.2.2.2.2.2.1.1.3">v</ci></apply></apply><apply id="S3.Ex2.m1.3.3.3.3.cmml" xref="S3.Ex2.m1.3.3.3.3"><times id="S3.Ex2.m1.3.3.3.3.2.cmml" xref="S3.Ex2.m1.3.3.3.3.2"></times><ci id="S3.Ex2.m1.3.3.3.3.3.cmml" xref="S3.Ex2.m1.3.3.3.3.3">ğ›¼</ci><apply id="S3.Ex2.m1.3.3.3.3.4.cmml" xref="S3.Ex2.m1.3.3.3.3.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.3.3.4.1.cmml" xref="S3.Ex2.m1.3.3.3.3.4">subscript</csymbol><ci id="S3.Ex2.m1.3.3.3.3.4.2.cmml" xref="S3.Ex2.m1.3.3.3.3.4.2">ğ‘”</ci><apply id="S3.Ex2.m1.3.3.3.3.4.3.cmml" xref="S3.Ex2.m1.3.3.3.3.4.3"><times id="S3.Ex2.m1.3.3.3.3.4.3.1.cmml" xref="S3.Ex2.m1.3.3.3.3.4.3.1"></times><ci id="S3.Ex2.m1.3.3.3.3.4.3.2.cmml" xref="S3.Ex2.m1.3.3.3.3.4.3.2">ğ‘“</ci><ci id="S3.Ex2.m1.3.3.3.3.4.3.3.cmml" xref="S3.Ex2.m1.3.3.3.3.4.3.3">ğ‘™</ci></apply></apply><apply id="S3.Ex2.m1.3.3.3.3.1.1.1.cmml" xref="S3.Ex2.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.3.3.1.1">subscript</csymbol><ci id="S3.Ex2.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.3.3.1.1.1.2">ğ—</ci><ci id="S3.Ex2.m1.3.3.3.3.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.3.3.1.1.1.3">v</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.3c">\mathbf{Z}_{\mathrm{v}}=(1-\alpha)g\left(\mathbf{X}_{\mathrm{v}}\right)+\alpha g_{fl}\left(\mathbf{X}_{\mathrm{v}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p3.8" class="ltx_p">In this equation, <math id="S3.SS1.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{Z}_{\mathrm{v}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.1.m1.1a"><msub id="S3.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.2.cmml">ğ™</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.2">ğ™</ci><ci id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.1.m1.1c">\mathbf{Z}_{\mathrm{v}}</annotation></semantics></math> represents the weighted combined visual features, <math id="S3.SS1.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="g\left(\mathbf{X}_{\mathrm{v}}\right)" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.2.m2.1a"><mrow id="S3.SS1.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.2.m2.1b"><apply id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1"><times id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.2"></times><ci id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.3">ğ‘”</ci><apply id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.2">ğ—</ci><ci id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.1.1.1.3">v</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.2.m2.1c">g\left(\mathbf{X}_{\mathrm{v}}\right)</annotation></semantics></math> indicates the visual features from the original visual encoder, <math id="S3.SS1.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="g_{fl}\left(\mathbf{X}_{\mathrm{v}}\right)" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.3.m3.1a"><mrow id="S3.SS1.SSS0.Px2.p3.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.cmml"><msub id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.2.cmml">g</mi><mrow id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.3.cmml">l</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.2.cmml">ğ—</mi><mi mathvariant="normal" id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1"><times id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.2"></times><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.2">ğ‘”</ci><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3"><times id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.2">ğ‘“</ci><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.3.3.3">ğ‘™</ci></apply></apply><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.2">ğ—</ci><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.1.3">v</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.3.m3.1c">g_{fl}\left(\mathbf{X}_{\mathrm{v}}\right)</annotation></semantics></math> refers to the visual features from the FL model, and <math id="S3.SS1.SSS0.Px2.p3.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.4.m4.1a"><mi id="S3.SS1.SSS0.Px2.p3.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p3.4.m4.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.4.m4.1b"><ci id="S3.SS1.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.4.m4.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.4.m4.1c">\alpha</annotation></semantics></math> is the dynamic weighting factor that adjusts the influence of each feature. During the pretraining phase, the CLIP model <math id="S3.SS1.SSS0.Px2.p3.5.m5.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.5.m5.1a"><mi id="S3.SS1.SSS0.Px2.p3.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p3.5.m5.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.5.m5.1b"><ci id="S3.SS1.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.5.m5.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.5.m5.1c">g</annotation></semantics></math>, the projector <math id="S3.SS1.SSS0.Px2.p3.6.m6.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.6.m6.1a"><mi id="S3.SS1.SSS0.Px2.p3.6.m6.1.1" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.6.m6.1b"><ci id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.6.m6.1c">W</annotation></semantics></math>, and the LLM are kept static, with only <math id="S3.SS1.SSS0.Px2.p3.7.m7.1" class="ltx_Math" alttext="g_{fl}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.7.m7.1a"><msub id="S3.SS1.SSS0.Px2.p3.7.m7.1.1" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2.cmml">g</mi><mrow id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.2" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.1" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.3" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.7.m7.1b"><apply id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2">ğ‘”</ci><apply id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3"><times id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.2">ğ‘“</ci><ci id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.7.m7.1c">g_{fl}</annotation></semantics></math>, the FL model component, being trainable. Initially, <math id="S3.SS1.SSS0.Px2.p3.8.m8.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.SSS0.Px2.p3.8.m8.1a"><mi id="S3.SS1.SSS0.Px2.p3.8.m8.1.1" xref="S3.SS1.SSS0.Px2.p3.8.m8.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.8.m8.1b"><ci id="S3.SS1.SSS0.Px2.p3.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.8.m8.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.8.m8.1c">\alpha</annotation></semantics></math> is set to 0, and as pretraining progresses, it gradually increases to 1, where it remains for the duration of the pretraining. This approach is termed Dynamic Weighted Pretraining. The rationale behind this strategy stems from the typically smaller size of the FL model compared to the original CLIP visual encoder. This size discrepancy is due to the constraints imposed by subsequent local training on edge devices within the FL framework. Directly substituting the large CLIP model with a more compact FL model could significantly hamper the process of multimodal alignment, owing to the vast difference in capacity between the two visual models resulting from their size difference.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Federated Finetuning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.6" class="ltx_p">In this subsection, we delve into the Federated Finetuning phase. Upon obtaining a pretrained FL model <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="g_{fl}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">g</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ‘”</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ğ‘“</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">g_{fl}</annotation></semantics></math> from the initial stage, we append classifier layers to it, tailoring the model for image classification tasks on local datasets on the client side. If we denote the set of all model parameters for the <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">k</annotation></semantics></math>-th client at the <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">t</annotation></semantics></math>-th local step as <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="w_{k}^{t}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msubsup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2.2" xref="S3.SS2.p1.4.m4.1.1.2.2.cmml">w</mi><mi id="S3.SS2.p1.4.m4.1.1.2.3" xref="S3.SS2.p1.4.m4.1.1.2.3.cmml">k</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2.2">ğ‘¤</ci><ci id="S3.SS2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.p1.4.m4.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">w_{k}^{t}</annotation></semantics></math>, and the local data as <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}^{k}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msup id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">k</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">ğ’Ÿ</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\mathcal{D}^{k}</annotation></semantics></math>, then the <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">k</annotation></semantics></math>-th client updates the received model in a manner similar to FedAvg:</p>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.2" class="ltx_Math" alttext="w_{k}^{t+1}\leftarrow w_{k}^{t}-\eta\nabla_{w}L_{\text{loc}}(w^{t};\mathcal{D}^{k})" display="block"><semantics id="S3.Ex3.m1.2a"><mrow id="S3.Ex3.m1.2.2" xref="S3.Ex3.m1.2.2.cmml"><msubsup id="S3.Ex3.m1.2.2.4" xref="S3.Ex3.m1.2.2.4.cmml"><mi id="S3.Ex3.m1.2.2.4.2.2" xref="S3.Ex3.m1.2.2.4.2.2.cmml">w</mi><mi id="S3.Ex3.m1.2.2.4.2.3" xref="S3.Ex3.m1.2.2.4.2.3.cmml">k</mi><mrow id="S3.Ex3.m1.2.2.4.3" xref="S3.Ex3.m1.2.2.4.3.cmml"><mi id="S3.Ex3.m1.2.2.4.3.2" xref="S3.Ex3.m1.2.2.4.3.2.cmml">t</mi><mo id="S3.Ex3.m1.2.2.4.3.1" xref="S3.Ex3.m1.2.2.4.3.1.cmml">+</mo><mn id="S3.Ex3.m1.2.2.4.3.3" xref="S3.Ex3.m1.2.2.4.3.3.cmml">1</mn></mrow></msubsup><mo stretchy="false" id="S3.Ex3.m1.2.2.3" xref="S3.Ex3.m1.2.2.3.cmml">â†</mo><mrow id="S3.Ex3.m1.2.2.2" xref="S3.Ex3.m1.2.2.2.cmml"><msubsup id="S3.Ex3.m1.2.2.2.4" xref="S3.Ex3.m1.2.2.2.4.cmml"><mi id="S3.Ex3.m1.2.2.2.4.2.2" xref="S3.Ex3.m1.2.2.2.4.2.2.cmml">w</mi><mi id="S3.Ex3.m1.2.2.2.4.2.3" xref="S3.Ex3.m1.2.2.2.4.2.3.cmml">k</mi><mi id="S3.Ex3.m1.2.2.2.4.3" xref="S3.Ex3.m1.2.2.2.4.3.cmml">t</mi></msubsup><mo id="S3.Ex3.m1.2.2.2.3" xref="S3.Ex3.m1.2.2.2.3.cmml">âˆ’</mo><mrow id="S3.Ex3.m1.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.cmml"><mi id="S3.Ex3.m1.2.2.2.2.4" xref="S3.Ex3.m1.2.2.2.2.4.cmml">Î·</mi><mo lspace="0.167em" rspace="0em" id="S3.Ex3.m1.2.2.2.2.3" xref="S3.Ex3.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex3.m1.2.2.2.2.5" xref="S3.Ex3.m1.2.2.2.2.5.cmml"><msub id="S3.Ex3.m1.2.2.2.2.5.1" xref="S3.Ex3.m1.2.2.2.2.5.1.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.2.2.2.2.5.1.2" xref="S3.Ex3.m1.2.2.2.2.5.1.2.cmml">âˆ‡</mo><mi id="S3.Ex3.m1.2.2.2.2.5.1.3" xref="S3.Ex3.m1.2.2.2.2.5.1.3.cmml">w</mi></msub><msub id="S3.Ex3.m1.2.2.2.2.5.2" xref="S3.Ex3.m1.2.2.2.2.5.2.cmml"><mi id="S3.Ex3.m1.2.2.2.2.5.2.2" xref="S3.Ex3.m1.2.2.2.2.5.2.2.cmml">L</mi><mtext id="S3.Ex3.m1.2.2.2.2.5.2.3" xref="S3.Ex3.m1.2.2.2.2.5.2.3a.cmml">loc</mtext></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.2.2.3a" xref="S3.Ex3.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex3.m1.2.2.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.Ex3.m1.2.2.2.2.2.2.3" xref="S3.Ex3.m1.2.2.2.2.2.3.cmml">(</mo><msup id="S3.Ex3.m1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml">w</mi><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msup><mo id="S3.Ex3.m1.2.2.2.2.2.2.4" xref="S3.Ex3.m1.2.2.2.2.2.3.cmml">;</mo><msup id="S3.Ex3.m1.2.2.2.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.2.2.2.2.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.2.2.2.2.cmml">ğ’Ÿ</mi><mi id="S3.Ex3.m1.2.2.2.2.2.2.2.3" xref="S3.Ex3.m1.2.2.2.2.2.2.2.3.cmml">k</mi></msup><mo stretchy="false" id="S3.Ex3.m1.2.2.2.2.2.2.5" xref="S3.Ex3.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.2b"><apply id="S3.Ex3.m1.2.2.cmml" xref="S3.Ex3.m1.2.2"><ci id="S3.Ex3.m1.2.2.3.cmml" xref="S3.Ex3.m1.2.2.3">â†</ci><apply id="S3.Ex3.m1.2.2.4.cmml" xref="S3.Ex3.m1.2.2.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.4.1.cmml" xref="S3.Ex3.m1.2.2.4">superscript</csymbol><apply id="S3.Ex3.m1.2.2.4.2.cmml" xref="S3.Ex3.m1.2.2.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.4.2.1.cmml" xref="S3.Ex3.m1.2.2.4">subscript</csymbol><ci id="S3.Ex3.m1.2.2.4.2.2.cmml" xref="S3.Ex3.m1.2.2.4.2.2">ğ‘¤</ci><ci id="S3.Ex3.m1.2.2.4.2.3.cmml" xref="S3.Ex3.m1.2.2.4.2.3">ğ‘˜</ci></apply><apply id="S3.Ex3.m1.2.2.4.3.cmml" xref="S3.Ex3.m1.2.2.4.3"><plus id="S3.Ex3.m1.2.2.4.3.1.cmml" xref="S3.Ex3.m1.2.2.4.3.1"></plus><ci id="S3.Ex3.m1.2.2.4.3.2.cmml" xref="S3.Ex3.m1.2.2.4.3.2">ğ‘¡</ci><cn type="integer" id="S3.Ex3.m1.2.2.4.3.3.cmml" xref="S3.Ex3.m1.2.2.4.3.3">1</cn></apply></apply><apply id="S3.Ex3.m1.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2"><minus id="S3.Ex3.m1.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.3"></minus><apply id="S3.Ex3.m1.2.2.2.4.cmml" xref="S3.Ex3.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.2.4.1.cmml" xref="S3.Ex3.m1.2.2.2.4">superscript</csymbol><apply id="S3.Ex3.m1.2.2.2.4.2.cmml" xref="S3.Ex3.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.2.4.2.1.cmml" xref="S3.Ex3.m1.2.2.2.4">subscript</csymbol><ci id="S3.Ex3.m1.2.2.2.4.2.2.cmml" xref="S3.Ex3.m1.2.2.2.4.2.2">ğ‘¤</ci><ci id="S3.Ex3.m1.2.2.2.4.2.3.cmml" xref="S3.Ex3.m1.2.2.2.4.2.3">ğ‘˜</ci></apply><ci id="S3.Ex3.m1.2.2.2.4.3.cmml" xref="S3.Ex3.m1.2.2.2.4.3">ğ‘¡</ci></apply><apply id="S3.Ex3.m1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2"><times id="S3.Ex3.m1.2.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.2.3"></times><ci id="S3.Ex3.m1.2.2.2.2.4.cmml" xref="S3.Ex3.m1.2.2.2.2.4">ğœ‚</ci><apply id="S3.Ex3.m1.2.2.2.2.5.cmml" xref="S3.Ex3.m1.2.2.2.2.5"><apply id="S3.Ex3.m1.2.2.2.2.5.1.cmml" xref="S3.Ex3.m1.2.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.2.2.5.1.1.cmml" xref="S3.Ex3.m1.2.2.2.2.5.1">subscript</csymbol><ci id="S3.Ex3.m1.2.2.2.2.5.1.2.cmml" xref="S3.Ex3.m1.2.2.2.2.5.1.2">âˆ‡</ci><ci id="S3.Ex3.m1.2.2.2.2.5.1.3.cmml" xref="S3.Ex3.m1.2.2.2.2.5.1.3">ğ‘¤</ci></apply><apply id="S3.Ex3.m1.2.2.2.2.5.2.cmml" xref="S3.Ex3.m1.2.2.2.2.5.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.2.2.5.2.1.cmml" xref="S3.Ex3.m1.2.2.2.2.5.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.2.2.5.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2.5.2.2">ğ¿</ci><ci id="S3.Ex3.m1.2.2.2.2.5.2.3a.cmml" xref="S3.Ex3.m1.2.2.2.2.5.2.3"><mtext mathsize="70%" id="S3.Ex3.m1.2.2.2.2.5.2.3.cmml" xref="S3.Ex3.m1.2.2.2.2.5.2.3">loc</mtext></ci></apply></apply><list id="S3.Ex3.m1.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.2.2.2"><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2">ğ‘¤</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.Ex3.m1.2.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.Ex3.m1.2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.Ex3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2.2.2.2.2">ğ’Ÿ</ci><ci id="S3.Ex3.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.2.2.2.2.3">ğ‘˜</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.2c">w_{k}^{t+1}\leftarrow w_{k}^{t}-\eta\nabla_{w}L_{\text{loc}}(w^{t};\mathcal{D}^{k})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.7" class="ltx_p">, where the <math id="S3.SS2.p1.7.m1.1" class="ltx_Math" alttext="L_{\text{loc}}" display="inline"><semantics id="S3.SS2.p1.7.m1.1a"><msub id="S3.SS2.p1.7.m1.1.1" xref="S3.SS2.p1.7.m1.1.1.cmml"><mi id="S3.SS2.p1.7.m1.1.1.2" xref="S3.SS2.p1.7.m1.1.1.2.cmml">L</mi><mtext id="S3.SS2.p1.7.m1.1.1.3" xref="S3.SS2.p1.7.m1.1.1.3a.cmml">loc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m1.1b"><apply id="S3.SS2.p1.7.m1.1.1.cmml" xref="S3.SS2.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m1.1.1.1.cmml" xref="S3.SS2.p1.7.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.7.m1.1.1.2.cmml" xref="S3.SS2.p1.7.m1.1.1.2">ğ¿</ci><ci id="S3.SS2.p1.7.m1.1.1.3a.cmml" xref="S3.SS2.p1.7.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.7.m1.1.1.3.cmml" xref="S3.SS2.p1.7.m1.1.1.3">loc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m1.1c">L_{\text{loc}}</annotation></semantics></math> represents the local loss function.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">After local training, the model parameters <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="w_{k}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">w</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ‘¤</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">w_{k}</annotation></semantics></math> are sent back to the server for global aggregation, where we also utilize FedAvg:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="w_{agg}^{t+1}=\sum_{k\in\Omega^{t}}\frac{\left|\mathcal{D}^{k}\right|}{\sum_{k\in\Omega^{t}}\left|\mathcal{D}^{k}\right|}w_{k}^{t+1}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><msubsup id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mi id="S3.E1.m1.2.3.2.2.2" xref="S3.E1.m1.2.3.2.2.2.cmml">w</mi><mrow id="S3.E1.m1.2.3.2.2.3" xref="S3.E1.m1.2.3.2.2.3.cmml"><mi id="S3.E1.m1.2.3.2.2.3.2" xref="S3.E1.m1.2.3.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.2.2.3.1" xref="S3.E1.m1.2.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.3.2.2.3.3" xref="S3.E1.m1.2.3.2.2.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.2.2.3.1a" xref="S3.E1.m1.2.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.3.2.2.3.4" xref="S3.E1.m1.2.3.2.2.3.4.cmml">g</mi></mrow><mrow id="S3.E1.m1.2.3.2.3" xref="S3.E1.m1.2.3.2.3.cmml"><mi id="S3.E1.m1.2.3.2.3.2" xref="S3.E1.m1.2.3.2.3.2.cmml">t</mi><mo id="S3.E1.m1.2.3.2.3.1" xref="S3.E1.m1.2.3.2.3.1.cmml">+</mo><mn id="S3.E1.m1.2.3.2.3.3" xref="S3.E1.m1.2.3.2.3.3.cmml">1</mn></mrow></msubsup><mo rspace="0.111em" id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><munder id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.2.3.3.1.2" xref="S3.E1.m1.2.3.3.1.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.2.3.3.1.3" xref="S3.E1.m1.2.3.3.1.3.cmml"><mi id="S3.E1.m1.2.3.3.1.3.2" xref="S3.E1.m1.2.3.3.1.3.2.cmml">k</mi><mo id="S3.E1.m1.2.3.3.1.3.1" xref="S3.E1.m1.2.3.3.1.3.1.cmml">âˆˆ</mo><msup id="S3.E1.m1.2.3.3.1.3.3" xref="S3.E1.m1.2.3.3.1.3.3.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.3.3.1.3.3.2" xref="S3.E1.m1.2.3.3.1.3.3.2.cmml">Î©</mi><mi id="S3.E1.m1.2.3.3.1.3.3.3" xref="S3.E1.m1.2.3.3.1.3.3.3.cmml">t</mi></msup></mrow></munder><mrow id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo><msup id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">k</mi></msup><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.3.2.cmml">k</mi><mo id="S3.E1.m1.2.2.2.2.3.1" xref="S3.E1.m1.2.2.2.2.3.1.cmml">âˆˆ</mo><msup id="S3.E1.m1.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.3.3.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.2.2.3.3.2" xref="S3.E1.m1.2.2.2.2.3.3.2.cmml">Î©</mi><mi id="S3.E1.m1.2.2.2.2.3.3.3" xref="S3.E1.m1.2.2.2.2.3.3.3.cmml">t</mi></msup></mrow></msub><mrow id="S3.E1.m1.2.2.2.1.1" xref="S3.E1.m1.2.2.2.1.2.cmml"><mo lspace="0em" id="S3.E1.m1.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.1.2.1.cmml">|</mo><msup id="S3.E1.m1.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.E1.m1.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.3.cmml">k</mi></msup><mo id="S3.E1.m1.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.1.2.1.cmml">|</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.2.1" xref="S3.E1.m1.2.3.3.2.1.cmml">â€‹</mo><msubsup id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml"><mi id="S3.E1.m1.2.3.3.2.2.2.2" xref="S3.E1.m1.2.3.3.2.2.2.2.cmml">w</mi><mi id="S3.E1.m1.2.3.3.2.2.2.3" xref="S3.E1.m1.2.3.3.2.2.2.3.cmml">k</mi><mrow id="S3.E1.m1.2.3.3.2.2.3" xref="S3.E1.m1.2.3.3.2.2.3.cmml"><mi id="S3.E1.m1.2.3.3.2.2.3.2" xref="S3.E1.m1.2.3.3.2.2.3.2.cmml">t</mi><mo id="S3.E1.m1.2.3.3.2.2.3.1" xref="S3.E1.m1.2.3.3.2.2.3.1.cmml">+</mo><mn id="S3.E1.m1.2.3.3.2.2.3.3" xref="S3.E1.m1.2.3.3.2.2.3.3.cmml">1</mn></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2">superscript</csymbol><apply id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.2.1.cmml" xref="S3.E1.m1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.3.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2.2">ğ‘¤</ci><apply id="S3.E1.m1.2.3.2.2.3.cmml" xref="S3.E1.m1.2.3.2.2.3"><times id="S3.E1.m1.2.3.2.2.3.1.cmml" xref="S3.E1.m1.2.3.2.2.3.1"></times><ci id="S3.E1.m1.2.3.2.2.3.2.cmml" xref="S3.E1.m1.2.3.2.2.3.2">ğ‘</ci><ci id="S3.E1.m1.2.3.2.2.3.3.cmml" xref="S3.E1.m1.2.3.2.2.3.3">ğ‘”</ci><ci id="S3.E1.m1.2.3.2.2.3.4.cmml" xref="S3.E1.m1.2.3.2.2.3.4">ğ‘”</ci></apply></apply><apply id="S3.E1.m1.2.3.2.3.cmml" xref="S3.E1.m1.2.3.2.3"><plus id="S3.E1.m1.2.3.2.3.1.cmml" xref="S3.E1.m1.2.3.2.3.1"></plus><ci id="S3.E1.m1.2.3.2.3.2.cmml" xref="S3.E1.m1.2.3.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.E1.m1.2.3.2.3.3.cmml" xref="S3.E1.m1.2.3.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><apply id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.1.1.cmml" xref="S3.E1.m1.2.3.3.1">subscript</csymbol><sum id="S3.E1.m1.2.3.3.1.2.cmml" xref="S3.E1.m1.2.3.3.1.2"></sum><apply id="S3.E1.m1.2.3.3.1.3.cmml" xref="S3.E1.m1.2.3.3.1.3"><in id="S3.E1.m1.2.3.3.1.3.1.cmml" xref="S3.E1.m1.2.3.3.1.3.1"></in><ci id="S3.E1.m1.2.3.3.1.3.2.cmml" xref="S3.E1.m1.2.3.3.1.3.2">ğ‘˜</ci><apply id="S3.E1.m1.2.3.3.1.3.3.cmml" xref="S3.E1.m1.2.3.3.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.1.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1.3.3">superscript</csymbol><ci id="S3.E1.m1.2.3.3.1.3.3.2.cmml" xref="S3.E1.m1.2.3.3.1.3.3.2">Î©</ci><ci id="S3.E1.m1.2.3.3.1.3.3.3.cmml" xref="S3.E1.m1.2.3.3.1.3.3.3">ğ‘¡</ci></apply></apply></apply><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><times id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2.1"></times><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">ğ’Ÿ</ci><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">ğ‘˜</ci></apply></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2">subscript</csymbol><sum id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"></sum><apply id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3"><in id="S3.E1.m1.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.3.1"></in><ci id="S3.E1.m1.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.3.2">ğ‘˜</ci><apply id="S3.E1.m1.2.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.3.3">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.3.3.2">Î©</ci><ci id="S3.E1.m1.2.2.2.2.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.3.3.3">ğ‘¡</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1"><abs id="S3.E1.m1.2.2.2.1.2.1.cmml" xref="S3.E1.m1.2.2.2.1.1.2"></abs><apply id="S3.E1.m1.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2">ğ’Ÿ</ci><ci id="S3.E1.m1.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.3">ğ‘˜</ci></apply></apply></apply></apply><apply id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.2.1.cmml" xref="S3.E1.m1.2.3.3.2.2">superscript</csymbol><apply id="S3.E1.m1.2.3.3.2.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.2.2.1.cmml" xref="S3.E1.m1.2.3.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.3.3.2.2.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2.2.2">ğ‘¤</ci><ci id="S3.E1.m1.2.3.3.2.2.2.3.cmml" xref="S3.E1.m1.2.3.3.2.2.2.3">ğ‘˜</ci></apply><apply id="S3.E1.m1.2.3.3.2.2.3.cmml" xref="S3.E1.m1.2.3.3.2.2.3"><plus id="S3.E1.m1.2.3.3.2.2.3.1.cmml" xref="S3.E1.m1.2.3.3.2.2.3.1"></plus><ci id="S3.E1.m1.2.3.3.2.2.3.2.cmml" xref="S3.E1.m1.2.3.3.2.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.E1.m1.2.3.3.2.2.3.3.cmml" xref="S3.E1.m1.2.3.3.2.2.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">w_{agg}^{t+1}=\sum_{k\in\Omega^{t}}\frac{\left|\mathcal{D}^{k}\right|}{\sum_{k\in\Omega^{t}}\left|\mathcal{D}^{k}\right|}w_{k}^{t+1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.3" class="ltx_p">, where the <math id="S3.SS2.p2.2.m1.1" class="ltx_Math" alttext="\Omega^{t}" display="inline"><semantics id="S3.SS2.p2.2.m1.1a"><msup id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p2.2.m1.1.1.2" xref="S3.SS2.p2.2.m1.1.1.2.cmml">Î©</mi><mi id="S3.SS2.p2.2.m1.1.1.3" xref="S3.SS2.p2.2.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.1b"><apply id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m1.1.1.2.cmml" xref="S3.SS2.p2.2.m1.1.1.2">Î©</ci><ci id="S3.SS2.p2.2.m1.1.1.3.cmml" xref="S3.SS2.p2.2.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.1c">\Omega^{t}</annotation></semantics></math> is the set of clients selected at the <math id="S3.SS2.p2.3.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p2.3.m2.1a"><mi id="S3.SS2.p2.3.m2.1.1" xref="S3.SS2.p2.3.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m2.1b"><ci id="S3.SS2.p2.3.m2.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">t</annotation></semantics></math>-th round.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">This stage mirrors the traditional FL framework closely. Drawing inspiration from learning paradigms in NLP, we refer to this phase as Federated Finetuning, in light of the pretraining conducted in the preceding step. Itâ€™s important to note the flexibility and compatibility of our framework; we can substitute FedAvg with any other existing FL methods designed to enhance local training and global aggregation, aiming to boost final utility performance metrics like accuracy, or system performance aspects like speed or computational efficiency. Furthermore, this phase does not cause additional privacy concerns, and existing methods for privacy or safety protection can be seamlessly integrated.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Global Alignment</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Recent studies on the alignment of large language models, such as Reinforcement Learning from Human Feedback (RLHF), are focused on refining the modelâ€™s outputs to more closely resonate with human-like understanding and reasoning. This enhancement significantly improves the modelâ€™s capability in tasks that require the interpretation and execution of complex instructions. In a similar vein, companies engaged in federated learning (FL) have analogous requirements for models after global aggregation. For instance, concerning safety requirements, an FL company must ensure that models trained via federated learning do not leak user information. Additionally, there are performance-related requirements, such as adjusting the model to prevent biases caused by long-tailed distributions or training the model on new datasets to acquire new skills. Typically, this involves constructing an alignment dataset <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{align}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.1" xref="S3.SS3.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.1.m1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.1a" xref="S3.SS3.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.1.m1.1.1.3.4" xref="S3.SS3.p1.1.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.1b" xref="S3.SS3.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.1.m1.1.1.3.5" xref="S3.SS3.p1.1.m1.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.1c" xref="S3.SS3.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.1.m1.1.1.3.6" xref="S3.SS3.p1.1.m1.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ’Ÿ</ci><apply id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><times id="S3.SS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.1"></times><ci id="S3.SS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p1.1.m1.1.1.3.4.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p1.1.m1.1.1.3.5.cmml" xref="S3.SS3.p1.1.m1.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p1.1.m1.1.1.3.6.cmml" xref="S3.SS3.p1.1.m1.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathcal{D}_{align}</annotation></semantics></math> and selecting a suitable global alignment function <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="L_{align}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">L</mi><mrow id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.1" xref="S3.SS3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.2.m2.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.1a" xref="S3.SS3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.2.m2.1.1.3.4" xref="S3.SS3.p1.2.m2.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.1b" xref="S3.SS3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.2.m2.1.1.3.5" xref="S3.SS3.p1.2.m2.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.1c" xref="S3.SS3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.2.m2.1.1.3.6" xref="S3.SS3.p1.2.m2.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">ğ¿</ci><apply id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><times id="S3.SS3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.1"></times><ci id="S3.SS3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p1.2.m2.1.1.3.4.cmml" xref="S3.SS3.p1.2.m2.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p1.2.m2.1.1.3.5.cmml" xref="S3.SS3.p1.2.m2.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p1.2.m2.1.1.3.6.cmml" xref="S3.SS3.p1.2.m2.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">L_{align}</annotation></semantics></math>:</p>
<table id="S3.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex4.m1.2" class="ltx_Math" alttext="w_{agg}^{new}\leftarrow w_{agg}-\eta\nabla_{w}L_{align}(w_{agg};\mathcal{D}_{align})" display="block"><semantics id="S3.Ex4.m1.2a"><mrow id="S3.Ex4.m1.2.2" xref="S3.Ex4.m1.2.2.cmml"><msubsup id="S3.Ex4.m1.2.2.4" xref="S3.Ex4.m1.2.2.4.cmml"><mi id="S3.Ex4.m1.2.2.4.2.2" xref="S3.Ex4.m1.2.2.4.2.2.cmml">w</mi><mrow id="S3.Ex4.m1.2.2.4.2.3" xref="S3.Ex4.m1.2.2.4.2.3.cmml"><mi id="S3.Ex4.m1.2.2.4.2.3.2" xref="S3.Ex4.m1.2.2.4.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.4.2.3.1" xref="S3.Ex4.m1.2.2.4.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.4.2.3.3" xref="S3.Ex4.m1.2.2.4.2.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.4.2.3.1a" xref="S3.Ex4.m1.2.2.4.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.4.2.3.4" xref="S3.Ex4.m1.2.2.4.2.3.4.cmml">g</mi></mrow><mrow id="S3.Ex4.m1.2.2.4.3" xref="S3.Ex4.m1.2.2.4.3.cmml"><mi id="S3.Ex4.m1.2.2.4.3.2" xref="S3.Ex4.m1.2.2.4.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.4.3.1" xref="S3.Ex4.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.4.3.3" xref="S3.Ex4.m1.2.2.4.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.4.3.1a" xref="S3.Ex4.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.4.3.4" xref="S3.Ex4.m1.2.2.4.3.4.cmml">w</mi></mrow></msubsup><mo stretchy="false" id="S3.Ex4.m1.2.2.3" xref="S3.Ex4.m1.2.2.3.cmml">â†</mo><mrow id="S3.Ex4.m1.2.2.2" xref="S3.Ex4.m1.2.2.2.cmml"><msub id="S3.Ex4.m1.2.2.2.4" xref="S3.Ex4.m1.2.2.2.4.cmml"><mi id="S3.Ex4.m1.2.2.2.4.2" xref="S3.Ex4.m1.2.2.2.4.2.cmml">w</mi><mrow id="S3.Ex4.m1.2.2.2.4.3" xref="S3.Ex4.m1.2.2.2.4.3.cmml"><mi id="S3.Ex4.m1.2.2.2.4.3.2" xref="S3.Ex4.m1.2.2.2.4.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.4.3.1" xref="S3.Ex4.m1.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.4.3.3" xref="S3.Ex4.m1.2.2.2.4.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.4.3.1a" xref="S3.Ex4.m1.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.4.3.4" xref="S3.Ex4.m1.2.2.2.4.3.4.cmml">g</mi></mrow></msub><mo id="S3.Ex4.m1.2.2.2.3" xref="S3.Ex4.m1.2.2.2.3.cmml">âˆ’</mo><mrow id="S3.Ex4.m1.2.2.2.2" xref="S3.Ex4.m1.2.2.2.2.cmml"><mi id="S3.Ex4.m1.2.2.2.2.4" xref="S3.Ex4.m1.2.2.2.2.4.cmml">Î·</mi><mo lspace="0.167em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.3" xref="S3.Ex4.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex4.m1.2.2.2.2.5" xref="S3.Ex4.m1.2.2.2.2.5.cmml"><msub id="S3.Ex4.m1.2.2.2.2.5.1" xref="S3.Ex4.m1.2.2.2.2.5.1.cmml"><mo rspace="0.167em" id="S3.Ex4.m1.2.2.2.2.5.1.2" xref="S3.Ex4.m1.2.2.2.2.5.1.2.cmml">âˆ‡</mo><mi id="S3.Ex4.m1.2.2.2.2.5.1.3" xref="S3.Ex4.m1.2.2.2.2.5.1.3.cmml">w</mi></msub><msub id="S3.Ex4.m1.2.2.2.2.5.2" xref="S3.Ex4.m1.2.2.2.2.5.2.cmml"><mi id="S3.Ex4.m1.2.2.2.2.5.2.2" xref="S3.Ex4.m1.2.2.2.2.5.2.2.cmml">L</mi><mrow id="S3.Ex4.m1.2.2.2.2.5.2.3" xref="S3.Ex4.m1.2.2.2.2.5.2.3.cmml"><mi id="S3.Ex4.m1.2.2.2.2.5.2.3.2" xref="S3.Ex4.m1.2.2.2.2.5.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.5.2.3.1" xref="S3.Ex4.m1.2.2.2.2.5.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.5.2.3.3" xref="S3.Ex4.m1.2.2.2.2.5.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.5.2.3.1a" xref="S3.Ex4.m1.2.2.2.2.5.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.5.2.3.4" xref="S3.Ex4.m1.2.2.2.2.5.2.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.5.2.3.1b" xref="S3.Ex4.m1.2.2.2.2.5.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.5.2.3.5" xref="S3.Ex4.m1.2.2.2.2.5.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.5.2.3.1c" xref="S3.Ex4.m1.2.2.2.2.5.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.5.2.3.6" xref="S3.Ex4.m1.2.2.2.2.5.2.3.6.cmml">n</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.3a" xref="S3.Ex4.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex4.m1.2.2.2.2.2.2" xref="S3.Ex4.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.Ex4.m1.2.2.2.2.2.2.3" xref="S3.Ex4.m1.2.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex4.m1.1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.1.1.2.cmml">w</mi><mrow id="S3.Ex4.m1.1.1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.3.2" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.1.1.3.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.3.3" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.1.1.3.1a" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.3.4" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.4.cmml">g</mi></mrow></msub><mo id="S3.Ex4.m1.2.2.2.2.2.2.4" xref="S3.Ex4.m1.2.2.2.2.2.3.cmml">;</mo><msub id="S3.Ex4.m1.2.2.2.2.2.2.2" xref="S3.Ex4.m1.2.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex4.m1.2.2.2.2.2.2.2.2" xref="S3.Ex4.m1.2.2.2.2.2.2.2.2.cmml">ğ’Ÿ</mi><mrow id="S3.Ex4.m1.2.2.2.2.2.2.2.3" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.cmml"><mi id="S3.Ex4.m1.2.2.2.2.2.2.2.3.2" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.2.2.2.3.1" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.2.2.2.3.3" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.2.2.2.3.1a" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.2.2.2.3.4" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.2.2.2.3.1b" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.2.2.2.3.5" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.2.2.2.3.1c" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.2.2.2.2.2.2.2.3.6" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.6.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.Ex4.m1.2.2.2.2.2.2.5" xref="S3.Ex4.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.2b"><apply id="S3.Ex4.m1.2.2.cmml" xref="S3.Ex4.m1.2.2"><ci id="S3.Ex4.m1.2.2.3.cmml" xref="S3.Ex4.m1.2.2.3">â†</ci><apply id="S3.Ex4.m1.2.2.4.cmml" xref="S3.Ex4.m1.2.2.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.4.1.cmml" xref="S3.Ex4.m1.2.2.4">superscript</csymbol><apply id="S3.Ex4.m1.2.2.4.2.cmml" xref="S3.Ex4.m1.2.2.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.4.2.1.cmml" xref="S3.Ex4.m1.2.2.4">subscript</csymbol><ci id="S3.Ex4.m1.2.2.4.2.2.cmml" xref="S3.Ex4.m1.2.2.4.2.2">ğ‘¤</ci><apply id="S3.Ex4.m1.2.2.4.2.3.cmml" xref="S3.Ex4.m1.2.2.4.2.3"><times id="S3.Ex4.m1.2.2.4.2.3.1.cmml" xref="S3.Ex4.m1.2.2.4.2.3.1"></times><ci id="S3.Ex4.m1.2.2.4.2.3.2.cmml" xref="S3.Ex4.m1.2.2.4.2.3.2">ğ‘</ci><ci id="S3.Ex4.m1.2.2.4.2.3.3.cmml" xref="S3.Ex4.m1.2.2.4.2.3.3">ğ‘”</ci><ci id="S3.Ex4.m1.2.2.4.2.3.4.cmml" xref="S3.Ex4.m1.2.2.4.2.3.4">ğ‘”</ci></apply></apply><apply id="S3.Ex4.m1.2.2.4.3.cmml" xref="S3.Ex4.m1.2.2.4.3"><times id="S3.Ex4.m1.2.2.4.3.1.cmml" xref="S3.Ex4.m1.2.2.4.3.1"></times><ci id="S3.Ex4.m1.2.2.4.3.2.cmml" xref="S3.Ex4.m1.2.2.4.3.2">ğ‘›</ci><ci id="S3.Ex4.m1.2.2.4.3.3.cmml" xref="S3.Ex4.m1.2.2.4.3.3">ğ‘’</ci><ci id="S3.Ex4.m1.2.2.4.3.4.cmml" xref="S3.Ex4.m1.2.2.4.3.4">ğ‘¤</ci></apply></apply><apply id="S3.Ex4.m1.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2"><minus id="S3.Ex4.m1.2.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.3"></minus><apply id="S3.Ex4.m1.2.2.2.4.cmml" xref="S3.Ex4.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.4.1.cmml" xref="S3.Ex4.m1.2.2.2.4">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.4.2.cmml" xref="S3.Ex4.m1.2.2.2.4.2">ğ‘¤</ci><apply id="S3.Ex4.m1.2.2.2.4.3.cmml" xref="S3.Ex4.m1.2.2.2.4.3"><times id="S3.Ex4.m1.2.2.2.4.3.1.cmml" xref="S3.Ex4.m1.2.2.2.4.3.1"></times><ci id="S3.Ex4.m1.2.2.2.4.3.2.cmml" xref="S3.Ex4.m1.2.2.2.4.3.2">ğ‘</ci><ci id="S3.Ex4.m1.2.2.2.4.3.3.cmml" xref="S3.Ex4.m1.2.2.2.4.3.3">ğ‘”</ci><ci id="S3.Ex4.m1.2.2.2.4.3.4.cmml" xref="S3.Ex4.m1.2.2.2.4.3.4">ğ‘”</ci></apply></apply><apply id="S3.Ex4.m1.2.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2"><times id="S3.Ex4.m1.2.2.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.3"></times><ci id="S3.Ex4.m1.2.2.2.2.4.cmml" xref="S3.Ex4.m1.2.2.2.2.4">ğœ‚</ci><apply id="S3.Ex4.m1.2.2.2.2.5.cmml" xref="S3.Ex4.m1.2.2.2.2.5"><apply id="S3.Ex4.m1.2.2.2.2.5.1.cmml" xref="S3.Ex4.m1.2.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.2.5.1.1.cmml" xref="S3.Ex4.m1.2.2.2.2.5.1">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.2.5.1.2.cmml" xref="S3.Ex4.m1.2.2.2.2.5.1.2">âˆ‡</ci><ci id="S3.Ex4.m1.2.2.2.2.5.1.3.cmml" xref="S3.Ex4.m1.2.2.2.2.5.1.3">ğ‘¤</ci></apply><apply id="S3.Ex4.m1.2.2.2.2.5.2.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.2.5.2.1.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.2.5.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.2">ğ¿</ci><apply id="S3.Ex4.m1.2.2.2.2.5.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3"><times id="S3.Ex4.m1.2.2.2.2.5.2.3.1.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3.1"></times><ci id="S3.Ex4.m1.2.2.2.2.5.2.3.2.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3.2">ğ‘</ci><ci id="S3.Ex4.m1.2.2.2.2.5.2.3.3.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3.3">ğ‘™</ci><ci id="S3.Ex4.m1.2.2.2.2.5.2.3.4.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3.4">ğ‘–</ci><ci id="S3.Ex4.m1.2.2.2.2.5.2.3.5.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3.5">ğ‘”</ci><ci id="S3.Ex4.m1.2.2.2.2.5.2.3.6.cmml" xref="S3.Ex4.m1.2.2.2.2.5.2.3.6">ğ‘›</ci></apply></apply></apply><list id="S3.Ex4.m1.2.2.2.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2"><apply id="S3.Ex4.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.2">ğ‘¤</ci><apply id="S3.Ex4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3"><times id="S3.Ex4.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.3">ğ‘”</ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.4">ğ‘”</ci></apply></apply><apply id="S3.Ex4.m1.2.2.2.2.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.2">ğ’Ÿ</ci><apply id="S3.Ex4.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3"><times id="S3.Ex4.m1.2.2.2.2.2.2.2.3.1.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.1"></times><ci id="S3.Ex4.m1.2.2.2.2.2.2.2.3.2.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.2">ğ‘</ci><ci id="S3.Ex4.m1.2.2.2.2.2.2.2.3.3.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.3">ğ‘™</ci><ci id="S3.Ex4.m1.2.2.2.2.2.2.2.3.4.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.4">ğ‘–</ci><ci id="S3.Ex4.m1.2.2.2.2.2.2.2.3.5.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.5">ğ‘”</ci><ci id="S3.Ex4.m1.2.2.2.2.2.2.2.3.6.cmml" xref="S3.Ex4.m1.2.2.2.2.2.2.2.3.6">ğ‘›</ci></apply></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.2c">w_{agg}^{new}\leftarrow w_{agg}-\eta\nabla_{w}L_{align}(w_{agg};\mathcal{D}_{align})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">To address the issue of long-tailed distributions, one could design <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{align}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1a" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3.4" xref="S3.SS3.p2.1.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1b" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3.5" xref="S3.SS3.p2.1.m1.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1c" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3.6" xref="S3.SS3.p2.1.m1.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ’Ÿ</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><times id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p2.1.m1.1.1.3.4.cmml" xref="S3.SS3.p2.1.m1.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p2.1.m1.1.1.3.5.cmml" xref="S3.SS3.p2.1.m1.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p2.1.m1.1.1.3.6.cmml" xref="S3.SS3.p2.1.m1.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{D}_{align}</annotation></semantics></math> as a small, class-balanced dataset encompassing all categories, with <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="L_{align}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">L</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1a" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.4" xref="S3.SS3.p2.2.m2.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1b" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.5" xref="S3.SS3.p2.2.m2.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1c" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.6" xref="S3.SS3.p2.2.m2.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ğ¿</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><times id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p2.2.m2.1.1.3.4.cmml" xref="S3.SS3.p2.2.m2.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p2.2.m2.1.1.3.5.cmml" xref="S3.SS3.p2.2.m2.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p2.2.m2.1.1.3.6.cmml" xref="S3.SS3.p2.2.m2.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">L_{align}</annotation></semantics></math> defined as follows:</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.3" class="ltx_Math" alttext="L_{align}=L_{ce}\left(y,p\right)+\beta\cdot KL\left(q\|p\right)," display="block"><semantics id="S3.Ex5.m1.3a"><mrow id="S3.Ex5.m1.3.3.1" xref="S3.Ex5.m1.3.3.1.1.cmml"><mrow id="S3.Ex5.m1.3.3.1.1" xref="S3.Ex5.m1.3.3.1.1.cmml"><msub id="S3.Ex5.m1.3.3.1.1.3" xref="S3.Ex5.m1.3.3.1.1.3.cmml"><mi id="S3.Ex5.m1.3.3.1.1.3.2" xref="S3.Ex5.m1.3.3.1.1.3.2.cmml">L</mi><mrow id="S3.Ex5.m1.3.3.1.1.3.3" xref="S3.Ex5.m1.3.3.1.1.3.3.cmml"><mi id="S3.Ex5.m1.3.3.1.1.3.3.2" xref="S3.Ex5.m1.3.3.1.1.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.3.3.1" xref="S3.Ex5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex5.m1.3.3.1.1.3.3.3" xref="S3.Ex5.m1.3.3.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.3.3.1a" xref="S3.Ex5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex5.m1.3.3.1.1.3.3.4" xref="S3.Ex5.m1.3.3.1.1.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.3.3.1b" xref="S3.Ex5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex5.m1.3.3.1.1.3.3.5" xref="S3.Ex5.m1.3.3.1.1.3.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.3.3.1c" xref="S3.Ex5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex5.m1.3.3.1.1.3.3.6" xref="S3.Ex5.m1.3.3.1.1.3.3.6.cmml">n</mi></mrow></msub><mo id="S3.Ex5.m1.3.3.1.1.2" xref="S3.Ex5.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.Ex5.m1.3.3.1.1.1" xref="S3.Ex5.m1.3.3.1.1.1.cmml"><mrow id="S3.Ex5.m1.3.3.1.1.1.3" xref="S3.Ex5.m1.3.3.1.1.1.3.cmml"><msub id="S3.Ex5.m1.3.3.1.1.1.3.2" xref="S3.Ex5.m1.3.3.1.1.1.3.2.cmml"><mi id="S3.Ex5.m1.3.3.1.1.1.3.2.2" xref="S3.Ex5.m1.3.3.1.1.1.3.2.2.cmml">L</mi><mrow id="S3.Ex5.m1.3.3.1.1.1.3.2.3" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.cmml"><mi id="S3.Ex5.m1.3.3.1.1.1.3.2.3.2" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.1.3.2.3.1" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.Ex5.m1.3.3.1.1.1.3.2.3.3" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.3.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.1.3.1" xref="S3.Ex5.m1.3.3.1.1.1.3.1.cmml">â€‹</mo><mrow id="S3.Ex5.m1.3.3.1.1.1.3.3.2" xref="S3.Ex5.m1.3.3.1.1.1.3.3.1.cmml"><mo id="S3.Ex5.m1.3.3.1.1.1.3.3.2.1" xref="S3.Ex5.m1.3.3.1.1.1.3.3.1.cmml">(</mo><mi id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml">y</mi><mo id="S3.Ex5.m1.3.3.1.1.1.3.3.2.2" xref="S3.Ex5.m1.3.3.1.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex5.m1.2.2" xref="S3.Ex5.m1.2.2.cmml">p</mi><mo id="S3.Ex5.m1.3.3.1.1.1.3.3.2.3" xref="S3.Ex5.m1.3.3.1.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex5.m1.3.3.1.1.1.2" xref="S3.Ex5.m1.3.3.1.1.1.2.cmml">+</mo><mrow id="S3.Ex5.m1.3.3.1.1.1.1" xref="S3.Ex5.m1.3.3.1.1.1.1.cmml"><mrow id="S3.Ex5.m1.3.3.1.1.1.1.3" xref="S3.Ex5.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.Ex5.m1.3.3.1.1.1.1.3.2" xref="S3.Ex5.m1.3.3.1.1.1.1.3.2.cmml">Î²</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex5.m1.3.3.1.1.1.1.3.1" xref="S3.Ex5.m1.3.3.1.1.1.1.3.1.cmml">â‹…</mo><mi id="S3.Ex5.m1.3.3.1.1.1.1.3.3" xref="S3.Ex5.m1.3.3.1.1.1.1.3.3.cmml">K</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.1.1.2" xref="S3.Ex5.m1.3.3.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.Ex5.m1.3.3.1.1.1.1.4" xref="S3.Ex5.m1.3.3.1.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.1.1.1.1.2a" xref="S3.Ex5.m1.3.3.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex5.m1.3.3.1.1.1.1.1.1" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex5.m1.3.3.1.1.1.1.1.1.2" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.2.cmml">q</mi><mo id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.1.cmml">âˆ¥</mo><mi id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.3.cmml">p</mi></mrow><mo id="S3.Ex5.m1.3.3.1.1.1.1.1.1.3" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex5.m1.3.3.1.2" xref="S3.Ex5.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.3b"><apply id="S3.Ex5.m1.3.3.1.1.cmml" xref="S3.Ex5.m1.3.3.1"><eq id="S3.Ex5.m1.3.3.1.1.2.cmml" xref="S3.Ex5.m1.3.3.1.1.2"></eq><apply id="S3.Ex5.m1.3.3.1.1.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.1.1.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.Ex5.m1.3.3.1.1.3.2.cmml" xref="S3.Ex5.m1.3.3.1.1.3.2">ğ¿</ci><apply id="S3.Ex5.m1.3.3.1.1.3.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3"><times id="S3.Ex5.m1.3.3.1.1.3.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.1"></times><ci id="S3.Ex5.m1.3.3.1.1.3.3.2.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.2">ğ‘</ci><ci id="S3.Ex5.m1.3.3.1.1.3.3.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.3">ğ‘™</ci><ci id="S3.Ex5.m1.3.3.1.1.3.3.4.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.4">ğ‘–</ci><ci id="S3.Ex5.m1.3.3.1.1.3.3.5.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.5">ğ‘”</ci><ci id="S3.Ex5.m1.3.3.1.1.3.3.6.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.6">ğ‘›</ci></apply></apply><apply id="S3.Ex5.m1.3.3.1.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1"><plus id="S3.Ex5.m1.3.3.1.1.1.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.2"></plus><apply id="S3.Ex5.m1.3.3.1.1.1.3.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3"><times id="S3.Ex5.m1.3.3.1.1.1.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.1"></times><apply id="S3.Ex5.m1.3.3.1.1.1.3.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.1.1.1.3.2.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2">subscript</csymbol><ci id="S3.Ex5.m1.3.3.1.1.1.3.2.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2.2">ğ¿</ci><apply id="S3.Ex5.m1.3.3.1.1.1.3.2.3.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3"><times id="S3.Ex5.m1.3.3.1.1.1.3.2.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.1"></times><ci id="S3.Ex5.m1.3.3.1.1.1.3.2.3.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.2">ğ‘</ci><ci id="S3.Ex5.m1.3.3.1.1.1.3.2.3.3.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.2.3.3">ğ‘’</ci></apply></apply><interval closure="open" id="S3.Ex5.m1.3.3.1.1.1.3.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.3.3.2"><ci id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1">ğ‘¦</ci><ci id="S3.Ex5.m1.2.2.cmml" xref="S3.Ex5.m1.2.2">ğ‘</ci></interval></apply><apply id="S3.Ex5.m1.3.3.1.1.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1"><times id="S3.Ex5.m1.3.3.1.1.1.1.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.2"></times><apply id="S3.Ex5.m1.3.3.1.1.1.1.3.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.3"><ci id="S3.Ex5.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.3.1">â‹…</ci><ci id="S3.Ex5.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.3.2">ğ›½</ci><ci id="S3.Ex5.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.3.3">ğ¾</ci></apply><ci id="S3.Ex5.m1.3.3.1.1.1.1.4.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.4">ğ¿</ci><apply id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.3.3.1.1.1.1.1.1.1.3">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.3c">L_{align}=L_{ce}\left(y,p\right)+\beta\cdot KL\left(q\|p\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.8" class="ltx_p">where <math id="S3.SS3.p2.3.m1.2" class="ltx_Math" alttext="L_{ce}(\cdot,\cdot)" display="inline"><semantics id="S3.SS3.p2.3.m1.2a"><mrow id="S3.SS3.p2.3.m1.2.3" xref="S3.SS3.p2.3.m1.2.3.cmml"><msub id="S3.SS3.p2.3.m1.2.3.2" xref="S3.SS3.p2.3.m1.2.3.2.cmml"><mi id="S3.SS3.p2.3.m1.2.3.2.2" xref="S3.SS3.p2.3.m1.2.3.2.2.cmml">L</mi><mrow id="S3.SS3.p2.3.m1.2.3.2.3" xref="S3.SS3.p2.3.m1.2.3.2.3.cmml"><mi id="S3.SS3.p2.3.m1.2.3.2.3.2" xref="S3.SS3.p2.3.m1.2.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m1.2.3.2.3.1" xref="S3.SS3.p2.3.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.3.m1.2.3.2.3.3" xref="S3.SS3.p2.3.m1.2.3.2.3.3.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m1.2.3.1" xref="S3.SS3.p2.3.m1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS3.p2.3.m1.2.3.3.2" xref="S3.SS3.p2.3.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m1.2.3.3.2.1" xref="S3.SS3.p2.3.m1.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m1.1.1" xref="S3.SS3.p2.3.m1.1.1.cmml">â‹…</mo><mo rspace="0em" id="S3.SS3.p2.3.m1.2.3.3.2.2" xref="S3.SS3.p2.3.m1.2.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m1.2.2" xref="S3.SS3.p2.3.m1.2.2.cmml">â‹…</mo><mo stretchy="false" id="S3.SS3.p2.3.m1.2.3.3.2.3" xref="S3.SS3.p2.3.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m1.2b"><apply id="S3.SS3.p2.3.m1.2.3.cmml" xref="S3.SS3.p2.3.m1.2.3"><times id="S3.SS3.p2.3.m1.2.3.1.cmml" xref="S3.SS3.p2.3.m1.2.3.1"></times><apply id="S3.SS3.p2.3.m1.2.3.2.cmml" xref="S3.SS3.p2.3.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m1.2.3.2.1.cmml" xref="S3.SS3.p2.3.m1.2.3.2">subscript</csymbol><ci id="S3.SS3.p2.3.m1.2.3.2.2.cmml" xref="S3.SS3.p2.3.m1.2.3.2.2">ğ¿</ci><apply id="S3.SS3.p2.3.m1.2.3.2.3.cmml" xref="S3.SS3.p2.3.m1.2.3.2.3"><times id="S3.SS3.p2.3.m1.2.3.2.3.1.cmml" xref="S3.SS3.p2.3.m1.2.3.2.3.1"></times><ci id="S3.SS3.p2.3.m1.2.3.2.3.2.cmml" xref="S3.SS3.p2.3.m1.2.3.2.3.2">ğ‘</ci><ci id="S3.SS3.p2.3.m1.2.3.2.3.3.cmml" xref="S3.SS3.p2.3.m1.2.3.2.3.3">ğ‘’</ci></apply></apply><interval closure="open" id="S3.SS3.p2.3.m1.2.3.3.1.cmml" xref="S3.SS3.p2.3.m1.2.3.3.2"><ci id="S3.SS3.p2.3.m1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1">â‹…</ci><ci id="S3.SS3.p2.3.m1.2.2.cmml" xref="S3.SS3.p2.3.m1.2.2">â‹…</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m1.2c">L_{ce}(\cdot,\cdot)</annotation></semantics></math> is the cross-entropy loss. <math id="S3.SS3.p2.4.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS3.p2.4.m2.1a"><mi id="S3.SS3.p2.4.m2.1.1" xref="S3.SS3.p2.4.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m2.1b"><ci id="S3.SS3.p2.4.m2.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m2.1c">y</annotation></semantics></math> is the label and <math id="S3.SS3.p2.5.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS3.p2.5.m3.1a"><mi id="S3.SS3.p2.5.m3.1.1" xref="S3.SS3.p2.5.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m3.1b"><ci id="S3.SS3.p2.5.m3.1.1.cmml" xref="S3.SS3.p2.5.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m3.1c">p</annotation></semantics></math> is the output logits vector of the FL models. Since the pretrained CLIP model in the pertaining mechanism has zero-shot image classification capability <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>, <math id="S3.SS3.p2.6.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS3.p2.6.m4.1a"><mi id="S3.SS3.p2.6.m4.1.1" xref="S3.SS3.p2.6.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m4.1b"><ci id="S3.SS3.p2.6.m4.1.1.cmml" xref="S3.SS3.p2.6.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m4.1c">q</annotation></semantics></math> denotes the output logits vector of the CLIP model. <math id="S3.SS3.p2.7.m5.1" class="ltx_Math" alttext="KL" display="inline"><semantics id="S3.SS3.p2.7.m5.1a"><mrow id="S3.SS3.p2.7.m5.1.1" xref="S3.SS3.p2.7.m5.1.1.cmml"><mi id="S3.SS3.p2.7.m5.1.1.2" xref="S3.SS3.p2.7.m5.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m5.1.1.1" xref="S3.SS3.p2.7.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p2.7.m5.1.1.3" xref="S3.SS3.p2.7.m5.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m5.1b"><apply id="S3.SS3.p2.7.m5.1.1.cmml" xref="S3.SS3.p2.7.m5.1.1"><times id="S3.SS3.p2.7.m5.1.1.1.cmml" xref="S3.SS3.p2.7.m5.1.1.1"></times><ci id="S3.SS3.p2.7.m5.1.1.2.cmml" xref="S3.SS3.p2.7.m5.1.1.2">ğ¾</ci><ci id="S3.SS3.p2.7.m5.1.1.3.cmml" xref="S3.SS3.p2.7.m5.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m5.1c">KL</annotation></semantics></math> is the Kullback-Leibler divergence and <math id="S3.SS3.p2.8.m6.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS3.p2.8.m6.1a"><mi id="S3.SS3.p2.8.m6.1.1" xref="S3.SS3.p2.8.m6.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m6.1b"><ci id="S3.SS3.p2.8.m6.1.1.cmml" xref="S3.SS3.p2.8.m6.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m6.1c">\beta</annotation></semantics></math> is a hyperparameter balanced these two losses.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Top-1 classification accuracy(%) on CIFAR-10-LT and CIFAR-100-LT datasets with different FL methods, where th results are referred in <cite class="ltx_cite ltx_citemacro_citep">(Shang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>. The best results are marked in bold.</figcaption>
<div id="S3.T2.7" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:224.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.4pt,31.8pt) scale(0.779215743899659,0.779215743899659) ;">
<table id="S3.T2.7.7" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.7.7.8.1" class="ltx_tr">
<td id="S3.T2.7.7.8.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T2.7.7.8.1.1.1" class="ltx_text">Type</span></td>
<td id="S3.T2.7.7.8.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T2.7.7.8.1.2.1" class="ltx_text">Method</span></td>
<td id="S3.T2.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">CIFAR-10-LT</td>
<td id="S3.T2.7.7.8.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">CIFAR-100-LT</td>
</tr>
<tr id="S3.T2.7.7.9.2" class="ltx_tr">
<td id="S3.T2.7.7.9.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">IF=100</td>
<td id="S3.T2.7.7.9.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">IF=50</td>
<td id="S3.T2.7.7.9.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">IF=10</td>
<td id="S3.T2.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">IF=100</td>
<td id="S3.T2.7.7.9.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">IF=50</td>
<td id="S3.T2.7.7.9.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">IF=10</td>
</tr>
<tr id="S3.T2.7.7.10.3" class="ltx_tr">
<td id="S3.T2.7.7.10.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="7"><span id="S3.T2.7.7.10.3.1.1" class="ltx_text">Heterogeneity-oriented FL methods</span></td>
<td id="S3.T2.7.7.10.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">FedAvg</td>
<td id="S3.T2.7.7.10.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">56.17</td>
<td id="S3.T2.7.7.10.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.36</td>
<td id="S3.T2.7.7.10.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">77.45</td>
<td id="S3.T2.7.7.10.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.34</td>
<td id="S3.T2.7.7.10.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">36.35</td>
<td id="S3.T2.7.7.10.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">45.87</td>
</tr>
<tr id="S3.T2.7.7.11.4" class="ltx_tr">
<td id="S3.T2.7.7.11.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedAvgM</td>
<td id="S3.T2.7.7.11.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.03</td>
<td id="S3.T2.7.7.11.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">57.11</td>
<td id="S3.T2.7.7.11.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70.81</td>
<td id="S3.T2.7.7.11.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.80</td>
<td id="S3.T2.7.7.11.4.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.33</td>
<td id="S3.T2.7.7.11.4.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">44.66</td>
</tr>
<tr id="S3.T2.7.7.12.5" class="ltx_tr">
<td id="S3.T2.7.7.12.5.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedProx</td>
<td id="S3.T2.7.7.12.5.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.92</td>
<td id="S3.T2.7.7.12.5.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">60.89</td>
<td id="S3.T2.7.7.12.5.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">76.53</td>
<td id="S3.T2.7.7.12.5.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.67</td>
<td id="S3.T2.7.7.12.5.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.30</td>
<td id="S3.T2.7.7.12.5.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.10</td>
</tr>
<tr id="S3.T2.7.7.13.6" class="ltx_tr">
<td id="S3.T2.7.7.13.6.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedDF</td>
<td id="S3.T2.7.7.13.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">55.15</td>
<td id="S3.T2.7.7.13.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">58.74</td>
<td id="S3.T2.7.7.13.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">76.51</td>
<td id="S3.T2.7.7.13.6.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.43</td>
<td id="S3.T2.7.7.13.6.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.22</td>
<td id="S3.T2.7.7.13.6.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.19</td>
</tr>
<tr id="S3.T2.7.7.14.7" class="ltx_tr">
<td id="S3.T2.7.7.14.7.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedBE</td>
<td id="S3.T2.7.7.14.7.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">55.79</td>
<td id="S3.T2.7.7.14.7.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">59.55</td>
<td id="S3.T2.7.7.14.7.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">77.78</td>
<td id="S3.T2.7.7.14.7.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.97</td>
<td id="S3.T2.7.7.14.7.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.39</td>
<td id="S3.T2.7.7.14.7.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.25</td>
</tr>
<tr id="S3.T2.7.7.15.8" class="ltx_tr">
<td id="S3.T2.7.7.15.8.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">CCVR</td>
<td id="S3.T2.7.7.15.8.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.53</td>
<td id="S3.T2.7.7.15.8.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">71.89</td>
<td id="S3.T2.7.7.15.8.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">78.48</td>
<td id="S3.T2.7.7.15.8.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.43</td>
<td id="S3.T2.7.7.15.8.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.98</td>
<td id="S3.T2.7.7.15.8.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.88</td>
</tr>
<tr id="S3.T2.7.7.16.9" class="ltx_tr">
<td id="S3.T2.7.7.16.9.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedNova</td>
<td id="S3.T2.7.7.16.9.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">57.79</td>
<td id="S3.T2.7.7.16.9.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">63.91</td>
<td id="S3.T2.7.7.16.9.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">77.79</td>
<td id="S3.T2.7.7.16.9.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.64</td>
<td id="S3.T2.7.7.16.9.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.62</td>
<td id="S3.T2.7.7.16.9.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.75</td>
</tr>
<tr id="S3.T2.7.7.17.10" class="ltx_tr">
<td id="S3.T2.7.7.17.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3"><span id="S3.T2.7.7.17.10.1.1" class="ltx_text">Imbalance-oriented FL methods</span></td>
<td id="S3.T2.7.7.17.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Fed-Focal Loss</td>
<td id="S3.T2.7.7.17.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">53.83</td>
<td id="S3.T2.7.7.17.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">57.42</td>
<td id="S3.T2.7.7.17.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">73.74</td>
<td id="S3.T2.7.7.17.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.67</td>
<td id="S3.T2.7.7.17.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">35.25</td>
<td id="S3.T2.7.7.17.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">45.52</td>
</tr>
<tr id="S3.T2.7.7.18.11" class="ltx_tr">
<td id="S3.T2.7.7.18.11.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Ratio Loss</td>
<td id="S3.T2.7.7.18.11.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">59.75</td>
<td id="S3.T2.7.7.18.11.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">64.77</td>
<td id="S3.T2.7.7.18.11.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">78.14</td>
<td id="S3.T2.7.7.18.11.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.95</td>
<td id="S3.T2.7.7.18.11.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.88</td>
<td id="S3.T2.7.7.18.11.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.79</td>
</tr>
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedAvg+ <math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mi id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\tau</annotation></semantics></math>-norm</td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.95</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.41</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.08</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">26.22</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.71</td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.65</td>
</tr>
<tr id="S3.T2.7.7.19.12" class="ltx_tr">
<td id="S3.T2.7.7.19.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Classifier-retraining</td>
<td id="S3.T2.7.7.19.12.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">CReFF</td>
<td id="S3.T2.7.7.19.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">70.55</td>
<td id="S3.T2.7.7.19.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">73.08</td>
<td id="S3.T2.7.7.19.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">80.71</td>
<td id="S3.T2.7.7.19.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34.67</td>
<td id="S3.T2.7.7.19.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">37.64</td>
<td id="S3.T2.7.7.19.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.08</td>
</tr>
<tr id="S3.T2.7.7.20.13" class="ltx_tr">
<td id="S3.T2.7.7.20.13.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SOTA</td>
<td id="S3.T2.7.7.20.13.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">CLIP2FL</td>
<td id="S3.T2.7.7.20.13.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">73.37</td>
<td id="S3.T2.7.7.20.13.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">75.35</td>
<td id="S3.T2.7.7.20.13.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">81.18</td>
<td id="S3.T2.7.7.20.13.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">37.56</td>
<td id="S3.T2.7.7.20.13.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">41.29</td>
<td id="S3.T2.7.7.20.13.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.20</td>
</tr>
<tr id="S3.T2.7.7.21.14" class="ltx_tr">
<td id="S3.T2.7.7.21.14.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T2.7.7.21.14.1.1" class="ltx_text">Our framework</span></td>
<td id="S3.T2.7.7.21.14.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T2.7.7.21.14.2.1" class="ltx_text">MLLM-FL</span></td>
<td id="S3.T2.7.7.21.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.7.7.21.14.3.1" class="ltx_text ltx_font_bold">75.49</span></td>
<td id="S3.T2.7.7.21.14.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.7.7.21.14.4.1" class="ltx_text ltx_font_bold">76.11</span></td>
<td id="S3.T2.7.7.21.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.7.7.21.14.5.1" class="ltx_text ltx_font_bold">81.45</span></td>
<td id="S3.T2.7.7.21.14.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.7.7.21.14.6.1" class="ltx_text ltx_font_bold">39.50</span></td>
<td id="S3.T2.7.7.21.14.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.7.7.21.14.7.1" class="ltx_text ltx_font_bold">42.34</span></td>
<td id="S3.T2.7.7.21.14.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.7.7.21.14.8.1" class="ltx_text ltx_font_bold">48.87</span></td>
</tr>
<tr id="S3.T2.7.7.7" class="ltx_tr">
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 2.12})" display="inline"><semantics id="S3.T2.2.2.2.1.m1.1a"><mrow id="S3.T2.2.2.2.1.m1.1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T2.2.2.2.1.m1.1.1.1.2" xref="S3.T2.2.2.2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.2.2.2.1.m1.1.1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.1.1.cmml"><mi id="S3.T2.2.2.2.1.m1.1.1.1.1.2" xref="S3.T2.2.2.2.1.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S3.T2.2.2.2.1.m1.1.1.1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S3.T2.2.2.2.1.m1.1.1.1.1.3" xref="S3.T2.2.2.2.1.m1.1.1.1.1.3.cmml">2.12</mn></mrow><mo stretchy="false" id="S3.T2.2.2.2.1.m1.1.1.1.3" xref="S3.T2.2.2.2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><apply id="S3.T2.2.2.2.1.m1.1.1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1.1"><ci id="S3.T2.2.2.2.1.m1.1.1.1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S3.T2.2.2.2.1.m1.1.1.1.1.2.cmml" xref="S3.T2.2.2.2.1.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.2.2.2.1.m1.1.1.1.1.3.cmml" xref="S3.T2.2.2.2.1.m1.1.1.1.1.3">2.12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">({\color[rgb]{0,1,0}\uparrow 2.12})</annotation></semantics></math></td>
<td id="S3.T2.3.3.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S3.T2.3.3.3.2.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 1.24})" display="inline"><semantics id="S3.T2.3.3.3.2.m1.1a"><mrow id="S3.T2.3.3.3.2.m1.1.1.1" xref="S3.T2.3.3.3.2.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T2.3.3.3.2.m1.1.1.1.2" xref="S3.T2.3.3.3.2.m1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.3.3.3.2.m1.1.1.1.1" xref="S3.T2.3.3.3.2.m1.1.1.1.1.cmml"><mi id="S3.T2.3.3.3.2.m1.1.1.1.1.2" xref="S3.T2.3.3.3.2.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S3.T2.3.3.3.2.m1.1.1.1.1.1" xref="S3.T2.3.3.3.2.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S3.T2.3.3.3.2.m1.1.1.1.1.3" xref="S3.T2.3.3.3.2.m1.1.1.1.1.3.cmml">1.24</mn></mrow><mo stretchy="false" id="S3.T2.3.3.3.2.m1.1.1.1.3" xref="S3.T2.3.3.3.2.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.2.m1.1b"><apply id="S3.T2.3.3.3.2.m1.1.1.1.1.cmml" xref="S3.T2.3.3.3.2.m1.1.1.1"><ci id="S3.T2.3.3.3.2.m1.1.1.1.1.1.cmml" xref="S3.T2.3.3.3.2.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S3.T2.3.3.3.2.m1.1.1.1.1.2.cmml" xref="S3.T2.3.3.3.2.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.3.3.3.2.m1.1.1.1.1.3.cmml" xref="S3.T2.3.3.3.2.m1.1.1.1.1.3">1.24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.2.m1.1c">({\color[rgb]{0,1,0}\uparrow 1.24})</annotation></semantics></math></td>
<td id="S3.T2.4.4.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S3.T2.4.4.4.3.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 0.27})" display="inline"><semantics id="S3.T2.4.4.4.3.m1.1a"><mrow id="S3.T2.4.4.4.3.m1.1.1.1" xref="S3.T2.4.4.4.3.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T2.4.4.4.3.m1.1.1.1.2" xref="S3.T2.4.4.4.3.m1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.4.4.4.3.m1.1.1.1.1" xref="S3.T2.4.4.4.3.m1.1.1.1.1.cmml"><mi id="S3.T2.4.4.4.3.m1.1.1.1.1.2" xref="S3.T2.4.4.4.3.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S3.T2.4.4.4.3.m1.1.1.1.1.1" xref="S3.T2.4.4.4.3.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S3.T2.4.4.4.3.m1.1.1.1.1.3" xref="S3.T2.4.4.4.3.m1.1.1.1.1.3.cmml">0.27</mn></mrow><mo stretchy="false" id="S3.T2.4.4.4.3.m1.1.1.1.3" xref="S3.T2.4.4.4.3.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.3.m1.1b"><apply id="S3.T2.4.4.4.3.m1.1.1.1.1.cmml" xref="S3.T2.4.4.4.3.m1.1.1.1"><ci id="S3.T2.4.4.4.3.m1.1.1.1.1.1.cmml" xref="S3.T2.4.4.4.3.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S3.T2.4.4.4.3.m1.1.1.1.1.2.cmml" xref="S3.T2.4.4.4.3.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.4.4.4.3.m1.1.1.1.1.3.cmml" xref="S3.T2.4.4.4.3.m1.1.1.1.1.3">0.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.3.m1.1c">({\color[rgb]{0,1,0}\uparrow 0.27})</annotation></semantics></math></td>
<td id="S3.T2.5.5.5.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S3.T2.5.5.5.4.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 1.94})" display="inline"><semantics id="S3.T2.5.5.5.4.m1.1a"><mrow id="S3.T2.5.5.5.4.m1.1.1.1" xref="S3.T2.5.5.5.4.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T2.5.5.5.4.m1.1.1.1.2" xref="S3.T2.5.5.5.4.m1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.5.5.5.4.m1.1.1.1.1" xref="S3.T2.5.5.5.4.m1.1.1.1.1.cmml"><mi id="S3.T2.5.5.5.4.m1.1.1.1.1.2" xref="S3.T2.5.5.5.4.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S3.T2.5.5.5.4.m1.1.1.1.1.1" xref="S3.T2.5.5.5.4.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S3.T2.5.5.5.4.m1.1.1.1.1.3" xref="S3.T2.5.5.5.4.m1.1.1.1.1.3.cmml">1.94</mn></mrow><mo stretchy="false" id="S3.T2.5.5.5.4.m1.1.1.1.3" xref="S3.T2.5.5.5.4.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.4.m1.1b"><apply id="S3.T2.5.5.5.4.m1.1.1.1.1.cmml" xref="S3.T2.5.5.5.4.m1.1.1.1"><ci id="S3.T2.5.5.5.4.m1.1.1.1.1.1.cmml" xref="S3.T2.5.5.5.4.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S3.T2.5.5.5.4.m1.1.1.1.1.2.cmml" xref="S3.T2.5.5.5.4.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.5.5.5.4.m1.1.1.1.1.3.cmml" xref="S3.T2.5.5.5.4.m1.1.1.1.1.3">1.94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.4.m1.1c">({\color[rgb]{0,1,0}\uparrow 1.94})</annotation></semantics></math></td>
<td id="S3.T2.6.6.6.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S3.T2.6.6.6.5.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 1.05})" display="inline"><semantics id="S3.T2.6.6.6.5.m1.1a"><mrow id="S3.T2.6.6.6.5.m1.1.1.1" xref="S3.T2.6.6.6.5.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T2.6.6.6.5.m1.1.1.1.2" xref="S3.T2.6.6.6.5.m1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.6.6.6.5.m1.1.1.1.1" xref="S3.T2.6.6.6.5.m1.1.1.1.1.cmml"><mi id="S3.T2.6.6.6.5.m1.1.1.1.1.2" xref="S3.T2.6.6.6.5.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S3.T2.6.6.6.5.m1.1.1.1.1.1" xref="S3.T2.6.6.6.5.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S3.T2.6.6.6.5.m1.1.1.1.1.3" xref="S3.T2.6.6.6.5.m1.1.1.1.1.3.cmml">1.05</mn></mrow><mo stretchy="false" id="S3.T2.6.6.6.5.m1.1.1.1.3" xref="S3.T2.6.6.6.5.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.5.m1.1b"><apply id="S3.T2.6.6.6.5.m1.1.1.1.1.cmml" xref="S3.T2.6.6.6.5.m1.1.1.1"><ci id="S3.T2.6.6.6.5.m1.1.1.1.1.1.cmml" xref="S3.T2.6.6.6.5.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S3.T2.6.6.6.5.m1.1.1.1.1.2.cmml" xref="S3.T2.6.6.6.5.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.6.6.6.5.m1.1.1.1.1.3.cmml" xref="S3.T2.6.6.6.5.m1.1.1.1.1.3">1.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.5.m1.1c">({\color[rgb]{0,1,0}\uparrow 1.05})</annotation></semantics></math></td>
<td id="S3.T2.7.7.7.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S3.T2.7.7.7.6.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 0.67})" display="inline"><semantics id="S3.T2.7.7.7.6.m1.1a"><mrow id="S3.T2.7.7.7.6.m1.1.1.1" xref="S3.T2.7.7.7.6.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T2.7.7.7.6.m1.1.1.1.2" xref="S3.T2.7.7.7.6.m1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.7.7.7.6.m1.1.1.1.1" xref="S3.T2.7.7.7.6.m1.1.1.1.1.cmml"><mi id="S3.T2.7.7.7.6.m1.1.1.1.1.2" xref="S3.T2.7.7.7.6.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S3.T2.7.7.7.6.m1.1.1.1.1.1" xref="S3.T2.7.7.7.6.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S3.T2.7.7.7.6.m1.1.1.1.1.3" xref="S3.T2.7.7.7.6.m1.1.1.1.1.3.cmml">0.67</mn></mrow><mo stretchy="false" id="S3.T2.7.7.7.6.m1.1.1.1.3" xref="S3.T2.7.7.7.6.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.7.6.m1.1b"><apply id="S3.T2.7.7.7.6.m1.1.1.1.1.cmml" xref="S3.T2.7.7.7.6.m1.1.1.1"><ci id="S3.T2.7.7.7.6.m1.1.1.1.1.1.cmml" xref="S3.T2.7.7.7.6.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S3.T2.7.7.7.6.m1.1.1.1.1.2.cmml" xref="S3.T2.7.7.7.6.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.7.7.7.6.m1.1.1.1.1.3.cmml" xref="S3.T2.7.7.7.6.m1.1.1.1.1.3">0.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.7.6.m1.1c">({\color[rgb]{0,1,0}\uparrow 0.67})</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">The idea of using a class-balanced dataset to alleviate the challenges of long-tailed distributions aligns with the concepts of data resampling in centralized training to handle class imbalance and client selection in federated learning. The feasibility of such an alignment dataset <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{align}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.2" xref="S3.SS3.p3.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.3.1" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.1.m1.1.1.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.3.1a" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.1.m1.1.1.3.4" xref="S3.SS3.p3.1.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.3.1b" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.1.m1.1.1.3.5" xref="S3.SS3.p3.1.m1.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.3.1c" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.1.m1.1.1.3.6" xref="S3.SS3.p3.1.m1.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ğ’Ÿ</ci><apply id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3"><times id="S3.SS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.1"></times><ci id="S3.SS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p3.1.m1.1.1.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p3.1.m1.1.1.3.4.cmml" xref="S3.SS3.p3.1.m1.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p3.1.m1.1.1.3.5.cmml" xref="S3.SS3.p3.1.m1.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p3.1.m1.1.1.3.6.cmml" xref="S3.SS3.p3.1.m1.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathcal{D}_{align}</annotation></semantics></math> existing on the server side is justifiable in most cases because, in practice, for global aggregation, the server typically predefines the categories for model classification and organizes them, which is essential for subsequent federated learning processes. Otherwise, the global aggregation of classifier layers would become chaotic. Knowing the categories, companies could feasibly collect data from the internet or generate data using powerful image-generation models like Stable Diffusion or Midjourney. However, we acknowledge that in some extreme cases, data collection can be challenging, necessitating the design of more specific <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="L_{align}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">L</mi><mrow id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml"><mi id="S3.SS3.p3.2.m2.1.1.3.2" xref="S3.SS3.p3.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.3.1" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.2.m2.1.1.3.3" xref="S3.SS3.p3.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.3.1a" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.2.m2.1.1.3.4" xref="S3.SS3.p3.2.m2.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.3.1b" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.2.m2.1.1.3.5" xref="S3.SS3.p3.2.m2.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.3.1c" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.2.m2.1.1.3.6" xref="S3.SS3.p3.2.m2.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ğ¿</ci><apply id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3"><times id="S3.SS3.p3.2.m2.1.1.3.1.cmml" xref="S3.SS3.p3.2.m2.1.1.3.1"></times><ci id="S3.SS3.p3.2.m2.1.1.3.2.cmml" xref="S3.SS3.p3.2.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p3.2.m2.1.1.3.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p3.2.m2.1.1.3.4.cmml" xref="S3.SS3.p3.2.m2.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p3.2.m2.1.1.3.5.cmml" xref="S3.SS3.p3.2.m2.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p3.2.m2.1.1.3.6.cmml" xref="S3.SS3.p3.2.m2.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">L_{align}</annotation></semantics></math> and <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{align}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.2" xref="S3.SS3.p3.3.m3.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.1" xref="S3.SS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.1a" xref="S3.SS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.3.m3.1.1.3.4" xref="S3.SS3.p3.3.m3.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.1b" xref="S3.SS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.3.m3.1.1.3.5" xref="S3.SS3.p3.3.m3.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.1c" xref="S3.SS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p3.3.m3.1.1.3.6" xref="S3.SS3.p3.3.m3.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ğ’Ÿ</ci><apply id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><times id="S3.SS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3.1"></times><ci id="S3.SS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3">ğ‘™</ci><ci id="S3.SS3.p3.3.m3.1.1.3.4.cmml" xref="S3.SS3.p3.3.m3.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.p3.3.m3.1.1.3.5.cmml" xref="S3.SS3.p3.3.m3.1.1.3.5">ğ‘”</ci><ci id="S3.SS3.p3.3.m3.1.1.3.6.cmml" xref="S3.SS3.p3.3.m3.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathcal{D}_{align}</annotation></semantics></math>, which we leave for future work.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experiment Setup</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dataset<math id="S4.SS1.SSS0.Px1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="S4.SS1.SSS0.Px1.1.m1.1b"><mo id="S4.SS1.SSS0.Px1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.1.m1.1c"><and id="S4.SS1.SSS0.Px1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.1.m1.1d">\&amp;</annotation></semantics></math>Implementation</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.7" class="ltx_p">We applied our MLLM-FL framework to three widely-used long-tailed datasets: CIFAR-10/100LT <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">[n.â€‰d.]</a>)</cite> and ImageNet-LT <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. As for the first two datasets, we adopt the same sampling technique as previous studies <cite class="ltx_cite ltx_citemacro_citep">(Cui etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> to create long-tailed distributions with various imbalance factors <math id="S4.SS1.SSS0.Px1.p1.1.m1.4" class="ltx_Math" alttext="(\mathrm{IF}=100,50,10)" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.4a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.cmml">(</mo><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">IF</mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.1.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">100</mn><mo id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.2.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.1.cmml">,</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.2.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.cmml">50</mn><mo id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.2.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.1.cmml">,</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.3.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.cmml">10</mn></mrow></mrow><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.4b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1"><eq id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.2">IF</ci><list id="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.4.4.1.1.3.2"><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">100</cn><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2">50</cn><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3">10</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.4c">(\mathrm{IF}=100,50,10)</annotation></semantics></math>, and we follow CReFF <cite class="ltx_cite ltx_citemacro_citep">(Shang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> to use Dirichlet distribution with the key parameter <math id="S4.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.1c">\alpha</annotation></semantics></math> to generate the heterogeneous data partition among clients, where the value of <math id="S4.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S4.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m3.1c">\alpha</annotation></semantics></math> is set to 0.5 on CIFAR-10/100-LT. ImageNet-LT has <math id="S4.SS1.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="115.8\mathrm{~{}K}" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.4.m4.1a"><mrow id="S4.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">115.8</mn><mo lspace="0.330em" rspace="0em" id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1"><times id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.1"></times><cn type="float" id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.2">115.8</cn><ci id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.3">K</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.4.m4.1c">115.8\mathrm{~{}K}</annotation></semantics></math> images from 1000 classes and the number of images per class ranging from 1280 to 5, where the value of <math id="S4.SS1.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.5.m5.1a"><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.5.m5.1b"><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.5.m5.1c">\alpha</annotation></semantics></math> is set to 0.1. We utilized ResNet-8 as the feature extractor for CIFAR-10/100-LT and ResNet-50 for ImageNet-LT, adding an MLP layer to each to align their feature dimensions with CLIPâ€™s outputs. The number of clients is set to 20, and we select 40<math id="S4.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.6.m6.1a"><mo id="S4.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.6.m6.1b"><csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.6.m6.1c">\%</annotation></semantics></math> at random for each training round. The client-side training batch size was uniform at 32 across Cifar-10/100 and imagenet. All the above settings are the same as the previous work in <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>. We employed the standard cross-entropy loss by default and executed 200 communication rounds. For the pertaining part, we adopt the pertaining dataset of LLaVA, CC-595K, and train the model for 4 epochs with a learning rate of 2e-3 and a batch size of 128. During the first 2 epochs, the <math id="S4.SS1.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="alpha" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.7.m7.1a"><mrow id="S4.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1a" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.4" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1b" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.5" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1c" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.6" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.7.m7.1b"><apply id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1"><times id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1"></times><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2">ğ‘</ci><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3">ğ‘™</ci><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.4.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.4">ğ‘</ci><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.5.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.5">â„</ci><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.6.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.6">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.7.m7.1c">alpha</annotation></semantics></math> in our pretraining mechanism increase from 0 to 1 following the cosine scheduler and then the value remains 1 for the following epochs. All the experiments were conducted using PyTorch on a single Nvidia A100 80G GPU.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.2" class="ltx_p">We compare MLLM4FL with 13 FL methods: FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2017</a>)</cite>, FedAvgM <cite class="ltx_cite ltx_citemacro_citep">(Hsu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>, FedDF <cite class="ltx_cite ltx_citemacro_citep">(Sui etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>, FedBE <cite class="ltx_cite ltx_citemacro_citep">(Chen and Chao, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, CCVR <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> and FedNova <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020a</a>)</cite>, Fed-Focal Loss <cite class="ltx_cite ltx_citemacro_citep">(Sarkar etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>, Ratio Loss <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2020b</a>)</cite> and FedAvg with <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">\tau</annotation></semantics></math>-norm <cite class="ltx_cite ltx_citemacro_citep">(Kang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, CReFF <cite class="ltx_cite ltx_citemacro_citep">(Shang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> and CLIP2FL <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>. The first seven approaches are heterogeneity-oriented , and Fed-Focal Loss, Ratio Loss and FedAvg with <math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">\tau</annotation></semantics></math>-norm are imbalance-oriented.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Experimental Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Results for CIFAR-10/100-LT are presented in Table <a href="#S3.T2" title="Table 2 â€£ 3.3. Global Alignment â€£ 3. Methodology â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where we evaluate the performance of our CLIP2FL against a range of FL approaches on both CIFAR-10-LT and CIFAR-100-LT datasets. Notably, MLLM-FL outperforms other methods in terms of classification accuracy on both datasets. Specifically, at an Imbalance Factor (IF) of 100, which presents a severe imbalance, MLLM shows an improvement of 2.12% and 1.94% in classification accuracy over CLIP2FL for CIFAR-10-LT and CIFAR-100-LT, respectively. Under the condition of IF = 50 or 10, MLLM still manages to enhance performance by around 1%. This underscores MLLM-FLâ€™s effectiveness and its outperforms over competing methods to deal with heterogeous and long-tailed distributions.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Top-1 accuracy(%) on ImageNet-LT dataset with different FL method</figcaption>
<div id="S4.T3.5" class="ltx_inline-block ltx_transformed_outer" style="width:346.9pt;height:175.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.5pt,29.1pt) scale(0.750978285435979,0.750978285435979) ;">
<table id="S4.T3.5.5" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.5.5.6.1" class="ltx_tr">
<td id="S4.T3.5.5.6.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T3.5.5.6.1.1.1" class="ltx_text">Type</span></td>
<td id="S4.T3.5.5.6.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T3.5.5.6.1.2.1" class="ltx_text">Method</span></td>
<td id="S4.T3.5.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="4">ImageNet-LT</td>
</tr>
<tr id="S4.T3.5.5.7.2" class="ltx_tr">
<td id="S4.T3.5.5.7.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">All</td>
<td id="S4.T3.5.5.7.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Many</td>
<td id="S4.T3.5.5.7.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Medium</td>
<td id="S4.T3.5.5.7.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Few</td>
</tr>
<tr id="S4.T3.5.5.8.3" class="ltx_tr">
<td id="S4.T3.5.5.8.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5"><span id="S4.T3.5.5.8.3.1.1" class="ltx_text">Heterogeneity-oriented FL methods</span></td>
<td id="S4.T3.5.5.8.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">FedAvg</td>
<td id="S4.T3.5.5.8.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">23.85</td>
<td id="S4.T3.5.5.8.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34.92</td>
<td id="S4.T3.5.5.8.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">19.18</td>
<td id="S4.T3.5.5.8.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.10</td>
</tr>
<tr id="S4.T3.5.5.9.4" class="ltx_tr">
<td id="S4.T3.5.5.9.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedAvgM</td>
<td id="S4.T3.5.5.9.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">22.57</td>
<td id="S4.T3.5.5.9.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.93</td>
<td id="S4.T3.5.5.9.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">18.55</td>
<td id="S4.T3.5.5.9.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">6.73</td>
</tr>
<tr id="S4.T3.5.5.10.5" class="ltx_tr">
<td id="S4.T3.5.5.10.5.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedProx</td>
<td id="S4.T3.5.5.10.5.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">22.99</td>
<td id="S4.T3.5.5.10.5.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.25</td>
<td id="S4.T3.5.5.10.5.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">17.06</td>
<td id="S4.T3.5.5.10.5.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">6.37</td>
</tr>
<tr id="S4.T3.5.5.11.6" class="ltx_tr">
<td id="S4.T3.5.5.11.6.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedDF</td>
<td id="S4.T3.5.5.11.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">21.63</td>
<td id="S4.T3.5.5.11.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.78</td>
<td id="S4.T3.5.5.11.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">15.52</td>
<td id="S4.T3.5.5.11.6.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">4.48</td>
</tr>
<tr id="S4.T3.5.5.12.7" class="ltx_tr">
<td id="S4.T3.5.5.12.7.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">CCVR</td>
<td id="S4.T3.5.5.12.7.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.49</td>
<td id="S4.T3.5.5.12.7.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.72</td>
<td id="S4.T3.5.5.12.7.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">20.24</td>
<td id="S4.T3.5.5.12.7.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">9.26</td>
</tr>
<tr id="S4.T3.5.5.13.8" class="ltx_tr">
<td id="S4.T3.5.5.13.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3"><span id="S4.T3.5.5.13.8.1.1" class="ltx_text">Imbalance-oriented FL methods</span></td>
<td id="S4.T3.5.5.13.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Fed-Focal Loss</td>
<td id="S4.T3.5.5.13.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">21.60</td>
<td id="S4.T3.5.5.13.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">31.74</td>
<td id="S4.T3.5.5.13.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.77</td>
<td id="S4.T3.5.5.13.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">5.52</td>
</tr>
<tr id="S4.T3.5.5.14.9" class="ltx_tr">
<td id="S4.T3.5.5.14.9.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Ratio Loss</td>
<td id="S4.T3.5.5.14.9.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">24.31</td>
<td id="S4.T3.5.5.14.9.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.33</td>
<td id="S4.T3.5.5.14.9.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">18.14</td>
<td id="S4.T3.5.5.14.9.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">7.41</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">FedAvg+ <math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\tau</annotation></semantics></math>-norm</td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">21.58</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.66</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">15.76</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">4.33</td>
</tr>
<tr id="S4.T3.5.5.15.10" class="ltx_tr">
<td id="S4.T3.5.5.15.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Classfier-retraining</td>
<td id="S4.T3.5.5.15.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">CReFF</td>
<td id="S4.T3.5.5.15.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.31</td>
<td id="S4.T3.5.5.15.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.5.5.15.10.4.1" class="ltx_text ltx_font_bold">37.44</span></td>
<td id="S4.T3.5.5.15.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">21.87</td>
<td id="S4.T3.5.5.15.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.29</td>
</tr>
<tr id="S4.T3.5.5.16.11" class="ltx_tr">
<td id="S4.T3.5.5.16.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T3.5.5.16.11.1.1" class="ltx_text">Our framework</span></td>
<td id="S4.T3.5.5.16.11.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T3.5.5.16.11.2.1" class="ltx_text">MLLM-FL</span></td>
<td id="S4.T3.5.5.16.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.5.5.16.11.3.1" class="ltx_text ltx_font_bold">27.53</span></td>
<td id="S4.T3.5.5.16.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.85</td>
<td id="S4.T3.5.5.16.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.5.5.16.11.5.1" class="ltx_text ltx_font_bold">25.89</span></td>
<td id="S4.T3.5.5.16.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.5.5.16.11.6.1" class="ltx_text ltx_font_bold">25.58</span></td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 1.22})" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mrow id="S4.T3.2.2.2.1.m1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T3.2.2.2.1.m1.1.1.1.2" xref="S4.T3.2.2.2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S4.T3.2.2.2.1.m1.1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.1.cmml"><mi id="S4.T3.2.2.2.1.m1.1.1.1.1.2" xref="S4.T3.2.2.2.1.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S4.T3.2.2.2.1.m1.1.1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S4.T3.2.2.2.1.m1.1.1.1.1.3" xref="S4.T3.2.2.2.1.m1.1.1.1.1.3.cmml">1.22</mn></mrow><mo stretchy="false" id="S4.T3.2.2.2.1.m1.1.1.1.3" xref="S4.T3.2.2.2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1"><ci id="S4.T3.2.2.2.1.m1.1.1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S4.T3.2.2.2.1.m1.1.1.1.1.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S4.T3.2.2.2.1.m1.1.1.1.1.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1.1.3">1.22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">({\color[rgb]{0,1,0}\uparrow 1.22})</annotation></semantics></math></td>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S4.T3.3.3.3.2.m1.1" class="ltx_Math" alttext="({\color[rgb]{1,0,0}\downarrow 6.59})" display="inline"><semantics id="S4.T3.3.3.3.2.m1.1a"><mrow id="S4.T3.3.3.3.2.m1.1.1.1" xref="S4.T3.3.3.3.2.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T3.3.3.3.2.m1.1.1.1.2" xref="S4.T3.3.3.3.2.m1.1.1.1.1.cmml">(</mo><mrow id="S4.T3.3.3.3.2.m1.1.1.1.1" xref="S4.T3.3.3.3.2.m1.1.1.1.1.cmml"><mi id="S4.T3.3.3.3.2.m1.1.1.1.1.2" xref="S4.T3.3.3.3.2.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#FF0000" stretchy="false" id="S4.T3.3.3.3.2.m1.1.1.1.1.1" xref="S4.T3.3.3.3.2.m1.1.1.1.1.1.cmml">â†“</mo><mn mathcolor="#FF0000" id="S4.T3.3.3.3.2.m1.1.1.1.1.3" xref="S4.T3.3.3.3.2.m1.1.1.1.1.3.cmml">6.59</mn></mrow><mo stretchy="false" id="S4.T3.3.3.3.2.m1.1.1.1.3" xref="S4.T3.3.3.3.2.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.2.m1.1b"><apply id="S4.T3.3.3.3.2.m1.1.1.1.1.cmml" xref="S4.T3.3.3.3.2.m1.1.1.1"><ci id="S4.T3.3.3.3.2.m1.1.1.1.1.1.cmml" xref="S4.T3.3.3.3.2.m1.1.1.1.1.1">â†“</ci><csymbol cd="latexml" id="S4.T3.3.3.3.2.m1.1.1.1.1.2.cmml" xref="S4.T3.3.3.3.2.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S4.T3.3.3.3.2.m1.1.1.1.1.3.cmml" xref="S4.T3.3.3.3.2.m1.1.1.1.1.3">6.59</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.2.m1.1c">({\color[rgb]{1,0,0}\downarrow 6.59})</annotation></semantics></math></td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S4.T3.4.4.4.3.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 4.02})" display="inline"><semantics id="S4.T3.4.4.4.3.m1.1a"><mrow id="S4.T3.4.4.4.3.m1.1.1.1" xref="S4.T3.4.4.4.3.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T3.4.4.4.3.m1.1.1.1.2" xref="S4.T3.4.4.4.3.m1.1.1.1.1.cmml">(</mo><mrow id="S4.T3.4.4.4.3.m1.1.1.1.1" xref="S4.T3.4.4.4.3.m1.1.1.1.1.cmml"><mi id="S4.T3.4.4.4.3.m1.1.1.1.1.2" xref="S4.T3.4.4.4.3.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S4.T3.4.4.4.3.m1.1.1.1.1.1" xref="S4.T3.4.4.4.3.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S4.T3.4.4.4.3.m1.1.1.1.1.3" xref="S4.T3.4.4.4.3.m1.1.1.1.1.3.cmml">4.02</mn></mrow><mo stretchy="false" id="S4.T3.4.4.4.3.m1.1.1.1.3" xref="S4.T3.4.4.4.3.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.3.m1.1b"><apply id="S4.T3.4.4.4.3.m1.1.1.1.1.cmml" xref="S4.T3.4.4.4.3.m1.1.1.1"><ci id="S4.T3.4.4.4.3.m1.1.1.1.1.1.cmml" xref="S4.T3.4.4.4.3.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S4.T3.4.4.4.3.m1.1.1.1.1.2.cmml" xref="S4.T3.4.4.4.3.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S4.T3.4.4.4.3.m1.1.1.1.1.3.cmml" xref="S4.T3.4.4.4.3.m1.1.1.1.1.3">4.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.3.m1.1c">({\color[rgb]{0,1,0}\uparrow 4.02})</annotation></semantics></math></td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"><math id="S4.T3.5.5.5.4.m1.1" class="ltx_Math" alttext="({\color[rgb]{0,1,0}\uparrow 15.29)}" display="inline"><semantics id="S4.T3.5.5.5.4.m1.1a"><mrow id="S4.T3.5.5.5.4.m1.1.1.1" xref="S4.T3.5.5.5.4.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T3.5.5.5.4.m1.1.1.1.2" xref="S4.T3.5.5.5.4.m1.1.1.1.1.cmml">(</mo><mrow id="S4.T3.5.5.5.4.m1.1.1.1.1" xref="S4.T3.5.5.5.4.m1.1.1.1.1.cmml"><mi id="S4.T3.5.5.5.4.m1.1.1.1.1.2" xref="S4.T3.5.5.5.4.m1.1.1.1.1.2.cmml"></mi><mo mathcolor="#00FF00" stretchy="false" id="S4.T3.5.5.5.4.m1.1.1.1.1.1" xref="S4.T3.5.5.5.4.m1.1.1.1.1.1.cmml">â†‘</mo><mn mathcolor="#00FF00" id="S4.T3.5.5.5.4.m1.1.1.1.1.3" xref="S4.T3.5.5.5.4.m1.1.1.1.1.3.cmml">15.29</mn></mrow><mo mathcolor="#00FF00" stretchy="false" id="S4.T3.5.5.5.4.m1.1.1.1.3" xref="S4.T3.5.5.5.4.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.4.m1.1b"><apply id="S4.T3.5.5.5.4.m1.1.1.1.1.cmml" xref="S4.T3.5.5.5.4.m1.1.1.1"><ci id="S4.T3.5.5.5.4.m1.1.1.1.1.1.cmml" xref="S4.T3.5.5.5.4.m1.1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="S4.T3.5.5.5.4.m1.1.1.1.1.2.cmml" xref="S4.T3.5.5.5.4.m1.1.1.1.1.2">absent</csymbol><cn type="float" id="S4.T3.5.5.5.4.m1.1.1.1.1.3.cmml" xref="S4.T3.5.5.5.4.m1.1.1.1.1.3">15.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.4.m1.1c">({\color[rgb]{0,1,0}\uparrow 15.29)}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In the context of ImageNet-LT, Table <a href="#S4.T3" title="Table 3 â€£ 4.2. Experimental Results â€£ 4. Experiment â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents a comparison of the accuracy achieved by our MLLM-FL framework against various FL approaches. The evaluation is segmented into four groups based on the number of samples per class: â€œManyâ€ (over 100 samples), â€œMediumâ€ (20 to 100 samples), â€œFewâ€ (less than 20 samples), and â€œAllâ€ (overall accuracy). While our method may not fully match the performance of CReFF in the â€œManyâ€ categories, it excels in â€œOverallâ€ accuracy and the â€œMediumâ€ and â€Fewâ€ category. These results underscore MLLM-FLâ€™s capability not just in enhancing overall model performance but also in significantly improving classification outcomes for categories with fewer samples. The ImageNet-LT results highlight the effectiveness of MLLM-FL in tackling the inherent challenges of long-tailed data distributions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Further Analysis</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We conduct some further analysis to verify the effectiveness of our framework, especially the importance of global pertaining and global alignment.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparison between pretrained and non-pretrained models under constrained training dataset settings. The format is (number of epochs, highest accuracy)</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:346.9pt;height:82.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.4pt,3.9pt) scale(0.913736642623647,0.913736642623647) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T4.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T4.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="S4.T4.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Initialization</span></span>
</span>
</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T4.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">0.4%</span></span>
</span>
</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T4.1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">1%</span></span>
</span>
</th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T4.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">2%</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<td id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.2.1.1.1.1" class="ltx_p" style="width:56.9pt;">Cifar10</span>
</span>
</td>
<td id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.2.1.2.1.1" class="ltx_p" style="width:85.4pt;">w/ Pretraining</span>
</span>
</td>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.2.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold">(3, 37.62)</span></span>
</span>
</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.2.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.2.1.4.1.1.1" class="ltx_text ltx_font_bold">(5, 39.46)</span></span>
</span>
</td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.2.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.2.1.5.1.1.1" class="ltx_text ltx_font_bold">(3, 37.83)</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<td id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.3.2.1.1.1" class="ltx_p" style="width:56.9pt;"></span>
</span>
</td>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.3.2.2.1.1" class="ltx_p" style="width:85.4pt;">w/o Pretraining</span>
</span>
</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.3.2.3.1.1" class="ltx_p" style="width:56.9pt;">(6, 30.44)</span>
</span>
</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.3.2.4.1.1" class="ltx_p" style="width:56.9pt;">(10, 29.8)</span>
</span>
</td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.3.2.5.1.1" class="ltx_p" style="width:56.9pt;">(14, 31.99)</span>
</span>
</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<td id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.4.3.1.1.1" class="ltx_p" style="width:56.9pt;">Cifar100</span>
</span>
</td>
<td id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.4.3.2.1.1" class="ltx_p" style="width:85.4pt;">w/ Pretraining</span>
</span>
</td>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.4.3.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.4.3.3.1.1.1" class="ltx_text ltx_font_bold">(5, 23.91)</span></span>
</span>
</td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.4.3.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.4.3.4.1.1.1" class="ltx_text ltx_font_bold">(6, 24.46)</span></span>
</span>
</td>
<td id="S4.T4.1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.4.3.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T4.1.1.4.3.5.1.1.1" class="ltx_text ltx_font_bold">(6, 24.28)</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<td id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.5.4.1.1.1" class="ltx_p" style="width:56.9pt;"></span>
</span>
</td>
<td id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.5.4.2.1.1" class="ltx_p" style="width:85.4pt;">w/o Pretraining</span>
</span>
</td>
<td id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.5.4.3.1.1" class="ltx_p" style="width:56.9pt;">(10, 18.61)</span>
</span>
</td>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.5.4.4.1.1" class="ltx_p" style="width:56.9pt;">(15, 20.27)</span>
</span>
</td>
<td id="S4.T4.1.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.5.4.5.1.1" class="ltx_p" style="width:56.9pt;">(11, 19.42)</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Ablation studies on our pretraining mechanism</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To evaluate the effectiveness of pretraining, we conducted a comparative analysis between a pretrained model and a non-pretrained model under a constrained training dataset scenario akin to few-shot learning, aiming to mirror real-world conditions where the amount of data available on each device is limited. We generated subsets of the CIFAR-10 and CIFAR-100 datasets at varying proportions and trained both models for 30 epochs. Our findings, detailed in Table <a href="#S4.T4" title="Table 4 â€£ 4.3. Further Analysis â€£ 4. Experiment â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, outline the number of epochs required by each model to reach predetermined accuracy thresholds with different training sample sizes, along with the highest accuracy achieved by each model within the 30-epoch span.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.2" class="ltx_p">Specifically, within the CIFAR-10 context, to attain a target accuracy of <math id="S4.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><mrow id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p2.1.m1.1.1.2" xref="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml">25</mn><mo id="S4.SS3.SSS1.p2.1.m1.1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><apply id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">25\%</annotation></semantics></math>, the pretrained model consistently outperformed the non-pretrained model, requiring fewer epochs across all sample sizes. Similarly, for the CIFAR-100 dataset, in pursuit of a target accuracy of <math id="S4.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="15\%" display="inline"><semantics id="S4.SS3.SSS1.p2.2.m2.1a"><mrow id="S4.SS3.SSS1.p2.2.m2.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS1.p2.2.m2.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml">15</mn><mo id="S4.SS3.SSS1.p2.2.m2.1.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.2.m2.1b"><apply id="S4.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.2.m2.1c">15\%</annotation></semantics></math>, the pretrained model proved more efficient, also necessitating fewer epochs. Figure <a href="#S4.F3" title="Figure 3 â€£ 4.3.1. Ablation studies on our pretraining mechanism â€£ 4.3. Further Analysis â€£ 4. Experiment â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> further illustrates the superiority of our pretraining approach, demonstrating that models pretrained using our methodology surpass those trained from scratch in terms of accuracy.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.06067/assets/figures/curves.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="359" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The comparative analysis of pretrained and non-pretrained models using 1% subsets of CIFAR-10/100 training data.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F3.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Ablation studies on our global alignment mechanism</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In Figure <a href="#S4.F4" title="Figure 4 â€£ 4.3.2. Ablation studies on our global alignment mechanism â€£ 4.3. Further Analysis â€£ 4. Experiment â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we present the confusion matrix for our model equipped with global alignment within the CIFAR-10-LT dataset, characterized by an imbalance factor of 100. We normalize this confusion matrix by the volume of data in each class. Additionally, we illustrate a normalized confusion matrix for a baseline model devoid of global alignment within our Federated Learning (FL) framework. The visual representations indicate that, with alignment in place, data from each class can be accurately classified. In contrast, the absence of alignment yields inferior results, particularly for classes with few data. Compared to classes with abundant data, those with fewer instances often experience misclassification, with minority class data being inaccurately labeled as belonging to majority classes. This starkly underscores the significance of our global alignment strategy in enhancing both the performance and fairness of the FL system.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.06067/assets/figures/confusion_m.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="538" height="216" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The comparative analysis of aligned and non-aligned models with normalized confusion matrices.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F4.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Discussion</h4>

<section id="S4.SS3.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy</h5>

<div id="S4.SS3.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px1.p1.1" class="ltx_p">: At the forefront of federated learning challenges is the protection of user privacy. Our strategy sidesteps the conventional requirement for clients to send gradients back to the server, as seen in methods like CReFF <cite class="ltx_cite ltx_citemacro_citep">(Shang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> and CLIP2FL <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>. This aspect is vital because the transmission of gradients could enable the server to perform reverse engineering attacks <cite class="ltx_cite ltx_citemacro_citep">(Geiping etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019</a>; Haim etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, potentially endangering client data confidentiality. By eliminating this step, our method diminishes the likelihood of leaking sensitive client information, promoting a more secure and privacy-centric learning environment.</p>
</div>
</section>
<section id="S4.SS3.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Computational Efficiency</h5>

<div id="S4.SS3.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px2.p1.1" class="ltx_p">: Our approach also stands out for its computational economy. Contrary to approaches like CLIP2FL, which necessitate deploying sizable multimodal models such as CLIP on client devicesâ€”demanding significant memory and potentially being unfeasible for edge devices with limited resourcesâ€”our method positions the MLLM solely on the server side. At the client level, we deploy only the compact FL models. This resolution not only addresses memory constraints but also reduces the time and energy expenditure associated with federated local training. Consequently, our framework is rendered more practical and appealing for an extensive array of devices, particularly those with restricted storage capacities.</p>
</div>
</section>
<section id="S4.SS3.SSS3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Compatibility</h5>

<div id="S4.SS3.SSS3.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px3.p1.1" class="ltx_p">: Our approach stands out for its adaptability, unlike specific methodologies like CReFF and CLIP2FL that impose unique requirements on federated local training and global aggregation. Our framework can be compatible with a wide array of existing FL algorithms. This includes, but is not limited to, client selection strategies and various techniques aimed at further enhancing client privacy protection. This flexibility ensures that our method can be seamlessly integrated into diverse FL environments and fully leverage the accumulated advancements from previous federated learning research.
For a more intuitive comparison, please refer to the Table <a href="#S1.T1" title="Table 1 â€£ 1. Introduction â€£ MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> highlighting the advantages of our method.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To overcome the challenges of federated learning in the context of heterogeneous and long-tailed data distributions, we introduced a novel framework, MLLM-FL. This framework is structured around three core stages: global pretraining, federated fine-tuning, and global alignment. This marks the inaugural integration of Multimodal Large Language Models (MLLMs) into an FL system. Leveraging the strong multimodal capacities of MLLMs, our approach taps into the vast yet previously underutilized reservoir of open-source data available online, alongside substantial server-side computational resources. Crucially, our methodology does not compromise privacy nor impose additional computational demands on client devices. Experimental evidence verifies the efficacy of our framework, paving the way for future research to explore a broader array of multimodal tasks beyond image-text interactions, thereby enhancing FL performance across diverse multimodality challenges.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Chao (2020)</span>
<span class="ltx_bibblock">
Hong-You Chen and Wei-Lun Chao. 2020.

</span>
<span class="ltx_bibblock">Fedbe: Making bayesian model ensemble applicable to federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.01974</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han-Wei Shen, and Wei-Lun Chao. 2022.

</span>
<span class="ltx_bibblock">On pre-training for federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.11488</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. 2020.

</span>
<span class="ltx_bibblock">Fedhealth: A federated transfer learning framework for wearable healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em> 35, 4 (2020), 83â€“93.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, JosephÂ E. Gonzalez, Ion Stoica, and EricÂ P. Xing. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://vicuna.lmsys.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vicuna.lmsys.org</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07919</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui etÂ al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019.

</span>
<span class="ltx_bibblock">Class-balanced loss based on effective number of samples. <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 9268â€“9277.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Chun-Mei Feng, Kai Yu, Nian Liu, Xinxing Xu, Salman Khan, and Wangmeng Zuo. 2023.

</span>
<span class="ltx_bibblock">Towards Instance-adaptive Inference for Federated Learning. <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 23287â€“23296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, etÂ al<span id="bib.bib9.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama-adapter v2: Parameter-efficient visual instruction model.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.15010</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah DrÃ¶ge, and Michael Moeller. 2020.

</span>
<span class="ltx_bibblock">Inverting gradients-how easy is it to break privacy in federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 16937â€“16947.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haim etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. 2022.

</span>
<span class="ltx_bibblock">Reconstructing training data from trained neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 35 (2022), 22911â€“22924.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao etÂ al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, KevinÂ J Liang, Changyou Chen, and LawrenceÂ Carin Duke. 2021.

</span>
<span class="ltx_bibblock">Towards Fair Federated Learning With Zero-Shot Data Augmentation. <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>. 3310â€“3319.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, etÂ al<span id="bib.bib13.3.1" class="ltx_text">.</span> 2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em> 2, 7 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Tzu-MingÂ Harry Hsu, Hang Qi, and Matthew Brown. 2019.

</span>
<span class="ltx_bibblock">Measuring the effects of non-identical data distribution for federated visual classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06335</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hua Huang, Fanhua Shang, Yuanyuan Liu, and Hongying Liu. 2021.

</span>
<span class="ltx_bibblock">Behavior mimics distribution: Combining individual and group behaviors for federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.12300</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. 2023.

</span>
<span class="ltx_bibblock">Rethinking federated learning with domain shift: A prototype view. <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE, 16312â€“16322.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuqi Jia, Saeed Vahidian, Jingwei Sun, Jianyi Zhang, Vyacheslav Kungurtsev, NeilÂ Zhenqiang Gong, and Yiran Chen. 2023.

</span>
<span class="ltx_bibblock">Unlocking the potential of federated learning: The symphony of dataset distillation via deep generative latents.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.01537</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, and Xiaolin Hu. 2019.

</span>
<span class="ltx_bibblock">Knowledge distillation via route constrained optimization. <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 1345â€“1354.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz etÂ al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, HÂ Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Bennis, ArjunÂ Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, etÂ al<span id="bib.bib19.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.4.1" class="ltx_emph ltx_font_italic">Foundations and TrendsÂ® in Machine Learning</em> 14, 1â€“2 (2021), 1â€“210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang etÂ al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. 2019.

</span>
<span class="ltx_bibblock">Decoupling representation and classifier for long-tailed recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.09217</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky etÂ al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> ([n.â€‰d.])</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. [n.â€‰d.].

</span>
<span class="ltx_bibblock">CIFAR-10 (Canadian Institute for Advanced Research).

</span>
<span class="ltx_bibblock">([n.â€‰d.]).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://www.cs.toronto.edu/~kriz/cifar.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.cs.toronto.edu/~kriz/cifar.html</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Bo Li, MikkelÂ N Schmidt, TommyÂ S AlstrÃ¸m, and SebastianÂ U Stich. 2022.

</span>
<span class="ltx_bibblock">On the effectiveness of partial variance reduction in federated learning with heterogeneous data.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.02191</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023a.

</span>
<span class="ltx_bibblock">Videochat: Chat-centric video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06355</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tian Li, AnitÂ Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2018.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.06127</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, SebastianÂ U Stich, and Martin Jaggi. 2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 2351â€“2363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and YongÂ Jae Lee. 2023.

</span>
<span class="ltx_bibblock">Visual Instruction Tuning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and StellaÂ X Yu. 2019.

</span>
<span class="ltx_bibblock">Large-scale long-tailed recognition in an open world. <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2537â€“2546.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. 2021.

</span>
<span class="ltx_bibblock">No fear of heterogeneity: Classifier calibration for federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 5972â€“5984.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and FahadÂ Shahbaz Khan. 2023.

</span>
<span class="ltx_bibblock">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05424</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahlool and Abed (2022)</span>
<span class="ltx_bibblock">
DhurghamÂ Hassan Mahlool and MohammedÂ Hamzah Abed. 2022.

</span>
<span class="ltx_bibblock">A Comprehensive Survey on Federated Learning: Concept and Applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Mobile Computing and Sustainable Informatics: Proceedings of ICMCSI 2022</em> (2022), 539â€“553.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and BlaiseÂ Aguera y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized data. <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>. PMLR, 1273â€“1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirzadeh etÂ al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
SeyedÂ Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020.

</span>
<span class="ltx_bibblock">Improved knowledge distillation via teacher assistant. <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, Vol.Â 34. 5191â€“5198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock">Introducing ChatGPT.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. 2019.

</span>
<span class="ltx_bibblock">Relational knowledge distillation. <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 3967â€“3976.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al<span id="bib.bib36.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. <em id="bib.bib36.4.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 8748â€“8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu etÂ al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
AnitÂ Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith. 2018.

</span>
<span class="ltx_bibblock">Federated optimization for heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.06127</em> 1, 2 (2018), 3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarkar etÂ al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Dipankar Sarkar, Ankur Narang, and Sumit Rai. 2020.

</span>
<span class="ltx_bibblock">Fed-focal loss for imbalanced data classification in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.06283</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang etÂ al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xinyi Shang, Yang Lu, Gang Huang, and Hanzi Wang. 2022.

</span>
<span class="ltx_bibblock">Federated learning on heterogeneous and long-tailed data via classifier re-training with federated features.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.13399</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiangming Shi, Shanshan Zheng, Xiangbo Yin, Yang Lu, Yuan Xie, and Yanyun Qu. 2023.

</span>
<span class="ltx_bibblock">CLIP-guided Federated Learning on Heterogeneous and Long-Tailed Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.08648</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sui etÂ al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun. 2020.

</span>
<span class="ltx_bibblock">FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction. <em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>. 2118â€“2128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team etÂ al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, AndrewÂ M Dai, Anja Hauth, etÂ al<span id="bib.bib42.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and HÂ Vincent Poor. 2020a.

</span>
<span class="ltx_bibblock">Tackling the objective inconsistency problem in heterogeneous federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.07481</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. 2020b.

</span>
<span class="ltx_bibblock">Addressing Class Imbalance in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.06217</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Visual chatgpt: Talking, drawing and editing with visual foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04671</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Chun-Mei Feng Bangjun LiÂ Xinxing Xu, Yong Liu, and Huazhu FuÂ Wangmeng Zuo. 2023.

</span>
<span class="ltx_bibblock">Learning Federated Visual Prompt in Null Space for MRI Reconstruction.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.16181</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, and Hua Qian. 2020.

</span>
<span class="ltx_bibblock">Federated learning with class imbalance reduction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2011.11266Â [cs.LG]

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Qian Yang, Jianyi Zhang, Weituo Hao, GregoryÂ P. Spell, and Lawrence Carin. 2021.

</span>
<span class="ltx_bibblock">FLOP: Federated Learning on Medical Datasets using Partial Networks. <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em> (Virtual Event, Singapore) <em id="bib.bib48.4.2" class="ltx_emph ltx_font_italic">(KDD â€™21)</em>. Association for Computing Machinery, New York, NY, USA, 3845â€“3853.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3447548.3467185" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3447548.3467185</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023.

</span>
<span class="ltx_bibblock">Mm-react: Prompting chatgpt for multimodal reasoning and action.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11381</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. 2021.

</span>
<span class="ltx_bibblock">Parameterized knowledge transfer for personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 10092â€“10104.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jianyi Zhang, Ang Li, Minxue Tang, Jingwei Sun, Xiang Chen, Fan Zhang, Changyou Chen, Yiran Chen, and Hai Li. 2023a.

</span>
<span class="ltx_bibblock">Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction. <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang, Changyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi, etÂ al<span id="bib.bib52.3.1" class="ltx_text">.</span> 2023b.

</span>
<span class="ltx_bibblock">Reaugkd: Retrieval-augmented knowledge distillation for pre-trained language models. <em id="bib.bib52.4.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>. 1128â€“1136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. 2024.

</span>
<span class="ltx_bibblock">Towards Building The Federatedgpt: Federated Instruction Tuning. <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. 6915â€“6919.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447454" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP48485.2024.10447454</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10592</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han. 2019.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.06066" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.06067" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.06067">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.06067" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.06068" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 21:22:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
