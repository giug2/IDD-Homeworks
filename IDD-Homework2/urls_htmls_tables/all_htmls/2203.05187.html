<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.05187] Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection</title><meta property="og:description" content="The food packaging industry handles an immense variety of food products with wide-ranging shapes and sizes, even within one kind of food. Menus are also diverse and change frequently, making automation of pick-and-plac…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.05187">

<!--Generated on Mon Mar 11 10:04:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Cluttered Food Grasping with Adaptive Fingers 
<br class="ltx_break">and Synthetic-Data Trained Object Detection
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Avinash Ummadisingu<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">†</span></sup>,
Kuniyuki Takahashi<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">†</span></sup>,
Naoki Fukaya<sup id="id7.7.id3" class="ltx_sup"><span id="id7.7.id3.1" class="ltx_text ltx_font_italic">†</span></sup>
</span><span class="ltx_author_notes"><sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">†</span></sup> A. Ummadisingu, K. Takahashi, and N. Fukaya are associated with Preferred Networks, Inc.
 {ummavi, takahashi, fukaya}@preferred.jp</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">The food packaging industry handles an immense variety of food products with wide-ranging shapes and sizes, even within one kind of food. Menus are also diverse and change frequently, making automation of pick-and-place difficult.
A popular approach to bin-picking is to first identify each piece of food in the tray by using an instance segmentation method.
However, human annotations to train these methods are unreliable and error-prone since foods are packed close together with unclear boundaries and visual similarity making separation of pieces difficult.
To address this problem, we propose a method that trains purely on synthetic data and successfully transfers to the real world using sim2real methods by creating datasets of filled food trays using high-quality 3d models of real pieces of food for the training instance segmentation models.
Another concern is that foods are easily damaged during grasping.
We address this by introducing two additional methods- a novel adaptive finger mechanism to passively retract when a collision occurs, and a method to filter grasps that are likely to cause damage to neighbouring pieces of food during a grasp.
We demonstrate the effectiveness of the proposed method on several kinds of real foods.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Convenience store meals are ubiquitous and continue to see fast-growing popularity globally. Despite the growing need for automation in the food-packing industry, its adoption sees limited success due to numerous factors. A major impediment to automation is the remarkable diversity in the types of foods handled and significant variance in shapes and sizes, even between pieces of the same food. To further complicate the issue, menus frequently change due to seasonality or customer interest. Custom-designed hardware and software for handling individual foodstuff see limited applicability due to the need to adapt to novel foods quickly.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A popular approach to bin-picking is using object detection algorithms to identify the objects in the bin, derive a grasp pose, and then plan and execute it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, collecting large datasets required to train these object detection methods for food is challenging due to the high costs of collecting data and annotations, as well as dealing with mistakes made due to the high visual similarity and difficulty in obtaining clear boundaries between pieces packed together in a bin. In addition, foods are fragile and easily damaged by interaction and change in behavior over time due to environmental factors (temperature, humidity) as well as loss of water content and spoilage.
It makes the gathering of large datasets of real food challenging and often requires techniques to learn from limited samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this problem, we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">1)</span> using synthetic data generated in simulation using highly realistic 3d models of real food to train an instance segmentation model. By applying ideas of sim2real and domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, we enable the model trained on synthetic data to transfer to the real world without additional training, overcoming the cost and problems faced in collecting human annotations (Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Another issue that sets apart the grasping of food from existing methods is their fragility.
Foods may be soft, delicate and easily damaged if grasped with excessive force incurring an unacceptable cost. This is especially likely since the grasping is done from trays packed full of food. In this work, we propose two methods to alleviate this- <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">2)</span> The introduction of a novel adaptive finger mechanism that passively retracts when it encounters the surface of the food, and <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">3)</span> a grasp filtering heuristic that filters risky grasp candidates that are likely to collide with neighbouring foods and damage them.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.05187/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Experiment Setup: Robot setup for the experiments with instance segmentation detections trained on synthetic images and used for grasping in the real-world.</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2203.05187/assets/fig/simulation_real_crop.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="837" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Real foods used for experiments and their corresponding 3d models the segmentation model was trained with.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Instance Segmentation of food</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There exist several labelled datasets for foods, often under the field of food computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Existing food image segmentation datasets, such as Foood201-Segmented <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, UECFoodPix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> provide instance-level labels but are not at the granularity required for our application (a plate full of pieces of chicken would be identified as one instance).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The gathering of datasets of the size required for deep learning models are unavoidably labor-intensive tasks.
Prior works have explored the use of simulation or synthetic data for training semantic or instance segmentation models in numerous application such as crop seed detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and even food <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, they use simple shapes and focus on detecting food areas like the datasets above.
Advances in photogrammetry and techniques such as differentiable rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> allow for easy capture and generation of near photo-realistic models of objects from a handful of images, eliminating the need for specialist knowledge to create them.
In our work, we use these techniques for quick and easy capture of novel foods that can be used in simulation to generate synthetic data to identify individual pieces of food.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Food Grasping from Hardware Perspective</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Rapid R&amp;D and adoption of soft grippers that reduce damage to objects or its surroundings is ongoing.
The gripper fingers are soft and do not require precise control, but adjust with the food passively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
However, the fingers tend to be thick, which makes it difficult to grasp food that is tightly packed in a tray.
Grasping by suction can be done even in a cramped setting, but is challenging to use due to concerns of hygiene and contamination of the vacuum hose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Prior work has also looked at grasping a single object with a needle gripper, but the needle is inserted into the food, raising two concerns- damage to the food and hygiene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
In our work, we develop a parallel gripper with an adaptive finger mechanism that passively retracts when it encounters the surface of the food to prevent damage.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Collision Avoidance in Grasping</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Grasping in clutter makes the task of collision avoidance significantly harder. This is of particular importance to food since collisions could cause undue damage to foodstuff.
There are three main approaches: 1) heuristic-based, 2) sampling-based, and 3) data-driven.
1) Heuristic-based approaches such as finding the highest grasp points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> are simple but powerful if the appropriate one is selected for the task.
2) Sampling-based approaches evaluate grasp candidates for collision by using analytical methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or by using task-and-motion planning libraries like OpenRAVE for collision checking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. These approaches are computationally expensive to do at the resolution needed to grasp small pieces of food.
3) Data-driven approaches are popular in robotics with deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
These methods require a large amount of data or learn in simulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. However, collection of a large manipulation dataset of food is not possible and realistic simulation of soft-bodied objects like food is still challenging to apply.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Our approach belongs to 1) heuristic-based.
The method closest to our own is that of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> where they represent a gripper model by two mask images representing a contact region and a collision region (which should be free of objects). However, they focus on rigid objects with regular shapes and are able to enforce collision regions to be completely empty. However, in dealing with soft, irregular foodstuffs, collisions to gently push food are unavoidable for a successful grasp.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2203.05187/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="830" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Grasping method that avoids damaging food to be grasped and surrounding pieces. The method is composed of instance segmentation, grasp detection, and grasp filtering. The green circles represent the best-fit elliptical mask for each identified instance. The blue rectangles represent candidate contact points between the gripper’s fingers and the food, and are placed around the ends of the minor axis of the ellipse.</span></figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2203.05187/assets/fig/solid_slide_finger_all.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="420" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Schematic diagram of the adaptive finger with a passive retraction mechanism to avoid damaging food on insertion.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our proposed method to grasp a piece of the target food from a cluttered tray consists of the following components:
A) Generate <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">synthetic images</em> and train with them, B) <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">grasp filtering</em> for collision avoidance during grasping, and C) grasp using <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">adaptive fingers</em>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Synthetic Image Generation &amp; Training</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we describe how we create a training dataset to train an instance segmentation model capable of detecting individual pieces of food in a cluttered tray.
We first create a high-quality 3d model from several photographs of each piece of food using Preferred Networks, Inc.’s technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
The details of the 3d model generation are outside the scope of this work.
Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the real foods and their corresponding 3d models.
Highly uniform foods, such as sausage, can easily be represented by a single model, while we prepare the capture of multiple pieces for foods that show variance in shapes and sizes, such as fried chicken.
We model a tray in simulation of the same size as the one shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and drop a number of these captured 3D models into the tray to generate training data for instance segmentation.
We vary the number of pieces generated and initialize them at random locations over the tray.
In doing this, we depend on the physics engine to prevent the models from conjoining and resolving collisions. Detailed parameters are described in Section <a href="#S4.SS1" title="IV-A Model training with Synthetic Data ‣ IV Experiment Setup ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Although we can capture high-quality models of the foods and tune the simulation to be as realistic as possible, photorealism to the level needed to transfer to the real world is unattainable.
To improve the model’s generalization, we adopt concepts of domain randomization from sim2real literature to randomly vary simulation parameters: the tray color, scale of the model, lighting parameters (direction, ambient light, shadows), and direction of the camera.
Using domain randomization allows the model to traverse the reality gap and be used directly for real-world predictions without the need to additionally fine-tune with actual food data.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Grasp Detection &amp; Filtering</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this section, we explain the method used to find a grasp point and pose for grasping a piece of food while avoiding damage to it or its neighbours when the gripper is inserted.
The method consists of three steps; 1) Instance segmentation, 2) grasp detection, and 3) grasp filtering (Fig. <a href="#S2.F3" title="Figure 3 ‣ II-C Collision Avoidance in Grasping ‣ II Related Work ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Instance Segmentation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">We use an instance segmentation model trained using images generated as per the previous section. We pass an RGB image of the food tray as input to the model and obtain a mask for each piece of food. <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">The use of depth for segmentation was omitted due to difficulty in adequately matching generated depth to reality and the lack of improved performance at the cost of inference speed.</span></p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>Grasp Detection</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.12" class="ltx_p">We employ a method of ellipse fitting to segmentation masks to identify a grasp.
The choice to use a best-fit ellipse to represent the food area instead of a typical minimum area rectangle, was motivated by the irregularity in the shapes of the pieces of food.
For each detected mask (which should, ideally, perfectly correspond to a piece of food), we identify its best fit ellipse (not bounding ellipse).
We use the parameterization of this best fit ellipse, defined by the 5-tuple <math id="S3.SS2.SSS2.p1.1.m1.5" class="ltx_Math" alttext="e=\{\mathcal{X},\mathcal{Y},\Theta,\mathcal{A},\mathcal{B}\}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.5a"><mrow id="S3.SS2.SSS2.p1.1.m1.5.6" xref="S3.SS2.SSS2.p1.1.m1.5.6.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.5.6.2" xref="S3.SS2.SSS2.p1.1.m1.5.6.2.cmml">e</mi><mo id="S3.SS2.SSS2.p1.1.m1.5.6.1" xref="S3.SS2.SSS2.p1.1.m1.5.6.1.cmml">=</mo><mrow id="S3.SS2.SSS2.p1.1.m1.5.6.3.2" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.5.6.3.2.1" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml">{</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">𝒳</mi><mo id="S3.SS2.SSS2.p1.1.m1.5.6.3.2.2" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.1.m1.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.cmml">𝒴</mi><mo id="S3.SS2.SSS2.p1.1.m1.5.6.3.2.3" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS2.p1.1.m1.3.3" xref="S3.SS2.SSS2.p1.1.m1.3.3.cmml">Θ</mi><mo id="S3.SS2.SSS2.p1.1.m1.5.6.3.2.4" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.1.m1.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.cmml">𝒜</mi><mo id="S3.SS2.SSS2.p1.1.m1.5.6.3.2.5" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.1.m1.5.5" xref="S3.SS2.SSS2.p1.1.m1.5.5.cmml">ℬ</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.5.6.3.2.6" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.5b"><apply id="S3.SS2.SSS2.p1.1.m1.5.6.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.6"><eq id="S3.SS2.SSS2.p1.1.m1.5.6.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.6.1"></eq><ci id="S3.SS2.SSS2.p1.1.m1.5.6.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.6.2">𝑒</ci><set id="S3.SS2.SSS2.p1.1.m1.5.6.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.6.3.2"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝒳</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2">𝒴</ci><ci id="S3.SS2.SSS2.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3">Θ</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4">𝒜</ci><ci id="S3.SS2.SSS2.p1.1.m1.5.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.5">ℬ</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.5c">e=\{\mathcal{X},\mathcal{Y},\Theta,\mathcal{A},\mathcal{B}\}</annotation></semantics></math>, to derive the grasp parameters of the parallel gripper where <math id="S3.SS2.SSS2.p1.2.m2.2" class="ltx_Math" alttext="(\mathcal{X},\mathcal{Y})" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.2a"><mrow id="S3.SS2.SSS2.p1.2.m2.2.3.2" xref="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.2.m2.2.3.2.1" xref="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">𝒳</mi><mo id="S3.SS2.SSS2.p1.2.m2.2.3.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.2.m2.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.2.cmml">𝒴</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.2.m2.2.3.2.3" xref="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.2b"><interval closure="open" id="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.3.2"><ci id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">𝒳</ci><ci id="S3.SS2.SSS2.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2">𝒴</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.2c">(\mathcal{X},\mathcal{Y})</annotation></semantics></math> is the center of the ellipse, <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mi mathvariant="normal" id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><ci id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">\Theta</annotation></semantics></math> its angle of rotation and <math id="S3.SS2.SSS2.p1.4.m4.2" class="ltx_Math" alttext="(\mathcal{A},\mathcal{B})" display="inline"><semantics id="S3.SS2.SSS2.p1.4.m4.2a"><mrow id="S3.SS2.SSS2.p1.4.m4.2.3.2" xref="S3.SS2.SSS2.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.4.m4.2.3.2.1" xref="S3.SS2.SSS2.p1.4.m4.2.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.4.m4.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml">𝒜</mi><mo id="S3.SS2.SSS2.p1.4.m4.2.3.2.2" xref="S3.SS2.SSS2.p1.4.m4.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.4.m4.2.2" xref="S3.SS2.SSS2.p1.4.m4.2.2.cmml">ℬ</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.4.m4.2.3.2.3" xref="S3.SS2.SSS2.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m4.2b"><interval closure="open" id="S3.SS2.SSS2.p1.4.m4.2.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.2.3.2"><ci id="S3.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1">𝒜</ci><ci id="S3.SS2.SSS2.p1.4.m4.2.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.2.2">ℬ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m4.2c">(\mathcal{A},\mathcal{B})</annotation></semantics></math> representing the length of its minor and major axis respectively.
We adopt the grasp parametrization of a parallel gripper with the 5-tuple <math id="S3.SS2.SSS2.p1.5.m5.5" class="ltx_Math" alttext="g=\{x,y,\theta,h,w\}" display="inline"><semantics id="S3.SS2.SSS2.p1.5.m5.5a"><mrow id="S3.SS2.SSS2.p1.5.m5.5.6" xref="S3.SS2.SSS2.p1.5.m5.5.6.cmml"><mi id="S3.SS2.SSS2.p1.5.m5.5.6.2" xref="S3.SS2.SSS2.p1.5.m5.5.6.2.cmml">g</mi><mo id="S3.SS2.SSS2.p1.5.m5.5.6.1" xref="S3.SS2.SSS2.p1.5.m5.5.6.1.cmml">=</mo><mrow id="S3.SS2.SSS2.p1.5.m5.5.6.3.2" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.5.m5.5.6.3.2.1" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml">{</mo><mi id="S3.SS2.SSS2.p1.5.m5.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.cmml">x</mi><mo id="S3.SS2.SSS2.p1.5.m5.5.6.3.2.2" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.5.m5.2.2" xref="S3.SS2.SSS2.p1.5.m5.2.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.5.m5.5.6.3.2.3" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.5.m5.3.3" xref="S3.SS2.SSS2.p1.5.m5.3.3.cmml">θ</mi><mo id="S3.SS2.SSS2.p1.5.m5.5.6.3.2.4" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.5.m5.4.4" xref="S3.SS2.SSS2.p1.5.m5.4.4.cmml">h</mi><mo id="S3.SS2.SSS2.p1.5.m5.5.6.3.2.5" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.5.m5.5.5" xref="S3.SS2.SSS2.p1.5.m5.5.5.cmml">w</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.5.m5.5.6.3.2.6" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.5.m5.5b"><apply id="S3.SS2.SSS2.p1.5.m5.5.6.cmml" xref="S3.SS2.SSS2.p1.5.m5.5.6"><eq id="S3.SS2.SSS2.p1.5.m5.5.6.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.5.6.1"></eq><ci id="S3.SS2.SSS2.p1.5.m5.5.6.2.cmml" xref="S3.SS2.SSS2.p1.5.m5.5.6.2">𝑔</ci><set id="S3.SS2.SSS2.p1.5.m5.5.6.3.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.5.6.3.2"><ci id="S3.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1">𝑥</ci><ci id="S3.SS2.SSS2.p1.5.m5.2.2.cmml" xref="S3.SS2.SSS2.p1.5.m5.2.2">𝑦</ci><ci id="S3.SS2.SSS2.p1.5.m5.3.3.cmml" xref="S3.SS2.SSS2.p1.5.m5.3.3">𝜃</ci><ci id="S3.SS2.SSS2.p1.5.m5.4.4.cmml" xref="S3.SS2.SSS2.p1.5.m5.4.4">ℎ</ci><ci id="S3.SS2.SSS2.p1.5.m5.5.5.cmml" xref="S3.SS2.SSS2.p1.5.m5.5.5">𝑤</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.5.m5.5c">g=\{x,y,\theta,h,w\}</annotation></semantics></math>, which is a common parallel gripper grasp parametrization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
The grasping point candidate’s <math id="S3.SS2.SSS2.p1.6.m6.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS2.SSS2.p1.6.m6.2a"><mrow id="S3.SS2.SSS2.p1.6.m6.2.3.2" xref="S3.SS2.SSS2.p1.6.m6.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.6.m6.2.3.2.1" xref="S3.SS2.SSS2.p1.6.m6.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS2.p1.6.m6.1.1" xref="S3.SS2.SSS2.p1.6.m6.1.1.cmml">x</mi><mo id="S3.SS2.SSS2.p1.6.m6.2.3.2.2" xref="S3.SS2.SSS2.p1.6.m6.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.6.m6.2.2" xref="S3.SS2.SSS2.p1.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.6.m6.2.3.2.3" xref="S3.SS2.SSS2.p1.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.6.m6.2b"><interval closure="open" id="S3.SS2.SSS2.p1.6.m6.2.3.1.cmml" xref="S3.SS2.SSS2.p1.6.m6.2.3.2"><ci id="S3.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p1.6.m6.1.1">𝑥</ci><ci id="S3.SS2.SSS2.p1.6.m6.2.2.cmml" xref="S3.SS2.SSS2.p1.6.m6.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.6.m6.2c">(x,y)</annotation></semantics></math> becomes the center of the ellipse <math id="S3.SS2.SSS2.p1.7.m7.2" class="ltx_Math" alttext="(\mathcal{X},\mathcal{Y})" display="inline"><semantics id="S3.SS2.SSS2.p1.7.m7.2a"><mrow id="S3.SS2.SSS2.p1.7.m7.2.3.2" xref="S3.SS2.SSS2.p1.7.m7.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.7.m7.2.3.2.1" xref="S3.SS2.SSS2.p1.7.m7.2.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.7.m7.1.1" xref="S3.SS2.SSS2.p1.7.m7.1.1.cmml">𝒳</mi><mo id="S3.SS2.SSS2.p1.7.m7.2.3.2.2" xref="S3.SS2.SSS2.p1.7.m7.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.7.m7.2.2" xref="S3.SS2.SSS2.p1.7.m7.2.2.cmml">𝒴</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.7.m7.2.3.2.3" xref="S3.SS2.SSS2.p1.7.m7.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.7.m7.2b"><interval closure="open" id="S3.SS2.SSS2.p1.7.m7.2.3.1.cmml" xref="S3.SS2.SSS2.p1.7.m7.2.3.2"><ci id="S3.SS2.SSS2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p1.7.m7.1.1">𝒳</ci><ci id="S3.SS2.SSS2.p1.7.m7.2.2.cmml" xref="S3.SS2.SSS2.p1.7.m7.2.2">𝒴</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.7.m7.2c">(\mathcal{X},\mathcal{Y})</annotation></semantics></math>.
The width of the minor axis of this ellipse <math id="S3.SS2.SSS2.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S3.SS2.SSS2.p1.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.8.m8.1.1" xref="S3.SS2.SSS2.p1.8.m8.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.8.m8.1b"><ci id="S3.SS2.SSS2.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS2.p1.8.m8.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.8.m8.1c">\mathcal{A}</annotation></semantics></math> becomes the grasp width <math id="S3.SS2.SSS2.p1.9.m9.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS2.SSS2.p1.9.m9.1a"><mi id="S3.SS2.SSS2.p1.9.m9.1.1" xref="S3.SS2.SSS2.p1.9.m9.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.9.m9.1b"><ci id="S3.SS2.SSS2.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS2.p1.9.m9.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.9.m9.1c">w</annotation></semantics></math> and the angle of rotation of the ellipse <math id="S3.SS2.SSS2.p1.10.m10.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S3.SS2.SSS2.p1.10.m10.1a"><mi mathvariant="normal" id="S3.SS2.SSS2.p1.10.m10.1.1" xref="S3.SS2.SSS2.p1.10.m10.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.10.m10.1b"><ci id="S3.SS2.SSS2.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS2.p1.10.m10.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.10.m10.1c">\Theta</annotation></semantics></math> gives us the gripper rotation <math id="S3.SS2.SSS2.p1.11.m11.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.SSS2.p1.11.m11.1a"><mi id="S3.SS2.SSS2.p1.11.m11.1.1" xref="S3.SS2.SSS2.p1.11.m11.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.11.m11.1b"><ci id="S3.SS2.SSS2.p1.11.m11.1.1.cmml" xref="S3.SS2.SSS2.p1.11.m11.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.11.m11.1c">\theta</annotation></semantics></math>.
The remaining grasp parameter <math id="S3.SS2.SSS2.p1.12.m12.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS2.SSS2.p1.12.m12.1a"><mi id="S3.SS2.SSS2.p1.12.m12.1.1" xref="S3.SS2.SSS2.p1.12.m12.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.12.m12.1b"><ci id="S3.SS2.SSS2.p1.12.m12.1.1.cmml" xref="S3.SS2.SSS2.p1.12.m12.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.12.m12.1c">h</annotation></semantics></math> is calculated from the median height of the <em id="S3.SS2.SSS2.p1.12.1" class="ltx_emph ltx_font_italic">food area</em> defined by the enclosing best fit ellipse.
These height values are obtained from the RGB-D camera.
To this median height, we add a fixed offset for each food determined experimentally over several trials.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Grasp Filtering</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">From the grasp candidates detected by 2) grasp detection, we apply 3) grasp filtering to filter out candidates that are likely to damage the food piece to be grasped and the surrounding pieces, as well as avoid unintentionally grasping a neighbouring piece, which, depending on the system’s application area might be unacceptable.
We represent the <em id="S3.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">contact area</em> with the gripper’s fingers as two rectangular masks of approximately the same size as the fingers of the gripper.
We then retain only the grasp candidates whose median height of both of these contact areas is lower than the median height of the food area thereby eliminating candidates that cause significant collision with the fingers.
We note that more stringent rules enforcing the entire area be lower or empty would lead to less collision, but some deformable foods may fill up existing free space leading to the filtering step being too harsh in its elimination. In this case, collisions are unavoidable and we need to depend on other means to minimize damage.
From the remaining candidates, we arbitrarily choose the one with the highest median height as the grasp target.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Adaptive finger</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">This section describes the mechanism of the proposed adaptive finger to grasp fragile foods in a cluttered tray.
Foods may vary in shape and size, and their surfaces are often uneven. This surface irregularity is further exaggerated when a large volume of them is placed in a tray.
Many foods are fragile and may be easily scratched or damaged during a grasp. Foods such as fried chicken with an outer coating are especially prone to having their surfaces broken.
When grasping such foods with a typical gripper with solid tongs-inspired fingers (which we call <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">fixed fingers</em> in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), a slight deviation in localization, sensor, or robot control may cause the grasp to fail or the food to be damaged.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In order to address this issue, we have developed a parallel gripper type with an adaptive function to conform to the shape of the food for the fingers (which we call <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">adaptive fingers</em> in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> &amp; Fig. <a href="#S2.F4" title="Figure 4 ‣ II-C Collision Avoidance in Grasping ‣ II Related Work ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a)). This adaptive function allows the gripper’s fingers to passively retract when they make contact with the surface of the food thereby avoiding damage caused by the insertion into the food surface.
The fingers are constructed to retract a total of 22.5mm with the spring providing a certain restoring force to the finger.
The contact force of fingertip at maximum shrink is 4.1N (Fig. <a href="#S2.F4" title="Figure 4 ‣ II-C Collision Avoidance in Grasping ‣ II Related Work ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b)) <span id="S3.SS3.p2.1.2" class="ltx_text" style="color:#000000;">and chosen such that is unable to penetrate the surface of foods like fried chicken but still be inserted into grooves between pieces</span>.
As a result, even if the foods to be grasped are densely packed, the fingertips enter the gap while sliding over the surface of the foods to be grasped, so that it is possible to grasp fragile foods without damaging them (Fig. <a href="#S2.F4" title="Figure 4 ‣ II-C Collision Avoidance in Grasping ‣ II Related Work ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (c)).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiment Setup</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Model training with Synthetic Data</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To create an instance segmentation model for grasping each kind of food, we first collect 3d models of the food (Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
We do this by capturing several images and combine them into a high-quality 3d model of the food.
For some foods like broccoli and fried chicken with remarkable diversity amongst pieces, we capture more than one model in an attempt to improve generalization.
To further decrease the reality gap, we create a simplified tray modelled after the industrial food tray shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and position the synthetic camera in a realistic way.
We use the Python bindings of the Bullet physics simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and employ ideas of domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to generate images of size <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="600{\times}600" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">600</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">600</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">600{\times}600</annotation></semantics></math> to use as training data for the segmentation model.
Pieces of food are dropped from some height over the tray and allowed to fall in order to generate a number of unique, but reasonably realistic images.
We vary a number of simulation parameters to improve generalization, including the number of pieces dropped (10-60), their sizes (0.7x-1.1x), the color of the tray, camera directions and lighting conditions (speculative coefficients, light directions, shadows).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Depending on the complexity of the 3d model, the time required for the physics engine used to generate plausible synthetic data may vary significantly as collision detection and handling becomes more expensive. The average time to generate a single image <span id="S4.SS1.p2.1.1" class="ltx_text" style="color:#000000;">(using the same machines used to train models specified below)</span> varies from 10 sec for simple models like sausage, meatball and taro, 14-18 sec for more complex models like fried chicken and mushroom, to 93-459 sec for complex models like gyoza and broccoli.
By breaking generation up into small batches of 10 and deploying them in parallel, we effectively generate the 1200 samples required for each kind of food in around 1 hour.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We train the instance segmentation model using the 1200 samples generated (1000 to train and 200 as the validation set). Additionally, during the training of the instance segmentation model, we apply a number of data augmentations including randomly flipping the images (and their masks) and apply Median Blur to the input images to make the model less reliant on perfectly visible borders.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">For the instance segmentation model, we use the highly popular and versatile Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> with a resnet50 backbone initialized with weights pretrained on the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
Therefore, the instance segmentation model classifies each piece as either background or belonging to an instance of food.
We trained the networks on a machine equipped with 384 GB RAM, an Intel(R) Xeon(R) Gold 6254 CPU @ 3.10GHz, and NVIDIA V100 with 32GB of memory.
Training for each model (initialized randomly for each food) took roughly 24 hours.
Our experiments for inference with trained models were performed on a machine equipped with 31.3 GB RAM, an Intel Core i7-7700K CPU, and GeForce GTX 1080 Ti. Inference with the trained model during experiments takes about 0.3 sec.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Robot Setup</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our robotic system, shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, consists of a Sawyer 7-DOF robotic arm equipped with a self-designed two-fingered parallel gripper driven by a servomotor (Dynamixel XM430-W350-R).
We use two custom designed end-effectors whose general shape was modelled after tongs commonly used to serve food.
As seen in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we call the two tongs-inspired end effectors the “Fixed Finger” and a custom designed passive retraction mechanism that allows the fingers to retract when it meets resistance the “Adaptive Finger”.
The shape of the fixed finger’s contact area is the same as the adaptive finger, and both are made of ABS.
Furthermore, we use an Ensenso N35 stereo camera in combination with an IDS uEye RGB camera to overlook the workspace of the robot arm and use them to retrieve registered point clouds of the scene.
We use two industry-standard food trays (<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="424mm{\times}308mm{\times}160mm" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mrow id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mrow id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml"><mrow id="S4.SS2.p1.1.m1.1.1.2.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.2.cmml"><mrow id="S4.SS2.p1.1.m1.1.1.2.2.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2.2.2.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.2.cmml">424</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.2.2.2.2.1" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.2.2.2.2.3" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.2.2.2.2.1a" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.2.2.2.2.4" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.4.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.2.2.2.1" xref="S4.SS2.p1.1.m1.1.1.2.2.2.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.2.2.2.3" xref="S4.SS2.p1.1.m1.1.1.2.2.2.3.cmml">308</mn></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.2.2.1" xref="S4.SS2.p1.1.m1.1.1.2.2.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.2.2.3" xref="S4.SS2.p1.1.m1.1.1.2.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.2.2.1a" xref="S4.SS2.p1.1.m1.1.1.2.2.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.2.2.4" xref="S4.SS2.p1.1.m1.1.1.2.2.4.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.2.1" xref="S4.SS2.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml">160</mn></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><times id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.1"></times><apply id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2"><times id="S4.SS2.p1.1.m1.1.1.2.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.1"></times><apply id="S4.SS2.p1.1.m1.1.1.2.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2"><times id="S4.SS2.p1.1.m1.1.1.2.2.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.1"></times><apply id="S4.SS2.p1.1.m1.1.1.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2"><times id="S4.SS2.p1.1.m1.1.1.2.2.2.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.2">424</cn><ci id="S4.SS2.p1.1.m1.1.1.2.2.2.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.3">𝑚</ci><ci id="S4.SS2.p1.1.m1.1.1.2.2.2.2.4.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.2.4">𝑚</ci></apply><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.2.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.2.3">308</cn></apply><ci id="S4.SS2.p1.1.m1.1.1.2.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.3">𝑚</ci><ci id="S4.SS2.p1.1.m1.1.1.2.2.4.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2.4">𝑚</ci></apply><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3">160</cn></apply><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑚</ci><ci id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">424mm{\times}308mm{\times}160mm</annotation></semantics></math>) and designate one to pick from and another to place in.
The Sawyer, gripper and RGB-D sensor are connected to a PC running Ubuntu 16.04 with ROS Kinetic.
We experiment with 7 foods; fried chicken, broccoli, meatball, sausage, mushroom, gyoza (steamed meat dumplings) and taro (colocasia) (Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Label agreement between labels obtained on the same image from 5 human annotators (H1-H5) and the trained model. Each row corresponds to taking the labels of that person as ground truth and calculating the Average Precision (IoU=0.5:0.95) with the others in the column.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row" style="padding-top:0.8pt;padding-bottom:0.8pt;"></th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.1.1.2.1" class="ltx_text" style="font-size:80%;">H1</span></th>
<th id="S4.T1.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.1.1.3.1" class="ltx_text" style="font-size:80%;">H2</span></th>
<th id="S4.T1.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.1.1.4.1" class="ltx_text" style="font-size:80%;">H3</span></th>
<th id="S4.T1.4.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.1.1.5.1" class="ltx_text" style="font-size:80%;">H4</span></th>
<th id="S4.T1.4.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.1.1.6.1" class="ltx_text" style="font-size:80%;">H5</span></th>
<th id="S4.T1.4.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.1.1.7.1" class="ltx_text" style="font-size:80%;">Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.2.1" class="ltx_tr">
<th id="S4.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.1.1" class="ltx_text" style="font-size:80%;">H1</span></th>
<td id="S4.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.2.1" class="ltx_text" style="font-size:80%;">1.0</span></td>
<td id="S4.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.3.1" class="ltx_text" style="font-size:80%;">0.13</span></td>
<td id="S4.T1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.09</span></td>
<td id="S4.T1.4.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.5.1" class="ltx_text" style="font-size:80%;">0.08</span></td>
<td id="S4.T1.4.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.04</span></td>
<td id="S4.T1.4.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.19</span></td>
</tr>
<tr id="S4.T1.4.3.2" class="ltx_tr">
<th id="S4.T1.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.1.1" class="ltx_text" style="font-size:80%;">H2</span></th>
<td id="S4.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.2.1" class="ltx_text" style="font-size:80%;">0.12</span></td>
<td id="S4.T1.4.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.3.1" class="ltx_text" style="font-size:80%;">1.0</span></td>
<td id="S4.T1.4.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.4.1" class="ltx_text" style="font-size:80%;">0.04</span></td>
<td id="S4.T1.4.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.5.1" class="ltx_text" style="font-size:80%;">0.07</span></td>
<td id="S4.T1.4.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.6.1" class="ltx_text" style="font-size:80%;">0.01</span></td>
<td id="S4.T1.4.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.3.2.7.1" class="ltx_text" style="font-size:80%;">0.11</span></td>
</tr>
<tr id="S4.T1.4.4.3" class="ltx_tr">
<th id="S4.T1.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.1.1" class="ltx_text" style="font-size:80%;">H3</span></th>
<td id="S4.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.2.1" class="ltx_text" style="font-size:80%;">0.08</span></td>
<td id="S4.T1.4.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.3.1" class="ltx_text" style="font-size:80%;">0.05</span></td>
<td id="S4.T1.4.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.4.1" class="ltx_text" style="font-size:80%;">1.0</span></td>
<td id="S4.T1.4.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.5.1" class="ltx_text" style="font-size:80%;">0.03</span></td>
<td id="S4.T1.4.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.04</span></td>
<td id="S4.T1.4.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.4.3.7.1" class="ltx_text" style="font-size:80%;">0.04</span></td>
</tr>
<tr id="S4.T1.4.5.4" class="ltx_tr">
<th id="S4.T1.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.1.1" class="ltx_text" style="font-size:80%;">H4</span></th>
<td id="S4.T1.4.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.2.1" class="ltx_text" style="font-size:80%;">0.07</span></td>
<td id="S4.T1.4.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.3.1" class="ltx_text" style="font-size:80%;">0.06</span></td>
<td id="S4.T1.4.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.4.1" class="ltx_text" style="font-size:80%;">0.04</span></td>
<td id="S4.T1.4.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.5.1" class="ltx_text" style="font-size:80%;">1.0</span></td>
<td id="S4.T1.4.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.6.1" class="ltx_text" style="font-size:80%;">0.02</span></td>
<td id="S4.T1.4.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.5.4.7.1" class="ltx_text" style="font-size:80%;">0.11</span></td>
</tr>
<tr id="S4.T1.4.6.5" class="ltx_tr">
<th id="S4.T1.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.1.1" class="ltx_text" style="font-size:80%;">H5</span></th>
<td id="S4.T1.4.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.2.1" class="ltx_text" style="font-size:80%;">0.11</span></td>
<td id="S4.T1.4.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.3.1" class="ltx_text" style="font-size:80%;">0.08</span></td>
<td id="S4.T1.4.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.4.1" class="ltx_text" style="font-size:80%;">0.06</span></td>
<td id="S4.T1.4.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.5.1" class="ltx_text" style="font-size:80%;">0.04</span></td>
<td id="S4.T1.4.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.6.1" class="ltx_text" style="font-size:80%;">1.0</span></td>
<td id="S4.T1.4.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.6.5.7.1" class="ltx_text" style="font-size:80%;">0.06</span></td>
</tr>
<tr id="S4.T1.4.7.6" class="ltx_tr">
<th id="S4.T1.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<td id="S4.T1.4.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.24</span></td>
<td id="S4.T1.4.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.14</span></td>
<td id="S4.T1.4.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.4.1" class="ltx_text" style="font-size:80%;">0.06</span></td>
<td id="S4.T1.4.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.16</span></td>
<td id="S4.T1.4.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.6.1" class="ltx_text" style="font-size:80%;">0.03</span></td>
<td id="S4.T1.4.7.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S4.T1.4.7.6.7.1" class="ltx_text" style="font-size:80%;">1.0</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiment Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">The goal of the experiments is to motivate the use of synthetic data to train the instance segmentation model, verify and compare the performance of the grasping pipeline, as well as understand the effects of the grasp filtering and choice of gripper on attempting to grasp various kinds of foods with a variety of physical properties.
For our experiments, we choose a variety of real foods that spans a wide range of properties of softness (mushroom <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\leftrightarrow" display="inline"><semantics id="S5.p1.1.m1.1a"><mo stretchy="false" id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\leftrightarrow</annotation></semantics></math> taro), fragility (gyoza <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="\leftrightarrow" display="inline"><semantics id="S5.p1.2.m2.1a"><mo stretchy="false" id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\leftrightarrow</annotation></semantics></math> taro), regularity in shapes and size (sausage are nearly identical, while pieces of fried chicken aren’t ), symmetry (meatballs are symmetric along all planes while broccoli isn’t), surface friction (broccoli <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="\leftrightarrow" display="inline"><semantics id="S5.p1.3.m3.1a"><mo stretchy="false" id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><ci id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">\leftrightarrow</annotation></semantics></math> sausage). The list of foods we experiment on includes fried chicken, broccoli, mushroom, meatball, taro, sausage, and gyoza (Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2203.05187/assets/fig/disagreement.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">A sample input image with a) predictions from the model and b) labels supplied by the 5 human annotators for the same image.
</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2203.05187/assets/fig/allfoods_real_trays_detections.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="957" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Detections of the instance segmentation model trained with synthetic images for each food and directly applied to real images.</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Evaluation of Human Annotation</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text" style="color:#000000;">In this section, we motivate the use of synthetic data over human annotations by describing difficulties in collection and their propensity for errors.</span>
Due to limitations in the image quality, a high degree of clutter and the visual similarity of each piece, it is sometimes difficult to identify the boundaries between the pieces.
This would, understandably, lead to noise in the labeling process, which may confuse the model.
To verify this hypothesis, we ask 5 unique individuals to label the same image using Amazon Mechanical Turk (AMT) <span id="S5.SS1.p1.1.2" class="ltx_text" style="color:#000000;">and their provided region annotation web tool</span>.
By treating each person’s labels as ground truth and comparing them against the others, we can measure the agreement between them as the Precision (IoU between 0.5 to 0.95) of segmentation labels provided. Precision measures the percentage of masks output by someone that overlaps at least 50% with the ground truth.
As seen in Table <a href="#S4.T1" title="Table I ‣ IV-B Robot Setup ‣ IV Experiment Setup ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, precision values between human annotators H1-H5 tend to be quite low (<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="&lt;0.1" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><lt id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">absent</csymbol><cn type="float" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">&lt;0.1</annotation></semantics></math>).
We take this as an indication of noise, or disagreement between individual annotators.
By visualizing the annotations of the bottom half of the tray in Fig. <a href="#S5.F5" title="Figure 5 ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we see a marked difference in how individuals handled areas where boundaries between the pieces are unclear.
We posit that this disagreement may hinder the learning process further indicating a need for the perfect labels we can obtain with the use of synthetic data.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">When manual annotations were sourced from AMT, we created a small dataset to be labeled. However, the collection of labels proved harder than anticipated due to many factors. A significant number of annotators submitted zero annotations, despite at least 10-20 pieces being visible at all times. Others labeled all the pieces with the same instance despite providing instructions and a sample image- possibly due to forgetting to switch the ID. A smaller fraction provided inconsistent labels where they started labeling them as multiple instances but then stopped and used the same one for a large number of pieces.
Since hundreds to thousands of images are typically needed for deep learning (especially so to overcome label noise), having these images collected, labeled, verified, and cleaned for every new kind of food would require an enormous investment of time, money and labor.
If the use of synthetic data is a feasible alternative, the cost required to train these models can be greatly reduced.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Simulation Training</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The purpose of this section is to show that training on synthetic data provides a practical alternative to training on real images, provides results agreeable with human annotators and is accurate enough to use for grasping.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">A direct comparison between training with synthetic data and human-annotated images is difficult due to the reasons discussed in the previous section.
To indirectly evaluate whether our model produces reasonable predictions that a human might make, we adopt the label agreement metric used in the previous section to the model’s predictions (taken as ground truth) and the 5 human annotators. From the precision values in bottom line of Table <a href="#S4.T1" title="Table I ‣ IV-B Robot Setup ‣ IV Experiment Setup ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we see that the model outputs predictions that 3 out of 5 humans most agree with, and the other 2 with reasonable agreement.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We find that despite being trained purely with synthetic data, the model is able to generalize to predictions in the real world quite well as seen in Fig. <a href="#S5.F6" title="Figure 6 ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We find that it is also robust to changes in lighting conditions, scales and angles and is competent enough to depend on for choosing and executing grasps with high accuracy using the parametrization strategy described in Section <a href="#S3.SS2" title="III-B Grasp Detection &amp; Filtering ‣ III Method ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.
We note that our end goal is not to train an ideal instance segmentation but one that can be used to grasp food well.
Even if some pieces of food are not detected, as long as we can find a single reasonable target to grasp, it is acceptable. Therefore, by setting a high detection threshold and only retaining predictions that it is confident in, we should be able to achieve good grasp results even if the model is unable to transfer fully to the real-world data, and the results are a little worse.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.4.1.1" class="ltx_text" style="font-size:113%;">TABLE II</span>: </span><span id="S5.T2.5.2" class="ltx_text" style="font-size:113%;">Performance comparison using adaptive and fixed fingers, with and without the grasp filtering.
We define success as when a single piece of food is moved from the food tray to the place tray. We also report in brackets the grasping success when more than one piece is deposited in the place tray.
</span></figcaption>
<table id="S5.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.6.1.1" class="ltx_tr">
<td id="S5.T2.6.1.1.1" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<th id="S5.T2.6.1.1.2" class="ltx_td ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"></th>
<th id="S5.T2.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;" colspan="5"><span id="S5.T2.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Success Rate (incl. multiple pieces)</span></th>
</tr>
<tr id="S5.T2.6.2.2" class="ltx_tr">
<td id="S5.T2.6.2.2.1" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<th id="S5.T2.6.2.2.2" class="ltx_td ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"></th>
<th id="S5.T2.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" colspan="2"><span id="S5.T2.6.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Adaptive Finger</span></th>
<th id="S5.T2.6.2.2.4" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"></th>
<th id="S5.T2.6.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" colspan="2"><span id="S5.T2.6.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Fixed Finger</span></th>
</tr>
<tr id="S5.T2.6.3.3" class="ltx_tr">
<td id="S5.T2.6.3.3.1" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<th id="S5.T2.6.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.3.3.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Food Softness</span></th>
<th id="S5.T2.6.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Grasp Filtering</span></th>
<th id="S5.T2.6.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">No Grasp Filtering</span></th>
<th id="S5.T2.6.3.3.5" class="ltx_td ltx_th ltx_th_column" style="padding-top:0.8pt;padding-bottom:0.8pt;"></th>
<th id="S5.T2.6.3.3.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.3.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Grasp Filtering</span></th>
<th id="S5.T2.6.3.3.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.3.3.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">No Grasp Filtering</span></th>
</tr>
<tr id="S5.T2.6.4.4" class="ltx_tr">
<td id="S5.T2.6.4.4.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.4.4.1.1" class="ltx_text" style="font-size:80%;">Fried Chicken</span></td>
<td id="S5.T2.6.4.4.2" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.4.4.2.1" class="ltx_text" style="font-size:80%;">Soft</span></td>
<td id="S5.T2.6.4.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.4.4.3.1" class="ltx_text" style="font-size:80%;">100% (100%)</span></td>
<td id="S5.T2.6.4.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.4.4.4.1" class="ltx_text" style="font-size:80%;">86% (86%)</span></td>
<td id="S5.T2.6.4.4.5" class="ltx_td ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="S5.T2.6.4.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.4.4.6.1" class="ltx_text" style="font-size:80%;">84% (90%)</span></td>
<td id="S5.T2.6.4.4.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.4.4.7.1" class="ltx_text" style="font-size:80%;">78% (84%)</span></td>
</tr>
<tr id="S5.T2.6.5.5" class="ltx_tr">
<td id="S5.T2.6.5.5.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.5.5.1.1" class="ltx_text" style="font-size:80%;">Broccoli</span></td>
<td id="S5.T2.6.5.5.2" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.5.5.2.1" class="ltx_text" style="font-size:80%;">Soft</span></td>
<td id="S5.T2.6.5.5.3" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.5.5.3.1" class="ltx_text" style="font-size:80%;">96% (98%)</span></td>
<td id="S5.T2.6.5.5.4" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.5.5.4.1" class="ltx_text" style="font-size:80%;">90% (96%)</span></td>
<td id="S5.T2.6.5.5.5" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="S5.T2.6.5.5.6" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.5.5.6.1" class="ltx_text" style="font-size:80%;">86% (94%)</span></td>
<td id="S5.T2.6.5.5.7" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.5.5.7.1" class="ltx_text" style="font-size:80%;">84% (94%)</span></td>
</tr>
<tr id="S5.T2.6.6.6" class="ltx_tr">
<td id="S5.T2.6.6.6.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.6.6.1.1" class="ltx_text" style="font-size:80%;">Mushroom</span></td>
<td id="S5.T2.6.6.6.2" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.6.6.2.1" class="ltx_text" style="font-size:80%;">Soft</span></td>
<td id="S5.T2.6.6.6.3" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.6.6.3.1" class="ltx_text" style="font-size:80%;">84% (96%)</span></td>
<td id="S5.T2.6.6.6.4" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.6.6.4.1" class="ltx_text" style="font-size:80%;">80% (98%)</span></td>
<td id="S5.T2.6.6.6.5" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="S5.T2.6.6.6.6" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.6.6.6.1" class="ltx_text" style="font-size:80%;">64% (70%)</span></td>
<td id="S5.T2.6.6.6.7" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.6.6.7.1" class="ltx_text" style="font-size:80%;">48% (54%)</span></td>
</tr>
<tr id="S5.T2.6.7.7" class="ltx_tr">
<td id="S5.T2.6.7.7.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.7.7.1.1" class="ltx_text" style="font-size:80%;">Meatball</span></td>
<td id="S5.T2.6.7.7.2" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.7.7.2.1" class="ltx_text" style="font-size:80%;">Hard</span></td>
<td id="S5.T2.6.7.7.3" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.7.7.3.1" class="ltx_text" style="font-size:80%;">98% (100%)</span></td>
<td id="S5.T2.6.7.7.4" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.7.7.4.1" class="ltx_text" style="font-size:80%;">90% (96%)</span></td>
<td id="S5.T2.6.7.7.5" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="S5.T2.6.7.7.6" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.7.7.6.1" class="ltx_text" style="font-size:80%;">96% (98%)</span></td>
<td id="S5.T2.6.7.7.7" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.7.7.7.1" class="ltx_text" style="font-size:80%;">96% (98%)</span></td>
</tr>
<tr id="S5.T2.6.8.8" class="ltx_tr">
<td id="S5.T2.6.8.8.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.8.8.1.1" class="ltx_text" style="font-size:80%;">Taro</span></td>
<td id="S5.T2.6.8.8.2" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.8.8.2.1" class="ltx_text" style="font-size:80%;">Hard</span></td>
<td id="S5.T2.6.8.8.3" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.8.8.3.1" class="ltx_text" style="font-size:80%;">92% (98%)</span></td>
<td id="S5.T2.6.8.8.4" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.8.8.4.1" class="ltx_text" style="font-size:80%;">90% (98%)</span></td>
<td id="S5.T2.6.8.8.5" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="S5.T2.6.8.8.6" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.8.8.6.1" class="ltx_text" style="font-size:80%;">94% (100%)</span></td>
<td id="S5.T2.6.8.8.7" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.8.8.7.1" class="ltx_text" style="font-size:80%;">94% (100%)</span></td>
</tr>
<tr id="S5.T2.6.9.9" class="ltx_tr">
<td id="S5.T2.6.9.9.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.9.9.1.1" class="ltx_text" style="font-size:80%;">Sausage</span></td>
<td id="S5.T2.6.9.9.2" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.9.9.2.1" class="ltx_text" style="font-size:80%;">Very hard</span></td>
<td id="S5.T2.6.9.9.3" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.9.9.3.1" class="ltx_text" style="font-size:80%;">88% (88%)</span></td>
<td id="S5.T2.6.9.9.4" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.9.9.4.1" class="ltx_text" style="font-size:80%;">82% (84%)</span></td>
<td id="S5.T2.6.9.9.5" class="ltx_td" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="S5.T2.6.9.9.6" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.9.9.6.1" class="ltx_text" style="font-size:80%;">100% (100%)</span></td>
<td id="S5.T2.6.9.9.7" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T2.6.9.9.7.1" class="ltx_text" style="font-size:80%;">96% (98%)</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Comparison of Adaptive Finger &amp; Fixed Finger</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this section, we study the effects of using the adaptive finger.
The success rates of 50 grasps with and without grasp filtering for the adaptive finger and fixed finger are shown in Table <a href="#S5.T2" title="Table II ‣ V-B Simulation Training ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
We define success as when each food item is grasped and put into another food tray.
We also show the incidence where multiple pieces are grasped in a single grasp.
Multiple grasps will be discussed in Section <a href="#S5.SS4" title="V-D Grasp Filtering ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-D</span></span></a>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.4" class="ltx_p">The experimental results in Table <a href="#S5.T2" title="Table II ‣ V-B Simulation Training ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show that using the adaptive finger (with and without the grasp filtering) tends to either match or exceed the performance of the fixed finger with the exception of sausage (which we discuss later).
Since the adaptive finger is able to adapt to the surface of the food, it is additionally able to compensate for inaccuracies in instance segmentation, sensing and actuation as visualized in Fig <a href="#S2.F4" title="Figure 4 ‣ II-C Collision Avoidance in Grasping ‣ II Related Work ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (c).
This improvement in success rate appears most significant in the case of fried chicken <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="(+16\%)" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p2.1.m1.1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p2.1.m1.1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.cmml"><mo id="S5.SS3.p2.1.m1.1.1.1.1a" xref="S5.SS3.p2.1.m1.1.1.1.1.cmml">+</mo><mrow id="S5.SS3.p2.1.m1.1.1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.1.1.2.cmml"><mn id="S5.SS3.p2.1.m1.1.1.1.1.2.2" xref="S5.SS3.p2.1.m1.1.1.1.1.2.2.cmml">16</mn><mo id="S5.SS3.p2.1.m1.1.1.1.1.2.1" xref="S5.SS3.p2.1.m1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS3.p2.1.m1.1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"><plus id="S5.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></plus><apply id="S5.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.1.1.2.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS3.p2.1.m1.1.1.1.1.2.2.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.2.2">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">(+16\%)</annotation></semantics></math>, broccoli <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="(+6\%)" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mrow id="S5.SS3.p2.2.m2.1.1.1" xref="S5.SS3.p2.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p2.2.m2.1.1.1.2" xref="S5.SS3.p2.2.m2.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p2.2.m2.1.1.1.1" xref="S5.SS3.p2.2.m2.1.1.1.1.cmml"><mo id="S5.SS3.p2.2.m2.1.1.1.1a" xref="S5.SS3.p2.2.m2.1.1.1.1.cmml">+</mo><mrow id="S5.SS3.p2.2.m2.1.1.1.1.2" xref="S5.SS3.p2.2.m2.1.1.1.1.2.cmml"><mn id="S5.SS3.p2.2.m2.1.1.1.1.2.2" xref="S5.SS3.p2.2.m2.1.1.1.1.2.2.cmml">6</mn><mo id="S5.SS3.p2.2.m2.1.1.1.1.2.1" xref="S5.SS3.p2.2.m2.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS3.p2.2.m2.1.1.1.3" xref="S5.SS3.p2.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1"><plus id="S5.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1"></plus><apply id="S5.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS3.p2.2.m2.1.1.1.1.2.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS3.p2.2.m2.1.1.1.1.2.2.cmml" xref="S5.SS3.p2.2.m2.1.1.1.1.2.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">(+6\%)</annotation></semantics></math> and mushroom <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="(+20\%)" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mrow id="S5.SS3.p2.3.m3.1.1.1" xref="S5.SS3.p2.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p2.3.m3.1.1.1.2" xref="S5.SS3.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p2.3.m3.1.1.1.1" xref="S5.SS3.p2.3.m3.1.1.1.1.cmml"><mo id="S5.SS3.p2.3.m3.1.1.1.1a" xref="S5.SS3.p2.3.m3.1.1.1.1.cmml">+</mo><mrow id="S5.SS3.p2.3.m3.1.1.1.1.2" xref="S5.SS3.p2.3.m3.1.1.1.1.2.cmml"><mn id="S5.SS3.p2.3.m3.1.1.1.1.2.2" xref="S5.SS3.p2.3.m3.1.1.1.1.2.2.cmml">20</mn><mo id="S5.SS3.p2.3.m3.1.1.1.1.2.1" xref="S5.SS3.p2.3.m3.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS3.p2.3.m3.1.1.1.3" xref="S5.SS3.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><apply id="S5.SS3.p2.3.m3.1.1.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1.1"><plus id="S5.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1.1"></plus><apply id="S5.SS3.p2.3.m3.1.1.1.1.2.cmml" xref="S5.SS3.p2.3.m3.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS3.p2.3.m3.1.1.1.1.2.1.cmml" xref="S5.SS3.p2.3.m3.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS3.p2.3.m3.1.1.1.1.2.2.cmml" xref="S5.SS3.p2.3.m3.1.1.1.1.2.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">(+20\%)</annotation></semantics></math>, which we attribute to the variety in size and surface profiles as well as some compressibility.
On the other hand, we see a drop with sausage <math id="S5.SS3.p2.4.m4.1" class="ltx_Math" alttext="(-12\%)" display="inline"><semantics id="S5.SS3.p2.4.m4.1a"><mrow id="S5.SS3.p2.4.m4.1.1.1" xref="S5.SS3.p2.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p2.4.m4.1.1.1.2" xref="S5.SS3.p2.4.m4.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p2.4.m4.1.1.1.1" xref="S5.SS3.p2.4.m4.1.1.1.1.cmml"><mo id="S5.SS3.p2.4.m4.1.1.1.1a" xref="S5.SS3.p2.4.m4.1.1.1.1.cmml">−</mo><mrow id="S5.SS3.p2.4.m4.1.1.1.1.2" xref="S5.SS3.p2.4.m4.1.1.1.1.2.cmml"><mn id="S5.SS3.p2.4.m4.1.1.1.1.2.2" xref="S5.SS3.p2.4.m4.1.1.1.1.2.2.cmml">12</mn><mo id="S5.SS3.p2.4.m4.1.1.1.1.2.1" xref="S5.SS3.p2.4.m4.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS3.p2.4.m4.1.1.1.3" xref="S5.SS3.p2.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.4.m4.1b"><apply id="S5.SS3.p2.4.m4.1.1.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1.1"><minus id="S5.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1.1"></minus><apply id="S5.SS3.p2.4.m4.1.1.1.1.2.cmml" xref="S5.SS3.p2.4.m4.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS3.p2.4.m4.1.1.1.1.2.1.cmml" xref="S5.SS3.p2.4.m4.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS3.p2.4.m4.1.1.1.1.2.2.cmml" xref="S5.SS3.p2.4.m4.1.1.1.1.2.2">12</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.4.m4.1c">(-12\%)</annotation></semantics></math>, which we attribute to the hardness of the food.
While a fixed finger is able to forcefully push neighbouring or blocking pieces away, the adaptive finger can retract prematurely and simply slip over the target piece due to its smooth, taut skin and may be more suited to be grasped with fixed fingers.
However, we note that this forced grasping may come at the cost of increase in damage done to food via piercing by downward motion or scratching the surface of the food during gripper closure(See “by fixed finger” Fig. <a href="#S5.F7" title="Figure 7 ‣ V-C Comparison of Adaptive Finger &amp; Fixed Finger ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">While it is difficult to come up with a generic subjective evaluation criterion of damage caused by the grasping process on each foodstuff, we instead choose to measure it specifically for a very delicate food- gyoza, which are meat-filled dumplings.
We do this by counting the number of pieces in both trays with fully pierced skins and whose stuffing is visible after the 50 grasp attempts made in Table <a href="#S5.T2" title="Table II ‣ V-B Simulation Training ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> for each experiment.
We note these numbers for each method evaluated in Table <a href="#S5.T3" title="Table III ‣ V-C Comparison of Adaptive Finger &amp; Fixed Finger ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
We see the stark contrast in the number of damaged pieces when the adaptive finger is used instead of the fixed one.
We also note that not just the number but also the amount of damage inflicted on the gyoza is also more significant when a fixed finger is used, with some pieces being practically ripped apart, as seen in Fig. <a href="#S5.F7" title="Figure 7 ‣ V-C Comparison of Adaptive Finger &amp; Fixed Finger ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. <span id="S5.SS3.p3.1.1" class="ltx_text" style="color:#000000;">We see that even with the use of adaptive finger, some pieces do get damaged due to a mixture of gripper closure and the spring potentially being too strong for something as delicate as gyoza and could be adjusted to further reduce it.</span>
The use of the adaptive finger presents a trade-off between performance and damage for some more rigid foodstuff. We believe it is a good default choice based on the foods tested.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2203.05187/assets/fig/damage_combined.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Damage caused by the use of fixed finger on neighbouring foodstuff for sausage, meatball and gyoza (left), by the use of the adaptive finger (right).</span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Performance comparison and number of pieces damaged by 50 attempts of grasping gyoza.</span></figcaption>
<table id="S5.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.4.1.1" class="ltx_tr">
<th id="S5.T3.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></th>
<th id="S5.T3.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="S5.T3.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.4.1.1.2.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T3.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Success Rate (incl. multiple grasping)</span></span>
</span>
</th>
<th id="S5.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;"># Damaged Pieces</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.4.2.1" class="ltx_tr">
<td id="S5.T3.4.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Adaptive Finger with Grasp Filtering</span></td>
<td id="S5.T3.4.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="S5.T3.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.4.2.1.2.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T3.4.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">74% (90%)</span></span>
</span>
</td>
<td id="S5.T3.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.2.1.3.1" class="ltx_text" style="font-size:80%;">6</span></td>
</tr>
<tr id="S5.T3.4.3.2" class="ltx_tr">
<td id="S5.T3.4.3.2.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.3.2.1.1" class="ltx_text" style="font-size:80%;">Adaptive Finger without Grasp Filtering</span></td>
<td id="S5.T3.4.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="S5.T3.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.4.3.2.2.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T3.4.3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">56% (84%)</span></span>
</span>
</td>
<td id="S5.T3.4.3.2.3" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.3.2.3.1" class="ltx_text" style="font-size:80%;">9</span></td>
</tr>
<tr id="S5.T3.4.4.3" class="ltx_tr">
<td id="S5.T3.4.4.3.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.4.3.1.1" class="ltx_text" style="font-size:80%;">Fixed Finger with Grasp Filtering</span></td>
<td id="S5.T3.4.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="S5.T3.4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.4.4.3.2.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T3.4.4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">76% (92%)</span></span>
</span>
</td>
<td id="S5.T3.4.4.3.3" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.4.3.3.1" class="ltx_text" style="font-size:80%;">14</span></td>
</tr>
<tr id="S5.T3.4.5.4" class="ltx_tr">
<td id="S5.T3.4.5.4.1" class="ltx_td ltx_align_left" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.5.4.1.1" class="ltx_text" style="font-size:80%;">Fixed Finger without Grasp Filtering</span></td>
<td id="S5.T3.4.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="S5.T3.4.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.4.5.4.2.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T3.4.5.4.2.1.1.1" class="ltx_text" style="font-size:80%;">66% (94%)</span></span>
</span>
</td>
<td id="S5.T3.4.5.4.3" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S5.T3.4.5.4.3.1" class="ltx_text" style="font-size:80%;">18</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Grasp Filtering</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In this section, we aim to study the effect of using the grasp filtering.
The success rates of 50 grasps with and without grasp filtering for the adaptive finger and fixed finger are shown in Table <a href="#S5.T2" title="Table II ‣ V-B Simulation Training ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> as explained in section <a href="#S5.SS3" title="V-C Comparison of Adaptive Finger &amp; Fixed Finger ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.7" class="ltx_p">From the results in Table <a href="#S5.T2" title="Table II ‣ V-B Simulation Training ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we see that the use of grasp filtering tends to increase the success rate for most foods for both the adaptive fingers and the fixed fingers.
The use of grasp filtering shows improved performance most prominently for fried chicken <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="(+14\%)" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p2.1.m1.1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p2.1.m1.1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.1.cmml"><mo id="S5.SS4.p2.1.m1.1.1.1.1a" xref="S5.SS4.p2.1.m1.1.1.1.1.cmml">+</mo><mrow id="S5.SS4.p2.1.m1.1.1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.1.1.2.cmml"><mn id="S5.SS4.p2.1.m1.1.1.1.1.2.2" xref="S5.SS4.p2.1.m1.1.1.1.1.2.2.cmml">14</mn><mo id="S5.SS4.p2.1.m1.1.1.1.1.2.1" xref="S5.SS4.p2.1.m1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS4.p2.1.m1.1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1"><plus id="S5.SS4.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1"></plus><apply id="S5.SS4.p2.1.m1.1.1.1.1.2.cmml" xref="S5.SS4.p2.1.m1.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.1.m1.1.1.1.1.2.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.1.m1.1.1.1.1.2.2.cmml" xref="S5.SS4.p2.1.m1.1.1.1.1.2.2">14</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">(+14\%)</annotation></semantics></math> and meatball <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="(+8\%)" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mrow id="S5.SS4.p2.2.m2.1.1.1" xref="S5.SS4.p2.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p2.2.m2.1.1.1.2" xref="S5.SS4.p2.2.m2.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p2.2.m2.1.1.1.1" xref="S5.SS4.p2.2.m2.1.1.1.1.cmml"><mo id="S5.SS4.p2.2.m2.1.1.1.1a" xref="S5.SS4.p2.2.m2.1.1.1.1.cmml">+</mo><mrow id="S5.SS4.p2.2.m2.1.1.1.1.2" xref="S5.SS4.p2.2.m2.1.1.1.1.2.cmml"><mn id="S5.SS4.p2.2.m2.1.1.1.1.2.2" xref="S5.SS4.p2.2.m2.1.1.1.1.2.2.cmml">8</mn><mo id="S5.SS4.p2.2.m2.1.1.1.1.2.1" xref="S5.SS4.p2.2.m2.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS4.p2.2.m2.1.1.1.3" xref="S5.SS4.p2.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><apply id="S5.SS4.p2.2.m2.1.1.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1.1"><plus id="S5.SS4.p2.2.m2.1.1.1.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1.1"></plus><apply id="S5.SS4.p2.2.m2.1.1.1.1.2.cmml" xref="S5.SS4.p2.2.m2.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.2.m2.1.1.1.1.2.1.cmml" xref="S5.SS4.p2.2.m2.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.2.m2.1.1.1.1.2.2.cmml" xref="S5.SS4.p2.2.m2.1.1.1.1.2.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">(+8\%)</annotation></semantics></math>.
The fixed fingers also shows improvement to a smaller extent for fried chicken <math id="S5.SS4.p2.3.m3.1" class="ltx_Math" alttext="(+8\%)" display="inline"><semantics id="S5.SS4.p2.3.m3.1a"><mrow id="S5.SS4.p2.3.m3.1.1.1" xref="S5.SS4.p2.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p2.3.m3.1.1.1.2" xref="S5.SS4.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p2.3.m3.1.1.1.1" xref="S5.SS4.p2.3.m3.1.1.1.1.cmml"><mo id="S5.SS4.p2.3.m3.1.1.1.1a" xref="S5.SS4.p2.3.m3.1.1.1.1.cmml">+</mo><mrow id="S5.SS4.p2.3.m3.1.1.1.1.2" xref="S5.SS4.p2.3.m3.1.1.1.1.2.cmml"><mn id="S5.SS4.p2.3.m3.1.1.1.1.2.2" xref="S5.SS4.p2.3.m3.1.1.1.1.2.2.cmml">8</mn><mo id="S5.SS4.p2.3.m3.1.1.1.1.2.1" xref="S5.SS4.p2.3.m3.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS4.p2.3.m3.1.1.1.3" xref="S5.SS4.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.3.m3.1b"><apply id="S5.SS4.p2.3.m3.1.1.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1.1"><plus id="S5.SS4.p2.3.m3.1.1.1.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1.1"></plus><apply id="S5.SS4.p2.3.m3.1.1.1.1.2.cmml" xref="S5.SS4.p2.3.m3.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.3.m3.1.1.1.1.2.1.cmml" xref="S5.SS4.p2.3.m3.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.3.m3.1.1.1.1.2.2.cmml" xref="S5.SS4.p2.3.m3.1.1.1.1.2.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.3.m3.1c">(+8\%)</annotation></semantics></math> and mushroom <math id="S5.SS4.p2.4.m4.1" class="ltx_Math" alttext="(+16\%)" display="inline"><semantics id="S5.SS4.p2.4.m4.1a"><mrow id="S5.SS4.p2.4.m4.1.1.1" xref="S5.SS4.p2.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p2.4.m4.1.1.1.2" xref="S5.SS4.p2.4.m4.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p2.4.m4.1.1.1.1" xref="S5.SS4.p2.4.m4.1.1.1.1.cmml"><mo id="S5.SS4.p2.4.m4.1.1.1.1a" xref="S5.SS4.p2.4.m4.1.1.1.1.cmml">+</mo><mrow id="S5.SS4.p2.4.m4.1.1.1.1.2" xref="S5.SS4.p2.4.m4.1.1.1.1.2.cmml"><mn id="S5.SS4.p2.4.m4.1.1.1.1.2.2" xref="S5.SS4.p2.4.m4.1.1.1.1.2.2.cmml">16</mn><mo id="S5.SS4.p2.4.m4.1.1.1.1.2.1" xref="S5.SS4.p2.4.m4.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo stretchy="false" id="S5.SS4.p2.4.m4.1.1.1.3" xref="S5.SS4.p2.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.4.m4.1b"><apply id="S5.SS4.p2.4.m4.1.1.1.1.cmml" xref="S5.SS4.p2.4.m4.1.1.1"><plus id="S5.SS4.p2.4.m4.1.1.1.1.1.cmml" xref="S5.SS4.p2.4.m4.1.1.1"></plus><apply id="S5.SS4.p2.4.m4.1.1.1.1.2.cmml" xref="S5.SS4.p2.4.m4.1.1.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.4.m4.1.1.1.1.2.1.cmml" xref="S5.SS4.p2.4.m4.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.4.m4.1.1.1.1.2.2.cmml" xref="S5.SS4.p2.4.m4.1.1.1.1.2.2">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.4.m4.1c">(+16\%)</annotation></semantics></math>.
Since the grasp filtering criterion picks grasps where the fingers are less likely to make contact with neighbouring pieces, we see that it is also able to reduce the incidence of grasping multiple pieces of food with a single grasp. This can be seen from looking at the differences in success rates when multiple pieces are included, which are reported in brackets in Table <a href="#S5.T2" title="Table II ‣ V-B Simulation Training ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. This reduction in the percentage of multiple grasps is observed in grasping of foods which are small such as mushroom (<math id="S5.SS4.p2.5.m5.1" class="ltx_Math" alttext="18\%\rightarrow 12\%" display="inline"><semantics id="S5.SS4.p2.5.m5.1a"><mrow id="S5.SS4.p2.5.m5.1.1" xref="S5.SS4.p2.5.m5.1.1.cmml"><mrow id="S5.SS4.p2.5.m5.1.1.2" xref="S5.SS4.p2.5.m5.1.1.2.cmml"><mn id="S5.SS4.p2.5.m5.1.1.2.2" xref="S5.SS4.p2.5.m5.1.1.2.2.cmml">18</mn><mo id="S5.SS4.p2.5.m5.1.1.2.1" xref="S5.SS4.p2.5.m5.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S5.SS4.p2.5.m5.1.1.1" xref="S5.SS4.p2.5.m5.1.1.1.cmml">→</mo><mrow id="S5.SS4.p2.5.m5.1.1.3" xref="S5.SS4.p2.5.m5.1.1.3.cmml"><mn id="S5.SS4.p2.5.m5.1.1.3.2" xref="S5.SS4.p2.5.m5.1.1.3.2.cmml">12</mn><mo id="S5.SS4.p2.5.m5.1.1.3.1" xref="S5.SS4.p2.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.5.m5.1b"><apply id="S5.SS4.p2.5.m5.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1"><ci id="S5.SS4.p2.5.m5.1.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1.1">→</ci><apply id="S5.SS4.p2.5.m5.1.1.2.cmml" xref="S5.SS4.p2.5.m5.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.5.m5.1.1.2.1.cmml" xref="S5.SS4.p2.5.m5.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.5.m5.1.1.2.2.cmml" xref="S5.SS4.p2.5.m5.1.1.2.2">18</cn></apply><apply id="S5.SS4.p2.5.m5.1.1.3.cmml" xref="S5.SS4.p2.5.m5.1.1.3"><csymbol cd="latexml" id="S5.SS4.p2.5.m5.1.1.3.1.cmml" xref="S5.SS4.p2.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.5.m5.1.1.3.2.cmml" xref="S5.SS4.p2.5.m5.1.1.3.2">12</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.5.m5.1c">18\%\rightarrow 12\%</annotation></semantics></math>) or meatball (<math id="S5.SS4.p2.6.m6.1" class="ltx_Math" alttext="6\%\rightarrow 2\%" display="inline"><semantics id="S5.SS4.p2.6.m6.1a"><mrow id="S5.SS4.p2.6.m6.1.1" xref="S5.SS4.p2.6.m6.1.1.cmml"><mrow id="S5.SS4.p2.6.m6.1.1.2" xref="S5.SS4.p2.6.m6.1.1.2.cmml"><mn id="S5.SS4.p2.6.m6.1.1.2.2" xref="S5.SS4.p2.6.m6.1.1.2.2.cmml">6</mn><mo id="S5.SS4.p2.6.m6.1.1.2.1" xref="S5.SS4.p2.6.m6.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S5.SS4.p2.6.m6.1.1.1" xref="S5.SS4.p2.6.m6.1.1.1.cmml">→</mo><mrow id="S5.SS4.p2.6.m6.1.1.3" xref="S5.SS4.p2.6.m6.1.1.3.cmml"><mn id="S5.SS4.p2.6.m6.1.1.3.2" xref="S5.SS4.p2.6.m6.1.1.3.2.cmml">2</mn><mo id="S5.SS4.p2.6.m6.1.1.3.1" xref="S5.SS4.p2.6.m6.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.6.m6.1b"><apply id="S5.SS4.p2.6.m6.1.1.cmml" xref="S5.SS4.p2.6.m6.1.1"><ci id="S5.SS4.p2.6.m6.1.1.1.cmml" xref="S5.SS4.p2.6.m6.1.1.1">→</ci><apply id="S5.SS4.p2.6.m6.1.1.2.cmml" xref="S5.SS4.p2.6.m6.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.6.m6.1.1.2.1.cmml" xref="S5.SS4.p2.6.m6.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.6.m6.1.1.2.2.cmml" xref="S5.SS4.p2.6.m6.1.1.2.2">6</cn></apply><apply id="S5.SS4.p2.6.m6.1.1.3.cmml" xref="S5.SS4.p2.6.m6.1.1.3"><csymbol cd="latexml" id="S5.SS4.p2.6.m6.1.1.3.1.cmml" xref="S5.SS4.p2.6.m6.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.6.m6.1.1.3.2.cmml" xref="S5.SS4.p2.6.m6.1.1.3.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.6.m6.1c">6\%\rightarrow 2\%</annotation></semantics></math>) or easily entangled like broccoli (<math id="S5.SS4.p2.7.m7.1" class="ltx_Math" alttext="6\%\rightarrow 2\%" display="inline"><semantics id="S5.SS4.p2.7.m7.1a"><mrow id="S5.SS4.p2.7.m7.1.1" xref="S5.SS4.p2.7.m7.1.1.cmml"><mrow id="S5.SS4.p2.7.m7.1.1.2" xref="S5.SS4.p2.7.m7.1.1.2.cmml"><mn id="S5.SS4.p2.7.m7.1.1.2.2" xref="S5.SS4.p2.7.m7.1.1.2.2.cmml">6</mn><mo id="S5.SS4.p2.7.m7.1.1.2.1" xref="S5.SS4.p2.7.m7.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S5.SS4.p2.7.m7.1.1.1" xref="S5.SS4.p2.7.m7.1.1.1.cmml">→</mo><mrow id="S5.SS4.p2.7.m7.1.1.3" xref="S5.SS4.p2.7.m7.1.1.3.cmml"><mn id="S5.SS4.p2.7.m7.1.1.3.2" xref="S5.SS4.p2.7.m7.1.1.3.2.cmml">2</mn><mo id="S5.SS4.p2.7.m7.1.1.3.1" xref="S5.SS4.p2.7.m7.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.7.m7.1b"><apply id="S5.SS4.p2.7.m7.1.1.cmml" xref="S5.SS4.p2.7.m7.1.1"><ci id="S5.SS4.p2.7.m7.1.1.1.cmml" xref="S5.SS4.p2.7.m7.1.1.1">→</ci><apply id="S5.SS4.p2.7.m7.1.1.2.cmml" xref="S5.SS4.p2.7.m7.1.1.2"><csymbol cd="latexml" id="S5.SS4.p2.7.m7.1.1.2.1.cmml" xref="S5.SS4.p2.7.m7.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.7.m7.1.1.2.2.cmml" xref="S5.SS4.p2.7.m7.1.1.2.2">6</cn></apply><apply id="S5.SS4.p2.7.m7.1.1.3.cmml" xref="S5.SS4.p2.7.m7.1.1.3"><csymbol cd="latexml" id="S5.SS4.p2.7.m7.1.1.3.1.cmml" xref="S5.SS4.p2.7.m7.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.7.m7.1.1.3.2.cmml" xref="S5.SS4.p2.7.m7.1.1.3.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.7.m7.1c">6\%\rightarrow 2\%</annotation></semantics></math>).</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.2" class="ltx_p">The grasp filtering criterion not only improves grasp performance, but in filtering out candidates where collision between the fingers and neighbouring pieces is likely, the amount of damage caused is also reduced during the grasp process.
This is demonstrated in the gyoza experiments in Table <a href="#S5.T3" title="Table III ‣ V-C Comparison of Adaptive Finger &amp; Fixed Finger ‣ V Experiment Results ‣ Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
We see that using grasp filtering decreases the number of pieces damaged for both the adaptive finger <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="(9\rightarrow 6)" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mrow id="S5.SS4.p3.1.m1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p3.1.m1.1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p3.1.m1.1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.1.cmml"><mn id="S5.SS4.p3.1.m1.1.1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.1.1.2.cmml">9</mn><mo stretchy="false" id="S5.SS4.p3.1.m1.1.1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.1.1.cmml">→</mo><mn id="S5.SS4.p3.1.m1.1.1.1.1.3" xref="S5.SS4.p3.1.m1.1.1.1.1.3.cmml">6</mn></mrow><mo stretchy="false" id="S5.SS4.p3.1.m1.1.1.1.3" xref="S5.SS4.p3.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><apply id="S5.SS4.p3.1.m1.1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1.1"><ci id="S5.SS4.p3.1.m1.1.1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1.1.1.1">→</ci><cn type="integer" id="S5.SS4.p3.1.m1.1.1.1.1.2.cmml" xref="S5.SS4.p3.1.m1.1.1.1.1.2">9</cn><cn type="integer" id="S5.SS4.p3.1.m1.1.1.1.1.3.cmml" xref="S5.SS4.p3.1.m1.1.1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">(9\rightarrow 6)</annotation></semantics></math> and the fixed finger <math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="(18\rightarrow 14)" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mrow id="S5.SS4.p3.2.m2.1.1.1" xref="S5.SS4.p3.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p3.2.m2.1.1.1.2" xref="S5.SS4.p3.2.m2.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p3.2.m2.1.1.1.1" xref="S5.SS4.p3.2.m2.1.1.1.1.cmml"><mn id="S5.SS4.p3.2.m2.1.1.1.1.2" xref="S5.SS4.p3.2.m2.1.1.1.1.2.cmml">18</mn><mo stretchy="false" id="S5.SS4.p3.2.m2.1.1.1.1.1" xref="S5.SS4.p3.2.m2.1.1.1.1.1.cmml">→</mo><mn id="S5.SS4.p3.2.m2.1.1.1.1.3" xref="S5.SS4.p3.2.m2.1.1.1.1.3.cmml">14</mn></mrow><mo stretchy="false" id="S5.SS4.p3.2.m2.1.1.1.3" xref="S5.SS4.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><apply id="S5.SS4.p3.2.m2.1.1.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1.1"><ci id="S5.SS4.p3.2.m2.1.1.1.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1.1.1.1">→</ci><cn type="integer" id="S5.SS4.p3.2.m2.1.1.1.1.2.cmml" xref="S5.SS4.p3.2.m2.1.1.1.1.2">18</cn><cn type="integer" id="S5.SS4.p3.2.m2.1.1.1.1.3.cmml" xref="S5.SS4.p3.2.m2.1.1.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">(18\rightarrow 14)</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose a method to grasp a piece of target food from a cluttered tray while minimizing the damage to it and its neighbors. Our proposed method consists of 3 components- 1) Generation of synthetic images from 3d models of real food, which we use to train an instance segmentation model and use sim2real methods to aid model transfer to identify real instances of food in the tray. 2) Creation of a novel adaptive mechanism that allows the gripper’s fingers to passively retract and avoid damaging food when inserted into the tray. 3) A grasp filtering strategy to evaluate candidate grasps and discard ones that would result in a collision between the gripper and the food and damage neighboring pieces. We confirm the veracity of the first in grasping diverse foods and verify that the use of the adaptive mechanism and the grasp filtering leads to a high success rate of 84% and above and significantly decreases the amount of damage inflicted on food during the grasping process.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Kleeberger, R. Bormann, W. Kraus, and M. F. Huber, “A survey on
learning-based robotic grasping,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Current Robotics Reports</em>, pp.
1–11, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. Takahashi, W. Ko, A. Ummadisingu, and S. Maeda, “Uncertainty-aware
self-supervised target-mass grasping of granular foods,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2021 IEEE
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2021, pp. 2620–2626.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Takahashi, N. Fukaya, and A. Ummadisingu, “Target-mass grasping of
entangled food using pre-grasping &amp; post-grasping,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and
Automation Letters</em>, pp. 1222–1229, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To,
E. Cameracci, S. Boochoon, and S. Birchfield, “Training deep networks with
synthetic data: Bridging the reality gap by domain randomization,” in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition workshops</em>, 2018, pp. 969–977.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Min, S. Jiang, L. Liu, Y. Rui, and R. Jain, “A survey on food computing,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, vol. 52, no. 5, pp. 1–36, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Meyers, N. Johnston, V. Rathod, A. Korattikara, A. Gorban, N. Silberman,
S. Guadarrama, G. Papandreou, J. Huang, and K. P. Murphy, “Im2calories:
towards an automated mobile vision food diary,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE International Conference on Computer Vision</em>, 2015, pp. 1233–1241.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Ege and K. Yanai, “A new large-scale food image segmentation dataset and
its application to food calorie estimation based on grains of rice,” in
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. of ACMMM Workshop on Multimedia Assisted Dietary
Management(MADiMa)</em>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Okamoto and K. Yanai, “UEC-FoodPIX Complete: A large-scale food image
segmentation dataset,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. of ICPR Workshop on Multimedia
Assisted Dietary Management(MADiMa)</em>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
G. Ciocca, P. Napoletano, and R. Schettini, “Food recognition: a new dataset,
experiments, and results,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE journal of biomedical and health
informatics</em>, vol. 21, no. 3, pp. 588–598, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Gao, W. Tan, L. Ma, Y. Wang, and W. Tang, “Musefood: Multi-sensor-based
food volume estimation on smartphones,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2019 IEEE SmartWorld,
Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computing, Scalable
Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People
and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)</em>.   IEEE, 2019, pp. 899–906.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Aslan, G. Ciocca, D. Mazzini, and R. Schettini, “Benchmarking algorithms
for food localization and semantic segmentation,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International
Journal of Machine Learning and Cybernetics</em>, vol. 11, no. 12, pp.
2827–2847, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Park, J. Lee, J. Lee, and K. Lee, “Deep learning based food instance
segmentation using synthetic data,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2021 18th International
Conference on Ubiquitous Robots (UR)</em>.   IEEE, 2021, pp. 499–505.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X. Wu, X. Fu, Y. Liu, E.-P. Lim, S. C. Hoi, and Q. Sun, “A large-scale
benchmark for food image segmentation,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2105.05409</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Toda, F. Okura, J. Ito, S. Okada, T. Kinoshita, H. Tsuji, and D. Saisho,
“Training instance segmentation neural network with synthetic datasets for
crop seed phenotyping,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Communications biology</em>, vol. 3, no. 1, pp.
1–12, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Kato, D. Beker, M. Morariu, T. Ando, T. Matsuoka, W. Kehl, and A. Gaidon,
“Differentiable rendering: A survey,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2006.12057</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
F. Dimeas, D. V. Sako, V. C. Moulianitis, and N. A. Aspragathos, “Design and
fuzzy control of a robotic gripper for efficient strawberry harvesting,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Robotica</em>, vol. 33, no. 5, pp. 1085–1098, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Yamanaka, S. Katagiri, N. Hiroyuki, K. Suzumori, and G. Endo, “Development
of a food handling soft robot hand considering a high-speed pick-and-place
task,” pp. 87–92, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Gafer, D. Heymans, D. Prattichizzo, and G. Salvietti, “The quad-spatula
gripper: A novel soft-rigid gripper for food handling,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2020 3rd
IEEE International Conference on Soft Robotics (RoboSoft)</em>.   IEEE, 2020, pp. 39–45.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. M. Dang, C. T. Vo, N. T. Trong, V. D. Nguyen, and V. B. Phung, “Design and
development of the soft robotic gripper used for the food packaging system,”
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Journal of Mechanical Engineering Research and Developments</em>, vol. 44,
no. 3, pp. 334–345, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Sam and S. Nefti, “Design and feasibility tests of flexible gripper for
handling variable shape of food products,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. 9th WSEAS
international conference on signal processing, robotics and automation</em>, pp.
329–335, 2010.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. Morales, F. J. Badesa, N. Garcia-Aracil, J. M. Sabater, and L. Zollo, “Soft
robotic manipulation of onions and artichokes in the food industry,”
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Mechanical Engineering</em>, 2014.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E. Natarajan, L. W. Hong, M. Ramasamy, C. C. Hou, and R. Sengottuvelu, “Design
and development of a robot gripper for food industries using coanda effect,”
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE 4th International Symposium in Robotics and Manufacturing
Automation</em>, pp. 1–5, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Makiyama, and S. Hirai, “A soft needle gripper capable of grasping
and piercing for handling food materials,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Journal of Robotics and
Mechatronics</em>, vol. 33, no. 4, pp. 935–943, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Wang, R. Cromwell, A. Kak, I. Kimura, and M. Osada, “Model-based vision for
robotic manipulation of twisted tubular parts: using affine transforms and
heuristic search,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1994 IEEE International
Conference on Robotics and Automation</em>.   IEEE, 1994, pp. 208–215.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
F. Spenrath and A. Pott, “Gripping point determination for bin picking using
heuristic search,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Procedia CIRP</em>, vol. 62, pp. 606–611, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Fan and M. Tomizuka, “Efficient grasp planning and execution with
multifingered hands by surface fitting,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation
Letters</em>, vol. 4, no. 4, pp. 3995–4002, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K. Mano, T. Hasegawa, T. Yamashita, H. Fujiyoshi, and Y. Domae, “Fast and
precise detection of object grasping positions with eigenvalue templates,”
in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Robotics and Automation
(ICRA)</em>.   IEEE, 2019, pp. 4403–4409.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Kleeberger, M. Völk, M. Moosmann, E. Thiessenhusen, F. Roth, R. Bormann,
and M. F. Huber, “Transferring experience from simulation to the real world
for precise pick-and-place tasks in highly cluttered scenes,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2020
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS)</em>.   IEEE, 2020, pp. 9681–9688.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Z. Xue, J. M. Zoellner, and R. Dillmann, “Automatic optimal grasp planning
based on found contact points,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2008 IEEE/ASME International
Conference on Advanced Intelligent Mechatronics</em>.   IEEE, 2008, pp. 1053–1058.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. Diankov, “Automated construction of robotic manipulation programs,” 2010.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt, “Grasp pose detection in
point clouds,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>,
vol. 36, no. 13-14, pp. 1455–1473, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. R. Dogar and S. S. Srinivasa, “Push-grasping with dexterous hands:
Mechanics and a method,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2010 IEEE/RSJ International Conference on
Intelligent Robots and Systems</em>.   IEEE,
2010, pp. 2123–2130.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
N. Kitaev, I. Mordatch, S. Patil, and P. Abbeel, “Physics-based trajectory
optimization for grasping in cluttered environments,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2015 IEEE
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2015, pp. 3102–3109.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D. Berenson, R. Diankov, K. Nishiwaki, S. Kagami, and J. Kuffner, “Grasp
planning in complex scenes,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2007 7th IEEE-RAS International
Conference on Humanoid Robots</em>.   IEEE,
2007, pp. 42–48.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Ciocarlie, K. Hsiao, E. G. Jones, S. Chitta, R. B. Rusu, and I. A.
Şucan, “Towards reliable grasping and manipulation in household
environments,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Experimental Robotics</em>.   Springer, 2014, pp. 241–252.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
H.-S. Fang, C. Wang, M. Gou, and C. Lu, “Graspnet-1billion: A large-scale
benchmark for general object grasping,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2020, pp.
11 444–11 453.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
K. Kleeberger, C. Landgraf, and M. F. Huber, “Large-scale 6d object pose
estimation dataset for industrial bin-picking,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2019, pp. 2573–2578.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Mahler and K. Goldberg, “Learning deep policies for robot bin picking by
simulating robust grasping sequences,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Conference on robot
learning</em>.   PMLR, 2017, pp. 515–524.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
L. Wang, Y. Xiang, and D. Fox, “Hierarchical policies for cluttered-scene
grasping with latent plans,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.01518</em>, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Lundell, F. Verdoja, and V. Kyrki, “Ddgc: Generative deep dexterous
grasping in clutter,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.04783</em>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Li, T. Kong, R. Chu, Y. Li, P. Wang, and L. Li, “Simultaneous semantic and
collision learning for 6-dof grasp pose estimation,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2108.02425</em>, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
X. Lou, Y. Yang, and C. Choi, “Collision-aware target-driven object grasping
in constrained environments,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.00776</em>, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox, “6-dof grasping for
target-driven object manipulation in clutter,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2020, pp. 6232–6238.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Domae, H. Okuda, Y. Taguchi, K. Sumi, and T. Hirai, “Fast graspability
evaluation on single depth maps for bin picking with general grippers,” in
<em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2014 IEEE International Conference on Robotics and Automation
(ICRA)</em>.   IEEE, 2014, pp. 1997–2004.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
“High-precision 3d model generation,”
<span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://www.preferred.jp/en/news/pr20210310/</span>, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic grasps,”
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>, vol. 34, no. 4-5, pp.
705–724, 2015.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
E. Coumans <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Bullet physics library,” <em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic">Open source:
bulletphysics. org</em>, vol. 15, no. 49, p. 5, 2013.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation in
robotics, games and machine learning,” 2017.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in
<em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2961–2969.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.05186" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.05187" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.05187">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.05187" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.05188" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 10:04:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
