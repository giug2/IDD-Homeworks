<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1809.03676] Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer</title><meta property="og:description" content="Robot perception systems need to perform reliable image segmentation in real-time on noisy, raw perception data. State-of-the-art segmentation approaches use large CNN models and carefully constructed datasets; however…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1809.03676">

<!--Generated on Thu Mar  7 13:23:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonathan C Balloch                   Varun Agrawal                   Irfan Essa                   Sonia Chernova 
<br class="ltx_break">Georgia Institute of Technology 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">balloch@gatech.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Robot perception systems need to perform reliable image segmentation in real-time on noisy, raw perception data. State-of-the-art segmentation approaches use large CNN models and carefully constructed datasets; however, these models focus on accuracy at the cost of real-time inference. Furthermore, the standard semantic segmentation datasets are not large enough for training CNNs without augmentation and are not representative of noisy, uncurated robot perception data. We propose improving the performance of real-time segmentation frameworks on robot perception data by transferring features learned from synthetic segmentation data. We show that pretraining real-time segmentation architectures with synthetic segmentation data instead of ImageNet improves fine-tuning performance by reducing the bias learned in pretraining and closing the <span id="id2.id1.1" class="ltx_text ltx_font_italic">transfer gap</span> as a result. Our experiments show that our real-time robot perception models pretrained on synthetic data outperform those pretrained on ImageNet for every scale of fine-tuning data examined. Moreover, the degree to which synthetic pretraining outperforms ImageNet pretraining increases as the availability of robot data decreases, making our approach attractive for robotics domains where dataset collection is hard and/or expensive.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Intelligent robots depend on reliable semantic segmentation of objects to support effective physical interactions. While convolutional neural networks (CNNs) have significantly improved the performance of segmentation solutions, two challenges remain. First, the pixel-level classification task of segmentation requires models that can represent more complex distributions than those of object detection with bounding box regression and image classification. Second, the cost of annotating pixel-level segmentation data is prohibitive at the scale needed to train CNNs. Furthermore, many state-of-the-art semantic segmentation approaches do not readily extend to robot vision, as models with millions of parameters requiring billions of operations to classify each image remain impractical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Robot vision requires efficient CNN segmentation architectures that can be successfully trained on uncurated datasets acquired from robots interacting with the world, and segment inherently noisy perception data in real-time.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To this end, we present a novel perspective for developing robust semantic segmentation systems for robot perception. Specifically, our approach of transferring representations pretrained on synthetic segmentation data to real-time perception systems strictly improves performance by reducing bias in training. Synthetic segmentation datasets have the advantages of being scalable to millions of examples, giving perfect ground truth labels without extra annotation effort, and having less dataset bias.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recent work has demonstrated that for large, non-real-time segmentation architectures, models pretrained with synthetic datasets can out-perform models pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. For our task aimed at robot vision, we explore whether this result holds for real-time segmentation architectures with less representational capacity by pretraining a real-time model with synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and comparing its fine-tuned performance with a model pretrained on ImageNet data. Our results to this ablation experiment show that for real-time segmentation architectures, synthetic data pretraining yields better performance than ImageNet, and that these performance improvements are greater than the performance gains in larger architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To further validate our hypothesis, we investigate how models that are pretrained with synthetic data handle noise and scale in robot datasets. Typically datasets acquired from robots have sparse supervision, making it difficult to train a semantic segmentation model that accommodates the increased noise and bias in both the inputs and the labels. We examine the performance of synthetic data pretrained models (in comparison to ImageNet pretrained models) by fine-tuning on various subsets of a robot navigation dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to quantify the benefits as the amount of supervised fine-tuning data is decreased. Our results show the synthetic data pretrained models outperform the models with ImageNet pretraining for every amount of fine-tuning data, and that the performance improvement increases as the number of robot data training samples decreases.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Lastly, we consider whether the improvements in performance due to synthetic data pretraining is caused strictly by similarity of the high-level scenarios of the synthetic pretraining data and the target data. To test this, we consider the two standard datasets from the ablation experiment as our target data, where the high-level scenarios are “driving” and “indoor navigation” respectively. For both of these target datasets sets we train two models, one pretrained on a dataset designed for a similar high-level scenario and the other pretrained on data from a different high-level scenario. Our results show that models pretrained using a synthetic dataset, that is similar to the target task, display improvements on the task. However, we also show that there is a greater benefit for the target robot vision task in pretraining on a synthetic dataset that has more input diversity, more coverage, and less bias, regardless of the high-level similarity.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our primary contributions are: (1) Extending the benefits of transferring features from synthetic data pretraining instead of ImageNet pretraining to real-time-optimized segmentation CNNs, (2) Demonstrating that transferring features from synthetic segmentation data helps reduce the amount of target robot data needed for strong real-time segmentation performance, and (3) Exploring the effect of “high-level domain similarity” between datasets, and showing that while synthetic data that has a similar high-level domain does give some improvement to performance, other properties of data, such as scale, diversity, and bias, have a greater effect on performance.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S1.F1.13" class="ltx_p ltx_figure_panel ltx_align_center">(a)    
<br class="ltx_break">(b)    
<br class="ltx_break">(c)    
<br class="ltx_break">(d)    
<br class="ltx_break"></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/4_256.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/19_256.jpg" id="S1.F1.2.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/input_1120_img.png" id="S1.F1.3.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/scenenet_4_img.png" id="S1.F1.4.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/scenenet_19_img.png" id="S1.F1.5.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/pred_1120_img.png" id="S1.F1.6.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/imagenet_4_img.png" id="S1.F1.7.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/imagenet_19_img.png" id="S1.F1.8.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/pred_1120_img_IN.png" id="S1.F1.9.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/target_4_img.png" id="S1.F1.10.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.11" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/target_19_img.png" id="S1.F1.11.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.12" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/target_1120_img.png" id="S1.F1.12.g1" class="ltx_graphics ltx_img_square" width="90" height="90" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Qualitative results from the ablation and training set reduction experiments. Cityscapes is in column 1, SUN RGBD in column 2, and the Robot@Home Data is in column 3. The first row (a) shows the original images, the second row (b) shows our results from the SceneNet RGB-D pretrained model, the third shows results from the ImageNet pretrained model, and the last row shows the ground truth.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Semantic Segmentation in Computer Vision</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Recent work on semantic segmentation using CNNs has greatly advanced the subfield, but with limited focus on robot vision. Most approaches often use some combination of large convolutions, a strictly serialized layer setup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, networks with over one hundred layers, or fully connected layers at the end of the network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. As a result, these methods have hundreds of millions of parameters, are slow to train, and evaluate far outside of real-time for semantic segmentation. Thus, models that can do segmentation in real-time, as is required for robotics, are sparse in the literature and while there has been some effort for real-time general vision architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, they often perform poorly at segmentation. E-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is the first effort to take advantage of all of these techniques, and the proposed architecture is specifically designed for real-time segmentation. None of these works, however, has examined the efficacy of the above architectures when trained on small, noisy robot vision datasets.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Segmentation Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Segmentation datasets have become more relevant and abundant recently as the computer vision community has become more interested in segmentation, but they rarely apply to robotics directly and are not large enough to train models without pretraining. Popular segmentation datasets that are relevant to robotics include datasets for autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and indoor datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Recently, the Robot@Home dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> was published, which provides over 30K instance-labeled frames from over 80 sequences of a real robot navigating in six unique indoor environments. This is a vast improvement over previously available datasets for robot vision research, however all these datasets are still too small to be useful without pretraining and do not represent the complexities of robot vision well.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Synthetic segmentation datasets have become popular recently, but are still underutilized and do not randomize the simulation conditions to increase diversity and remove bias. For autonomous driving, efforts including Shafaei <em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Virtual KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, “Driving in the Matrix” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and the GTA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> all use high-fidelity simulations or video games to efficiently build realistic datasets, but all have less than 50K frames, which is many times too small to train robust autonomous vehicle perception systems. The SYNTHIA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> contains 200K frames captured across eight cameras that form a 360-degree array, but this leaves only 25K examples to train systems with forward-facing cameras. For indoor environments, Song <em id="S2.SS2.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p2.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and Qui <em id="S2.SS2.p2.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p2.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> created datasets of over 2 million frames from thousands of rooms, but still are subject to dataset bias by being highly ordered. Taking this idea even farther, McCormac <em id="S2.SS2.p2.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p2.1.8" class="ltx_text"></span> created a comprehensive indoor dataset called SceneNet RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> which generates and renders 5 million rendered RGB-D frames sampled from video through 3D scenes with randomized object compositions, textures, lighting, and camera trajectories. However, despite the availability of this large scale simulated data, computer vision researchers and roboticists alike continue to use ImageNet for pretraining since little evidence exists to properly explain the benefits of using simulated data.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Real-time Segmentation in Robot Vision</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">There are some recent efforts catering to real-time semantic segmentation of objects for robotic systems, especially with regards to autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and grasping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>; however the majority of these do not strictly enforce real-time requirements, nor do they utilize synthetic data. James <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> examine the effect of changing the amount of synthetic pretraining data for their grasping task, but the smallest amount of data they consider is 100K images, they do not operate in natural environments, nor are they concerned with real-time performance. Madaan <em id="S2.SS3.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p1.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Lin <em id="S2.SS3.p1.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p1.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> utilize synthetic data to train a custom real-time CNN for segmentation on a robot. However, each of these works focus on simply segmenting a binary mask. Our work segments entire scenes into many objects, motivated by scenarios of more complex robots like home robots and autonomous cars. Most closely related to our work, McCormac <em id="S2.SS3.p1.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p1.1.8" class="ltx_text"></span> improves semantic segmentation for the task of depth-based simultaneous localization and mapping for robots <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> using synthetic image data. McCormac <em id="S2.SS3.p1.1.9" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p1.1.10" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> go on to demonstrate that the large U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> architecture pretrained on SceneNet RGB-D outperforms the same architecture pretrained on ImageNet. Our work differs from prior efforts in that it examines whether the use of synthetic data improves a small, real-time architecture, analyzes the ways in which using synthetic data affects performance as we vary the amount of target task data, and examine how high-level similarity between pretraining and target data effects performance.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Closely related to this work is recent work on domain adaptation, including in semantic segmentation. In different task areas, the domain randomization work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> <em id="S2.SS3.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p2.1.2" class="ltx_text"></span> illustrates how randomization helps with transfer and adaptation for robot learning from vision. Mayer <em id="S2.SS3.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p2.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> expands further on this notion in learning optical flow, observing that accuracy is not necessarily beneficial to domain adaptation from simulation. In segmentation, the aforementioned SYNTHIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> dataset uses diverse simulation data as a means of on domain adaptation between different weather conditions in autonomous driving. Hoffman <em id="S2.SS3.p2.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p2.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and Zhang <em id="S2.SS3.p2.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p2.1.8" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> take a slightly different tack, exploring methods of building domain adaptation capabilities into the learner itself, instead of depending on diverse training data. While these works are related, our work focuses specifically on the challenge of adaptation of small networks that lack representational capacity to distinguish signal from dataset bias, and how carefully selecting pre-training data can alleviate this issue and provide increasing degrees of improvement as fine-tuning data is reduced.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Transfer Learning with Synthetic Data as Bias Reduction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Dataset bias is an often overlooked component of training data-driven computer vision models. Especially in the context of transfer learning, the assumptions made about which distributions are “similar” are often naive, and as a result leave performance gains unrealized. We first examine the distributions modeled to mathematically justify the use of a synthetic dataset over ImageNet data.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transfer learning and pretraining</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">The most effective and commonly utilized method for augmenting performance of models using small datasets is <span id="S3.SS1.p1.4.1" class="ltx_text ltx_font_italic">transfer learning</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, where one exploits the similarity in two distributions, <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">P</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑃</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">P_{i}</annotation></semantics></math> and <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="P_{t}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">P</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑃</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">P_{t}</annotation></semantics></math>, by using parameters optimized to represent the initial distribution <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">P</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑃</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">P_{i}</annotation></semantics></math> to provide an improved starting point for learning the target distribution <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="P_{t}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">P</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑃</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">P_{t}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Transfer learning is most often executed using <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">pretraining</span>, where a model is first trained on one task with a similar input domain to the target task, and then that model is used to initialize the network parameters. There are two typical ways of using the pretrained model: either the target task data is used to continue training the entire model over the transfered parameters, or the pretrained parameters are “frozen”, except for the inference parameters which are reinitialized randomly, and the target task data is used to optimize only the inference layers.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">In supervised learning, the CNN and its parameters <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="M_{\theta}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">M</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑀</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">M_{\theta}</annotation></semantics></math> can be thought of as forming an approximate representation of the task likelihood distribution <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="P_{t}(Y|X)" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><msub id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.3.2.cmml">P</mi><mi id="S3.SS1.p3.2.m2.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml">X</mi></mrow><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2"></times><apply id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.2">𝑃</ci><ci id="S3.SS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3">𝑡</ci></apply><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2">𝑌</ci><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3">𝑋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">P_{t}(Y|X)</annotation></semantics></math>. By Bayes Theorem, we know that</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="M_{\theta}\approx P_{t}(Y|X)=\frac{P_{t}(Y)P_{t}(X|Y)}{P_{t}(X)}" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><msub id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml">M</mi><mi id="S3.E1.m1.4.4.3.3" xref="S3.E1.m1.4.4.3.3.cmml">θ</mi></msub><mo id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml">≈</mo><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml"><msub id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.4.4.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.4.4.1.3.2.cmml">P</mi><mi id="S3.E1.m1.4.4.1.3.3" xref="S3.E1.m1.4.4.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.4.4.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.3.cmml">X</mi></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.4.5.cmml">=</mo><mfrac id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mi id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml">P</mi><mi id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.5.2.1" xref="S3.E1.m1.2.2.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">Y</mi><mo stretchy="false" id="S3.E1.m1.2.2.2.5.2.2" xref="S3.E1.m1.2.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3a" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><msub id="S3.E1.m1.2.2.2.6" xref="S3.E1.m1.2.2.2.6.cmml"><mi id="S3.E1.m1.2.2.2.6.2" xref="S3.E1.m1.2.2.2.6.2.cmml">P</mi><mi id="S3.E1.m1.2.2.2.6.3" xref="S3.E1.m1.2.2.2.6.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3b" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml">X</mi><mo fence="false" id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">|</mo><mi id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><msub id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.2.cmml">P</mi><mi id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.4.2" xref="S3.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.4.2.1" xref="S3.E1.m1.3.3.3.cmml">(</mo><mi id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">X</mi><mo stretchy="false" id="S3.E1.m1.3.3.3.4.2.2" xref="S3.E1.m1.3.3.3.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><and id="S3.E1.m1.4.4a.cmml" xref="S3.E1.m1.4.4"></and><apply id="S3.E1.m1.4.4b.cmml" xref="S3.E1.m1.4.4"><approx id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"></approx><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3.1.cmml" xref="S3.E1.m1.4.4.3">subscript</csymbol><ci id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2">𝑀</ci><ci id="S3.E1.m1.4.4.3.3.cmml" xref="S3.E1.m1.4.4.3.3">𝜃</ci></apply><apply id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1"><times id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.2"></times><apply id="S3.E1.m1.4.4.1.3.cmml" xref="S3.E1.m1.4.4.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2">𝑃</ci><ci id="S3.E1.m1.4.4.1.3.3.cmml" xref="S3.E1.m1.4.4.1.3.3">𝑡</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2">𝑌</ci><ci id="S3.E1.m1.4.4.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3">𝑋</ci></apply></apply></apply><apply id="S3.E1.m1.4.4c.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4.5"></eq><share href="#S3.E1.m1.4.4.1.cmml" id="S3.E1.m1.4.4d.cmml" xref="S3.E1.m1.4.4"></share><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><divide id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2">𝑃</ci><ci id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3">𝑡</ci></apply><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑌</ci><apply id="S3.E1.m1.2.2.2.6.cmml" xref="S3.E1.m1.2.2.2.6"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.6.1.cmml" xref="S3.E1.m1.2.2.2.6">subscript</csymbol><ci id="S3.E1.m1.2.2.2.6.2.cmml" xref="S3.E1.m1.2.2.2.6.2">𝑃</ci><ci id="S3.E1.m1.2.2.2.6.3.cmml" xref="S3.E1.m1.2.2.2.6.3">𝑡</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1">conditional</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">𝑋</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">𝑌</ci></apply></apply><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2"></times><apply id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.2">𝑃</ci><ci id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3">𝑡</ci></apply><ci id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1">𝑋</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">M_{\theta}\approx P_{t}(Y|X)=\frac{P_{t}(Y)P_{t}(X|Y)}{P_{t}(X)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.3" class="ltx_p">where the posterior distribution <math id="S3.SS1.p3.3.m1.1" class="ltx_Math" alttext="P_{t}(X|Y)" display="inline"><semantics id="S3.SS1.p3.3.m1.1a"><mrow id="S3.SS1.p3.3.m1.1.1" xref="S3.SS1.p3.3.m1.1.1.cmml"><msub id="S3.SS1.p3.3.m1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.3.cmml"><mi id="S3.SS1.p3.3.m1.1.1.3.2" xref="S3.SS1.p3.3.m1.1.1.3.2.cmml">P</mi><mi id="S3.SS1.p3.3.m1.1.1.3.3" xref="S3.SS1.p3.3.m1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.3.m1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.2.cmml">​</mo><mrow id="S3.SS1.p3.3.m1.1.1.1.1" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.3.m1.1.1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.3.m1.1.1.1.1.1" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS1.p3.3.m1.1.1.1.1.1.1" xref="S3.SS1.p3.3.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p3.3.m1.1.1.1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS1.p3.3.m1.1.1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m1.1b"><apply id="S3.SS1.p3.3.m1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1"><times id="S3.SS1.p3.3.m1.1.1.2.cmml" xref="S3.SS1.p3.3.m1.1.1.2"></times><apply id="S3.SS1.p3.3.m1.1.1.3.cmml" xref="S3.SS1.p3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m1.1.1.3.1.cmml" xref="S3.SS1.p3.3.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.3.m1.1.1.3.2.cmml" xref="S3.SS1.p3.3.m1.1.1.3.2">𝑃</ci><ci id="S3.SS1.p3.3.m1.1.1.3.3.cmml" xref="S3.SS1.p3.3.m1.1.1.3.3">𝑡</ci></apply><apply id="S3.SS1.p3.3.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.3.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p3.3.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1.1.2">𝑋</ci><ci id="S3.SS1.p3.3.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m1.1c">P_{t}(X|Y)</annotation></semantics></math> in this case is the generative task of constructing images given a label as input.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In transfer learning, the assumption is that, for two supervised problems where the input is sampled from all natural images, the divergence between task likelihood distributions varies as the divergence between the input distributions:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.6" class="ltx_Math" alttext="D_{f}(P_{i}(X),P_{t}(X))\sim D_{f}(P_{i}(Y|X),P_{t}(Y|X))" display="block"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml"><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><msub id="S3.E2.m1.4.4.2.4" xref="S3.E2.m1.4.4.2.4.cmml"><mi id="S3.E2.m1.4.4.2.4.2" xref="S3.E2.m1.4.4.2.4.2.cmml">D</mi><mi id="S3.E2.m1.4.4.2.4.3" xref="S3.E2.m1.4.4.2.4.3.cmml">f</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.cmml">P</mi><mi id="S3.E2.m1.3.3.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">X</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.2.2.2.4" xref="S3.E2.m1.4.4.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.cmml"><msub id="S3.E2.m1.4.4.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.cmml"><mi id="S3.E2.m1.4.4.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.cmml">P</mi><mi id="S3.E2.m1.4.4.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.2.2.1" xref="S3.E2.m1.4.4.2.2.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.2.2.2.3.2" xref="S3.E2.m1.4.4.2.2.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.2.3.2.1" xref="S3.E2.m1.4.4.2.2.2.2.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">X</mi><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.2.3.2.2" xref="S3.E2.m1.4.4.2.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.5" xref="S3.E2.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.5" xref="S3.E2.m1.6.6.5.cmml">∼</mo><mrow id="S3.E2.m1.6.6.4" xref="S3.E2.m1.6.6.4.cmml"><msub id="S3.E2.m1.6.6.4.4" xref="S3.E2.m1.6.6.4.4.cmml"><mi id="S3.E2.m1.6.6.4.4.2" xref="S3.E2.m1.6.6.4.4.2.cmml">D</mi><mi id="S3.E2.m1.6.6.4.4.3" xref="S3.E2.m1.6.6.4.4.3.cmml">f</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.4.3" xref="S3.E2.m1.6.6.4.3.cmml">​</mo><mrow id="S3.E2.m1.6.6.4.2.2" xref="S3.E2.m1.6.6.4.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.4.2.2.3" xref="S3.E2.m1.6.6.4.2.3.cmml">(</mo><mrow id="S3.E2.m1.5.5.3.1.1.1" xref="S3.E2.m1.5.5.3.1.1.1.cmml"><msub id="S3.E2.m1.5.5.3.1.1.1.3" xref="S3.E2.m1.5.5.3.1.1.1.3.cmml"><mi id="S3.E2.m1.5.5.3.1.1.1.3.2" xref="S3.E2.m1.5.5.3.1.1.1.3.2.cmml">P</mi><mi id="S3.E2.m1.5.5.3.1.1.1.3.3" xref="S3.E2.m1.5.5.3.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.3.1.1.1.2" xref="S3.E2.m1.5.5.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.3.1.1.1.1.1" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.3.1.1.1.1.1.2" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.3.1.1.1.1.1.1" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.3.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="S3.E2.m1.5.5.3.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E2.m1.5.5.3.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.3.cmml">X</mi></mrow><mo stretchy="false" id="S3.E2.m1.5.5.3.1.1.1.1.1.3" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.4.2.2.4" xref="S3.E2.m1.6.6.4.2.3.cmml">,</mo><mrow id="S3.E2.m1.6.6.4.2.2.2" xref="S3.E2.m1.6.6.4.2.2.2.cmml"><msub id="S3.E2.m1.6.6.4.2.2.2.3" xref="S3.E2.m1.6.6.4.2.2.2.3.cmml"><mi id="S3.E2.m1.6.6.4.2.2.2.3.2" xref="S3.E2.m1.6.6.4.2.2.2.3.2.cmml">P</mi><mi id="S3.E2.m1.6.6.4.2.2.2.3.3" xref="S3.E2.m1.6.6.4.2.2.2.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.4.2.2.2.2" xref="S3.E2.m1.6.6.4.2.2.2.2.cmml">​</mo><mrow id="S3.E2.m1.6.6.4.2.2.2.1.1" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.4.2.2.2.1.1.2" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.4.2.2.2.1.1.1" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.6.6.4.2.2.2.1.1.1.2" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.2.cmml">Y</mi><mo fence="false" id="S3.E2.m1.6.6.4.2.2.2.1.1.1.1" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.E2.m1.6.6.4.2.2.2.1.1.1.3" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.3.cmml">X</mi></mrow><mo stretchy="false" id="S3.E2.m1.6.6.4.2.2.2.1.1.3" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.6.6.4.2.2.5" xref="S3.E2.m1.6.6.4.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"><csymbol cd="latexml" id="S3.E2.m1.6.6.5.cmml" xref="S3.E2.m1.6.6.5">similar-to</csymbol><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><times id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3"></times><apply id="S3.E2.m1.4.4.2.4.cmml" xref="S3.E2.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.4.1.cmml" xref="S3.E2.m1.4.4.2.4">subscript</csymbol><ci id="S3.E2.m1.4.4.2.4.2.cmml" xref="S3.E2.m1.4.4.2.4.2">𝐷</ci><ci id="S3.E2.m1.4.4.2.4.3.cmml" xref="S3.E2.m1.4.4.2.4.3">𝑓</ci></apply><interval closure="open" id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"></times><apply id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2">𝑃</ci><ci id="S3.E2.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑋</ci></apply><apply id="S3.E2.m1.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2"><times id="S3.E2.m1.4.4.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.1"></times><apply id="S3.E2.m1.4.4.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2">𝑃</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.3">𝑡</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑋</ci></apply></interval></apply><apply id="S3.E2.m1.6.6.4.cmml" xref="S3.E2.m1.6.6.4"><times id="S3.E2.m1.6.6.4.3.cmml" xref="S3.E2.m1.6.6.4.3"></times><apply id="S3.E2.m1.6.6.4.4.cmml" xref="S3.E2.m1.6.6.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.4.1.cmml" xref="S3.E2.m1.6.6.4.4">subscript</csymbol><ci id="S3.E2.m1.6.6.4.4.2.cmml" xref="S3.E2.m1.6.6.4.4.2">𝐷</ci><ci id="S3.E2.m1.6.6.4.4.3.cmml" xref="S3.E2.m1.6.6.4.4.3">𝑓</ci></apply><interval closure="open" id="S3.E2.m1.6.6.4.2.3.cmml" xref="S3.E2.m1.6.6.4.2.2"><apply id="S3.E2.m1.5.5.3.1.1.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1"><times id="S3.E2.m1.5.5.3.1.1.1.2.cmml" xref="S3.E2.m1.5.5.3.1.1.1.2"></times><apply id="S3.E2.m1.5.5.3.1.1.1.3.cmml" xref="S3.E2.m1.5.5.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.3.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.5.5.3.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.3.1.1.1.3.2">𝑃</ci><ci id="S3.E2.m1.5.5.3.1.1.1.3.3.cmml" xref="S3.E2.m1.5.5.3.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.5.5.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.5.5.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.5.5.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.2">𝑌</ci><ci id="S3.E2.m1.5.5.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.1.1.3">𝑋</ci></apply></apply><apply id="S3.E2.m1.6.6.4.2.2.2.cmml" xref="S3.E2.m1.6.6.4.2.2.2"><times id="S3.E2.m1.6.6.4.2.2.2.2.cmml" xref="S3.E2.m1.6.6.4.2.2.2.2"></times><apply id="S3.E2.m1.6.6.4.2.2.2.3.cmml" xref="S3.E2.m1.6.6.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.2.2.2.3.1.cmml" xref="S3.E2.m1.6.6.4.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.6.6.4.2.2.2.3.2.cmml" xref="S3.E2.m1.6.6.4.2.2.2.3.2">𝑃</ci><ci id="S3.E2.m1.6.6.4.2.2.2.3.3.cmml" xref="S3.E2.m1.6.6.4.2.2.2.3.3">𝑡</ci></apply><apply id="S3.E2.m1.6.6.4.2.2.2.1.1.1.cmml" xref="S3.E2.m1.6.6.4.2.2.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.4.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.6.6.4.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.2">𝑌</ci><ci id="S3.E2.m1.6.6.4.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.6.6.4.2.2.2.1.1.1.3">𝑋</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">D_{f}(P_{i}(X),P_{t}(X))\sim D_{f}(P_{i}(Y|X),P_{t}(Y|X))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.3" class="ltx_p">This assumption is justified by empirical successes improving a wide range of computer vision problems by first pretraining on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. These assumptions, however, do not account for dataset bias nor the difference between <math id="S3.SS1.p5.1.m1.2" class="ltx_Math" alttext="P_{i}(Y)P_{i}(X|Y)" display="inline"><semantics id="S3.SS1.p5.1.m1.2a"><mrow id="S3.SS1.p5.1.m1.2.2" xref="S3.SS1.p5.1.m1.2.2.cmml"><msub id="S3.SS1.p5.1.m1.2.2.3" xref="S3.SS1.p5.1.m1.2.2.3.cmml"><mi id="S3.SS1.p5.1.m1.2.2.3.2" xref="S3.SS1.p5.1.m1.2.2.3.2.cmml">P</mi><mi id="S3.SS1.p5.1.m1.2.2.3.3" xref="S3.SS1.p5.1.m1.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.2.2.2" xref="S3.SS1.p5.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS1.p5.1.m1.2.2.4.2" xref="S3.SS1.p5.1.m1.2.2.cmml"><mo stretchy="false" id="S3.SS1.p5.1.m1.2.2.4.2.1" xref="S3.SS1.p5.1.m1.2.2.cmml">(</mo><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS1.p5.1.m1.2.2.4.2.2" xref="S3.SS1.p5.1.m1.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.2.2.2a" xref="S3.SS1.p5.1.m1.2.2.2.cmml">​</mo><msub id="S3.SS1.p5.1.m1.2.2.5" xref="S3.SS1.p5.1.m1.2.2.5.cmml"><mi id="S3.SS1.p5.1.m1.2.2.5.2" xref="S3.SS1.p5.1.m1.2.2.5.2.cmml">P</mi><mi id="S3.SS1.p5.1.m1.2.2.5.3" xref="S3.SS1.p5.1.m1.2.2.5.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.2.2.2b" xref="S3.SS1.p5.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS1.p5.1.m1.2.2.1.1" xref="S3.SS1.p5.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p5.1.m1.2.2.1.1.2" xref="S3.SS1.p5.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p5.1.m1.2.2.1.1.1" xref="S3.SS1.p5.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p5.1.m1.2.2.1.1.1.2" xref="S3.SS1.p5.1.m1.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS1.p5.1.m1.2.2.1.1.1.1" xref="S3.SS1.p5.1.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p5.1.m1.2.2.1.1.1.3" xref="S3.SS1.p5.1.m1.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS1.p5.1.m1.2.2.1.1.3" xref="S3.SS1.p5.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.2b"><apply id="S3.SS1.p5.1.m1.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2"><times id="S3.SS1.p5.1.m1.2.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2.2"></times><apply id="S3.SS1.p5.1.m1.2.2.3.cmml" xref="S3.SS1.p5.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.2.2.3.1.cmml" xref="S3.SS1.p5.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.p5.1.m1.2.2.3.2.cmml" xref="S3.SS1.p5.1.m1.2.2.3.2">𝑃</ci><ci id="S3.SS1.p5.1.m1.2.2.3.3.cmml" xref="S3.SS1.p5.1.m1.2.2.3.3">𝑖</ci></apply><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝑌</ci><apply id="S3.SS1.p5.1.m1.2.2.5.cmml" xref="S3.SS1.p5.1.m1.2.2.5"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.2.2.5.1.cmml" xref="S3.SS1.p5.1.m1.2.2.5">subscript</csymbol><ci id="S3.SS1.p5.1.m1.2.2.5.2.cmml" xref="S3.SS1.p5.1.m1.2.2.5.2">𝑃</ci><ci id="S3.SS1.p5.1.m1.2.2.5.3.cmml" xref="S3.SS1.p5.1.m1.2.2.5.3">𝑖</ci></apply><apply id="S3.SS1.p5.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p5.1.m1.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p5.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p5.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS1.p5.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.2c">P_{i}(Y)P_{i}(X|Y)</annotation></semantics></math> and <math id="S3.SS1.p5.2.m2.2" class="ltx_Math" alttext="P_{t}(Y)P_{t}(X|Y)" display="inline"><semantics id="S3.SS1.p5.2.m2.2a"><mrow id="S3.SS1.p5.2.m2.2.2" xref="S3.SS1.p5.2.m2.2.2.cmml"><msub id="S3.SS1.p5.2.m2.2.2.3" xref="S3.SS1.p5.2.m2.2.2.3.cmml"><mi id="S3.SS1.p5.2.m2.2.2.3.2" xref="S3.SS1.p5.2.m2.2.2.3.2.cmml">P</mi><mi id="S3.SS1.p5.2.m2.2.2.3.3" xref="S3.SS1.p5.2.m2.2.2.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.2.2.2" xref="S3.SS1.p5.2.m2.2.2.2.cmml">​</mo><mrow id="S3.SS1.p5.2.m2.2.2.4.2" xref="S3.SS1.p5.2.m2.2.2.cmml"><mo stretchy="false" id="S3.SS1.p5.2.m2.2.2.4.2.1" xref="S3.SS1.p5.2.m2.2.2.cmml">(</mo><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS1.p5.2.m2.2.2.4.2.2" xref="S3.SS1.p5.2.m2.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.2.2.2a" xref="S3.SS1.p5.2.m2.2.2.2.cmml">​</mo><msub id="S3.SS1.p5.2.m2.2.2.5" xref="S3.SS1.p5.2.m2.2.2.5.cmml"><mi id="S3.SS1.p5.2.m2.2.2.5.2" xref="S3.SS1.p5.2.m2.2.2.5.2.cmml">P</mi><mi id="S3.SS1.p5.2.m2.2.2.5.3" xref="S3.SS1.p5.2.m2.2.2.5.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.2.2.2b" xref="S3.SS1.p5.2.m2.2.2.2.cmml">​</mo><mrow id="S3.SS1.p5.2.m2.2.2.1.1" xref="S3.SS1.p5.2.m2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p5.2.m2.2.2.1.1.2" xref="S3.SS1.p5.2.m2.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p5.2.m2.2.2.1.1.1" xref="S3.SS1.p5.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p5.2.m2.2.2.1.1.1.2" xref="S3.SS1.p5.2.m2.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS1.p5.2.m2.2.2.1.1.1.1" xref="S3.SS1.p5.2.m2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p5.2.m2.2.2.1.1.1.3" xref="S3.SS1.p5.2.m2.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS1.p5.2.m2.2.2.1.1.3" xref="S3.SS1.p5.2.m2.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.2b"><apply id="S3.SS1.p5.2.m2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2"><times id="S3.SS1.p5.2.m2.2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2.2"></times><apply id="S3.SS1.p5.2.m2.2.2.3.cmml" xref="S3.SS1.p5.2.m2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.2.2.3.1.cmml" xref="S3.SS1.p5.2.m2.2.2.3">subscript</csymbol><ci id="S3.SS1.p5.2.m2.2.2.3.2.cmml" xref="S3.SS1.p5.2.m2.2.2.3.2">𝑃</ci><ci id="S3.SS1.p5.2.m2.2.2.3.3.cmml" xref="S3.SS1.p5.2.m2.2.2.3.3">𝑡</ci></apply><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝑌</ci><apply id="S3.SS1.p5.2.m2.2.2.5.cmml" xref="S3.SS1.p5.2.m2.2.2.5"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.2.2.5.1.cmml" xref="S3.SS1.p5.2.m2.2.2.5">subscript</csymbol><ci id="S3.SS1.p5.2.m2.2.2.5.2.cmml" xref="S3.SS1.p5.2.m2.2.2.5.2">𝑃</ci><ci id="S3.SS1.p5.2.m2.2.2.5.3.cmml" xref="S3.SS1.p5.2.m2.2.2.5.3">𝑡</ci></apply><apply id="S3.SS1.p5.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p5.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p5.2.m2.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p5.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p5.2.m2.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS1.p5.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p5.2.m2.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.2c">P_{t}(Y)P_{t}(X|Y)</annotation></semantics></math>, or <math id="S3.SS1.p5.3.m3.2" class="ltx_Math" alttext="\Delta P(Y)P(X|Y)" display="inline"><semantics id="S3.SS1.p5.3.m3.2a"><mrow id="S3.SS1.p5.3.m3.2.2" xref="S3.SS1.p5.3.m3.2.2.cmml"><mi mathvariant="normal" id="S3.SS1.p5.3.m3.2.2.3" xref="S3.SS1.p5.3.m3.2.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.3.m3.2.2.2" xref="S3.SS1.p5.3.m3.2.2.2.cmml">​</mo><mi id="S3.SS1.p5.3.m3.2.2.4" xref="S3.SS1.p5.3.m3.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.3.m3.2.2.2a" xref="S3.SS1.p5.3.m3.2.2.2.cmml">​</mo><mrow id="S3.SS1.p5.3.m3.2.2.5.2" xref="S3.SS1.p5.3.m3.2.2.cmml"><mo stretchy="false" id="S3.SS1.p5.3.m3.2.2.5.2.1" xref="S3.SS1.p5.3.m3.2.2.cmml">(</mo><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS1.p5.3.m3.2.2.5.2.2" xref="S3.SS1.p5.3.m3.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p5.3.m3.2.2.2b" xref="S3.SS1.p5.3.m3.2.2.2.cmml">​</mo><mi id="S3.SS1.p5.3.m3.2.2.6" xref="S3.SS1.p5.3.m3.2.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.3.m3.2.2.2c" xref="S3.SS1.p5.3.m3.2.2.2.cmml">​</mo><mrow id="S3.SS1.p5.3.m3.2.2.1.1" xref="S3.SS1.p5.3.m3.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p5.3.m3.2.2.1.1.2" xref="S3.SS1.p5.3.m3.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p5.3.m3.2.2.1.1.1" xref="S3.SS1.p5.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS1.p5.3.m3.2.2.1.1.1.2" xref="S3.SS1.p5.3.m3.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS1.p5.3.m3.2.2.1.1.1.1" xref="S3.SS1.p5.3.m3.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p5.3.m3.2.2.1.1.1.3" xref="S3.SS1.p5.3.m3.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS1.p5.3.m3.2.2.1.1.3" xref="S3.SS1.p5.3.m3.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.2b"><apply id="S3.SS1.p5.3.m3.2.2.cmml" xref="S3.SS1.p5.3.m3.2.2"><times id="S3.SS1.p5.3.m3.2.2.2.cmml" xref="S3.SS1.p5.3.m3.2.2.2"></times><ci id="S3.SS1.p5.3.m3.2.2.3.cmml" xref="S3.SS1.p5.3.m3.2.2.3">Δ</ci><ci id="S3.SS1.p5.3.m3.2.2.4.cmml" xref="S3.SS1.p5.3.m3.2.2.4">𝑃</ci><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">𝑌</ci><ci id="S3.SS1.p5.3.m3.2.2.6.cmml" xref="S3.SS1.p5.3.m3.2.2.6">𝑃</ci><apply id="S3.SS1.p5.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p5.3.m3.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p5.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p5.3.m3.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p5.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p5.3.m3.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS1.p5.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p5.3.m3.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.2c">\Delta P(Y)P(X|Y)</annotation></semantics></math>, and as such leave room for improvement.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The transfer learning gap</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The Torralba <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> study on dataset bias shows that even the most carefully constructed image datasets contain significant and distinct enough bias that a simple linear discriminative model can distinguish between them based on their inputs alone. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> concludes that dataset bias, specifically input bias, comes in three main forms: <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">selection bias</span> (images selected manually inherently have more bias than those obtained randomly or automatically), <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_bold">capture bias</span> (image content is curated, e.g. photographs taken by people, objects are most often photographed from specific angles), and <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_bold">negative set bias</span>, (datasets only collect items of interest, yielding models that do not sufficiently represent negative cases)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>It is worth noting that negative set bias is less of a concern in segmentation because pixels are conditionally dependent on their neighborhood, and in general the more classes something is trying to predict the more negative examples that class has.</span></span></span>. Additionally, one bias that Torralba <em id="S3.SS2.p1.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p1.1.7" class="ltx_text"></span> do not mention is <span id="S3.SS2.p1.1.8" class="ltx_text ltx_font_bold">annotation bias</span>, or the bias from errors in the human annotation; this is especially important in segmentation due to the large effect annotation errors near object boundaries have on prediction. Standard computer vision datasets like ImageNet have inputs that are carefully selected, and sampled from biased Web images that were captured and selected by the humans that took them, and have humans annotating them.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">Robot vision datasets, on the other hand, are mostly uncurated, noisy, and have human annotators who bias segmentation annotations differently than the annotations of an image classification the task like ImageNet. We can assume, therefore, that in transferring features from a dataset like ImageNet there exist some difference in the biases of the inputs <math id="S3.SS2.p2.1.m1.1" class="ltx_math_unparsed" alttext="P_{(}X)" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1b"><msub id="S3.SS2.p2.1.m1.1.1"><mi id="S3.SS2.p2.1.m1.1.1.2">P</mi><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.3">(</mo></msub><mi id="S3.SS2.p2.1.m1.1.2">X</mi><mo stretchy="false" id="S3.SS2.p2.1.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">P_{(}X)</annotation></semantics></math> and annotations <math id="S3.SS2.p2.2.m2.1" class="ltx_math_unparsed" alttext="P_{(}Y)" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1b"><msub id="S3.SS2.p2.2.m2.1.1"><mi id="S3.SS2.p2.2.m2.1.1.2">P</mi><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.3">(</mo></msub><mi id="S3.SS2.p2.2.m2.1.2">Y</mi><mo stretchy="false" id="S3.SS2.p2.2.m2.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">P_{(}Y)</annotation></semantics></math> that cannot be closed trivially. These three types of input bias, annotation bias, and <math id="S3.SS2.p2.3.m3.2" class="ltx_Math" alttext="\Delta P(Y)P(X|Y)" display="inline"><semantics id="S3.SS2.p2.3.m3.2a"><mrow id="S3.SS2.p2.3.m3.2.2" xref="S3.SS2.p2.3.m3.2.2.cmml"><mi mathvariant="normal" id="S3.SS2.p2.3.m3.2.2.3" xref="S3.SS2.p2.3.m3.2.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.cmml">​</mo><mi id="S3.SS2.p2.3.m3.2.2.4" xref="S3.SS2.p2.3.m3.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.2.2.2a" xref="S3.SS2.p2.3.m3.2.2.2.cmml">​</mo><mrow id="S3.SS2.p2.3.m3.2.2.5.2" xref="S3.SS2.p2.3.m3.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.2.2.5.2.1" xref="S3.SS2.p2.3.m3.2.2.cmml">(</mo><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS2.p2.3.m3.2.2.5.2.2" xref="S3.SS2.p2.3.m3.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.2.2.2b" xref="S3.SS2.p2.3.m3.2.2.2.cmml">​</mo><mi id="S3.SS2.p2.3.m3.2.2.6" xref="S3.SS2.p2.3.m3.2.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.2.2.2c" xref="S3.SS2.p2.3.m3.2.2.2.cmml">​</mo><mrow id="S3.SS2.p2.3.m3.2.2.1.1" xref="S3.SS2.p2.3.m3.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.2.2.1.1.2" xref="S3.SS2.p2.3.m3.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.3.m3.2.2.1.1.1" xref="S3.SS2.p2.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.3.m3.2.2.1.1.1.2" xref="S3.SS2.p2.3.m3.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS2.p2.3.m3.2.2.1.1.1.1" xref="S3.SS2.p2.3.m3.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS2.p2.3.m3.2.2.1.1.1.3" xref="S3.SS2.p2.3.m3.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS2.p2.3.m3.2.2.1.1.3" xref="S3.SS2.p2.3.m3.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.2b"><apply id="S3.SS2.p2.3.m3.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2"><times id="S3.SS2.p2.3.m3.2.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2.2"></times><ci id="S3.SS2.p2.3.m3.2.2.3.cmml" xref="S3.SS2.p2.3.m3.2.2.3">Δ</ci><ci id="S3.SS2.p2.3.m3.2.2.4.cmml" xref="S3.SS2.p2.3.m3.2.2.4">𝑃</ci><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑌</ci><ci id="S3.SS2.p2.3.m3.2.2.6.cmml" xref="S3.SS2.p2.3.m3.2.2.6">𝑃</ci><apply id="S3.SS2.p2.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.p2.3.m3.2.2.1.1"><csymbol cd="latexml" id="S3.SS2.p2.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS2.p2.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS2.p2.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.3.m3.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.2c">\Delta P(Y)P(X|Y)</annotation></semantics></math> comprise the <span id="S3.SS2.p2.4.1" class="ltx_text ltx_font_italic">transfer learning gap</span> of two datasets, which we will call <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="G^{tr}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msup id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">G</mi><mrow id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.1" xref="S3.SS2.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝐺</ci><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><times id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2">𝑡</ci><ci id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">G^{tr}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.3" class="ltx_p">Synthetic image datasets created with the same task as the target task can reduce <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="G^{tr}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">G</mi><mrow id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.3.1" xref="S3.SS2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐺</ci><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><times id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">G^{tr}</annotation></semantics></math> for the process of transfer learning. With camera angles and lighting generated randomly, these datasets have virtually no selection and capture bias, except for the capture bias in the construction and arrangement of the simulated scene. This can be further reduced by introducing some inherent randomness in its initialization. Simulation also has the benefit of generating perfect annotations for free with objectively no bias. Intuitively one might guess that transferring from the same task domain will improve the effectiveness of pretraining. We can justify this intuition by inspecting the decomposition of Equation <a href="#S3.E1" title="In 3.1 Transfer learning and pretraining ‣ 3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>; while the distribution <math id="S3.SS2.p3.2.m2.2" class="ltx_Math" alttext="P_{t}(Y)P_{t}(X|Y)" display="inline"><semantics id="S3.SS2.p3.2.m2.2a"><mrow id="S3.SS2.p3.2.m2.2.2" xref="S3.SS2.p3.2.m2.2.2.cmml"><msub id="S3.SS2.p3.2.m2.2.2.3" xref="S3.SS2.p3.2.m2.2.2.3.cmml"><mi id="S3.SS2.p3.2.m2.2.2.3.2" xref="S3.SS2.p3.2.m2.2.2.3.2.cmml">P</mi><mi id="S3.SS2.p3.2.m2.2.2.3.3" xref="S3.SS2.p3.2.m2.2.2.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.2.2" xref="S3.SS2.p3.2.m2.2.2.2.cmml">​</mo><mrow id="S3.SS2.p3.2.m2.2.2.4.2" xref="S3.SS2.p3.2.m2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.2.2.4.2.1" xref="S3.SS2.p3.2.m2.2.2.cmml">(</mo><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS2.p3.2.m2.2.2.4.2.2" xref="S3.SS2.p3.2.m2.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.2.2a" xref="S3.SS2.p3.2.m2.2.2.2.cmml">​</mo><msub id="S3.SS2.p3.2.m2.2.2.5" xref="S3.SS2.p3.2.m2.2.2.5.cmml"><mi id="S3.SS2.p3.2.m2.2.2.5.2" xref="S3.SS2.p3.2.m2.2.2.5.2.cmml">P</mi><mi id="S3.SS2.p3.2.m2.2.2.5.3" xref="S3.SS2.p3.2.m2.2.2.5.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.2.2b" xref="S3.SS2.p3.2.m2.2.2.2.cmml">​</mo><mrow id="S3.SS2.p3.2.m2.2.2.1.1" xref="S3.SS2.p3.2.m2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.2.2.1.1.2" xref="S3.SS2.p3.2.m2.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.2.m2.2.2.1.1.1" xref="S3.SS2.p3.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.2.2.1.1.1.2" xref="S3.SS2.p3.2.m2.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS2.p3.2.m2.2.2.1.1.1.1" xref="S3.SS2.p3.2.m2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS2.p3.2.m2.2.2.1.1.1.3" xref="S3.SS2.p3.2.m2.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS2.p3.2.m2.2.2.1.1.3" xref="S3.SS2.p3.2.m2.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.2b"><apply id="S3.SS2.p3.2.m2.2.2.cmml" xref="S3.SS2.p3.2.m2.2.2"><times id="S3.SS2.p3.2.m2.2.2.2.cmml" xref="S3.SS2.p3.2.m2.2.2.2"></times><apply id="S3.SS2.p3.2.m2.2.2.3.cmml" xref="S3.SS2.p3.2.m2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.2.2.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.2.2.3.2.cmml" xref="S3.SS2.p3.2.m2.2.2.3.2">𝑃</ci><ci id="S3.SS2.p3.2.m2.2.2.3.3.cmml" xref="S3.SS2.p3.2.m2.2.2.3.3">𝑡</ci></apply><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝑌</ci><apply id="S3.SS2.p3.2.m2.2.2.5.cmml" xref="S3.SS2.p3.2.m2.2.2.5"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.2.2.5.1.cmml" xref="S3.SS2.p3.2.m2.2.2.5">subscript</csymbol><ci id="S3.SS2.p3.2.m2.2.2.5.2.cmml" xref="S3.SS2.p3.2.m2.2.2.5.2">𝑃</ci><ci id="S3.SS2.p3.2.m2.2.2.5.3.cmml" xref="S3.SS2.p3.2.m2.2.2.5.3">𝑡</ci></apply><apply id="S3.SS2.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1"><csymbol cd="latexml" id="S3.SS2.p3.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS2.p3.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS2.p3.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.2c">P_{t}(Y)P_{t}(X|Y)</annotation></semantics></math> is difficult to observe, we can see that by rewriting Equation <a href="#S3.E1" title="In 3.1 Transfer learning and pretraining ‣ 3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for a known image <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="x\in X" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">x</mi><mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">∈</mo><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">X</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><in id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></in><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑥</ci><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">x\in X</annotation></semantics></math>, the target task distribution is proportional to the product of its label distribution and its generative posterior.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="M\propto P_{t}(Y)P_{t}(X=x|Y)" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mi id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml">M</mi><mo id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml">∝</mo><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.cmml"><msub id="S3.E3.m1.2.2.1.3" xref="S3.E3.m1.2.2.1.3.cmml"><mi id="S3.E3.m1.2.2.1.3.2" xref="S3.E3.m1.2.2.1.3.2.cmml">P</mi><mi id="S3.E3.m1.2.2.1.3.3" xref="S3.E3.m1.2.2.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.4.2" xref="S3.E3.m1.2.2.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.4.2.1" xref="S3.E3.m1.2.2.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Y</mi><mo stretchy="false" id="S3.E3.m1.2.2.1.4.2.2" xref="S3.E3.m1.2.2.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.2a" xref="S3.E3.m1.2.2.1.2.cmml">​</mo><msub id="S3.E3.m1.2.2.1.5" xref="S3.E3.m1.2.2.1.5.cmml"><mi id="S3.E3.m1.2.2.1.5.2" xref="S3.E3.m1.2.2.1.5.2.cmml">P</mi><mi id="S3.E3.m1.2.2.1.5.3" xref="S3.E3.m1.2.2.1.5.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.2b" xref="S3.E3.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.2.cmml">X</mi><mo id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.1.3.2.cmml">x</mi><mo fence="false" id="S3.E3.m1.2.2.1.1.1.1.3.1" xref="S3.E3.m1.2.2.1.1.1.1.3.1.cmml">|</mo><mi id="S3.E3.m1.2.2.1.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.3.3.cmml">Y</mi></mrow></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><csymbol cd="latexml" id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2">proportional-to</csymbol><ci id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3">𝑀</ci><apply id="S3.E3.m1.2.2.1.cmml" xref="S3.E3.m1.2.2.1"><times id="S3.E3.m1.2.2.1.2.cmml" xref="S3.E3.m1.2.2.1.2"></times><apply id="S3.E3.m1.2.2.1.3.cmml" xref="S3.E3.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.3.1.cmml" xref="S3.E3.m1.2.2.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.3.2.cmml" xref="S3.E3.m1.2.2.1.3.2">𝑃</ci><ci id="S3.E3.m1.2.2.1.3.3.cmml" xref="S3.E3.m1.2.2.1.3.3">𝑡</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑌</ci><apply id="S3.E3.m1.2.2.1.5.cmml" xref="S3.E3.m1.2.2.1.5"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.5.1.cmml" xref="S3.E3.m1.2.2.1.5">subscript</csymbol><ci id="S3.E3.m1.2.2.1.5.2.cmml" xref="S3.E3.m1.2.2.1.5.2">𝑃</ci><ci id="S3.E3.m1.2.2.1.5.3.cmml" xref="S3.E3.m1.2.2.1.5.3">𝑡</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><eq id="S3.E3.m1.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"></eq><ci id="S3.E3.m1.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2">𝑋</ci><apply id="S3.E3.m1.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3"><csymbol cd="latexml" id="S3.E3.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.1">conditional</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.2">𝑥</ci><ci id="S3.E3.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3.3">𝑌</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">M\propto P_{t}(Y)P_{t}(X=x|Y)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">As a result, it can be stated that by attempting to match the target task as closely as possible in simulation, we can reduce <math id="S3.SS2.p4.1.m1.2" class="ltx_Math" alttext="\Delta P(Y)P(X|Y)" display="inline"><semantics id="S3.SS2.p4.1.m1.2a"><mrow id="S3.SS2.p4.1.m1.2.2" xref="S3.SS2.p4.1.m1.2.2.cmml"><mi mathvariant="normal" id="S3.SS2.p4.1.m1.2.2.3" xref="S3.SS2.p4.1.m1.2.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.2.2.2" xref="S3.SS2.p4.1.m1.2.2.2.cmml">​</mo><mi id="S3.SS2.p4.1.m1.2.2.4" xref="S3.SS2.p4.1.m1.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.2.2.2a" xref="S3.SS2.p4.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS2.p4.1.m1.2.2.5.2" xref="S3.SS2.p4.1.m1.2.2.cmml"><mo stretchy="false" id="S3.SS2.p4.1.m1.2.2.5.2.1" xref="S3.SS2.p4.1.m1.2.2.cmml">(</mo><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS2.p4.1.m1.2.2.5.2.2" xref="S3.SS2.p4.1.m1.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.2.2.2b" xref="S3.SS2.p4.1.m1.2.2.2.cmml">​</mo><mi id="S3.SS2.p4.1.m1.2.2.6" xref="S3.SS2.p4.1.m1.2.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.2.2.2c" xref="S3.SS2.p4.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS2.p4.1.m1.2.2.1.1" xref="S3.SS2.p4.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p4.1.m1.2.2.1.1.2" xref="S3.SS2.p4.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.p4.1.m1.2.2.1.1.1" xref="S3.SS2.p4.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.2.2.1.1.1.2" xref="S3.SS2.p4.1.m1.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS2.p4.1.m1.2.2.1.1.1.1" xref="S3.SS2.p4.1.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS2.p4.1.m1.2.2.1.1.1.3" xref="S3.SS2.p4.1.m1.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS2.p4.1.m1.2.2.1.1.3" xref="S3.SS2.p4.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.2b"><apply id="S3.SS2.p4.1.m1.2.2.cmml" xref="S3.SS2.p4.1.m1.2.2"><times id="S3.SS2.p4.1.m1.2.2.2.cmml" xref="S3.SS2.p4.1.m1.2.2.2"></times><ci id="S3.SS2.p4.1.m1.2.2.3.cmml" xref="S3.SS2.p4.1.m1.2.2.3">Δ</ci><ci id="S3.SS2.p4.1.m1.2.2.4.cmml" xref="S3.SS2.p4.1.m1.2.2.4">𝑃</ci><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝑌</ci><ci id="S3.SS2.p4.1.m1.2.2.6.cmml" xref="S3.SS2.p4.1.m1.2.2.6">𝑃</ci><apply id="S3.SS2.p4.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p4.1.m1.2.2.1.1"><csymbol cd="latexml" id="S3.SS2.p4.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS2.p4.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS2.p4.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.2c">\Delta P(Y)P(X|Y)</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Improving small models with simulated data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">Given that synthetic datasets show the ability to reduce these four biases, plus <math id="S3.SS3.p1.1.m1.2" class="ltx_Math" alttext="\Delta P(Y)P(X|Y)" display="inline"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml"><mi mathvariant="normal" id="S3.SS3.p1.1.m1.2.2.3" xref="S3.SS3.p1.1.m1.2.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.cmml">​</mo><mi id="S3.SS3.p1.1.m1.2.2.4" xref="S3.SS3.p1.1.m1.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.2.2.2a" xref="S3.SS3.p1.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS3.p1.1.m1.2.2.5.2" xref="S3.SS3.p1.1.m1.2.2.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.5.2.1" xref="S3.SS3.p1.1.m1.2.2.cmml">(</mo><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.5.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.2.2.2b" xref="S3.SS3.p1.1.m1.2.2.2.cmml">​</mo><mi id="S3.SS3.p1.1.m1.2.2.6" xref="S3.SS3.p1.1.m1.2.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.2.2.2c" xref="S3.SS3.p1.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS3.p1.1.m1.2.2.1.1" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.1.1.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.1.m1.2.2.1.1.1" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.2.2.1.1.1.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.cmml">X</mi><mo fence="false" id="S3.SS3.p1.1.m1.2.2.1.1.1.1" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS3.p1.1.m1.2.2.1.1.1.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.1.1.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><apply id="S3.SS3.p1.1.m1.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2"><times id="S3.SS3.p1.1.m1.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2"></times><ci id="S3.SS3.p1.1.m1.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.3">Δ</ci><ci id="S3.SS3.p1.1.m1.2.2.4.cmml" xref="S3.SS3.p1.1.m1.2.2.4">𝑃</ci><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑌</ci><ci id="S3.SS3.p1.1.m1.2.2.6.cmml" xref="S3.SS3.p1.1.m1.2.2.6">𝑃</ci><apply id="S3.SS3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">\Delta P(Y)P(X|Y)</annotation></semantics></math>, we can confidently predict that <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="G^{tr}_{ImNet}&gt;G^{tr}_{Synth}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><msubsup id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.2.cmml">G</mi><mrow id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.3.2" xref="S3.SS3.p1.2.m2.1.1.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.2.3.1" xref="S3.SS3.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.2.3.3" xref="S3.SS3.p1.2.m2.1.1.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.2.3.1a" xref="S3.SS3.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.2.3.4" xref="S3.SS3.p1.2.m2.1.1.2.3.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.2.3.1b" xref="S3.SS3.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.2.3.5" xref="S3.SS3.p1.2.m2.1.1.2.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.2.3.1c" xref="S3.SS3.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.2.3.6" xref="S3.SS3.p1.2.m2.1.1.2.3.6.cmml">t</mi></mrow><mrow id="S3.SS3.p1.2.m2.1.1.2.2.3" xref="S3.SS3.p1.2.m2.1.1.2.2.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2.3.2" xref="S3.SS3.p1.2.m2.1.1.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.2.2.3.1" xref="S3.SS3.p1.2.m2.1.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.2.2.3.3" xref="S3.SS3.p1.2.m2.1.1.2.2.3.3.cmml">r</mi></mrow></msubsup><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">&gt;</mo><msubsup id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.2.2" xref="S3.SS3.p1.2.m2.1.1.3.2.2.cmml">G</mi><mrow id="S3.SS3.p1.2.m2.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.3.2" xref="S3.SS3.p1.2.m2.1.1.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.3.1" xref="S3.SS3.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.3.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.3.1a" xref="S3.SS3.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.3.3.4" xref="S3.SS3.p1.2.m2.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.3.1b" xref="S3.SS3.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.3.3.5" xref="S3.SS3.p1.2.m2.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.3.1c" xref="S3.SS3.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.3.3.6" xref="S3.SS3.p1.2.m2.1.1.3.3.6.cmml">h</mi></mrow><mrow id="S3.SS3.p1.2.m2.1.1.3.2.3" xref="S3.SS3.p1.2.m2.1.1.3.2.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.2.3.2" xref="S3.SS3.p1.2.m2.1.1.3.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.3.2.3.1" xref="S3.SS3.p1.2.m2.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.3.2.3.3" xref="S3.SS3.p1.2.m2.1.1.3.2.3.3.cmml">r</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><gt id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></gt><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2">subscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.2">𝐺</ci><apply id="S3.SS3.p1.2.m2.1.1.2.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.3"><times id="S3.SS3.p1.2.m2.1.1.2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.3.1"></times><ci id="S3.SS3.p1.2.m2.1.1.2.2.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.3.2">𝑡</ci><ci id="S3.SS3.p1.2.m2.1.1.2.2.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.3.3">𝑟</ci></apply></apply><apply id="S3.SS3.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3"><times id="S3.SS3.p1.2.m2.1.1.2.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3.1"></times><ci id="S3.SS3.p1.2.m2.1.1.2.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3.2">𝐼</ci><ci id="S3.SS3.p1.2.m2.1.1.2.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3.3">𝑚</ci><ci id="S3.SS3.p1.2.m2.1.1.2.3.4.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3.4">𝑁</ci><ci id="S3.SS3.p1.2.m2.1.1.2.3.5.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3.5">𝑒</ci><ci id="S3.SS3.p1.2.m2.1.1.2.3.6.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3.6">𝑡</ci></apply></apply><apply id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3">subscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2.2">𝐺</ci><apply id="S3.SS3.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2.3"><times id="S3.SS3.p1.2.m2.1.1.3.2.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2.3.1"></times><ci id="S3.SS3.p1.2.m2.1.1.3.2.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2.3.2">𝑡</ci><ci id="S3.SS3.p1.2.m2.1.1.3.2.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2.3.3">𝑟</ci></apply></apply><apply id="S3.SS3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3"><times id="S3.SS3.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS3.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.2">𝑆</ci><ci id="S3.SS3.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.3">𝑦</ci><ci id="S3.SS3.p1.2.m2.1.1.3.3.4.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.4">𝑛</ci><ci id="S3.SS3.p1.2.m2.1.1.3.3.5.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.5">𝑡</ci><ci id="S3.SS3.p1.2.m2.1.1.3.3.6.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.6">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">G^{tr}_{ImNet}&gt;G^{tr}_{Synth}</annotation></semantics></math>. However, simulations come with their own additional bias, referred to by the research community as the “Sim2Real” or the <span id="S3.SS3.p1.3.1" class="ltx_text ltx_font_italic">reality gap</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="G^{s2r}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msup id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">G</mi><mrow id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.1" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mn id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.1a" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.3.4" xref="S3.SS3.p1.3.m3.1.1.3.4.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝐺</ci><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><times id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.1"></times><ci id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">𝑠</ci><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3">2</cn><ci id="S3.SS3.p1.3.m3.1.1.3.4.cmml" xref="S3.SS3.p1.3.m3.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">G^{s2r}</annotation></semantics></math>. Notably, no matter their fidelity and realism, simulations will always struggle to properly model sensor noise, imperfections, and complex physical phenomena. This bias manifests as a lack of what Zhou <em id="S3.SS3.p1.3.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.3.3" class="ltx_text"></span> call “coverage” in the data, defined as “quasi-exhaustive representation of the classes and variety of exemplars” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Good coverage in synthetic datasets is generally achieved by having a wide range of random camera angles, lighting, textures, object arrangements, and additional noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. We hypothesize that as long as the dataset compensates for the reality gap by giving sufficient coverage of the input distribution, we find that,</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="G^{tr}_{ImNet}&gt;G^{tr}_{Synth}+G^{s2r}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msubsup id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.2.2.2" xref="S3.E4.m1.1.1.2.2.2.cmml">G</mi><mrow id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.2.3.2" xref="S3.E4.m1.1.1.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.3.1" xref="S3.E4.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.3.3" xref="S3.E4.m1.1.1.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.3.1a" xref="S3.E4.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.3.4" xref="S3.E4.m1.1.1.2.3.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.3.1b" xref="S3.E4.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.3.5" xref="S3.E4.m1.1.1.2.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.3.1c" xref="S3.E4.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.3.6" xref="S3.E4.m1.1.1.2.3.6.cmml">t</mi></mrow><mrow id="S3.E4.m1.1.1.2.2.3" xref="S3.E4.m1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.2.3.1" xref="S3.E4.m1.1.1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.2.2.3.3.cmml">r</mi></mrow></msubsup><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">&gt;</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><msubsup id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2.2" xref="S3.E4.m1.1.1.3.2.2.2.cmml">G</mi><mrow id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.3.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.1" xref="S3.E4.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.3.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.1a" xref="S3.E4.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.4" xref="S3.E4.m1.1.1.3.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.1b" xref="S3.E4.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.5" xref="S3.E4.m1.1.1.3.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.1c" xref="S3.E4.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.6" xref="S3.E4.m1.1.1.3.2.3.6.cmml">h</mi></mrow><mrow id="S3.E4.m1.1.1.3.2.2.3" xref="S3.E4.m1.1.1.3.2.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.2.3.2" xref="S3.E4.m1.1.1.3.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.2.3.1" xref="S3.E4.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.2.3.3" xref="S3.E4.m1.1.1.3.2.2.3.3.cmml">r</mi></mrow></msubsup><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><msup id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">G</mi><mrow id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mn id="S3.E4.m1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.3.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1a" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3.4" xref="S3.E4.m1.1.1.3.3.3.4.cmml">r</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><gt id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></gt><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2">subscript</csymbol><apply id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.2">superscript</csymbol><ci id="S3.E4.m1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.2.2.2">𝐺</ci><apply id="S3.E4.m1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.2.2.3"><times id="S3.E4.m1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.2.2.3.1"></times><ci id="S3.E4.m1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.2.2.3.2">𝑡</ci><ci id="S3.E4.m1.1.1.2.2.3.3.cmml" xref="S3.E4.m1.1.1.2.2.3.3">𝑟</ci></apply></apply><apply id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3"><times id="S3.E4.m1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.2.3.1"></times><ci id="S3.E4.m1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.2.3.2">𝐼</ci><ci id="S3.E4.m1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.2.3.3">𝑚</ci><ci id="S3.E4.m1.1.1.2.3.4.cmml" xref="S3.E4.m1.1.1.2.3.4">𝑁</ci><ci id="S3.E4.m1.1.1.2.3.5.cmml" xref="S3.E4.m1.1.1.2.3.5">𝑒</ci><ci id="S3.E4.m1.1.1.2.3.6.cmml" xref="S3.E4.m1.1.1.2.3.6">𝑡</ci></apply></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2">subscript</csymbol><apply id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.2.1.cmml" xref="S3.E4.m1.1.1.3.2">superscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2.2">𝐺</ci><apply id="S3.E4.m1.1.1.3.2.2.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3"><times id="S3.E4.m1.1.1.3.2.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.2.3.1"></times><ci id="S3.E4.m1.1.1.3.2.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.2.3.2">𝑡</ci><ci id="S3.E4.m1.1.1.3.2.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3.3">𝑟</ci></apply></apply><apply id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3"><times id="S3.E4.m1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3.1"></times><ci id="S3.E4.m1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.2">𝑆</ci><ci id="S3.E4.m1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3">𝑦</ci><ci id="S3.E4.m1.1.1.3.2.3.4.cmml" xref="S3.E4.m1.1.1.3.2.3.4">𝑛</ci><ci id="S3.E4.m1.1.1.3.2.3.5.cmml" xref="S3.E4.m1.1.1.3.2.3.5">𝑡</ci><ci id="S3.E4.m1.1.1.3.2.3.6.cmml" xref="S3.E4.m1.1.1.3.2.3.6">ℎ</ci></apply></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">superscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝐺</ci><apply id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3"><times id="S3.E4.m1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.3.2">𝑠</ci><cn type="integer" id="S3.E4.m1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3.3">2</cn><ci id="S3.E4.m1.1.1.3.3.3.4.cmml" xref="S3.E4.m1.1.1.3.3.3.4">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">G^{tr}_{ImNet}&gt;G^{tr}_{Synth}+G^{s2r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.4" class="ltx_p">As a result CNN pretraining with synthetic data will still be beneficial.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The shortcomings of transfer learning affect all CNNs, but not equally. During pretraining, if bias exists in the pretraining dataset, large models with many parameters have the capacity to model both the underlying distribution and the bias. Small models, on the other hand, do not have the capacity to model both, and over time will tend toward to learning the bias because it gives the lowest loss. As a result, large models will transfer the underlying distribution during fine-tuning and be able to ignore pretraining bias, while small models will have to unlearn the effects of the pretraining bias. Therefore, we hypothesize that for transfer learning, pretraining on data that has less bias with respect to the target data will show greater performance improvement with small models. Comparing our results to that of McCormac <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> validates this hypothesis.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/log_datasize_miou_line.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:80%;">This plot shows the mean IoU performance of models pretrained on SceneNet RGB-D and ImageNet as a function of the total amount of fine-tuning data a pretrained model is trained with, scaled logarithmically. Results for models pretrained on SceneNet RGB-D are in blue and results for models pretrained on ImageNet are in red.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.03676/assets/figs/datasize_miou_diff.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:80%;">This plot shows the percent improvement of the SceneNet RGB-D pretrained models over the ImageNet pretrained models for a given percentage of the total fine-tuning data. We can see that pretraining using synthetic segmentation data generally gives more improvement for smaller quantities of fine-tuning data</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Chart of results from the Robot@Home training size variation experiment. It is interesting to note that the SceneNet RGB-D pretrained models always outperforms the ImageNet pretrained model for all the different training set sizes. </figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Approach</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conducted three experiments to validate that Equation <a href="#S3.E4" title="In 3.3 Improving small models with simulated data ‣ 3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> holds for small CNNs trained with sparsely supervised robot perception data: (1) An ablation experiment to demonstrate that using synthetic data for pretraining (compared to ImageNet) improves real-time models on standard semantic segmentation datasets, (2) a data withholding experiment to compare the models pretrained with synthetic data to the models pretrained with ImageNet by measuring their performance on a held out set of robot perception data after being fine-tuned using increasingly restricted amounts of robot perception data, and (3) a high-level similarity experiment to demonstrate that the high-level task similarity between pretraining and fine-tuning datasets has only a minor effect on model performance compared to the effects of bias reduction discussed in Section <a href="#S3" title="3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In this section, we outline how we selected a synthetic dataset that met the requirements outlined in Section <a href="#S3.SS2" title="3.2 The transfer learning gap ‣ 3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, standard datasets relevant to robotics, preferably one for autonomous driving and one for an indoor scenario as those are two domains in which robots can benefit from segmentation, a robot dataset for the second experiment, and a semantic segmentation CNN architecture that could run in real-time on a robot.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset Selection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For our standard segmentation datasets, we use the SUN RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> indoor dataset and the Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> autonomous driving dataset. We selected SUN RGB-D because it has 37 challenging semantic classes and it is one of the largest real semantic segmentation datasets for indoor environments. We selected Cityscapes as it is the most recent real autonomous driving dataset, has 19 semantic classes which is more than most driving datasets, and 5K frames with “fine” annotations from a series of driving videos.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For our primary synthetic pretraining dataset, we used SceneNet RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. SceneNet RGB-D has over 5 million photorealistic training images sampled from randomized smooth trajectories through 16K room configurations. Each unique room configuration has a random set of contextually-relevant objects initialized with both random pose and texture, and random lighting, and all frames are intentionally perturbed with realistic noise. Moreover, the dataset is labeled instance-wise, so we opted to mapping the objects to 13 classes, which made it more adaptable as a pretraining model for transfer learning and consistent with the experiments of McCormac <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS1.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The diversity of the data in this dataset makes it especially well suited for large-dataset pretraining.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Lastly, for our robot data we use the Robot@Home dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Created with the intention of semantic mapping by a household robot, Robot@Home was collected over four years by recording 81 video sequences of a robot driving around 36 unique unstructured human spaces. The robot was equipped with four Primesense RGB-D cameras for recording the visual frames and a 2D laser scanner to improve mapping capabilities. Objects instance segmentation labels are provided for 32937 frames across 72 sequences, which like SceneNet RGB-D, has mappings to standard object segmentation class labels. We chose to use the mapping to the original 41 SUN categories to increase the difficulty of this last experiment.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The architecture best suited to our task is the E-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> architecture. E-Net is an encoder-decoder style CNN composed mainly of “bottleneck” modules, which combine three convolutions and a skip layer. The E-Net design combines an initial network-in-network module with quick downsampling to strike a compromise between reducing the number of parameters in the network and the representational power of low-level features from layers close to the original image resolution. The remainder of the network is 16 bottleneck modules for the encoder, and 5 such modules for the decoder. Unlike most segmentation networks, E-Net has an asymmetric encoder-decoder design as the encoder is the main feature extractor and the decoders are often responsible for using large quantities of parameters. Our implementation of E-Net runs very efficiently: over 56 frames per second on average for a 256x256 input on a single NVIDIA GTX 1080Ti GPU.</p>
</div>
<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Ablation Study Metrics - Pixel Accuracy</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.1" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Real Dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.1.2.1.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">Cityscapes</span></td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">SUN RGB-D</span></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">No pretraining</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.851</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.483</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ImageNet</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.860</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.516</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SceneNet RGB-D</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.1.5.4.2.1" class="ltx_text ltx_font_bold">0.863</span></td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.1.5.4.3.1" class="ltx_text ltx_font_bold">0.585</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation Study Metrics - Mean Accuracy</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.2" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S4.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">Real Dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.1" class="ltx_tr">
<th id="S4.T3.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.2.2.1.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<td id="S4.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.2.2.1.2.1" class="ltx_text ltx_font_bold">Cityscapes</span></td>
<td id="S4.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.2.2.1.3.1" class="ltx_text ltx_font_bold">SUN RGB-D</span></td>
</tr>
<tr id="S4.T3.2.3.2" class="ltx_tr">
<th id="S4.T3.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">No pretraining</th>
<td id="S4.T3.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.410</td>
<td id="S4.T3.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.200</td>
</tr>
<tr id="S4.T3.2.4.3" class="ltx_tr">
<th id="S4.T3.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ImageNet</th>
<td id="S4.T3.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.501</td>
<td id="S4.T3.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.2.4.3.3.1" class="ltx_text ltx_font_bold">0.368</span></td>
</tr>
<tr id="S4.T3.2.5.4" class="ltx_tr">
<th id="S4.T3.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SceneNet RGB-D</th>
<td id="S4.T3.2.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.2.5.4.2.1" class="ltx_text ltx_font_bold">0.585</span></td>
<td id="S4.T3.2.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.346</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation Study Metrics - mean IOU</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S4.T3.3.1.1.2.1" class="ltx_text ltx_font_bold">Real Dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.2.1" class="ltx_tr">
<th id="S4.T3.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.3.2.1.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<td id="S4.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.3.2.1.2.1" class="ltx_text ltx_font_bold">Cityscapes</span></td>
<td id="S4.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.3.2.1.3.1" class="ltx_text ltx_font_bold">SUN RGB-D</span></td>
</tr>
<tr id="S4.T3.3.3.2" class="ltx_tr">
<th id="S4.T3.3.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">No pretraining</th>
<td id="S4.T3.3.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.346</td>
<td id="S4.T3.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.118</td>
</tr>
<tr id="S4.T3.3.4.3" class="ltx_tr">
<th id="S4.T3.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ImageNet</th>
<td id="S4.T3.3.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.392</td>
<td id="S4.T3.3.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.180</td>
</tr>
<tr id="S4.T3.3.5.4" class="ltx_tr">
<th id="S4.T3.3.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SceneNet RGB-D</th>
<td id="S4.T3.3.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.3.5.4.2.1" class="ltx_text ltx_font_bold">0.489</span></td>
<td id="S4.T3.3.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T3.3.5.4.3.1" class="ltx_text ltx_font_bold">0.227</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments and Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We compare models trained from scratch in two different ways. In the first case, a model is pretrained on an image classification task using the ImageNet dataset. As ImageNet is an image classification dataset it is only used to pretrain the encoder — standard practice for transfer learning from classification to segmentation. The model is then fine tuned on the target dataset with randomly initialized output layers, which in the case of segmentation is the “decoder”. In the second case, a model is pretrained end-to-end on a semantic segmentation task using the SceneNet RGB-D dataset, and then the entire model is fine tuned with the target dataset.</p>
</div>
<figure id="S5.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Robot@Home mIoU Dataset Variance Evaluation</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T4.1" class="ltx_p ltx_figure_panel ltx_align_center">This table shows the performance difference in the two pretrained models given different amounts of fine-tuning data.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.T4.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:504.4pt;height:85.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.0pt,11.3pt) scale(0.79,0.79) ;">
<table id="S5.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.2.1.1.1" class="ltx_tr">
<th id="S5.T4.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<td id="S5.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="13"><span id="S5.T4.2.1.1.1.2.1" class="ltx_text ltx_font_bold">Percentage of Full Robot Fine-tuning Training Set (number of examples)</span>
</td>
</tr>
<tr id="S5.T4.2.1.2.2" class="ltx_tr">
<th id="S5.T4.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T4.2.1.2.2.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<td id="S5.T4.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.8%</td>
<td id="S5.T4.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6%</td>
<td id="S5.T4.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.1%</td>
<td id="S5.T4.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.3%</td>
<td id="S5.T4.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.3%</td>
<td id="S5.T4.2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.5%</td>
<td id="S5.T4.2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.3%</td>
<td id="S5.T4.2.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.7%</td>
<td id="S5.T4.2.1.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.0%</td>
<td id="S5.T4.2.1.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.0%</td>
<td id="S5.T4.2.1.2.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.3%</td>
<td id="S5.T4.2.1.2.2.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.0%</td>
<td id="S5.T4.2.1.2.2.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100.0%</td>
</tr>
<tr id="S5.T4.2.1.3.3" class="ltx_tr">
<th id="S5.T4.2.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr"><span id="S5.T4.2.1.3.3.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<td id="S5.T4.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">(179)</td>
<td id="S5.T4.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">(358)</td>
<td id="S5.T4.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">(717)</td>
<td id="S5.T4.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">(1,434)</td>
<td id="S5.T4.2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r">(1,912)</td>
<td id="S5.T4.2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r">(2,868)</td>
<td id="S5.T4.2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r">(3,278)</td>
<td id="S5.T4.2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r">(3,824)</td>
<td id="S5.T4.2.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r">(4,589)</td>
<td id="S5.T4.2.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r">(5,736)</td>
<td id="S5.T4.2.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r">(7,648)</td>
<td id="S5.T4.2.1.3.3.13" class="ltx_td ltx_align_center ltx_border_r">(11,473)</td>
<td id="S5.T4.2.1.3.3.14" class="ltx_td ltx_align_center ltx_border_r">(22,946)</td>
</tr>
<tr id="S5.T4.2.1.4.4" class="ltx_tr">
<th id="S5.T4.2.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">ImageNet</th>
<td id="S5.T4.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.072</td>
<td id="S5.T4.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.115</td>
<td id="S5.T4.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.19</td>
<td id="S5.T4.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.332</td>
<td id="S5.T4.2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.369</td>
<td id="S5.T4.2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.433</td>
<td id="S5.T4.2.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.464</td>
<td id="S5.T4.2.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.504</td>
<td id="S5.T4.2.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.528</td>
<td id="S5.T4.2.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.55</td>
<td id="S5.T4.2.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.56</td>
<td id="S5.T4.2.1.4.4.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.58</td>
<td id="S5.T4.2.1.4.4.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.626</td>
</tr>
<tr id="S5.T4.2.1.5.5" class="ltx_tr">
<th id="S5.T4.2.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">SceneNet RGBD</th>
<td id="S5.T4.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.105</td>
<td id="S5.T4.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.152</td>
<td id="S5.T4.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.236</td>
<td id="S5.T4.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.373</td>
<td id="S5.T4.2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.391</td>
<td id="S5.T4.2.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.484</td>
<td id="S5.T4.2.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.495</td>
<td id="S5.T4.2.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.536</td>
<td id="S5.T4.2.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.556</td>
<td id="S5.T4.2.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.579</td>
<td id="S5.T4.2.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.631</td>
<td id="S5.T4.2.1.5.5.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.643</td>
<td id="S5.T4.2.1.5.5.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.665</td>
</tr>
<tr id="S5.T4.2.1.6.6" class="ltx_tr">
<th id="S5.T4.2.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">% Improvement</th>
<td id="S5.T4.2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">45.8%</td>
<td id="S5.T4.2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32.2%</td>
<td id="S5.T4.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">24.2%</td>
<td id="S5.T4.2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">12.3%</td>
<td id="S5.T4.2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.0%</td>
<td id="S5.T4.2.1.6.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">11.8%</td>
<td id="S5.T4.2.1.6.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.7%</td>
<td id="S5.T4.2.1.6.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.3%</td>
<td id="S5.T4.2.1.6.6.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5.3%</td>
<td id="S5.T4.2.1.6.6.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5.3%</td>
<td id="S5.T4.2.1.6.6.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">12.7%</td>
<td id="S5.T4.2.1.6.6.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10.9%</td>
<td id="S5.T4.2.1.6.6.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.2%</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.4" class="ltx_p">We implemented E-Net using the <span id="S5.SS1.p1.4.1" class="ltx_text ltx_font_italic">PyTorch</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> framework <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>All code and models can be found on our github <a target="_blank" href="https://github.com/balloch/synth-seg" title="" class="ltx_ref ltx_href">here</a></span></span></span>. The network is trained using negative log linear loss on a Softmax function and optimized using the adaptive gradient descent algorithm, Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. A brief hyper-parameter search was conducted on the initial learning rate <math id="S5.SS1.p1.1.m1.5" class="ltx_Math" alttext="\alpha\in[10^{-2},5*10^{-3},10^{-3},5*10^{-4},10^{-4}]" display="inline"><semantics id="S5.SS1.p1.1.m1.5a"><mrow id="S5.SS1.p1.1.m1.5.5" xref="S5.SS1.p1.1.m1.5.5.cmml"><mi id="S5.SS1.p1.1.m1.5.5.7" xref="S5.SS1.p1.1.m1.5.5.7.cmml">α</mi><mo id="S5.SS1.p1.1.m1.5.5.6" xref="S5.SS1.p1.1.m1.5.5.6.cmml">∈</mo><mrow id="S5.SS1.p1.1.m1.5.5.5.5" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml"><mo stretchy="false" id="S5.SS1.p1.1.m1.5.5.5.5.6" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml">[</mo><msup id="S5.SS1.p1.1.m1.1.1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.1.1.1.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.1.1.1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.1.1.1.3.cmml"><mo id="S5.SS1.p1.1.m1.1.1.1.1.1.3a" xref="S5.SS1.p1.1.m1.1.1.1.1.1.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.1.1.1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.1.1.1.3.2.cmml">2</mn></mrow></msup><mo id="S5.SS1.p1.1.m1.5.5.5.5.7" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml">,</mo><mrow id="S5.SS1.p1.1.m1.2.2.2.2.2" xref="S5.SS1.p1.1.m1.2.2.2.2.2.cmml"><mn id="S5.SS1.p1.1.m1.2.2.2.2.2.2" xref="S5.SS1.p1.1.m1.2.2.2.2.2.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p1.1.m1.2.2.2.2.2.1" xref="S5.SS1.p1.1.m1.2.2.2.2.2.1.cmml">∗</mo><msup id="S5.SS1.p1.1.m1.2.2.2.2.2.3" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.cmml"><mn id="S5.SS1.p1.1.m1.2.2.2.2.2.3.2" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.2.2.2.2.2.3.3" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.cmml"><mo id="S5.SS1.p1.1.m1.2.2.2.2.2.3.3a" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.2" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.2.cmml">3</mn></mrow></msup></mrow><mo id="S5.SS1.p1.1.m1.5.5.5.5.8" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml">,</mo><msup id="S5.SS1.p1.1.m1.3.3.3.3.3" xref="S5.SS1.p1.1.m1.3.3.3.3.3.cmml"><mn id="S5.SS1.p1.1.m1.3.3.3.3.3.2" xref="S5.SS1.p1.1.m1.3.3.3.3.3.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.3.3.3.3.3.3" xref="S5.SS1.p1.1.m1.3.3.3.3.3.3.cmml"><mo id="S5.SS1.p1.1.m1.3.3.3.3.3.3a" xref="S5.SS1.p1.1.m1.3.3.3.3.3.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.3.3.3.3.3.3.2" xref="S5.SS1.p1.1.m1.3.3.3.3.3.3.2.cmml">3</mn></mrow></msup><mo id="S5.SS1.p1.1.m1.5.5.5.5.9" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml">,</mo><mrow id="S5.SS1.p1.1.m1.4.4.4.4.4" xref="S5.SS1.p1.1.m1.4.4.4.4.4.cmml"><mn id="S5.SS1.p1.1.m1.4.4.4.4.4.2" xref="S5.SS1.p1.1.m1.4.4.4.4.4.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p1.1.m1.4.4.4.4.4.1" xref="S5.SS1.p1.1.m1.4.4.4.4.4.1.cmml">∗</mo><msup id="S5.SS1.p1.1.m1.4.4.4.4.4.3" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.cmml"><mn id="S5.SS1.p1.1.m1.4.4.4.4.4.3.2" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.4.4.4.4.4.3.3" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.cmml"><mo id="S5.SS1.p1.1.m1.4.4.4.4.4.3.3a" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.2" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.2.cmml">4</mn></mrow></msup></mrow><mo id="S5.SS1.p1.1.m1.5.5.5.5.10" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml">,</mo><msup id="S5.SS1.p1.1.m1.5.5.5.5.5" xref="S5.SS1.p1.1.m1.5.5.5.5.5.cmml"><mn id="S5.SS1.p1.1.m1.5.5.5.5.5.2" xref="S5.SS1.p1.1.m1.5.5.5.5.5.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.5.5.5.5.5.3" xref="S5.SS1.p1.1.m1.5.5.5.5.5.3.cmml"><mo id="S5.SS1.p1.1.m1.5.5.5.5.5.3a" xref="S5.SS1.p1.1.m1.5.5.5.5.5.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.5.5.5.5.5.3.2" xref="S5.SS1.p1.1.m1.5.5.5.5.5.3.2.cmml">4</mn></mrow></msup><mo stretchy="false" id="S5.SS1.p1.1.m1.5.5.5.5.11" xref="S5.SS1.p1.1.m1.5.5.5.6.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.5b"><apply id="S5.SS1.p1.1.m1.5.5.cmml" xref="S5.SS1.p1.1.m1.5.5"><in id="S5.SS1.p1.1.m1.5.5.6.cmml" xref="S5.SS1.p1.1.m1.5.5.6"></in><ci id="S5.SS1.p1.1.m1.5.5.7.cmml" xref="S5.SS1.p1.1.m1.5.5.7">𝛼</ci><list id="S5.SS1.p1.1.m1.5.5.5.6.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5"><apply id="S5.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.2">10</cn><apply id="S5.SS1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.3"><minus id="S5.SS1.p1.1.m1.1.1.1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.3"></minus><cn type="integer" id="S5.SS1.p1.1.m1.1.1.1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.3.2">2</cn></apply></apply><apply id="S5.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2"><times id="S5.SS1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.1"></times><cn type="integer" id="S5.SS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.2">5</cn><apply id="S5.SS1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.2.2.2.2.2.3.1.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.2.2.2.2.2.3.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.2">10</cn><apply id="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.3"><minus id="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.1.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.3"></minus><cn type="integer" id="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2.3.3.2">3</cn></apply></apply></apply><apply id="S5.SS1.p1.1.m1.3.3.3.3.3.cmml" xref="S5.SS1.p1.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.3.3.3.3.3.1.cmml" xref="S5.SS1.p1.1.m1.3.3.3.3.3">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.3.3.3.3.3.2.cmml" xref="S5.SS1.p1.1.m1.3.3.3.3.3.2">10</cn><apply id="S5.SS1.p1.1.m1.3.3.3.3.3.3.cmml" xref="S5.SS1.p1.1.m1.3.3.3.3.3.3"><minus id="S5.SS1.p1.1.m1.3.3.3.3.3.3.1.cmml" xref="S5.SS1.p1.1.m1.3.3.3.3.3.3"></minus><cn type="integer" id="S5.SS1.p1.1.m1.3.3.3.3.3.3.2.cmml" xref="S5.SS1.p1.1.m1.3.3.3.3.3.3.2">3</cn></apply></apply><apply id="S5.SS1.p1.1.m1.4.4.4.4.4.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4"><times id="S5.SS1.p1.1.m1.4.4.4.4.4.1.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.1"></times><cn type="integer" id="S5.SS1.p1.1.m1.4.4.4.4.4.2.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.2">5</cn><apply id="S5.SS1.p1.1.m1.4.4.4.4.4.3.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.4.4.4.4.4.3.1.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.4.4.4.4.4.3.2.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.2">10</cn><apply id="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.3"><minus id="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.1.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.3"></minus><cn type="integer" id="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.2.cmml" xref="S5.SS1.p1.1.m1.4.4.4.4.4.3.3.2">4</cn></apply></apply></apply><apply id="S5.SS1.p1.1.m1.5.5.5.5.5.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5.5"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.5.5.5.5.5.1.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5.5">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.5.5.5.5.5.2.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5.5.2">10</cn><apply id="S5.SS1.p1.1.m1.5.5.5.5.5.3.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5.5.3"><minus id="S5.SS1.p1.1.m1.5.5.5.5.5.3.1.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5.5.3"></minus><cn type="integer" id="S5.SS1.p1.1.m1.5.5.5.5.5.3.2.cmml" xref="S5.SS1.p1.1.m1.5.5.5.5.5.3.2">4</cn></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.5c">\alpha\in[10^{-2},5*10^{-3},10^{-3},5*10^{-4},10^{-4}]</annotation></semantics></math> and we found that the initial learning rate of <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="\alpha=1e^{-3}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">α</mi><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml"><mn id="S5.SS1.p1.2.m2.1.1.3.2" xref="S5.SS1.p1.2.m2.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p1.2.m2.1.1.3.1" xref="S5.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><msup id="S5.SS1.p1.2.m2.1.1.3.3" xref="S5.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S5.SS1.p1.2.m2.1.1.3.3.2" xref="S5.SS1.p1.2.m2.1.1.3.3.2.cmml">e</mi><mrow id="S5.SS1.p1.2.m2.1.1.3.3.3" xref="S5.SS1.p1.2.m2.1.1.3.3.3.cmml"><mo id="S5.SS1.p1.2.m2.1.1.3.3.3a" xref="S5.SS1.p1.2.m2.1.1.3.3.3.cmml">−</mo><mn id="S5.SS1.p1.2.m2.1.1.3.3.3.2" xref="S5.SS1.p1.2.m2.1.1.3.3.3.2.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><eq id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></eq><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝛼</ci><apply id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3"><times id="S5.SS1.p1.2.m2.1.1.3.1.cmml" xref="S5.SS1.p1.2.m2.1.1.3.1"></times><cn type="integer" id="S5.SS1.p1.2.m2.1.1.3.2.cmml" xref="S5.SS1.p1.2.m2.1.1.3.2">1</cn><apply id="S5.SS1.p1.2.m2.1.1.3.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3">superscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.2">𝑒</ci><apply id="S5.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.3"><minus id="S5.SS1.p1.2.m2.1.1.3.3.3.1.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.3"></minus><cn type="integer" id="S5.SS1.p1.2.m2.1.1.3.3.3.2.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.3.2">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\alpha=1e^{-3}</annotation></semantics></math> was good for both training from scratch and fine tuning. For the other hyper-parameters, we used the values suggested by Kingma <em id="S5.SS1.p1.4.2" class="ltx_emph ltx_font_italic">et al</em>. We also experimented with mini-batch sizes <math id="S5.SS1.p1.3.m3.4" class="ltx_Math" alttext="b\in[10,32,64,128]" display="inline"><semantics id="S5.SS1.p1.3.m3.4a"><mrow id="S5.SS1.p1.3.m3.4.5" xref="S5.SS1.p1.3.m3.4.5.cmml"><mi id="S5.SS1.p1.3.m3.4.5.2" xref="S5.SS1.p1.3.m3.4.5.2.cmml">b</mi><mo id="S5.SS1.p1.3.m3.4.5.1" xref="S5.SS1.p1.3.m3.4.5.1.cmml">∈</mo><mrow id="S5.SS1.p1.3.m3.4.5.3.2" xref="S5.SS1.p1.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S5.SS1.p1.3.m3.4.5.3.2.1" xref="S5.SS1.p1.3.m3.4.5.3.1.cmml">[</mo><mn id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">10</mn><mo id="S5.SS1.p1.3.m3.4.5.3.2.2" xref="S5.SS1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S5.SS1.p1.3.m3.2.2" xref="S5.SS1.p1.3.m3.2.2.cmml">32</mn><mo id="S5.SS1.p1.3.m3.4.5.3.2.3" xref="S5.SS1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S5.SS1.p1.3.m3.3.3" xref="S5.SS1.p1.3.m3.3.3.cmml">64</mn><mo id="S5.SS1.p1.3.m3.4.5.3.2.4" xref="S5.SS1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S5.SS1.p1.3.m3.4.4" xref="S5.SS1.p1.3.m3.4.4.cmml">128</mn><mo stretchy="false" id="S5.SS1.p1.3.m3.4.5.3.2.5" xref="S5.SS1.p1.3.m3.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.4b"><apply id="S5.SS1.p1.3.m3.4.5.cmml" xref="S5.SS1.p1.3.m3.4.5"><in id="S5.SS1.p1.3.m3.4.5.1.cmml" xref="S5.SS1.p1.3.m3.4.5.1"></in><ci id="S5.SS1.p1.3.m3.4.5.2.cmml" xref="S5.SS1.p1.3.m3.4.5.2">𝑏</ci><list id="S5.SS1.p1.3.m3.4.5.3.1.cmml" xref="S5.SS1.p1.3.m3.4.5.3.2"><cn type="integer" id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">10</cn><cn type="integer" id="S5.SS1.p1.3.m3.2.2.cmml" xref="S5.SS1.p1.3.m3.2.2">32</cn><cn type="integer" id="S5.SS1.p1.3.m3.3.3.cmml" xref="S5.SS1.p1.3.m3.3.3">64</cn><cn type="integer" id="S5.SS1.p1.3.m3.4.4.cmml" xref="S5.SS1.p1.3.m3.4.4">128</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.4c">b\in[10,32,64,128]</annotation></semantics></math> for training, and found that the results were fairly similar, with 128 converging the most efficiently for the pretraining datasets. The real, non-ImageNet datasets were trained with a batch size of 32. For weight initialization we randomly sample from a Gaussian <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="=N(\mu=0,\sigma=0.02)" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mrow id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mi id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml"></mi><mo id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml">=</mo><mrow id="S5.SS1.p1.4.m4.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.cmml"><mi id="S5.SS1.p1.4.m4.1.1.1.3" xref="S5.SS1.p1.4.m4.1.1.1.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.4.m4.1.1.1.2" xref="S5.SS1.p1.4.m4.1.1.1.2.cmml">​</mo><mrow id="S5.SS1.p1.4.m4.1.1.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.cmml"><mo stretchy="false" id="S5.SS1.p1.4.m4.1.1.1.1.1.2" xref="S5.SS1.p1.4.m4.1.1.1.cmml">(</mo><mrow id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml"><mrow id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.2" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.2.cmml">μ</mi><mo id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.3" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.3" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.3a.cmml">,</mo><mrow id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.cmml"><mi id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.2" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.2.cmml">σ</mi><mo id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.1" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.1.cmml">=</mo><mn id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.3" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.3.cmml">0.02</mn></mrow></mrow><mo stretchy="false" id="S5.SS1.p1.4.m4.1.1.1.1.1.3" xref="S5.SS1.p1.4.m4.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><eq id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2"></eq><csymbol cd="latexml" id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">absent</csymbol><apply id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1"><times id="S5.SS1.p1.4.m4.1.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.1.2"></times><ci id="S5.SS1.p1.4.m4.1.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.1.3">𝑁</ci><apply id="S5.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.1.1.1.1.3a.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1"><eq id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.1"></eq><ci id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.2">𝜇</ci><cn type="integer" id="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2"><eq id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.1"></eq><ci id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.2">𝜎</ci><cn type="float" id="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.3.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.2.2.3">0.02</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">=N(\mu=0,\sigma=0.02)</annotation></semantics></math> distribution. All images were scaled to a resolution of 256x256 for our experiments. Training was performed on NVIDIA K40 Quadro, NVIDIA TITAN X, and NVIDIA GTX 1080Ti GPUs.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.5" class="ltx_p">In each experiment, the final dataset was evaluated on three metrics standard to semantic segmentation. These are pixel accuracy, mean accuracy, and the mean Intersection over Union (mIoU) measure. For <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">M</annotation></semantics></math> total classes, and for some predicted class <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">j</annotation></semantics></math>, <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="m_{ij}" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><msub id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">m</mi><mrow id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml"><mi id="S5.SS1.p2.3.m3.1.1.3.2" xref="S5.SS1.p2.3.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.3.m3.1.1.3.1" xref="S5.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p2.3.m3.1.1.3.3" xref="S5.SS1.p2.3.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝑚</ci><apply id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3"><times id="S5.SS1.p2.3.m3.1.1.3.1.cmml" xref="S5.SS1.p2.3.m3.1.1.3.1"></times><ci id="S5.SS1.p2.3.m3.1.1.3.2.cmml" xref="S5.SS1.p2.3.m3.1.1.3.2">𝑖</ci><ci id="S5.SS1.p2.3.m3.1.1.3.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">m_{ij}</annotation></semantics></math> are the number of pixels in class <math id="S5.SS1.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.4.m4.1a"><mi id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><ci id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">i</annotation></semantics></math> that are predicted to be in class <math id="S5.SS1.p2.5.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S5.SS1.p2.5.m5.1a"><mi id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><ci id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">j</annotation></semantics></math>. The pixel accuracy measures the ratio of pixels predicted correctly to all labeled pixels; this is a good indicator of how well the segmentation did relative to random chance. The mean accuracy measures the average across classes of the ratios of pixels predicted correctly to the total number of pixels in a label class. This measures the accuracy of the assignment over all classes. Lastly, mean IoU measures the average across classes of the the ratios of pixels predicted correctly to the total number of pixels in a label class plus the number of pixels in the prediction class that were not correctly classified. This is the most stringent measurement, and the best indicator of model performance in practice.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Experiment</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this experiment, we used each of the standard datasets, SUN RGB-D and Cityscapes, to train a model from scratch, to fine tune over ImageNet pretraining, and to fine tune over SceneNet RGB-D pretraining. In addition to the two regular training paradigms, in this experiment, models were trained for each target dataset using just the target training data (from-scratch) and for consistency with standard research practices this dataset was augmented using resizing and horizontal flipping for all three training scenarios. For Cityscapes, the predesignated train-val-test splits on the “fine” annotations were used. For SUN RGB-D, which does not have predetermined splits, the 10335 images were randomly sampled and split into a <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="70\%-10\%-20\%" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mrow id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml"><mn id="S5.SS2.p1.1.m1.1.1.2.2" xref="S5.SS2.p1.1.m1.1.1.2.2.cmml">70</mn><mo id="S5.SS2.p1.1.m1.1.1.2.1" xref="S5.SS2.p1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml"><mn id="S5.SS2.p1.1.m1.1.1.3.2" xref="S5.SS2.p1.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.SS2.p1.1.m1.1.1.3.1" xref="S5.SS2.p1.1.m1.1.1.3.1.cmml">%</mo></mrow><mo id="S5.SS2.p1.1.m1.1.1.1a" xref="S5.SS2.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S5.SS2.p1.1.m1.1.1.4" xref="S5.SS2.p1.1.m1.1.1.4.cmml"><mn id="S5.SS2.p1.1.m1.1.1.4.2" xref="S5.SS2.p1.1.m1.1.1.4.2.cmml">20</mn><mo id="S5.SS2.p1.1.m1.1.1.4.1" xref="S5.SS2.p1.1.m1.1.1.4.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><minus id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></minus><apply id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.2.1.cmml" xref="S5.SS2.p1.1.m1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS2.p1.1.m1.1.1.2.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2.2">70</cn></apply><apply id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.3.1.cmml" xref="S5.SS2.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS2.p1.1.m1.1.1.3.2.cmml" xref="S5.SS2.p1.1.m1.1.1.3.2">10</cn></apply><apply id="S5.SS2.p1.1.m1.1.1.4.cmml" xref="S5.SS2.p1.1.m1.1.1.4"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.4.1.cmml" xref="S5.SS2.p1.1.m1.1.1.4.1">percent</csymbol><cn type="integer" id="S5.SS2.p1.1.m1.1.1.4.2.cmml" xref="S5.SS2.p1.1.m1.1.1.4.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">70\%-10\%-20\%</annotation></semantics></math> train-val-test split. It was observed that at a batch size of 32 it would take roughly 50 epochs for convergence of training with no loss of validation performance (i.e. early stopping).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">For the fine-tuning process over ImageNet, the decoder was initialized in the same manner as the network when training from scratch, and all datasets were trained upon with validation monitoring for early stopping (around 30 epochs). For the fine-tuning process over SceneNet RGB-D, only the decoder was trained with validation showing the models converging at roughly 30 epochs. Mirroring the process of the from-scratch training, SUN RGB-D and Cityscapes were used to fine tune both pretrained models at a batch size of 32, validating after each epoch with early stopping.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.6" class="ltx_p">Tables <a href="#S4.T3" title="Table 3 ‣ 4.2 Model Architecture ‣ 4 Approach ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-<a href="#S4.T3" title="Table 3 ‣ 4.2 Model Architecture ‣ 4 Approach ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show the results of the ablation experiment. In the ablation experiment, the results show that for fine-tuning and testing on Cityscapes, the SceneNet RGB-D pretrained model outperforms the ImageNet pretrained model by <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="0.43\%" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mn id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">0.43</mn><mo id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">0.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">0.43\%</annotation></semantics></math> in pixel accuracy, <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="16.68\%" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mrow id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mn id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">16.68</mn><mo id="S5.SS2.p3.2.m2.1.1.1" xref="S5.SS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">16.68</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">16.68\%</annotation></semantics></math> in mean accuracy, and <math id="S5.SS2.p3.3.m3.1" class="ltx_Math" alttext="24.85\%" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><mrow id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml"><mn id="S5.SS2.p3.3.m3.1.1.2" xref="S5.SS2.p3.3.m3.1.1.2.cmml">24.85</mn><mo id="S5.SS2.p3.3.m3.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><apply id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p3.3.m3.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.3.m3.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.2">24.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">24.85\%</annotation></semantics></math> in mean IoU. For fine-tuning and testing on SUN RGB-D the SceneNet RGB-D pretrained model outperforms the ImageNet pretrained model by <math id="S5.SS2.p3.4.m4.1" class="ltx_Math" alttext="13.37\%" display="inline"><semantics id="S5.SS2.p3.4.m4.1a"><mrow id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml"><mn id="S5.SS2.p3.4.m4.1.1.2" xref="S5.SS2.p3.4.m4.1.1.2.cmml">13.37</mn><mo id="S5.SS2.p3.4.m4.1.1.1" xref="S5.SS2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><apply id="S5.SS2.p3.4.m4.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p3.4.m4.1.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.4.m4.1.1.2.cmml" xref="S5.SS2.p3.4.m4.1.1.2">13.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">13.37\%</annotation></semantics></math> in pixel accuracy and <math id="S5.SS2.p3.5.m5.1" class="ltx_Math" alttext="26.34\%" display="inline"><semantics id="S5.SS2.p3.5.m5.1a"><mrow id="S5.SS2.p3.5.m5.1.1" xref="S5.SS2.p3.5.m5.1.1.cmml"><mn id="S5.SS2.p3.5.m5.1.1.2" xref="S5.SS2.p3.5.m5.1.1.2.cmml">26.34</mn><mo id="S5.SS2.p3.5.m5.1.1.1" xref="S5.SS2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.5.m5.1b"><apply id="S5.SS2.p3.5.m5.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1"><csymbol cd="latexml" id="S5.SS2.p3.5.m5.1.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.5.m5.1.1.2.cmml" xref="S5.SS2.p3.5.m5.1.1.2">26.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.5.m5.1c">26.34\%</annotation></semantics></math> in mean IoU. For SUN RGB-D, the ImageNet pretrained model outperformed the SceneNet RGB-D pretrained model by <math id="S5.SS2.p3.6.m6.1" class="ltx_Math" alttext="5.96\%" display="inline"><semantics id="S5.SS2.p3.6.m6.1a"><mrow id="S5.SS2.p3.6.m6.1.1" xref="S5.SS2.p3.6.m6.1.1.cmml"><mn id="S5.SS2.p3.6.m6.1.1.2" xref="S5.SS2.p3.6.m6.1.1.2.cmml">5.96</mn><mo id="S5.SS2.p3.6.m6.1.1.1" xref="S5.SS2.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.6.m6.1b"><apply id="S5.SS2.p3.6.m6.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1"><csymbol cd="latexml" id="S5.SS2.p3.6.m6.1.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.6.m6.1.1.2.cmml" xref="S5.SS2.p3.6.m6.1.1.2">5.96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.6.m6.1c">5.96\%</annotation></semantics></math> in mean accuracy; however models that perform well in mean accuracy and not as well in mean IoU learn to over-fit to a subset of the most heavily represented classes in the dataset, which is consistent with our hypothesis that a model will have a harder time training over the biases of ImageNet. These results validate the hypothesis that using synthetic data to pretrain real datasets is still a viable approach for architectures like E-Net that have far fewer parameters than typical segmentation networks. Furthermore, this confirms the findings of McCormac <em id="S5.SS2.p3.6.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS2.p3.6.2" class="ltx_text"></span> and demonstrates that their conclusions extend beyond large parameter networks. Interestingly, the ablation results shown here are more dramatic than in McCormac <em id="S5.SS2.p3.6.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS2.p3.6.4" class="ltx_text"></span> in spite of the fact that they pretrained for longer on a larger model. This validates our hypothesis from Section <a href="#S3.SS3" title="3.3 Improving small models with simulated data ‣ 3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> that transfer learning from synthetic data is more effective for smaller networks.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Robot@Home Dataset Experiment</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">Like SUN RGB-D, the Robot@Home dataset did not have predetermined splits, so following our methods with the ablation study, the 32937 images were randomly sampled into a <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">70</mn><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">70\%</annotation></semantics></math>-<math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">10\%</annotation></semantics></math>-<math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">20</mn><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">20\%</annotation></semantics></math> train-val-test split.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.11" class="ltx_p">The purpose of this experiment is to examine how the efficacy of transfer learning changes with fine tuning robot datasets of a variety of sizes. To examine the effects, we down-sample the training split into other small training sets. Specifically, keeping the validation and test sets untouched and unchanged, we create additional training datasets that are <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="\frac{1}{2}" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mfrac id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mn id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><divide id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"></divide><cn type="integer" id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\frac{1}{2}</annotation></semantics></math>, <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="\frac{1}{3}" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mfrac id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mn id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">3</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><divide id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"></divide><cn type="integer" id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">\frac{1}{3}</annotation></semantics></math>, <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="\frac{1}{4}" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mfrac id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml"><mn id="S5.SS3.p2.3.m3.1.1.2" xref="S5.SS3.p2.3.m3.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.3.m3.1.1.3" xref="S5.SS3.p2.3.m3.1.1.3.cmml">4</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><apply id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1"><divide id="S5.SS3.p2.3.m3.1.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1"></divide><cn type="integer" id="S5.SS3.p2.3.m3.1.1.2.cmml" xref="S5.SS3.p2.3.m3.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.3.m3.1.1.3.cmml" xref="S5.SS3.p2.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">\frac{1}{4}</annotation></semantics></math>, <math id="S5.SS3.p2.4.m4.1" class="ltx_Math" alttext="\frac{1}{5}" display="inline"><semantics id="S5.SS3.p2.4.m4.1a"><mfrac id="S5.SS3.p2.4.m4.1.1" xref="S5.SS3.p2.4.m4.1.1.cmml"><mn id="S5.SS3.p2.4.m4.1.1.2" xref="S5.SS3.p2.4.m4.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.4.m4.1.1.3" xref="S5.SS3.p2.4.m4.1.1.3.cmml">5</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.4.m4.1b"><apply id="S5.SS3.p2.4.m4.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1"><divide id="S5.SS3.p2.4.m4.1.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1"></divide><cn type="integer" id="S5.SS3.p2.4.m4.1.1.2.cmml" xref="S5.SS3.p2.4.m4.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.4.m4.1.1.3.cmml" xref="S5.SS3.p2.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.4.m4.1c">\frac{1}{5}</annotation></semantics></math>, <math id="S5.SS3.p2.5.m5.1" class="ltx_Math" alttext="\frac{1}{6}" display="inline"><semantics id="S5.SS3.p2.5.m5.1a"><mfrac id="S5.SS3.p2.5.m5.1.1" xref="S5.SS3.p2.5.m5.1.1.cmml"><mn id="S5.SS3.p2.5.m5.1.1.2" xref="S5.SS3.p2.5.m5.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.5.m5.1.1.3" xref="S5.SS3.p2.5.m5.1.1.3.cmml">6</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.5.m5.1b"><apply id="S5.SS3.p2.5.m5.1.1.cmml" xref="S5.SS3.p2.5.m5.1.1"><divide id="S5.SS3.p2.5.m5.1.1.1.cmml" xref="S5.SS3.p2.5.m5.1.1"></divide><cn type="integer" id="S5.SS3.p2.5.m5.1.1.2.cmml" xref="S5.SS3.p2.5.m5.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.5.m5.1.1.3.cmml" xref="S5.SS3.p2.5.m5.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.5.m5.1c">\frac{1}{6}</annotation></semantics></math>, <math id="S5.SS3.p2.6.m6.1" class="ltx_Math" alttext="\frac{1}{7}" display="inline"><semantics id="S5.SS3.p2.6.m6.1a"><mfrac id="S5.SS3.p2.6.m6.1.1" xref="S5.SS3.p2.6.m6.1.1.cmml"><mn id="S5.SS3.p2.6.m6.1.1.2" xref="S5.SS3.p2.6.m6.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.6.m6.1.1.3" xref="S5.SS3.p2.6.m6.1.1.3.cmml">7</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.6.m6.1b"><apply id="S5.SS3.p2.6.m6.1.1.cmml" xref="S5.SS3.p2.6.m6.1.1"><divide id="S5.SS3.p2.6.m6.1.1.1.cmml" xref="S5.SS3.p2.6.m6.1.1"></divide><cn type="integer" id="S5.SS3.p2.6.m6.1.1.2.cmml" xref="S5.SS3.p2.6.m6.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.6.m6.1.1.3.cmml" xref="S5.SS3.p2.6.m6.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.6.m6.1c">\frac{1}{7}</annotation></semantics></math>, <math id="S5.SS3.p2.7.m7.1" class="ltx_Math" alttext="\frac{1}{8}" display="inline"><semantics id="S5.SS3.p2.7.m7.1a"><mfrac id="S5.SS3.p2.7.m7.1.1" xref="S5.SS3.p2.7.m7.1.1.cmml"><mn id="S5.SS3.p2.7.m7.1.1.2" xref="S5.SS3.p2.7.m7.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.7.m7.1.1.3" xref="S5.SS3.p2.7.m7.1.1.3.cmml">8</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.7.m7.1b"><apply id="S5.SS3.p2.7.m7.1.1.cmml" xref="S5.SS3.p2.7.m7.1.1"><divide id="S5.SS3.p2.7.m7.1.1.1.cmml" xref="S5.SS3.p2.7.m7.1.1"></divide><cn type="integer" id="S5.SS3.p2.7.m7.1.1.2.cmml" xref="S5.SS3.p2.7.m7.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.7.m7.1.1.3.cmml" xref="S5.SS3.p2.7.m7.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.7.m7.1c">\frac{1}{8}</annotation></semantics></math>, <math id="S5.SS3.p2.8.m8.1" class="ltx_Math" alttext="\frac{1}{16}" display="inline"><semantics id="S5.SS3.p2.8.m8.1a"><mfrac id="S5.SS3.p2.8.m8.1.1" xref="S5.SS3.p2.8.m8.1.1.cmml"><mn id="S5.SS3.p2.8.m8.1.1.2" xref="S5.SS3.p2.8.m8.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.8.m8.1.1.3" xref="S5.SS3.p2.8.m8.1.1.3.cmml">16</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.8.m8.1b"><apply id="S5.SS3.p2.8.m8.1.1.cmml" xref="S5.SS3.p2.8.m8.1.1"><divide id="S5.SS3.p2.8.m8.1.1.1.cmml" xref="S5.SS3.p2.8.m8.1.1"></divide><cn type="integer" id="S5.SS3.p2.8.m8.1.1.2.cmml" xref="S5.SS3.p2.8.m8.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.8.m8.1.1.3.cmml" xref="S5.SS3.p2.8.m8.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.8.m8.1c">\frac{1}{16}</annotation></semantics></math>, <math id="S5.SS3.p2.9.m9.1" class="ltx_Math" alttext="\frac{1}{32}" display="inline"><semantics id="S5.SS3.p2.9.m9.1a"><mfrac id="S5.SS3.p2.9.m9.1.1" xref="S5.SS3.p2.9.m9.1.1.cmml"><mn id="S5.SS3.p2.9.m9.1.1.2" xref="S5.SS3.p2.9.m9.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.9.m9.1.1.3" xref="S5.SS3.p2.9.m9.1.1.3.cmml">32</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.9.m9.1b"><apply id="S5.SS3.p2.9.m9.1.1.cmml" xref="S5.SS3.p2.9.m9.1.1"><divide id="S5.SS3.p2.9.m9.1.1.1.cmml" xref="S5.SS3.p2.9.m9.1.1"></divide><cn type="integer" id="S5.SS3.p2.9.m9.1.1.2.cmml" xref="S5.SS3.p2.9.m9.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.9.m9.1.1.3.cmml" xref="S5.SS3.p2.9.m9.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.9.m9.1c">\frac{1}{32}</annotation></semantics></math>, <math id="S5.SS3.p2.10.m10.1" class="ltx_Math" alttext="\frac{1}{64}" display="inline"><semantics id="S5.SS3.p2.10.m10.1a"><mfrac id="S5.SS3.p2.10.m10.1.1" xref="S5.SS3.p2.10.m10.1.1.cmml"><mn id="S5.SS3.p2.10.m10.1.1.2" xref="S5.SS3.p2.10.m10.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.10.m10.1.1.3" xref="S5.SS3.p2.10.m10.1.1.3.cmml">64</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.10.m10.1b"><apply id="S5.SS3.p2.10.m10.1.1.cmml" xref="S5.SS3.p2.10.m10.1.1"><divide id="S5.SS3.p2.10.m10.1.1.1.cmml" xref="S5.SS3.p2.10.m10.1.1"></divide><cn type="integer" id="S5.SS3.p2.10.m10.1.1.2.cmml" xref="S5.SS3.p2.10.m10.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.10.m10.1.1.3.cmml" xref="S5.SS3.p2.10.m10.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.10.m10.1c">\frac{1}{64}</annotation></semantics></math>, and <math id="S5.SS3.p2.11.m11.1" class="ltx_Math" alttext="\frac{1}{128}" display="inline"><semantics id="S5.SS3.p2.11.m11.1a"><mfrac id="S5.SS3.p2.11.m11.1.1" xref="S5.SS3.p2.11.m11.1.1.cmml"><mn id="S5.SS3.p2.11.m11.1.1.2" xref="S5.SS3.p2.11.m11.1.1.2.cmml">1</mn><mn id="S5.SS3.p2.11.m11.1.1.3" xref="S5.SS3.p2.11.m11.1.1.3.cmml">128</mn></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.11.m11.1b"><apply id="S5.SS3.p2.11.m11.1.1.cmml" xref="S5.SS3.p2.11.m11.1.1"><divide id="S5.SS3.p2.11.m11.1.1.1.cmml" xref="S5.SS3.p2.11.m11.1.1"></divide><cn type="integer" id="S5.SS3.p2.11.m11.1.1.2.cmml" xref="S5.SS3.p2.11.m11.1.1.2">1</cn><cn type="integer" id="S5.SS3.p2.11.m11.1.1.3.cmml" xref="S5.SS3.p2.11.m11.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.11.m11.1c">\frac{1}{128}</annotation></semantics></math> of the size of the original 22,946 training set (see Table<a href="#S5.T4" title="Table 4 ‣ 5 Experiments and Results ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for more details). With those sub-sampled fine-tuning datasets, an ImageNet pretrained model and a SceneNet RGB-D pretrained model is fine-tuned for each of the 12 training datasets.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.2" class="ltx_p">For the Robot@Home dataset, the evaluations for the two types of models show very interesting results. In Figure <a href="#S3.F2.sf2" title="In Figure 2 ‣ 3.3 Improving small models with simulated data ‣ 3 Transfer Learning with Synthetic Data as Bias Reduction ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> we show the mean IoU of the SceneNet RGB-D and ImageNet models as a function of what fraction of the dataset they were trained on. SceneNet’s best model, trained on the full set of training data, outperformed ImageNet’s best model by 15.6% in mean IoU. The model pretrained with SceneNet outperforms ImageNet for every data subdivision; even more interestingly, the performance difference is such that in most cases the SceneNet pretrained model requires between anywhere from <math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="15\%" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mrow id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mn id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml">15</mn><mo id="S5.SS3.p3.1.m1.1.1.1" xref="S5.SS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">15\%</annotation></semantics></math> to <math id="S5.SS3.p3.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS3.p3.2.m2.1a"><mrow id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mn id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">50</mn><mo id="S5.SS3.p3.2.m2.1.1.1" xref="S5.SS3.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">50\%</annotation></semantics></math> less finetuning data than the ImageNet model to match its performance. This is an especially meaningful result because it shows that roboticists considering the time and monetary investment of acquiring and labeling more data may want to first consider investing time in sampling data from a simulation before expensively collecting more real world data.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>High-Level Similarity Experiment</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To explore the effects of high-level similarity between pretraining and target task datasets, the third experiment compares results of models pretrained on different synthetic data on real segmentation datasets. We ran evaluations for a four-way cross comparison to test if high-level domain similarity in two datasets impacts training, looking at the indoor navigation and autonomous driving datasets for both synthetic and real data.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">For this experiment, the GTA dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is used as the autonomous driving pretraining data. This dataset has 25K densely annotated frames sampled from the video game Grand Theft Auto (GTA), and while 25K is small for a pretraining dataset, is was sufficient for the purpose of the experiment. To make a more apt comparison, we sub-sampled a 25K training set from SceneNet RGB-D, which we refer to as SceneNet RGB-D (25K). This was used as the indoor navigation pretraining dataset. The four-way cross comparison therefore was:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">SUN RGB-D pretrained on SceneNet RGB-D (25K) (similar)</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Cityscapes pretrained on GTA (similar)</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">SUN RGB-D pretrained on GTA (not similar)</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">Cityscapes pretrained on SceneNet RGB-D (25K) (not similar)</p>
</div>
</li>
</ul>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>High-Level Similarity - mean IoU Comparison</figcaption>
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S5.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Real Dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.2.1" class="ltx_tr">
<th id="S5.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.1.2.1.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<td id="S5.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.1.2.1.2.1" class="ltx_text ltx_font_bold">Cityscapes</span></td>
<td id="S5.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.1.2.1.3.1" class="ltx_text ltx_font_bold">SUN RGB-D</span></td>
</tr>
<tr id="S5.T5.1.3.2" class="ltx_tr">
<th id="S5.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">GTA</th>
<td id="S5.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.467</td>
<td id="S5.T5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.205</td>
</tr>
<tr id="S5.T5.1.4.3" class="ltx_tr">
<th id="S5.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SceneNet RGB-D (25K)</th>
<td id="S5.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.379</td>
<td id="S5.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.193</td>
</tr>
<tr id="S5.T5.1.5.4" class="ltx_tr">
<th id="S5.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SceneNet RGB-D</th>
<td id="S5.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.1.5.4.2.1" class="ltx_text ltx_font_bold">0.489</span></td>
<td id="S5.T5.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.1.5.4.3.1" class="ltx_text ltx_font_bold">0.227</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.3" class="ltx_p">When comparing the models pretrained on the 25K synthetic image datasets, the “high-level domain similarity” pairs i.e. Cityscapes trained over the GTA dataset and SUN RGB-D trained over SceneNet RGB-D (25K), achieved mIoU scores of <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="0.467" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mn id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">0.467</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><cn type="float" id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">0.467</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">0.467</annotation></semantics></math> and <math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="0.193" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mn id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">0.193</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><cn type="float" id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">0.193</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">0.193</annotation></semantics></math> respectively. It is worth noting that even though these datasets are considered to be typically far too small to be used for pretraining, these mean IoU scores are greater than those achieved by Cityscapes and SUN RGB-D models pretrained on ImageNet. For the other two cases, Cityscapes trained over SceneNet RGB-D (25k) achieved a score of <math id="S5.SS4.p3.3.m3.1" class="ltx_Math" alttext="0.379" display="inline"><semantics id="S5.SS4.p3.3.m3.1a"><mn id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml">0.379</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.1b"><cn type="float" id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1">0.379</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.1c">0.379</annotation></semantics></math> mIoU and SUN RGB-D trained over GTA achieved a score of 0.205 mIoU. The results of this experiment reinforce our hypothesis that pretraining data with high-level similarity has some positive effect on performance.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">It is worth noting that the SUN RGB-D model trained over GTA performed better even though the dataset domains are semantically less similar, which indicates that high-level domain similarity may not help in all cases. These results show that for two synthetic pretraining datasets of the same size from different semantic domains, models may perform better if they are pretrained on data that is similar to their goal domain. However, neither 25K frame dataset gave better performance in this experiment than the Cityscapes and SUN RGB-D models trained over the full SceneNet RGB-D training set, as can be seen in Table <a href="#S5.T5" title="Table 5 ‣ 5.4 High-Level Similarity Experiment ‣ 5 Experiments and Results ‣ Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, which is further consistent with our hypothesis that for two synthetic datasets that address the same task and sample their input from the same domain, a separate factor, in this case dataset size, dominates the other, smaller differences that affect <math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="G^{tr}_{Synth}" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><msubsup id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml"><mi id="S5.SS4.p4.1.m1.1.1.2.2" xref="S5.SS4.p4.1.m1.1.1.2.2.cmml">G</mi><mrow id="S5.SS4.p4.1.m1.1.1.3" xref="S5.SS4.p4.1.m1.1.1.3.cmml"><mi id="S5.SS4.p4.1.m1.1.1.3.2" xref="S5.SS4.p4.1.m1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.1.m1.1.1.3.1" xref="S5.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p4.1.m1.1.1.3.3" xref="S5.SS4.p4.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.1.m1.1.1.3.1a" xref="S5.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p4.1.m1.1.1.3.4" xref="S5.SS4.p4.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.1.m1.1.1.3.1b" xref="S5.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p4.1.m1.1.1.3.5" xref="S5.SS4.p4.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.1.m1.1.1.3.1c" xref="S5.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p4.1.m1.1.1.3.6" xref="S5.SS4.p4.1.m1.1.1.3.6.cmml">h</mi></mrow><mrow id="S5.SS4.p4.1.m1.1.1.2.3" xref="S5.SS4.p4.1.m1.1.1.2.3.cmml"><mi id="S5.SS4.p4.1.m1.1.1.2.3.2" xref="S5.SS4.p4.1.m1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.1.m1.1.1.2.3.1" xref="S5.SS4.p4.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS4.p4.1.m1.1.1.2.3.3" xref="S5.SS4.p4.1.m1.1.1.2.3.3.cmml">r</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><apply id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p4.1.m1.1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1">subscript</csymbol><apply id="S5.SS4.p4.1.m1.1.1.2.cmml" xref="S5.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p4.1.m1.1.1.2.1.cmml" xref="S5.SS4.p4.1.m1.1.1">superscript</csymbol><ci id="S5.SS4.p4.1.m1.1.1.2.2.cmml" xref="S5.SS4.p4.1.m1.1.1.2.2">𝐺</ci><apply id="S5.SS4.p4.1.m1.1.1.2.3.cmml" xref="S5.SS4.p4.1.m1.1.1.2.3"><times id="S5.SS4.p4.1.m1.1.1.2.3.1.cmml" xref="S5.SS4.p4.1.m1.1.1.2.3.1"></times><ci id="S5.SS4.p4.1.m1.1.1.2.3.2.cmml" xref="S5.SS4.p4.1.m1.1.1.2.3.2">𝑡</ci><ci id="S5.SS4.p4.1.m1.1.1.2.3.3.cmml" xref="S5.SS4.p4.1.m1.1.1.2.3.3">𝑟</ci></apply></apply><apply id="S5.SS4.p4.1.m1.1.1.3.cmml" xref="S5.SS4.p4.1.m1.1.1.3"><times id="S5.SS4.p4.1.m1.1.1.3.1.cmml" xref="S5.SS4.p4.1.m1.1.1.3.1"></times><ci id="S5.SS4.p4.1.m1.1.1.3.2.cmml" xref="S5.SS4.p4.1.m1.1.1.3.2">𝑆</ci><ci id="S5.SS4.p4.1.m1.1.1.3.3.cmml" xref="S5.SS4.p4.1.m1.1.1.3.3">𝑦</ci><ci id="S5.SS4.p4.1.m1.1.1.3.4.cmml" xref="S5.SS4.p4.1.m1.1.1.3.4">𝑛</ci><ci id="S5.SS4.p4.1.m1.1.1.3.5.cmml" xref="S5.SS4.p4.1.m1.1.1.3.5">𝑡</ci><ci id="S5.SS4.p4.1.m1.1.1.3.6.cmml" xref="S5.SS4.p4.1.m1.1.1.3.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">G^{tr}_{Synth}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">In this work, we investigated the potential gains of using synthetic data to augment the training process of small CNNs designed for real-time semantic segmentation in robots with small target training sets. We compared the improvements afforded by synthetic data to traditional data augmentation and transfer learning from image classification. The performance gains from pretraining with synthetic data indicate that the degree to which this closes the transfer gap <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="G^{tr}" display="inline"><semantics id="S6.p1.1.m1.1a"><msup id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">G</mi><mrow id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml"><mi id="S6.p1.1.m1.1.1.3.2" xref="S6.p1.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.3.1" xref="S6.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S6.p1.1.m1.1.1.3.3" xref="S6.p1.1.m1.1.1.3.3.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1">superscript</csymbol><ci id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">𝐺</ci><apply id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3"><times id="S6.p1.1.m1.1.1.3.1.cmml" xref="S6.p1.1.m1.1.1.3.1"></times><ci id="S6.p1.1.m1.1.1.3.2.cmml" xref="S6.p1.1.m1.1.1.3.2">𝑡</ci><ci id="S6.p1.1.m1.1.1.3.3.cmml" xref="S6.p1.1.m1.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">G^{tr}</annotation></semantics></math> for these models is reliably greater than the bias introduced by the “Sim2Real” problem, <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="G^{s2r}" display="inline"><semantics id="S6.p1.2.m2.1a"><msup id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mi id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">G</mi><mrow id="S6.p1.2.m2.1.1.3" xref="S6.p1.2.m2.1.1.3.cmml"><mi id="S6.p1.2.m2.1.1.3.2" xref="S6.p1.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.p1.2.m2.1.1.3.1" xref="S6.p1.2.m2.1.1.3.1.cmml">​</mo><mn id="S6.p1.2.m2.1.1.3.3" xref="S6.p1.2.m2.1.1.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S6.p1.2.m2.1.1.3.1a" xref="S6.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S6.p1.2.m2.1.1.3.4" xref="S6.p1.2.m2.1.1.3.4.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1">superscript</csymbol><ci id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">𝐺</ci><apply id="S6.p1.2.m2.1.1.3.cmml" xref="S6.p1.2.m2.1.1.3"><times id="S6.p1.2.m2.1.1.3.1.cmml" xref="S6.p1.2.m2.1.1.3.1"></times><ci id="S6.p1.2.m2.1.1.3.2.cmml" xref="S6.p1.2.m2.1.1.3.2">𝑠</ci><cn type="integer" id="S6.p1.2.m2.1.1.3.3.cmml" xref="S6.p1.2.m2.1.1.3.3">2</cn><ci id="S6.p1.2.m2.1.1.3.4.cmml" xref="S6.p1.2.m2.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">G^{s2r}</annotation></semantics></math>. We also documented the evolution of improvements to real-time semantic segmentation models as access to real data decreases, and showed that as dataset size decreases, the improvements from using task similar synthetic data increase exponentially compared to ImageNet.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We are currently considering how this technique might be used with other solutions to semi-supervised or weakly supervised problems, and whether there are other, more interesting ways to effectively use simulation to improve real-time segmentation, potentially as a feedback signal for model architecture search, or even more interestingly as an oracle in an active vision or lifelong learning agent.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Using simulation and domain adaptation to improve efficiency of deep
robotic grasping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1709.07857</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
G. J. Brostow, J. Fauqueur, and R. Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Semantic object classes in video: A high-definition ground truth
database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition Letters</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 30(2):88–97, 2009.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Canziani, A. Paszke, and E. Culurciello.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">An analysis of deep neural network models for practical applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1605.07678</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
U. Franke, S. Roth, and B. Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">The cityscapes dataset for semantic urban scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
R. Detry, J. Papon, and L. H. Matthies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Task-oriented grasping with semantic and geometric scene
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems
(IROS)</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
A. Gaidon, Q. Wang, Y. Cabon, and E. Vig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Virtual worlds as proxy for multi-object tracking analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of IEEE Conf. on Computer Vision and Pattern
Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 4340–4349, 2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, and
J. G. Rodríguez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">A review on deep learning techniques applied to semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, abs/1704.06857, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
A. Geiger, P. Lenz, and R. Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR), IEEE Conf.
on</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2012.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Deep learning</span><span id="bib.bib9.3.2" class="ltx_text" style="font-size:90%;">, volume 1.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">MIT press Cambridge, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Q. Ha, K. Watanabe, T. Karasawa, Y. Ushiku, and T. Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Mfnet: Towards real-time semantic segmentation for autonomous
vehicles with multi-spectral scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems
(IROS)</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, pages 5108–5115, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
J. Hoffman, D. Wang, F. Yu, and T. Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Fcns in the wild: Pixel-level adversarial and constraint-based
adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, abs/1612.02649, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
S. James, A. J. Davison, and E. Johns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Transferring end-to-end visuomotor control from simulation to real
world for a multi-stage task.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In S. Levine, V. Vanhoucke, and K. Goldberg, editors, </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of
the 1st Annual Conf. on Robot Learning</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, Proc. of Machine Learning Research,
pages 334–343, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
A. Janoch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">The berkeley 3d object dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">Master’s thesis, EECS Department, UC Berkeley, 2012.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar, K. Rosaen, and
R. Vasudevan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Driving in the matrix: Can virtual worlds replace human-generated
annotations for real world tasks?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Conf. on Robotics and Automation</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8,
2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
D. Kingma and J. Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
I. Kostavelis and A. Gasteratos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Semantic mapping for mobile robotics tasks: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics and Autonomous Systems</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 66:86–103, 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
J. Lin, W.-J. Wang, S.-K. Huang, and H.-C. Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Learning based semantic segmentation for robot navigation in outdoor
environment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">9th Int. Conf. on Soft Computing and Intelligent Systems
(IFSA-SCIS)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 1–5. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
R. Madaan, D. Maturana, and S. Scherer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Wire detection using synthetic data and dilated convolutional
networks for unmanned aerial vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems
(IROS)</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, pages 3487–3494, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
N. Mayer, E. Ilg, P. Fischer, C. Hazirbas, D. Cremers, A. Dosovitskiy, and
T. Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">What makes good synthetic training data for learning disparity and
optical flow estimation?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, pages 1–19.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J. McCormac, A. Handa, A. Davison, and S. Leutenegger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Semanticfusion: Dense 3d semantic mapping with convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1609.05130</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
J. McCormac, A. Handa, S. Leutenegger, and A. Davison.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Scenenet rgb-d: Can 5m synthetic images beat generic imagenet
pre-training on indoor segmentation?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of IEEE Int. Conf. on Computer Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Scenenet rgb-d: 5m photorealistic images of synthetic indoor
trajectories with ground truth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1612.05079</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
M. Oquab, L. Bottou, I. Laptev, and J. Sivic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Learning and transferring mid-level image representations using
convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc of IEEE Conf. on Computer Vision and Pattern
Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 1717–1724, 2014.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Enet: A deep neural network architecture for real-time semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.02147</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Automatic differentiation in pytorch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
W. Qiu and A. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Unrealcv: Connecting computer vision to unreal engine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV 2016 Workshops</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Xnor-net: Imagenet classification using binary convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conf. on Computer Vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
S. R. Richter, V. Vineet, S. Roth, and V. Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conf. on Computer Vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 102–118. Springer,
2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
O. Ronneberger, P. Fischer, and T. Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. on Medical Image Computing and Computer-Assisted
Intervention</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 234–241. Springer, 2015.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of IEEE Conf. on Computer Vision and Pattern
Recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 3234–3243, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
J. Ruiz-Sarmiento, C. Galindo, and J. González-Jiménez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Robot@ home, a robotic dataset for semantic mapping of home
environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The Int. Journal of Robotics Research</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 36(2), 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
A. Shafaei, J. J. Little, and M. Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Play and learn: Using video games to train computer vision models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1608.01745</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Indoor segmentation and support inference from rgbd images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
K. Simonyan and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1409.1556</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
S. Song, S. P. Lichtenberg, and J. Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Sun rgb-d: A rgb-d scene understanding benchmark suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of IEEE Conf. on computer vision and pattern
recognition</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 567–576, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Semantic scene completion from a single depth image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. on Computer Vision and Pattern Recognition</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Multinet: Real-time joint semantic reasoning for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1612.07695</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for transferring deep neural networks from
simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, abs/1703.06907, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
A. Torralba and A. A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Unbiased look at dataset bias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
Conf. on</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 1521–1528. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
J. Vertens, A. Valada, and W. Burgard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Smsnet: Semantic motion segmentation using deep convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems
(IROS)</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
B. Wu, F. Iandola, P. H. Jin, and K. Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Squeezedet: Unified, small, low power fully convolutional neural
networks for real-time object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition Workshops</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages
446–454. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
J. Xiao, A. Owens, and A. Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Sun3d: A database of big spaces reconstructed using sfm and object
labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of the IEEE Int. Conf. on Computer Vision</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages
1625–1632, 2013.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">How transferable are features in deep neural networks?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages
3320–3328, 2014.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhang, P. David, and B. Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Curriculum domain adaptation for semantic segmentation of urban
scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision (ICCV), 2017 IEEE International Conference
on</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 2039–2049. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Z. Zhang, S. Fidler, and R. Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Instance-level segmentation for autonomous driving with deep densely
connected mrfs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of IEEE Conf. on Computer Vision and Pattern
Recognition</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Places: A 10 million image database for scene recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1809.03675" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1809.03676" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1809.03676">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1809.03676" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1809.03677" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 13:23:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
