<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Present and Future Generalization of Synthetic Image Detectors</title>
<!--Generated on Sat Sep 21 12:29:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="" lang="en" name="keywords"/>
<base href="/html/2409.14128v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S1" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S2" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS1" title="In 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS2" title="In 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Train Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS3" title="In 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS4" title="In 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Computation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Train Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.SS1" title="In 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Single Class Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.SS2" title="In 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Multi-class Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.SS3" title="In 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Image Alteration Methods</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.SS3.SSS1" title="In 4.3. Image Alteration Methods ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span><span class="ltx_text ltx_font_typewriter">SuSy</span> - Our robust model</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.SS1" title="In 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Generalization to source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.SS2" title="In 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Generalization to image</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Generalization of state-of-the-art models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.SS1" title="In 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Benchmarking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.SS2" title="In 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Scale Generalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S7" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S7.SS1" title="In 7. Conclusions ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Ethical risks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A1" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A2" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Evaluation Dataset Image Samples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A3" title="In Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Evaluation Image Resolution Distribution</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Present and Future Generalization of Synthetic Image Detectors</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pablo Bernabeu-Perez
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0005-0480-1336" title="ORCID identifier">0009-0005-0480-1336</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Barcelona Supercomputing Center (BSC)</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Barcelona</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pablo.bernabeu@bsc.es">pablo.bernabeu@bsc.es</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enrique Lopez-Cuena
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0001-4004-955X" title="ORCID identifier">0009-0001-4004-955X</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Barcelona Supercomputing Center (BSC)</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Barcelona</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:enrique.lopez@bsc.es">enrique.lopez@bsc.es</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dario Garcia-Gasulla
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-6732-5641" title="ORCID identifier">0000-0001-6732-5641</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Barcelona Supercomputing Center (BSC)</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Barcelona</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dario.garcia@bsc.es">dario.garcia@bsc.es</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">The continued release of new and better image generation models increases the demand for synthetic image detectors. In such a dynamic field, detectors need to be able to generalize widely and be robust to uncontrolled alterations. The present work is motivated by this setting, when looking at the role of time, image transformations and data sources, for detector generalization. In these experiments, none of the evaluated detectors is found universal, but results indicate an ensemble could be. Experiments on data collected <span class="ltx_text ltx_font_italic" id="id10.id1.1">in the wild</span> show this task to be more challenging than the one defined by large-scale datasets, pointing to a gap between experimentation and actual practice. Finally, we observe a race equilibrium effect, where better generators lead to better detectors, and vice versa. We hypothesize this pushes the field towards a perpetually close race between generators and detectors.</p>
</div>
<div class="ltx_keywords">
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; ; </span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancement of generative AI has revolutionized image generation, presenting challenges related to visual information integrity, trust and rights in digital environments, and mitigation of misinformation, among others. As a result, recent legislation mandates the identification and notification of the synthetic nature of such digital content <cite class="ltx_cite ltx_citemacro_citep">(Union, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib39" title="">2024</a>)</cite>. By virtue of all these needs, correctly attributing synthetic content has become a social demand, and a top scientific priority (<span class="ltx_text ltx_font_italic" id="S1.p1.1.1">i.e., </span>is this image <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">authentic</span> or <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">synthetic</span>?).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The field of synthetic image <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">detection</span> is in a continuous race with the field of synthetic image <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">generation</span>. The detection field tries to prevent the race by developing universal detectors <cite class="ltx_cite ltx_citemacro_citep">(Ojha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib33" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib9" title="">2024</a>)</cite>, but the generalization capacity of these solutions is not entirely known. Meanwhile, new generative models join the race every month (if not every day), begging for the question, where is this going?</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This work uncovers where current Synthetic Image Detection (SID) is, and where it seems to be headed to. To do so, it first analyzes the impact of several training conditions on model generalization, producing a recipe for building robust detectors. Those guidelines are then used to train a baseline for the study of generalization across source variations (who created the data), model variation (which model was used) and model age (when was the model released). The same experimental setup is used to conduct an updated benchmark of state-of-the-art detector models, using synthetic data produced by the latest generators.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Findings indicate current detectors are insufficient for synthetic content detection, especially when used alone. Suggestions include the use of detector ensembles, as well as the training of generator-specific detectors. This last option appears as the most reliable solution and is shown to generalize to different data sources. Finally, the ethical considerations of this line of work are discussed, including when and how should detectors be publicly released, considering how these may be used to train generators which are harder to identify. To complete the work contributions we release two novel datasets, the software library used for experimentation and the weights of the best detector trained.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Any work done on training and analyzing synthetic image detectors is defined by the models it is trained and evaluated on. This limits the consistency and relevance of results in a fast-moving field like the one of generative AI, where models are continuously and quickly improved and replaced. The main body of previous work on synthetic image detectors was conducted on Generative Adversarial Network (GAN) generators (now only used because of their relative speed), as well as early versions of diffusion-based models <cite class="ltx_cite ltx_citemacro_citep">(Wißmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib43" title="">2024</a>; Corvi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib10" title="">2023</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib44" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib28" title="">2024</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib49" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib48" title="">2023</a>; Grommelt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib18" title="">2024</a>; Ojha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib33" title="">2023</a>)</cite>. A few recent works target data produced by some of the latest diffusion models, which are the ones currently compromising the human ability to discriminate between real and fake <cite class="ltx_cite ltx_citemacro_citep">(Cozzolino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib11" title="">2024</a>; Ricker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib34" title="">2022</a>; Cazenavette et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib8" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib9" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Generalization is key in the SID field, as failing to successfully detect samples produced by alternative generative models limits the applicability of any proposed solution. To study generalization, common practice in the field is to train models using data produced by one model, while evaluating performance on data produced by other models <cite class="ltx_cite ltx_citemacro_citep">(Corvi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib10" title="">2023</a>; Ricker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib34" title="">2022</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib49" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib48" title="">2023</a>; Grommelt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib18" title="">2024</a>; Ojha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib33" title="">2023</a>; Cazenavette et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib8" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib9" title="">2024</a>)</cite>. However, none of these studies considers the temporal natural of generative models nor what this may tell us about the detection performance on models to be released in the future. Remarkably, this is the case that matters the most for the practical use of synthetic image detectors.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">While the field is missing an exhaustive and updated study on the generalization of SID, a few sources of bias have been considered before. Particularly, the effect of image format and resolution has been studied in the past. In <cite class="ltx_cite ltx_citemacro_citep">(Corvi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib10" title="">2023</a>)</cite>, authors highlight the impact of the resizing operation, which is common in deep learning architectures, and used to adapt any image to the desired network input resolution. This transformation could remove the high-frequency artifacts created by the generative models, hindering the detection process. GenImage <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib49" title="">2024</a>)</cite> studies the image content generalization, that is, how a detector performs on domains outside the training data, such as art and face images. They show a great generalization capability for these two cases, with 95% and 99.9% accuracy scores respectively. Yet, the generalization of image content has not been thoroughly explored. This is particularly important given that SID models, which often rely on common training datasets like LSUN <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib46" title="">2015</a>)</cite> and ProGAN <cite class="ltx_cite ltx_citemacro_citep">(Karras et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib23" title="">2017</a>)</cite>, suffer from limited class representation.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The study presented in <cite class="ltx_cite ltx_citemacro_citep">(Grommelt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib18" title="">2024</a>)</cite> highlights biases associated with <span class="ltx_text ltx_font_italic" id="S2.p4.1.1">JPEG</span> compression and image size, demonstrating that these biases are prevalent in many training datasets. Notably, they reveal a significant disparity in image formats between authentic and synthetic images, with authentic images typically stored in <span class="ltx_text ltx_font_italic" id="S2.p4.1.2">JPEG</span> format and synthetic images in <span class="ltx_text ltx_font_italic" id="S2.p4.1.3">PNG</span>. Furthermore, the authors demonstrate a size bias affecting detector performance, with detectors generally performing better on natural images that differ significantly in size from the generated images used in training. This observation aligns with findings in <cite class="ltx_cite ltx_citemacro_citep">(Cozzolino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib11" title="">2024</a>)</cite>, which demonstrate that dataset choice significantly impacts detection performance. The authors found that using the LSUN dataset for authentic samples, instead of COCO, led to considerable degradation in results. This was attributed to LSUN’s lack of diversity and inherent biases.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Meanwhile, relevant factors remain to be studied. This includes model family, model release date, and dataset source, among others. These are of special relevance for understanding generalization, and the practical future of the field.
</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To focus on the properties of synthetic images, we will fix the detector architecture used in all our experiments (§<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS1" title="3.1. Architecture ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.1</span></a>). In contrast, we consider a variety of recent synthetic image datasets, and by extension of AI image generators, as this is the real target of our work (see §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS2" title="3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.2</span></a> and §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS3" title="3.3. Evaluation Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Finally, we report computation details, enabling full reproducibility of our work, in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS4" title="3.4. Computation ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Architecture</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">There are two popular architectural choices when building a synthetic image detector: training a direct classifier or using the features extracted from a pre-trained or fine-tuned model to discriminate samples. Both CNNs <cite class="ltx_cite ltx_citemacro_citep">(Ricker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib34" title="">2022</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib49" title="">2024</a>; Grommelt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib18" title="">2024</a>)</cite> and transformers (ViT encoder) <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib44" title="">2023</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib48" title="">2023</a>; Ojha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib33" title="">2023</a>; Cozzolino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib11" title="">2024</a>)</cite> have been considered to that end, both of them performing competitively.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In this work we fix the architecture, to focus on the evolution, nature and practical use of generalization models. For our experimentation, we choose a ResNet<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib20" title="">2016</a>)</cite>, as this has shown competitive results <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib49" title="">2024</a>; Grommelt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib18" title="">2024</a>; Cazenavette et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib8" title="">2024</a>)</cite>, and it’s a relatively lightweight architecture. In particular, we follow the staircase design proposed in <cite class="ltx_cite ltx_citemacro_citep">(López Cuena, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib31" title="">2023</a>)</cite> with a ResNet-18 backbone (12.7M parameters), which feeds features extracted at different depths into an MLP (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.F1" title="Figure 1 ‣ 3.1. Architecture ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="S3.F1.g1" src="extracted/5870075/images/model_architecture_new.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Detector architecture used, based on a ResNet-18 from <cite class="ltx_cite ltx_citemacro_citep">(López Cuena, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib31" title="">2023</a>)</cite>. Includes ResNet blocks (blue), bottlenecks (red), adaptative average pooling 2D (orange), concatenation (yellow) and an MLP (green).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">SID often relies on analyzing specific portions of an image rather than the entire composition. This approach is advantageous for several reasons. Firstly, not all parts of an image may be artificial, as they can be partially edited or altered through methods like inpainting. Secondly, its nature may be better identified in specific portions of the image as some regions might contain more telltale signs of manipulation than others. Finally, processing entire high-resolution images can be computationally intensive and time-consuming.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">To address these challenges, most detectors are trained on image patches. This introduces new variables in the detection process: the number of patches and their selection strategy, as well as the criterion to convert single-patch predictions to whole image ones. To determine our patch selection strategy, we conduct early experiments and find that patches which exhibit the highest contrast in their grey-level co-occurrence matrix (GLCM) produce slightly more performant models. Due to the size of our input images, we consider a maximum of five <math alttext="224\times 224" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mn id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">224</mn><mo id="S3.SS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><cn id="S3.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p4.1.m1.1.1.2">224</cn><cn id="S3.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p4.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">224 × 224</annotation></semantics></math> patches. The predictions of these individual patches can then be aggregated according to the use case requirements (<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">e.g., </span>maximizing the detection of false positives, or prioritizing false negatives instead). In particular, we select a voting mechanism and set a minimum amount of patches for an image to be considered synthetic. We will study the effect of the value of this threshold in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.SS2" title="5.2. Generalization to image ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Train Datasets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To train the synthetic content detectors we consider two types of datasets. The first contains real-world images, that is photographs of different scenes taken under different light conditions. We use <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">COCO</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib27" title="">2015</a>)</cite> for training and validation (§<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4" title="4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a>). The second type of datasets considered are those composed of synthetic images. That is, AI-generated images. Five datasets are used, created using five different AI image generators: <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.2">dalle3-images</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.3">diffusiondb</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.4">SDXL</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.5">mj-tti</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.6">mj-images</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1">dalle3-images</span> <cite class="ltx_cite ltx_citemacro_citep">(ehristoforu, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib15" title="">2024a</a>)</cite> dataset contains 1,647 unique, deduplicated images generated by <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.2">DALLE3</span> (2023), encompassing both photorealistic and digital art styles. Another dataset, the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.3">diffusiondb</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib41" title="">2022</a>)</cite>, was created using models of the 1.x Stable Diffusion series, which were released in 2022. In this dataset, we filter samples making sure that <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.4">”photo”</span> appears in the prompt. Images from this dataset are of lower quality and visual detail than those of its successor <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.5">SDXL</span> <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib4" title="">2023</a>)</cite>, which was released in 2023. The associated dataset, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.6">SDXL</span> <cite class="ltx_cite ltx_citemacro_citep">(DucHaiten, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib14" title="">2023</a>)</cite> contains 5,435 images in the <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.7">”realistic”</span> subset.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Beyond DALL-E and Stable Diffusion, the third main provider of synthetic images is Midjourney. Its early iterations, the V1 and V2 models, date from early 2022, and were used to populate the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">mj-tti</span> <cite class="ltx_cite ltx_citemacro_citep">(Turc and Nemade, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib38" title="">2022</a>)</cite> dataset, which contains 4,530 images. Collage images and mosaics made of synthetic images were removed from this dataset. Later models, the V5 and V6 models from 2023 compose our last training dataset, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2">mj-images</span> <cite class="ltx_cite ltx_citemacro_citep">(ehristoforu, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib16" title="">2024b</a>)</cite>, with 1,226 images. This dataset also had to be deduplicated. All these synthetic models will be used for training and validation in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4" title="4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">In §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4" title="4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a>, during the training stages of this work, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.1">COCO</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.2">dif</span>-<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.3">fusiondb</span> datasets are undersampled to include a maximum of 5,435 samples. Existing train, validation and test partitions are respected. When undefined a 60%-20%-20% random split is performed. For the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.4">SDXL</span> dataset we include the <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.5">realistic-2.2</span> split for training and validation, and the <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.6">realistic-1</span> split for test. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.T1" title="Table 1 ‣ 3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a> shows the exact composition per data source.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Train Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Year</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">Form</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.5.1">A/S</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.6.1">Tra/Val/Test</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.2.2.1.1">COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">2017</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">JPG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">Aut</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">2,967/1,234/1,234</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.3.3.1.1">dalle3-images</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">DALLE-3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">JPG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">987/330/330</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.4.4.1.1">diffusiondb</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">SD 1.X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">2,967/1,234/1,234</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.5.5.1.1">SDXL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">SDXL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">2,967/1,234/1,234</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.6.6.1.1">mj-tti</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">MJ V1/2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">2,718/906/906</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.7.7.1.1">mj-images</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">MJ V5/6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">JPG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">1,845/617/617</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.8.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.1.1">Test Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.2.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.3.1">Year</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.4.1">Form</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.5.1">A/S</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.6.1">Test Size</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.9.9.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.9.9.1.1">Flickr30k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.9.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.9.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">2014</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">JPEG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">Aut</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">31,655</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.10.10.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.10.10.1.1">GLDv2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.10.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.10.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">2020</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">JPEG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">Aut</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">5,000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.11.11.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.11.11.1.1">In-the-wild</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.11.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.11.11.3" style="padding-left:2.0pt;padding-right:2.0pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.4" style="padding-left:2.0pt;padding-right:2.0pt;">Mix</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.5" style="padding-left:2.0pt;padding-right:2.0pt;">Aut</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.6" style="padding-left:2.0pt;padding-right:2.0pt;">121</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.12.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.12.12.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.12.12.2" style="padding-left:2.0pt;padding-right:2.0pt;">Many</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.12.12.3" style="padding-left:2.0pt;padding-right:2.0pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">9,000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.13.13.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.13.13.1.1">SD3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.13.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">SD 3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.13.13.3" style="padding-left:2.0pt;padding-right:2.0pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.6" style="padding-left:2.0pt;padding-right:2.0pt;">8,192</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.14.14.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.14.14.1.1">FLUX.1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.14.14.2" style="padding-left:2.0pt;padding-right:2.0pt;">FLUX.1-dev</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.14.14.3" style="padding-left:2.0pt;padding-right:2.0pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">8,192</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.15.15.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T1.1.15.15.1.1">In-the-wild</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.15.15.2" style="padding-left:2.0pt;padding-right:2.0pt;">?</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.15.15.3" style="padding-left:2.0pt;padding-right:2.0pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.15.4" style="padding-left:2.0pt;padding-right:2.0pt;">PNG</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.15.5" style="padding-left:2.0pt;padding-right:2.0pt;">Syn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.15.6" style="padding-left:2.0pt;padding-right:2.0pt;">99</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Datasets, including generative models included, release date, image format, authentic or synthetic, and number of samples within train, validation and test.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Evaluation Datasets</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The datasets in this section will assess the detector’s generalization ability when trained on limited data and applied to diverse external sources (§ <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.SS1" title="5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5.1</span></a>). The evaluation aims to cover three scenarios: datasets from the same model used in training but generated by different users or slightly different software or model versions; datasets from entirely different models; and samples from various unknown sources and models, simulating real-world applications. Examples of these datasets are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A2" title="Appendix B Evaluation Dataset Image Samples ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">B</span></a>, with the resolution distribution of each dataset detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A3.F6" title="Figure 6 ‣ Appendix C Evaluation Image Resolution Distribution ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">For samples representing the class of authentic images, three datasets are added. The <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.1.1">Flickr30k</span> <cite class="ltx_cite ltx_citemacro_citep">(Young et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib45" title="">2014</a>)</cite> dataset, which includes 31,655 samples primarily depicting scenes with humans. A subset of the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.1.2">Google Landmarks v2</span> <cite class="ltx_cite ltx_citemacro_citep">(Weyand et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib42" title="">2020</a>)</cite> test partition, containing 5,000 images of both natural and human-made landmarks, and finally, the authors have curated a small-scale dataset referred to as <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.1.3">In-the-wild</span>. The first subset of this dataset, containing authentic images from platforms such as <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.4">Reddit</span> (from communities that forbid AI content) and <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.5">Flickr</span> (uploaded before 2020), is included as an authentic evaluation dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Synthetic data for evaluation is obtained either from the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">Synth</span>-<span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.2">buster</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Bammey, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib6" title="">2023</a>)</cite> or is collected/generated by the authors. <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.3">Synth</span>-<span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.4">buster</span> contains images generated with nine different models (<math alttext="1,000" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.2"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.3.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">1</mn><mo id="S3.SS3.p3.1.m1.2.3.2.1" xref="S3.SS3.p3.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p3.1.m1.2.2" xref="S3.SS3.p3.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><list id="S3.SS3.p3.1.m1.2.3.1.cmml" xref="S3.SS3.p3.1.m1.2.3.2"><cn id="S3.SS3.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1">1</cn><cn id="S3.SS3.p3.1.m1.2.2.cmml" type="integer" xref="S3.SS3.p3.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">1,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.2d">1 , 000</annotation></semantics></math> images each), and provides a wide coverage for our experimentation by including data from models with our train set (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.5">e.g., </span><span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.6">SD1.X</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.7">DALLE3</span>) and others outside of it (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.8">e.g., </span><span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.9">DALLE2</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.10">Firefly</span>). The images are based on the RAISE-1k dataset <cite class="ltx_cite ltx_citemacro_citep">(Dang-Nguyen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib12" title="">2015</a>)</cite>, the corresponding prompts were manually refined to enhance photorealism and remove references to living persons or specific artists.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Three datasets are crafted for this paper. The first contains 8,192 images generated with Stable Diffusion 3 Medium (<span class="ltx_text ltx_font_typewriter" id="S3.SS3.p4.1.1">SD3</span>), a Multimodal Diffusion Transformer (MMDiT) text-to-image model. Input prompts are extracted from <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2">Gustavosta/Stable-Diffusion-Prompts<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_upright" id="S3.SS3.p4.1.2.1.1">(</span>Gustavosta<span class="ltx_text ltx_font_upright" id="S3.SS3.p4.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib19" title="">2023</a><span class="ltx_text ltx_font_upright" id="S3.SS3.p4.1.2.3.3">)</span></cite></span>. For each image, height and width were randomly selected from a uniform distribution over the set <math alttext="\{512,768,1024,1344\}" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.4"><semantics id="S3.SS3.p4.1.m1.4a"><mrow id="S3.SS3.p4.1.m1.4.5.2" xref="S3.SS3.p4.1.m1.4.5.1.cmml"><mo id="S3.SS3.p4.1.m1.4.5.2.1" stretchy="false" xref="S3.SS3.p4.1.m1.4.5.1.cmml">{</mo><mn id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">512</mn><mo id="S3.SS3.p4.1.m1.4.5.2.2" xref="S3.SS3.p4.1.m1.4.5.1.cmml">,</mo><mn id="S3.SS3.p4.1.m1.2.2" xref="S3.SS3.p4.1.m1.2.2.cmml">768</mn><mo id="S3.SS3.p4.1.m1.4.5.2.3" xref="S3.SS3.p4.1.m1.4.5.1.cmml">,</mo><mn id="S3.SS3.p4.1.m1.3.3" xref="S3.SS3.p4.1.m1.3.3.cmml">1024</mn><mo id="S3.SS3.p4.1.m1.4.5.2.4" xref="S3.SS3.p4.1.m1.4.5.1.cmml">,</mo><mn id="S3.SS3.p4.1.m1.4.4" xref="S3.SS3.p4.1.m1.4.4.cmml">1344</mn><mo id="S3.SS3.p4.1.m1.4.5.2.5" stretchy="false" xref="S3.SS3.p4.1.m1.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.4b"><set id="S3.SS3.p4.1.m1.4.5.1.cmml" xref="S3.SS3.p4.1.m1.4.5.2"><cn id="S3.SS3.p4.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p4.1.m1.1.1">512</cn><cn id="S3.SS3.p4.1.m1.2.2.cmml" type="integer" xref="S3.SS3.p4.1.m1.2.2">768</cn><cn id="S3.SS3.p4.1.m1.3.3.cmml" type="integer" xref="S3.SS3.p4.1.m1.3.3">1024</cn><cn id="S3.SS3.p4.1.m1.4.4.cmml" type="integer" xref="S3.SS3.p4.1.m1.4.4">1344</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.4c">\{512,768,1024,1344\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.4d">{ 512 , 768 , 1024 , 1344 }</annotation></semantics></math> pixels. The images were generated using the official model accessed through Hugging Face <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib5" title="">2024</a>)</cite>. We employed a consistent generation process across all images, utilizing 28 inference steps for each generation. To enhance the quality and realism of the generated images, we added a set of negative prompts: <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.3">poorly rendered face, poor facial details, poorly rendered hands, low resolution, blurry image, oversaturated, extra fingers, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, extra foot</span></p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">The same strategy is used to generate the second dataset, with the recent <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p5.1.1">FLUX.1</span>-dev <cite class="ltx_cite ltx_citemacro_citep">(Labs, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib25" title="">2024</a>)</cite> model, a 12B parameter rectified flow transformer model, which leverages a mixture of MMDiT and DiT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib26" title="">2022</a>)</cite>. We used <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.2">torch.bfloat16</span> precision, 50 inference steps, a guidance scale of 3.5, and a maximum sequence length of 512.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">Finally, the synthetic subset of the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p6.1.1">In-the-wild</span> is included, composed of manually selected images that exhibit the highest photorealism according to the author’s criteria. Samples were extracted from <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.2">Civitai</span> <cite class="ltx_cite ltx_citemacro_citep">(civ, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib2" title="">[n. d.]</a>)</cite>, a platform intended for sharing generative models and synthetic images, and from Reddit communities dedicated to the sharing of synthetic content. This dataset is designed to challenge the detector’s ability to generalize across diverse content, simulating real-world scenarios where the origins of synthetic images are often unknown.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Computation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The software stack used to develop and evaluate SID models was based on PyTorch.
To enable the full reproducibility of our work, we share the code needed to run the experiments <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/HPAI-BSC/SuSy</span></span></span>, the datasets as compiled and cleaned <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/datasets/HPAI-BSC/SuSy-Dataset</span></span></span>, and the model weights <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/HPAI-BSC/SuSy</span></span></span> for our best detector (see §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.SS3.SSS1" title="4.3.1. SuSy - Our robust model ‣ 4.3. Image Alteration Methods ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">For the benchmarking of other detectors (§ <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6" title="6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a>), we use the <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.1">sidbench</span> library <cite class="ltx_cite ltx_citemacro_citep">(Schinas and Papadopoulos, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib35" title="">2024</a>)</cite>, which supports several models while facilitating inference on custom datasets. Available checkpoints were downloaded from the AIGCDetectBenchmark <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib47" title="">2023</a>)</cite> repository. We download the missing models from their corresponding official sources.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.2">We utilize an Intel Xeon Platinum 8460Y processor and one NVIDIA Hopper H100 64GB GPU. Seventy-five training runs were conducted with this setup, totalling sixteen hours of computing time, while continuously monitoring GPU power usage. Using the European Union’s latest <math alttext="\mathrm{CO_{2}}" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">CO</mi><mn id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">CO</ci><cn id="S3.SS4.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\mathrm{CO_{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">roman_CO start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> emission ratio <cite class="ltx_cite ltx_citemacro_citep">(Agency, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib3" title="">2024</a>)</cite>, we estimated the carbon footprint of these experiments to be 0.63 kg of <math alttext="\mathrm{CO_{2}}" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><msub id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">CO</mi><mn id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">CO</ci><cn id="S3.SS4.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS4.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\mathrm{CO_{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">roman_CO start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Train Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section we explore the effect of varying the training strategy, considering single-class models, multiclass classifiers, and finally, image alterations.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Single Class Models</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To study the relation between the images produced using the latest image generation models, let us consider the generalization capacity of single-class discriminative models trained on those images. In this section we train binary classifiers, using each synthetic dataset of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.T1" title="Table 1 ‣ 3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a> as a positive class, and the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.1">COCO</span> dataset as a negative class. Those models are then tested on the remaining datasets, to see how they generalize. Single-class models are trained for a maximum of 20 epochs with an early-stopping mechanism monitoring validation accuracy with patience of 2. Data augmentation is limited to horizontal flips with a 50% probability, with further transformation studies left for § <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.SS3" title="4.3. Image Alteration Methods ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.1.1.2.1">DALLE3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.1.1.3.1">SD1.X</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.1.1.4.1">SDXL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1.5"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.1.1.5.1">MJ 1/2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.1.1.1.6"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.1.1.6.1">MJ 5/6</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.7.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.2.1.1.1">DALLE3</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.2.1">97.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">27.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">70.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5">50.73</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.6">97.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.7">68.64</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.3.2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.3.2.1.1">SD1.X</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">49.76</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.3.1">98.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4">68.23</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5">39.65</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.2.6">40.36</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.7">59.26</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.4.3.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.4.3.1.1">SDXL</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.2">51.27</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3">33.45</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.4.1">97.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5">59.14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.3.6">67.97</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.7">61.88</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.5.4.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.5.4.1.1">MJ 1/2</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.2">31.39</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3">17.63</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4">73.14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.5.1">99.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.5.4.6">51.51</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.7">54.55</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.6.5.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.6.5.1.1">MJ 5/6</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.2">91.76</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.3">26.13</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.4">64.75</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.5">62.69</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.6.1">99.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.7">68.92</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.1.1">Avg.</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.6.2">64.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.6.3">40.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.6.4">74.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.6.5">62.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.7.6.6">71.22</td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.7.6.7"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>On each row, patch-level recall of a single-class discriminative model when evaluating all datasets. In bold, performance on the dataset used for training.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The results from the single-class model evaluations, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T2" title="Table 2 ‣ 4.1. Single Class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">2</span></a>, reveal a discrepancy between performance in academic settings and practical applicability. While all models achieved high recall (over 97%) on their respective binary tasks, their performance degraded significantly when applied to other datasets.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Additionally, results indicate that the family of generative models has a weak effect on generalization capabilities. The discriminator trained on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.1">SDXL</span> is below average when tested on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.2">SD1.X</span>. Likewise, the model trained on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.3">MJ 1/2</span> discriminator is not particularly accurate on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.4">MJ 5/6</span>. Although the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.5">MJ 5/6</span> discriminator performs well on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.6">MJ 1/2</span>, it still generalizes better to other classes. The effect of image format is also inconclusive, with the model trained on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.7">MJ 1/2</span> generalizing poorly to <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.8">SD1.X</span>, both containing <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.9">PNG</span> images.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">On the other hand, the time of release of the generative model correlates well with generalized performance. The models trained on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.1">DALLE3</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.2">MJ 5/6</span> strongly generalize to each other. While this may be influenced by shared dataset authorship, both models also generalize well to <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.3">SDXL</span>, demonstrating accurate performance on recent models regardless of their provenance. In fact, the three discriminators trained on models from 2023 are the top three models on average. That is, <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.4">newer models are good at detecting new and old synthetic images</span>. This could be caused by the quality and the lack of artifacts in the images generated by the latest models. In which case it would hold that <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.5">images generated by older models are harder to identify than images from newer models</span>. The bottom row of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T2" title="Table 2 ‣ 4.1. Single Class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">2</span></a>, shows this is the case, with detection accuracy on older datasets performing worse of all (<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.6">SD1.X</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.7">MJ 1/2</span>, from 2022).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Multi-class Models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The performance of binary classifiers can be misleading <cite class="ltx_cite ltx_citemacro_citep">(Ojha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib33" title="">2023</a>)</cite>, learning only one class and predicting the alternative only by omission. To produce more robust detection models, better at generalizing, we must include datasets from different sources in their training set, enriching and detailing the decision boundary available for prediction.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We train two models to evaluate the effect of multi-class learning on generalization capacity. First, we train a binary classifier merging all synthetic data sources from Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.T1" title="Table 1 ‣ 3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a> into a single synthetic class, including 14,323 images. An analogous amount of samples is drawn from the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.1">COCO</span> dataset to compose the authentic class. Then, we train a six-class recognition model using the original splits defined in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.T1" title="Table 1 ‣ 3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a>. To enable a direct comparison, in this experiment, a prediction of a multiclass model will be considered correct for a synthetic image if any synthetic class is predicted, regardless of whether it matches the ground truth class.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T3.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Auth.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.1.1.1.3.1">DALLE3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.1.1.1.4.1">SD1.X</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.1.1.1.5.1">SDXL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.1.1.1.6.1">MJ 1/2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.1.1.1.7.1">MJ 5/6</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.1.1">Single</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">97.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">98.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">97.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.6" style="padding-left:4.0pt;padding-right:4.0pt;">99.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.7" style="padding-left:4.0pt;padding-right:4.0pt;">99.25</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.1.1">Binary</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">94.85</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">99.64</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.4.1">98.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">99.22</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">99.60</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">99.74</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.1.1">Six-class</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.2.1">97.39</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.3.1">99.76</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">97.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.5.1">99.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.6.1">99.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.7.1">99.97</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>On each row, patch level recall for different models. On columns, recall for the authentic class, and each synthetic class in a binary setting. <span class="ltx_text ltx_font_italic" id="S4.T3.5.1">Single</span>: Trained on the dataset of each column (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T2" title="Table 2 ‣ 4.1. Single Class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">2</span></a> diagonal). <span class="ltx_text ltx_font_italic" id="S4.T3.6.2">Binary</span>: Trained with all synthetic datasets merged into one. <span class="ltx_text ltx_font_italic" id="S4.T3.7.3">Six-class</span>: Multi-class recognition model, trained to discriminate all six datasets. In bold best performance per dataset.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T3" title="Table 3 ‣ 4.2. Multi-class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3</span></a> show that, when trained on multiple data sources, both the classifier and the recognition perform equally well, outperforming the models trained on a single dataset (top row). This indicates that a certain amount of useful features are shared among generators, and these are better characterized in conjunction. The results of the six-class model can be attributed to the training in the harder multi-class classification task, which enriches the model’s understanding of the feature space, enhancing its ability to distinguish between classes, even in a binary context.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Image Alteration Methods</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Images undergo various transformations when uploaded to social media and online hosting services. These modifications, primarily aimed at reducing file sizes to optimize storage and transmission costs, can significantly alter the original image characteristics. In addition, malicious actors may intentionally edit or manipulate images to obscure artifacts and patterns that synthetic image detectors rely on for accurate predictions. If image analysis models are not robust enough to withstand these transformations, their utility in real-world scenarios becomes severely limited.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To address this challenge, we conducted a series of experiments focused on understanding and enhancing model robustness. Our approach involves training six-class recognition models, which performed best in the previous experiments, using different data augmentation modalities in isolation, including various forms of blur (<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.1">AdvancedBlur</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.2">GaussianBlur</span>), brightness and gamma alterations (<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.3">RandomBrightnessContrast</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.4">RandomGamma</span>) and compression of images through the JPEG algorithm, from the Albumentations library <cite class="ltx_cite ltx_citemacro_citep">(Buslaev et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib7" title="">2020</a>)</cite>. These augmentation techniques were chosen to simulate the range of transformations that images might undergo in real-world conditions.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">In addition to the baseline with no alterations, five additional models are trained (one for each alteration), and five new test sets are created (also one per alteration). With these, cross-testing is conducted to explore generalization across alteration methods. The results, how well a model trained with one alteration detects images subjected to another alteration, are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T4" title="Table 4 ‣ 4.3. Image Alteration Methods ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a>. Multi-class macro average recall is reported, where a confusion between synthetic classes is considered an error, unlike in the previous section where we used binary metrics for comparison.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T4.1.1.2" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.3" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.1">None</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.4" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.1">Bright</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1" style="padding-left:3.7pt;padding-right:3.7pt;"><math alttext="\gamma" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mi id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">italic_γ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.5" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.1">JPEG</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.6" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.1">ABlur</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T4.1.1.7" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.1">GBlur</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.8" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.2.3.1.1" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.1.1.1">None</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.2" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.1.2.1">90.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.3" style="padding-left:3.7pt;padding-right:3.7pt;">86.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.4" style="padding-left:3.7pt;padding-right:3.7pt;">90.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.5" style="padding-left:3.7pt;padding-right:3.7pt;">90.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.6" style="padding-left:3.7pt;padding-right:3.7pt;">81.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.7" style="padding-left:3.7pt;padding-right:3.7pt;">54.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.8" style="padding-left:3.7pt;padding-right:3.7pt;">82.44</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.2.4.2.1" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.2.1.1">Bright</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2.2" style="padding-left:3.7pt;padding-right:3.7pt;">91.28</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2.3" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.2.3.1">89.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2.4" style="padding-left:3.7pt;padding-right:3.7pt;">91.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2.5" style="padding-left:3.7pt;padding-right:3.7pt;">90.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2.6" style="padding-left:3.7pt;padding-right:3.7pt;">84.61</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.4.2.7" style="padding-left:3.7pt;padding-right:3.7pt;">63.55</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2.8" style="padding-left:3.7pt;padding-right:3.7pt;">85.06</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.2.2.1" style="padding-left:3.7pt;padding-right:3.7pt;"><math alttext="\gamma" class="ltx_Math" display="inline" id="S4.T4.2.2.1.m1.1"><semantics id="S4.T4.2.2.1.m1.1a"><mi id="S4.T4.2.2.1.m1.1.1" xref="S4.T4.2.2.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.1.m1.1b"><ci id="S4.T4.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.1.m1.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.1.m1.1d">italic_γ</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2" style="padding-left:3.7pt;padding-right:3.7pt;">91.52</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.3" style="padding-left:3.7pt;padding-right:3.7pt;">87.51</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.4" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.4.1">91.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.5" style="padding-left:3.7pt;padding-right:3.7pt;">90.02</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.6" style="padding-left:3.7pt;padding-right:3.7pt;">85.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.2.7" style="padding-left:3.7pt;padding-right:3.7pt;">65.22</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.8" style="padding-left:3.7pt;padding-right:3.7pt;">85.19</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.2.5.3.1" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.5.3.1.1">JPEG</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3.2" style="padding-left:3.7pt;padding-right:3.7pt;">87.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3.3" style="padding-left:3.7pt;padding-right:3.7pt;">83.15</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3.4" style="padding-left:3.7pt;padding-right:3.7pt;">87.79</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3.5" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.5.3.5.1">86.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3.6" style="padding-left:3.7pt;padding-right:3.7pt;">78.42</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.5.3.7" style="padding-left:3.7pt;padding-right:3.7pt;">55.29</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3.8" style="padding-left:3.7pt;padding-right:3.7pt;">79.78</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.2.6.4.1" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.6.4.1.1">ABlur</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4.2" style="padding-left:3.7pt;padding-right:3.7pt;">90.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4.3" style="padding-left:3.7pt;padding-right:3.7pt;">86.23</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4.4" style="padding-left:3.7pt;padding-right:3.7pt;">90.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4.5" style="padding-left:3.7pt;padding-right:3.7pt;">88.15</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4.6" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.6.4.6.1">88.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.6.4.7" style="padding-left:3.7pt;padding-right:3.7pt;">81.54</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4.8" style="padding-left:3.7pt;padding-right:3.7pt;">87.37</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.2.7.5.1" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.7.5.1.1">GBlur</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.5.2" style="padding-left:3.7pt;padding-right:3.7pt;">88.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.5.3" style="padding-left:3.7pt;padding-right:3.7pt;">84.02</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.5.4" style="padding-left:3.7pt;padding-right:3.7pt;">88.65</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.5.5" style="padding-left:3.7pt;padding-right:3.7pt;">87.37</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.5.6" style="padding-left:3.7pt;padding-right:3.7pt;">86.78</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.7.5.7" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.7.5.7.1">81.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.5.8" style="padding-left:3.7pt;padding-right:3.7pt;">86.27</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.2.8.6.1" style="padding-left:3.7pt;padding-right:3.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.8.6.1.1">Avg.</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6.2" style="padding-left:3.7pt;padding-right:3.7pt;">90.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6.3" style="padding-left:3.7pt;padding-right:3.7pt;">86.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6.4" style="padding-left:3.7pt;padding-right:3.7pt;">89.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6.5" style="padding-left:3.7pt;padding-right:3.7pt;">88.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.8.6.6" style="padding-left:3.7pt;padding-right:3.7pt;">84.16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.8.6.7" style="padding-left:3.7pt;padding-right:3.7pt;">67.04</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.8.6.8" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>On each row, patch-level accuracy of a six-class recognition model when trained on one alteration method and evaluated on all. In bold, performance on the alteration used for training. Last column shows the average performance models across all transformations. Bottom row shows the average performance of all models for each transformation.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">The effectiveness of targeted data augmentation can be seen by comparing the first row of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T4" title="Table 4 ‣ 4.3. Image Alteration Methods ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a> with the values in bold. Specialized models demonstrate a significant performance boost over the baseline across most alterations, with the exception of JPEG compression. This anomaly may be attributed to the inclusion of JPEG data in the baseline training set. Despite this apparent performance drop, training with compressed images remains relevant to prevent the recognition of JPEG-introduced patterns as class-specific artifacts, as both authentic and synthetic images may undergo this transformation in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">Among the various transformations, blur —particularly GaussianBlur —has the most pronounced impact on detector performance. Models not specifically trained on blur transformations experience a substantial drop in recall, up to 30%, when faced with blurred images. In contrast, models trained on blur transformations exhibit remarkable consistency and competitive performance across all experiments. Based on the average performance metrics presented in the last column of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T4" title="Table 4 ‣ 4.3. Image Alteration Methods ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a>, these blur-trained models emerge as the overall best performers among the alteration-based models.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span><span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS1.1.1">SuSy</span> - Our robust model</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">In the following section (§ <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5" title="5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a>), generalization is explored by evaluating different data sources. To scale experimentation, we fix a model based on the insights from this section, which we call <span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS1.p1.1.1">SuSy</span>. In detail, we train a six-class classifier, using the original splits defined in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.T1" title="Table 1 ‣ 3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a> and incorporating the alteration methods explored in the previous section to enhance its resilience against image manipulations. Each transformation is applied with a 20% probability, besides the horizontal flip which remains at 50%, allowing the model to see unaltered images and images that have suffered one or multiple transformations. We train this model for a maximum of 20 epochs with early stopping monitoring validation loss with patience 2, as in previous experiments.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">So far generalization has been explored from the perspective of time, task, and alterations. All of them, in a controlled setting, using a limited amount of data sources. But this is unrealistic in practice. AI image generators have become a wildly popular technology, with large communities of users re-training and sharing models. Meanwhile, new and updated models keep coming out (<span class="ltx_text ltx_font_italic" id="S5.p1.1.1">e.g., </span>FLUX.1).</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In this situation, the biggest challenge for synthetic image detectors is the adaptation to a changing and uncontrolled generative environment. To test generalization in this context, we explore the role of data source, using the datasets described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS3" title="3.3. Evaluation Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.3</span></a>. First, this section considers the performance of <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.1">SuSy</span> (described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4" title="4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">4</span></a>), using the central patch of images. Then, the same experiments are conducted by making predictions at image level, using the patching strategy defined in § <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS1" title="3.1. Architecture ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Additionally, to confirm the validity and difficulty of the <span class="ltx_text ltx_font_typewriter" id="S5.p3.1.1">In-the</span>-<span class="ltx_text ltx_font_typewriter" id="S5.p3.1.2">wild</span> dataset, we conduct a small human experiment. We ask 10 volunteers aged 22-30 who have social media accounts and are potential targets of AI-generated deception, to discriminate between both <span class="ltx_text ltx_font_typewriter" id="S5.p3.1.3">In-the-wild</span> versions (authentic and synthetic). To ensure unbiased results, images were presented in random order and the evaluators were not informed about the distribution of the data. Participants took an average of 15 minutes to label them, with no time constraints imposed. Results are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T6" title="Table 6 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a> and used as a baseline.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Generalization to source</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Let us explore the performance of <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.1">SuSy</span> when evaluating it on the datasets described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS3" title="3.3. Evaluation Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.3</span></a>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results, sorted by category. For the authentic datasets, reported at the top part of the table, <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.2">SuSy</span> generalizes well to the <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.3">Flickr30k</span> dataset and decently to <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.4">Google Landmarks v2</span>, but suffers a large drop in performance for <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.5">In-the-wild</span> images. This could be caused by changes in the scale of images, differences in the domain of information represented in the pictures and a higher amount of image postprocessing, and could potentially be mitigated by using a richer authentic class that combines images from different sources in the training dataset.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The second set of data sources considered include datasets produced using models that were also used during the training of <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.1">SuSy</span>, listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.T1" title="Table 1 ‣ 3.2. Train Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">1</span></a>. Even though the underlying generative models are the same, differences in the generation process (<span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">e.g., </span>prompts, generator hyperparameters, <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.3">etc. </span>) can induce significant biases. In this case, generalization holds significantly well, with all datasets reaching a recall between 73% and 89%.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.2.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.1.3"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.T5.1.1.1.3.1">SuSy<span class="ltx_text ltx_font_serif" id="S5.T5.1.1.1.3.1.1"> (Patch)</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.T5.1.1.1.4.1">SuSy<span class="ltx_text ltx_font_serif" id="S5.T5.1.1.1.4.1.1"> (Image)</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T5.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.2.1.1">Authentic Data Sources</span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.3.3.1.1">Flickr30k</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.3.3.2">None</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.3">90.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.4">93.30</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.4.4.1">GLDv2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.4.4.2">None</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.3">64.54</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.4">75.17</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.5.5.1.1">In-the-wild</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.5.5.2">None</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.3">33.06</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.4">33.88</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T5.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.1.1">Synthetic Data from Models In Train</span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.7.7.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.7.7.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.7.7.2">SD 1.3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.7.7.3">87.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.7.7.4">91.70</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.8.8.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.8.8.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.8.8.2">SD 1.4</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.8.8.3">87.10</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.8.8.4">91.20</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.9.9.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.9.9.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.9.9.2">MJ V5</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.3">73.10</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.4">78.10</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.10.10.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.10.10.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.10.10.2">SD XL</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.10.10.3">79.50</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.10.10.4">84.40</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.11.11.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.11.11.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.11.11.2">DALLE-3</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.11.11.3">88.60</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.11.11.4">90.70</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T5.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.12.12.1.1">Synthetic Data From Models Not in Train</span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.13.13.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.13.13.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.13.13.2">GLIDE</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.13.13.3">53.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.13.13.4">53.70</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.14.14.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.14.14.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.14.14.2">SD 2</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.14.14.3">68.40</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.14.14.4">70.10</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.15.15.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.15.15.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.15.15.2">DALLE-2</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.15.15.3">20.70</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.15.15.4">19.10</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.16.16.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.16.16.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.16.16.2">Firefly</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.16.16.3">40.90</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.16.16.4">52.00</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.17.17.1">Authors</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.17.17.2">SD 3</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.17.17.3">93.23</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.17.17.4">94.79</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.18.18.1">Authors</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.18.18.2">FLUX.1-dev</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.18.18.3">96.46</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.18.18.4">96.85</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.19.19.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.1.19.19.1.1">In-the-wild</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.19.19.2">Unknown</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.19.19.3">89.90</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.19.19.4">92.93</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>Recall of <span class="ltx_text ltx_font_typewriter" id="S5.T5.3.1">SuSy</span> on authentic images (top), from unseen datasets. In the middle, recall on synthetic datasets generated by generative models represented in the train data, but produced by alternative sources. Bottom, synthetic image datasets generated by models not seen before. Performance at patch and image level using majority voting (threshold of three out of five).</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">In the third set of experiments, which involves datasets obtained from models unseen during training, performance varies significantly, with recalls ranging from as high as 96% to as low as 20%. The impact of model family on performance is also inconsistent, as <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.1">SuSy</span> exhibits decent to strong generalization for Stable Diffusion models, with SD2 achieving a recall of 68.40% and SD3 reaching 93.23%. However, it only detects 20.70% of images from DALLE-2, despite being trained on DALLE-3.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Regardless of its blind spots, <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.1">SuSy</span> performs significantly well in the most realistic setting, the one using <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.2">In-the-wild</span> images collected from unknown sources and manually selected based on their quality, aligning with the performance on the latest and most realistic models (SD3 and Flux). A comparison of performance on this task with human evaluators is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T6" title="Table 6 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a>. Results indicate synthetic image detectors trained on varied sets of data, like <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.3">SuSy</span>, are already at the level of expert humans, making them useful tools in practice (as long as false positive rates are contained).</p>
</div>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T6.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.2.1">In the wild (Auth.)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.1.1.3" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.3.1">In the wild (Synth.)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.1.2.1.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.T6.1.2.1.1.1">SuSy<span class="ltx_text ltx_font_serif" id="S5.T6.1.2.1.1.1.1"> (Patch)</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">33.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">89.90</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.1.3.2.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.T6.1.3.2.1.1">SuSy<span class="ltx_text ltx_font_serif" id="S5.T6.1.3.2.1.1.1"> (Image)</span></span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">65.29</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">72.73</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.1.4.3.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.4.3.1.1">Best Human</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">75.21</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">83.84</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.1.5.4.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.5.4.1.1">Avg. Human</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">72.82</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">69.14</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6. </span>Recall of the <span class="ltx_text ltx_font_typewriter" id="S5.T6.5.1">In-the-wild</span> dataset by <span class="ltx_text ltx_font_typewriter" id="S5.T6.6.2">SuSy</span> and human evaluators. For <span class="ltx_text ltx_font_typewriter" id="S5.T6.7.3">SuSy</span>, performance for central patch, and at image level when requiring five positive detections on the five patches with maximum contrast.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Generalization to image</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">While our initial experiments focused solely on the central patch of each sample, a prediction at image level is likely to be of interest in real-world scenarios. In this section, we explore the transition from patch-level to image-level predictions, and how this affects generalization. To achieve this, we employ the patch selection method described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS1" title="3.1. Architecture ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.1</span></a>, selecting five patches and using a fixed threshold (minimum number of synthetic patches) to classify an entire image as synthetic.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">By default, we set the threshold at three out of five patches, which equates to majority voting. This approach ensures a balanced performance between authentic and synthetic classes. In the case of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.1">GLIDE</span>, due to image size constraints, we adjust the requirement to two out of three patches for a synthetic prediction. The results of this image-level analysis are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a>, showing consistent improvements across all datasets. We observe performance increases of up to 11% of recall, with an average improvement of 3.4% across datasets.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Optimizing the threshold for a specific domain or dataset can be done using a small subset of data, adjusting the detector’s sensitivity to the frequency of artifacts found in a specific domain. For instance, in the case of the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.1">In-the-wild</span> dataset, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a> shows a high recall in the synthetic class (89.9% and 92.9%) but also a large number of false positives in the authentic class (33.0% and 33.8%). To address this, the threshold can be increased (<span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.2">e.g., </span>requiring all five patches to indicate a synthetic prediction for the entire image to be classified as such). As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T6" title="Table 6 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a>, this setup significantly improves the model’s balance between authentic and synthetic image classification, increasing authentic recall by 31% while decreasing synthetic recall by 20% points. Notably, this adjustment results in a performance comparable to that of the average human evaluator.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Generalization of state-of-the-art models</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we study the performance of state-of-the-art models on the evaluation datasets listed in § <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S3.SS3" title="3.3. Evaluation Datasets ‣ 3. Methods ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3.3</span></a>. We analyzed ten different models available in SIDBench <cite class="ltx_cite ltx_citemacro_citep">(Schinas and Papadopoulos, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib35" title="">2024</a>)</cite>, focusing on the six that demonstrated superior performance across our tests. The results for the four underperforming models (CNNDetect <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib40" title="">2020</a>)</cite>, FreqDetect <cite class="ltx_cite ltx_citemacro_citep">(Frank et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib17" title="">2020</a>)</cite>, Fusing <cite class="ltx_cite ltx_citemacro_citep">(Ju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib22" title="">2022</a>)</cite> and UnivFD <cite class="ltx_cite ltx_citemacro_citep">(Ojha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib33" title="">2023</a>)</cite>) are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A1" title="Appendix A Additional Evaluation Metrics ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">A</span></a> for completeness.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.T7" title="Table 7 ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">7</span></a> showcases the performance of the best models, LGrad <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib37" title="">2023</a>)</cite>, GramNet <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib30" title="">2020</a>)</cite>, Rine <cite class="ltx_cite ltx_citemacro_citep">(Koutlis and Papadopoulos, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib24" title="">2024</a>)</cite>, DIMD <cite class="ltx_cite ltx_citemacro_citep">(Koutlis and Papadopoulos, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib24" title="">2024</a>)</cite>, DeFake <cite class="ltx_cite ltx_citemacro_citep">(Corvi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib10" title="">2023</a>)</cite> and Dire <cite class="ltx_cite ltx_citemacro_citep">(Sha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib36" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S6.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T7.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.1.1.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.1.1.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.3.1">Year</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.1.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.4.1">LGrad</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.1.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.5.1">GramNet</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.1.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.6.1">Rine</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.1.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.7.1">DIMD</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.1.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.8.1">DeFake</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.1.1.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.9.1">Dire</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.1.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S6.T7.1.1.1.10.1">SuSy</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="10" id="S6.T7.1.2.2.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.2.2.1.1">Authentic Data Sources</span></th>
</tr>
<tr class="ltx_tr" id="S6.T7.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.3.3.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.3.3.1.1">Flickr30k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.3.3.2" style="padding-left:4.5pt;padding-right:4.5pt;">None</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T7.1.3.3.3" style="padding-left:4.5pt;padding-right:4.5pt;">2014</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.3.4" style="padding-left:4.5pt;padding-right:4.5pt;">88.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.3.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.3.3.5.1">100.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.3.6" style="padding-left:4.5pt;padding-right:4.5pt;">99.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.3.7" style="padding-left:4.5pt;padding-right:4.5pt;">99.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.3.8" style="padding-left:4.5pt;padding-right:4.5pt;">60.76</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.1.3.3.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.3.3.9.1">33.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.3.10" style="padding-left:4.5pt;padding-right:4.5pt;">90.53</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.4.4.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.4.4.1.1">COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.4.4.2" style="padding-left:4.5pt;padding-right:4.5pt;">None</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.4.4.3" style="padding-left:4.5pt;padding-right:4.5pt;">2017</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.4.4" style="padding-left:4.5pt;padding-right:4.5pt;">92.46</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.4.5" style="padding-left:4.5pt;padding-right:4.5pt;">67.67</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.4.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.4.4.6.1">100.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.4.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.4.4.7.1">100.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.4.8" style="padding-left:4.5pt;padding-right:4.5pt;">76.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.4.4.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.4.4.9.1">32.58</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.4.10" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.5.5.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.5.5.1.1">Google Landmarks v2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.5.5.2" style="padding-left:4.5pt;padding-right:4.5pt;">None</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.5.5.3" style="padding-left:4.5pt;padding-right:4.5pt;">2020</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.5.4" style="padding-left:4.5pt;padding-right:4.5pt;">68.84</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.5.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.5.5.5.1">100.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.5.6" style="padding-left:4.5pt;padding-right:4.5pt;">77.42</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.5.7" style="padding-left:4.5pt;padding-right:4.5pt;">96.54</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.5.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.5.5.8.1">38.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.5.5.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.5.5.9.1">39.46</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.5.10" style="padding-left:4.5pt;padding-right:4.5pt;">64.54</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.6.6.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.6.6.1.1">In-the-wild</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.6.6.2" style="padding-left:4.5pt;padding-right:4.5pt;">None</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.6.6.3" style="padding-left:4.5pt;padding-right:4.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.6.6.4" style="padding-left:4.5pt;padding-right:4.5pt;">85.95</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.6.6.5" style="padding-left:4.5pt;padding-right:4.5pt;">92.56</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.6.6.6" style="padding-left:4.5pt;padding-right:4.5pt;">95.87</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.6.6.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.6.6.7.1">96.69</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.6.6.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.6.6.8.1">28.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.6.6.9" style="padding-left:4.5pt;padding-right:4.5pt;">50.41</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.6.6.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.6.6.10.1">33.06</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="3" id="S6.T7.1.7.7.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.7.7.1.1">Average</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.7.7.2" style="padding-left:4.5pt;padding-right:4.5pt;">83.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.7.7.3" style="padding-left:4.5pt;padding-right:4.5pt;">90.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.7.7.4" style="padding-left:4.5pt;padding-right:4.5pt;">93.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.7.7.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.7.7.5.1">98.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.7.7.6" style="padding-left:4.5pt;padding-right:4.5pt;">51.14</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.1.7.7.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.7.7.7.1">39.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.7.7.8" style="padding-left:4.5pt;padding-right:4.5pt;">62.71</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" colspan="3" id="S6.T7.1.8.8.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.1.1">Average (Resize)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.8.2" style="padding-left:4.5pt;padding-right:4.5pt;">84.16 <span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.2.1" style="font-size:80%;color:#008000;"> +0.21</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.8.3" style="padding-left:4.5pt;padding-right:4.5pt;">55.69<span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.3.1" style="font-size:80%;color:#990000;"> -34.87</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.8.4" style="padding-left:4.5pt;padding-right:4.5pt;">92.29<span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.4.1" style="font-size:80%;color:#990000;"> -0.93</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.8.5" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.8.8.5.1">99.78</span><span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.5.2" style="font-size:80%;color:#008000;"> +1.51</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.8.6" style="padding-left:4.5pt;padding-right:4.5pt;">66.34<span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.6.1" style="font-size:80%;color:#008000;"> +15.21</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.8.8.7" style="padding-left:4.5pt;padding-right:4.5pt;">64.87<span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.7.1" style="font-size:80%;color:#008000;"> +25.88</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.8.8" style="padding-left:4.5pt;padding-right:4.5pt;">92.84<span class="ltx_text ltx_font_bold" id="S6.T7.1.8.8.8.1" style="font-size:80%;color:#008000;"> +30.13</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="10" id="S6.T7.1.9.9.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.9.9.1.1">Synthetic Data Sources</span></th>
</tr>
<tr class="ltx_tr" id="S6.T7.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.10.10.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.10.10.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.10.10.2" style="padding-left:4.5pt;padding-right:4.5pt;">GLIDE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T7.1.10.10.3" style="padding-left:4.5pt;padding-right:4.5pt;">2021</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.10.4" style="padding-left:4.5pt;padding-right:4.5pt;">68.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.10.5" style="padding-left:4.5pt;padding-right:4.5pt;">80.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.10.6" style="padding-left:4.5pt;padding-right:4.5pt;">83.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.10.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.10.10.7.1">6.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.10.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.10.8.1">85.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.1.10.10.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.10.10.9.1">27.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.10.10" style="padding-left:4.5pt;padding-right:4.5pt;">53.50</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.11.11.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.11.11.1.1">mj-tti</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.11.11.2" style="padding-left:4.5pt;padding-right:4.5pt;">MJ V1/2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.11.11.3" style="padding-left:4.5pt;padding-right:4.5pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.11.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.11.4.1">15.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.11.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.11.5.1">35.98</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.11.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.11.6.1">14.57</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.11.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.11.7.1">2.87</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.11.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.11.11.8.1">64.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.11.11.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.11.9.1">43.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.11.10" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.12.12.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.12.12.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.12.12.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD 1.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.12.12.3" style="padding-left:4.5pt;padding-right:4.5pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.12.12.4" style="padding-left:4.5pt;padding-right:4.5pt;">86.80</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.12.12.5" style="padding-left:4.5pt;padding-right:4.5pt;">92.90</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.12.12.6" style="padding-left:4.5pt;padding-right:4.5pt;">99.90</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.12.12.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.12.12.7.1">100.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.12.12.8" style="padding-left:4.5pt;padding-right:4.5pt;">52.50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.12.12.9" style="padding-left:4.5pt;padding-right:4.5pt;">82.70</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.12.12.10" style="padding-left:4.5pt;padding-right:4.5pt;">87.00</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.13.13.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.13.13.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.13.13.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD 1.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.13.13.3" style="padding-left:4.5pt;padding-right:4.5pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.13.13.4" style="padding-left:4.5pt;padding-right:4.5pt;">89.20</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.13.13.5" style="padding-left:4.5pt;padding-right:4.5pt;">93.60</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.13.13.6" style="padding-left:4.5pt;padding-right:4.5pt;">99.60</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.13.13.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.13.13.7.1">100.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.13.13.8" style="padding-left:4.5pt;padding-right:4.5pt;">51.90</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.13.13.9" style="padding-left:4.5pt;padding-right:4.5pt;">82.60</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.13.13.10" style="padding-left:4.5pt;padding-right:4.5pt;">87.10</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.14.14.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.14.14.1.1">diffusiondb</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.14.14.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD 1.X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.14.14.3" style="padding-left:4.5pt;padding-right:4.5pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.14.14.4" style="padding-left:4.5pt;padding-right:4.5pt;">60.13</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.14.14.5" style="padding-left:4.5pt;padding-right:4.5pt;">94.98</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.14.14.6" style="padding-left:4.5pt;padding-right:4.5pt;">96.06</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.14.14.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.14.14.7.1">99.92</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.14.14.8" style="padding-left:4.5pt;padding-right:4.5pt;">68.40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.14.14.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.14.14.9.1">41.33</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.14.14.10" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.15.15.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.15.15.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.15.15.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD 2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.15.15.3" style="padding-left:4.5pt;padding-right:4.5pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.15.15.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.15.15.4.1">29.80</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.15.15.5" style="padding-left:4.5pt;padding-right:4.5pt;">52.50</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.15.15.6" style="padding-left:4.5pt;padding-right:4.5pt;">85.80</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.15.15.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.15.15.7.1">97.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.15.15.8" style="padding-left:4.5pt;padding-right:4.5pt;">56.40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.15.15.9" style="padding-left:4.5pt;padding-right:4.5pt;">89.10</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.15.15.10" style="padding-left:4.5pt;padding-right:4.5pt;">68.40</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.16.16.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.16.16.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.16.16.2" style="padding-left:4.5pt;padding-right:4.5pt;">DALLE-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.16.16.3" style="padding-left:4.5pt;padding-right:4.5pt;">2022</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.16.16.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.16.16.4.1">32.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.16.16.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.16.16.5.1">96.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.16.16.6" style="padding-left:4.5pt;padding-right:4.5pt;">70.80</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.16.16.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.16.16.7.1">0.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.16.16.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.16.16.8.1">22.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.16.16.9" style="padding-left:4.5pt;padding-right:4.5pt;">54.90</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.16.16.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.16.16.10.1">20.70</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.17.17.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.17.17.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.17.17.2" style="padding-left:4.5pt;padding-right:4.5pt;">MJ V5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.17.17.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.17.17.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.17.17.4.1">30.60</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.17.17.5" style="padding-left:4.5pt;padding-right:4.5pt;">51.70</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.17.17.6" style="padding-left:4.5pt;padding-right:4.5pt;">87.00</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.17.17.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.17.17.7.1">98.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.17.17.8" style="padding-left:4.5pt;padding-right:4.5pt;">56.30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.17.17.9" style="padding-left:4.5pt;padding-right:4.5pt;">61.10</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.17.17.10" style="padding-left:4.5pt;padding-right:4.5pt;">73.10</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.18.18.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.18.18.1.1">mj-images</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.18.18.2" style="padding-left:4.5pt;padding-right:4.5pt;">MJ V5/6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.18.18.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.18.18.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.18.18.4.1">8.91</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.18.18.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.18.18.5.1">0.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.18.18.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.18.18.6.1">31.28</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.18.18.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.18.18.7.1">90.11</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.18.18.8" style="padding-left:4.5pt;padding-right:4.5pt;">82.01</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.18.18.9" style="padding-left:4.5pt;padding-right:4.5pt;">59.81</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.18.18.10" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.19.19.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.19.19.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.19.19.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD XL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.19.19.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.19.19.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.19.19.4.1">37.20</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.19.19.5" style="padding-left:4.5pt;padding-right:4.5pt;">79.80</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.19.19.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.19.19.6.1">97.60</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.19.19.7" style="padding-left:4.5pt;padding-right:4.5pt;">94.40</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.19.19.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.19.19.8.1">36.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.19.19.9" style="padding-left:4.5pt;padding-right:4.5pt;">73.20</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.19.19.10" style="padding-left:4.5pt;padding-right:4.5pt;">79.50</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.20.20.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.20.20.1.1">SDXL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.20.20.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD XL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.20.20.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.20.20.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.20.20.4.1">72.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.20.20.5" style="padding-left:4.5pt;padding-right:4.5pt;">62.07</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.20.20.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.20.20.6.1">17.83</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.20.20.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.20.20.7.1">2.35</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.20.20.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.20.20.8.1">23.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.20.20.9" style="padding-left:4.5pt;padding-right:4.5pt;">65.64</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.20.20.10" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.21.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.21.21.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.21.21.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.21.21.2" style="padding-left:4.5pt;padding-right:4.5pt;">Firefly</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.21.21.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.21.21.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.21.21.4.1">14.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.21.21.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.21.21.5.1">22.90</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.21.21.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.21.21.6.1">43.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.21.21.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.21.21.7.1">18.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.21.21.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.21.21.8.1">29.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.21.21.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.21.21.9.1">66.50</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.21.21.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.21.21.10.1">40.90</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.22.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.22.22.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.22.22.1.1">Synthbuster</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.22.22.2" style="padding-left:4.5pt;padding-right:4.5pt;">DALLE-3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.22.22.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.22.22.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.22.22.4.1">5.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.22.22.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.22.22.5.1">9.50</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.22.22.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.22.22.6.1">2.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.22.22.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.22.22.7.1">0.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.22.22.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.22.22.8.1">96.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.22.22.9" style="padding-left:4.5pt;padding-right:4.5pt;">86.40</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.22.22.10" style="padding-left:4.5pt;padding-right:4.5pt;">88.60</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.23.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.23.23.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.23.23.1.1">dalle3-images</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.23.23.2" style="padding-left:4.5pt;padding-right:4.5pt;">DALLE-3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.23.23.3" style="padding-left:4.5pt;padding-right:4.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.23.23.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.23.23.4.1">13.33</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.23.23.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.23.23.5.1">0.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.23.23.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.23.23.6.1">28.79</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.23.23.7" style="padding-left:4.5pt;padding-right:4.5pt;">61.82</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.23.23.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.23.23.8.1">89.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.23.23.9" style="padding-left:4.5pt;padding-right:4.5pt;">58.48</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.23.23.10" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.24.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.24.24.1" style="padding-left:4.5pt;padding-right:4.5pt;">Authors</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.24.24.2" style="padding-left:4.5pt;padding-right:4.5pt;">SD3-Med.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.24.24.3" style="padding-left:4.5pt;padding-right:4.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.24.24.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.24.24.4.1">35.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.24.24.5" style="padding-left:4.5pt;padding-right:4.5pt;">82.81</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.24.24.6" style="padding-left:4.5pt;padding-right:4.5pt;">85.05</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.24.24.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.24.24.7.1">99.24</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.24.24.8" style="padding-left:4.5pt;padding-right:4.5pt;">94.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.24.24.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.24.24.9.1">33.57</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.24.24.10" style="padding-left:4.5pt;padding-right:4.5pt;">93.23</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.25.25">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.25.25.1" style="padding-left:4.5pt;padding-right:4.5pt;">Authors</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.25.25.2" style="padding-left:4.5pt;padding-right:4.5pt;">FLUX.1-dev</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.25.25.3" style="padding-left:4.5pt;padding-right:4.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.25.25.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.25.25.4.1">27.15</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.25.25.5" style="padding-left:4.5pt;padding-right:4.5pt;">91.08</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.25.25.6" style="padding-left:4.5pt;padding-right:4.5pt;">54.72</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.25.25.7" style="padding-left:4.5pt;padding-right:4.5pt;">62.90</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.25.25.8" style="padding-left:4.5pt;padding-right:4.5pt;">88.63</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.25.25.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.25.25.9.1">37.77</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.25.25.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.25.25.10.1">96.46</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.26.26">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.26.26.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_typewriter" id="S6.T7.1.26.26.1.1">In-the-wild</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T7.1.26.26.2" style="padding-left:4.5pt;padding-right:4.5pt;">Unknown</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.1.26.26.3" style="padding-left:4.5pt;padding-right:4.5pt;">–</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.26.26.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.26.26.4.1">16.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.26.26.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.26.26.5.1">19.19</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.26.26.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.26.26.6.1">16.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.26.26.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.26.26.7.1">47.47</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.26.26.8" style="padding-left:4.5pt;padding-right:4.5pt;">87.88</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.26.26.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.26.26.9.1">48.48</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.26.26.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.26.26.10.1">89.90</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.27.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="3" id="S6.T7.1.27.27.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.27.27.1.1">Average</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.27.27.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.27.27.2.1">35.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.27.27.3" style="padding-left:4.5pt;padding-right:4.5pt;">55.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.27.27.4" style="padding-left:4.5pt;padding-right:4.5pt;">63.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.27.27.5" style="padding-left:4.5pt;padding-right:4.5pt;">63.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.27.27.6" style="padding-left:4.5pt;padding-right:4.5pt;">67.07</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.1.27.27.7" style="padding-left:4.5pt;padding-right:4.5pt;">57.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.27.27.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.27.27.8.1">73.20</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.28.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" colspan="3" id="S6.T7.1.28.28.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.1.1">Average (Resize)</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.1.28.28.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.2.1">33.80<span class="ltx_text" id="S6.T7.1.28.28.2.1.1" style="font-size:80%;color:#990000;"> -1.46</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.1.28.28.3" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.28.28.3.1">87.73</span><span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.3.2" style="font-size:80%;color:#008000;"> +31.62</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.1.28.28.4" style="padding-left:4.5pt;padding-right:4.5pt;">61.68<span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.4.1" style="font-size:80%;color:#990000;"> -1.75</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.1.28.28.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.5.1">0.95<span class="ltx_text" id="S6.T7.1.28.28.5.1.1" style="font-size:80%;color:#990000;"> -62.35</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.1.28.28.6" style="padding-left:4.5pt;padding-right:4.5pt;">82.73<span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.6.1" style="font-size:80%;color:#008000;"> +15.67</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T7.1.28.28.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.7.1">33.76<span class="ltx_text" id="S6.T7.1.28.28.7.1.1" style="font-size:80%;color:#990000;"> -23.97</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.1.28.28.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.28.28.8.1">36.39<span class="ltx_text" id="S6.T7.1.28.28.8.1.1" style="font-size:80%;color:#990000;"> -36.81</span></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7. </span>Center-patch recall of state-of-the-art detector models across evaluation datasets. On top, performance for authentic images. At bottom, performance on synthetic datasets. Recalls below 50%, akin to random chance, in bold. Best recall for each dataset is underlined. <span class="ltx_text ltx_font_italic" id="S6.T7.5.1">Average</span> shows aggregated center-patch performance across datasets of the same class (auth. or synth.). <span class="ltx_text ltx_font_italic" id="S6.T7.6.2">Average (Resize)</span> shows the same with images resized instead of cropped (see §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.SS2" title="6.2. Scale Generalization ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6.2</span></a>), together with the increment or decrement <span class="ltx_text ltx_font_italic" id="S6.T7.7.3">w.r.t. </span>the patched approach.</figcaption>
</figure>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The majority of the models examined leverage pre-trained neural networks as feature extractors, adapting them for SID. LGrad, GramNet, and DIMD all utilize CNNs, each with a unique emphasis on different image characteristics. Rine and DeFAKE shift towards more recent architectures, employing transformer-based models. In contrast, Dire does not rely on direct feature extraction, instead using the diffusion concept of image reconstruction.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">LGrad</span> trains a ResNet-50 classifier using image gradients from a pre-trained CNN, with images generated by ProGAN and authentic images from Celeba-HQ <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib21" title="">2018</a>)</cite>. Similarly, <span class="ltx_text ltx_font_bold" id="S6.p3.1.2">GramNet</span> employs global image texture representations extracted at different levels from a ResNet-18, trained on StyleGAN-generated images and authentic Celeba-HQ images. <span class="ltx_text ltx_font_bold" id="S6.p3.1.3">Rine</span> leverages image representations extracted by intermediate blocks of CLIP, with an additional trainable module. We use the checkpoint trained with Latent Diffusion Model <cite class="ltx_cite ltx_citemacro_citep">(Corvi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib10" title="">2023</a>)</cite> and ProGAN images. <span class="ltx_text ltx_font_bold" id="S6.p3.1.4">DIMD</span> trains a ResNet-50 avoiding downsampling step, to preserve high-frequency fingerprints. We take the checkpoint trained on Latent Diffusion images. Training authentic images are taken from MSCOCO and LSUN. In <span class="ltx_text ltx_font_bold" id="S6.p3.1.5">DeFAKE</span>, text and image encoders from a Visual-Language Model model are finetuned to detect synthetic images, using Latent Diffusion data. <span class="ltx_text ltx_font_bold" id="S6.p3.1.6">Dire</span> uses the error between an input image and its reconstruction by a pre-trained diffusion model for identification. A ResNet50 is trained as a classifier on their DiffusionForensics dataset. Synthetic images generated with, ADM <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal and Nichol, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib13" title="">2021</a>)</cite>, iDDPM <cite class="ltx_cite ltx_citemacro_citep">(Nichol and Dhariwal, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib32" title="">2021</a>)</cite> and PNDM <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib29" title="">2022</a>)</cite>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Benchmarking</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Overall, results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.T7" title="Table 7 ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">7</span></a> indicate the lack of universal detectors, with the performance of all methods failing at random classifier level in six or more datasets. Performance inconsistency is generalized, with all models being the best and worst detectors for at least one dataset.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">In addition, some models fail to characterize authentic images. DeFake and Dire achieve particularly low recalls on all real-world datasets tested. This compromises their utility, as it increases the number of false positive detections (misclassifying authentic images as synthetic). Meanwhile, GramNet, Rine and DIMD demonstrate consistently low false positive rates.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Focusing on the models with a reasonable amount of false positives (LGrad to DIMD in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.T7" title="Table 7 ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">7</span></a>) still leads to contractive insights. At times the underlying generative model seems relevant for detection (variants of SD models are identified at higher rates, even for models not trained on diffusion) while other cases show discrepancies between datasets generated using the same model (<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.1">DALLE3</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.2">SDXL</span>). This evidence suggests a complex relationship between training data, data sources and detection efficacy.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Time and obsolescence are another critical factor, exemplified by LGrad’s diminished effectiveness on data beyond 2022. A continuous evaluation of old detectors is thus recommended, together with the development of new detectors to keep pace with advancing generation techniques, particularly considering the significant existing blind spots in recent models (<span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.1">e.g., </span>recalls over 90% are scarce).</p>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<p class="ltx_p" id="S6.SS1.p5.1">Finally, let us examine the behaviour on the <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p5.1.1">In-the-wild</span>, as an approximation of performance in practice. None of the tested detectors reaches a recall above 50% for both the authentic and synthetic versions of it. The current best-performing models are DIMD and Rine (with a large false negative rate), and <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p5.1.2">SuSy</span> and DeFake (with a large false positive rate).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Scale Generalization</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In our final generalization study, rather than cropping, we resize all images to <math alttext="224\times 224" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mrow id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><mn id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml">224</mn><mo id="S6.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S6.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><times id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1"></times><cn id="S6.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S6.SS2.p1.1.m1.1.1.2">224</cn><cn id="S6.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S6.SS2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">224 × 224</annotation></semantics></math> pixels regardless of the original size. This approach notably impacts the frequency components of the images, as well as the content itself, particularly for high-resolution originals. The resizing process can alter or eliminate certain frequency artifacts and defects that some SID models rely on. Aggregated results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.T7" title="Table 7 ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">7</span></a> (<span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">Resize</span> row). See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A1" title="Appendix A Additional Evaluation Metrics ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">A</span></a> for further details.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.3">Results reveal certain patterns across detection models. Three models (GramNet, Dire and <span class="ltx_text ltx_font_typewriter" id="S6.SS2.p2.3.1">SuSy</span>) exhibit a balanced trade-off between false positives and false negatives, approximately losing in one recall what is gained in the other. For instance, Dire shows a substantial improvement for authentic images (<math alttext="+25.88" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mo id="S6.SS2.p2.1.m1.1.1a" xref="S6.SS2.p2.1.m1.1.1.cmml">+</mo><mn id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">25.88</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><plus id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"></plus><cn id="S6.SS2.p2.1.m1.1.1.2.cmml" type="float" xref="S6.SS2.p2.1.m1.1.1.2">25.88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">+25.88</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">+ 25.88</annotation></semantics></math>) but an analogous decrease for synthetic ones (<math alttext="-23.97" class="ltx_Math" display="inline" id="S6.SS2.p2.2.m2.1"><semantics id="S6.SS2.p2.2.m2.1a"><mrow id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml"><mo id="S6.SS2.p2.2.m2.1.1a" xref="S6.SS2.p2.2.m2.1.1.cmml">−</mo><mn id="S6.SS2.p2.2.m2.1.1.2" xref="S6.SS2.p2.2.m2.1.1.2.cmml">23.97</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><apply id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1"><minus id="S6.SS2.p2.2.m2.1.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1"></minus><cn id="S6.SS2.p2.2.m2.1.1.2.cmml" type="float" xref="S6.SS2.p2.2.m2.1.1.2">23.97</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">-23.97</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.2.m2.1d">- 23.97</annotation></semantics></math>). This indicates that, for these models, changing the resolution changes the decision boundary, in a way which is independent from the features characterizing the final discrimination between authentic and synthetic images. Such that, when the model becomes more/less sensitive, this affects equally both the authentic and synthetic predictions. Preliminary experimentation on Dire shows this symmetry to be consistent on other resolutions (<math alttext="512\times 512" class="ltx_Math" display="inline" id="S6.SS2.p2.3.m3.1"><semantics id="S6.SS2.p2.3.m3.1a"><mrow id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml"><mn id="S6.SS2.p2.3.m3.1.1.2" xref="S6.SS2.p2.3.m3.1.1.2.cmml">512</mn><mo id="S6.SS2.p2.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S6.SS2.p2.3.m3.1.1.1.cmml">×</mo><mn id="S6.SS2.p2.3.m3.1.1.3" xref="S6.SS2.p2.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><apply id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1"><times id="S6.SS2.p2.3.m3.1.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1.1"></times><cn id="S6.SS2.p2.3.m3.1.1.2.cmml" type="integer" xref="S6.SS2.p2.3.m3.1.1.2">512</cn><cn id="S6.SS2.p2.3.m3.1.1.3.cmml" type="integer" xref="S6.SS2.p2.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.3.m3.1d">512 × 512</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">One model collapses, suffering catastrophic losses after the resolution change. DIMD’s average recall on the synthetic class drops 62%, while only gaining 1.5% on the authentic class. This could be caused by DIMD’s avoidance of the downsampling step, making it more dependent on image scale. Regardless, it points towards the importance of considering model sensitivity during the design and training of detectors. This is a valuable feature that is present in two other models, LGrad and Rine. Remarkably, their performance generalizes very well to scale changes.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">Lastly, DeFake stands out by showing large improvements in both recall scores (<math alttext="&gt;+15\%" class="ltx_Math" display="inline" id="S6.SS2.p4.1.m1.1"><semantics id="S6.SS2.p4.1.m1.1a"><mrow id="S6.SS2.p4.1.m1.1.1" xref="S6.SS2.p4.1.m1.1.1.cmml"><mi id="S6.SS2.p4.1.m1.1.1.2" xref="S6.SS2.p4.1.m1.1.1.2.cmml"></mi><mo id="S6.SS2.p4.1.m1.1.1.1" xref="S6.SS2.p4.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S6.SS2.p4.1.m1.1.1.3" xref="S6.SS2.p4.1.m1.1.1.3.cmml"><mo id="S6.SS2.p4.1.m1.1.1.3a" xref="S6.SS2.p4.1.m1.1.1.3.cmml">+</mo><mrow id="S6.SS2.p4.1.m1.1.1.3.2" xref="S6.SS2.p4.1.m1.1.1.3.2.cmml"><mn id="S6.SS2.p4.1.m1.1.1.3.2.2" xref="S6.SS2.p4.1.m1.1.1.3.2.2.cmml">15</mn><mo id="S6.SS2.p4.1.m1.1.1.3.2.1" xref="S6.SS2.p4.1.m1.1.1.3.2.1.cmml">%</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.1.m1.1b"><apply id="S6.SS2.p4.1.m1.1.1.cmml" xref="S6.SS2.p4.1.m1.1.1"><gt id="S6.SS2.p4.1.m1.1.1.1.cmml" xref="S6.SS2.p4.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S6.SS2.p4.1.m1.1.1.2.cmml" xref="S6.SS2.p4.1.m1.1.1.2">absent</csymbol><apply id="S6.SS2.p4.1.m1.1.1.3.cmml" xref="S6.SS2.p4.1.m1.1.1.3"><plus id="S6.SS2.p4.1.m1.1.1.3.1.cmml" xref="S6.SS2.p4.1.m1.1.1.3"></plus><apply id="S6.SS2.p4.1.m1.1.1.3.2.cmml" xref="S6.SS2.p4.1.m1.1.1.3.2"><csymbol cd="latexml" id="S6.SS2.p4.1.m1.1.1.3.2.1.cmml" xref="S6.SS2.p4.1.m1.1.1.3.2.1">percent</csymbol><cn id="S6.SS2.p4.1.m1.1.1.3.2.2.cmml" type="integer" xref="S6.SS2.p4.1.m1.1.1.3.2.2">15</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.1.m1.1c">&gt;+15\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.1.m1.1d">&gt; + 15 %</annotation></semantics></math>) under scale reduction. This may be caused by the Visual Language Model architecture empowering the detector, which could benefit from having the whole image context as input, despite losing frequency artifacts. We find these results to be consistent with the findings in <cite class="ltx_cite ltx_citemacro_citep">(Schinas and Papadopoulos, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#bib.bib35" title="">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The race between synthetic image generators and detectors is far from over. New and better generative models appear regularly, and detectors cannot generalize to all (see §<a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6" title="6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a>). Meanwhile, the increasing realism of synthetically generated content brings along a proportional demand for detectors, as tools for protecting social trust and digital rights, while preventing disinformation.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">The biggest challenge for detectors is generalization. Changes of generative model, data source, image post-processing and training task, affect detector performance, limiting their reliability. This work studied all these factors, starting with the underlying generative model used to produce the images. By temporally grounding our experiments, we observe how older models/datasets (less realistic and producing bigger artifacts) can be easily detected when trained for them, but are harder to generalize to when not seen during training (when compared to more recent generative models, see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T2" title="Table 2 ‣ 4.1. Single Class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">2</span></a>). This indicates the stronger biases defining older synthetic images are rarely found in newer datasets. As a result, including early generative models in the train set of new detector models should become common practice. Another consistent family of generative models that is consistently harder to generalize to are private models (<span class="ltx_text ltx_font_italic" id="S7.p2.1.1">e.g., </span><span class="ltx_text ltx_font_typewriter" id="S7.p2.1.2">DALLE3</span>, <span class="ltx_text ltx_font_typewriter" id="S7.p2.1.3">Firefly</span>), as shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T2" title="Table 2 ‣ 4.1. Single Class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.T7" title="Table 7 ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">7</span></a>. This speaks of the importance of open science in the field.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Data source is another key factor in detection. Using the same generative model, individuals employing various software and hardware configurations can produce significantly different content, to the point where generalization to the same model but a different source often becomes challenging (see Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S6.T7" title="Table 7 ‣ 6. Generalization of state-of-the-art models ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">7</span></a>). As a result, detector models can never be assumed to work in data obtained from an untested source. This is further reinforced by tests conducted on data gathered <span class="ltx_text ltx_font_typewriter" id="S7.p3.1.1">In-the-wild</span>, showing the dangerous effect of uncontrolled data sources. To mitigate this risk, recurrent sanity checks are recommended, as well as dataset-specific fine-tunes.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">A common pitfall of detector models is the lack of a proper characterization of <span class="ltx_text ltx_font_italic" id="S7.p4.1.1">authentic</span> images. This has a higher prevalence on binary classifiers (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T3" title="Table 3 ‣ 4.2. Multi-class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">3</span></a>) but also happens on multi-class ones (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a>). Since characterizing authentic images is essential for practical use, specific tests using different sources of authentic data should be prioritized. Training on authentic data from many sources should also be common practice.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">This work shows how to train detectors that maximize generalization, by focusing on many different data sources and generative models. This alone is no guarantee, and diversity, even within the same source or model, is of essence. Our limited field study indicates that, while we are far from a universal detector, models trained for specific targets may already be as good as humans at identifying synthetic content (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T6" title="Table 6 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div class="ltx_para" id="S7.p6">
<p class="ltx_p" id="S7.p6.1">The race between generators and detectors will go on, particularly since data from better generative models produces detectors that generalize better (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S4.T2" title="Table 2 ‣ 4.1. Single Class Models ‣ 4. Train Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">2</span></a>), while better detectors are likely to be used to improve generators. This leads to a race equilibrium paradox, ensuring that the race for synthetic content detection is always going to be a close one.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Ethical risks</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">To finalize, let us consider the ethical risks associated with this work and with the materials released. The most obvious of which is the fallibility of the detectors trained. The <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1">SuSy</span>, as any other detector model, makes both false positive and negative predictions (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#S5.T5" title="Table 5 ‣ 5.1. Generalization to source ‣ 5. Evaluation Experiments ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">5</span></a>), potentially labelling authentic images as synthetic and vice versa. Digital rights could be affected and censorship could be enabled through its use. For this reason, when applied in a setting affecting the rights and privileges of humans, the model should always be overseen by a human expert, and its predictions should never be taken as conclusive evidence.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">The datasets used in this work are biased in many ways. Some of these biases are learnt by the models and could influence its decision in undesired ways (<span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1">e.g., </span>countryside images could be more likely to be tagged as synthetic than urban landscapes). The most effective mitigation strategy in this case is to conduct a study on model performance, based on the existing populations within the application domain. Once undesirable biases are identified, these can be corrected through customized fine-tuning using synthetic data.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">While all datasets used for training are publicly available, these may include samples with personal data. <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p3.1.1">COCO</span> contains images of real people, and synthetic datasets used could include realistic depictions of specific individuals. It is however unlikely, both for the training task and the size of <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p3.1.2">SuSy</span>, that such information would be encoded and retrieved from the released weights alone.</p>
</div>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">A final risk we consider is the use of released detector models for the purpose of training generative models that can overcome them (<span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.1">e.g., </span>by adding a specific loss). In fact, this is already a common strategy in academia and industry (<span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.2">i.e., </span>GANs), that can be applied to our work. To mitigate that, we add a specific clause in the terms of use of the model prohibiting such practice. The alternative, not doing research in this line, would deprive society of means of protection against the increasing presence of synthetic images.</p>
</div>
<div class="ltx_para" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.1">Any detector model publicly released is compromised (<span class="ltx_text ltx_font_italic" id="S7.SS1.p5.1.1">e.g., </span>by finding counterfactuals examples). We recommend potential users of <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p5.1.2">SuSy</span> to tune it on private data, as to mitigate that risk, while considering in the process the aforementioned limitations with regards to accuracy and bias. We also recommend fine-tuned model weights are kept private, as long as their public release holds no special academic or social value.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work has been partly funded by the AI4Media and AI4Europe projects from the European Union’s Horizon 2020 programme (Grant Agreements Nº951911 and Nº101070000), and by a SGR-GRE grant from the Generalitat de Catalunya (code 2021 SGR 01187). The authors would like to acknowledge Mauro Achile, Eric Arean, Nura Mangado, Diego Rios and Daniel Pulido who contributed to motivating and contextualizing this work. Special thanks to the volunteers who participated in the human evaluation experiment.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">civ ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock">Civitai.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://civitai.com/" title="">https://civitai.com/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agency (2024)</span>
<span class="ltx_bibblock">
European Environment Agency. 2024.

</span>
<span class="ltx_bibblock">Greenhouse gas emission intensity of electricity generation in Europe.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eea.europa.eu/en/analysis/indicators/greenhouse-gas-emission-intensity-of-1" title="">https://www.eea.europa.eu/en/analysis/indicators/greenhouse-gas-emission-intensity-of-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2023)</span>
<span class="ltx_bibblock">
Stability AI. 2023.

</span>
<span class="ltx_bibblock">Stable Diffusion XL.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stability.ai/news/stable-diffusion-sdxl-1-announcement" title="">https://stability.ai/news/stable-diffusion-sdxl-1-announcement</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Stability AI. 2024.

</span>
<span class="ltx_bibblock">Stable Diffusion 3 Medium.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/stabilityai/stable-diffusion-3-medium" title="">https://huggingface.co/stabilityai/stable-diffusion-3-medium</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bammey (2023)</span>
<span class="ltx_bibblock">
Quentin Bammey. 2023.

</span>
<span class="ltx_bibblock">Synthbuster: Towards detection of diffusion model generated images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Open Journal of Signal Processing</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buslaev et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020.

</span>
<span class="ltx_bibblock">Albumentations: Fast and Flexible Image Augmentations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Information</em> 11, 2 (2020).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/info11020125" title="">https://doi.org/10.3390/info11020125</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cazenavette et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman. 2024.

</span>
<span class="ltx_bibblock">FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10759–10769.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Baoying Chen, Jishen Zeng, Jianquan Yang, and Rui Yang. 2024.

</span>
<span class="ltx_bibblock">DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Forty-first International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corvi et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. 2023.

</span>
<span class="ltx_bibblock">On the detection of synthetic images generated by diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cozzolino et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias Nießner, and Luisa Verdoliva. 2024.

</span>
<span class="ltx_bibblock">Raising the Bar of AI-generated Image Detection with CLIP. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 4356–4366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dang-Nguyen et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Duc-Tien Dang-Nguyen, Cecilia Pasquini, Valentina Conotter, and Giulia Boato. 2015.

</span>
<span class="ltx_bibblock">Raise: A raw images dataset for digital image forensics. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 6th ACM multimedia systems conference</em>. 219–224.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol. 2021.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Advances in neural information processing systems</em> 34 (2021), 8780–8794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DucHaiten (2023)</span>
<span class="ltx_bibblock">
DucHaiten. 2023.

</span>
<span class="ltx_bibblock">realisticSDXL Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/DucHaiten/DucHaiten-realistic-SDXL" title="">https://huggingface.co/datasets/DucHaiten/DucHaiten-realistic-SDXL</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ehristoforu (2024a)</span>
<span class="ltx_bibblock">
ehristoforu. 2024a.

</span>
<span class="ltx_bibblock">dalle-3-images Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/ehristoforu/dalle-3-images" title="">https://huggingface.co/datasets/ehristoforu/dalle-3-images</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ehristoforu (2024b)</span>
<span class="ltx_bibblock">
ehristoforu. 2024b.

</span>
<span class="ltx_bibblock">midjourney-images Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/ehristoforu/midjourney-images" title="">https://huggingface.co/datasets/ehristoforu/midjourney-images</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frank et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Joel Frank, Thorsten Eisenhofer, Lea Schönherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. 2020.

</span>
<span class="ltx_bibblock">Leveraging frequency analysis for deep fake image recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">International conference on machine learning</em>. PMLR, 3247–3258.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grommelt et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, and Janis Keuper. 2024.

</span>
<span class="ltx_bibblock">Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">arXiv preprint arXiv:2403.17608</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gustavosta (2023)</span>
<span class="ltx_bibblock">
Gustavosta. 2023.

</span>
<span class="ltx_bibblock">Stable-Diffusion-Prompts.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts" title="">https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 770–778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Huaibo Huang, Zhihang Li, Ran He, Zhenan Sun, and Tieniu Tan. 2018.

</span>
<span class="ltx_bibblock">IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1807.06358 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1807.06358" title="">https://arxiv.org/abs/1807.06358</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ju et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, and Siwei Lyu. 2022.

</span>
<span class="ltx_bibblock">Fusing global and local features for generalized ai-synthesized image detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">2022 IEEE International Conference on Image Processing (ICIP)</em>. IEEE, 3465–3469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017.

</span>
<span class="ltx_bibblock">Progressive growing of gans for improved quality, stability, and variation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:1710.10196</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koutlis and Papadopoulos (2024)</span>
<span class="ltx_bibblock">
Christos Koutlis and Symeon Papadopoulos. 2024.

</span>
<span class="ltx_bibblock">Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2402.19091</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labs (2024)</span>
<span class="ltx_bibblock">
Black Forest Labs. 2024.

</span>
<span class="ltx_bibblock">FLUX.1-dev.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/black-forest-labs/FLUX.1-dev" title="">https://huggingface.co/black-forest-labs/FLUX.1-dev</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Dit: Self-supervised pre-training for document image transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 30th ACM International Conference on Multimedia</em>. 3530–3539.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2015.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1405.0312 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Huan Liu, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Jingdong Wang, and Yao Zhao. 2024.

</span>
<span class="ltx_bibblock">Forgery-aware adaptive transformer for generalizable synthetic image detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10770–10780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. 2022.

</span>
<span class="ltx_bibblock">Pseudo numerical methods for diffusion models on manifolds.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:2202.09778</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhengzhe Liu, Xiaojuan Qi, and Philip HS Torr. 2020.

</span>
<span class="ltx_bibblock">Global texture enhancement for fake face detection in the wild. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 8060–8069.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">López Cuena (2023)</span>
<span class="ltx_bibblock">
Enrique López Cuena. 2023.

</span>
<span class="ltx_bibblock">Super-resolution assessment and detection.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://hdl.handle.net/2117/395959" title="">http://hdl.handle.net/2117/395959</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol and Dhariwal (2021)</span>
<span class="ltx_bibblock">
Alexander Quinn Nichol and Prafulla Dhariwal. 2021.

</span>
<span class="ltx_bibblock">Improved denoising diffusion probabilistic models. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">International conference on machine learning</em>. PMLR, 8162–8171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ojha et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock">Towards universal fake image detectors that generalize across generative models. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 24480–24489.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ricker et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fischer. 2022.

</span>
<span class="ltx_bibblock">Towards the detection of diffusion model deepfakes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2210.14571</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schinas and Papadopoulos (2024)</span>
<span class="ltx_bibblock">
Manos Schinas and Symeon Papadopoulos. 2024.

</span>
<span class="ltx_bibblock">SIDBench: A Python framework for reliably assessing synthetic image detection methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2404.18552</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sha et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. 2023.

</span>
<span class="ltx_bibblock">De-fake: Detection and attribution of fake images generated by text-to-image generation models. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security</em>. 3418–3432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. 2023.

</span>
<span class="ltx_bibblock">Learning on gradients: Generalized artifacts representation for gan-generated images detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 12105–12114.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turc and Nemade (2022)</span>
<span class="ltx_bibblock">
Iulia Turc and Gaurav Nemade. 2022.

</span>
<span class="ltx_bibblock">Midjourney User Prompts &amp; Generated Images (250k).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.34740/KAGGLE/DS/2349267" title="">https://doi.org/10.34740/KAGGLE/DS/2349267</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Union (2024)</span>
<span class="ltx_bibblock">
European Union. 2024.

</span>
<span class="ltx_bibblock">Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://artificialintelligenceact.eu/" title="">https://artificialintelligenceact.eu/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. 2020.

</span>
<span class="ltx_bibblock">CNN-generated images are surprisingly easy to spot… for now. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 8695–8704.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022.

</span>
<span class="ltx_bibblock">DiffusionDB: A Large-Scale Prompt Gallery Dataset for Text-to-Image Generative Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">arXiv:2210.14896 [cs]</em> (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.14896" title="">https://arxiv.org/abs/2210.14896</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weyand et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. 2020.

</span>
<span class="ltx_bibblock">Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2575–2584.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wißmann et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Alexander Wißmann, Steffen Zeiler, Robert M Nickel, and Dorothea Kolossa. 2024.

</span>
<span class="ltx_bibblock">Whodunit: Detection and Attribution of Synthetic Images by Leveraging Model-specific Fingerprints. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation</em>. 65–72.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Qiang Xu, Hao Wang, Laijin Meng, Zhongjie Mi, Jianye Yuan, and Hong Yan. 2023.

</span>
<span class="ltx_bibblock">Exposing fake images generated by text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Pattern Recognition Letters</em> 176 (2023), 76–82.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Transactions of the Association for Computational Linguistics</em> 2 (2014), 67–78.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. 2015.

</span>
<span class="ltx_bibblock">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">arXiv preprint arXiv:1506.03365</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang. 2023.

</span>
<span class="ltx_bibblock">Rich and Poor Texture Contrast: A Simple yet Effective Approach for AI-generated Image Detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">arXiv preprint arXiv:2311.12397</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mingjian Zhu, Hanting Chen, Mouxiao Huang, Wei Li, Hailin Hu, Jie Hu, and Yunhe Wang. 2023.

</span>
<span class="ltx_bibblock">Gendet: Towards good generalizations for ai-generated image detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv preprint arXiv:2312.08880</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. 2024.

</span>
<span class="ltx_bibblock">Genimage: A million-scale benchmark for detecting ai-generated image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Evaluation Metrics</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14128v1#A1.T8" title="Table 8 ‣ Appendix A Additional Evaluation Metrics ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">8</span></a> summarizes the recall rates of four detection methods—CNNDetect, FreqDetect, Fusing, and UnivFD—across various datasets, when applying a center crop of size <math alttext="224\times 224" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mn id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">224</mn><mo id="A1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.1.m1.1.1.1.cmml">×</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><times id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></times><cn id="A1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.p1.1.m1.1.1.2">224</cn><cn id="A1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">224 × 224</annotation></semantics></math> to the input image.</p>
</div>
<figure class="ltx_table" id="A1.T8">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.2.1">CNNDetect</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.3.1">FreqDetect</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.4.1">Fusing</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.5.1">UnivFD</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.2.1.1">In the wild (Authentic)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.2.1.2">97.52</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.2.1.3">98.35</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.2.1.4">99.17</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T8.1.2.1.5">73.55</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T8.1.3.2.1">Flickr30k</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.3.2.2">99.79</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.3.2.3">99.67</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.3.2.4">99.98</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.3.2.5">99.76</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.3.1">Google Landmarks v2</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.3.2">98.64</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.3.3">99.54</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.3.4">99.90</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.4.3.5">97.70</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.4.1">In the wild (Synthetic)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.4.2">1.01</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.4.3">2.02</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.4.4">1.01</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.5.4.5">5.05</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.5.1">FLUX.1-dev</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.5.2">7.36</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.5.3">0.55</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.5.4">6.98</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.6.5.5">1.51</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.6.1">Stable Diffusion 3 Medium</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.6.2">16.72</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.6.3">12.29</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.6.4">23.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.7.6.5">4.79</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.8.7">
<td class="ltx_td ltx_align_left" id="A1.T8.1.8.7.1">DALLE-2 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.8.7.2">20.90</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.8.7.3">7.10</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.8.7.4">22.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.8.7.5">72.70</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.9.8">
<td class="ltx_td ltx_align_left" id="A1.T8.1.9.8.1">DALLE-3 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.9.8.2">0.20</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.9.8.3">0.20</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.9.8.4">0.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.9.8.5">0.40</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.10.9">
<td class="ltx_td ltx_align_left" id="A1.T8.1.10.9.1">Firefly (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.10.9.2">17.40</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.10.9.3">3.00</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.10.9.4">17.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.10.9.5">85.50</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.11.10">
<td class="ltx_td ltx_align_left" id="A1.T8.1.11.10.1">GLIDE (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.11.10.2">9.30</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.11.10.3">10.70</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.11.10.4">12.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.11.10.5">12.20</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.12.11">
<td class="ltx_td ltx_align_left" id="A1.T8.1.12.11.1">Midjourney v5 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.12.11.2">6.60</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.12.11.3">1.20</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.12.11.4">9.90</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.12.11.5">10.70</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.13.12">
<td class="ltx_td ltx_align_left" id="A1.T8.1.13.12.1">Stable Diffusion 1.3 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.13.12.2">9.50</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.13.12.3">8.40</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.13.12.4">9.50</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.13.12.5">46.50</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.14.13">
<td class="ltx_td ltx_align_left" id="A1.T8.1.14.13.1">Stable Diffusion 1.4 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.14.13.2">9.70</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.14.13.3">7.90</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.14.13.4">9.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.14.13.5">45.40</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.15.14">
<td class="ltx_td ltx_align_left" id="A1.T8.1.15.14.1">Stable Diffusion 2 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.15.14.2">9.40</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.15.14.3">0.40</td>
<td class="ltx_td ltx_align_left" id="A1.T8.1.15.14.4">9.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T8.1.15.14.5">57.10</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T8.1.16.15.1">Stable Diffusion XL (SB)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T8.1.16.15.2">18.20</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T8.1.16.15.3">0.30</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T8.1.16.15.4">6.50</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="A1.T8.1.16.15.5">41.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8. </span>Recall of four additional detection methods when applied to the evaluation datasets.</figcaption>
</figure>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">The following table contains the scores for all studied models after resizing the input to <math alttext="224\times 224" class="ltx_Math" display="inline" id="A1.p2.1.m1.1"><semantics id="A1.p2.1.m1.1a"><mrow id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><mn id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml">224</mn><mo id="A1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p2.1.m1.1.1.1.cmml">×</mo><mn id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><times id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1.1"></times><cn id="A1.p2.1.m1.1.1.2.cmml" type="integer" xref="A1.p2.1.m1.1.1.2">224</cn><cn id="A1.p2.1.m1.1.1.3.cmml" type="integer" xref="A1.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="A1.p2.1.m1.1d">224 × 224</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<table class="ltx_tabular ltx_align_middle" id="A1.T9.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.3.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.2.1">CNNDetect</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.3.1">DIMD</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.4.1">DeFake</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.5.1">Dire</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.6.1">FreqDetect</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.7"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.7.1">Fusing</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.8"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.8.1">GramNet</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.9"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.9.1">LGrad</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.10"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.10.1">Rine</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="A1.T9.3.1.1.11"><span class="ltx_text ltx_font_bold" id="A1.T9.3.1.1.11.1">UnivFD</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.1">Flickr30K</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.2">99.94</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.3">100</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.4">79.96</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.5">64.84</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.6">99.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.7">100</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.8">98.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.9">89.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.2.2.10">99.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T9.3.2.2.11">99.84</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.3.3">
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.1">COCO (test)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.2">92.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.3">100</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.4">86.06</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.5">60.62</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.6">97.16</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.7">98.62</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.8">14.99</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.9">92.46</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.3.3.10">88.17</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.3.3.11">99.03</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.4.4">
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.1">Google Landmarks v2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.2">99.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.3">99.98</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.4">64.66</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.5">62.96</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.6">99.58</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.7">99.92</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.8">88.42</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.9">68.84</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.4.4.10">97.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.4.4.11">98.98</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.5.5">
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.1">In the wild (Authentic)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.2">90.08</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.3">99.17</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.4">34.71</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.5">71.07</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.6">99.17</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.7">99.17</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.8">20.66</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.9">85.95</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.5.5.10">83.47</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.5.5.11">99.17</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.1">GLIDE (SB)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.2">11.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.3">3.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.4">87.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.5">15.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.6">10.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.7">10.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.8">84.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.9">68.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.3.6.6.10">83.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T9.3.6.6.11">10.1</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.7.7">
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.1">Midjourney TTI (test)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.2">5.52</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.3">1.77</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.4">82.67</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.5">20.75</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.6">5.96</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.7">7.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.8">80.02</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.9">15.89</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.7.7.10">81.57</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.7.7.11">10.82</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.8.8">
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.1">Stable Diffusion 1.3 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.2">13.6</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.3">0.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.4">85.7</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.5">53.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.6">2.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.7">7.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.8">94.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.9">86.8</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.8.8.10">93.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.8.8.11">4.4</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.9.9">
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.1">Stable Diffusion 1.4 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.2">13.7</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.3">0.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.4">87.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.5">53.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.6">2.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.7">7.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.8">95.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.9">89.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.9.9.10">93.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.9.9.11">3.6</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.10.10">
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.1">Diffusiondb (test)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.2">9.56</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.3">6.24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.4">81.69</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.5">21.64</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.6">7.05</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.7">4.62</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.8">92.79</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.9">60.13</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.10.10.10">91.57</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.10.10.11">13.53</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.11.11">
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.1">Stable Diffusion 2 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.2">29.7</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.3">0.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.4">61.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.5">79.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.6">1.5</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.7">6.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.8">82.6</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.9">29.8</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.11.11.10">82.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.11.11.11">34.1</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.12.12">
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.1">DALLE-2 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.2">10.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.3">0.6</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.4">47.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.5">29.8</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.6">5.5</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.7">21.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.8">79.5</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.9">32</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.12.12.10">67.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.12.12.11">5.9</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.13.13">
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.1">Midjourney v5 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.2">17.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.3">0.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.4">75.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.5">33.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.6">3.5</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.7">5.5</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.8">98.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.9">30.6</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.13.13.10">67.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.13.13.11">0.5</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.14.14">
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.1">MJ V5/6 (test)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.2">5.51</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.3">0.49</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.4">85.74</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.5">26.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.6">4.54</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.7">0.49</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.8">76.18</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.9">8.91</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.14.14.10">14.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.14.14.11">1.3</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.15.15">
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.1">Stable Diffusion XL (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.2">28.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.3">0.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.4">78.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.5">55.8</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.6">1.8</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.7">2.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.8">97.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.9">37.2</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.15.15.10">87.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.15.15.11">14</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.16.16">
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.1">Stable Diffusion XL (test)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.2">16.45</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.3">2.35</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.4">94.73</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.5">15.48</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.6">2.67</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.7">20.18</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.8">96.27</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.9">27.96</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.16.16.10">53.81</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.16.16.11">0.49</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.17.17">
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.1">Firefly (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.2">14.7</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.3">0</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.4">79.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.5">36.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.6">4.6</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.7">10</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.8">89.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.9">14.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.17.17.10">56.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.17.17.11">2.2</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.18.18">
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.1">DALLE-3 (SB)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.2">1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.3">0</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.4">95.4</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.5">50.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.6">3.1</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.7">0</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.8">93</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.9">5.3</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.18.18.10">59.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.18.18.11">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.19.19">
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.1">DALLE-3 (test)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.2">4.55</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.3">0</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.4">87.58</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.5">31.21</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.6">4.85</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.7">0.61</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.8">50.61</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.9">13.33</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.19.19.10">10.61</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.19.19.11">0.3</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.20.20">
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.1">FLUX.1-dev</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.2">1.39</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.3">0.04</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.4">89.87</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.5">15.82</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.6">9.29</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.7">7.68</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.8">95.69</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.9">27.15</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.20.20.10">56.79</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.20.20.11">0.28</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.21.21">
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.1">In the wild (Synthetic)</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.2">6.06</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.3">0</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.4">89.9</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.5">28.28</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.6">2.02</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.7">3.03</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.8">91.92</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.9">16.16</td>
<td class="ltx_td ltx_align_left" id="A1.T9.3.21.21.10">18.18</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T9.3.21.21.11">0</td>
</tr>
<tr class="ltx_tr" id="A1.T9.3.22.22">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.1">Stable Diffusion 3 Medium</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.2">5.36</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.3">0.09</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.4">92.49</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.5">12.12</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.6">6.88</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.7">29.46</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.8">90.77</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.9">35.89</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.10">74.01</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="A1.T9.3.22.22.11">2.28</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9. </span>Recall of state-of-the-art detector models across evaluation datasets when resizing the input image to <math alttext="224\times 224" class="ltx_Math" display="inline" id="A1.T9.2.m1.1"><semantics id="A1.T9.2.m1.1b"><mrow id="A1.T9.2.m1.1.1" xref="A1.T9.2.m1.1.1.cmml"><mn id="A1.T9.2.m1.1.1.2" xref="A1.T9.2.m1.1.1.2.cmml">224</mn><mo id="A1.T9.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.T9.2.m1.1.1.1.cmml">×</mo><mn id="A1.T9.2.m1.1.1.3" xref="A1.T9.2.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T9.2.m1.1c"><apply id="A1.T9.2.m1.1.1.cmml" xref="A1.T9.2.m1.1.1"><times id="A1.T9.2.m1.1.1.1.cmml" xref="A1.T9.2.m1.1.1.1"></times><cn id="A1.T9.2.m1.1.1.2.cmml" type="integer" xref="A1.T9.2.m1.1.1.2">224</cn><cn id="A1.T9.2.m1.1.1.3.cmml" type="integer" xref="A1.T9.2.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.2.m1.1d">224\times 224</annotation><annotation encoding="application/x-llamapun" id="A1.T9.2.m1.1e">224 × 224</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation Dataset Image Samples</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">This section includes several sample images from the evaluation datasets utilized.</p>
</div>
<figure class="ltx_figure" id="A2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="A2.F2.g1" src="x1.png" width="1136"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Sample authentic images from Flickr30K (top) and Google Landmarks v2 (bottom)</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="421" id="A2.F3.g1" src="x2.png" width="1136"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Sample images from our In-The-Wild dataset. Synthetic images (top) and authentic images (bottom)</figcaption>
</figure>
<figure class="ltx_figure" id="A2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1642" id="A2.1.g1" src="x3.png" width="1196"/>
</figure>
<figure class="ltx_figure" id="A2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1556" id="A2.F4.g1" src="x4.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Sample synthetic images from Synthbuster. Note that the same prompt is used for each column.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="426" id="A2.F5.g1" src="x5.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Sample synthetic images from our generated Stable Diffusion 3 Medium (top) and FLUX.1-dev (bottom) datasets.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation Image Resolution Distribution</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We calculate the resolution distribution for each of the evaluation dataset. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.14128v1#A3.F6" title="Figure 6 ‣ Appendix C Evaluation Image Resolution Distribution ‣ Present and Future Generalization of Synthetic Image Detectors"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> contains information regarding the size and aspect ratio of the datasets used. Top plot shows the width distribution of all samples, dataset-wise. The bottom plot shows the same for height.</p>
</div>
<figure class="ltx_figure" id="A3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="539" id="A3.F6.g1" src="extracted/5870075/images/resolution_distribution_violin.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Resolution distribution of images across various datasets. The plots illustrate the width (top) and height (bottom) distributions.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 21 12:29:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
