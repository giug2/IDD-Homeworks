<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.04282] Classification of Natural Language Processing Techniques for Requirements Engineering</title><meta property="og:description" content="Research in applying natural language processing (NLP) techniques to requirements engineering (RE) tasks spans more than 40 years, from initial efforts carried out in the 1980s to more recent attempts with machine lear…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Classification of Natural Language Processing Techniques for Requirements Engineering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Classification of Natural Language Processing Techniques for Requirements Engineering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.04282">

<!--Generated on Wed Feb 28 07:38:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Requirements Engineering (RE),  Natural Language Processing (NLP),  NLP Tasks,  NLP Techniques
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Classification of Natural Language Processing Techniques for Requirements Engineering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liping Zhao, Waad Alhoshan, Alessio Ferrari, Keletso J. Letsholo
</span><span class="ltx_author_notes">L. Zhao is with the University of Manchester, Manchester, UKW. Alhoshan is with Al-Imam Mohammed ibn Saud Islamic University, Riyadh, Saudi ArabiaA. Ferrari is with Consiglio Nazionale delle Ricerche, CNR-ISTI, Pisa, ItalyK.J. Letsholo is with Higher Colleges of Technology, Abu Dhabi, UAE</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Research in applying natural language processing (NLP) techniques to requirements engineering (RE) tasks spans more than 40 years, from initial efforts carried out in the 1980s to more recent attempts with machine learning (ML) and deep learning (DL) techniques. However, in spite of the progress, our recent survey shows that there is still a lack of systematic understanding and organization of commonly used NLP techniques in RE.
We believe one hurdle facing the industry is lack of shared knowledge of NLP techniques and their usage in RE tasks. In this paper, we present our effort to synthesize and organize 57 most frequently used NLP techniques in RE. We classify these NLP techniques in two ways: first, by their NLP tasks in typical pipelines and second, by their linguist analysis levels. We believe these two ways of classification are complementary, contributing to a better understanding of the NLP techniques in RE and such understanding is crucial to the development of better NLP tools for RE.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Requirements Engineering (RE), Natural Language Processing (NLP), NLP Tasks, NLP Techniques

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Research in developing natural language processing (NLP) support for requirements engineering (RE), or NLP4RE for short, dates back to the early 1980s and has seen a continuous flow of contributions in the past 40 years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. However, in spite of huge improvements and advances in NLP in the last 20 years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and great progress in NLP4RE research in the last 10 years, the uptake of NLP technologies in RE, and their industrial penetration, is still limited and fragmented<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Thus large gaps remain between NLP4RE research and its practical application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A recent survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> cites insufficient industrial evaluation of NLP4RE research, the lack of shared RE-specific language resources, and the lack of technology know-how in NLP among the reasons for these gaps. As a first step to close these gaps, this paper aims to classify the NLP techniques commonly used in RE so that they are easy to understand. We believe that a better understanding of the NLP techniques in RE is not only crucial to the development of better NLP tools for RE, but also to their industrial adoption. In particular, the paper lays foundations for establishing a common terminology and vocabulary of the NLP techniques through the following contributions:</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We extract and synthesize 57 commonly used NLP techniques in RE for NLP4RE research and practice.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We systematically classify these techniques in two ways: by their tasks typically performed in NLP pipelines and then by their linguistic analysis capability.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The paper is organized as follows. Sect. <a href="#S2" title="II Background: 40 Years of NLP4RE ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides a brief history of NLP for RE, as the background and motivation for this paper. Sect. <a href="#S3" title="III Method ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> describes how we extract and synthesize the common set of NLP techniques for RE. Sect. <a href="#S4" title="IV Classifying NLP Techniques by Tasks ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and Sect. <a href="#S5" title="V Classifying NLP Techniques by Linguistic Analysis Levels ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> present our classification of these techniques. Sect. <a href="#S6" title="VI Conclusion ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background: 40 Years of NLP4RE</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">As a background to this paper, we provide a brief history of NLP4RE. We first point to some notable contributions in RE that use traditional NLP techniques, and then outline the recent application of machine learning (ML) and deep learning (DL) in RE. However, the focus of this paper is on NLP techniques, not ML and DL techniques.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Traditional NLP for RE</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The relationship between NLP and RE is well established and widely discussed, with supporters and detractors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Pioneering researchers in the field are Chen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and Abbott <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, who, in the early 1980s, proposed using syntactic features of English sentences for database modeling and program design. Abbott’s approach was subsequently adapted to a program design tool by Berry <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. These works were mostly based on extracting relevant entities from the requirements text through simple syntactic rules, assuming that NL requirements were expressed in some constrained, predictable format, which, however, is rarely the case in practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">After these pioneering works, the beginning of 1990s saw some serious attempts to develop NLP4RE tools, introducing techniques to account for the complexity and variety of NL. Two well-known NLP tools, <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">findphrases</span> by Aguilera and Berry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and OICSI by Rolland and Proix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, were the results of these efforts. Both tools were still oriented to the extraction task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, also referred as abstraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and used lexical affinity and semantic cases, respectively, two techniques that are far more sophisticated than those previously used.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">For the remaining 1990s right up to the beginning of 2000s, a succession of NL tools had been proposed, among which were AbstFinder by Goldin and Berry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, NL-OOPS by Mich <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Circe by Ambriola and Gervasi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, CM-Builder by Harmain and Gaizauskas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These works normally use traditional rule-based NLP techniques, and are oriented to term extraction and model generation. Other tools, such as QuARS by Fabbrini <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and ARM by Wilson <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, focus on defect detection and mostly use dictionary-based techniques.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The early 2000s appeared to be a period of experimentation of new NLP techniques and new ideas addressing other tasks and phases of the RE process. Information retrieval (IR) techniques were used to improve requirements tracing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, statistical NLP techniques were applied to identify “shallow knowledge” from requirements text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and to tracing relationships between requirements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Since the late 2000s, NLP4RE has become a full-fledged research area, attracting researchers from the wider RE community. A large number of tools have since been developed, among which are SREE (Tjong and Berry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>) for ambiguity detection and aToucan (Yue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>) for model generation. Further developments include tools detection of defects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, smells <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and equivalent requirements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">Given the increasing need to make software systems trustworthy, accountable, legally compliant, as well as security- and privacy-aware, NLP has been largely applied also to legal documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and privacy policies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, in the field of RE and Law. Finally, to support agile software development, requirements expressed in the form of user stories have been identified as an interesting area of application for NLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Machine Learning and Deep Learning for RE</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Following the development of successful statistical NLP methods based on ML in the 1990s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, ML techniques have become increasingly important to NLP. The advantages of the ML-based approaches over the traditional, rule-based NLP approaches are effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In RE, the earliest adoption of ML to NLP can be traced to a study by Cleland-Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, published in 2007, in which the authors presented an approach for automatically detecting and classifying non-functional requirements (NFRs) from requirements documents. The approach uses a set of weighted indicator terms to classify requirements; a probability value of each indicator term is computed by a probability function similar to Naïve Bayes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, to estimate the likelihood of an input requirement being classified into a certain NFR category. The development of this approach thus marked the beginning of the work on ML-based approaches for RE and, as a seminal work in this area, this approach has been frequently used as the baseline to assess the performance of new techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">With the recent widespread availability of NL content relevant to RE, such as feedback from users in app stores and social media, and developers’ comments in discussion forums and bug tracking systems, we have observed a rising interest in using ML techniques to support data-driven RE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and crowd-based RE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. These areas aim to leverage information available from stakeholders’ implicit and explicit feedback, including diverse sources as app reviews <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, issue tracking systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Twitter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> or user fora <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, to improve RE activities such as requirements elicitation and prioritization. Most of the works use ML techniques, as these can be effectively exploited when the task can be reduced to a classification problem, and a large amount of data is available. The analysis of different forms of feedback can be regarded as the main trend of the last years in NLP4RE research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">However, several other RE tasks have profited from ML and even DL techniques, for example: glossary extraction, with the usage of unsupervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and convolutional neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>; requirements classification with the early works from Casamayor <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and developments from Kurtanovic and Maalej  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>; requirements tracing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, which can be regarded as the field where ML/DL have been more widely experimented for traditional requirements, especially due to the inherent nature of the problem, which entails finding relevant relationship (i.e., trace links) within a large amount of potential ones.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">With the advent of DL and transfer learning in particular, initial experiments have been carried out in RE with promising results. In particular, DL-based approaches have been proposed to classify software requirements into FR or NFR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, to discovery requirements from open source issue reports <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and to extract and classify requirements from software project contracts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. We predict that research in developing DL-based approaches for RE tasks will grow rapidly in the coming years, overtaking the work on ML-based approaches.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The main source of the literature used for our data collection is the set of 404 NLP4RE studies identified in our systematic review <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The references of these papers are made available by Zhao <span id="footnote1.1" class="ltx_text ltx_font_italic">et al.</span> at: https://github.com/waadalhoshan/NLP4RE.</span></span></span>, which covers the studies up to 2019. We then performed a complementary targeted review to identify recent publications, to find more recent techniques emerging in the last 2 years. This complementary review focused on the major RE and software engineering conferences (i.e., RE, REFSQ and ICSE) and journals (i.e., REJ, JSS, ASE, DKE, IST, and TSE). Based on this updated literature, we extract NLP techniques.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To help us identify and extract NLP techniques from each paper, we followed this definition: ”<span id="S3.p2.1.1" class="ltx_text ltx_font_italic">An NLP technique is a practical method, approach, process, or procedure for performing a particular NLP task, such as POS tagging, parsing or tokenizing</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.” Our data extraction resulted in a large collection of diverse terms and phrases. To synthesize different terms and phrases into a coherent set of standard terms, we consulted many books written by NLP experts (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>). This process gave rise to a total of <span id="S3.p2.1.2" class="ltx_text ltx_font_bold">57 different NLP techniques</span>. Table <a href="#S3.T1" title="TABLE I ‣ III Method ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and Table <a href="#S3.T2" title="TABLE II ‣ III Method ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> summarize these 57 techniques.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>List of NLP Techniques (Part 1)</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p" style="width:11.4pt;"><span id="S3.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">ID</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Name</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Explanation</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1.1" class="ltx_p" style="width:11.4pt;">1</span>
</span>
</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p" style="width:113.8pt;">Part-of-Speech (POS) Tagging</span>
</span>
</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.3.1.1" class="ltx_p" style="width:313.0pt;">POS Tagging (or Tagging) processes a sequence of words, and attaches a POS tag to each word. Parts of speech are also known as word classes or lexical categories.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p" style="width:11.4pt;">2</span>
</span>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p" style="width:113.8pt;">Term Extraction</span>
</span>
</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.3.1.1" class="ltx_p" style="width:313.0pt;">The process of extracting the most relevant words and expressions from text. Related terms: Keyword Extraction, Word Extraction</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.1.1.1" class="ltx_p" style="width:11.4pt;">3</span>
</span>
</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.2.1.1" class="ltx_p" style="width:113.8pt;">Keyword Searching</span>
</span>
</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.3.1.1" class="ltx_p" style="width:313.0pt;">The technique of finding strings that match a pattern. Related terms: Term Matching, Word Matching</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.1.1.1" class="ltx_p" style="width:11.4pt;">4</span>
</span>
</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.2.1.1" class="ltx_p" style="width:113.8pt;">Chunking</span>
</span>
</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.3.1.1" class="ltx_p" style="width:313.0pt;">Chunking (or text chunking) is a type of shallow parsing that analyses a sentence by first identifying its constituent parts (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.). Related term: Shallow Parsing.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.1.1.1" class="ltx_p" style="width:11.4pt;">5</span>
</span>
</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.2.1.1" class="ltx_p" style="width:113.8pt;">Named Entity Recognition (NER)</span>
</span>
</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.3.1.1" class="ltx_p" style="width:313.0pt;">Subtask of information extraction that is based to find and classify named entities in a certain text into pre-defined categories or class such as the names of persons, organizations, locations, etc. Related terms: Entity Identification, Concept Extraction.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.1.1.1" class="ltx_p" style="width:11.4pt;">6</span>
</span>
</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.2.1.1" class="ltx_p" style="width:113.8pt;">Semantic Role Labelling (SRL)</span>
</span>
</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.3.1.1" class="ltx_p" style="width:313.0pt;">The process of detecting the semantic arguments linked with the predicate or verb of a sentence and their classification into their specific roles. Related Term: Semantic parsing, semantic trees, shallow parsing, and shallow semantic analysis.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<td id="S3.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.1.1.1" class="ltx_p" style="width:11.4pt;">7</span>
</span>
</td>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.2.1.1" class="ltx_p" style="width:113.8pt;">Temporal Tagging</span>
</span>
</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.3.1.1" class="ltx_p" style="width:313.0pt;">The task of finding phrases with temporal meaning within the context of a larger document.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.1.1.1" class="ltx_p" style="width:11.4pt;">8</span>
</span>
</td>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.2.1.1" class="ltx_p" style="width:113.8pt;">Dependency Parsing</span>
</span>
</td>
<td id="S3.T1.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.3.1.1" class="ltx_p" style="width:313.0pt;">Dependency parsing is the process of analyzing the grammatical structure of a sentence based on the dependencies between the words in a sentence. Related terms: Syntactic Patterns, Syntactic Structure</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<td id="S3.T1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.9.1.1.1" class="ltx_p" style="width:11.4pt;">9</span>
</span>
</td>
<td id="S3.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.9.2.1.1" class="ltx_p" style="width:113.8pt;">Constituency Parsing</span>
</span>
</td>
<td id="S3.T1.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.9.3.1.1" class="ltx_p" style="width:313.0pt;">The process of analyzing the sentences by breaking down it into sub-phrases also known as constituents. These sub-phrases belong to a specific category of grammar like NP (noun phrase) and VP(verb phrase). Related terms: Phrase Parsing, Phrase Detection, Phrasal Verb Extraction</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.11.10" class="ltx_tr">
<td id="S3.T1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.10.1.1.1" class="ltx_p" style="width:11.4pt;">10</span>
</span>
</td>
<td id="S3.T1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.10.2.1.1" class="ltx_p" style="width:113.8pt;">Link Grammar</span>
</span>
</td>
<td id="S3.T1.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.10.3.1.1" class="ltx_p" style="width:313.0pt;">Builds relations between pairs of words, rather than constructing constituents in a phrase structure hierarchy.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.12.11" class="ltx_tr">
<td id="S3.T1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.11.1.1.1" class="ltx_p" style="width:11.4pt;">11</span>
</span>
</td>
<td id="S3.T1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.11.2.1.1" class="ltx_p" style="width:113.8pt;">Semantic Parsing</span>
</span>
</td>
<td id="S3.T1.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.11.3.1.1" class="ltx_p" style="width:313.0pt;">The task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.13.12" class="ltx_tr">
<td id="S3.T1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.12.1.1.1" class="ltx_p" style="width:11.4pt;">12</span>
</span>
</td>
<td id="S3.T1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.12.2.1.1" class="ltx_p" style="width:113.8pt;">Sentiment Analysis</span>
</span>
</td>
<td id="S3.T1.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.12.3.1.1" class="ltx_p" style="width:313.0pt;">The process of computationally identifying and categorizing opinions expressed in a piece of text</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.14.13" class="ltx_tr">
<td id="S3.T1.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.13.1.1.1" class="ltx_p" style="width:11.4pt;">13</span>
</span>
</td>
<td id="S3.T1.1.14.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.13.2.1.1" class="ltx_p" style="width:113.8pt;">Text Annotation</span>
</span>
</td>
<td id="S3.T1.1.14.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.13.3.1.1" class="ltx_p" style="width:313.0pt;">The practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.15.14" class="ltx_tr">
<td id="S3.T1.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.14.1.1.1" class="ltx_p" style="width:11.4pt;">14</span>
</span>
</td>
<td id="S3.T1.1.15.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.14.2.1.1" class="ltx_p" style="width:113.8pt;">Semantic Annotation</span>
</span>
</td>
<td id="S3.T1.1.15.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.14.3.1.1" class="ltx_p" style="width:313.0pt;">The process of attaching to a text document or other unstructured content, metadata about concepts (e.g., people, places, organizations, products or topics) relevant to it.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.16.15" class="ltx_tr">
<td id="S3.T1.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.16.15.1.1.1" class="ltx_p" style="width:11.4pt;">15</span>
</span>
</td>
<td id="S3.T1.1.16.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.16.15.2.1.1" class="ltx_p" style="width:113.8pt;">Topic Modelling</span>
</span>
</td>
<td id="S3.T1.1.16.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.16.15.3.1.1" class="ltx_p" style="width:313.0pt;">A type of statistical model for discovering the abstract ”topics” that occur in a collection of documents</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.17.16" class="ltx_tr">
<td id="S3.T1.1.17.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.17.16.1.1.1" class="ltx_p" style="width:11.4pt;">16</span>
</span>
</td>
<td id="S3.T1.1.17.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.17.16.2.1.1" class="ltx_p" style="width:113.8pt;">Summarization</span>
</span>
</td>
<td id="S3.T1.1.17.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.17.16.3.1.1" class="ltx_p" style="width:313.0pt;">The practice of breaking down long publications into manageable paragraphs or sentences. The procedure extracts important information while also ensuring that the paragraph’s sense is preserved.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.18.17" class="ltx_tr">
<td id="S3.T1.1.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.18.17.1.1.1" class="ltx_p" style="width:11.4pt;">17</span>
</span>
</td>
<td id="S3.T1.1.18.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.18.17.2.1.1" class="ltx_p" style="width:113.8pt;">Latent Dirichlet Allocation (LDA)</span>
</span>
</td>
<td id="S3.T1.1.18.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.18.17.3.1.1" class="ltx_p" style="width:313.0pt;">The process of analysing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.19.18" class="ltx_tr">
<td id="S3.T1.1.19.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.19.18.1.1.1" class="ltx_p" style="width:11.4pt;">18</span>
</span>
</td>
<td id="S3.T1.1.19.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.19.18.2.1.1" class="ltx_p" style="width:113.8pt;">Latent Semantic Indexing (LSI)</span>
</span>
</td>
<td id="S3.T1.1.19.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.19.18.3.1.1" class="ltx_p" style="width:313.0pt;">A mathematical practice that helps classify and retrieve information on particular key terms and concepts using singular value decomposition (SVD). Related Term: Latent Semantic Analysis (LSA)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.20.19" class="ltx_tr">
<td id="S3.T1.1.20.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.20.19.1.1.1" class="ltx_p" style="width:11.4pt;">19</span>
</span>
</td>
<td id="S3.T1.1.20.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.20.19.2.1.1" class="ltx_p" style="width:113.8pt;">Semantic Patterns</span>
</span>
</td>
<td id="S3.T1.1.20.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.20.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.20.19.3.1.1" class="ltx_p" style="width:313.0pt;">Semantic patterns are generated based on common matching concepts. The top matching concepts of each word are considered. One semantic pattern can relate to several concepts and a single semantic clique can contain several semantic patterns.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.21.20" class="ltx_tr">
<td id="S3.T1.1.21.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.21.20.1.1.1" class="ltx_p" style="width:11.4pt;">20</span>
</span>
</td>
<td id="S3.T1.1.21.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.21.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.21.20.2.1.1" class="ltx_p" style="width:113.8pt;">Case Grammar</span>
</span>
</td>
<td id="S3.T1.1.21.20.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.21.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.21.20.3.1.1" class="ltx_p" style="width:313.0pt;">A system of linguistic analysis, focusing on the link between the valence, or number of subjects, objects, etc., of a verb and the grammatical context it requires.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.22.21" class="ltx_tr">
<td id="S3.T1.1.22.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.22.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.22.21.1.1.1" class="ltx_p" style="width:11.4pt;">21</span>
</span>
</td>
<td id="S3.T1.1.22.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.22.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.22.21.2.1.1" class="ltx_p" style="width:113.8pt;">Semantic Frames</span>
</span>
</td>
<td id="S3.T1.1.22.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.22.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.22.21.3.1.1" class="ltx_p" style="width:313.0pt;">A coherent structure of concepts that are related such that without knowledge of all of them, one does not have complete knowledge of one of the either.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.23.22" class="ltx_tr">
<td id="S3.T1.1.23.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.23.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.23.22.1.1.1" class="ltx_p" style="width:11.4pt;">22</span>
</span>
</td>
<td id="S3.T1.1.23.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.23.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.23.22.2.1.1" class="ltx_p" style="width:113.8pt;">Knowledge Graph</span>
</span>
</td>
<td id="S3.T1.1.23.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.23.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.23.22.3.1.1" class="ltx_p" style="width:313.0pt;">A way of storing data that resulted from an information extraction task.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.24.23" class="ltx_tr">
<td id="S3.T1.1.24.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.24.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.24.23.1.1.1" class="ltx_p" style="width:11.4pt;">23</span>
</span>
</td>
<td id="S3.T1.1.24.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.24.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.24.23.2.1.1" class="ltx_p" style="width:113.8pt;">Bag-of-Words (BOW)</span>
</span>
</td>
<td id="S3.T1.1.24.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.24.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.24.23.3.1.1" class="ltx_p" style="width:313.0pt;">A representation that turns arbitrary text into fixed-length vectors by counting how many times each word appears. This process is often referred to as vectorization.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.25.24" class="ltx_tr">
<td id="S3.T1.1.25.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.25.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.25.24.1.1.1" class="ltx_p" style="width:11.4pt;">24</span>
</span>
</td>
<td id="S3.T1.1.25.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.25.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.25.24.2.1.1" class="ltx_p" style="width:113.8pt;">Word Frequency</span>
</span>
</td>
<td id="S3.T1.1.25.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.25.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.25.24.3.1.1" class="ltx_p" style="width:313.0pt;">How often a word appears in a document, divided by how many words there are. Related Terms: Term Frequency, Domain Term Frequency</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.26.25" class="ltx_tr">
<td id="S3.T1.1.26.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.26.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.26.25.1.1.1" class="ltx_p" style="width:11.4pt;">25</span>
</span>
</td>
<td id="S3.T1.1.26.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.26.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.26.25.2.1.1" class="ltx_p" style="width:113.8pt;">Term Frequency-Inverse Document Frequency (TF-IDF)</span>
</span>
</td>
<td id="S3.T1.1.26.25.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.26.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.26.25.3.1.1" class="ltx_p" style="width:313.0pt;">A statistical measure that evaluates how relevant a word is to a document in a collection of documents.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.27.26" class="ltx_tr">
<td id="S3.T1.1.27.26.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.27.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.27.26.1.1.1" class="ltx_p" style="width:11.4pt;">26</span>
</span>
</td>
<td id="S3.T1.1.27.26.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.27.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.27.26.2.1.1" class="ltx_p" style="width:113.8pt;">Co-location Analysis</span>
</span>
</td>
<td id="S3.T1.1.27.26.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.27.26.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.27.26.3.1.1" class="ltx_p" style="width:313.0pt;">A Co-location is an expression consisting of two or more words that correspond to some conventional way of saying things.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.28.27" class="ltx_tr">
<td id="S3.T1.1.28.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.28.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.28.27.1.1.1" class="ltx_p" style="width:11.4pt;">27</span>
</span>
</td>
<td id="S3.T1.1.28.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.28.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.28.27.2.1.1" class="ltx_p" style="width:113.8pt;">Term-Document Matrix</span>
</span>
</td>
<td id="S3.T1.1.28.27.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.28.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.28.27.3.1.1" class="ltx_p" style="width:313.0pt;">A mathematical matrix that describes the frequency of terms that occur in a collection of documents.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.29.28" class="ltx_tr">
<td id="S3.T1.1.29.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.29.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.29.28.1.1.1" class="ltx_p" style="width:11.4pt;">28</span>
</span>
</td>
<td id="S3.T1.1.29.28.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.29.28.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.29.28.2.1.1" class="ltx_p" style="width:113.8pt;">Character Counting</span>
</span>
</td>
<td id="S3.T1.1.29.28.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.29.28.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.29.28.3.1.1" class="ltx_p" style="width:313.0pt;">Counts the number of characters in a line of text, page or group of text.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.30.29" class="ltx_tr">
<td id="S3.T1.1.30.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.30.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.30.29.1.1.1" class="ltx_p" style="width:11.4pt;">29</span>
</span>
</td>
<td id="S3.T1.1.30.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.30.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.30.29.2.1.1" class="ltx_p" style="width:113.8pt;">Concordance</span>
</span>
</td>
<td id="S3.T1.1.30.29.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.30.29.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.30.29.3.1.1" class="ltx_p" style="width:313.0pt;">An alphabetical list of the words (especially the important ones) present in a text, usually with citations of the passages in which they are found.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.31.30" class="ltx_tr">
<td id="S3.T1.1.31.30.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.1.31.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.31.30.1.1.1" class="ltx_p" style="width:11.4pt;">30</span>
</span>
</td>
<td id="S3.T1.1.31.30.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.1.31.30.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.31.30.2.1.1" class="ltx_p" style="width:113.8pt;">Cosine Similarity</span>
</span>
</td>
<td id="S3.T1.1.31.30.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.31.30.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.31.30.3.1.1" class="ltx_p" style="width:313.0pt;">A metric used to measure how similar the documents are irrespective of their size.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>List of NLP Techniques (Part 2)</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_p" style="width:11.4pt;"><span id="S3.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">ID</span></span>
</span>
</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Name</span></span>
</span>
</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="S3.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Explanation</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1.1" class="ltx_p" style="width:11.4pt;">31</span>
</span>
</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.2.1.1" class="ltx_p" style="width:113.8pt;">Lexical Affinity</span>
</span>
</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.3.1.1" class="ltx_p" style="width:313.0pt;">Assigns to arbitrary words a probabilistic ’affinity’ for a particular category.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1.1" class="ltx_p" style="width:11.4pt;">32</span>
</span>
</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.2.1.1" class="ltx_p" style="width:113.8pt;">Similarity Distance</span>
</span>
</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.3.1.1" class="ltx_p" style="width:313.0pt;">Determines the minimum number of single character edits required to change one word to another.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.1.1.1" class="ltx_p" style="width:11.4pt;">33</span>
</span>
</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.2.1.1" class="ltx_p" style="width:113.8pt;">Document Similarity</span>
</span>
</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.3.1.1" class="ltx_p" style="width:313.0pt;">Computing the similarity between two text documents by transforming the input documents into real-valued vectors.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.1.1.1" class="ltx_p" style="width:11.4pt;">34</span>
</span>
</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.2.1.1" class="ltx_p" style="width:113.8pt;">Lexical Similarity</span>
</span>
</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.3.1.1" class="ltx_p" style="width:313.0pt;">Provides a measure of the similarity of two texts based on the intersection of the word sets of same or different languages.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.1.1.1" class="ltx_p" style="width:11.4pt;">35</span>
</span>
</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.2.1.1" class="ltx_p" style="width:113.8pt;">Regular Expression</span>
</span>
</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.3.1.1" class="ltx_p" style="width:313.0pt;">A special series of strings for describing a a text pattern for the purpose of searching or replacing the described items.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<td id="S3.T2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.1.1.1" class="ltx_p" style="width:11.4pt;">36</span>
</span>
</td>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.2.1.1" class="ltx_p" style="width:113.8pt;">Lexical Patterns</span>
</span>
</td>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.3.1.1" class="ltx_p" style="width:313.0pt;">Words or chuck of text that occurs in language with high frequency and the meaning of the parts are sometime different than the meaning of the whole.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<td id="S3.T2.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.1.1.1" class="ltx_p" style="width:11.4pt;">37</span>
</span>
</td>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.2.1.1" class="ltx_p" style="width:113.8pt;">Generation Rules</span>
</span>
</td>
<td id="S3.T2.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.3.1.1" class="ltx_p" style="width:313.0pt;">Generation rules to produce meaningful sentences in Natural Language.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.9.8" class="ltx_tr">
<td id="S3.T2.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.9.8.1.1.1" class="ltx_p" style="width:11.4pt;">38</span>
</span>
</td>
<td id="S3.T2.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.9.8.2.1.1" class="ltx_p" style="width:113.8pt;">Stemming</span>
</span>
</td>
<td id="S3.T2.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.9.8.3.1.1" class="ltx_p" style="width:313.0pt;">A crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.10.9" class="ltx_tr">
<td id="S3.T2.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.10.9.1.1.1" class="ltx_p" style="width:11.4pt;">39</span>
</span>
</td>
<td id="S3.T2.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.10.9.2.1.1" class="ltx_p" style="width:113.8pt;">Lemmatization</span>
</span>
</td>
<td id="S3.T2.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.10.9.3.1.1" class="ltx_p" style="width:313.0pt;">Use a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.11.10" class="ltx_tr">
<td id="S3.T2.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.11.10.1.1.1" class="ltx_p" style="width:11.4pt;">40</span>
</span>
</td>
<td id="S3.T2.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.11.10.2.1.1" class="ltx_p" style="width:113.8pt;">Stop-Word Removal</span>
</span>
</td>
<td id="S3.T2.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.11.10.3.1.1" class="ltx_p" style="width:313.0pt;">Words which are filtered out before or after processing of natural language data (text).</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.12.11" class="ltx_tr">
<td id="S3.T2.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.12.11.1.1.1" class="ltx_p" style="width:11.4pt;">41</span>
</span>
</td>
<td id="S3.T2.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.12.11.2.1.1" class="ltx_p" style="width:113.8pt;">Noise Removal</span>
</span>
</td>
<td id="S3.T2.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.12.11.3.1.1" class="ltx_p" style="width:313.0pt;">Removing characters digits and pieces of text that can interfere with your text analysis.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.13.12" class="ltx_tr">
<td id="S3.T2.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.13.12.1.1.1" class="ltx_p" style="width:11.4pt;">42</span>
</span>
</td>
<td id="S3.T2.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.13.12.2.1.1" class="ltx_p" style="width:113.8pt;">Punctuation Removal</span>
</span>
</td>
<td id="S3.T2.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.13.12.3.1.1" class="ltx_p" style="width:313.0pt;">Removing puncuatations marks.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.14.13" class="ltx_tr">
<td id="S3.T2.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.14.13.1.1.1" class="ltx_p" style="width:11.4pt;">43</span>
</span>
</td>
<td id="S3.T2.1.14.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.14.13.2.1.1" class="ltx_p" style="width:113.8pt;">Lowercasing</span>
</span>
</td>
<td id="S3.T2.1.14.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.14.13.3.1.1" class="ltx_p" style="width:313.0pt;">Converting all your data to lowercase helps in the process of preprocessing and in later stages in the NLP application, when you are doing parsing.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.15.14" class="ltx_tr">
<td id="S3.T2.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.15.14.1.1.1" class="ltx_p" style="width:11.4pt;">44</span>
</span>
</td>
<td id="S3.T2.1.15.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.15.14.2.1.1" class="ltx_p" style="width:113.8pt;">Camel Case Splitting</span>
</span>
</td>
<td id="S3.T2.1.15.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.15.14.3.1.1" class="ltx_p" style="width:313.0pt;">Split CamelCase string to individual strings.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.16.15" class="ltx_tr">
<td id="S3.T2.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.16.15.1.1.1" class="ltx_p" style="width:11.4pt;">45</span>
</span>
</td>
<td id="S3.T2.1.16.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.16.15.2.1.1" class="ltx_p" style="width:113.8pt;">Tokenization</span>
</span>
</td>
<td id="S3.T2.1.16.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.16.15.3.1.1" class="ltx_p" style="width:313.0pt;">The process of breaking a stream of text into words, phrases, symbols, or other meaningful tokens. Related terms: Word Segmentation</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.17.16" class="ltx_tr">
<td id="S3.T2.1.17.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.17.16.1.1.1" class="ltx_p" style="width:11.4pt;">46</span>
</span>
</td>
<td id="S3.T2.1.17.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.17.16.2.1.1" class="ltx_p" style="width:113.8pt;">Sentence Segmentation</span>
</span>
</td>
<td id="S3.T2.1.17.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.17.16.3.1.1" class="ltx_p" style="width:313.0pt;">Split a document into sentences, each containing a list of tokens. Related terms: Sentence Splitting</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.18.17" class="ltx_tr">
<td id="S3.T2.1.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.18.17.1.1.1" class="ltx_p" style="width:11.4pt;">47</span>
</span>
</td>
<td id="S3.T2.1.18.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.18.17.2.1.1" class="ltx_p" style="width:113.8pt;">n-gram</span>
</span>
</td>
<td id="S3.T2.1.18.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.18.17.3.1.1" class="ltx_p" style="width:313.0pt;">A representation of a text using a sequence of N words or N characters (character n-gram), where N can be any number. Thus we can have 1-gram (unigram), 2-gram (bigram), 3-gram (trigram), etc.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.19.18" class="ltx_tr">
<td id="S3.T2.1.19.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.19.18.1.1.1" class="ltx_p" style="width:11.4pt;">48</span>
</span>
</td>
<td id="S3.T2.1.19.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.19.18.2.1.1" class="ltx_p" style="width:113.8pt;">Word Embedding</span>
</span>
</td>
<td id="S3.T2.1.19.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.19.18.3.1.1" class="ltx_p" style="width:313.0pt;">One of the most popular technique to learn word embeddings using shallow neural network. Word embeddings are vector representations of a particular word. Related terms: Word2Vec</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.20.19" class="ltx_tr">
<td id="S3.T2.1.20.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.20.19.1.1.1" class="ltx_p" style="width:11.4pt;">49</span>
</span>
</td>
<td id="S3.T2.1.20.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.20.19.2.1.1" class="ltx_p" style="width:113.8pt;">Contextualized word embedding</span>
</span>
</td>
<td id="S3.T2.1.20.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.20.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.20.19.3.1.1" class="ltx_p" style="width:313.0pt;">A neural model that learns a generic embedding function for variable length contexts of target words. Related terms: Context2Vec</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.21.20" class="ltx_tr">
<td id="S3.T2.1.21.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.21.20.1.1.1" class="ltx_p" style="width:11.4pt;">50</span>
</span>
</td>
<td id="S3.T2.1.21.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.21.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.21.20.2.1.1" class="ltx_p" style="width:113.8pt;">Sentence and document Embedding</span>
</span>
</td>
<td id="S3.T2.1.21.20.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.21.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.21.20.3.1.1" class="ltx_p" style="width:313.0pt;">A generalized word2vec method, for representing documents as a vector. Related term: Doc2Vec</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.22.21" class="ltx_tr">
<td id="S3.T2.1.22.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.22.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.22.21.1.1.1" class="ltx_p" style="width:11.4pt;">51</span>
</span>
</td>
<td id="S3.T2.1.22.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.22.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.22.21.2.1.1" class="ltx_p" style="width:113.8pt;">GloVe</span>
</span>
</td>
<td id="S3.T2.1.22.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.22.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.22.21.3.1.1" class="ltx_p" style="width:313.0pt;">An alternative to word2vec for the representation of the distributed words.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.23.22" class="ltx_tr">
<td id="S3.T2.1.23.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.23.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.23.22.1.1.1" class="ltx_p" style="width:11.4pt;">52</span>
</span>
</td>
<td id="S3.T2.1.23.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.23.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.23.22.2.1.1" class="ltx_p" style="width:113.8pt;">FastText</span>
</span>
</td>
<td id="S3.T2.1.23.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.23.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.23.22.3.1.1" class="ltx_p" style="width:313.0pt;">An alternative to word2vec, FastText represents each word as a bag of character n-gram.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.24.23" class="ltx_tr">
<td id="S3.T2.1.24.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.24.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.24.23.1.1.1" class="ltx_p" style="width:11.4pt;">53</span>
</span>
</td>
<td id="S3.T2.1.24.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.24.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.24.23.2.1.1" class="ltx_p" style="width:113.8pt;">Textual Entailment Recognition</span>
</span>
</td>
<td id="S3.T2.1.24.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.24.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.24.23.3.1.1" class="ltx_p" style="width:313.0pt;">Deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.25.24" class="ltx_tr">
<td id="S3.T2.1.25.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.25.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.25.24.1.1.1" class="ltx_p" style="width:11.4pt;">54</span>
</span>
</td>
<td id="S3.T2.1.25.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.25.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.25.24.2.1.1" class="ltx_p" style="width:113.8pt;">Homonym Detection</span>
</span>
</td>
<td id="S3.T2.1.25.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.25.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.25.24.3.1.1" class="ltx_p" style="width:313.0pt;">Detecting the words that are pronounced the same as each other (e.g., ”maid” and ”made”) or have the same spelling (e.g., ”lead weight” and ”to lead”).</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.26.25" class="ltx_tr">
<td id="S3.T2.1.26.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.26.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.26.25.1.1.1" class="ltx_p" style="width:11.4pt;">55</span>
</span>
</td>
<td id="S3.T2.1.26.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.26.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.26.25.2.1.1" class="ltx_p" style="width:113.8pt;">Synonym Detection</span>
</span>
</td>
<td id="S3.T2.1.26.25.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.26.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.26.25.3.1.1" class="ltx_p" style="width:313.0pt;">Finding a a word or phrase that means exactly or nearly the same as another word or phrase in a text.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.27.26" class="ltx_tr">
<td id="S3.T2.1.27.26.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.27.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.27.26.1.1.1" class="ltx_p" style="width:11.4pt;">56</span>
</span>
</td>
<td id="S3.T2.1.27.26.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.27.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.27.26.2.1.1" class="ltx_p" style="width:113.8pt;">Coreference Resolution</span>
</span>
</td>
<td id="S3.T2.1.27.26.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.27.26.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.27.26.3.1.1" class="ltx_p" style="width:313.0pt;">Finding all expressions that refer to the same entity in a discourse.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.28.27" class="ltx_tr">
<td id="S3.T2.1.28.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.28.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.28.27.1.1.1" class="ltx_p" style="width:11.4pt;">57</span>
</span>
</td>
<td id="S3.T2.1.28.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.28.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.28.27.2.1.1" class="ltx_p" style="width:113.8pt;">Anaphora Resolution</span>
</span>
</td>
<td id="S3.T2.1.28.27.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T2.1.28.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.28.27.3.1.1" class="ltx_p" style="width:313.0pt;">Resolving what a pronoun, or a noun phrase refers to in a discourse.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Classifying NLP Techniques by Tasks</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first classify the NLP techniques based on their text processing tasks. Figure <a href="#S4.F1" title="Figure 1 ‣ IV Classifying NLP Techniques by Tasks ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the relationship between NLP techniques, NLP tasks, NLP resources, and tools. We define <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">a NLP task as a piece of text processing work that can be done by means of one or more NLP techniques, supported by some NLP tools and resources</span>. A list of frequently performed NLP tasks in RE are desribed below:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Part-of-Speech (POS) Tagging:</span> To associate words with part-of-speech (POS) tags to distinguish between nouns, verbs, adjectives, adverbs, etc. The input unit is a sentence, as context words (i.e., neighbouring ones) are normally used to infer the POS of a word.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Semantic Tagging:</span> To extract useful bits of information (words, terms, relations, etc.) from the text.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Syntactic Analysis:</span> To analyze the syntactic structure of a sentence to represent the relationship between its components. Different representation structures can be used, such as the parse tree, or the dependency parsing graph.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Semantic Analysis:</span> To identify and label semantically relevant components and relations in the text. These entails identifying the meaning of a certain word or phrase in a context and the relationship between words or terms.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Frequency Analysis:</span> To analyze the frequencies of words or terms in a certain context and to produce probabilistic data.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Similarity Analysis:</span> To calculate the numerical estimates of similarity between text elements, for example to identify semantic relatedness, synonyms, or to support topic modelling.</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p"><span id="S4.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Rule-Based Analysis:</span> To use grammar rules, semantic rules or patterns to analyse the syntax of a text.</p>
</div>
</li>
<li id="S4.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i8.p1" class="ltx_para">
<p id="S4.I1.i8.p1.1" class="ltx_p"><span id="S4.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">Text Normalization:</span> To convert the words into their original form and remove
unnecessary words or characters from the text.</p>
</div>
</li>
<li id="S4.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i9.p1" class="ltx_para">
<p id="S4.I1.i9.p1.1" class="ltx_p"><span id="S4.I1.i9.p1.1.1" class="ltx_text ltx_font_bold">Text Segmentation:</span> To break down a text into a sequence of individual sentences or words.</p>
</div>
</li>
<li id="S4.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i10.p1" class="ltx_para">
<p id="S4.I1.i10.p1.1" class="ltx_p"><span id="S4.I1.i10.p1.1.1" class="ltx_text ltx_font_bold">Text Normalization:</span> To reduce the words to a standardised format, with the removal of stop words, and reduction of typographical forms (e.g., upper case, camel case) to a unique form.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2204.04282/assets/Figures/NLPTasks.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Relationship between NLP techniques and NLP Tasks.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Classifying NLP Techniques by Tasks.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S4.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">NLP Tasks</span></span>
</span>
</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.2.1.1" class="ltx_p" style="width:156.5pt;"><span id="S4.T3.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Explanation</span></span>
</span>
</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T3.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">NLP Techniques</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Part-of-Speech Tagging</span></span>
</span>
</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.2.1.1" class="ltx_p" style="width:156.5pt;">Associate words with part-of-speech (POS) tags to distinguish between nouns, verbs, adjectives, adverbs, etc.</span>
</span>
</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.3.1.1" class="ltx_p" style="width:213.4pt;">Part-of-Speech (POS) Tagging</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Semantic Tagging</span></span>
</span>
</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.2.1.1" class="ltx_p" style="width:156.5pt;">Extract useful bits of information (words, terms, relations, etc.) from the text.</span>
</span>
</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.3.1.1" class="ltx_p" style="width:213.4pt;">Term Extraction, Term Matching, Chunking, Concept Extraction, Named Entity Recognition (NER), Semantic Role Labelling (SRL), Temporal Tagging</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.4.3.1.1.1.1" class="ltx_text ltx_font_bold">Syntactic Analysis</span></span>
</span>
</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.2.1.1" class="ltx_p" style="width:156.5pt;">Construct a syntactic structure representing the relationship between the logical components in a stream of text, such as the parse tree, or the dependency parsing graph.</span>
</span>
</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.3.1.1" class="ltx_p" style="width:213.4pt;">Dependency Parsing, Constituency Parsing, Link Grammar</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.5.4.1.1.1.1" class="ltx_text ltx_font_bold">Semantic Analysis</span></span>
</span>
</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.2.1.1" class="ltx_p" style="width:156.5pt;">Identify and label semantically relevant components and relations in the text.</span>
</span>
</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.3.1.1" class="ltx_p" style="width:213.4pt;">Semantic Parsing, Sentiment Analysis, Text Annotation, Semantic Annotation, Topic Modelling, Summarization, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing (LSI), Semantic Patterns, Case Grammar, Semantic Frames, Knowledge Graph, Textual Entailment Recognition (TER), Homonym Detection, Synonym Detection, Coreference Resolution, Anaphora Resolution</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<td id="S4.T3.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.5.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.6.5.1.1.1.1" class="ltx_text ltx_font_bold">Frequency Analysis</span></span>
</span>
</td>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.5.2.1.1" class="ltx_p" style="width:156.5pt;">Analyse the frequency of occurrence of lexical elements (e.g., words and characters) and groups of elements (e.g., phrases and multiwords) in a given text.</span>
</span>
</td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.5.3.1.1" class="ltx_p" style="width:213.4pt;">Bag-of-Words (BOW), Word Frequency, Term Frequency (TF), Term Frequency &amp; Inverse Document Frequency (TF-IDF), Co-location Analysis, Term-Document Matrix, Character Counting, Concordance</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<td id="S4.T3.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.7.6.1.1.1.1" class="ltx_text ltx_font_bold">Similarity Analysis</span></span>
</span>
</td>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.2.1.1" class="ltx_p" style="width:156.5pt;">Calculate numerical values of the similarity between text elements, such as to identify semantic relatedness.</span>
</span>
</td>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.3.1.1" class="ltx_p" style="width:213.4pt;">Cosine Similarity, Lexical Affinity, Similarity Distance, Document Similarity, Lexical Similarity</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.8.7" class="ltx_tr">
<td id="S4.T3.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.8.7.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.8.7.1.1.1.1" class="ltx_text ltx_font_bold">Rule-Based Analysis</span></span>
</span>
</td>
<td id="S4.T3.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.8.7.2.1.1" class="ltx_p" style="width:156.5pt;">Use rules or patterns to analyse the syntax or semantics of a text or transform the text.</span>
</span>
</td>
<td id="S4.T3.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.8.7.3.1.1" class="ltx_p" style="width:213.4pt;">Regular Expression, Lexical Patterns, Generation Rules</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.9.8" class="ltx_tr">
<td id="S4.T3.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.9.8.1.1.1.1" class="ltx_text ltx_font_bold">Text Normalization</span></span>
</span>
</td>
<td id="S4.T3.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.2.1.1" class="ltx_p" style="width:156.5pt;">Convert the words into their original form and remove unnecessary words or characters from the text.</span>
</span>
</td>
<td id="S4.T3.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.3.1.1" class="ltx_p" style="width:213.4pt;">Stemming, Lemmatization, Stop-Word Removal, Noise Removal, Punctuation Removal, Lowercasing, Camel Case Splitting</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.10.9" class="ltx_tr">
<td id="S4.T3.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.10.9.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.10.9.1.1.1.1" class="ltx_text ltx_font_bold">Text Segmentation</span></span>
</span>
</td>
<td id="S4.T3.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.10.9.2.1.1" class="ltx_p" style="width:156.5pt;">Break down a text into a sequence of individual tokens (i.e., words or sentences).</span>
</span>
</td>
<td id="S4.T3.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.10.9.3.1.1" class="ltx_p" style="width:213.4pt;">Tokenization, Sentence Segmentation</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.11.10" class="ltx_tr">
<td id="S4.T3.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.11.10.1.1.1" class="ltx_p" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S4.T3.1.11.10.1.1.1.1" class="ltx_text ltx_font_bold">Text Representation</span></span>
</span>
</td>
<td id="S4.T3.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.11.10.2.1.1" class="ltx_p" style="width:156.5pt;">Represent words, sentences or documents using vectors of real numbers.</span>
</span>
</td>
<td id="S4.T3.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.T3.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.11.10.3.1.1" class="ltx_p" style="width:213.4pt;">N-gram, Word2Vec, Context2Vec, Doc2Vec, GloVe, FastText</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Table <a href="#S4.T3" title="TABLE III ‣ IV Classifying NLP Techniques by Tasks ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the classification results of the NLP techniques for RE based on these tasks.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Classifying NLP Techniques by Linguistic Analysis Levels</span>
</h2>

<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Classifying NLP Techniques by Levels of Analysis.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S5.T4.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Analysis Level</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T4.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Explanation</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S5.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="S5.T4.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">NLP Techniques</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<td id="S5.T4.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S5.T4.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Morphology</span></span>
</span>
</td>
<td id="S5.T4.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.1.2.1.1" class="ltx_p" style="width:213.4pt;">This is the lowest level of text analysis, dealing with the smallest parts of words that carry meaning. All the techniques used for text normalization belong to this category.</span>
</span>
</td>
<td id="S5.T4.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">Stemming, Lemmatization, Stop-Word Removal, Noise Removal, Punctuation Removal, Lowercasing, Camel Case Splitting</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<td id="S5.T4.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.2.1.1.1" class="ltx_p" style="width:56.9pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S5.T4.1.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Lexical</span></span>
</span>
</td>
<td id="S5.T4.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.2.2.1.1" class="ltx_p" style="width:213.4pt;">This is the word-level of text analysis, interpreting the meaning of individual words to gain word-level understanding. All the techniques used for frequency analysis belong to this category. In addition, Tokenization and n-gram should also be in this category.</span>
</span>
</td>
<td id="S5.T4.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">BOW, TF, TF-IDF, Co-location Analysis, Term-Document Matrix, Character Counting, Concordance, n-gram</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.4.3" class="ltx_tr">
<td id="S5.T4.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.3.1.1.1" class="ltx_p" style="width:56.9pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S5.T4.1.4.3.1.1.1.1" class="ltx_text ltx_font_bold">Syntactic</span></span>
</span>
</td>
<td id="S5.T4.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.3.2.1.1" class="ltx_p" style="width:213.4pt;">This level focuses on analyzing
the words in a sentence through the grammatical structure of the sentence. All the techniques used for syntactic analysis belong to this category. In addition, the techniques used for text segmentation and Regular Expression for Rule-Based Analysis should also belong to this category.</span>
</span>
</td>
<td id="S5.T4.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">POS Tagging, Dependency Parsing, Constituency Parsing, Link Grammar, Regular Expression, Tokenization, Sentence Segmentation</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.5.4" class="ltx_tr">
<td id="S5.T4.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.4.1.1.1" class="ltx_p" style="width:56.9pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span></span>
<span id="S5.T4.1.5.4.1.1.2" class="ltx_p ltx_align_left"><span id="S5.T4.1.5.4.1.1.2.1" class="ltx_text ltx_font_bold">Semantic (Word-Level)</span></span>
</span>
</td>
<td id="S5.T4.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.4.2.1.1" class="ltx_p" style="width:213.4pt;">We split the semantic level into word-level semantic and sentence-level semantic. The word-level semantic focuses on the meanings of individual words (e.g., dictionary definitions of words and word-sense disambiguation). Most techniques used for semantic tagging and similarly analysis belong to this level. In addition, apart from n-gram, the techniques used for text representation belong to this category.</span>
</span>
</td>
<td id="S5.T4.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">Term Extraction, Keyword Searching, Chunking, NER, Temporal Tagging, Lexical Patterns, Cosine Similarity, Lexical Affinity, Similarity Distance, Document Similarity, Lexical Similarity, Word2Vec, Context2Vec, Doc2Vec, GloVe, FastText</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.6.5" class="ltx_tr">
<td id="S5.T4.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.5.1.1.1" class="ltx_p" style="width:56.9pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span></span>
<span id="S5.T4.1.6.5.1.1.2" class="ltx_p ltx_align_left"><span id="S5.T4.1.6.5.1.1.2.1" class="ltx_text ltx_font_bold">Semantic (Sentence-Level)</span></span>
</span>
</td>
<td id="S5.T4.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.5.2.1.1" class="ltx_p" style="width:213.4pt;">This level deals with the compositional semantics, which looks at the interactions among word-level meanings in sentences (e.g., semantic role labeling). Most techniques used for semantic analysis belong to this category. In addition, SRL and most techniques for disambiguation should also belong to this category.</span>
</span>
</td>
<td id="S5.T4.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">Semantic Parsing, Sentiment Analysis, Text Annotation, Semantic Annotation, Topic Modelling, SRL, Summarization, LDA, LSI, Semantic Patterns, Case Grammar, Semantic Frames, Knowledge Graph, TER, Homonym Detection, Synonymy Detection</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.7.6" class="ltx_tr">
<td id="S5.T4.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T4.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.6.1.1.1" class="ltx_p" style="width:56.9pt;"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="S5.T4.1.7.6.1.1.1.1" class="ltx_text ltx_font_bold">Discourse</span></span>
</span>
</td>
<td id="S5.T4.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T4.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.6.2.1.1" class="ltx_p" style="width:213.4pt;">This level focuses on the properties of the text as a whole that convey meaning by making connections between component sentences. Only three techniques belong to this category.</span>
</span>
</td>
<td id="S5.T4.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T4.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">Coreference Resolution, Anaphora Resolution, Generation Rules</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Here, we classify the NLP techniques by levels of linguistic analysis. According to Liddy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, linguistic analysis can be performed at the following seven levels:</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Phonology</span>. This level deals with the interpretation of speech sounds within and across words.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Morphology.</span> This is the lowest level of text analysis. At this level, a NLP technique analyzes the smallest parts of words that carry meaning, which are composed of morphemes, including prefixes, roots and suffixes of words.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Lexical.</span> A NLP technique at this level can interpret the meaning of individual words to gain <span id="S5.p4.1.2" class="ltx_text ltx_font_italic">word-level understanding</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. Lexical analysis may require a lexicon or dictionary, which may be quite simple, with only the words and their POS tags, or may be increasingly complex and contain information on the semantic class of the word, its arguments etc. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Syntactic.</span> A NLP technique at this level focuses on analyzing the words in a sentence through the grammatical structure of the sentence. This requires both a grammar and a parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. There are two general types of parser: dependency and constituency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. The dependency parser produces a syntactic representation of a sentence based on the dependencies between the words in the sentence, whereas the constituency parser represents a sentence as a parse tree of related constituents (i.e., sub-phrases). These representations (i.e., syntax) carry meaning in most languages, because the the arrangement of words or sub-phrases in a sentence contributes to meaning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Semantic.</span> A NLP technique at this level may focus on the meanings of individual words (e.g., dictionary definitions of words and word-sense disambiguation), or compositional semantics, which looks at the interactions among word-level meanings in sentences (e.g., semantic role labeling). Semantic analysis thus can be divided into <span id="S5.p6.1.2" class="ltx_text ltx_font_italic">word-level semantic</span> and <span id="S5.p6.1.3" class="ltx_text ltx_font_italic">sentence-level semantic</span> (groups of words or sentence-level). Semantic role labelling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and Case Grammar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> are among the examples of semantic analysis techniques.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">Discourse.</span> A NLP technique at this level focuses on the properties of the text as a whole that convey meaning by making connections between component sentences. Several types of discourse processing can occur at this level, two of the most common being <span id="S5.p7.1.2" class="ltx_text ltx_font_italic">anaphora resolution</span> and <span id="S5.p7.1.3" class="ltx_text ltx_font_italic">coreference resolution</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p"><span id="S5.p8.1.1" class="ltx_text ltx_font_bold">Pragmatic.</span> This is the highest level of NLP. To reach this level, NLP techniques need to be able to achieve human-like language understanding, the ultimate goal of natural language understanding (NLU). This entails inferring extra meaning from texts that is not actually encoded in them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and understanding narratives according to different contexts and with respect to different actors and their intentions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. This requires NLP tools to have world knowledge and human intelligence, and the ability to project semantics and sentics dynamically <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Pragmatic analysis appears to be the most challenging NLP curve to jump <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.1" class="ltx_p">It is assumed that humans normally produce or comprehend language by utilizing all of these levels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. These levels thus represent the competence of a NLP tool: The more levels of analysis the tool supports, the stronger or more capable the tool; the more higher-levels of analysis the tool supports, the more advance the tool.</p>
</div>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.1" class="ltx_p">Table <a href="#S5.T4" title="TABLE IV ‣ V Classifying NLP Techniques by Linguistic Analysis Levels ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> classifies the NLP techniques based on these levels. As the table shows, we have not found any techniques for the phonetic level analysis, as NLP techniques have been largely use to deal with texts (including requirements documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, app reviews, tweets, social media posts and usage data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, legal documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and privacy policies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>). In addition, we have not found any techniques for pragmatic analysis either. This is because NLP4RE research has so far focused on text processing of documents and has not reached the level of natural language understanding (NLU).</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper presents 57 commonly used NLP techniques in RE and organizes them in two different ways: by their NLP tasks and by their analysis levels. The organization provides a knowledge base for sharing these techniques. A user of this knowledge base can query each NLP technique progressively: Through Table <a href="#S3.T1" title="TABLE I ‣ III Method ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and Table <a href="#S3.T2" title="TABLE II ‣ III Method ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, the user can ask: What technique is it? Does it work at the word-level or sentence-level of text processing? Based on Table <a href="#S4.T3" title="TABLE III ‣ IV Classifying NLP Techniques by Tasks ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the user can ask: Which text processing task does this technique support? What are the alternative techniques for the same task? From Table <a href="#S5.T4" title="TABLE IV ‣ V Classifying NLP Techniques by Linguistic Analysis Levels ‣ Classification of Natural Language Processing Techniques for Requirements Engineering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, the user can ask: What level of language analysis does this technique provide? What are the techniques for performing other levels of analysis? The answers to these questions can help the user to decide if a specific technique is relevant to the task at hand.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Our future work will improve this knowledge base as follows:</p>
</div>
<div id="S6.p3" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">To show the relationship between a given technique and other techniques. For example, for text normalization, what techniques can I use together? in what order? For text representation, which technique is better for my case?</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">To provide information on the available NLP tools that support each technique.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Ryan, “The role of natural language in requirements engineering,” in
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">[1993] Proceedings of the IEEE International Symposium on Requirements
Engineering</em>.   IEEE, 1993, pp.
240–242.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Sawyer, P. Rayson, and K. Cosh, “Shallow knowledge as an aid to deep
understanding in early phase requirements engineering,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Software Engineering</em>, vol. 31, no. 11, pp. 969–981, 2005.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
V. Berzins, C. Martell, P. Adams <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Innovations in natural
language document processing for requirements engineering,” in
<em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">Monterey Workshop</em>.   Springer,
2007, pp. 125–146.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Berry, R. Gacitua, P. Sawyer, and S. F. Tjong, “The case for dumb
requirements engineering tools,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Working Conference
on Requirements Engineering: Foundation for Software Quality</em>.   Springer, 2012, pp. 211–217.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Hirschberg and C. D. Manning, “Advances in natural language processing,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Science</em>, vol. 349, no. 6245, pp. 261–266, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
G. Goth, “Deep or shallow, nlp is breaking out,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Communications of the
ACM</em>, vol. 59, no. 3, pp. 13–16, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
F. Dalpiaz, A. Ferrari, X. Franch, and C. Palomares, “Natural language
processing for requirements engineering: The best is yet to come,”
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE software</em>, vol. 35, no. 5, pp. 115–119, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
L. Zhao, W. Alhoshan, A. Ferrari, K. J. Letsholo, M. A. Ajagbe, E.-V. Chioasca,
and R. T. Batista-Navarro, “Natural language processing (nlp) for
requirements engineering (re): A systematic mapping study,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ACM
Computing Surveys, in press</em>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Ferrari, F. Dell’Orletta, A. Esuli, V. Gervasi, and S. Gnesi, “Natural
language requirements processing: a 4D vision,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Software</em>,
vol. 34, no. 06, pp. 28–35, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
D. M. Berry, “Evaluation of tools for hairy requirements and software
engineering tasks,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 25th International Requirements
Engineering Conference Workshops (REW)</em>.   IEEE, 2017, pp. 284–291.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P. P.-S. Chen, “English sentence structure and entity-relationship diagrams,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, vol. 29, no. 2, pp. 127–149, 1983. [Online].
Available:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/0020025583900142" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/0020025583900142</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
R. J. Abbott, “Program design by informal english descriptions,”
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol. 26, no. 11, pp. 882–894, 1983.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. M. Berry, N. Yavne, and M. Yavne, “Application of program design language
tools to abbott’s method of program design by informal natural language
descriptions,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of Systems and Software</em>, vol. 7, no. 3, pp.
221–247, 1987. [Online]. Available:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/0164121287900446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/0164121287900446</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
G. Booch, R. A. Maksimchuk, M. W. Engle, B. J. Young, J. Connallen, and K. A.
Houston, “Object-oriented analysis and design with applications,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACM
SIGSOFT software engineering notes</em>, vol. 33, no. 5, pp. 29–29, 2008.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C. Aguilera and D. M. Berry, “The use of a repeated phrase finder in
requirements extraction,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Journal of Systems and Software</em>, vol. 13,
no. 3, pp. 209–230, 1990. [Online]. Available:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/0164121290900976" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/0164121290900976</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Rolland and C. Proix, “A natural language approach for requirements
engineering,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Conference on Advanced Information
Systems Engineering</em>.   Springer, 1992,
pp. 257–277.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L. Goldin and D. M. Berry, “Abstfinder, a prototype natural language text
abstraction finder for use in requirements elicitation,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Automated
Software Engineering</em>, vol. 4, no. 4, pp. 375–412, 1997.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Mich, “Nl-oops: from natural language to object oriented requirements using
the natural language processing system lolita,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Natural language
engineering</em>, vol. 2, no. 2, pp. 161–187, 1996.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
V. Ambriola and V. Gervasi, “On the systematic analysis of natural language
requirements with Circe,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Automated Software Engineering</em>, vol. 13,
no. 1, pp. 107–167, 2006.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. M. Harmain and R. Gaizauskas, “Cm-builder: an automated nl-based case
tool,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings ASE 2000. Fifteenth IEEE International
Conference on Automated Software Engineering</em>.   IEEE, 2000, pp. 45–53.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
F. Fabbrini, M. Fusani, S. Gnesi, and G. Lami, “The linguistic approach to the
natural language requirements quality: benefit of the use of an automatic
tool,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings 26th Annual NASA Goddard Software Engineering
Workshop</em>.   IEEE, 2001, pp. 97–105.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
W. M. Wilson, L. H. Rosenberg, and L. E. Hyatt, “Automated analysis of
requirement specifications,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th international
conference on Software engineering</em>, 1997, pp. 161–171.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. H. Hayes, A. Dekhtyar, and J. Osborne, “Improving requirements tracing via
information retrieval,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings. 11th IEEE International
Requirements Engineering Conference, 2003.</em>   IEEE, 2003, pp. 138–147.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Cleland-Huang, B. Berenbach, S. Clark, R. Settimi, and E. Romanova, “Best
practices for automated traceability,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Computer</em>, vol. 40, no. 6, pp.
27–35, 2007.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S. F. Tjong and D. M. Berry, “The design of sree—a prototype potential
ambiguity finder for requirements specifications and lessons learned,” in
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Working Conference on Requirements Engineering:
Foundation for Software Quality</em>.   Springer, 2013, pp. 80–95.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Yue, L. C. Briand, and Y. Labiche, “atoucan: an automated framework to
derive uml analysis models from use case models,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on
Software Engineering and Methodology (TOSEM)</em>, vol. 24, no. 3, pp. 1–52,
2015.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Ferrari, G. Gori, B. Rosadini, I. Trotta, S. Bacherini, A. Fantechi, and
S. Gnesi, “Detecting requirements defects with nlp patterns: an industrial
experience in the railway domain,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Empirical Software Engineering</em>,
vol. 23, no. 6, pp. 3684–3733, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Femmer, D. M. Fernández, S. Wagner, and S. Eder, “Rapid quality
assurance with requirements smells,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Journal of Systems and Software</em>,
vol. 123, pp. 190–213, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D. Falessi, G. Cantone, and G. Canfora, “Empirical principles and an
industrial case study in retrieving equivalent requirements via natural
language processing techniques,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software
Engineering</em>, vol. 39, no. 1, pp. 18–44, 2011.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Sleimi, N. Sannier, M. Sabetzadeh, L. Briand, and J. Dann, “Automated
extraction of semantic legal metadata using natural language processing,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2018 IEEE 26th International Requirements Engineering Conference
(RE)</em>.   IEEE, 2018, pp. 124–135.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Bhatia, T. D. Breaux, and F. Schaub, “Mining privacy goals from privacy
policies using hybridized task recomposition,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on
Software Engineering and Methodology (TOSEM)</em>, vol. 25, no. 3, pp. 1–24,
2016.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Robeer, G. Lucassen, J. M. E. Van Der Werf, F. Dalpiaz, and S. Brinkkemper,
“Automated extraction of conceptual models from user stories via nlp,” in
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2016 IEEE 24th international requirements engineering conference
(re)</em>.   IEEE, 2016, pp. 196–205.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
E. Cambria and B. White, “Jumping nlp curves: A review of natural language
processing research,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Computational intelligence magazine</em>,
vol. 9, no. 2, pp. 48–57, 2014.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
F. Sebastiani, “Machine learning in automated text categorization,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ACM
computing surveys (CSUR)</em>, vol. 34, no. 1, pp. 1–47, 2002.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Cleland-Huang, R. Settimi, X. Zou, and P. Solc, “Automated classification
of non-functional requirements,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Requirements engineering</em>, vol. 12,
no. 2, pp. 103–120, 2007.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D. D. Lewis, “Naive (bayes) at forty: The independence assumption in
information retrieval,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">European conference on machine
learning</em>.   Springer, 1998, pp. 4–15.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z. Kurtanović and W. Maalej, “Automatically classifying functional and
non-functional requirements using supervised machine learning,” in
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 25th International Requirements Engineering Conference
(RE)</em>.   Ieee, 2017, pp. 490–495.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
W. Maalej, M. Nayebi, T. Johann, and G. Ruhe, “Toward data-driven requirements
engineering,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IEEE Software</em>, vol. 33, no. 1, pp. 48–54, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. C. Groen, N. Seyff, R. Ali, F. Dalpiaz, J. Doerr, E. Guzman, M. Hosseini,
J. Marco, M. Oriol, A. Perini <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The crowd in requirements
engineering: The landscape and challenges,” <em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic">IEEE software</em>, vol. 34,
no. 2, pp. 44–52, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
W. Maalej, Z. Kurtanović, H. Nabil, and C. Stanik, “On the automatic
classification of app reviews,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Requirements Engineering</em>, vol. 21,
no. 3, pp. 311–331, 2016.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T. Merten, M. Falis, P. Hübner, T. Quirchmayr, S. Bürsner, and
B. Paech, “Software feature request detection in issue tracking systems,”
in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">2016 IEEE 24th International Requirements Engineering Conference
(RE)</em>.   IEEE, 2016, pp. 166–175.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
E. Guzman, M. Ibrahim, and M. Glinz, “A little bird told me: Mining tweets for
requirements and software evolution,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 25th International
Requirements Engineering Conference (RE)</em>.   IEEE, 2017, pp. 11–20.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
I. Morales-Ramirez, F. M. Kifetew, and A. Perini, “Speech-acts based analysis
for requirements discovery from online discussions,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Information
Systems</em>, vol. 86, pp. 94–112, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
C. Arora, M. Sabetzadeh, L. Briand, and F. Zimmer, “Automated extraction and
clustering of requirements glossary terms,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Software Engineering</em>, vol. 43, no. 10, pp. 918–945, 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. Winkler and A. Vogelsang, “Automatic classification of requirements based
on convolutional neural networks,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2016 IEEE 24th International
Requirements Engineering Conference Workshops (REW)</em>.   IEEE, 2016, pp. 39–45.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Casamayor, D. Godoy, and M. Campo, “Identification of non-functional
requirements in textual specifications: A semi-supervised learning
approach,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Information and Software Technology</em>, vol. 52, no. 4, pp.
436–445, 2010.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. Guo, J. Cheng, and J. Cleland-Huang, “Semantically enhanced software
traceability using deep learning techniques,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/ACM 39th
International Conference on Software Engineering (ICSE)</em>.   IEEE, 2017, pp. 3–14.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
H. Sultanov and J. H. Hayes, “Application of reinforcement learning to
requirements engineering: requirements tracing,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">2013 21st IEEE
International Requirements Engineering Conference (RE)</em>.   IEEE, 2013, pp. 52–61.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
T. Hey, J. Keim, A. Koziolek, and W. F. Tichy, “NoRBERT: Transfer learning
for requirements classification,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 28th International
Requirements Engineering Conference (RE)</em>.   IEEE, 2020, pp. 169–179.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
M. Li, L. Shi, Y. Yang, and Q. Wang, “A deep multitask learning approach for
requirements discovery and annotation from open forum,” in <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">2020 35th
IEEE/ACM International Conference on Automated Software Engineering
(ASE)</em>.   IEEE, 2020, pp. 336–348.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. González-Carvajal and E. C. Garrido-Merchán, “Comparing bert
against traditional machine learning text classification,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2005.13012</em>, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
D. Jurafsky, <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Speech &amp; language processing</em>.   Pearson Education India, 2000.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S. Bird, E. Klein, and E. Loper, <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Natural language processing with Python:
analyzing text with the natural language toolkit</em>.   ” O’Reilly Media, Inc.”, 2009.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
D. Sarkar, <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Text Analytics with python</em>.   Springer, 2016.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
E. D. Liddy, “Natural language processing,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Encyclopedia of Library
and Information Science</em>.   NY. Marcel
Decker, Inc., 2001.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
D. Gildea and D. Jurafsky, “Automatic labeling of semantic roles,”
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Computational linguistics</em>, vol. 28, no. 3, pp. 245–288, 2002.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
C. J. Fillmore <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Frame semantics and the nature of language,”
in <em id="bib.bib57.2.2" class="ltx_emph ltx_font_italic">Annals of the New York Academy of Sciences: Conference on the origin
and development of language and speech</em>, vol. 280, no. 1.   New York, 1976, pp. 20–32.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
C. J. Fillmore, M. R. Petruck, J. Ruppenhofer, and A. Wright, “Framenet in
action: The case of attaching,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">International journal of
lexicography</em>, vol. 16, no. 3, pp. 297–332, 2003.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
G. G. Chowdhury, “Natural language processing,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Annual review of
information science and technology</em>, vol. 37, no. 1, pp. 51–89, 2003.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
A. Ferrari, G. O. Spagnolo, and S. Gnesi, “Pure: A dataset of public
requirements documents,” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 25th International Requirements
Engineering Conference (RE)</em>.   IEEE,
2017, pp. 502–505.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
A. Ferrari, L. Zhao, and W. Alhoshan, “Nlp for requirements engineering:
Tasks, techniques, tools, and technologies,” in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
43rd IEEE/ACM International Conference on Software Engineering</em>.   IEEE, 2021.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
W. Maalej, M. Nayebi, and G. Ruhe, “Data-driven requirements engineering-an
update,” in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/ACM 41st International Conference on Software
Engineering: Software Engineering in Practice (ICSE-SEIP)</em>.   IEEE, 2019, pp. 289–290.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
I. Tenney, D. Das, and E. Pavlick, “Bert rediscovers the classical nlp
pipeline,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.05950</em>, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
M. Nayebi, “Eye of the mind: image processing for social coding,” in
<em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM/IEEE 42nd International Conference on Software
Engineering: New Ideas and Emerging Results</em>, 2020, pp. 49–52.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1706.03762</em>, 2017.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
F. Movahedi, J. L. Coyle, and E. Sejdić, “Deep belief networks for
electroencephalography: A review of recent contributions and future
outlooks,” <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">IEEE journal of biomedical and health informatics</em>,
vol. 22, no. 3, pp. 642–652, 2017.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Y. Yu, M. Li, L. Liu, Y. Li, and J. Wang, “Clinical big data and deep
learning: Applications, challenges, and future outlooks,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Big Data
Mining and Analytics</em>, vol. 2, no. 4, pp. 288–305, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
S. Cornegruta, R. Bakewell, S. Withey, and G. Montana, “Modelling radiological
language with bidirectional long short-term memory networks,” <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1609.08409</em>, 2016.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">nature</em>, vol. 521,
no. 7553, pp. 436–444, 2015.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
X. Deng, Y. Li, J. Weng, and J. Zhang, “Feature selection for text
classification: A review,” <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>,
vol. 78, no. 3, pp. 3797–3816, 2019.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
A. K. Uysal and S. Gunal, “The impact of preprocessing on text
classification,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Information Processing &amp; Management</em>, vol. 50,
no. 1, pp. 104–112, 2014.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Y. Wang, L. Shi, M. Li, Q. Wang, and Y. Yang, “A deep context-wise method for
coreference detection in natural language requirements,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
28th International Requirements Engineering Conference (RE)</em>.   IEEE, 2020, pp. 180–191.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
A. Sainani, P. R. Anish, V. Joshi, and S. Ghaisas, “Extracting and classifying
requirements from software engineering contracts,” in <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 28th
International Requirements Engineering Conference (RE)</em>.   IEEE, 2020, pp. 147–157.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on knowledge and data engineering</em>, vol. 22, no. 10, pp.
1345–1359, 2009.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
C. C. Aggarwal and C. Zhai, “A survey of text classification algorithms,” in
<em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Mining text data</em>.   Springer,
2012, pp. 163–222.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
F. Pereira, T. Mitchell, and M. Botvinick, “Machine learning classifiers and
fmri: a tutorial overview,” <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Neuroimage</em>, vol. 45, no. 1, pp.
S199–S209, 2009.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
R. Wagland, A. Recio-Saucedo, M. Simon, M. Bracher, K. Hunt, C. Foster,
A. Downing, A. Glaser, and J. Corner, “Development and testing of a
text-mining approach to analyse patients’ comments on their experiences of
colorectal cancer care,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">BMJ quality &amp; safety</em>, vol. 25, no. 8, pp.
604–614, 2016.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
K. Shameer, K. W. Johnson, B. S. Glicksberg, J. T. Dudley, and P. P. Sengupta,
“Machine learning in cardiovascular medicine: are we there yet?”
<em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Heart</em>, vol. 104, no. 14, pp. 1156–1164, 2018.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
M. Li, Y. Yang, L. Shi, Q. Wang, J. Hu, X. Peng, W. Liao, and G. Pi,
“Automated extraction of requirement entities by leveraging lstm-crf and
transfer learning,” in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Software
Maintenance and Evolution (ICSME)</em>.   IEEE, 2020, pp. 208–219.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
C. Palomares, C. Quer, and X. Franch, “Requirements reuse and requirement
patterns: a state of the practice survey,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Empirical Software
Engineering</em>, vol. 22, no. 6, pp. 2719–2762, 2017.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
C. Arora, M. Sabetzadeh, A. Goknil, L. C. Briand, and F. Zimmer, “Change
impact analysis for natural language requirements: An nlp approach,” in
<em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">2015 IEEE 23rd International Requirements Engineering Conference
(RE)</em>.   IEEE, 2015, pp. 6–15.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
S. Ezzini, S. Abualhaija, C. Arora, M. Sabetzadeh, and L. Briand, “Using
domain-specific corpora for improved handling of ambiguity in requirements,”
in <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">In Proceedings of the 43rd International Conference on Software
Engineering (ICSE’21), Madrid 25-28 May 2021</em>, 2021.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
C. Boufaied, M. Jukss, D. Bianculli, L. C. Briand, and Y. I. Parache,
“Signal-based properties of cyber-physical systems: Taxonomy and logic-based
characterization,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Journal of Systems and Software</em>, vol. 174, p.
110881, 2021.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
W. Martin, F. Sarro, Y. Jia, Y. Zhang, and M. Harman, “A survey of app store
analysis for software engineering,” <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on software
engineering</em>, vol. 43, no. 9, pp. 817–847, 2016.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
E. Guzman, L. Oliveira, Y. Steiner, L. C. Wagner, and M. Glinz, “User feedback
in the app store: a cross-cultural study,” in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/ACM 40th
International Conference on Software Engineering: Software Engineering in
Society (ICSE-SEIS)</em>.   IEEE, 2018, pp.
13–22.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
C. Stanik, M. Haering, and W. Maalej, “Classifying multilingual user feedback
using traditional machine learning and deep learning,” in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">2019 IEEE
27th International Requirements Engineering Conference Workshops
(REW)</em>.   IEEE, 2019, pp. 220–226.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
G. Lucassen, M. Robeer, F. Dalpiaz, J. M. E. Van Der Werf, and S. Brinkkemper,
“Extracting conceptual models from user stories with visual narrator,”
<em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">Requirements Engineering</em>, vol. 22, no. 3, pp. 339–358, 2017.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
A. Khan, B. Baharudin, L. H. Lee, and K. Khan, “A review of machine learning
algorithms for text-documents classification,” <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Journal of advances in
information technology</em>, vol. 1, no. 1, pp. 4–20, 2010.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
P. H. Swain and H. Hauska, “The decision tree classifier: Design and
potential,” <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience Electronics</em>, vol. 15,
no. 3, pp. 142–147, 1977.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
G. Forman <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “An extensive empirical study of feature selection
metrics for text classification.” <em id="bib.bib90.2.2" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, vol. 3, no.
Mar, pp. 1289–1305, 2003.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
C. Cortes and V. Vapnik, “Support-vector networks,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Machine learning</em>,
vol. 20, no. 3, pp. 273–297, 1995.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
J. Natt och Dag, B. Regnell, P. Carlshamre, M. Andersson, and J. Karlsson, “A
Feasibility Study of Automated Natural Language Requirements
Analysis in Market-Driven Development,” <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Requirements
Engineering</em>, vol. 7, no. 1, pp. 20–33, Apr. 2002. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1007/s007660200002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s007660200002</a>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
B. Gleich, O. Creighton, and L. Kof, “Ambiguity detection: Towards a tool
explaining ambiguity sources,” in <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">International Working Conference on
Requirements Engineering: Foundation for Software Quality</em>.   Springer, 2010, pp. 218–232.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Brownlee, “Deep learning and time series forecasting: Machine learning
mastery,”
<a target="_blank" href="https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/</a>,
2016, online; accessed 17 August 2020.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
T. Hastie, R. Tibshirani, and J. Friedman, “Overview of supervised learning,”
in <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">The elements of statistical learning</em>.   Springer, 2009, pp. 9–41.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
F. Dalpiaz, D. Dell’Anna, F. B. Aydemir, and S. Çevikol, “Requirements
classification with interpretable machine learning and dependency parsing,”
in <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 27th International Requirements Engineering Conference
(RE)</em>.   IEEE, 2019, pp. 142–152.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up? sentiment classification
using machine learning techniques,” <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">arXiv preprint cs/0205070</em>, 2002.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery: Opinion
extraction and semantic classification of product reviews,” in
<em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th international conference on World Wide Web</em>,
2003, pp. 519–528.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
N. K. Conroy, V. L. Rubin, and Y. Chen, “Automatic deception detection:
Methods for finding fake news,” <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Association for
Information Science and Technology</em>, vol. 52, no. 1, pp. 1–4, 2015.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
J. A. Sidey-Gibbons and C. J. Sidey-Gibbons, “Machine learning in medicine: a
practical introduction,” <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">BMC medical research methodology</em>, vol. 19,
no. 1, pp. 1–18, 2019.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
A. Agarwal, M. Mittal, A. Pathak, and L. M. Goyal, “Fake news detection using
a blend of neural networks: an application of deep learning,” <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">SN
Computer Science</em>, vol. 1, no. 3, pp. 1–9, 2020.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
W. Jin, H. H. Ho, and R. K. Srihari, “Opinionminer: a novel machine learning
system for web opinion mining and extraction,” in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
15th ACM SIGKDD international conference on Knowledge discovery and data
mining</em>, 2009, pp. 1195–1204.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
J. Harding, <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Qualitative data analysis: From start to finish</em>.   Sage, 2018.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
B. Kitchenham and S. Charters, “Guidelines for performing systematic
literature reviews in software engineering,” 2007.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
F. Fabbrini, M. Fusani, S. Gnesi, and G. Lami, “An automatic quality
evaluation for natural language requirements,” in <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
Seventh International Workshop on Requirements Engineering: Foundation for
Software Quality REFSQ</em>, vol. 1, 2001, pp. 4–5.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
S. Abualhaija, C. Arora, M. Sabetzadeh, L. C. Briand, and E. Vaz, “A machine
learning-based approach for demarcating requirements in textual
specifications,” in <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 27th International Requirements
Engineering Conference (RE)</em>.   IEEE,
2019, pp. 51–62.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
N. H. Bakar, Z. M. Kasirun, and N. Salleh, “Feature extraction approaches from
natural language requirements for reuse in software product lines: A
systematic literature review,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Journal of Systems and Software</em>, vol.
106, pp. 132–149, 2015.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
R. Santos, E. C. Groen, and K. Villela, “An overview of user feedback
classification approaches,” in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Joint Proceedings of REFSQ-2019
Workshops, Doctoral Symposium, Live Studies Track, and Poster Track
co-located with the 25th International Conference on Requirements
Engineering: Foundation for Software Quality (REFSQ 2019), Essen, Germany,
March 18th, 2019</em>, ser. CEUR Workshop Proceedings, P. Spoletini,
P. Mäder, D. M. Berry, F. Dalpiaz, M. Daneva, A. Ferrari, X. Franch,
S. Gregory, E. C. Groen, A. Herrmann, A. Hess, F. Houdek, O. Karras,
A. Koziolek, K. Lauenroth, C. Palomares, M. Sabetzadeh, N. Seyff, M. Trapp,
A. Vogelsang, and T. Weyer, Eds., vol. 2376.   CEUR-WS.org, 2019. [Online]. Available:
<a target="_blank" href="http://ceur-ws.org/Vol-2376/NLP4RE19_paper11.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ceur-ws.org/Vol-2376/NLP4RE19_paper11.pdf</a>

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
C. F. Baker, C. J. Fillmore, and J. B. Lowe, “The berkeley framenet project,”
in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">36th Annual Meeting of the Association for Computational Linguistics
and 17th International Conference on Computational Linguistics, Volume 1</em>,
1998, pp. 86–90.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
C. J. Fillmore, C. R. Johnson, and M. R. Petruck, “Background to framenet,”
<em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">International journal of lexicography</em>, vol. 16, no. 3, pp. 235–250,
2003.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
N. Yu, M. Zhang, and G. Fu, “Transition-based neural rst parsing with implicit
syntax features,” in <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th International Conference
on Computational Linguistics</em>, 2018, pp. 559–570.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
A. Ferrari, G. Lipari, S. Gnesi, and G. O. Spagnolo, “Pragmatic ambiguity
detection in natural language requirements,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">2014 IEEE 1st
International Workshop on Artificial Intelligence for Requirements
Engineering (AIRE)</em>.   IEEE, 2014, pp.
1–8.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
A. Ferrari and A. Esuli, “An nlp approach for cross-domain ambiguity detection
in requirements engineering,” <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Automated Software Engineering</em>,
vol. 26, no. 3, pp. 559–598, 2019.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard, and
D. McClosky, “The stanford corenlp natural language processing toolkit,” in
<em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Proceedings of 52nd annual meeting of the association for computational
linguistics: system demonstrations</em>, 2014, pp. 55–60.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
D. Jurafsky and J. Martin, <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">Speech and Language Processing: An
Introduction to Natural Language Processing, Computational Linguistics, and
Speech Recognition (Draft)</em>, 12 2020, vol. 3.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
T. Sanders and W. Spooren, “Discourse and text structure,” <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Handbook of
cognitive linguistics</em>, pp. 916–941, 2007.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
E. D. Liddy, “Anaphora in natural language processing and information
retrieval,” <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Information processing &amp; management</em>, vol. 26, no. 1, pp.
39–52, 1990.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
M. Binkhonain and L. Zhao, “A review of machine learning algorithms for
identification and classification of non-functional requirements,”
<em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications: X</em>, vol. 1, p. 100001, 2019.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
——, “Dealing with imbalanced class, short text and high dimensionality
problems in machine learning-based requirements classification: Method
development and evaluation,” <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Under review</em>, 2021.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep
bidirectional transformers for language understanding,” <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in deep
learning based natural language processing,” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">ieee Computational
intelligenCe magazine</em>, vol. 13, no. 3, pp. 55–75, 2018.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Human-level control through deep reinforcement learning,” <em id="bib.bib122.2.2" class="ltx_emph ltx_font_italic">nature</em>,
vol. 518, no. 7540, pp. 529–533, 2015.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word
representations in vector space,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1301.3781</em>,
2013.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
K. Kowsari, K. Jafari Meimandi, M. Heidarysafa, S. Mendu, L. Barnes, and
D. Brown, “Text classification algorithms: A survey,” <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">Information</em>,
vol. 10, no. 4, p. 150, 2019.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
D. S. Cruzes and T. Dyba, “Recommended steps for thematic synthesis in
software engineering,” in <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">2011 international symposium on empirical
software engineering and measurement</em>.   IEEE, 2011, pp. 275–284.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
I. Hussain, L. Kosseim, and O. Ormandjieva, “Using linguistic knowledge to
classify non-functional requirements in srs documents,” in
<em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">International Conference on Application of Natural Language to
Information Systems</em>.   Springer, 2008,
pp. 287–298.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
H. Yang, A. De Roeck, V. Gervasi, A. Willis, and B. Nuseibeh, “Analysing
anaphoric ambiguity in natural language requirements,” <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Requirements
engineering</em>, vol. 16, no. 3, p. 163, 2011.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
E. Knauss, S. Houmb, K. Schneider, S. Islam, and J. Jürjens, “Supporting
requirements engineers in recognising security issues,” in
<em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">International Working Conference on Requirements Engineering:
Foundation for Software Quality</em>.   Springer, 2011, pp. 4–18.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
L. Yi, W. Zhang, H. Zhao, Z. Jin, and H. Mei, “Mining binary constraints in
the construction of feature models,” in <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">2012 20th IEEE International
Requirements Engineering Conference (RE)</em>.   IEEE, 2012, pp. 141–150.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
M. Riaz, J. King, J. Slankas, and L. Williams, “Hidden in plain sight:
Automatically identifying security requirements from natural language
artifacts,” in <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">2014 IEEE 22nd international requirements engineering
conference (RE)</em>.   IEEE, 2014, pp.
183–192.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
E. Knauss and D. Ott, “(semi-) automatic categorization of natural language
requirements,” in <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">International Working Conference on Requirements
Engineering: Foundation for Software Quality</em>.   Springer, 2014, pp. 39–54.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
A. Mahmoud and G. Williams, “Detecting, classifying, and tracing
non-functional software requirements,” <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">Requirements Engineering</em>,
vol. 21, no. 3, pp. 357–381, 2016.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Z. S. H. Abad, O. Karras, P. Ghazi, M. Glinz, G. Ruhe, and K. Schneider, “What
works better? a study of classifying requirements,” in <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 25th
International Requirements Engineering Conference (RE)</em>.   IEEE, 2017, pp. 496–501.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
M. Lu and P. Liang, “Automatic classification of non-functional requirements
from augmented app user reviews,” in <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st
International Conference on Evaluation and Assessment in Software
Engineering</em>, 2017, pp. 344–353.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
C. Li, L. Huang, J. Ge, B. Luo, and V. Ng, “Automatically classifying user
requests in crowdsourcing requirements engineering,” <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Journal of
Systems and Software</em>, vol. 138, pp. 108–123, 2018.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Z. S. H. Abad, V. Gervasi, D. Zowghi, and B. H. Far, “Supporting analysts by
dynamic extraction and classification of requirements-related knowledge,” in
<em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE)</em>.   IEEE, 2019, pp. 442–453.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
J. A. Khan, Y. Xie, L. Liu, and L. Wen, “Analysis of requirements-related
arguments in user forums,” in <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 27th International
Requirements Engineering Conference (RE)</em>.   IEEE, 2019, pp. 63–74.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
W. Zheng, H. Lu, Y. Zhou, J. Liang, H. Zheng, and Y. Deng, “ifeedback:
exploiting user feedback for real-time issue detection in large-scale online
service systems,” in <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE)</em>.   IEEE, 2019, pp. 352–363.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
D. Martens and W. Maalej, “Towards understanding and detecting fake reviews in
app stores,” <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">Empirical Software Engineering</em>, vol. 24, no. 6, pp.
3316–3355, 2019.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
D. Falessi, J. Roll, J. L. Guo, and J. Cleland-Huang, “Leveraging historical
associations between requirements and source code to identify impacted
classes,” <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, vol. 46, no. 4,
pp. 420–441, 2018.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
J. Frattini, M. Junker, M. Unterkalmsteiner, and D. Mendez, “Automatic
extraction of cause-effect-relations from requirements artifacts,” in
<em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">2020 35th IEEE/ACM International Conference on Automated Software
Engineering (ASE)</em>.   IEEE, 2020, pp.
561–572.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
W. Yin, J. Hay, and D. Roth, “Benchmarking zero-shot text classification:
Datasets, evaluation and entailment approach,” <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1909.00161</em>, 2019.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
E. Arisoy, T. N. Sainath, B. Kingsbury, and B. Ramabhadran, “Deep neural
network language models,” in <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram Model? On the Future of
Language Modeling for HLT</em>, 2012, pp. 20–28.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
B. Chiu, A. Korhonen, and S. Pyysalo, “Intrinsic evaluation of word vectors
fails to predict extrinsic performance,” in <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st
workshop on evaluating vector-space representations for NLP</em>, 2016, pp. 1–6.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Y. Shi, Y. Zheng, K. Guo, L. Zhu, and Y. Qu, “Intrinsic or extrinsic
evaluation: An overview of word embedding evaluation,” in <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">2018 IEEE
International Conference on Data Mining Workshops (ICDMW)</em>.   IEEE, 2018, pp. 1255–1262.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
U. Kamath, J. Liu, and J. Whitaker, <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">Deep learning for NLP and speech
recognition</em>.   Springer, 2019, vol. 84.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
S. Landolt, T. Wambsganss, and M. Söllner, “A taxonomy for deep learning
in natural language processing.”   Hawaii International Conference on System Sciences, 2021.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
H. N. Mhaskar and T. Poggio, “Deep vs. shallow networks: An approximation
theory perspective,” <em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">Analysis and Applications</em>, vol. 14, no. 06, pp.
829–848, 2016.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with
neural networks,” <em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.3215</em>, 2014.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
L. Zettlemoyer, “Deep contextualized word representations,” <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1802.05365</em>, 2018.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language
understanding by generative pre-training,” 2018.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
J. Howard and S. Ruder, “Universal language model fine-tuning for text
classification,” <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1801.06146</em>, 2018.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.04281" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.04282" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.04282">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.04282" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.04283" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:38:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
