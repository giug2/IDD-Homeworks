<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Revisiting the Solution of Meta KDD Cup 2024: CRAG</title>
<!--Generated on Mon Sep  9 07:23:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Retrieval-Augmented Generation,  Large Language Model" lang="en" name="keywords"/>
<base href="/html/2409.15337v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S1" title="In Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S1.SS1" title="In 1. Introduction ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Dataset Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S1.SS2" title="In 1. Introduction ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Task Desription</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2" title="In Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodlogy</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS1" title="In 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Router</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS2" title="In 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS2.SSS1" title="In 2.2. Retrieval ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span><span class="ltx_text ltx_font_bold">Web Pages</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS2.SSS2" title="In 2.2. Retrieval ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span><span class="ltx_text ltx_font_bold">Mock APIs</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS3" title="In 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS4" title="In 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS4.SSS1" title="In 2.4. Generation ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span><span class="ltx_text ltx_font_bold">Chain of Thought</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS4.SSS2" title="In 2.4. Generation ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span><span class="ltx_text ltx_font_bold">In-context Learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.SS4.SSS3" title="In 2.4. Generation ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span><span class="ltx_text ltx_font_bold">Post-processing</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S3" title="In Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S3.SS1" title="In 3. Experiments ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Metrics and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S3.SS2" title="In 3. Experiments ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Overall Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S3.SS3" title="In 3. Experiments ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S4" title="In Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Perspectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S5" title="In Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1" title="In Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS1" title="In Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>An Example of Web Search Results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS2" title="In Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>An Example of Mock APIs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS3" title="In Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Experiment Results of HTML Parsing Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS4" title="In Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Prompts Used for Name Entity Recognition</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Revisiting the Solution of Meta KDD Cup 2024: CRAG</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Ouyang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ouyang%CB%99jie@mail.ustc.edu.cn">ouyang˙jie@mail.ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yucong Luo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:prime666@mail.ustc.edu.cn">prime666@mail.ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingyue Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-9873-7681" title="ORCID identifier">0000-0001-9873-7681</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mycheng@ustc.edu.cn">mycheng@ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daoyu Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wdy030428@mail.ustc.edu.cn">wdy030428@mail.ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuo Yu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yu12345@mail.ustc.edu.cn">yu12345@mail.ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qi Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id22.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id23.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id24.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:qiliuql@ustc.edu.cn">qiliuql@ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enhong Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Hefei</span><span class="ltx_text ltx_affiliation_state" id="id27.3.id3">Anhui</span><span class="ltx_text ltx_affiliation_country" id="id28.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:cheneh@ustc.edu.cn">cheneh@ustc.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id29.id1">This paper presents the solution of our team APEX in the Meta KDD CUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the limitations of existing QA benchmarks in evaluating the diverse and dynamic challenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a more comprehensive assessment of RAG performance and contributes to advancing research in this field. We propose a routing-based domain and dynamic adaptive RAG pipeline, which performs specific processing for the diverse and dynamic nature of the question in all three stages: retrieval, augmentation, and generation. Our method achieved superior performance on CRAG and ranked 2nd for Task 2&amp;3 on the final competition leaderboard. Our implementation is available at this link: <a class="ltx_ref ltx_href" href="https://github.com/USTCAGI/CRAG-in-KDD-Cup2024" title="">https://github.com/USTCAGI/CRAG-in-KDD-Cup2024</a>.</p>
</div>
<div class="ltx_keywords">Retrieval-Augmented Generation, Large Language Model
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining; August 25-29,
2024; Barcelona</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Information retrieval</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have revolutionized the landscape of Natural Language Processing (NLP) tasks <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib6" title="">2024</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib11" title="">2023</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib9" title="">2023</a>)</cite>, particularly in question answering (QA). Despite advances in LLMs, hallucination remains a significant challenge, particularly for dynamic facts and information about less prominent entities.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib10" title="">2020</a>)</cite> has recently emerged as a promising solution to mitigate LLMs’ knowledge deficiencies. Given a question, a RAG system queries external sources to retrieve relevant information and subsequently provides grounded answers. Despite its potential, RAG continues to face numerous challenges, including the selection of the most relevant information, the reduction of question answering latency, and the synthesis of information to address complex questions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To bridge this gap, Meta introduced the Comprehensive RAG Benchmark (CRAG) <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib14" title="">2024</a>)</cite>, a factual question answering benchmark of 4,409 question-answer pairs and Mock APIs to simulate web and Knowledge Graph (KG) search, and hosted the KDD CUP 2024 Challenge.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Dataset Description</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">The CRAG contains two parts of data: the QA pairs and the content for retrieval.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.p2.1.1">QA pairs</span>. The CRAG dataset contains a rich set of 4,409 QA pairs covering five domains: finance, sports, music, movie, and open domain, and eight types of questions. For the KDD CUP 2024 Challenge, the benchmark data were splited into three sets with similar distributions: validation, public test, and private test at 30%, 30%, and 40%, respectively. In total, 2,706 examples from validation and public test sets were shared.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">The dataset also reflects varied entity popularity from popular to long-tail entities, and temporal spans ranging from seconds to years. Given the temporal nature of many questions, each question-answer pair is accompanied by an additional field denoted as ”query time.” This temporal marker ensures the reliability and uniqueness of the answers within their specific temporal context.</p>
</div>
<div class="ltx_para" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.p4.1.1">Content for retrieval</span>. The CRAG dataset incorporates two types of content for retrieval to simulate a practical scenario for RAG: web search and knowledge graph (KG) search. This encompasses up to 50 full HTML pages for each question, retrieved from a real-world search engine, as well as Mock KGs containing 2.6 million entities. Additionally, CRAG provides Mock APIs to simulate retrieval from a wide range of available information sources.</p>
</div>
<div class="ltx_para" id="S1.SS1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Web search results</span> Each retrieved web search result comprises five fields: <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.2">page name</span>, <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.3">page url</span>, <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.4">page snippet</span>, <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.5">page last modified</span> and <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.6">page result</span>. See Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.T3" title="Table 3 ‣ A.1. An Example of Web Search Results. ‣ Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">3</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS1" title="A.1. An Example of Web Search Results. ‣ Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">A.1</span></a> for an example.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Mock KGs</span> A Knowledge Graph containing 2.6 million entities, which is accessed through Mock API.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Mock APIs</span> APIs for retrieving structured data from Mock KGs with predefined parameters, which are categorized by domain and output in JSON format. See Example in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS2" title="A.2. An Example of Mock APIs. ‣ Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">A.2</span></a>
.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>Task Desription</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">This challenge comprises three tasks designed to improve question-answering (QA) systems.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.1">TASK 1:</span>
The organizers provide 5 web pages per question, potentially containing relevant information. The objective is to measure the systems’ capability to identify and condense this information into accurate answers.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.p3.1.1">TASK 2:</span>
This task introduces mock APIs to access information from underlying mock Knowledge Graphs (KGs), with structured data possibly related to the questions. Participants use mock APIs, inputting parameters derived from the questions, to retrieve relevant data for answer formulation. The evaluation focuses on the systems’ ability to query structured data and integrate information from various sources into comprehensive answers.</p>
</div>
<div class="ltx_para" id="S1.SS2.p4">
<p class="ltx_p" id="S1.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.p4.1.1">TASK 3:</span>
The third task increases complexity by providing 50 web pages and mock API access for each question, encountering both relevant information and noise. It assesses the systems’ skill in selecting the most important data from a larger set, reflecting the challenges of real-world information retrieval and integration.</p>
</div>
<div class="ltx_para" id="S1.SS2.p5">
<p class="ltx_p" id="S1.SS2.p5.1">Each task builds upon the previous, steering participants toward developing sophisticated end-to-end RAG systems. This challenge showcases the potential of RAG technology in navigating and making sense of extensive information repositories, setting the stage for future AI research and development breakthroughs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Methodlogy</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="406" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The overall pipeline of our solution.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our approach, akin to most Retrieval-Augmented Generation (RAG) systems, comprises three primary phases: retrieval, augmentation and generation. In all phases, we implement routing mechanisms to address diverse query types. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.F1" title="Figure 1 ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the pipeline of our solution, while the remainder of this section details the three main phases and the routers employed in this challenge.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Router</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Routing is a crucial component of RAG systems, especially in real-world QA scenarios. In practical applications, RAG systems frequently incorporate multiple data sources. In the CRAG Challenge, we have three distinct data sources: Web Pages, Mock KGs, and Mock APIs. The diversity of questions requires routing queries to different data sources, individually or in combination. Even within a single data source, such as Mock APIs, the question-specific selection of appropriate APIs is crucial. Furthermore, we can tailor prompt templates based on the nature of the question or route questions to different post-processing components.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In response to the specific characteristics of the questions in the CRAG Challenge, we designed two specialized routers: the Domain Router and the Dynamism Router. These routers are designed to efficiently navigate the complex landscape of multisource information retrieval and question-specific processing in our RAG system.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Domain Router</span>. Domain router is fundamentally a classifier, more specifically, a sentence classifier. Given a query, the domain router assigns a specific domain from a predefined set: finance, sports, music, movie, and open. Based on the assigned domain, the workflow is then routed to the corresponding path.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">We utilize <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">Llama3-8B-Instruct</span> <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib3" title="">2024</a>)</cite> as our base model and enhance it with a classification head (Multilayer Perceptron, MLP) for domain classification. The 8B model inherently demonstrates a robust capability to comprehend the domain of queries. We randomly split the CRAG dataset into training, validation, and test sets with a ratio of 8:1:1. Based on this split, we performed a simple LORA (Low-Rank Adaptation) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib8" title="">2021</a>)</cite> fine-tuning to adapt to the distribution of the CRAG dataset. This approach facilitates the development of a high-quality classifier with minimal additional training.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">The trained Domain router is employed at multiple stages within the system. During the retrieval phase, the Domain router is primarily used to select appropriate APIs. Following the retrieval of Web Pages and APIs, it is further applied for selective fusion of the retrieved knowledge. In the generation phase, we first customize the prompt templates based on the domain. Subsequently, after the model completes its generation, the Domain is also utilized for corresponding post-processing.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p6.1.1">Dynamic Router</span>. Analogous to the Domain router, the Dynamism router is also a sentence classifier. Given a question, the Dynamism router assigns a specific Dynamism, specifically one of: static, slow-changing, fast-changing, or real-time. The specific training methodology for the Dynamism router is congruent with that of the Domain Router, and thus will not be recapitulated here.</p>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">Due to the inherent limitation of Large Language Models in updating their internal knowledge, they are prone to providing outdated answers to dynamic questions. Even when employing external knowledge through RAG, LLMs can readily generate hallucinations as most data sources are static, unless real-time APIs are utilized. In the absence of real-time APIs, the more rapidly a question changes over time, the more susceptible LLMs are to hallucinations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p8">
<p class="ltx_p" id="S2.SS1.p8.1">To attenuate hallucinations arising from dynamic questions, we implemented the Dynamism router for post-processing. In scenarios where real-time APIs are inaccessible, we excluded certain real-time questions and those that change rapidly over time.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Retrieval</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">As mentioned above, the CRAG dataset encompasses three types of content for retrieval: Web Pages, Mock KGs, and Mock APIs. For our final solution, we utilize two of these content types: Web Pages and Mock APIs.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The pipeline of Web Retriever.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.1.1">Web Pages</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Web Pages are available for all 3 tasks of the challenge. For Task 1&amp;2, 5 Web Pages per question are provided, each potentially containing relevant information. Task 3 increases complexity by offering 50 Web Pages for each question, presenting both pertinent information and noise.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">To enhance our QA system, we need to extract useful and relevant information from web search results. The primary process for retrieving web content is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.F2" title="Figure 2 ‣ 2.2. Retrieval ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">HTML Parsing</span>: Structured HTML is often unnecessarily verbose and contains substantial extraneous information that can impede subsequent segmentation operations. Therefore, it is crucial to first convert this structured format into natural language text that is more amenable to processing by Large Language Models. We conducted experiments with various HTML parsing methods, including BeautifulSoup, Newspaper, Markdownify, and several others. After evaluating both parsing efficiency and quality, we ultimately selected Newspaper. See <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS3" title="A.3. Experiment Results of HTML Parsing Methods ‣ Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">A.3</span></a> for more details about experiment results.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Pre-ranking</span> (Task 3 only): For Task 3, fine-grained processing 50 Web Pages would be excessively time-consuming. Therefore, we initially filter out an appropriate amount of relevant text before ranking. Specifically, we segment all the text from the Web Pages into chunks of 1024 tokens (calculated based on tokens rather than characters). For these segmented text chunks, we use BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib12" title="">2009</a>)</cite> to select the top 50 most relevant text blocks.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Ranking</span>: In the ranking phase, we further refine the pre-ranked text blocks. For task 1&amp;2, the text blocks are the 5 raw plain text extracted from HTML. The text blocks are segmented into smaller chunks, each comprising 256 tokens. We then transform these 256-token chunks into embeddings utilizing the <span class="ltx_text ltx_font_italic" id="S2.I1.i3.p1.1.2">bge-m3</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib5" title="">2024</a>)</cite> model. Finally, we calculate the cosine similarity to select top 10 relevant chunks.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Re-ranking</span>: We utilized <span class="ltx_text ltx_font_italic" id="S2.I1.i4.p1.1.2">bge-m3-v2-reranker</span> <cite class="ltx_cite ltx_citemacro_citep">(FlagOpen, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib7" title="">2024</a>)</cite> to re-rank the aforementioned 10 relevant chunks, ultimately selecting the top 5 segments.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="649" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The pipeline of API Extractor.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.1.1">Mock APIs</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">A total of 38 Mock APIs were provided for tasks 2&amp;3. As mentioned above, these Mock APIs can be categorized into five distinct domains, with no overlap between the APIs of different domains. Naturally, we designed separate workflows for each domain using a Domain Router. However, the overarching process flow of the workflows across all domains remains consistent, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S2.F3" title="Figure 3 ‣ 2.2.1. Web Pages ‣ 2.2. Retrieval ‣ 2. Methodlogy ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">3</span></a>:</p>
<ol class="ltx_enumerate" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Named Entity Recognition (NER)</span>: We directly utilize <span class="ltx_text ltx_font_italic" id="S2.I2.i1.p1.1.2">Llama3-70B-Instruct</span> <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib3" title="">2024</a>)</cite> to identify and classify named entities in the question into predefined categories, such as movie names and artist names. Specific prompts are presented in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#A1.SS4" title="A.4. Prompts Used for Name Entity Recognition ‣ Appendix A Appendix ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">Entity Match</span>: Matching extracted entities with API input parameters. Taking finance as an example, the input parameter for finance API is typically the ticker symbol, while user questions often contain full company names. We need to convert the company names to their corresponding ticker symbols. We first perform exact matching, requiring the extracted entity to be exactly the same as the input parameter. If no match is found, we then use BM25 to select the most similar one.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i3.p1.1.1">Time Information Extraction</span>: In addition to entities, numerous API inputs incorporate temporal information, requiring the extraction of relevant time points or intervals from user inputs. Notably, temporal information is often dependent with query time, as illustrated by terms such as ”yesterday.” In such cases, we must determine the specific time point based on the query time through relative time computation. We first use regular expressions to match certain time and date-related terms. Once matched, we use two Python packages (pytz and datetime) to calculate the Absolute Time. If no matches are found, the current time is used by default.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S2.I2.i4.p1">
<p class="ltx_p" id="S2.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i4.p1.1.1">API Select</span>: Each domain comprises numerous APIs, not all of which are inherently relevant to a user’s query. We have manually designed a set of rules to select APIs that correspond to the given question. To minimize the risk of overfitting the rules to the training set, we implemented constraints on the rule design process, prioritizing simplicity and robustness.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span>
<div class="ltx_para" id="S2.I2.i5.p1">
<p class="ltx_p" id="S2.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i5.p1.1.1">Json to Markdown</span>: The JSON output from APIs, while structured and machine-readable, may not be optimal for large language models (LLMs) to process efficiently. Converting this JSON data into a more LLM-friendly Markdown format can enhance the model’s ability to understand and utilize the information.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Augmentation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We employ input-layer integration for generation augmentation, which combines retrieved
information/documents with the original input/query and jointly passes them to the generator. In contrast to common input-layer integration, we do not utilize all retrieved documents. For different domains, we select specific data sources and integrate them to construct the final reference.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">For the open domain, since we did not employ a Mock API, we exclusively utilized web search results. For the movie and music domains, where most queries are relatively static or evolve slowly, results retrieved from both web pages and mock APIs remain relevant. Therefore, we chose to integrate these two sources. For the sports and finance domains, which involve numerous real-time and fast-changing queries, we exclusively used Mock APIs to ensure the timeliness and relevance of the retrieved information.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Generation</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In the generation phase, we employed two widely-used methodologies: Chain-of-Thought (CoT) reasoning and In-context Learning. After generation, we performed a simple post-processing procedure on the generated results based on the Domain and Dynamism Routers.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1. </span><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS1.1.1">Chain of Thought</span>
</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.1">Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib13" title="">2022</a>)</cite> enhances the reasoning process of language models by prompting them to articulate intermediate steps in problem-solving. This approach not only enhances the model’s ability to handle complex tasks but also significantly reduces hallucinations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2. </span><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.1.1">In-context Learning</span>
</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">We improve the model’s ability to recognize invalid questions, particularly those based on false premises, through In-context Learning. We develop adaptive few-shot examples <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib4" title="">2020</a>)</cite>, selecting two of the most representative invalid question samples for each domain and elucidating the reasons for their invalidity. Using the sports domain as an example, our few-shot samples are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS2.p2">
<svg class="ltx_picture" height="97.92" id="S2.SS4.SSS2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,97.92) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 92.02 C 0 95.28 2.64 97.92 5.91 97.92 L 594.09 97.92 C 597.36 97.92 600 95.28 600 92.02 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 73.81 L 598.03 73.81 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g color="#404040" fill="#404040" stroke="#404040" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-opacity="1.0"><path d="M 1.97 37.89 L 598.03 37.89" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 79.72)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS4.SSS2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.SS4.SSS2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p2.pic1.1.1.1.1.1.1.1">Few-shot Example I</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 49.7)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS4.SSS2.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.SS4.SSS2.p2.pic1.2.2.2.1.1.1">What’s the latest score for OKC’s game today?</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS4.SSS2.p2.pic1.3.3.3.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.SS4.SSS2.p2.pic1.3.3.3.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.SS4.SSS2.p2.pic1.3.3.3.1.1.1.1">There is no game for OKC today.</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS2.p3">
<svg class="ltx_picture" height="97.92" id="S2.SS4.SSS2.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,97.92) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 92.02 C 0 95.28 2.64 97.92 5.91 97.92 L 594.09 97.92 C 597.36 97.92 600 95.28 600 92.02 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 73.81 L 598.03 73.81 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g color="#404040" fill="#404040" stroke="#404040" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-opacity="1.0"><path d="M 1.97 37.89 L 598.03 37.89" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 79.72)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS4.SSS2.p3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.SS4.SSS2.p3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p3.pic1.1.1.1.1.1.1.1">Few-shot Example II</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 49.7)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS4.SSS2.p3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.SS4.SSS2.p3.pic1.2.2.2.1.1.1">How many times has Curry won the NBA dunk contest?</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS4.SSS2.p3.pic1.3.3.3.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.SS4.SSS2.p3.pic1.3.3.3.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.SS4.SSS2.p3.pic1.3.3.3.1.1.1.1">Steph Curry has never participated in the NBA dunk contest.</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3. </span><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS3.1.1">Post-processing</span>
</h4>
<div class="ltx_para" id="S2.SS4.SSS3.p1">
<p class="ltx_p" id="S2.SS4.SSS3.p1.1">Before finalizing the results, we implement basic post-processing strategies. Based on the question domain and volatility, we assign ”I don’t know” responses to queries susceptible to hallucination. For domains lacking real-time API access, specifically open, movie, and music categories, we designated ”I don’t know” answers to fast-changing and real-time questions. Furthermore, due to the model’s limited mathematical computation capabilities, we implement same processing for questions requiring numerical calculations, such as those involving the term ”average.” Although these approach may be deemed simplistic, it demonstrated efficacy in significantly reducing hallucinations.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Experiments</h2>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Overall Preformance of our solutions on all 3 Tasks.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;"></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1.2" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Score(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1.3" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Accuracy(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1.4" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">Hallucination(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1.5" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.5.1">Missing(%)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.2.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.1.1">LLM Only</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.2.2" style="padding-left:12.0pt;padding-right:12.0pt;">-7.29</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.2.3" style="padding-left:12.0pt;padding-right:12.0pt;">28.01</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.2.4" style="padding-left:12.0pt;padding-right:12.0pt;">35.30</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.2.5" style="padding-left:12.0pt;padding-right:12.0pt;">36.69</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.3.3.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.3.1.1">Direct RAG</span></th>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.3.2" style="padding-left:12.0pt;padding-right:12.0pt;">-6.78</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.3.3" style="padding-left:12.0pt;padding-right:12.0pt;">34.79</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.3.4" style="padding-left:12.0pt;padding-right:12.0pt;">41.58</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.3.5" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.3.5.1">23.63</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.4.4.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.4.1.1">Task 1</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.4.4.2" style="padding-left:12.0pt;padding-right:12.0pt;">11.82</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.4.4.3" style="padding-left:12.0pt;padding-right:12.0pt;">29.98</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.4.4.4" style="padding-left:12.0pt;padding-right:12.0pt;">18.16</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.4.4.5" style="padding-left:12.0pt;padding-right:12.0pt;">51.86</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.5.5.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.5.5.1.1">Task 2</span></th>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.5.2" style="padding-left:12.0pt;padding-right:12.0pt;">31.22</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.5.3" style="padding-left:12.0pt;padding-right:12.0pt;">46.75</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.5.4" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.5.5.4.1">15.54</span></td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.5.5" style="padding-left:12.0pt;padding-right:12.0pt;">37.71</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.6.6.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.6.1.1">Task 3</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.6.6.2" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.6.2.1">31.66</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.6.6.3" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.6.3.1">48.21</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.6.6.4" style="padding-left:12.0pt;padding-right:12.0pt;">16.56</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.6.6.5" style="padding-left:12.0pt;padding-right:12.0pt;">35.23</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Ablation Study for Major Strategies Employed in the System.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S3.T2.1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">Score(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Accuracy(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.4.1">Hallucination(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.5.1">Missing(%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.6" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.6.1">Time Cost(s)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.2.1" rowspan="7" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.1.1">Task 2</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.2.1">w/o Rerank</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.2.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">29.17</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.2.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">43.54</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.2.2.5" style="padding-left:10.0pt;padding-right:10.0pt;">14.37</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.2.2.6" style="padding-left:10.0pt;padding-right:10.0pt;">42.09</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.2.2.7" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.3.3.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.1.1">w/o EntityMatch</span></th>
<td class="ltx_td ltx_align_right" id="S3.T2.1.3.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">21.44</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">32.31</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.3.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">10.87</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.3.3.5" style="padding-left:10.0pt;padding-right:10.0pt;">56.82</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.3.3.6" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.4.4.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.1.1">w/o TimeInfoExtract</span></th>
<td class="ltx_td ltx_align_right" id="S3.T2.1.4.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">18.45</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.4.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">28.45</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.4.4.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.4.1">9.99</span></td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.4.4.5" style="padding-left:10.0pt;padding-right:10.0pt;">61.56</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.4.4.6" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.5.5.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.5.5.1.1">w/o Fewshot&amp;CoT</span></th>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.5.2" style="padding-left:10.0pt;padding-right:10.0pt;">25.53</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">52.08</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">26.55</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.5.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.5.5.5.1">21.37</span></td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.5.6" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.6.6.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.6.6.1.1">w/o Fewshot</span></th>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.6.2" style="padding-left:10.0pt;padding-right:10.0pt;">27.13</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.6.3" style="padding-left:10.0pt;padding-right:10.0pt;">51.35</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.6.4" style="padding-left:10.0pt;padding-right:10.0pt;">24.22</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.6.5" style="padding-left:10.0pt;padding-right:10.0pt;">24.43</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.6.6" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.7.7.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.7.7.1.1">w/o CoT</span></th>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.7.2" style="padding-left:10.0pt;padding-right:10.0pt;">28.52</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.7.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.7.7.3.1">53.32</span></td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.7.4" style="padding-left:10.0pt;padding-right:10.0pt;">24.80</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.7.5" style="padding-left:10.0pt;padding-right:10.0pt;">21.88</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.7.6" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.8.8.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.8.8.1.1">Ours</span></th>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.8.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.8.8.2.1">31.22</span></td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.8.3" style="padding-left:10.0pt;padding-right:10.0pt;">46.75</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.8.4" style="padding-left:10.0pt;padding-right:10.0pt;">15.54</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.8.5" style="padding-left:10.0pt;padding-right:10.0pt;">37.71</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.8.6" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T2.1.9.9.1" rowspan="2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.9.9.1.1">Task 3</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.9.9.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.9.9.2.1">w/o Prerank</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.9.9.3" style="padding-left:10.0pt;padding-right:10.0pt;">29.53</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.9.9.4" style="padding-left:10.0pt;padding-right:10.0pt;">44.34</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.9.9.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.9.9.5.1">14.81</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.9.9.6" style="padding-left:10.0pt;padding-right:10.0pt;">40.85</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.9.9.7" style="padding-left:10.0pt;padding-right:10.0pt;">68.17</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.10.10.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.1.1">Ours</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.10.10.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.2.1">31.66</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.10.10.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.3.1">48.21</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.10.10.4" style="padding-left:10.0pt;padding-right:10.0pt;">16.56</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.10.10.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.5.1">35.23</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.10.10.6" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.6.1">5.96</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present our main results and ablation studies for
some crucial components.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We did not employ a strategy of fine-tuning the LLM; instead, we used the LLM in a zero-shot setting. According to the rules set by the organizers, we used <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">Llama3-70B-Instruct</span> <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib3" title="">2024</a>)</cite> for all our LLMs. For the embedding model, we used <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">BAAI/bge-m3</span>, and for the rerank model, we used <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">BAAI/bge-m3-v2-reranker</span>. In the 1371 public test cases officially released for this round, we compared the following baselines: LLM only (using the LLM without retrieving references) and straightforward RAG (the baseline provided by the organizers, using straightforward RAG solutions).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Metrics and Evaluation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In line with CRAG Benchmark, we conduct a model-based automatic evaluation for our experiment. Automatic evaluation employs rule-based matching and GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#bib.bib2" title="">2023</a>)</cite> assessment to check answer correctness. It will assign three scores: correct (1 point), missing (0 points), and incorrect (-1 point). The final score for a given RAG system is computed as the average score across all examples in the evaluation set.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Overall Performance</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Comparing our solutions to the RAG Baseline, we observe significant advantages in performance across all three tasks. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S3.T1" title="Table 1 ‣ 3. Experiments ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">1</span></a>, our approach showcases notable improvements in accuracy and information retention. Specifically, when contrasted with the RAG Baseline, our solutions demonstrate superior results with reduced hallucination rates and enhanced information completeness. Task 2 and Task 3, in particular, exhibit substantial enhancements in accuracy and reduced hallucination percentages, highlighting the effectiveness of our proposed methodologies in addressing these key metrics.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Ablation Study</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15337v1#S3.T2" title="Table 2 ‣ 3. Experiments ‣ Revisiting the Solution of Meta KDD Cup 2024: CRAG"><span class="ltx_text ltx_ref_tag">2</span></a> presents the ablation study for major strategies employed in our solution.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">During the retrieval phase, we implemented several strategies, including pre-ranking, re-ranking, Entity Match, and Time Information Extraction. Ablation studies revealed that pre-ranking and re-ranking marginally reduce performance, while Entity Match and Time Information Extraction significantly decrease performance. Both pre-ranking and re-ranking significantly contribute to the improvement of retrieval quality. Pre-ranking enhances retrieval performance by proactively filtering out a significant amount of noise, while re-ranking ensures the accuracy of retrieval results through more refined and granular sorting. The enhancement in retrieval quality ultimately translates into an increase in answer accuracy. The absence of pre-ranking and re-ranking demonstrably leads to a substantial decrease in the accuracy of the final answers. Furthermore, pre-ranking significantly enhances retrieval effiency and reduces the retrieval time. Entity Matching and Time Information Extraction form the basis of using MOCK APIs. They ensure the accuracy of API call parameters, which is crucially linked to the overall performance. The absence of either component can result in a significant performance decline.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">During the generation phase, we employed 2 main components: domain specific fewshot examples and Chain of Thought prompt. Both aforementioned components led to a substantial reduction in hallucinations. The combined implementation of these components yielded a reduction in hallucinations of up to 71%, consequently resulting in a 22% increase in the final score. The experimental results demonstrate the effectiveness and necessity of the two components.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Perspectives</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our method presents a robust and versatile framework for addressing a wide range of dynamic and complex real-world problems. This approach, however, also opens up several avenues for further investigation.</p>
</div>
<div class="ltx_para" id="S4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Model Cognitive Ability Assessment</span> Most conventional QA evaluation methods primarily focus on accuracy, neglecting the impact of hallucinations. Models should be aware of their knowledge boundaries, discerning what they should and should not answer. CRAG incorporates hallucinations into evaluation metrics, but its settings lack sufficient justification. Responding ”I don’t know” to all questions can yield a satisfactory score. Exploring the assessment of models’ cognitive abilities using methodologies for evaluating human cognition is a promising research direction.
</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">API Integration and Scalability</span>. In real-world scenarios, where extensive API usage is common, our manually designed matching rules are likely to prove inadequate. The development of a more universal method for selecting and calling APIs, as well as processing the returned results, represents a promising avenue for future research.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Handling Dynamic Information</span>. For questions that involve information that changes dynamically over time, simply refusing to answer is merely a basic solution. Future research should focus on exploring methods to acquire the most up-to-date knowledge and determine the timeliness of information. This is crucial to avoid hallucinations caused by outdated knowledge and to ensure the system provides accurate, current information. Developing techniques for real-time information retrieval and verification, as well as implementing mechanisms to assess the reliability and currency of data sources, are key areas for investigation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduce our solution for the Meta KDD CUP 2024: CRAG Comprehensive RAG Benchmark. We adopt a classic RAG framework with two specific routers. In the retrieval phase, we demonstrated the process of obtaining high-quality information from various data sources and utilizing the Domain Router for information filtering. In the augmentation phase, we employed the Domain Router in a similar manner for information aggregation based on domain characteristics. Finally, in the generation phase, we implemented two methods to significantly improve the model’s accuracy and reduce hallucinations, while further mitigating hallucinations through post-processing based on the question’s domain and dynamic nature.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our approach offers a viable pathway for addressing the diverse and dynamic challenges encountered in real-world scenarios. Nevertheless, our method has certain limitations. We have identified several inherent issues in the current methodology and provided our insights and reflections on the specific problems related to our approach. Ultimately, we anticipate that this study will make a modest contribution to the broader RAG and LLM communities.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research was supported by grants from the National Key Research and Development Program of China (Grant No. 2021YFF0901000), the National Natural Science Foundation of China (No. 62337001) and the Fundamental Research Funds for the Central Universities.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">arXiv preprint arXiv:2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Meta AI. 2024.

</span>
<span class="ltx_bibblock">Meta LLaMA 3.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Meta AI Blog</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3/" title="">https://ai.meta.com/blog/meta-llama-3/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em> (Vancouver, BC, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.2">(NIPS ’20)</em>. Curran Associates Inc., Red Hook, NY, USA, Article 159, 25 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.

</span>
<span class="ltx_bibblock">Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2402.03216</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock">Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Companion Proceedings of the ACM on Web Conference 2024</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:268379217" title="">https://api.semanticscholar.org/CorpusID:268379217</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FlagOpen (2024)</span>
<span class="ltx_bibblock">
FlagOpen. 2024.

</span>
<span class="ltx_bibblock">FlagEmbedding Reranker.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker" title="">https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">ArXiv</em> abs/2106.09685 (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:235458009" title="">https://api.semanticscholar.org/CorpusID:235458009</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junzhe Jiang, Shang Qu, Mingyue Cheng, and Qi Liu. 2023.

</span>
<span class="ltx_bibblock">Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">ArXiv</em> abs/2309.10435 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:262054865" title="">https://api.semanticscholar.org/CorpusID:262054865</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yucong Luo, Mingyue Cheng, Hao Zhang, Junyu Lu, Qi Liu, and Enhong Chen. 2023.

</span>
<span class="ltx_bibblock">Unlocking the Potential of Large Language Models for Explainable Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">ArXiv</em> abs/2312.15661 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:266551720" title="">https://api.semanticscholar.org/CorpusID:266551720</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson, Hugo Zaragoza, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2009.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: BM25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">Foundations and Trends® in Information Retrieval</em> 3, 4 (2009), 333–389.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib13.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.4.1">Advances in neural information processing systems</em> 35 (2022), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, et al<span class="ltx_text" id="bib.bib14.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">CRAG–Comprehensive RAG Benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">arXiv preprint arXiv:2406.04744</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>An Example of Web Search Results.</h3>
<figure class="ltx_table" id="A1.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>An Example of Web Search Results.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A1.T3.1.1.1.1">Key</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="A1.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.1.1.2.1">
<span class="ltx_p" id="A1.T3.1.1.1.2.1.1">Value</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.2.1.1">”page name”</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="A1.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.2.1.2.1">
<span class="ltx_p" id="A1.T3.1.2.1.2.1.1">”Microsoft Office 2019 - Wikipedia”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T3.1.3.2.1">”page url”</th>
<td class="ltx_td ltx_align_justify" id="A1.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.3.2.2.1">
<span class="ltx_p" id="A1.T3.1.3.2.2.1.1">”https://en.wikipedia.org/wiki/
<br class="ltx_break"/>Microsoft_Office_2019”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T3.1.4.3.1">”page snippet”</th>
<td class="ltx_td ltx_align_justify" id="A1.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.4.3.2.1">
<span class="ltx_p" id="A1.T3.1.4.3.2.1.1">”For Office 2013 and 2016, various editions containing the client apps were available in both Click-To-Run…”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T3.1.5.4.1">”page last modified”</th>
<td class="ltx_td ltx_align_justify" id="A1.T3.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.5.4.2.1">
<span class="ltx_p" id="A1.T3.1.5.4.2.1.1">”Tue, 27 Feb 2024 22:55:55 GMT”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A1.T3.1.6.5.1">”html page”</th>
<td class="ltx_td ltx_align_justify ltx_border_b" id="A1.T3.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T3.1.6.5.2.1">
<span class="ltx_p" id="A1.T3.1.6.5.2.1.1">”¡!DOCTYPE html¿
<br class="ltx_break"/>¡html class=…¿
<br class="ltx_break"/>¡head¿
<br class="ltx_break"/>…
<br class="ltx_break"/>¡title¿Microsoft Office 2019 - Wikipedia¡/title¿…”</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>An Example of Mock APIs.</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Most mock APIs typically take entities as input and output entity-related information in JSON format. The following is an example of an API in the economic domain:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<svg class="ltx_picture" height="83.06" id="A1.SS2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,83.06) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 77.16 C 0 80.42 2.64 83.06 5.91 83.06 L 594.09 83.06 C 597.36 83.06 600 80.42 600 77.16 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 58.8 L 598.03 58.8 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 64.7)"><foreignobject color="#FFFFFF" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A1.SS2.p2.pic1.1.1.1.1.1.1.1">get_detailed_price_history</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="33.21" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS2.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS2.p2.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.pic1.2.2.2.1.1.1.1">Description:</span> The function returns the past 5 days’ history of 1-minute Stock price, starting from 09:30:00 EST to 15:59:00 EST.</span>
<span class="ltx_p" id="A1.SS2.p2.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.pic1.2.2.2.1.1.2.1">Input:</span></span>
<span class="ltx_itemize" id="A1.I1">
<span class="ltx_item" id="A1.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A1.I1.i1.p1">
<span class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.1">ticker_name</span>: <span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.2">str</span>, upper case</span>
</span></span>
</span>
<span class="ltx_p" id="A1.SS2.p2.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.pic1.2.2.2.1.1.3.1">Output:</span></span>
<span class="ltx_itemize" id="A1.I2">
<span class="ltx_item" id="A1.I2.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A1.I2.i1.p1">
<span class="ltx_p" id="A1.I2.i1.p1.1">Past 5 days’ 1-minute price history: <span class="ltx_text ltx_font_typewriter" id="A1.I2.i1.p1.1.1">json</span></span>
</span></span>
</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Experiment Results of HTML Parsing Methods</h3>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Experiment Results of HTML Parsing Methods</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Time Cost(s)</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.3.1">Success Rate(%)</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.4.1">Score(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.1.1">beautifulsoup</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T4.1.2.1.2">0.31</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.3.1">100.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T4.1.2.1.4">5.11</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.3.2.1.1">boilerpy3</span></th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.3.2.2.1">0.25</span></td>
<td class="ltx_td ltx_align_right" id="A1.T4.1.3.2.3">97.2</td>
<td class="ltx_td ltx_align_right" id="A1.T4.1.3.2.4">9.42</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.4.3.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.4.3.1.1">trafilatura</span></th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.4.3.2">0.64</td>
<td class="ltx_td ltx_align_right" id="A1.T4.1.4.3.3">96.4</td>
<td class="ltx_td ltx_align_right" id="A1.T4.1.4.3.4">10.14</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.5.4.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.5.4.1.1">newspaper</span></th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.5.4.2">1.96</td>
<td class="ltx_td ltx_align_right" id="A1.T4.1.5.4.3">98.8</td>
<td class="ltx_td ltx_align_right" id="A1.T4.1.5.4.4"><span class="ltx_text ltx_font_bold" id="A1.T4.1.5.4.4.1">11.82</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A1.T4.1.6.5.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.6.5.1.1">markdownify</span></th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A1.T4.1.6.5.2">3.65</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A1.T4.1.6.5.3"><span class="ltx_text ltx_font_bold" id="A1.T4.1.6.5.3.1">100.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A1.T4.1.6.5.4">10.94</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4. </span>Prompts Used for Name Entity Recognition</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<svg class="ltx_picture" height="212.82" id="A1.SS4.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,212.82) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 206.92 C 0 210.18 2.64 212.82 5.91 212.82 L 594.09 212.82 C 597.36 212.82 600 210.18 600 206.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 188.87 L 598.03 188.87 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 194.77)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p1.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.p1.pic1.1.1.1.1.1.1.1">Music NER Prompt</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="163.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.1.1">Please identify and list all the named entities present in the following question about music instead answering it, categorizing them appropriately (e.g., persons, song, band) Your answer should be short and concise in 50 words.</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.2.1">Format your response as follows: For each entity, provide the name followed by its category in parentheses. Categories include persons, songs and bands. Ensure that your response is clearly structured and easy to read.</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.3.1">Question: ”</span>{<span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.3.2">query</span>}<span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.3.3">”</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.4"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.4.1">Output only the named entities present in the question. Do not include any other information. If there are no named entities in the question, please provide an empty response.</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.5"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.5.1">Expected Output Format:</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.6"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.6.1">a name of a person in the sentence (person)</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.7"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.7.1">a name of a song in the sentence (song)</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.8"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.8.1">a name of a band in the sentence (band);</span></span>
<span class="ltx_p" id="A1.SS4.p1.pic1.2.2.2.1.1.9"><span class="ltx_text ltx_font_italic" id="A1.SS4.p1.pic1.2.2.2.1.1.9.1">Every entity should be in a new line and be in the format of ”entity_name (entity_category)”
</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p2">
<svg class="ltx_picture" height="246.03" id="A1.SS4.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,246.03) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 240.13 C 0 243.39 2.64 246.03 5.91 246.03 L 594.09 246.03 C 597.36 246.03 600 243.39 600 240.13 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 222.08 L 598.03 222.08 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 227.98)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.p2.pic1.1.1.1.1.1.1.1">Sports NER Prompt</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="196.49" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.1.1">Please identify and list all the named entities present in the following question about sports instead answering it, categorizing them appropriately (e.g., nba team, soccer team, nba player, soccer player) Your answer should be short and concise in 50 words.</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.2.1">Format your response as follows: For each entity, provide the name followed by its category in parentheses. Categories include nba teams, soccer teams, nba players, soccer players. Ensure that your response is clearly structured and easy to read.</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.3.1">Question: ”</span>{<span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.3.2">query</span>}<span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.3.3">”</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.4"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.4.1">Output only the named entities present in the question. Do not include any other information. If there are no named entities in the question, please provide an empty response.</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.5"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.5.1">Expected Output Format:</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.6"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.6.1">a name of a nba team in the sentence (nba team)</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.7"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.7.1">a name of a soccer team in the sentence (soccer team)</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.8"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.8.1">a name of a nba player in the sentence (nba player)</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.9"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.9.1">a name of a soccer player in the sentence (soccer player);</span></span>
<span class="ltx_p" id="A1.SS4.p2.pic1.2.2.2.1.1.10"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.pic1.2.2.2.1.1.10.1">Every entity should be in a new line and be in the format of ”entity_name (entity_category)”
</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p3">
<svg class="ltx_picture" height="212.82" id="A1.SS4.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,212.82) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 206.92 C 0 210.18 2.64 212.82 5.91 212.82 L 594.09 212.82 C 597.36 212.82 600 210.18 600 206.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 188.87 L 598.03 188.87 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 194.77)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.p3.pic1.1.1.1.1.1.1.1">Movie NER Prompt</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="163.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.1.1">Please identify and list all the named entities present in the following question about movie instead answering it, categorizing them appropriately (e.g., person, movie) Your answer should be short and concise in 50 words.</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.2.1">Format your response as follows: For each entity, provide the name followed by its category in parentheses. Categories include persons, and movies. Ensure that your response is clearly structured and easy to read.</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.3.1">Question: ”</span>{<span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.3.2">query</span>}<span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.3.3">”</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.4"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.4.1">Output only the named entities present in the question. Do not include any other information. If there are no named entities in the question, please provide an empty response.</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.5"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.5.1">Expected Output Format:</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.6"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.6.1">a name of a person in the sentence (person)</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.7"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.7.1">a name of a movie in the sentence (movie)</span></span>
<span class="ltx_p" id="A1.SS4.p3.pic1.2.2.2.1.1.8"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.pic1.2.2.2.1.1.8.1">Every entity should be in a new line and be in the format of ”entity_name (entity_category)”
</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p4">
<svg class="ltx_picture" height="215.59" id="A1.SS4.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,215.59) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 209.69 C 0 212.95 2.64 215.59 5.91 215.59 L 594.09 215.59 C 597.36 215.59 600 212.95 600 209.69 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 191.63 L 598.03 191.63 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 197.54)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.p4.pic1.1.1.1.1.1.1.1">Finance NER Prompt</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="166.04" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS4.p4.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.1.1">Please identify and list all the named entities present in the following question about finance instead answering it, categorizing them appropriately (e.g., company, ticker symbol) Your answer should be short and concise in 50 words.</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.2.1">Format your response as follows: For each entity, provide the name followed by its category in parentheses. Categories include company, and symbol(which means the ticker symbol of a company). Ensure that your response is clearly structured and easy to read.</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.3.1">Question: ”</span>{<span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.3.2">query</span>}<span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.3.3">”</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.4"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.4.1">Output only the named entities present in the question. Do not include any other information. If there are no named entities in the question, please provide an empty response.</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.5"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.5.1">Expected Output Format:</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.6"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.6.1">a name of a company in the sentence (company)</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.7"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.7.1">a name of a ticker symbol in the sentence (symbol);</span></span>
<span class="ltx_p" id="A1.SS4.p4.pic1.2.2.2.1.1.8"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.pic1.2.2.2.1.1.8.1">Every entity should be in a new line and be in the format of ”entity_name (entity_category)”
</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  9 07:23:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
