<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.11570] FLRA: A Reference Architecture for Federated Learning Systems</title><meta property="og:description" content="Federated learning is an emerging machine learning paradigm that enables multiple devices to train models locally and formulate a global model, without sharing the clients’ local data. A federated learning system can b…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FLRA: A Reference Architecture for Federated Learning Systems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FLRA: A Reference Architecture for Federated Learning Systems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.11570">

<!--Generated on Wed Mar  6 23:39:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Software architecture Reference architecture Federated learning Pattern Software engineering Machine learning Artificial Intelligence.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Data61, CSIRO, Sydney, Australia </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of New South Wales, Sydney, Australia</span></span></span>
<h1 class="ltx_title ltx_title_document">FLRA: A Reference Architecture for 
<br class="ltx_break">Federated Learning Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sin Kit Lo

</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9156-3225" title="ORCID identifier" class="ltx_ref">0000-0002-9156-3225</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qinghua Lu
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7783-5183" title="ORCID identifier" class="ltx_ref">0000-0002-7783-5183</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hye-Young Paik
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-4425-7388" title="ORCID identifier" class="ltx_ref">0000-0003-4425-7388</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liming Zhu
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5839-3765" title="ORCID identifier" class="ltx_ref">0000-0001-5839-3765</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning is an emerging machine learning paradigm that enables multiple devices to train models locally and formulate a global model, without sharing the clients’ local data. A federated learning system can be viewed as a large-scale distributed system, involving different components and stakeholders with diverse requirements and constraints. Hence, developing a federated learning system requires both software system design thinking and machine learning knowledge. Although much effort has been put into federated learning from the machine learning perspectives, our previous systematic literature review on the area shows that there is a distinct lack of considerations for software architecture design for federated learning. In this paper, we propose FLRA, a reference architecture for federated learning systems, which provides a template design for federated learning-based solutions. The proposed FLRA reference architecture is based on an extensive review of existing patterns of federated learning systems found in the literature and existing industrial implementation. The FLRA reference architecture consists of a pool of architectural patterns that could address the frequently recurring design problems in federated learning architectures. The FLRA reference architecture can serve as a design guideline to assist architects and developers with practical solutions for their problems, which can be further customised.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Software architecture Reference architecture Federated learning Pattern Software engineering Machine learning Artificial Intelligence.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2106.11570/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Federated Learning Overview <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ever-growing use of industrial-scale IoT platforms and smart devices contribute to the exponential growth in data dimensions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which, in turn, empowers the research and applications in AI and machine learning. However, the development of AI and machine learning also significantly elevates data privacy concerns, and General Data Protection Regulation (GDPR)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://gdpr-info.eu/</span></span></span></span> stipulates a range of data protection measures with which many of these systems must comply. This is a particular challenge in machine learning systems as the data that is ready for model training is often insufficient and they frequently suffer from “data hungriness issues”. As data privacy is now one of the most important ethical principles of machine learning systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, there needs to be a solution that can deliver sufficient amount of data for training while the privacy of the data owners is respected.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To tackle this challenge, Google proposed federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> in 2016. Federated learning is a variation of distributed machine learning techniques that enables model training on a highly distributed client devices network. The key feature of federated learning is the training of models using the data collected locally, without transferring the data out of the client devices. A global model is initialised on a central server and broadcast to the participating client devices for local training. The locally trained model parameters are then collected by the central server and aggregated to update global model parameters. The global model parameters are broadcast again for the next training round. Each local training round usually takes a step in the gradient descent process. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an overview of the federated learning process.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A federated learning system can be viewed as a large-scale distributed system, involving different components and stakeholders with diverse requirements and constraints. Hence, developing a federated learning system requires both software system design thinking and machine learning knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Further, despite having various reference architectures for machine learning, big data, industrial IoT, and edge computing systems, to the best of our knowledge, there is still no reference architecture for an end-to-end federated learning system. Based on findings in several federated learning reviews <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the application of federated learning is still limited and immature, with only certain stages of an end-to-end federated learning architecture are extensively studied, leaving many unfilled gaps for architecture and pipeline development. In contrast, many reusable solutions and components were proposed to solve the different challenges of federated learning systems and this motivates the design of a general federated learning system reference architecture. Therefore, this paper presents a pattern-oriented reference architecture that serves as an architecture design guideline and to facilitate the end-to-end development and operations of federated learning systems, while taking different quality attributes and constraints into considerations. This work provides the following contributions:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A pattern-oriented federated learning reference architecture named FLRA, generated from the findings of a systematic literature review (SLR) and mining of industrial best practices on machine learning system implementations.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A pool of patterns associated with the different components of the FLRA reference architecture that target to address the recurring design problems in federated learning architectures.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The structure of the paper is as follows. Section <a href="#S2" title="2 Methodology ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> introduces the methodology for the reference architecture design, followed by the presentation of the reference architecture in Section <a href="#S3" title="3 FLRA Reference Architecture ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Section <a href="#S4" title="4 Related Work ‣ 3.5 Model monitoring ‣ 3.4 Model deployment ‣ 3.3.4 Model aggregation. ‣ 3.3 Model training ‣ 3.2 Data collection &amp; preprocessing ‣ 3.1 Job creation ‣ 3 FLRA Reference Architecture ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the related work. Section <a href="#S5" title="5 Discussion &amp; Conclusion ‣ 4 Related Work ‣ 3.5 Model monitoring ‣ 3.4 Model deployment ‣ 3.3.4 Model aggregation. ‣ 3.3 Model training ‣ 3.2 Data collection &amp; preprocessing ‣ 3.1 Job creation ‣ 3 FLRA Reference Architecture ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the discussions of this work and finally concludes this paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We have employed parts of an empirically-grounded design methodology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to design the federated learning reference architecture. Firstly, the design and development of this reference architecture are based on empirical evidence collected through our systematic literature review on 231 federated learning academic literature from a software engineering perspective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The review is conducted based on Kitchenham’s guideline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> with which we designed a comprehensive protocol for the review’s initial paper search, paper screening, quality assessments, data extractions, analyses, and synthesis. We have also adopted the software development practices of machine learning systems in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to describe the software development lifecycle (SDLC) for federated learning. Using the stages of this lifeycycle as a guide, we formulated our research questions as: (1) Background understanding; (2) Requirement analysis; (3) Architecture design; and (4) Implementation &amp; evaluation. One major finding of the SLR is that federated learning research and applications are still highly immature, and certain stages of an end-to-end federated learning architecture still lack extensive studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. However, we have also identified many solutions and components proposed to solve the different challenges of federated learning systems, which can be reused and adapted. This motivates the design of a federated learning system reference architecture.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Based on the findings, we specifically adopted the qualitative methods in empirical studies of software architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> to develop and confirm the theory for the reference architecture design. The proposition is generated based on syntheses and validations of the different recurring customers and business needs of the federated learning systems, in addition to the collections and analyses of the reusable patterns to address these architectural needs. We then conducted studies on some of the best practices in centralised and distributed machine learning systems to cover some of the components that are not covered in the federated learning studies. The main processes are the: (1) <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">generation of theory</span> and (2) <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">confirmation of theory</span>. The architecture design methodology is illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Methodology ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2106.11570/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="210" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Methodology for federated learning reference architecture design.</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generation of theory</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The generation of the initial design of the reference architecture theory is performed in this stage. Since there is no standard reference architecture for federated learning yet, we generated the theory by referring to the architecture of a machine learning system. Here, we adopted <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">cross-case analysis</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as the theory generation method, which is an analysis method that compares two different cases based on some attributes and examines their similarities and differences. We performed a <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">cross-case analysis</span> on the pipeline design of conventional machine learning and federated learning systems. Here, we reviewed several machine learning architectures proposed by well-known companies, such as Google<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</span></span></span></span>, Microsoft<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment</span></span></span></span>, and Amazon<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</span></span></span></span>, specifically on their machine learning pipeline designs. Furthermore, based on our previous project implementation experience, we defined a general federated learning pipeline based on the standards proposed by these industry players that covers <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_italic">job creation, data collection, data preprocessing (cleaning, labeling, augmentation, etc.), model training, model evaluation, model deployment</span>, and <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_italic">model monitoring</span> stage. Since federated learning was first introduced by Google, the pipeline components analysis and mining are performed heavily on the federated learning standards proposed by Google researchers in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and the frameworks for federated learning system benchmark and simulation, such as Tensorflow Federated (TFF)<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.tensorflow.org/federated</span></span></span></span>, LEAF<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/TalwalkarLab/leaf</span></span></span></span>, and FedML<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://fedml.ai/</span></span></span></span>. From the findings, we were able to conclude that the <span id="S2.SS1.p1.1.5" class="ltx_text ltx_font_italic">data collection</span> is fairly similar whereas <span id="S2.SS1.p1.1.6" class="ltx_text ltx_font_italic">data preprocessing, model training, model evaluation, model deployment</span>, and <span id="S2.SS1.p1.1.7" class="ltx_text ltx_font_italic">model monitoring</span> stages for federated learning systems are different from machine learning pipelines. Especially for the <span id="S2.SS1.p1.1.8" class="ltx_text ltx_font_italic">model training</span> stage, the federated learning pipelines encapsulate <span id="S2.SS1.p1.1.9" class="ltx_text ltx_font_italic">model broadcast, local model training, model upload and collection</span>, and <span id="S2.SS1.p1.1.10" class="ltx_text ltx_font_italic">model aggregation</span> operation under this single stage. Furthermore, the iterative interaction between multiple client devices with one central server is the key design consideration of the federated learning architecture, and therefore, most academic work extensively studied the <span id="S2.SS1.p1.1.11" class="ltx_text ltx_font_italic">model training</span> stage and proposed many solutions which can be adapted as reusable components or patterns to address different requirements.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Besides observing the pipeline design and the components, we performed <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">qualitative content analyses</span> on existing machine learning and federated learning systems proposed by industrial practitioners and academics to extract requirements, reusable patterns, and components for the design of the reference architecture. In the SLR, a series of system quality attributes are defined based on ISO/IEC 25010 System and Software Quality model<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://iso25000.com/index.php/en/iso-25000-standards/iso-25010</span></span></span></span> and ISO/IEC 25012<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://iso25000.com/index.php/en/iso-25000-standards/iso-25012</span></span></span></span> Data Quality model to record the different challenges of a federated learning system addressed by researchers. The empirical evidence associated with each quality attribute and business need is analysed and validated as the support for the design proposition of the reference architecture. After the generation of theory for the hypothesis of the reference architecture, we designed the reference architecture according to the theory.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Confirmation of theory</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this stage, we confirmed and verified the viability and applicability of the reference architecture proposed. Since this reference architecture is built from scratch based on the patterns and requirements collected through qualitative analyses, we evaluated the architecture by building a convincing body of evidence to support the reference architecture, which is different from conventional evaluation approaches. We employed the qualitative validation method known as <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">triangulation</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The basic idea is to gather different types of evidence to support a proposition. The evidence might come from different sources, be collected using different methods, and in our case, the evidence is from the SLR and the industrial implementations of machine learning systems from renowned institutions and companies, and our previous implementation experience.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We have reviewed these evidence based on the SLDC lifecycle of machine learning systems we developed for the SLR to identify the adoptions of the different reusable patterns or components, in addition to the basic machine learning pipeline components that are mentioned in these evidence. These mentions and adoptions are collected to prove applicability of the instantiated components in the federated learning reference architecture. In short, the <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">triangulation</span> process justified that the reference architecture is applicable as it is supported by various empirical evidence we collected and analysed.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2106.11570/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="322" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">FLRA: a reference architecture of federated learning systems.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FLRA Reference Architecture</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present FLRA, a pattern-oriented reference architecture for federated learning systems. Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Confirmation of theory ‣ 2 Methodology ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the overall reference architecture. A base version of a federated learning system consists of two main participants: (1) central server and (2) client devices. A central server initialises a machine learning job and coordinates the federated training process, whereas client devices perform model training using local data and computation resources.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Underneath the two participants, there are two types of components: (1) Mandatory components and (2) optional components. The mandatory components provide the basic functions required by a federated machine learning pipeline.
To fulfill the different software quality requirements and design constraints in federated learning systems, we collected and defined a set of patterns based on our SLR results and the mining of some existing federated learning simulation frameworks. Each pattern is embedded as optional components to facilitate the architecture design.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We summarised all the mandatory and optional components of the reference architecture and briefly highlighted the functionalities and responsibility of each component in Table <a href="#S3" title="3 FLRA Reference Architecture ‣ FLRA: A Reference Architecture for Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The table presents the details of each component associated with the federated learning pipeline stages.</p>
</div>
<figure id="S3.tab1" class="ltx_table">

<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S3.tab1.3.1.1" class="ltx_text" style="font-size:129%;">Table 1</span>: </span><span id="S3.tab1.4.2" class="ltx_text" style="font-size:129%;">Components of the federated learning reference architecture</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.tab1.5" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S3.tab1.5.1" class="ltx_tr">
<td id="S3.tab1.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.1.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.1.1.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.1.1.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">
<span id="S3.tab1.5.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.tab1.5.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S3.tab1.5.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Stages</span></span>
</span></span><span id="S3.tab1.5.1.1.1.1.3" class="ltx_text"></span><span id="S3.tab1.5.1.1.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.tab1.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.1.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.1.2.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.1.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">
<span id="S3.tab1.5.1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.tab1.5.1.2.1.1.2.1.1" class="ltx_tr">
<span id="S3.tab1.5.1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Types</span></span>
</span></span><span id="S3.tab1.5.1.2.1.1.3" class="ltx_text"></span><span id="S3.tab1.5.1.2.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.tab1.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.1.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.1.3.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.1.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">
<span id="S3.tab1.5.1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.tab1.5.1.3.1.1.2.1.1" class="ltx_tr">
<span id="S3.tab1.5.1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Components</span></span>
</span></span><span id="S3.tab1.5.1.3.1.1.3" class="ltx_text"></span><span id="S3.tab1.5.1.3.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.tab1.5.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.1.4.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.1.4.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.1.4.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">
<span id="S3.tab1.5.1.4.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.tab1.5.1.4.1.1.2.1.1" class="ltx_tr">
<span id="S3.tab1.5.1.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Responsibility</span></span>
</span></span><span id="S3.tab1.5.1.4.1.1.3" class="ltx_text"></span><span id="S3.tab1.5.1.4.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;"></span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.2" class="ltx_tr">
<td id="S3.tab1.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="4">
<span id="S3.tab1.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.2.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.2.1.1.1.1" class="ltx_text" style="font-size:70%;">
Job creation</span></span>
</span>
</td>
<td id="S3.tab1.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.2.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.2.2.1.1.1" class="ltx_text" style="font-size:70%;">Mandatory</span></span>
</span>
</td>
<td id="S3.tab1.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.2.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.2.3.1.1.1" class="ltx_text" style="font-size:70%;">Job creator</span></span>
</span>
</td>
<td id="S3.tab1.5.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.2.4.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.2.4.1.1.1" class="ltx_text" style="font-size:70%;">Initialises training job and global model</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.3" class="ltx_tr">
<td id="S3.tab1.5.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S3.tab1.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.3.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Optional</span></span>
</span>
</td>
<td id="S3.tab1.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.3.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.3.2.1.1.1" class="ltx_text" style="font-size:70%;">Client registry</span></span>
</span>
</td>
<td id="S3.tab1.5.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.3.3.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.3.3.1.1.1" class="ltx_text" style="font-size:70%;">Improves system’s </span><span id="S3.tab1.5.3.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">maintainability</span><span id="S3.tab1.5.3.3.1.1.3" class="ltx_text" style="font-size:70%;"> and </span><span id="S3.tab1.5.3.3.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;">reliability</span><span id="S3.tab1.5.3.3.1.1.5" class="ltx_text" style="font-size:70%;"> by maintaining client’s information</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.4" class="ltx_tr">
<td id="S3.tab1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.4.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.4.1.1.1.1" class="ltx_text" style="font-size:70%;">Client cluster</span></span>
</span>
</td>
<td id="S3.tab1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.4.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.4.2.1.1.1" class="ltx_text" style="font-size:70%;">Tackles </span><span id="S3.tab1.5.4.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">statistical heterogeneity</span><span id="S3.tab1.5.4.2.1.1.3" class="ltx_text" style="font-size:70%;"> &amp; </span><span id="S3.tab1.5.4.2.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;">system heterogeneity</span><span id="S3.tab1.5.4.2.1.1.5" class="ltx_text" style="font-size:70%;"> by grouping clients with similar data distribution or resources before aggregation</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.5" class="ltx_tr">
<td id="S3.tab1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.5.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.5.1.1.1.1" class="ltx_text" style="font-size:70%;">Client selector</span></span>
</span>
</td>
<td id="S3.tab1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.5.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.5.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.5.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">model &amp; system’s performance</span><span id="S3.tab1.5.5.2.1.1.3" class="ltx_text" style="font-size:70%;"> by selecting high performance client devices</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.6" class="ltx_tr">
<td id="S3.tab1.5.6.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" rowspan="3">
<span id="S3.tab1.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.6.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.6.1.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.6.1.1.1.2" class="ltx_text" style="font-size:70%;">
Data
collection
&amp;
preprocessing</span></span>
</span>
</td>
<td id="S3.tab1.5.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S3.tab1.5.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.6.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.6.2.1.1.1" class="ltx_text" style="font-size:70%;">Mandatory</span></span>
</span>
</td>
<td id="S3.tab1.5.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.6.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.6.3.1.1.1" class="ltx_text" style="font-size:70%;">Data collector</span></span>
</span>
</td>
<td id="S3.tab1.5.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.6.4.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.6.4.1.1.1" class="ltx_text" style="font-size:70%;">Collects raw data through sensors or smart devices deployed</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.7" class="ltx_tr">
<td id="S3.tab1.5.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.7.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.7.1.1.1.1" class="ltx_text" style="font-size:70%;">Data preprocessor</span></span>
</span>
</td>
<td id="S3.tab1.5.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.7.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.7.2.1.1.1" class="ltx_text" style="font-size:70%;">Preprocesses raw data</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.8" class="ltx_tr">
<td id="S3.tab1.5.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.8.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.8.1.1.1.1" class="ltx_text" style="font-size:70%;">Optional</span></span>
</span>
</td>
<td id="S3.tab1.5.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.8.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.8.2.1.1.1" class="ltx_text" style="font-size:70%;">Heterogeneous Data Handler</span></span>
</span>
</td>
<td id="S3.tab1.5.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.8.3.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.8.3.1.1.1" class="ltx_text" style="font-size:70%;">Tackles </span><span id="S3.tab1.5.8.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">statistical heterogeneity</span><span id="S3.tab1.5.8.3.1.1.3" class="ltx_text" style="font-size:70%;"> through data augmentation methods</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.9" class="ltx_tr">
<td id="S3.tab1.5.9.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" rowspan="10">
<span id="S3.tab1.5.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.9.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.9.1.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.9.1.1.1.2" class="ltx_text" style="font-size:70%;">
Model
training</span></span>
</span>
</td>
<td id="S3.tab1.5.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="3">
<span id="S3.tab1.5.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.9.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.9.2.1.1.1" class="ltx_text" style="font-size:70%;">Mandatory</span></span>
</span>
</td>
<td id="S3.tab1.5.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.9.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.9.3.1.1.1" class="ltx_text" style="font-size:70%;">Model trainer</span></span>
</span>
</td>
<td id="S3.tab1.5.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.9.4.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.9.4.1.1.1" class="ltx_text" style="font-size:70%;">Trains local model</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.10" class="ltx_tr">
<td id="S3.tab1.5.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.10.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.10.1.1.1.1" class="ltx_text" style="font-size:70%;">Local model evaluator</span></span>
</span>
</td>
<td id="S3.tab1.5.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.10.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.10.2.1.1.1" class="ltx_text" style="font-size:70%;">Evaluates local model performance after each local training round</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.11" class="ltx_tr">
<td id="S3.tab1.5.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.11.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.11.1.1.1.1" class="ltx_text" style="font-size:70%;">Model aggregator</span></span>
</span>
</td>
<td id="S3.tab1.5.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.11.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.11.2.1.1.1" class="ltx_text" style="font-size:70%;">Aggregates local models to produce new global model</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.12" class="ltx_tr">
<td id="S3.tab1.5.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="7">
<span id="S3.tab1.5.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.12.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.12.1.1.1.1" class="ltx_text" style="font-size:70%;">Optional</span></span>
</span>
</td>
<td id="S3.tab1.5.12.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.12.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.12.2.1.1.1" class="ltx_text" style="font-size:70%;">Multi-task</span><span id="S3.tab1.5.12.2.1.1.2" class="ltx_text" style="font-size:70%;">
model trainer</span></span>
</span>
</td>
<td id="S3.tab1.5.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.12.3.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.12.3.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.12.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">model performance</span><span id="S3.tab1.5.12.3.1.1.3" class="ltx_text" style="font-size:70%;"> (personalisation) by adopting multi-task training methods</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.13" class="ltx_tr">
<td id="S3.tab1.5.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.13.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.13.1.1.1.1" class="ltx_text" style="font-size:70%;">Message compressor</span></span>
</span>
</td>
<td id="S3.tab1.5.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.13.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.13.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.13.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">communication efficiency</span><span id="S3.tab1.5.13.2.1.1.3" class="ltx_text" style="font-size:70%;"> through message size reduction to reduce bandwidth consumption</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.14" class="ltx_tr">
<td id="S3.tab1.5.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.14.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.14.1.1.1.1" class="ltx_text" style="font-size:70%;">Secure aggregator</span></span>
</span>
</td>
<td id="S3.tab1.5.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.14.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.14.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.14.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">data privacy</span><span id="S3.tab1.5.14.2.1.1.3" class="ltx_text" style="font-size:70%;"> &amp; </span><span id="S3.tab1.5.14.2.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;">system security</span><span id="S3.tab1.5.14.2.1.1.5" class="ltx_text" style="font-size:70%;"> through different secure multiparty computation protocols</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.15" class="ltx_tr">
<td id="S3.tab1.5.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.15.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.15.1.1.1.1" class="ltx_text" style="font-size:70%;">Asynchronous aggregator</span></span>
</span>
</td>
<td id="S3.tab1.5.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.15.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.15.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.15.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">system performance</span><span id="S3.tab1.5.15.2.1.1.3" class="ltx_text" style="font-size:70%;"> by reducing aggregation pending time of late client updates</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.16" class="ltx_tr">
<td id="S3.tab1.5.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.16.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.16.1.1.1.1" class="ltx_text" style="font-size:70%;">Decentralised aggregator</span></span>
</span>
</td>
<td id="S3.tab1.5.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.16.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.16.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves system </span><span id="S3.tab1.5.16.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">reliability</span><span id="S3.tab1.5.16.2.1.1.3" class="ltx_text" style="font-size:70%;"> through the removal of single-point-of-failure</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.17" class="ltx_tr">
<td id="S3.tab1.5.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.17.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.17.1.1.1.1" class="ltx_text" style="font-size:70%;">Hierarchical aggregator</span></span>
</span>
</td>
<td id="S3.tab1.5.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.17.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.17.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.17.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">system performance</span><span id="S3.tab1.5.17.2.1.1.3" class="ltx_text" style="font-size:70%;"> &amp; tackle </span><span id="S3.tab1.5.17.2.1.1.4" class="ltx_text ltx_font_bold" style="font-size:70%;">statistical heterogeneity</span><span id="S3.tab1.5.17.2.1.1.5" class="ltx_text" style="font-size:70%;"> &amp; </span><span id="S3.tab1.5.17.2.1.1.6" class="ltx_text ltx_font_bold" style="font-size:70%;">system heterogeneity</span><span id="S3.tab1.5.17.2.1.1.7" class="ltx_text" style="font-size:70%;"> by aggregating models from similar clients before global aggregation</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.18" class="ltx_tr">
<td id="S3.tab1.5.18.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.18.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.18.1.1.1.1" class="ltx_text" style="font-size:70%;">Model co-versioning</span><span id="S3.tab1.5.18.1.1.1.2" class="ltx_text" style="font-size:70%;">
registry</span></span>
</span>
</td>
<td id="S3.tab1.5.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.18.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.18.2.1.1.1" class="ltx_text" style="font-size:70%;">Improves system’s </span><span id="S3.tab1.5.18.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">accountability</span><span id="S3.tab1.5.18.2.1.1.3" class="ltx_text" style="font-size:70%;"> by recording the local models associated to each global models to track clients’ performances</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.19" class="ltx_tr">
<td id="S3.tab1.5.19.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" rowspan="4">
<span id="S3.tab1.5.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.19.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.19.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span><span id="S3.tab1.5.19.1.1.1.2" class="ltx_text" style="font-size:70%;">
deployment</span></span>
</span>
</td>
<td id="S3.tab1.5.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S3.tab1.5.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.19.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.19.2.1.1.1" class="ltx_text" style="font-size:70%;">Mandatory</span></span>
</span>
</td>
<td id="S3.tab1.5.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.19.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.19.3.1.1.1" class="ltx_text" style="font-size:70%;">Model deployer</span></span>
</span>
</td>
<td id="S3.tab1.5.19.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.19.4.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.19.4.1.1.1" class="ltx_text" style="font-size:70%;">Deploys completely-trained-models</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.20" class="ltx_tr">
<td id="S3.tab1.5.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.20.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.20.1.1.1.1" class="ltx_text" style="font-size:70%;">Decision maker</span></span>
</span>
</td>
<td id="S3.tab1.5.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.20.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.20.2.1.1.1" class="ltx_text" style="font-size:70%;">Decides model deployment</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.21" class="ltx_tr">
<td id="S3.tab1.5.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S3.tab1.5.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.21.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.21.1.1.1.1" class="ltx_text" style="font-size:70%;">Optional</span></span>
</span>
</td>
<td id="S3.tab1.5.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.21.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.21.2.1.1.1" class="ltx_text" style="font-size:70%;">Deployment selector</span></span>
</span>
</td>
<td id="S3.tab1.5.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.21.3.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.21.3.1.1.1" class="ltx_text" style="font-size:70%;">Improves </span><span id="S3.tab1.5.21.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">model performance</span><span id="S3.tab1.5.21.3.1.1.3" class="ltx_text" style="font-size:70%;"> (personalisation) through suitable model users selection according to data or applications</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.22" class="ltx_tr">
<td id="S3.tab1.5.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.22.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.22.1.1.1.1" class="ltx_text" style="font-size:70%;">Incentive registry</span></span>
</span>
</td>
<td id="S3.tab1.5.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.tab1.5.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.22.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.22.2.1.1.1" class="ltx_text" style="font-size:70%;">Increases clients’ </span><span id="S3.tab1.5.22.2.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">motivatability</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.23" class="ltx_tr">
<td id="S3.tab1.5.23.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt" rowspan="2">
<span id="S3.tab1.5.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.23.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.23.1.1.1.1" class="ltx_text"></span><span id="S3.tab1.5.23.1.1.1.2" class="ltx_text" style="font-size:70%;">
Model
monitoring</span></span>
</span>
</td>
<td id="S3.tab1.5.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.23.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.23.2.1.1.1" class="ltx_text" style="font-size:70%;">Mandatory</span></span>
</span>
</td>
<td id="S3.tab1.5.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.23.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.23.3.1.1.1" class="ltx_text" style="font-size:70%;">Model monitor</span></span>
</span>
</td>
<td id="S3.tab1.5.23.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.tab1.5.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.23.4.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.23.4.1.1.1" class="ltx_text" style="font-size:70%;">Monitors model’s data inference performance</span></span>
</span>
</td>
</tr>
<tr id="S3.tab1.5.24" class="ltx_tr">
<td id="S3.tab1.5.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.tab1.5.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.24.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.tab1.5.24.1.1.1.1" class="ltx_text" style="font-size:70%;">Optional</span></span>
</span>
</td>
<td id="S3.tab1.5.24.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.tab1.5.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.24.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.tab1.5.24.2.1.1.1" class="ltx_text" style="font-size:70%;">Model replacement</span><span id="S3.tab1.5.24.2.1.1.2" class="ltx_text" style="font-size:70%;">
trigger</span></span>
</span>
</td>
<td id="S3.tab1.5.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.tab1.5.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.tab1.5.24.3.1.1" class="ltx_p" style="width:195.1pt;"><span id="S3.tab1.5.24.3.1.1.1" class="ltx_text" style="font-size:70%;">Maintains </span><span id="S3.tab1.5.24.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">system &amp; model performance</span><span id="S3.tab1.5.24.3.1.1.3" class="ltx_text" style="font-size:70%;"> by replacing outdated models due to performance degrades</span></span>
</span>
</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S3.SS1" class="ltx_subsection ltx_figure_panel">
<h3 class="ltx_title ltx_title_subsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Job creation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:70%;">The federated learning process starts with the creation of a model training job (including initial model and training configurations) via </span><span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">job creator</span><span id="S3.SS1.p1.1.3" class="ltx_text" style="font-size:70%;"> on the central server. Within the </span><span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">job creator</span><span id="S3.SS1.p1.1.5" class="ltx_text" style="font-size:70%;"> component, three optional components could be considered are: </span><span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client registry</span><span id="S3.SS1.p1.1.7" class="ltx_text" style="font-size:70%;">, </span><span id="S3.SS1.p1.1.8" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client cluster</span><span id="S3.SS1.p1.1.9" class="ltx_text" style="font-size:70%;">, </span><span id="S3.SS1.p1.1.10" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client selector</span><span id="S3.SS1.p1.1.11" class="ltx_text" style="font-size:70%;">. In a federated learning system, client devices may be owned by different parties, constantly connect and drop out from the system. Hence, it is challenging to keep track of all the participating client devices including dropout devices and dishonest devices. This is different from distributed or centralised machine learning systems in which both clients and the server are typically owned and managed by a single party </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.12.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.SS1.p1.1.13.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p1.1.14" class="ltx_text" style="font-size:70%;">. A </span><span id="S3.SS1.p1.1.15" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client registry</span><span id="S3.SS1.p1.1.16" class="ltx_text" style="font-size:70%;"> is required to maintain all the information of the client devices that are registered, (e.g., ID, resource information, number of participating rounds, local model performance, etc.) Both IBM Federated Learning Framework</span><span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/IBM/federated-learning-lib</span></span></span></span><span id="S3.SS1.p1.1.17" class="ltx_text" style="font-size:70%;"> and doc.ai</span><span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doc.ai/</span></span></span></span><span id="S3.SS1.p1.1.18" class="ltx_text" style="font-size:70%;"> adopted client registry in their design to improve maintainability and reliability of the system since the system can manage the devices effectively and quickly identify the problematic ones via the </span><span id="S3.SS1.p1.1.19" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client registry</span><span id="S3.SS1.p1.1.20" class="ltx_text" style="font-size:70%;"> component. FedML which is a federated learning benchmarking and simulation framework has also explicitly covered the client manager module in their framework that serves the same purpose as the </span><span id="S3.SS1.p1.1.21" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client registry</span><span id="S3.SS1.p1.1.22" class="ltx_text" style="font-size:70%;">. However, the system may sacrifice client data privacy due to the recording of the device information on the central server.</span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text" style="font-size:70%;">The non-IID</span><span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Non-Identically and Independently Distribution: Highly-skewed and personalised data distribution that vary heavily between different clients and affects the model performance and generalisation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</span></span></span><span id="S3.SS1.p2.1.2" class="ltx_text" style="font-size:70%;"> data characteristics of local raw data and the data-sharing restriction translates to model performance challenge </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p2.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S3.SS1.p2.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p2.1.5" class="ltx_text" style="font-size:70%;">. When the data from client devices are non-IID, the global models aggregated is less generalised to the entire data. To improve the generalisation of the global model and speed up model convergence, a </span><span id="S3.SS1.p2.1.6" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client cluster</span><span id="S3.SS1.p2.1.7" class="ltx_text" style="font-size:70%;"> component can be added to cluster the client devices into groups according to their data distribution, gradient loss, and feature similarities. This design has been used in Google’s IFCA algorithm</span><span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/felisat/clustered-federated-learning</span></span></span></span><span id="S3.SS1.p2.1.8" class="ltx_text" style="font-size:70%;">, TiFL system</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p2.1.9.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S3.SS1.p2.1.10.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p2.1.11" class="ltx_text" style="font-size:70%;">, and Massachusetts General Hospital’s patient system</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p2.1.12.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S3.SS1.p2.1.13.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p2.1.14" class="ltx_text" style="font-size:70%;">. The side effect of </span><span id="S3.SS1.p2.1.15" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client cluster</span><span id="S3.SS1.p2.1.16" class="ltx_text" style="font-size:70%;"> is the extra computation cost caused by client relationship quantification.</span></p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text" style="font-size:70%;">The central servers interacts with a massive number of client devices that are both system heterogeneous and statistically heterogeneous. The magnitude of client devices number is also several times larger than that of the distributed machine learning systems </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.SS1.p3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p3.1.4" class="ltx_text" style="font-size:70%;">. To increase the model and system performance, client devices can be selected every round with predefined criteria (e.g., resource, data, or performance) via </span><span id="S3.SS1.p3.1.5" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">client selector</span><span id="S3.SS1.p3.1.6" class="ltx_text" style="font-size:70%;"> component. This has been integrated into Google’s FedAvg </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p3.1.7.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS1.p3.1.8.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p3.1.9" class="ltx_text" style="font-size:70%;"> algorithm and IBM’s Helios </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p3.1.10.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S3.SS1.p3.1.11.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS1.p3.1.12" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data collection &amp; preprocessing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="font-size:70%;">Each client device gathers data using different </span><span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">sensors</span><span id="S3.SS2.p1.1.3" class="ltx_text" style="font-size:70%;"> through the </span><span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">data collector</span><span id="S3.SS2.p1.1.5" class="ltx_text" style="font-size:70%;"> component and process the data (i.e., feature extraction, data cleaning, labeling, augmentation, etc.) locally through the </span><span id="S3.SS2.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">data preprocessor</span><span id="S3.SS2.p1.1.7" class="ltx_text" style="font-size:70%;"> component, due to the data-sharing constraint. This is different from centralised or distributed machine learning systems in which the non-IID data characteristic is negligible since the data collected on client devices are usually shuffled and processed on the central server. Thus, within the </span><span id="S3.SS2.p1.1.8" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">data preprocessor</span><span id="S3.SS2.p1.1.9" class="ltx_text" style="font-size:70%;">, an optional component </span><span id="S3.SS2.p1.1.10" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">heterogeneous data handler</span><span id="S3.SS2.p1.1.11" class="ltx_text" style="font-size:70%;"> is adopted to deal with the non-IID and skewed data distribution issue through data augmentation techniques. The known uses of the component include Astraea</span><span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/mtang724/Self-Balancing-Federated-Learning</span></span></span></span><span id="S3.SS2.p1.1.12" class="ltx_text" style="font-size:70%;">, FAug scheme </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.13.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S3.SS2.p1.1.14.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS2.p1.1.15" class="ltx_text" style="font-size:70%;"> and Federated Distillation (FD) method </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.16.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S3.SS2.p1.1.17.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS2.p1.1.18" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model training</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Local model training.</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p"><span id="S3.SS3.SSS1.p1.1.1" class="ltx_text" style="font-size:70%;">Once the client receives the job from the central server, the </span><span id="S3.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model trainer</span><span id="S3.SS3.SSS1.p1.1.3" class="ltx_text" style="font-size:70%;"> component performs model training based on configured hyperparameters (number of epochs, learning rate, etc.). In the standard federated learning training process proposed by McMahan in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS1.p1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS3.SSS1.p1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS1.p1.1.6" class="ltx_text" style="font-size:70%;">, only model parameters (i.e., weight/gradient) are mentioned to be sent from the central server, whereas in this reference architecture, the models include not only the model parameters but also the hyperparameters. For multi-task machine learning scenarios, a </span><span id="S3.SS3.SSS1.p1.1.7" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">multi-task model trainer</span><span id="S3.SS3.SSS1.p1.1.8" class="ltx_text" style="font-size:70%;"> component can be chosen to train task-related models to improve model performance and learning efficiency. Multitask Learning is a machine learning approach to transfer and share knowledge through training of individual models. It improves model generalisation by using the domain information contained in the parameters of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS1.p1.1.9.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S3.SS3.SSS1.p1.1.10.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS1.p1.1.11" class="ltx_text" style="font-size:70%;">. In federated learning scenarios, this technique is particularly relevant when faced with non-IID data which can produce personalised model that may outperform the best possible shared global model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS1.p1.1.12.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.SS3.SSS1.p1.1.13.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS1.p1.1.14" class="ltx_text" style="font-size:70%;">. This best practice solution is identified based on Google’s MultiModel</span><span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html</span></span></span></span><span id="S3.SS3.SSS1.p1.1.15" class="ltx_text" style="font-size:70%;"> architecture, and Microsoft’s MT-DNN</span><span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/microsoft/MT-DNN</span></span></span></span><span id="S3.SS3.SSS1.p1.1.16" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Model evaluation.</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p"><span id="S3.SS3.SSS2.p1.1.1" class="ltx_text" style="font-size:70%;">The </span><span id="S3.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">local model evaluator</span><span id="S3.SS3.SSS2.p1.1.3" class="ltx_text" style="font-size:70%;"> component measures the performance of the local model and uploads the model to the </span><span id="S3.SS3.SSS2.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model aggregator</span><span id="S3.SS3.SSS2.p1.1.5" class="ltx_text" style="font-size:70%;"> on the central server if the performance requirement is met. In distributed machine learning systems, the performance evaluation on client devices is not conducted locally, and only the aggregated server model is evaluated. However, for federated learning systems, local model performance evaluation is required for system operations such as client selection, model co-versioning, contributions calculation, incentive provision, client clustering, etc.</span></p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Model uploading.</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p"><span id="S3.SS3.SSS3.p1.1.1" class="ltx_text" style="font-size:70%;">The trained local model parameters or gradients are uploaded to the central server for model aggregation. Unlike centralised machine learning systems that performs model training in a central server or distributed machine learning systems that deals with fairly small amount of client nodes, the cost for transmitting model parameters or gradients between the bandwidth-limited client devices and central server is high when the system scales up </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS3.p1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.SS3.SSS3.p1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS3.p1.1.4" class="ltx_text" style="font-size:70%;">. A </span><span id="S3.SS3.SSS3.p1.1.5" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">message compressor</span><span id="S3.SS3.SSS3.p1.1.6" class="ltx_text" style="font-size:70%;"> component can be added to improve communication efficiency. The embedded pattern are extracted from Google Sketched Update </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS3.p1.1.7.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.SS3.SSS3.p1.1.8.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS3.p1.1.9" class="ltx_text" style="font-size:70%;">, and IBM PruneFL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS3.p1.1.10.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.SS3.SSS3.p1.1.11.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS3.p1.1.12" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Model aggregation.</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.1" class="ltx_p"><span id="S3.SS3.SSS4.p1.1.1" class="ltx_text" style="font-size:70%;">The </span><span id="S3.SS3.SSS4.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model aggregator</span><span id="S3.SS3.SSS4.p1.1.3" class="ltx_text" style="font-size:70%;"> formulates the new global model based on the received local models. There are four types of aggregator-related optional components within the </span><span id="S3.SS3.SSS4.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model aggregator</span><span id="S3.SS3.SSS4.p1.1.5" class="ltx_text" style="font-size:70%;"> component: </span><span id="S3.SS3.SSS4.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">secure aggregator</span><span id="S3.SS3.SSS4.p1.1.7" class="ltx_text" style="font-size:70%;">, </span><span id="S3.SS3.SSS4.p1.1.8" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">asynchronous aggregator</span><span id="S3.SS3.SSS4.p1.1.9" class="ltx_text" style="font-size:70%;">, </span><span id="S3.SS3.SSS4.p1.1.10" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">decentralised aggregator</span><span id="S3.SS3.SSS4.p1.1.11" class="ltx_text" style="font-size:70%;">, and </span><span id="S3.SS3.SSS4.p1.1.12" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">hierarchical aggregator</span><span id="S3.SS3.SSS4.p1.1.13" class="ltx_text" style="font-size:70%;">. A </span><span id="S3.SS3.SSS4.p1.1.14" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">secure aggregator</span><span id="S3.SS3.SSS4.p1.1.15" class="ltx_text" style="font-size:70%;"> component prevents adversarial parties from accessing the models during model exchanges through multiparty computation protocols, such as differential privacy or cryptographic techniques. These techniques provide security proof to guarantee that each party knows only its input and output. For centralised and distributed machine learning settings that practice centralised system orchestration, communication security between clients and server is not the main concern. In contrast, for federated learning settings, this best practices are used in SecAgg </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.16.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S3.SS3.SSS4.p1.1.17.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.18" class="ltx_text" style="font-size:70%;">, HybridAlpha  </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.19.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S3.SS3.SSS4.p1.1.20.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.21" class="ltx_text" style="font-size:70%;">, and TensorFlow Privacy Library</span><span id="footnote17" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/tensorflow/privacy/</span></span></span></span><span id="S3.SS3.SSS4.p1.1.22" class="ltx_text" style="font-size:70%;">. </span><span id="S3.SS3.SSS4.p1.1.23" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">Asynchronous aggregator</span><span id="S3.SS3.SSS4.p1.1.24" class="ltx_text" style="font-size:70%;"> is identified from ASO-fed </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.25.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S3.SS3.SSS4.p1.1.26.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.27" class="ltx_text" style="font-size:70%;">, AFSGD-VP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.28.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S3.SS3.SSS4.p1.1.29.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.30" class="ltx_text" style="font-size:70%;">, and FedAsync </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.31.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S3.SS3.SSS4.p1.1.32.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.33" class="ltx_text" style="font-size:70%;">. The </span><span id="S3.SS3.SSS4.p1.1.34" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">asynchronous aggregator</span><span id="S3.SS3.SSS4.p1.1.35" class="ltx_text" style="font-size:70%;"> component enables the global model aggregation to be conducted asynchronously whenever a local model update arrives. Similar technique have been adopted in distributed machine learning approaches such as iHadoop </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.36.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.SS3.SSS4.p1.1.37.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.38" class="ltx_text" style="font-size:70%;"> and it is proven that this can effectively reduce the overall training time. The conventional design of a federated learning system that relies on a central server to orchestrate the learning process might lead to a single point of failure. A </span><span id="S3.SS3.SSS4.p1.1.39" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">decentralise aggregator</span><span id="S3.SS3.SSS4.p1.1.40" class="ltx_text" style="font-size:70%;"> performs model exchanges and aggregation in decentralised manner to improve system reliability. The known uses of </span><span id="S3.SS3.SSS4.p1.1.41" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">decentralised aggregator</span><span id="S3.SS3.SSS4.p1.1.42" class="ltx_text" style="font-size:70%;"> include BrainTorrent </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.43.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S3.SS3.SSS4.p1.1.44.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.45" class="ltx_text" style="font-size:70%;"> and FedPGA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.46.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.SS3.SSS4.p1.1.47.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.48" class="ltx_text" style="font-size:70%;">. Blockchain can be employed as a decentralised solution for federated learning systems. In distributed machine learning systems, p2p network topology is employed to in MapReduce </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.49.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.SS3.SSS4.p1.1.50.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.51" class="ltx_text" style="font-size:70%;"> to resolve the single-point-of-failure threat on parameter servers. A </span><span id="S3.SS3.SSS4.p1.1.52" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">hierarchical aggregator</span><span id="S3.SS3.SSS4.p1.1.53" class="ltx_text" style="font-size:70%;"> component can be selected to improve system efficiency by adding an intermediate edge layer to aggregate the model updates from related client devices partially before performing the final global aggregation. This pattern has been adopted by HierFAVG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.54.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.SS3.SSS4.p1.1.55.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.56" class="ltx_text" style="font-size:70%;">, Astraea, and HFL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.SSS4.p1.1.57.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S3.SS3.SSS4.p1.1.58.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS3.SSS4.p1.1.59" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<div id="S3.SS3.SSS4.p2" class="ltx_para">
<p id="S3.SS3.SSS4.p2.1" class="ltx_p"><span id="S3.SS3.SSS4.p2.1.1" class="ltx_text" style="font-size:70%;">In addition to aggregator-related optional components, a </span><span id="S3.SS3.SSS4.p2.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model co-versioning registry</span><span id="S3.SS3.SSS4.p2.1.3" class="ltx_text" style="font-size:70%;"> component can be embedded within the </span><span id="S3.SS3.SSS4.p2.1.4" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model aggregator</span><span id="S3.SS3.SSS4.p2.1.5" class="ltx_text" style="font-size:70%;"> component to map all the local models and their corresponding global models. This enables the model provernance and improves system accountability. The </span><span id="S3.SS3.SSS4.p2.1.6" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model co-versioning registry</span><span id="S3.SS3.SSS4.p2.1.7" class="ltx_text" style="font-size:70%;"> pattern is summarised and adopted from the version control methods in DVC</span><span id="footnote18" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://dvc.org</span></span></span></span><span id="S3.SS3.SSS4.p2.1.8" class="ltx_text" style="font-size:70%;">, Replicate.ai</span><span id="footnote19" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://replicate.ai</span></span></span></span><span id="S3.SS3.SSS4.p2.1.9" class="ltx_text" style="font-size:70%;">, and Pachyderm</span><span id="footnote20" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.pachyderm.com</span></span></span></span><span id="S3.SS3.SSS4.p2.1.10" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model deployment</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text" style="font-size:70%;">After the aggregation, the </span><span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">global model evaluator</span><span id="S3.SS4.p1.1.3" class="ltx_text" style="font-size:70%;"> assesses the performance of the global model. One example is TensorFlow Extended (TFX)</span><span id="footnote21" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.tensorflow.org/tfx</span></span></span></span><span id="S3.SS4.p1.1.4" class="ltx_text" style="font-size:70%;"> that provides a model validator function to assess the federated learning model performance. If the global model performs well, the </span><span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model deployer</span><span id="S3.SS4.p1.1.6" class="ltx_text" style="font-size:70%;"> component deploys the global model to the client device for decision-making through the </span><span id="S3.SS4.p1.1.7" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">decision-maker</span><span id="S3.SS4.p1.1.8" class="ltx_text" style="font-size:70%;"> component. For instance, TensorFlow lite</span><span id="footnote22" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.tensorflow.org/lite</span></span></span></span><span id="S3.SS4.p1.1.9" class="ltx_text" style="font-size:70%;"> prepares the final validated model for deployment to the client devices for data inference. Within the </span><span id="S3.SS4.p1.1.10" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model deployer</span><span id="S3.SS4.p1.1.11" class="ltx_text" style="font-size:70%;"> component, there are two optional components for selection: </span><span id="S3.SS4.p1.1.12" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">deployment selector</span><span id="S3.SS4.p1.1.13" class="ltx_text" style="font-size:70%;"> and </span><span id="S3.SS4.p1.1.14" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">incentive registry</span><span id="S3.SS4.p1.1.15" class="ltx_text" style="font-size:70%;">. The </span><span id="S3.SS4.p1.1.16" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">deployment selector</span><span id="S3.SS4.p1.1.17" class="ltx_text" style="font-size:70%;"> component examines the client devices and selects clients to receive the global model based on their data characteristics or applications. The </span><span id="S3.SS4.p1.1.18" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">deployment selector</span><span id="S3.SS4.p1.1.19" class="ltx_text" style="font-size:70%;"> pattern has been applied in Azure Machine Learning</span><span id="footnote23" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment</span></span></span></span><span id="S3.SS4.p1.1.20" class="ltx_text" style="font-size:70%;">, Amazon SageMaker</span><span id="footnote24" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</span></span></span></span><span id="S3.SS4.p1.1.21" class="ltx_text" style="font-size:70%;">, and Google Cloud</span><span id="footnote25" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://cloud.google.com/ai-platform/prediction/docs/deploying-models</span></span></span></span><span id="S3.SS4.p1.1.22" class="ltx_text" style="font-size:70%;"> to improve model performance. The incentive registry component maintains all the client devices’ incentives based on their contributions and agreed rates to motivate clients to contribute to the training. Blockchain has been leveraged in FLChain </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.1.23.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.SS4.p1.1.24.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS4.p1.1.25" class="ltx_text" style="font-size:70%;"> and DeepChain </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.1.26.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S3.SS4.p1.1.27.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S3.SS4.p1.1.28" class="ltx_text" style="font-size:70%;"> to build a </span><span id="S3.SS4.p1.1.29" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">incentive registry</span><span id="S3.SS4.p1.1.30" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:70%;">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Model monitoring</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p"><span id="S3.SS5.p1.1.1" class="ltx_text" style="font-size:70%;">After the deployment of models for the actual data inference, a </span><span id="S3.SS5.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model monitor</span><span id="S3.SS5.p1.1.3" class="ltx_text" style="font-size:70%;"> keeps track of the model performance continuously. If the performance degrades below a predefined threshold value, the </span><span id="S3.SS5.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model replacement trigger</span><span id="S3.SS5.p1.1.5" class="ltx_text" style="font-size:70%;"> component notifies the </span><span id="S3.SS5.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model trainer</span><span id="S3.SS5.p1.1.7" class="ltx_text" style="font-size:70%;"> for local fine-tuning or sends an alert to the </span><span id="S3.SS5.p1.1.8" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">job creator</span><span id="S3.SS5.p1.1.9" class="ltx_text" style="font-size:70%;"> for a new model generation. The </span><span id="S3.SS5.p1.1.10" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:70%;">model replacement trigger</span><span id="S3.SS5.p1.1.11" class="ltx_text" style="font-size:70%;"> pattern is identified based on the known uses including Microsoft Azure Machine Learning Designer</span><span id="footnote26" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://azure.microsoft.com/en-au/services/machine-learning/designer</span></span></span></span><span id="S3.SS5.p1.1.12" class="ltx_text" style="font-size:70%;">, Amazon SageMaker</span><span id="footnote27" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aws.amazon.com/sagemaker</span></span></span></span><span id="S3.SS5.p1.1.13" class="ltx_text" style="font-size:70%;">, Alibaba Machine Learning Platform</span><span id="footnote28" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.alibabacloud.com/product/machine-learning</span></span></span></span><span id="S3.SS5.p1.1.14" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:70%;">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:70%;">The most widely mentioned definition of a reference architecture is defined by Bass et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S4.p1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p1.1.4" class="ltx_text" style="font-size:70%;"> as “a reference model mapped onto software elements (that cooperatively implement the functionality defined in the reference model) and the data flow between them. Whereas a reference model divides the functionality, a reference architecture is the mapping of that functionality onto a system decomposition.” Nakagawa et al. collected a series of definitions of reference architectures by various researchers and summarised them as follows: “the reference architecture encompasses the knowledge about how to design system architectures of a given application domain. It must address the business rules, architectural styles (sometimes also defined as architectural patterns that address quality attributes in the reference architecture), best practices of software development (architectural decisions, domain constraints, legislation, and standards), and the software elements that support the development of systems for that domain </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S4.p1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p1.1.7" class="ltx_text" style="font-size:70%;">.”</span></p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text" style="font-size:70%;">Reference architectures for machine learning applications and big data analysis were researched comprehensively. For instance, Pääkkönen and Pakkala proposed a reference architecture of big data systems for machine learning in an edge computing environment </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S4.p2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p2.1.4" class="ltx_text" style="font-size:70%;">. IBM AI Infrastructure Reference Architecture is proposed to be used as a reference by data scientists and IT professionals who are defining, deploying, and integrating AI solutions into an organization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p2.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S4.p2.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p2.1.7" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text" style="font-size:70%;">Reference architectures for edge computing systems are also widely studied. For example, H2020 FAR-Edge-project, Edge Computing Reference Architecture 2.0, Intel-SAP Reference Architecture, IBM Edge computing reference architecture, and Industrial Internet Reference Architecture (IIRA) are proposed by practitioners to support the development of multi-tenant edge systems.</span></p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text" style="font-size:70%;">There are existing works proposed to support federated learning system and architecture design. For instance, Google was the earliest to introduce a system design approach for federated learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.p4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p4.1.4" class="ltx_text" style="font-size:70%;">. A scalable production system for federated learning in the domain of mobile devices, based on TensorFlow described from a high-level perspective. A collection of architectural patterns for the design of federated learning systems are summarised and presented by </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p4.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.p4.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p4.1.7" class="ltx_text" style="font-size:70%;">. There are also many architectures and adoptions of federated learning systems proposed by researchers for diverse applications. For instance, Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p4.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S4.p4.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p4.1.10" class="ltx_text" style="font-size:70%;"> proposed a blockchain-based federated learning architecture for industrial IoT to improve client motivatability through an incentive mechanism. Samarakoon et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p4.1.11.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.p4.1.12.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p4.1.13" class="ltx_text" style="font-size:70%;"> have adopted federated learning to improve reliability and communication latency for vehicle-to-vehicle networks. Another real-world federated learning adoption by Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p4.1.14.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S4.p4.1.15.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S4.p4.1.16" class="ltx_text" style="font-size:70%;"> is a dynamic fusion-based federated learning approach for medical diagnostic image analysis to detect COVID-19 infections. We observed that there have been multiple studies on federated learning from different aspects and their design methods are highly diverse and isolated which makes their proposals challenging to be reproduced.</span></p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text" style="font-size:70%;">Motivated by the previous works mentioned above, we intend to fill the research gap by putting forward an end-to-end reference architecture for federated learning systems development and deployment which has been distinctly lacking in the current state-of-the-art.</span></p>
</div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:70%;">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion &amp; Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:70%;">A reference architecture can be served as a standard guideline for system designers and developers for quick selection of best practice solutions for their problems, which can be further customised as required. To the best of our knowledge, there is still no reference architecture proposed for an end-to-end federated learning system while many reusable components and patterns have been proposed. Thus, in this paper, we proposed FLRA, a pattern-oriented reference architecture for federated learning system design to increase the real-world adoption of federated learning.</span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text" style="font-size:70%;">To design the reference architecture, we developed an empirically-grounded qualitative analysis method as the basis of design theory generation. The empirical evidence to support the reference architecture design is a collection of findings (requirements, patterns, and components) gathered and defined by our previous systematic literature review on federated learning and well-known industry practices of machine learning systems.</span></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text" style="font-size:70%;">After developing the reference architecture, we compared it with existing machine learning architectures of Google, Amazon, Microsoft, and IBM to examine its applicability. The key differences between centralised or distributed machine learning with federated learning are the non-IIDness of training data, variation in the data partitioning (e.g., vertical, horizontal, and transfer federated learning) and device partitioning (e.g., cross-device, cross-silo), the ownership and security requirements of different client devices, the system heterogeneity, and the participation of client nodes. The proposed FLRA architecture adopted many reusable machine learning and federated learning patterns while maintaining most of the mandatory machine learning pipeline components. This ensures that the reference architecture is generalised to support the basic model training tasks in the real world.</span></p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text" style="font-size:70%;">While there are different constraints when developing a federated learning system for different applications and settings, the possible trade-offs and the pattern solutions to these challenges are discussed comprehensively. The confirmation of theory justified the applicability of FLRA and the patterns associated with the support of empirical evidence collected. Hence, the FLRA proposed is applicable in the real world for a general, end-to-end development of federated learning systems. Our future work will focus on developing an architecture decision model for federated learning system design. We will also work on the architecture design for trust in federated learning systems.</span></p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:70%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:70%;">
Abad, M.S.H., Ozfatura, E., GUndUz, D., Ercetin, O.: Hierarchical
federated learning across heterogeneous cellular networks. In: ICASSP 2020 -
2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). pp. 8866–8870 (2020)
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:70%;">
Ahn, J., Simeone, O., Kang, J.: Wireless federated distillation for
distributed edge learning with heterogeneous data. In: 2019 IEEE 30th Annual
International Symposium on Personal, Indoor and Mobile Radio Communications
(PIMRC). pp. 1–6 (2019)
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:70%;">
Bao, X., Su, C., Xiong, Y., Huang, W., Hu, Y.: Flchain: A blockchain
for auditable federated learning with trust and incentive. In: 2019 5th
International Conference on Big Data Computing and Communications (BIGCOM).
pp. 151–159 (2019)
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:70%;">
Bass, L., Clements, P., Kazman, R.: Software architecture in practice.
Addison-Wesley Professional (2003)
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:70%;">
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
Kiddon, C., Konečnỳ, J., Mazzocchi, S., McMahan, H.B., et al.:
Towards federated learning at scale: System design. arXiv preprint
arXiv:1902.01046 (2019)
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:70%;">
Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S.,
Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for
privacy-preserving machine learning. Association for Computing Machinery, New
York, NY, USA (2017)
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:70%;">
Caruana, R.: Multitask Learning, pp. 95–133. Springer US, Boston, MA (1998)
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:70%;">
Chai, Z., Ali, A., Zawad, S., Truex, S., Anwar, A., Baracaldo, N., Zhou, Y.,
Ludwig, H., Yan, F., Cheng, Y.: Tifl: A tier-based federated learning system.
In: Proceedings of the 29th International Symposium on High-Performance
Parallel and Distributed Computing. p. 125–136. HPDC ’20, Association for
Computing Machinery, New York, NY, USA (2020)
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:70%;">
Chen, Y., Ning, Y., Slawski, M., Rangwala, H.: Asynchronous online federated
learning for edge devices with non-iid data. In: 2020 IEEE International
Conference on Big Data (Big Data). pp. 15–24. IEEE (2020)
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:70%;">
Elnikety, E., Elsayed, T., Ramadan, H.E.: ihadoop: Asynchronous iterations for
mapreduce. In: 2011 IEEE Third International Conference on Cloud Computing
Technology and Science. pp. 81–90 (2011)
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:70%;">
Galster, M., Avgeriou, P.: Empirically-grounded reference architectures: A
proposal. In: Proceedings of the Joint ACM SIGSOFT Conference – QoSA and ACM
SIGSOFT Symposium – ISARCS on Quality of Software Architectures – QoSA and
Architecting Critical Systems – ISARCS. p. 153–158. QoSA-ISARCS ’11,
Association for Computing Machinery, New York, NY, USA (2011)
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:70%;">
Gu, B., Xu, A., Huo, Z., Deng, C., Huang, H.: Privacy-preserving asynchronous
federated learning algorithms for multi-party vertically collaborative
learning. arXiv preprint arXiv:2008.06233 (2020)
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:70%;">
Huang, L., Shea, A.L., Qian, H., Masurkar, A., Deng, H., Liu, D.: Patient
clustering improves efficiency of federated machine learning to predict
mortality and hospital stay time using distributed electronic medical
records. Journal of Biomedical Informatics </span><span id="bib.bib13.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">99</span><span id="bib.bib13.3.3" class="ltx_text" style="font-size:70%;">, 103291 (2019)
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:70%;">
Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., Kim, S.L.:
Communication-efficient on-device machine learning: Federated distillation
and augmentation under non-iid private data. arXiv preprint arXiv:1811.11479
(2018)
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:70%;">
Jiang, J., Hu, L.: Decentralised federated learning with adaptive partial
gradient aggregation. CAAI Transactions on Intelligence Technology
</span><span id="bib.bib15.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">5</span><span id="bib.bib15.3.3" class="ltx_text" style="font-size:70%;">(3), 230–236 (2020)
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:70%;">
Jiang, Y., Wang, S., Valls, V., Ko, B.J., Lee, W.H., Leung, K.K., Tassiulas,
L.: Model pruning enables efficient federated learning on edge devices. arXiv
preprint arXiv:1909.12326 (2019)
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:70%;">
Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI ethics
guidelines. Nature Machine Intelligence </span><span id="bib.bib17.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">1</span><span id="bib.bib17.3.3" class="ltx_text" style="font-size:70%;">(9), 389–399 (2019)
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:70%;">
Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and
open problems in federated learning. arXiv preprint arXiv:1912.04977 (2019)
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:70%;">
Kitchenham, B., Brereton, O.P., Budgen, D., Turner, M., Bailey, J., Linkman,
S.: Systematic literature reviews in software engineering–a systematic
literature review. Information and software technology </span><span id="bib.bib19.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">51</span><span id="bib.bib19.3.3" class="ltx_text" style="font-size:70%;">(1),
7–15 (2009)
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:70%;">
Konečnỳ, J., McMahan, H.B., Yu, F.X., Richtárik, P., Suresh,
A.T., Bacon, D.: Federated learning: Strategies for improving communication
efficiency. arXiv preprint arXiv:1610.05492 (2016)
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:70%;">
Li, X., Huang, K., Yang, W., Wang, S., Zhang, Z.: On the convergence of fedavg
on non-iid data. arXiv preprint arXiv:1907.02189 (2019)
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:70%;">
Liu, L., Zhang, J., Song, S.H., Letaief, K.B.: Client-edge-cloud
hierarchical federated learning. In: ICC 2020 - 2020 IEEE International
Conference on Communications (ICC). pp. 1–6 (2020)
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:70%;">
Lo, S.K., Liew, C.S., Tey, K.S., Mekhilef, S.: An interoperable component-based
architecture for data-driven iot system. Sensors </span><span id="bib.bib23.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">19</span><span id="bib.bib23.3.3" class="ltx_text" style="font-size:70%;">(20) (2019)
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:70%;">
Lo, S.K., Lu, Q., Wang, C., Paik, H.Y., Zhu, L.: A systematic literature review
on federated machine learning: From a software engineering perspective. ACM
Comput. Surv. </span><span id="bib.bib24.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">54</span><span id="bib.bib24.3.3" class="ltx_text" style="font-size:70%;">(5) (May 2021)
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:70%;">
Lo, S.K., Lu, Q., Zhu, L., Paik, H.Y., Xu, X., Wang, C.: Architectural patterns
for the design of federated learning systems. arXiv preprint arXiv:2101.02373
(2021)
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:70%;">
Lui, K., Karmiol, J.: AI Infrastructure Reference Architecture. IBM Systems
(2018), </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:70%;">https://www.ibm.com/downloads/cas/W1JQBNJV</span><span id="bib.bib26.2.2" class="ltx_text" style="font-size:70%;">
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:70%;">
Marozzo, F., Talia, D., Trunfio, P.: P2p-mapreduce: Parallel data processing in
dynamic cloud environments. Journal of Computer and System Sciences
</span><span id="bib.bib27.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">78</span><span id="bib.bib27.3.3" class="ltx_text" style="font-size:70%;">(5), 1382–1402 (2012), jCSS Special Issue: Cloud Computing 2011
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:70%;">
McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:
Communication-efficient learning of deep networks from decentralized data.
In: Artificial Intelligence and Statistics. pp. 1273–1282. PMLR (2017)
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:70%;">
Nakagawa, E.Y., Oliveira Antonino, P., Becker, M.: Reference architecture and
product line architecture: A subtle but critical difference. In: Crnkovic,
I., Gruhn, V., Book, M. (eds.) Software Architecture. pp. 207–211. Springer
Berlin Heidelberg, Berlin, Heidelberg (2011)
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:70%;">
Pääkkönen, P., Pakkala, D.: Extending reference architecture of big
data systems towards machine learning in edge computing environments. Journal
of Big Data </span><span id="bib.bib30.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">7</span><span id="bib.bib30.3.3" class="ltx_text" style="font-size:70%;">, 1–29 (2020)
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:70%;">
Roy, A.G., Siddiqui, S., Pölsterl, S., Navab, N., Wachinger, C.:
Braintorrent: A peer-to-peer environment for decentralized federated
learning. arXiv preprint arXiv:1905.06731 (2019)
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:70%;">
Samarakoon, S., Bennis, M., Saad, W., Debbah, M.: Federated learning for
ultra-reliable low-latency v2v communications. In: 2018 IEEE Global
Communications Conference (GLOBECOM). pp. 1–7 (2018)
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:70%;">
Sattler, F., Wiedemann, S., Müller, K.R., Samek, W.: Robust and
communication-efficient federated learning from non-i.i.d. data. IEEE
Transactions on Neural Networks and Learning Systems </span><span id="bib.bib33.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">31</span><span id="bib.bib33.3.3" class="ltx_text" style="font-size:70%;">(9),
3400–3413 (2020)
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:70%;">
Seaman, C.: Qualitative methods in empirical studies of software engineering.
IEEE Transactions on Software Engineering </span><span id="bib.bib34.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">25</span><span id="bib.bib34.3.3" class="ltx_text" style="font-size:70%;">(4), 557–572 (1999)
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:70%;">
Wan, Z., Xia, X., Lo, D., Murphy, G.C.: How does machine learning
change software development practices? IEEE Transactions on Software
Engineering pp. 1–1 (2019)
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:70%;">
Weng, J., Weng, J., Zhang, J., Li, M., Zhang, Y., Luo, W.:
Deepchain: Auditable and privacy-preserving deep learning with
blockchain-based incentive. IEEE Transactions on Dependable and Secure
Computing pp. 1–1 (2019)
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:70%;">
Xie, C., Koyejo, S., Gupta, I.: Asynchronous federated optimization. arXiv
preprint arXiv:1903.03934 (2019)
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:70%;">
Xu, R., Baracaldo, N., Zhou, Y., Anwar, A., Ludwig, H.: Hybridalpha: An
efficient approach for privacy-preserving federated learning. In: Proceedings
of the 12th ACM Workshop on Artificial Intelligence and Security. p. 13–23.
AISec’19, Association for Computing Machinery, New York, NY, USA (2019)
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:70%;">
Xu, Z., Yu, F., Xiong, J., Chen, X.: Helios: Heterogeneity-aware federated
learning with dynamically balanced collaboration. arXiv preprint
arXiv:1912.01684 (2019)
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:70%;">
Zhang, W., Lu, Q., Yu, Q., Li, Z., Liu, Y., Lo, S.K., Chen, S., Xu, X., Zhu,
L.: Blockchain-based federated learning for device failure detection in
industrial iot. IEEE Internet of Things Journal </span><span id="bib.bib40.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">8</span><span id="bib.bib40.3.3" class="ltx_text" style="font-size:70%;">(7), 5926–5937
(2021)
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:70%;">
Zhang, W., Zhou, T., Lu, Q., Wang, X., Zhu, C., Sun, H., Wang, Z., Lo, S.K.,
Wang, F.Y.: Dynamic fusion-based federated learning for covid-19 detection.
IEEE Internet of Things Journal pp. 1–1 (2021)
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:70%;">
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 (2018)
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.11569" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.11570" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.11570">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.11570" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.11572" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 23:39:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
