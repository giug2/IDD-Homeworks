<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.06340] Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations</title><meta property="og:description" content="Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator.
However, FL can be disadvantaged by statistical â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.06340">

<!--Generated on Fri Jul  5 21:11:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Basem Suleiman 
<br class="ltx_break">University of New South Wales 
<br class="ltx_break">Australia
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">b.suleiman@unsw.edu.au</span> 
<br class="ltx_break">&amp;Muhammad Johan Alibasa 
<br class="ltx_break">Sampoerna University 
<br class="ltx_break">Indonesia
<br class="ltx_break">&amp;Rizka Widyarini Purwanto 
<br class="ltx_break">Monash University 
<br class="ltx_break">Indonesia 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">rizka.purwanto@monash.edu</span> 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_ERROR undefined">\AND</span>Lewis Jeffries 
<br class="ltx_break">University of Sydney 
<br class="ltx_break">Australia 
<br class="ltx_break">&amp;Ali Anaissi 
<br class="ltx_break">University of Sydney 
<br class="ltx_break">Australia 
<br class="ltx_break">&amp;Jacky Song 
<br class="ltx_break">University of Sydney 
<br class="ltx_break">Australia 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator.
However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local deviceâ€™s data distribution, which creates different levels of Independent and Identically Distributed (IID) data. Furthermore, this can be more complex when optimising different combinations of FL parameters and choosing optimal aggregation. In this paper, we present an empirical analysis of different FL training parameters and aggregators over various levels of statistical heterogeneity on three datasets. We propose a systematic data partition strategy to simulate different levels of statistical heterogeneity and a metric to measure the level of IID. Additionally, we empirically identify the best FL model and key parameters for datasets of different characteristics. On the basis of these, we present recommended guidelines for FL parameters and aggregators to optimise model performance under different levels of IID and with different datasets.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.2" class="ltx_p"><em id="p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.2.2" class="ltx_text ltx_font_bold">eywords</span>â€‚federated learning Â <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
statistical heterogeneity Â <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
FL aggregators</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Advancements in Artificial Intelligence (AI) have enabled increasingly personalised smart services. With huge amounts of data being collected in real-time throughout all areas of our lifestyle, service providers across all industries are rushing to equip themselves with the knowledge and methodologies to understand our ever-changing demands and behaviours.
For example, machine learning (ML) algorithms can learn from connected wearable devices to ensure that patients can benefit from a more personalised and data-driven standard of care from a healthcare professional <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Such advancements pose considerations regarding both data privacy amongst the user base and learning efficiency for data scientists.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Traditional machine learning algorithms are centralised in their approach.
For example, all local data is sent to a common server which facilitates the model training centrally, before releasing a global update back to all devices.
However, as more devices and sensors are connected, this raises the issue of privacy and security concerns because such devices can potentially collect highly sensitive user information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Furthermore, it is expensive and inefficient to facilitate communication and store the increasingly collected and generated data throughout the network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
A Federated Learning (FL) approach has recently been introduced to address such concerns. FL decentralises the model training so that the participating devices all train on their own local models and only share their respective model updates to a central server, thus preserving raw data locally and reducing communication load <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
The central server subsequently aggregates all the federated model parameters to push the next global update to local devices, and the process repeats itself until a stop criterion is met. Studies comparing Centralised and Federated Learning across a variety of standard datasets have concluded that not only can FL achieve a higher test accuracy of approximately 25%, it can also match the performance of a centralised model by engaging significantly less devices at each communication round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">The traditional approach to FL takes an average of the local model parameters using the FedAvg method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, although this method yields highly favourable results in a theoretical setting, it is known to suffer from system and statistical heterogeneity upon scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Statistical heterogeneity indicates a scenario in which data are not independent and identically distributed (IID) between devices. Research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> shows that when using the FedAvg algorithm in one-class low-IID dataset, the test accuracy reduces by 55% compared to a high-IID dataset. This is an important consideration, as the FL approach takes away the ability to address statistical heterogeneity across all devices centrally and instead relies on individual devices to train on their own local datasets, which can realistically inherit a certain degree of skewness.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The goal of this research is to empirically find the optimal model using four FL aggregators - FedAvg, FedProx, FedPer, and SCAFFOLD - under various levels of statistical heterogeneity, which is determined by label distribution skewness and data quantity skewness. Our contributions are fourfold. First, we empirically analyse different FL aggregators and training parameters over various levels of statistical heterogeneity using three datasets (MNIST, CIFAR-10, and Health Care Obesity). Second, we propose a novel approach to a systematic data partition strategy to simulate different levels of statistical heterogeneity in FL settings. Third, we empirically identify the best FL model and key associated parameters, e.g., number of clients, number of communication rounds, and the number of samples needed in each local model. Forth, based on our empirical evaluation and analysis, we present guidelines for recommended FL settings given different levels of statistical heterogeneity and data characteristics. This can guide researchers in determining the optimal FL parameters and aggregators given the characteristics of their datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">One of the issues in federated learning is when the training data on local devices are not independent and identically distributed (non-IID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This issue leads to a worse performance of the federated learning model compared to the models that are trained using a non-federated learning (centralised) approach due to inconsistent update optimisation across every devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Several past studies discussed federated learning approaches and model architectures that are robust on heterogeneous data. In terms of model architecture, Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> found that self-attention-based models (e.g., Transformers) are suitable for deferated learning over heterogeneous data due to its robustness to distribution shifts. On the other hand, our study focuses on the optimal aggregation technique, which could be applied to federated learning in general regardless of the selected model architecture.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">There are various methods to quantify and measure the effect of statistical heterogeneity on overall model performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. There is no one-size-fits-all approach to measuring this variable; however, we reviewed three different approaches to achieve this goal.
The Earth Moverâ€™s Distance (EMD) approach has been used to measure the degree of local data class imbalance in FL scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This methodology quantifies the degree of skewness of the data label by measuring the variation in the distribution of data classes across the local datasets. The outcome of this study revealed a positive correlation between the EMD and the resulting weight divergence between localised and central models. Another study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> improved this approach by proposing a grouping-based scheduling method to select the client that ensures that the label distributions are balanced. The study found that this method improved the performance results.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">Furthermore, studies have also used the Dirichlet distribution as a measure of label distribution in local datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. This approach generates local data of various skewnesses by taking a vector of data label probabilities sampled from the Dirichlet distribution. This led to the conclusion that as the distribution of the labels varied increasingly between local devices, this contributed to a decrease in the overall performance of the model.
Finally, a Quantity-based Label Distribution Skew <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is also an effective method to simulate different degrees of local data skewness. This approach randomly assigns k different labels to each local device and ensures that there are no repeats of samples across the federated network. This study found that the number of local data labels (k) had a significant impact on model accuracy, especially comparing one-class low-IID settings to other levels of IID settings.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">Many advances in traditional FedAvg have been proposed, one of which is FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. There are two particular challenges of FedAvg that FedProx attempts to overcome: systems and statistical heterogeneity. First, the problem of system heterogeneity refers to the degree to which participating devices vary in terms of their capacity to train and share models with factors such as memory, CPU power, network connectivity, and battery life. Some studies make the assumption that there is full participation of all devices in the network; however, for almost all practical applications of FL, this is unrealistic due to data transfer bottlenecks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Furthermore, variations in device memory and capacity will restrict the degree of meaningful contribution of learnings from devices to the central model. Taken together, some devices will be able to participate more fully than others. One of the contributions of FedProx is the ability to tolerate partial work of a device, whereby updates are allowed if they meet some threshold of improvement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Another significant contribution from FedProx is the addition of a proximal (i.e., regularisation) term. The purpose of the proximal term is to address the problem of statistical heterogeneity. The proximal term, like other regularisation methods in machine learning, seeks to penalise large weight updates to help keep the local updates from diverging; particularly in the case where a local model may be more affected by statistical heterogeneity.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p">Stochastic Controlled Averaging for Federated Learning (SCAFFOLD) is another variation in the FL aggregator that has been shown to outperform FedAvg on image datasets in low-IID scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. SCAFFOLD produces a similar result to the regularisation method discussed above with FedProx, as it is designed to reduce the variance of changes in local model updates by introducing a control variate, helping to achieve faster convergence in the overall accuracy score relative to FedProx and FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed Federated learning via Logits Calibration (FedLC) that showed improved performance in highly skewed data. This approach reduces significantly the possibility of overfitting to minority classes and missing classes by performing calibration of the logits of before softmax cross-entropy. The approach also included pairwise label margin during this calibration process of each local update.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p">FL algorithms have also addressed the problem of statistical heterogeneity with methods other than common machine learning parameter-based approaches such as regularisation or momentum. One such approach is that of FedPer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which takes advantage of the federated environment and its many clients by adding personalisation layers to each of the local neural network models. The idea behind this contribution is that the data exposed to each local model will vary due to statistical heterogeneity, and this should ideally be addressed by local models at the device level. When a personalised learnable layer is added to the base model on each local server, the local model can learn better from its own data, rather than when every layer of the model is always updated with a globally aggregated layer. Consequently, the addition of the personalised layer(s) increases the effectiveness of the base model that is common to all the servers in the network. Once local training is completed in each round, the base model parameters (excluding the personalised layer parameters) are shared and aggregated as in FedAvg. Compared to FedAvg, FedPer has a faster convergence speed in terms of global communication rounds, as well as lower variance in test accuracies and higher test accuracy overall, especially when trained on a relatively low IID dataset.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodologies</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">To simulate real-world FL scenarios, where devices have their own local data in isolation from each other, with various classes distributions to simulate personal usage behaviour, we will introduce label distribution skewness and data quantity skewness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to the local data of each device. Label distribution skewness refers to the imbalanced or uneven allocation of labels to various local clients in the federated learning system. Meanwhile, data quantity skewness refers to the variation in the number of data samples assigned to each local device in the federated learning system. The skewness in label distribution and data quantity on each local dataset can impact the performance of federated learning systems. In federated learning systems, model updates are aggregated from local devices. Label distribution skewness and data quantity skewness, also known as statistical heterogeneity, may lead to biased or uneven contributions from different devices, resulting in a biased global parameter. Furthermore, statistical heterogeneity influences convergence behaviour and the time required to reach a global consensus in the federated learning process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In this study, three different datasets were used: MNIST, CIFAR-10 and Health Care Obesity dataset. The MNIST (Modified National Institute of Standards and Technology) dataset of handwritten digits was downloaded directly from the official website<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>http://yann.lecun.com/exdb/mnist/</span></span></span>. It has a training set of 60,000 examples and a test set of 10,000 examples. The digits have been size-normalised and centred in a fixed-size image. The CIFAR-10 dataset sourced from the official website<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.cs.toronto.edu/Â kriz/cifar.html</span></span></span> consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">The Health Care Obesity dataset was sourced from Kaggle<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.kaggle.com/mpwolke/obesity-levels-life-style/notebook Version 1</span></span></span>. It presents data for the estimation of an individualâ€™s levels of obesity based on their eating habits and physical condition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. It has a total of 2,111 rows, 16 variables, and 7 target labels, namely Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II, and Obesity Type III. Data balancing techniques has been applied to the dataset, resulting in a balanced distribution of labels across the dataset as shown in the Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3.1 Data Collection â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> that illustrates the balanced distribution of labels, which includes seven target labels.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2406.06340/assets/figures/revisions/Health_data_label_dist.jpg" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Label distribution in the Health Care Obesity dataset</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Partitioning</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">One of the main contributions of our paper is to analyse FL aggregators where devices have varying distributions of data. Specifically, we are interested in the degree to which the data on the local devices can be made more or less IID to test aggregators under different conditions of skewness. As such, we put forward a novel method, different from those found in the literature, to quantify the extent to which data vary amongst the network of devices.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">There are two mechanisms that we consider as ways to vary the skewness of data assigned to each client: label skew and data quantity skew. Label skew refers to the number of distinct labels assigned to each active device. A smaller number of labels represents a lower IID setting, whereas a larger number of labels represents a higher IID setting. Devices may also be subjected to data quantity skew. When each device in the system has a less similar number of data samples to other devices in the federated system, that is, that there is increased data quantity variance amongst the devices, it can be considered as a lower IID setting. Likewise, when each device has a more similar number of samples (i.e., lower data quantity variance), it is considered as a higher IID setting.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">Our novel data partitioning framework encompasses both of these mechanisms. The design of label distribution skew in this study has been taken from similar experiments in the FL field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and has been integrated within our novel data partitioning methodology, as shown in AlgorithmÂ <a href="#alg1" title="Algorithm 1 â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <a href="#alg2" title="Algorithm 2 â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#alg3" title="Algorithm 3 â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> k Label allocation to all devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="algx1.l1" class="ltx_listingline">
<span id="algx1.l1.3" class="ltx_text ltx_font_bold">function</span>Â <span id="algx1.l1.4" class="ltx_text ltx_font_smallcaps">k_label_allocation</span>(<math id="algx1.l1.m1.3" class="ltx_Math" alttext="k,K,D" display="inline"><semantics id="algx1.l1.m1.3a"><mrow id="algx1.l1.m1.3.4.2" xref="algx1.l1.m1.3.4.1.cmml"><mi id="algx1.l1.m1.1.1" xref="algx1.l1.m1.1.1.cmml">k</mi><mo id="algx1.l1.m1.3.4.2.1" xref="algx1.l1.m1.3.4.1.cmml">,</mo><mi id="algx1.l1.m1.2.2" xref="algx1.l1.m1.2.2.cmml">K</mi><mo id="algx1.l1.m1.3.4.2.2" xref="algx1.l1.m1.3.4.1.cmml">,</mo><mi id="algx1.l1.m1.3.3" xref="algx1.l1.m1.3.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="algx1.l1.m1.3b"><list id="algx1.l1.m1.3.4.1.cmml" xref="algx1.l1.m1.3.4.2"><ci id="algx1.l1.m1.1.1.cmml" xref="algx1.l1.m1.1.1">ğ‘˜</ci><ci id="algx1.l1.m1.2.2.cmml" xref="algx1.l1.m1.2.2">ğ¾</ci><ci id="algx1.l1.m1.3.3.cmml" xref="algx1.l1.m1.3.3">ğ·</ci></list></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m1.3c">k,K,D</annotation></semantics></math>) <span id="algx1.l1.2" class="ltx_text" style="float:right;"><math id="algx1.l1.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx1.l1.1.m1.1a"><mo id="algx1.l1.1.m1.1.1" xref="algx1.l1.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx1.l1.1.m1.1b"><ci id="algx1.l1.1.m1.1.1.cmml" xref="algx1.l1.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx1.l1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="algx1.l1.2.m2.1a"><mi id="algx1.l1.2.m2.1.1" xref="algx1.l1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="algx1.l1.2.m2.1b"><ci id="algx1.l1.2.m2.1.1.cmml" xref="algx1.l1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.2.m2.1c">k</annotation></semantics></math>: number of allocated labels
</span>
</div>
<div id="algx1.l2" class="ltx_listingline">Â Â Â Â Â <span id="algx1.l2.2" class="ltx_text" style="float:right;"><math id="algx1.l2.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx1.l2.1.m1.1a"><mo id="algx1.l2.1.m1.1.1" xref="algx1.l2.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx1.l2.1.m1.1b"><ci id="algx1.l2.1.m1.1.1.cmml" xref="algx1.l2.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx1.l2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="algx1.l2.2.m2.1a"><mi id="algx1.l2.2.m2.1.1" xref="algx1.l2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="algx1.l2.2.m2.1b"><ci id="algx1.l2.2.m2.1.1.cmml" xref="algx1.l2.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.2.m2.1c">K</annotation></semantics></math>: total number of labels
</span>
</div>
<div id="algx1.l3" class="ltx_listingline">Â Â Â Â Â <span id="algx1.l3.2" class="ltx_text" style="float:right;"><math id="algx1.l3.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx1.l3.1.m1.1a"><mo id="algx1.l3.1.m1.1.1" xref="algx1.l3.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx1.l3.1.m1.1b"><ci id="algx1.l3.1.m1.1.1.cmml" xref="algx1.l3.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx1.l3.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="algx1.l3.2.m2.1a"><mi id="algx1.l3.2.m2.1.1" xref="algx1.l3.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="algx1.l3.2.m2.1b"><ci id="algx1.l3.2.m2.1.1.cmml" xref="algx1.l3.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.2.m2.1c">D</annotation></semantics></math>: number of devices
</span>
</div>
<div id="algx1.l4" class="ltx_listingline">Â Â Â Â Â <span id="algx1.l4.1" class="ltx_text ltx_font_bold">for</span>Â device d=0,1,..,DÂ <span id="algx1.l4.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx1.l5" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Randomly select k labels from K such that there is no repetition

</div>
<div id="algx1.l6" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Store {d: k} in the device:label allocation dictionary D_K

</div>
<div id="algx1.l7" class="ltx_listingline">Â Â Â Â Â <span id="algx1.l7.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx1.l7.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx1.l8" class="ltx_listingline">Â Â Â Â Â <span id="algx1.l8.1" class="ltx_text ltx_font_bold">return</span> D_K

</div>
<div id="algx1.l9" class="ltx_listingline">
<span id="algx1.l9.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx1.l9.2" class="ltx_text ltx_font_bold">function</span>
</div>
</div>
</figure>
<figure id="alg2" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg2.2.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span> Distribution of samples with replacement</figcaption>
<div id="alg2.3" class="ltx_listing ltx_listing">
<div id="algx2.l1" class="ltx_listingline">
<span id="algx2.l1.1" class="ltx_text ltx_font_bold">function</span>Â <span id="algx2.l1.2" class="ltx_text ltx_font_smallcaps">Sample_distribution_with_replacement</span>(<math id="algx2.l1.m1.2" class="ltx_Math" alttext="D\_K,s" display="inline"><semantics id="algx2.l1.m1.2a"><mrow id="algx2.l1.m1.2.2.1" xref="algx2.l1.m1.2.2.2.cmml"><mrow id="algx2.l1.m1.2.2.1.1" xref="algx2.l1.m1.2.2.1.1.cmml"><mi id="algx2.l1.m1.2.2.1.1.2" xref="algx2.l1.m1.2.2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algx2.l1.m1.2.2.1.1.1" xref="algx2.l1.m1.2.2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="algx2.l1.m1.2.2.1.1.3" xref="algx2.l1.m1.2.2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="algx2.l1.m1.2.2.1.1.1a" xref="algx2.l1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="algx2.l1.m1.2.2.1.1.4" xref="algx2.l1.m1.2.2.1.1.4.cmml">K</mi></mrow><mo id="algx2.l1.m1.2.2.1.2" xref="algx2.l1.m1.2.2.2.cmml">,</mo><mi id="algx2.l1.m1.1.1" xref="algx2.l1.m1.1.1.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="algx2.l1.m1.2b"><list id="algx2.l1.m1.2.2.2.cmml" xref="algx2.l1.m1.2.2.1"><apply id="algx2.l1.m1.2.2.1.1.cmml" xref="algx2.l1.m1.2.2.1.1"><times id="algx2.l1.m1.2.2.1.1.1.cmml" xref="algx2.l1.m1.2.2.1.1.1"></times><ci id="algx2.l1.m1.2.2.1.1.2.cmml" xref="algx2.l1.m1.2.2.1.1.2">ğ·</ci><ci id="algx2.l1.m1.2.2.1.1.3.cmml" xref="algx2.l1.m1.2.2.1.1.3">_</ci><ci id="algx2.l1.m1.2.2.1.1.4.cmml" xref="algx2.l1.m1.2.2.1.1.4">ğ¾</ci></apply><ci id="algx2.l1.m1.1.1.cmml" xref="algx2.l1.m1.1.1">ğ‘ </ci></list></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m1.2c">D\_K,s</annotation></semantics></math>)

</div>
<div id="algx2.l2" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l2.2" class="ltx_text" style="float:right;"><math id="algx2.l2.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx2.l2.1.m1.1a"><mo id="algx2.l2.1.m1.1.1" xref="algx2.l2.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx2.l2.1.m1.1b"><ci id="algx2.l2.1.m1.1.1.cmml" xref="algx2.l2.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l2.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx2.l2.2.m2.1" class="ltx_Math" alttext="D\_K" display="inline"><semantics id="algx2.l2.2.m2.1a"><mrow id="algx2.l2.2.m2.1.1" xref="algx2.l2.2.m2.1.1.cmml"><mi id="algx2.l2.2.m2.1.1.2" xref="algx2.l2.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algx2.l2.2.m2.1.1.1" xref="algx2.l2.2.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="algx2.l2.2.m2.1.1.3" xref="algx2.l2.2.m2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="algx2.l2.2.m2.1.1.1a" xref="algx2.l2.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx2.l2.2.m2.1.1.4" xref="algx2.l2.2.m2.1.1.4.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="algx2.l2.2.m2.1b"><apply id="algx2.l2.2.m2.1.1.cmml" xref="algx2.l2.2.m2.1.1"><times id="algx2.l2.2.m2.1.1.1.cmml" xref="algx2.l2.2.m2.1.1.1"></times><ci id="algx2.l2.2.m2.1.1.2.cmml" xref="algx2.l2.2.m2.1.1.2">ğ·</ci><ci id="algx2.l2.2.m2.1.1.3.cmml" xref="algx2.l2.2.m2.1.1.3">_</ci><ci id="algx2.l2.2.m2.1.1.4.cmml" xref="algx2.l2.2.m2.1.1.4">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l2.2.m2.1c">D\_K</annotation></semantics></math>: Device:label allocation dictionary 
</span>
</div>
<div id="algx2.l3" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l3.2" class="ltx_text" style="float:right;"><math id="algx2.l3.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx2.l3.1.m1.1a"><mo id="algx2.l3.1.m1.1.1" xref="algx2.l3.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx2.l3.1.m1.1b"><ci id="algx2.l3.1.m1.1.1.cmml" xref="algx2.l3.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l3.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx2.l3.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="algx2.l3.2.m2.1a"><mi id="algx2.l3.2.m2.1.1" xref="algx2.l3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="algx2.l3.2.m2.1b"><ci id="algx2.l3.2.m2.1.1.cmml" xref="algx2.l3.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l3.2.m2.1c">s</annotation></semantics></math>: maximum samples per device
</span>
</div>
<div id="algx2.l4" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l4.1" class="ltx_text ltx_font_bold">for</span>Â label k=0,1,..,KÂ <span id="algx2.l4.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx2.l5" class="ltx_listingline">Â Â Â Â Â Â Â Â Â d_k = count of d which contain k from D_K

</div>
<div id="algx2.l6" class="ltx_listingline">Â Â Â Â Â Â Â Â Â k_n = total number of k samples / d_k

</div>
<div id="algx2.l7" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Store k:k_n in the label sample count dictionary K_N

</div>
<div id="algx2.l8" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l8.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx2.l8.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx2.l9" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l9.1" class="ltx_text ltx_font_bold">for</span>Â device d=0,1,..,DÂ <span id="algx2.l9.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx2.l10" class="ltx_listingline">Â Â Â Â Â Â Â Â Â <span id="algx2.l10.1" class="ltx_text ltx_font_bold">for</span>Â allocated label k=0,1,..,k_nÂ <span id="algx2.l10.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx2.l11" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Assign k_n samples of label k to device d from K_N

</div>
<div id="algx2.l12" class="ltx_listingline">Â Â Â Â Â Â Â Â Â <span id="algx2.l12.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx2.l12.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx2.l13" class="ltx_listingline">Â Â Â Â Â Â Â Â Â randomly shuffle &amp; split device d samples into d_train/d_val/d_test (80:10:10)

</div>
<div id="algx2.l14" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Store d: d_train into train set dictionary D_TRAIN

</div>
<div id="algx2.l15" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Store d: d_val into validation set dictionary D_VAL

</div>
<div id="algx2.l16" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Store d: d_test into test set dictionary D_TEST

</div>
<div id="algx2.l17" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l17.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx2.l17.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx2.l18" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l18.1" class="ltx_text ltx_font_bold">for</span>Â train device d_train=0,1,..,D_TRAINÂ <span id="algx2.l18.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx2.l19" class="ltx_listingline">Â Â Â Â Â Â Â Â Â <span id="algx2.l19.1" class="ltx_text ltx_font_bold">if</span>Â count(d_train) <math id="algx2.l19.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="algx2.l19.m1.1a"><mo id="algx2.l19.m1.1.1" xref="algx2.l19.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="algx2.l19.m1.1b"><gt id="algx2.l19.m1.1.1.cmml" xref="algx2.l19.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="algx2.l19.m1.1c">&gt;</annotation></semantics></math> sÂ <span id="algx2.l19.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="algx2.l20" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Â Â Â Â Â randomly shuffle samples and remove <math id="algx2.l20.m1.1" class="ltx_Math" alttext="\Delta s" display="inline"><semantics id="algx2.l20.m1.1a"><mrow id="algx2.l20.m1.1.1" xref="algx2.l20.m1.1.1.cmml"><mi mathvariant="normal" id="algx2.l20.m1.1.1.2" xref="algx2.l20.m1.1.1.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="algx2.l20.m1.1.1.1" xref="algx2.l20.m1.1.1.1.cmml">â€‹</mo><mi id="algx2.l20.m1.1.1.3" xref="algx2.l20.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="algx2.l20.m1.1b"><apply id="algx2.l20.m1.1.1.cmml" xref="algx2.l20.m1.1.1"><times id="algx2.l20.m1.1.1.1.cmml" xref="algx2.l20.m1.1.1.1"></times><ci id="algx2.l20.m1.1.1.2.cmml" xref="algx2.l20.m1.1.1.2">Î”</ci><ci id="algx2.l20.m1.1.1.3.cmml" xref="algx2.l20.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l20.m1.1c">\Delta s</annotation></semantics></math> from d_train

</div>
<div id="algx2.l21" class="ltx_listingline">Â Â Â Â Â Â Â Â Â <span id="algx2.l21.1" class="ltx_text ltx_font_bold">else</span>Â <span id="algx2.l21.2" class="ltx_text ltx_font_bold">if</span>Â count(d_train) <math id="algx2.l21.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="algx2.l21.m1.1a"><mo id="algx2.l21.m1.1.1" xref="algx2.l21.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="algx2.l21.m1.1b"><lt id="algx2.l21.m1.1.1.cmml" xref="algx2.l21.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="algx2.l21.m1.1c">&lt;</annotation></semantics></math> sÂ <span id="algx2.l21.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="algx2.l22" class="ltx_listingline">Â Â Â Â Â Â Â Â Â Â Â Â Â Â randomly add <math id="algx2.l22.m1.1" class="ltx_Math" alttext="\Delta s" display="inline"><semantics id="algx2.l22.m1.1a"><mrow id="algx2.l22.m1.1.1" xref="algx2.l22.m1.1.1.cmml"><mi mathvariant="normal" id="algx2.l22.m1.1.1.2" xref="algx2.l22.m1.1.1.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="algx2.l22.m1.1.1.1" xref="algx2.l22.m1.1.1.1.cmml">â€‹</mo><mi id="algx2.l22.m1.1.1.3" xref="algx2.l22.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="algx2.l22.m1.1b"><apply id="algx2.l22.m1.1.1.cmml" xref="algx2.l22.m1.1.1"><times id="algx2.l22.m1.1.1.1.cmml" xref="algx2.l22.m1.1.1.1"></times><ci id="algx2.l22.m1.1.1.2.cmml" xref="algx2.l22.m1.1.1.2">Î”</ci><ci id="algx2.l22.m1.1.1.3.cmml" xref="algx2.l22.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l22.m1.1c">\Delta s</annotation></semantics></math> samples from the dataset to d_train based on D_K

</div>
<div id="algx2.l23" class="ltx_listingline">Â Â Â Â Â Â Â Â Â <span id="algx2.l23.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx2.l23.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="algx2.l24" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l24.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx2.l24.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx2.l25" class="ltx_listingline">Â Â Â Â Â <span id="algx2.l25.1" class="ltx_text ltx_font_bold">return</span> D_TRAIN, D_VAL, D_TEST

</div>
<div id="algx2.l26" class="ltx_listingline">
<span id="algx2.l26.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx2.l26.2" class="ltx_text ltx_font_bold">function</span>
</div>
</div>
</figure>
<figure id="alg3" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg3.2.1.1" class="ltx_text ltx_font_bold">Algorithm 3</span> </span> Data Quantity Skew</figcaption>
<div id="alg3.3" class="ltx_listing ltx_listing">
<div id="algx3.l1" class="ltx_listingline">
<span id="algx3.l1.1" class="ltx_text ltx_font_bold">function</span>Â <span id="algx3.l1.2" class="ltx_text ltx_font_smallcaps">Data_quantity_skew</span>(<math id="algx3.l1.m1.2" class="ltx_Math" alttext="D\_TRAIN,var" display="inline"><semantics id="algx3.l1.m1.2a"><mrow id="algx3.l1.m1.2.2.2" xref="algx3.l1.m1.2.2.3.cmml"><mrow id="algx3.l1.m1.1.1.1.1" xref="algx3.l1.m1.1.1.1.1.cmml"><mi id="algx3.l1.m1.1.1.1.1.2" xref="algx3.l1.m1.1.1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.1.1.1.1.1" xref="algx3.l1.m1.1.1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="algx3.l1.m1.1.1.1.1.3" xref="algx3.l1.m1.1.1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.1.1.1.1.1a" xref="algx3.l1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="algx3.l1.m1.1.1.1.1.4" xref="algx3.l1.m1.1.1.1.1.4.cmml">T</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.1.1.1.1.1b" xref="algx3.l1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="algx3.l1.m1.1.1.1.1.5" xref="algx3.l1.m1.1.1.1.1.5.cmml">R</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.1.1.1.1.1c" xref="algx3.l1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="algx3.l1.m1.1.1.1.1.6" xref="algx3.l1.m1.1.1.1.1.6.cmml">A</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.1.1.1.1.1d" xref="algx3.l1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="algx3.l1.m1.1.1.1.1.7" xref="algx3.l1.m1.1.1.1.1.7.cmml">I</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.1.1.1.1.1e" xref="algx3.l1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="algx3.l1.m1.1.1.1.1.8" xref="algx3.l1.m1.1.1.1.1.8.cmml">N</mi></mrow><mo id="algx3.l1.m1.2.2.2.3" xref="algx3.l1.m1.2.2.3.cmml">,</mo><mrow id="algx3.l1.m1.2.2.2.2" xref="algx3.l1.m1.2.2.2.2.cmml"><mi id="algx3.l1.m1.2.2.2.2.2" xref="algx3.l1.m1.2.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.2.2.2.2.1" xref="algx3.l1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="algx3.l1.m1.2.2.2.2.3" xref="algx3.l1.m1.2.2.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="algx3.l1.m1.2.2.2.2.1a" xref="algx3.l1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="algx3.l1.m1.2.2.2.2.4" xref="algx3.l1.m1.2.2.2.2.4.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx3.l1.m1.2b"><list id="algx3.l1.m1.2.2.3.cmml" xref="algx3.l1.m1.2.2.2"><apply id="algx3.l1.m1.1.1.1.1.cmml" xref="algx3.l1.m1.1.1.1.1"><times id="algx3.l1.m1.1.1.1.1.1.cmml" xref="algx3.l1.m1.1.1.1.1.1"></times><ci id="algx3.l1.m1.1.1.1.1.2.cmml" xref="algx3.l1.m1.1.1.1.1.2">ğ·</ci><ci id="algx3.l1.m1.1.1.1.1.3.cmml" xref="algx3.l1.m1.1.1.1.1.3">_</ci><ci id="algx3.l1.m1.1.1.1.1.4.cmml" xref="algx3.l1.m1.1.1.1.1.4">ğ‘‡</ci><ci id="algx3.l1.m1.1.1.1.1.5.cmml" xref="algx3.l1.m1.1.1.1.1.5">ğ‘…</ci><ci id="algx3.l1.m1.1.1.1.1.6.cmml" xref="algx3.l1.m1.1.1.1.1.6">ğ´</ci><ci id="algx3.l1.m1.1.1.1.1.7.cmml" xref="algx3.l1.m1.1.1.1.1.7">ğ¼</ci><ci id="algx3.l1.m1.1.1.1.1.8.cmml" xref="algx3.l1.m1.1.1.1.1.8">ğ‘</ci></apply><apply id="algx3.l1.m1.2.2.2.2.cmml" xref="algx3.l1.m1.2.2.2.2"><times id="algx3.l1.m1.2.2.2.2.1.cmml" xref="algx3.l1.m1.2.2.2.2.1"></times><ci id="algx3.l1.m1.2.2.2.2.2.cmml" xref="algx3.l1.m1.2.2.2.2.2">ğ‘£</ci><ci id="algx3.l1.m1.2.2.2.2.3.cmml" xref="algx3.l1.m1.2.2.2.2.3">ğ‘</ci><ci id="algx3.l1.m1.2.2.2.2.4.cmml" xref="algx3.l1.m1.2.2.2.2.4">ğ‘Ÿ</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="algx3.l1.m1.2c">D\_TRAIN,var</annotation></semantics></math>)

</div>
<div id="algx3.l2" class="ltx_listingline">Â Â Â Â Â <span id="algx3.l2.2" class="ltx_text" style="float:right;"><math id="algx3.l2.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx3.l2.1.m1.1a"><mo id="algx3.l2.1.m1.1.1" xref="algx3.l2.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx3.l2.1.m1.1b"><ci id="algx3.l2.1.m1.1.1.cmml" xref="algx3.l2.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx3.l2.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx3.l2.2.m2.1" class="ltx_Math" alttext="D\_TRAIN" display="inline"><semantics id="algx3.l2.2.m2.1a"><mrow id="algx3.l2.2.m2.1.1" xref="algx3.l2.2.m2.1.1.cmml"><mi id="algx3.l2.2.m2.1.1.2" xref="algx3.l2.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algx3.l2.2.m2.1.1.1" xref="algx3.l2.2.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="algx3.l2.2.m2.1.1.3" xref="algx3.l2.2.m2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="algx3.l2.2.m2.1.1.1a" xref="algx3.l2.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l2.2.m2.1.1.4" xref="algx3.l2.2.m2.1.1.4.cmml">T</mi><mo lspace="0em" rspace="0em" id="algx3.l2.2.m2.1.1.1b" xref="algx3.l2.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l2.2.m2.1.1.5" xref="algx3.l2.2.m2.1.1.5.cmml">R</mi><mo lspace="0em" rspace="0em" id="algx3.l2.2.m2.1.1.1c" xref="algx3.l2.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l2.2.m2.1.1.6" xref="algx3.l2.2.m2.1.1.6.cmml">A</mi><mo lspace="0em" rspace="0em" id="algx3.l2.2.m2.1.1.1d" xref="algx3.l2.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l2.2.m2.1.1.7" xref="algx3.l2.2.m2.1.1.7.cmml">I</mi><mo lspace="0em" rspace="0em" id="algx3.l2.2.m2.1.1.1e" xref="algx3.l2.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l2.2.m2.1.1.8" xref="algx3.l2.2.m2.1.1.8.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="algx3.l2.2.m2.1b"><apply id="algx3.l2.2.m2.1.1.cmml" xref="algx3.l2.2.m2.1.1"><times id="algx3.l2.2.m2.1.1.1.cmml" xref="algx3.l2.2.m2.1.1.1"></times><ci id="algx3.l2.2.m2.1.1.2.cmml" xref="algx3.l2.2.m2.1.1.2">ğ·</ci><ci id="algx3.l2.2.m2.1.1.3.cmml" xref="algx3.l2.2.m2.1.1.3">_</ci><ci id="algx3.l2.2.m2.1.1.4.cmml" xref="algx3.l2.2.m2.1.1.4">ğ‘‡</ci><ci id="algx3.l2.2.m2.1.1.5.cmml" xref="algx3.l2.2.m2.1.1.5">ğ‘…</ci><ci id="algx3.l2.2.m2.1.1.6.cmml" xref="algx3.l2.2.m2.1.1.6">ğ´</ci><ci id="algx3.l2.2.m2.1.1.7.cmml" xref="algx3.l2.2.m2.1.1.7">ğ¼</ci><ci id="algx3.l2.2.m2.1.1.8.cmml" xref="algx3.l2.2.m2.1.1.8">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx3.l2.2.m2.1c">D\_TRAIN</annotation></semantics></math>: train set dictionary 
</span>
</div>
<div id="algx3.l3" class="ltx_listingline">Â Â Â Â Â <span id="algx3.l3.2" class="ltx_text" style="float:right;"><math id="algx3.l3.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="algx3.l3.1.m1.1a"><mo id="algx3.l3.1.m1.1.1" xref="algx3.l3.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="algx3.l3.1.m1.1b"><ci id="algx3.l3.1.m1.1.1.cmml" xref="algx3.l3.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="algx3.l3.1.m1.1c">\triangleright</annotation></semantics></math> <math id="algx3.l3.2.m2.1" class="ltx_Math" alttext="var" display="inline"><semantics id="algx3.l3.2.m2.1a"><mrow id="algx3.l3.2.m2.1.1" xref="algx3.l3.2.m2.1.1.cmml"><mi id="algx3.l3.2.m2.1.1.2" xref="algx3.l3.2.m2.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="algx3.l3.2.m2.1.1.1" xref="algx3.l3.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l3.2.m2.1.1.3" xref="algx3.l3.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="algx3.l3.2.m2.1.1.1a" xref="algx3.l3.2.m2.1.1.1.cmml">â€‹</mo><mi id="algx3.l3.2.m2.1.1.4" xref="algx3.l3.2.m2.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="algx3.l3.2.m2.1b"><apply id="algx3.l3.2.m2.1.1.cmml" xref="algx3.l3.2.m2.1.1"><times id="algx3.l3.2.m2.1.1.1.cmml" xref="algx3.l3.2.m2.1.1.1"></times><ci id="algx3.l3.2.m2.1.1.2.cmml" xref="algx3.l3.2.m2.1.1.2">ğ‘£</ci><ci id="algx3.l3.2.m2.1.1.3.cmml" xref="algx3.l3.2.m2.1.1.3">ğ‘</ci><ci id="algx3.l3.2.m2.1.1.4.cmml" xref="algx3.l3.2.m2.1.1.4">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx3.l3.2.m2.1c">var</annotation></semantics></math>: data quantity variance threshold
</span>
</div>
<div id="algx3.l4" class="ltx_listingline">Â Â Â Â Â <span id="algx3.l4.1" class="ltx_text ltx_font_bold">for</span>Â train device d_train=0,1,..,D_TRAINÂ <span id="algx3.l4.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx3.l5" class="ltx_listingline">Â Â Â Â Â Â Â Â Â d_var = randomly sample one integer within (var, 1)

</div>
<div id="algx3.l6" class="ltx_listingline">Â Â Â Â Â Â Â Â Â shuffle samples and remove (1 - d_var) sample proportion from d_train

</div>
<div id="algx3.l7" class="ltx_listingline">Â Â Â Â Â <span id="algx3.l7.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx3.l7.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx3.l8" class="ltx_listingline">Â Â Â Â Â <span id="algx3.l8.1" class="ltx_text ltx_font_bold">return</span> D_TRAIN

</div>
<div id="algx3.l9" class="ltx_listingline">
<span id="algx3.l9.1" class="ltx_text ltx_font_bold">end</span>Â <span id="algx3.l9.2" class="ltx_text ltx_font_bold">function</span>
</div>
</div>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Label Distribution Skew</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The label distribution methodology is shown in detail in pseudocode format in AlgorithmÂ <a href="#alg1" title="Algorithm 1 â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#alg2" title="Algorithm 2 â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To fulfil the skewness of the label distribution, we first take a parameter for the number of distinct labels (k) that each device will receive. We randomly allocate k unique labels to each device. At this stage, no data have been assigned. Once each device is allocated its respective k labels, the function will then evenly partition out all available samples from a given dataset to each device, so that there are no sample repeats across all devices. The justification for this is to ensure a fair distribution of labels from the original dataset amongst all the devices in the network. Finally, using the methodology described in SectionÂ <a href="#S3.SS3" title="3.3 Experiment Setup â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we take a random 80/10/10 train/validation/test split before continuing with any further processing to produce data quantity skewness. FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3.2.2 Data Quantity Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows different outputs of the label distribution process. In a high IID setting (FigureÂ <a href="#S3.F2.sf1" title="In Figure 2 â€£ 3.2.2 Data Quantity Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>), each device contains as many distinct labels as there are all available labels in the dataset, while in the highly skewed low IID setting (FigureÂ <a href="#S3.F2.sf2" title="In Figure 2 â€£ 3.2.2 Data Quantity Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>), each device is assigned only 2 labels. Each client, shown as a column in the figure, is only able to train and test on the samples it has been provided.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Data Quantity Skew</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The next step in the data partitioning methodology controls how much data each device receives and is shown as pseudocode in AlgorithmÂ <a href="#alg3" title="Algorithm 3 â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This part of the data partitioning functions takes in a parameter for the number of samples per device to determine the threshold of training samples which each device will be allocated. Devices that are over this threshold will have samples randomly removed, whilst devices that are under the threshold will undergo sampling with replacement (from samples with labels that the device has been allocated) from the global dataset. Since data partitioning comes before training, there is no violation of FL privacy principles because there will be no sharing of samples once training has started <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.06340/assets/figures/Extra/Figure2a.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="408" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Label Distribution k=10</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.06340/assets/figures/Extra/Figure2b.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Label Distribution k=2</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of high-IID and low-IID label distribution allocations</figcaption>
</figure>
<div id="S3.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">Additionally, this function will also take in a parameter for the data quantity variance to determine the threshold of samples to be removed from each device, hence inducing a data quantity skew. Each device is randomly allocated a proportion of samples to be kept and a portion that will be removed via a threshold parameter. For example, if a device has randomly been assigned a threshold value of 0.7, then 30% of the samples on the device will be removed at random, and this constitutes a setting of 30% data quantity variance. FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2.2 Data Quantity Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows different outputs of the data quantity skewness while fixing the label distribution. In the low-variance setting (FigureÂ <a href="#S3.F3.sf1" title="In Figure 3 â€£ 3.2.2 Data Quantity Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>), each device contains all available labels, whilst in the highly skewed setting (FigureÂ <a href="#S3.F3.sf2" title="In Figure 3 â€£ 3.2.2 Data Quantity Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>), some devices may have up to 90% of samples removed.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.06340/assets/figures/Extra/Figure3a.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="408" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Data Quantity Variance = 0%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.06340/assets/figures/Extra/Figure3b.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Data Quantity Variance = 90%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of low and high variance data quantity distribution allocations</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Quantifying Overall Data Skew</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">After the allocation of samples of multiple classes and varying amounts, we next quantify how much variation there is among the entire system using the Earth-Mover distance metric (EMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. EMD is the size of the distance in probability variation of two distributions of classes, and can be used in an FL setting when the underlying distribution of classes on each local device varies. As such, the difference in EMD between the distributions of classes on all the local devices in the system quantifies the degree of IID of the system. An array of literatures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> have used EMD to measure the disparity between local devices and therefore quantify the degree of data skewness. Being a distance-based metric, higher values of EMD represent higher degrees of data skewness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">In the context of this research, each distribution is defined as the list of labels assigned to each local client after the data partition process. To calculate the EMD value of our federated network of clients, we compare the distributed labels of all combinations of local clients after data partitioning. Then we take the average of all pairs of EMD scores to calculate the average EMD for that data partitioning condition. TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example of the effect of how EMD changes with label skewness, that is, the number of labels assigned to the local client, while the quantity of data skew was kept constant.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>EMD analysis on MNIST dataset</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number of</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number of</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Data quantity</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Max Number</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T1.1.1.1.5.1" class="ltx_text">EMD</span></th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">labels</th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">clients</th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">threshold</th>
<th id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">of samples</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr">
<td id="S3.T1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S3.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">30</td>
<td id="S3.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.7</td>
<td id="S3.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">400</td>
<td id="S3.T1.1.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">3.2</td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<td id="S3.T1.1.4.2.1" class="ltx_td ltx_align_center">2</td>
<td id="S3.T1.1.4.2.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.4.2.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.4.2.5" class="ltx_td ltx_nopad_r ltx_align_center">2.8</td>
</tr>
<tr id="S3.T1.1.5.3" class="ltx_tr">
<td id="S3.T1.1.5.3.1" class="ltx_td ltx_align_center">3</td>
<td id="S3.T1.1.5.3.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.5.3.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.5.3.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.5.3.5" class="ltx_td ltx_nopad_r ltx_align_center">2</td>
</tr>
<tr id="S3.T1.1.6.4" class="ltx_tr">
<td id="S3.T1.1.6.4.1" class="ltx_td ltx_align_center">4</td>
<td id="S3.T1.1.6.4.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.6.4.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.6.4.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.6.4.5" class="ltx_td ltx_nopad_r ltx_align_center">1.9</td>
</tr>
<tr id="S3.T1.1.7.5" class="ltx_tr">
<td id="S3.T1.1.7.5.1" class="ltx_td ltx_align_center">5</td>
<td id="S3.T1.1.7.5.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.7.5.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.7.5.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.7.5.5" class="ltx_td ltx_nopad_r ltx_align_center">1.6</td>
</tr>
<tr id="S3.T1.1.8.6" class="ltx_tr">
<td id="S3.T1.1.8.6.1" class="ltx_td ltx_align_center">6</td>
<td id="S3.T1.1.8.6.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.8.6.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.8.6.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.8.6.5" class="ltx_td ltx_nopad_r ltx_align_center">1.2</td>
</tr>
<tr id="S3.T1.1.9.7" class="ltx_tr">
<td id="S3.T1.1.9.7.1" class="ltx_td ltx_align_center">7</td>
<td id="S3.T1.1.9.7.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.9.7.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.9.7.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.9.7.5" class="ltx_td ltx_nopad_r ltx_align_center">0.9</td>
</tr>
<tr id="S3.T1.1.10.8" class="ltx_tr">
<td id="S3.T1.1.10.8.1" class="ltx_td ltx_align_center">8</td>
<td id="S3.T1.1.10.8.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.10.8.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.10.8.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.10.8.5" class="ltx_td ltx_nopad_r ltx_align_center">0.8</td>
</tr>
<tr id="S3.T1.1.11.9" class="ltx_tr">
<td id="S3.T1.1.11.9.1" class="ltx_td ltx_align_center">9</td>
<td id="S3.T1.1.11.9.2" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.1.11.9.3" class="ltx_td ltx_align_center">0.7</td>
<td id="S3.T1.1.11.9.4" class="ltx_td ltx_align_center">400</td>
<td id="S3.T1.1.11.9.5" class="ltx_td ltx_nopad_r ltx_align_center">0.5</td>
</tr>
<tr id="S3.T1.1.12.10" class="ltx_tr">
<td id="S3.T1.1.12.10.1" class="ltx_td ltx_align_center ltx_border_bb">10</td>
<td id="S3.T1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_bb">30</td>
<td id="S3.T1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_bb">0.7</td>
<td id="S3.T1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_bb">400</td>
<td id="S3.T1.1.12.10.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">Intuitively, we can see that as the number of classes assigned to local clients increases while all other parameters are kept identical, the EMD decreases as the degree of label skewness has been reduced. For a 10-class label dataset such as MNIST, the highest EMD score fluctuates to a value of approximately 4, and this occurs in a single pair of clients when only one class has been assigned to each local device. This aligns with our expectation, as more class labels means more chances for similarities between local clients and therefore a reduced EMD value. However, due to the effect of random sampling and quantity skewness, in our study, EMD does not ever reach 0, although this is the metricâ€™s lower bound. In our experimentation, there will always be some differences in terms of labels between local clients.</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p4.1" class="ltx_p">Additionally, we attempted to quantify the impact of how data quantity skewness impacted EMD along with label distribution skewness. When both are taken into account, it is observed that the label distribution is the most influential parameter in changing the statistical heterogeneity of the data, as shown in FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The heat map allows investigation of the interaction between the variance in sample quantity (var) and the number of labels assigned to local clients (<span id="S3.SS2.SSS3.p4.1.1" class="ltx_text ltx_font_italic">k_classes</span>) in the output EMD score. It is also observed that data quantity variance has some influence in changing the level of statistical heterogeneity; however, the influence is inconsistent and not as clear when compared to the impact of label distribution.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>EMD analysis using MNIST</figcaption>
</figure>
<div id="S3.SS2.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p5.1" class="ltx_p">Similar results were held for the CIFAR-10 dataset, which, like MNIST, contains 10 class labels. Unlike MNIST and CIFAR-10, the Healthcare dataset contains only 7 classes. Using the same analysis approach to investigate the interaction between the distribution of labels, the quantity skewness and the EMD values on the Healthcare dataset is shown in FigureÂ <a href="#S3.F5" title="Figure 5 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, it is evident that the maximum EMD score is lower than when using 10 class labels. With a maximum score of 2.3, this impacts how we characterise the degree of data skewness in the federated system when using datasets with different numbers of distinct class labels.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>EMD analysis using Healthcare Obesity dataset</figcaption>
</figure>
<div id="S3.SS2.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p6.1" class="ltx_p">Our literature review suggests that the performance of a federated model deteriorates as the data become more statistically heterogeneous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Importantly, we can now quantify different statistically heterogeneous settings. To validate the level of data skewness using EMD in our experiment, we ran some preliminary experiments using the four different aggregators, in order to better understand the impact of statistically heterogeneous settings on aggregator performance. Ideally, if EMD could quantify degrees of IID, and if different degrees of IID were to have an impact on aggregator performance (measured by F1 scores), we would observe decreasing F1 scores as EMD increased. If such categories are to exist, we might expect to see distinct "elbow points" where F1 scores drop substantially. Additionally, if the variance in data quantity had an impact, we would see an increase in the variance in the F1 scores with an increase in the variance in data quantity.</p>
</div>
<div id="S3.SS2.SSS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p7.1" class="ltx_p">As seen in FigureÂ <a href="#S3.F6" title="Figure 6 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, when there is more variance in the quantity of data, in the lower ranges of the EMD values in each figure, there is more variance in the F1 scores. With increased variance in the quantity of data, for example, variance in the quantity of data 50%, as shown in FigureÂ <a href="#S3.F6.sf1" title="In Figure 6 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, increasing EMD values are associated with increased volatility in F1 scores. This is possibly due to fluctuation in the number of training samples available in local models. Therefore, increasing the degree of statistical heterogeneity along with increased data quantity variance is associated with increased variance in F1 scores.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.06340/assets/figures/Extra/Figure6a.png" id="S3.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Average F1 accuracy vs. EMD over 5 trials, with 50% data quantity variance</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.06340/assets/figures/Extra/Figure6b.png" id="S3.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Average F1 accuracy vs. EMD over 5 trials, with 10% data quantity variance</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>F1 accuracy vs. EMD using MNIST across two levels of data quantity variance</figcaption>
</figure>
<div id="S3.SS2.SSS3.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p8.1" class="ltx_p">Even with increased volatility in F1 scores, there appears to be a pattern that characterises distinct IID settings by their EMD values and F1 scores. Following the elbow points, when the EMD is less than 1.2, the F1 scores are high in value and stable. As the EMD becomes larger than 1.2, the F1 scores start to experience a large drop, representing the first â€™elbow pointâ€™. Following that, another large drop is observed when the EMD becomes larger than 2.2. By these three ranges of EMD values, we classify each as low, moderate and high IID respectively, as shown in TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>EMD Thresholds and Values of k Labels for each IID Setting</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Levels of</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Number of</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">EMD range</span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Label</span></th>
<th id="S3.T2.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">k Labels</span></th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">IID</span></th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">labels</span></th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T2.1.2.2.3.1" class="ltx_text ltx_font_bold">distance</span></th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column"><span id="S3.T2.1.2.2.4.1" class="ltx_text ltx_font_bold">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<td id="S3.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T2.1.3.1.1.1" class="ltx_text">Low</span></td>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">10</td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">&gt;2.2</td>
<td id="S3.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">1, 2</td>
<td id="S3.T2.1.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">2</td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<td id="S3.T2.1.4.2.1" class="ltx_td ltx_align_center">7</td>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_center">&gt;1.7</td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_center">1, 2</td>
<td id="S3.T2.1.4.2.4" class="ltx_td ltx_nopad_r ltx_align_center">2</td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<td id="S3.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T2.1.5.3.1.1" class="ltx_text">Medium</span></td>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_t">10</td>
<td id="S3.T2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">Between 1.2 and 2.2</td>
<td id="S3.T2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">3-6</td>
<td id="S3.T2.1.5.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5</td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<td id="S3.T2.1.6.4.1" class="ltx_td ltx_align_center">7</td>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_center">Between 1 and 1.7</td>
<td id="S3.T2.1.6.4.3" class="ltx_td ltx_align_center">3, 4</td>
<td id="S3.T2.1.6.4.4" class="ltx_td ltx_nopad_r ltx_align_center">4</td>
</tr>
<tr id="S3.T2.1.7.5" class="ltx_tr">
<td id="S3.T2.1.7.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S3.T2.1.7.5.1.1" class="ltx_text">High</span></td>
<td id="S3.T2.1.7.5.2" class="ltx_td ltx_align_center ltx_border_t">10</td>
<td id="S3.T2.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t">&lt;1.2</td>
<td id="S3.T2.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">7-10</td>
<td id="S3.T2.1.7.5.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">10</td>
</tr>
<tr id="S3.T2.1.8.6" class="ltx_tr">
<td id="S3.T2.1.8.6.1" class="ltx_td ltx_align_center ltx_border_bb">7</td>
<td id="S3.T2.1.8.6.2" class="ltx_td ltx_align_center ltx_border_bb">&lt;1</td>
<td id="S3.T2.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb">5-7</td>
<td id="S3.T2.1.8.6.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">7</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.SSS3.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p9.1" class="ltx_p">We repeated the analysis in MNIST above using the CIFAR-10 and Healthcare datasets, as shown in FiguresÂ <a href="#S3.F7" title="Figure 7 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> andÂ <a href="#S3.F8" title="Figure 8 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Although this time with 0% data quantity variance, the same elbow points appear to hold for CIFAR-10. This is likely due to the fact that CIFAR-10 has 10 class labels, the same as MNIST.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>F1 accuracy vs. EMD using CIFAR-10</figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure8.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>F1 accuracy vs. EMD using Health Care Obesity dataset</figcaption>
</figure>
<div id="S3.SS2.SSS3.p10" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p10.1" class="ltx_p">Similarly, while using the elbow point approach for the Healthcare dataset, despite the presence of some outlier results in FigureÂ <a href="#S3.F8" title="Figure 8 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (e.g., FedProx at EMD = 1), we observed that the F1 accuracy experiences a drop for the EMD values between 0 and 1 and a large drop after EMD = 1.7. Therefore, we establish the TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to conclude the relationship between the IID levels and the class distribution.</p>
</div>
<div id="S3.SS2.SSS3.p11" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p11.1" class="ltx_p">Taking into account the time and resource constraints when running several FL experiments, and given that similar F1 scores were observed within each category of IID values, we selected a single value of k labels to run as a proxy for the entire range of labels that constitute an IID category. These values are shown in the column "Selected Value of k Labels" in TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It is worth noting that previous research suggests that assigning only one class to local devices in a low IID setting is not suitable for running federated learning experiments, as FedAvg and FedProx will become very unstable, or even completely unable to learn, as was the case with SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiment Setup</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">In each experiment, we will compare the performance of 4 aggregators: FedAvg, FedProx, FedPer, and SCAFFOLD. In evaluating results, FedAvg, as the original FL aggregator, will be considered as the baseline aggregator with which the other aggregators are compared. FedAvg is chosen as the baseline aggregator as it is a straightforward algorithm. The aggregation involves performing training locally on each client, and averaging the model updates at each client. Due to its simplicity, FedAvg is often used as the starting point and baseline for comparison with other aggregation algorithms in federated learning. Before introducing different types of data skew, first we take each dataset in full, and split it into training and testing sets.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">From the training set, we further split it into training and validation sets. Each client is assigned data that are partitioned from these sets. We will use the testing set to evaluate the performance of the aggregators after training and aggregation has been performed in each communication round. One communication round involves training on selected devices, model aggregation, sending the aggregated model to all devices, and testing. To further simulate the impact of real-world FL, we randomly select a subset of local clients, which is an idea that was inspired by research by Cho, JJ, Wang, J, and Joshi, G <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. In their paper, they performed a convergence analysis of federated optimisation and measured the impact of client selection strategies and selection bias on the speed of convergence. Based on their study, they found a client selection technique to achieve faster error convergence by prioritising clients with higher local loss. This inspired our experimental design, as we adopted a similar approach to client selection.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">In all experiments, we consider the below parameters as relevant to developing and testing the FL framework:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The number of active devices randomly selected from the total number of available devices.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Local training epochs used by the devices.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i3.p1.1" class="ltx_p">Batch size used by the devices.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">In addition, we decided on the values for the number of communication rounds used in total by the system for each dataset, the learning rates, the number of samples each device receives, and the values for the parameters specific to each aggregator. We discuss this further in SectionÂ <a href="#S3.SS4" title="3.4 Hyperparameters for Experiments â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>. To isolate the impact of each parameter on the FL setting, we performed ablation studies, keeping all other parameters consistent and varying only that parameter. We conducted these experiments in three datasets: two image recognition datasets (MNIST and CIFAR-10) that are well-researched in the literature, and one external tabular dataset (Health Care Obesity dataset) which poses a novel problem.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p">Each dataset has an associated neural network model. Specifically, for MNIST, we have used an enhanced version of the LeNet-5 Convolutional Neural Network model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The model has two convolutional layers, followed by a pooling layer twice with 32 filters and 64 filters, respectively, and three fully connected layers with a SoftMax unit in the end with 10 classes. This is similar to the model used by McMahan et al. in MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
For the more complex image recognition tasks found in the CIFAR-10 dataset, we have used MobileNetV2, which is a lightweight network that uses residual bottleneck layers to limit the number of model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. MobileNetV2, as its name suggests, was designed for use on handheld mobile devices and is therefore relevant for FL experimentation. This decision was driven by resource constraints, since models with fewer parameters are likely to be faster to train, although at the expense of benchmark results seen in the literature.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p">To further validate our FL models, we sourced an external healthcare dataset (hereafter referred to as Healthcare dataset), authored by Palechor and de la Hoz Manotas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. This dataset is relevant to the FL data privacy objective, since data in industry may be confined to many siloes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, such as hospitals. This dataset aims to predict the estimated level of obesity based on variables such as gender, age, height, weight, and family history of being overweight. Obesity levels are divided into seven different classes ranging from 1-7. We have used a simple, multi-layer perceptron (MLP) model that makes use of batch normalisation, dropout and ReLU activation for classification. The reasons of using MLP model is that the model is well-suited for handling categorical variables and can effectively classify data into multiple classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The dropout and ReLU activation can be used to prevent overfitting and introduce non-linearity into the model, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.1" class="ltx_p">After implementing the four FL aggregators and collecting the data, we simulated 30 devices with their respective local models and a central client that aggregates the global model. To begin the FL learning process, we applied our novel data partitioning method to the three datasets in order to vary the degree of IID amongst the network. The resulting EMD value when using our partitioning function is deterministic and is based on the input values such as number of labels, number of clients, number of samples, etc. This results in the EMD value becoming repeatable, and will be the same as a result of the indices and labels of samples selected by the function, which are also deterministic in a seeded environment. Practically, this improves the robustness of each permutation of the experiment condition and allows reproducible experimentation and results.
Having set the degree of IID for the data in the network, we keep the output samples constant while varying the values of other parameters, such as local epochs and batch size, which are consequential for FL in our experiment. The overall process is described in FigureÂ <a href="#S3.F9" title="Figure 9 â€£ 3.3 Experiment Setup â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2406.06340/assets/figures/revisions/flowchart.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Diagram of the overall experimental process</figcaption>
</figure>
<div id="S3.SS3.p8" class="ltx_para ltx_noindent">
<p id="S3.SS3.p8.1" class="ltx_p">As observed in the FL literature, results are visually displayed in terms of F1 scores by communication rounds.
To analyse results, we built functions to display results in a reproducible way, and also to record the best F1 scores over all communication rounds.
As outlined in SectionÂ <a href="#S3.SS3" title="3.3 Experiment Setup â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, experimentation in this study involves different combinations of FL parameter values such as the number of active devices, local training epochs, and batch size. We selected two values for each of these parameters to test: one lower and one higher. Importantly, as we are able to keep all other parameters constant, as well as the IID setting, we are then able to fairly compare the performance of aggregators by their F1 scores.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hyperparameters for Experiments</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">To select the values of the parameters for experimentation, we were informed both by a review of the literature and by our own experimentation about what was practical, reasonable, and impactful.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Number of devices</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">A total of 30 devices were available for selection during the FL training process in all experiments. During preliminary experiments and investigations into data partitioning and EMD, we determined that a reasonably large number of devices was necessary, relative to the smaller number of devices that has been cited in the literature, for example, 10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The reasoning for this was that when only 10 devices are sampled and when data quantity variance is high, there may be an excessive amount of samples that are not used at all from the original dataset. On the other hand, if the total number of devices is too high and the data quantity variance is low, samples will be frequently resampled with replacement throughout the network. This could impact results as a consequence of overfitting. No consistent value for this parameter was reported in our review of the literature, with some experiments allowing up to 100 devices across the network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Taken together, we found that the practical number of devices is 30, which provides a healthy balance between sample utilisation and sample replacement.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Number of active devices</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">In the original FedAvg paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, a relatively comprehensive investigation was carried out that offered insight into reasonable values for this parameter: five levels of fractions of participating devices were tested. Other aggregators did not reliably test this parameter. Based on this, we chose the number of active devices as 20% and 50% of the total available devices. With 30 total devices, this meant values of 6 and 15 active devices. For the MNIST and Healthcare datasets, this was practical. Full device participation was not tested as it was reported <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that there were little or no performance gains to full participation. However, running a single experiment with CIFAR-10 would take approximately 71 minutes when using 15 active devices. Following this, to appropriately test CIFAR-10 in a practical way, we instead use values of 10% (3) and 20% (6) active devices to improve run times to be as short as 5 minutes.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Data quantity variance</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">After analysing the results of the experiment on the EMD and F1 scores, as shown in SectionÂ <a href="#S3.SS2" title="3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, it was observed that the "elbow points" of the F1 scores in multiple values of data quantity variance were relatively consistent. The impact of data quantity variance appeared primarily to be the increase in the variation in F1 scores rather than the radical alteration of the IID category. Therefore, in an effort to reduce the scope of experimentation so as to alleviate time and resource constraints, we opted to conduct all of our experiments using a data quantity variance value of 0%. After this, we conducted a small investigation of data quantity variance again to confirm the effects of EMD on F1 scores, but using the best aggregators from each combination of IID category and dataset.</p>
</div>
</section>
<section id="S3.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>Number of samples per client</h4>

<div id="S3.SS4.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS4.p1.1" class="ltx_p">We were guided by the original FedAvg paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for the number of samples each client would receive. It was reported that in the case of MNIST, 600 samples per client were used. In order to find a smaller number of samples to assign per client, we found that we were able to achieve similar accuracy scores using half as many samples. In the same paper, CIFAR-10 was tested with 500 samples per client. We opted to use 600 samples because our experimentation achieved similar F1 scores more slowly than what was presented in the literature. For the Healthcare dataset, unlike MNIST and CIFAR-10 with tens of thousands of samples, it contained 2100 training samples. Hence, we selected a smaller number, with 200 samples to be allocated to each device.</p>
</div>
</section>
<section id="S3.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.5 </span>Number of communication rounds</h4>

<div id="S3.SS4.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS5.p1.1" class="ltx_p">For MNIST, we chose 100 communication rounds, similar to how many were used in the original FedProx paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Although CIFAR-10 has been presented in the literature with up to 3000 communication rounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Li et al. displayed F1 scores similar to what we were able to achieve with 50 communication rounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. For the Healthcare dataset, 200 communication rounds were selected, as initial experiments showed that after 200 communication rounds, F1 scores tended to plateau.</p>
</div>
</section>
<section id="S3.SS4.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.6 </span>Learning rate</h4>

<div id="S3.SS4.SSS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS6.p1.1" class="ltx_p">Although research has suggested that learning rate is an important factor to consider in a federated setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, our initial experimentation did not reveal substantial interactions, so we opted to use values cited in the literature. For MNIST and CIFAR-10, we chose a learning rate of 0.01 and 0.001 respectively. We then confirmed that these values were appropriate based on the literature reviewed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We were unable to find studies that have trained models using the Healthcare dataset, so it posed a novel challenge in finding a suitable learning rate. Among the values of 0.01 and 0.001, 0.001 demonstrated the best F1 scores.</p>
</div>
</section>
<section id="S3.SS4.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.7 </span>Number of local epochs</h4>

<div id="S3.SS4.SSS7.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS7.p1.1" class="ltx_p">The selected number of local epochs used is not consistent in the literature. In the FedAvg paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, MNIST was tested using values for the number of local epochs as 1, 5 and 20. Given resource and time constraints, we, therefore, opted to use 1 and 5 epochs. Furthermore, for the Healthcare dataset, initial experimentation demonstrated that performance decreased when epochs increased beyond 5 (FigureÂ <a href="#S3.F10" title="Figure 10 â€£ 3.4.7 Number of local epochs â€£ 3.4 Hyperparameters for Experiments â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). Therefore, 20 epochs were deemed unnecessary for further experimentation.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure10-2.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>FedAvg Healthcare Epoch Testing</figcaption>
</figure>
<div id="S3.SS4.SSS7.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS7.p2.1" class="ltx_p">For CIFAR-10 experiments, similar experiments in the literature have used 4 epochs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which is comparatively low. However, with 50 communication rounds and 4 local epochs, the experiment duration for a single setting takes approximately 60 minutes, while using 2 epochs takes approximately 23 minutes. Therefore, we opt to use the values of 1 and 2 for CIFAR-10.</p>
</div>
</section>
<section id="S3.SS4.SSS8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.8 </span>Batch size</h4>

<div id="S3.SS4.SSS8.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS8.p1.1" class="ltx_p">The literature is also not consistent in selecting the values for batch sizes. For MNIST, batch sizes of 10 and 50 have been previously tested <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and therefore were selected. For consistency, these batch sizes were also applied to the Healthcare dataset. For CIFAR-10, a batch size of 128 is often cited as an appropriate batch size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. To create the lower parameter value, this was halved to become 64.</p>
</div>
</section>
<section id="S3.SS4.SSS9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.9 </span>Aggregator Parameters</h4>

<div id="S3.SS4.SSS9.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS9.p1.1" class="ltx_p">In addition to FedAvg, each of the other aggregators has a unique parameter that influences the mechanism of the aggregator. In FedProx, the term mu determines the influence of the proximal term on the regularisation of the objective. It is suggested that any value greater than zero is effective, and we take the recommendation to set the parameter at a value of 0.001 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. For FedPer, the number of personalised layers was shown to have no particular correlation with performance, but at least one layer was effective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. As such, we used one layer of personalisation. Finally, for SCAFFOLD, the global learning rate is a term that scales the degree of influence that updated local models have on the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. No reason is offered for tuning this value or if doing so would even be desirable. We kept the global learning rate at a value of 1.0.</p>
</div>
<div id="S3.SS4.SSS9.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS9.p2.1" class="ltx_p">Lastly, a decision was made not to experiment with FedProx on CIFAR-10 due to runtime constraints. In the worst case, a single experiment with FedProx on CIFAR-10 would take 4.5 hours. This decision was justified by the results of Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, who also noted that the large computational overhead of FedProx is due to the modification of the objective function that requires the norm to be taken of all layers of weights, resulting in a slow and complex operation.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Having performed the methodologies described in the previous section, we outline the results of our experiments. Each section breaks down the experimental results by the parameters that were varied: batch size, active devices, and local epochs. Within each section, we describe the effects of each of the parameters tested by IID setting: highly IID, moderately IID, and low-IID.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Active Devices</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">As shown in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, in the high IID setting, the best F1 scores were observed in all three datasets with fewer active devices than more. When using fewer devices, there was only a small increase in the best F1 scores compared to the use of more devices on the MNIST and CIFAR-10 datasets. However, in the case of the Healthcare dataset, which was a tabular dataset, there was a clear disadvantage to selecting more active devices.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Aggregator results by number of active devices across all datasets in the high IID setting</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Active</span></th>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.2.2.2.1" class="ltx_text ltx_font_bold">Devices</span></th>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></td>
<td id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></td>
<td id="S4.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<th id="S4.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">MNIST</th>
<th id="S4.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">6</th>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">98.4</td>
<td id="S4.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.3.4.1" class="ltx_text ltx_font_bold">98.6</span></td>
<td id="S4.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.3.5.1" class="ltx_text ltx_font_bold">98.2</span></td>
<td id="S4.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.3.6.1" class="ltx_text ltx_font_bold">98.5</span></td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<th id="S4.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MNIST</th>
<th id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">15</th>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T3.1.4.4.4" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T3.1.4.4.5" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T3.1.4.4.6" class="ltx_td ltx_align_center">98.5</td>
</tr>
<tr id="S4.T3.1.5.5" class="ltx_tr">
<th id="S4.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S4.T3.1.5.5.3" class="ltx_td ltx_align_center">50.9</td>
<td id="S4.T3.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.5.5.5" class="ltx_td ltx_align_center">50.3</td>
<td id="S4.T3.1.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.5.6.1" class="ltx_text ltx_font_bold">52.9</span></td>
</tr>
<tr id="S4.T3.1.6.6" class="ltx_tr">
<th id="S4.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S4.T3.1.6.6.3" class="ltx_td ltx_align_center">49.1</td>
<td id="S4.T3.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.6.5" class="ltx_td ltx_align_center">47.6</td>
<td id="S4.T3.1.6.6.6" class="ltx_td ltx_align_center">52.2</td>
</tr>
<tr id="S4.T3.1.7.7" class="ltx_tr">
<th id="S4.T3.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Healthcare</th>
<th id="S4.T3.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S4.T3.1.7.7.3" class="ltx_td ltx_align_center">82.5</td>
<td id="S4.T3.1.7.7.4" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T3.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.7.5.1" class="ltx_text ltx_font_bold">85.7</span></td>
<td id="S4.T3.1.7.7.6" class="ltx_td ltx_align_center">83.5</td>
</tr>
<tr id="S4.T3.1.8.8" class="ltx_tr">
<th id="S4.T3.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Healthcare</th>
<th id="S4.T3.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">15</th>
<td id="S4.T3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">74.3</td>
<td id="S4.T3.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">73</td>
<td id="S4.T3.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">73</td>
<td id="S4.T3.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">74.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">Across the three datasets, there was no clear trend toward the best aggregator in the high IID setting. The best F1 score was recorded by FedProx (on MNIST), FedPer (on Healthcare) and SCAFFOLD (on CIFAR-10). FedAvg however ranked second best for each dataset.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Aggregator results by number of active devices across all datasets in the moderate IID setting</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Active</span></th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.2.2.2.1" class="ltx_text ltx_font_bold">Devices</span></th>
<th id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></th>
<th id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T4.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></th>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<td id="S4.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">MNIST</td>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">6</td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.3.3.1" class="ltx_text ltx_font_bold">99.1</span></td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">98.8</td>
<td id="S4.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">98.5</td>
<td id="S4.T4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">99</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<td id="S4.T4.1.4.4.1" class="ltx_td ltx_align_center">MNIST</td>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center">15</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center">98.6</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center">98.7</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T4.1.4.4.6" class="ltx_td ltx_align_center">98.9</td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<td id="S4.T4.1.5.5.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T4.1.5.5.2" class="ltx_td ltx_align_center">3</td>
<td id="S4.T4.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.5.3.1" class="ltx_text ltx_font_bold">52</span></td>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.5.5.5" class="ltx_td ltx_align_center">50.2</td>
<td id="S4.T4.1.5.5.6" class="ltx_td ltx_align_center">49.4</td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<td id="S4.T4.1.6.6.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T4.1.6.6.2" class="ltx_td ltx_align_center">6</td>
<td id="S4.T4.1.6.6.3" class="ltx_td ltx_align_center">47.6</td>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.6.6.5" class="ltx_td ltx_align_center">44.7</td>
<td id="S4.T4.1.6.6.6" class="ltx_td ltx_align_center">47.3</td>
</tr>
<tr id="S4.T4.1.7.7" class="ltx_tr">
<td id="S4.T4.1.7.7.1" class="ltx_td ltx_align_center">Healthcare</td>
<td id="S4.T4.1.7.7.2" class="ltx_td ltx_align_center">6</td>
<td id="S4.T4.1.7.7.3" class="ltx_td ltx_align_center">80</td>
<td id="S4.T4.1.7.7.4" class="ltx_td ltx_align_center">81.3</td>
<td id="S4.T4.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.7.7.5.1" class="ltx_text ltx_font_bold">87.8</span></td>
<td id="S4.T4.1.7.7.6" class="ltx_td ltx_align_center">83</td>
</tr>
<tr id="S4.T4.1.8.8" class="ltx_tr">
<td id="S4.T4.1.8.8.1" class="ltx_td ltx_align_center ltx_border_bb">Healthcare</td>
<td id="S4.T4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">15</td>
<td id="S4.T4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">73.4</td>
<td id="S4.T4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">67.9</td>
<td id="S4.T4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">66.9</td>
<td id="S4.T4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">70.9</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">In the moderately IID setting, and again in all three datasets, fewer active devices gave the best F1 results (TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Compared to the high IID setting, there was a greater difference in the best F1 scores between the larger and smaller number of active devices. In addition to having two of the three best F1 scores in each dataset, FedAvg was also the best scoring aggregator in most experiments in the moderately IID setting, regardless of whether more or fewer devices were used. FedAvgâ€™s scores were also more similar between the two conditions when compared to other aggregators. By comparison, SCAFFOLD similarly showed lower variance between the best F1 scores in the lower and higher device conditions, and FedPer achieved the best results in the Healthcare dataset.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Aggregator results by number of active devices across all datasets in the low IID setting</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Active</span></th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<th id="S4.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S4.T5.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S4.T5.1.2.2.2.1" class="ltx_text ltx_font_bold">Devices</span></th>
<th id="S4.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></th>
<th id="S4.T5.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T5.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.3.1" class="ltx_tr">
<th id="S4.T5.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">MNIST</th>
<th id="S4.T5.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">6</th>
<td id="S4.T5.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.1.3.1" class="ltx_text ltx_font_bold">99.3</span></td>
<td id="S4.T5.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">99.2</td>
<td id="S4.T5.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">99</td>
<td id="S4.T5.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">99.2</td>
</tr>
<tr id="S4.T5.1.4.2" class="ltx_tr">
<th id="S4.T5.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MNIST</th>
<th id="S4.T5.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">15</th>
<td id="S4.T5.1.4.2.3" class="ltx_td ltx_align_center">99</td>
<td id="S4.T5.1.4.2.4" class="ltx_td ltx_align_center">98.8</td>
<td id="S4.T5.1.4.2.5" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T5.1.4.2.6" class="ltx_td ltx_align_center">98.7</td>
</tr>
<tr id="S4.T5.1.5.3" class="ltx_tr">
<th id="S4.T5.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T5.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S4.T5.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.3.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T5.1.5.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.5.3.5" class="ltx_td ltx_align_center">64.4</td>
<td id="S4.T5.1.5.3.6" class="ltx_td ltx_align_center">64.3</td>
</tr>
<tr id="S4.T5.1.6.4" class="ltx_tr">
<th id="S4.T5.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T5.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S4.T5.1.6.4.3" class="ltx_td ltx_align_center">39.7</td>
<td id="S4.T5.1.6.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.6.4.5" class="ltx_td ltx_align_center">43.6</td>
<td id="S4.T5.1.6.4.6" class="ltx_td ltx_align_center">48.4</td>
</tr>
<tr id="S4.T5.1.7.5" class="ltx_tr">
<th id="S4.T5.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Healthcare</th>
<th id="S4.T5.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S4.T5.1.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.5.3.1" class="ltx_text ltx_font_bold">78.7</span></td>
<td id="S4.T5.1.7.5.4" class="ltx_td ltx_align_center">76</td>
<td id="S4.T5.1.7.5.5" class="ltx_td ltx_align_center">69.7</td>
<td id="S4.T5.1.7.5.6" class="ltx_td ltx_align_center">78.5</td>
</tr>
<tr id="S4.T5.1.8.6" class="ltx_tr">
<th id="S4.T5.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Healthcare</th>
<th id="S4.T5.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">15</th>
<td id="S4.T5.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb">59.3</td>
<td id="S4.T5.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb">55.8</td>
<td id="S4.T5.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb">59.4</td>
<td id="S4.T5.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb">58.5</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p">In the low IID setting, fewer active devices again demonstrated the best scores in all three datasets (TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). FedAvg was the best performing aggregator, achieving the best score on all three datasets. However, FedAvg also had the largest performance gap between the two device conditions. Compared to high IID and moderately IID settings, in low IID settings, there were greater gaps in F1 scores between the number of active devices for each aggregator and dataset.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p">With an unusually high score for CIFAR-10 in the low IID setting, FedAvg was able to outperform the best results in CIFAR-10 in the moderately IID and high IID settings. Upon inspecting the learning curves (FigureÂ <a href="#S4.F11" title="Figure 11 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>), this result can be interpreted as the consequence of an outlier score. The low IID setting also demonstrates volatile learning curves, where there is a high variance between F1 scores in each consecutive communication round.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure11.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Evidence of outliers and volatility in FedAvg experiments on CIFAR-10, low IID setting using both batch size 64 and 128</figcaption>
</figure>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p">To find greater clarity, outliers greater than three times the standard deviation of the learning curve were removed. Volatility was also smoothed using a moving average (window size = 3), and backfilling removed outliers. After this process, it became clearer that FedAvg was prone to outliers in the low IID setting and that SCAFFOLD was the best performing aggregator in CIFAR-10 in the low IID setting, as shown in TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
Lastly, it was generally observed that fewer active devices resulted in higher F1 scores being achieved earlier than with fewer communication rounds, as seen in FigureÂ <a href="#S4.F12" title="Figure 12 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Aggregator results by number of active devices across all datasets in the low IID setting, after removing outliers and smoothing volatility on CIFAR-10</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T6.1.2.2" class="ltx_tr">
<th id="S4.T6.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T6.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T6.1.2.2.2.1" class="ltx_text ltx_font_bold">Active Devices</span></th>
<th id="S4.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T6.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T6.1.2.2.4.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T6.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T6.1.2.2.5.1" class="ltx_text ltx_font_bold">SCAFFOLD</span></th>
</tr>
<tr id="S4.T6.1.3.3" class="ltx_tr">
<td id="S4.T6.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S4.T6.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T6.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">48.3</td>
<td id="S4.T6.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">43.4</td>
<td id="S4.T6.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.3.3.5.1" class="ltx_text ltx_font_bold">51.7</span></td>
</tr>
<tr id="S4.T6.1.4.4" class="ltx_tr">
<td id="S4.T6.1.4.4.1" class="ltx_td ltx_align_center ltx_border_bb">CIFAR-10</td>
<td id="S4.T6.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">6</td>
<td id="S4.T6.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">36.1</td>
<td id="S4.T6.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">35.7</td>
<td id="S4.T6.1.4.4.5" class="ltx_td ltx_align_center ltx_border_bb">39</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure13.png" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Results from CIFAR-10 in a highly IID setting, with fewer devices (darker lines) showing better F1 scores being achieved before the same score was achieved by more active devices</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Epochs</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">The FL training results when varying the number of local training epochs in a high IID setting are shown in TableÂ <a href="#S4.T7" title="Table 7 â€£ 4.2 Epochs â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. It can be seen that the general trend in the collected results suggests that the overall accuracy score improved with more training epochs. This was found that in most cases, when comparing the aggregator with the aggregator, the accuracy usually improved with a greater number of epochs. For example, in experiments with MNIST and CIFAR-10 as datasets, we can see that there was a consistent improvement in accuracy scores across all aggregators. FedPer and SCAFFOLD recorded an improvement of approximately 10% in the case of CIFAR-10.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">However, an interesting observation was made in experiments done for the Healthcare dataset. The accuracy of FedAvg and FedProx was better in the lower epoch condition. FedAvg recorded an accuracy drop of 4% with the highest epoch value, while the number of epochs had no significant impact on how FedProx worked with the dataset. FedPer and SCAFFOLD gave better accuracy with more epochs, and that was in line with what was observed for the MNIST and CIFAR-10 datasets. SCAFFOLD and FedPer gave consistently better results with more epochs in a high IID setting for all 3 datasets.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Aggregator results by number of epochs across all datasets in the high IID setting after removing outliers and smoothing volatility on CIFAR-10</figcaption>
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T7.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S4.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T7.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T7.1.2.2" class="ltx_tr">
<th id="S4.T7.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S4.T7.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T7.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S4.T7.1.2.2.2.1" class="ltx_text ltx_font_bold">Epochs</span></th>
<th id="S4.T7.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T7.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T7.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T7.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></th>
<th id="S4.T7.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T7.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T7.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T7.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.3.1" class="ltx_tr">
<th id="S4.T7.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">MNIST</th>
<th id="S4.T7.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T7.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">98.3</td>
<td id="S4.T7.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">98.2</td>
<td id="S4.T7.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">98.1</td>
<td id="S4.T7.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">98.3</td>
</tr>
<tr id="S4.T7.1.4.2" class="ltx_tr">
<th id="S4.T7.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MNIST</th>
<th id="S4.T7.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<td id="S4.T7.1.4.2.3" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T7.1.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.4.2.4.1" class="ltx_text ltx_font_bold">98.6</span></td>
<td id="S4.T7.1.4.2.5" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T7.1.4.2.6" class="ltx_td ltx_align_center">98.5</td>
</tr>
<tr id="S4.T7.1.5.3" class="ltx_tr">
<th id="S4.T7.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T7.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T7.1.5.3.3" class="ltx_td ltx_align_center">46.7</td>
<td id="S4.T7.1.5.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.1.5.3.5" class="ltx_td ltx_align_center">41.9</td>
<td id="S4.T7.1.5.3.6" class="ltx_td ltx_align_center">44.2</td>
</tr>
<tr id="S4.T7.1.6.4" class="ltx_tr">
<th id="S4.T7.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T7.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S4.T7.1.6.4.3" class="ltx_td ltx_align_center">51</td>
<td id="S4.T7.1.6.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.1.6.4.5" class="ltx_td ltx_align_center">50.4</td>
<td id="S4.T7.1.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T7.1.6.4.6.1" class="ltx_text ltx_font_bold">52.9</span></td>
</tr>
<tr id="S4.T7.1.7.5" class="ltx_tr">
<th id="S4.T7.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Healthcare</th>
<th id="S4.T7.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T7.1.7.5.3" class="ltx_td ltx_align_center">82.5</td>
<td id="S4.T7.1.7.5.4" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T7.1.7.5.5" class="ltx_td ltx_align_center">82.9</td>
<td id="S4.T7.1.7.5.6" class="ltx_td ltx_align_center">83.5</td>
</tr>
<tr id="S4.T7.1.8.6" class="ltx_tr">
<th id="S4.T7.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Healthcare</th>
<th id="S4.T7.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">5</th>
<td id="S4.T7.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb">78.4</td>
<td id="S4.T7.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb">79.4</td>
<td id="S4.T7.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.8.6.5.1" class="ltx_text ltx_font_bold">85.7</span></td>
<td id="S4.T7.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb">77.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">TableÂ <a href="#S4.T8" title="Table 8 â€£ 4.2 Epochs â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the results obtained from varying numbers of training epochs in a moderately IID environment. In contrast to what was observed for a high IID setting, in a moderate IID setting, it was observed that 2 out of 3 datasets (MNIST and Healthcare) performed better with fewer epoch values.
Experiments carried out with the Healthcare dataset showed a notable improvement in accuracy of approximately 30% accuracy when using fewer epochs. In the case of MNIST, there was not much difference in the performance of the aggregators under different epoch conditions, but the best overall result was achieved with fewer epoch values. However, in the case of CIFAR-10, most of the experiments showed that the aggregators worked better when the highest epoch value was selected.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p">Across all aggregators in these experiments, FedAvg performed relatively well for all data sets and obtained two of the three best accuracy scores (MNIST and CIFAR-10). Furthermore, it was observed that while FedPer was definitely not the best performing aggregator for the image recognition datasets, it was the best performing aggregator for the Healthcare dataset (a tabular dataset) outputting highest accuracy with both high and low epochs.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Aggregator results by number of epochs across all datasets in the moderate IID setting after removing outliers and smoothing volatility on CIFAR-10</figcaption>
<table id="S4.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T8.1.1.1" class="ltx_tr">
<td id="S4.T8.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T8.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T8.1.2.2" class="ltx_tr">
<th id="S4.T8.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T8.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T8.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T8.1.2.2.2.1" class="ltx_text ltx_font_bold">Epoch</span></th>
<th id="S4.T8.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T8.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></th>
<th id="S4.T8.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T8.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></th>
</tr>
<tr id="S4.T8.1.3.3" class="ltx_tr">
<td id="S4.T8.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">MNIST</td>
<td id="S4.T8.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T8.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.1.3.3.3.1" class="ltx_text ltx_font_bold">99.1</span></td>
<td id="S4.T8.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">98.8</td>
<td id="S4.T8.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">98.3</td>
<td id="S4.T8.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">98.6</td>
</tr>
<tr id="S4.T8.1.4.4" class="ltx_tr">
<td id="S4.T8.1.4.4.1" class="ltx_td ltx_align_center">MNIST</td>
<td id="S4.T8.1.4.4.2" class="ltx_td ltx_align_center">5</td>
<td id="S4.T8.1.4.4.3" class="ltx_td ltx_align_center">98.8</td>
<td id="S4.T8.1.4.4.4" class="ltx_td ltx_align_center">98.8</td>
<td id="S4.T8.1.4.4.5" class="ltx_td ltx_align_center">98.5</td>
<td id="S4.T8.1.4.4.6" class="ltx_td ltx_align_center">99</td>
</tr>
<tr id="S4.T8.1.5.5" class="ltx_tr">
<td id="S4.T8.1.5.5.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T8.1.5.5.2" class="ltx_td ltx_align_center">1</td>
<td id="S4.T8.1.5.5.3" class="ltx_td ltx_align_center">48.6</td>
<td id="S4.T8.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T8.1.5.5.5" class="ltx_td ltx_align_center">47.7</td>
<td id="S4.T8.1.5.5.6" class="ltx_td ltx_align_center">49.4</td>
</tr>
<tr id="S4.T8.1.6.6" class="ltx_tr">
<td id="S4.T8.1.6.6.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T8.1.6.6.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T8.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T8.1.6.6.3.1" class="ltx_text ltx_font_bold">52</span></td>
<td id="S4.T8.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T8.1.6.6.5" class="ltx_td ltx_align_center">50.2</td>
<td id="S4.T8.1.6.6.6" class="ltx_td ltx_align_center">48.3</td>
</tr>
<tr id="S4.T8.1.7.7" class="ltx_tr">
<td id="S4.T8.1.7.7.1" class="ltx_td ltx_align_center">Healthcare</td>
<td id="S4.T8.1.7.7.2" class="ltx_td ltx_align_center">1</td>
<td id="S4.T8.1.7.7.3" class="ltx_td ltx_align_center">80</td>
<td id="S4.T8.1.7.7.4" class="ltx_td ltx_align_center">81.3</td>
<td id="S4.T8.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T8.1.7.7.5.1" class="ltx_text ltx_font_bold">87.8</span></td>
<td id="S4.T8.1.7.7.6" class="ltx_td ltx_align_center">83</td>
</tr>
<tr id="S4.T8.1.8.8" class="ltx_tr">
<td id="S4.T8.1.8.8.1" class="ltx_td ltx_align_center ltx_border_bb">Healthcare</td>
<td id="S4.T8.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">5</td>
<td id="S4.T8.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">53.3</td>
<td id="S4.T8.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">50.8</td>
<td id="S4.T8.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">56.7</td>
<td id="S4.T8.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">49</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p">FigureÂ <a href="#S4.F13" title="Figure 13 â€£ 4.2 Epochs â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the learning curve of the best observed aggregator (FedPer) for Healthcare dataset under moderate IID setting. It can clearly be seen that throughout all the communications rounds the aggregator performed significantly better with fewer epochs (blue line) as opposed to more epochs (orange line).</p>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure14.png" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_img_square" width="329" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Comparison of model performance with fewer (blue line) and greater (orange line) epoch values in moderate IID setting</figcaption>
</figure>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p">The experimental results conducted in low IID settings are shown in TableÂ <a href="#S4.T9" title="Table 9 â€£ 4.2 Epochs â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. These experiments highlight that in a low IID data distribution when using MNIST and Healthcare, the aggregators performed better with fewer epochs. In particular, experiments with the Healthcare dataset showed a significant improvement in accuracy (that is, 21. 6% in the case of FedProx and 16. 5% in the case of FedAvg) when the lower epoch value was selected. However, experiments using the CIFAR-10 data set did not show similar improvements with fewer epochs, and FL aggregators were found to perform best when training with more epochs.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p">Among the four FL aggregators used in these experiments, FedAvg performed best in 5 out of 6 experiments. SCAFFOLD also demonstrated good results in the low IID setting and obtained results that were close to FedAvgâ€™s accuracy for the MNIST and Healthcare datasets. Interestingly, FedPer did not perform well in the low IID experiments, and achieved accuracy scores that were approximately 10% lower than the best performing aggregator for the CIFAR-10 and Healthcare datasets.</p>
</div>
<figure id="S4.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Aggregator results by number of epochs across all datasets in low IID setting after removing outliers and smoothing volatility on CIFAR-10</figcaption>
<table id="S4.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T9.1.1.1" class="ltx_tr">
<th id="S4.T9.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T9.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S4.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T9.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T9.1.2.2" class="ltx_tr">
<th id="S4.T9.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S4.T9.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T9.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S4.T9.1.2.2.2.1" class="ltx_text ltx_font_bold">Epoch</span></th>
<th id="S4.T9.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T9.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T9.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T9.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></th>
<th id="S4.T9.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T9.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T9.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T9.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T9.1.3.1" class="ltx_tr">
<th id="S4.T9.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">MNIST</th>
<th id="S4.T9.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T9.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.1.3.1.3.1" class="ltx_text ltx_font_bold">99.3</span></td>
<td id="S4.T9.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">99.2</td>
<td id="S4.T9.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">98.7</td>
<td id="S4.T9.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">99.1</td>
</tr>
<tr id="S4.T9.1.4.2" class="ltx_tr">
<th id="S4.T9.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MNIST</th>
<th id="S4.T9.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<td id="S4.T9.1.4.2.3" class="ltx_td ltx_align_center">99.2</td>
<td id="S4.T9.1.4.2.4" class="ltx_td ltx_align_center">99.2</td>
<td id="S4.T9.1.4.2.5" class="ltx_td ltx_align_center">99</td>
<td id="S4.T9.1.4.2.6" class="ltx_td ltx_align_center">99.2</td>
</tr>
<tr id="S4.T9.1.5.3" class="ltx_tr">
<th id="S4.T9.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T9.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T9.1.5.3.3" class="ltx_td ltx_align_center">61.5</td>
<td id="S4.T9.1.5.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T9.1.5.3.5" class="ltx_td ltx_align_center">57.2</td>
<td id="S4.T9.1.5.3.6" class="ltx_td ltx_align_center">57.5</td>
</tr>
<tr id="S4.T9.1.6.4" class="ltx_tr">
<th id="S4.T9.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T9.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S4.T9.1.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T9.1.6.4.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T9.1.6.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T9.1.6.4.5" class="ltx_td ltx_align_center">64.4</td>
<td id="S4.T9.1.6.4.6" class="ltx_td ltx_align_center">64.3</td>
</tr>
<tr id="S4.T9.1.7.5" class="ltx_tr">
<th id="S4.T9.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Healthcare</th>
<th id="S4.T9.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T9.1.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T9.1.7.5.3.1" class="ltx_text ltx_font_bold">78.7</span></td>
<td id="S4.T9.1.7.5.4" class="ltx_td ltx_align_center">76</td>
<td id="S4.T9.1.7.5.5" class="ltx_td ltx_align_center">69.7</td>
<td id="S4.T9.1.7.5.6" class="ltx_td ltx_align_center">78.5</td>
</tr>
<tr id="S4.T9.1.8.6" class="ltx_tr">
<th id="S4.T9.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Healthcare</th>
<th id="S4.T9.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">5</th>
<td id="S4.T9.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb">62.2</td>
<td id="S4.T9.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb">54.4</td>
<td id="S4.T9.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb">62.1</td>
<td id="S4.T9.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb">69.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p8" class="ltx_para ltx_noindent">
<p id="S4.SS2.p8.1" class="ltx_p">FigureÂ <a href="#S4.F14" title="Figure 14 â€£ 4.2 Epochs â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> shows the learning curve of the best observed aggregator (FedAvg) for Healthcare dataset under the low IID setting. Even though volatility in learning was observed, overall the aggregator performed much better with fewer epochs (blue line) throughout the communication rounds of the experiment.</p>
</div>
<figure id="S4.F14" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure15.png" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_img_square" width="329" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Comparison of model performance with fewer (blue line) and greater (orange line) epoch values in low IID setting</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Batch Size</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">In the highly IID setting, best results were observed typically with larger batch sizes, as seen in TableÂ <a href="#S4.T10" title="Table 10 â€£ 4.3 Batch Size â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. However, the best score for MNIST was achieved with a smaller batch size using FedProx. The next best score in the larger batch size setting came from SCAFFOLD and was only slightly less than this though (98.6% vs 98.5%).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">When observing the performance over communication rounds for the Healthcare dataset, the most stable aggregators did not offer the best accuracy. When keeping all other experimental parameters the same, we see from FigureÂ <a href="#S4.F15" title="Figure 15 â€£ 4.3 Batch Size â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> there is a preference towards the smaller batch size of size 10 (orange line) than a larger batch size of size 35 (blue line) for most aggregators. This provides further support to the notion that a smaller batch size is preferred in a high IID setting.</p>
</div>
<figure id="S4.F15" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure16.png" id="S4.F15.g1" class="ltx_graphics ltx_centering ltx_img_square" width="568" height="496" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Comparison of batch size by aggregator Healthcare dataset in high IID settings</figcaption>
</figure>
<figure id="S4.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Aggregator results by batch size across all datasets in highly IID settings</figcaption>
<table id="S4.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T10.1.1.1" class="ltx_tr">
<th id="S4.T10.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T10.1.1.1.2.1" class="ltx_text ltx_font_bold">Batch</span></th>
<td id="S4.T10.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T10.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></td>
</tr>
<tr id="S4.T10.1.2.2" class="ltx_tr">
<th id="S4.T10.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T10.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T10.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T10.1.2.2.2.1" class="ltx_text ltx_font_bold">size</span></th>
<td id="S4.T10.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T10.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></td>
<td id="S4.T10.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T10.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></td>
<td id="S4.T10.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T10.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></td>
<td id="S4.T10.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T10.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></td>
</tr>
<tr id="S4.T10.1.3.3" class="ltx_tr">
<th id="S4.T10.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">MNIST</th>
<th id="S4.T10.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">10</th>
<td id="S4.T10.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">98.4</td>
<td id="S4.T10.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">98.6</td>
<td id="S4.T10.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">98.2</td>
<td id="S4.T10.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">98.5</td>
</tr>
<tr id="S4.T10.1.4.4" class="ltx_tr">
<th id="S4.T10.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MNIST</th>
<th id="S4.T10.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S4.T10.1.4.4.3" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T10.1.4.4.4" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T10.1.4.4.5" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T10.1.4.4.6" class="ltx_td ltx_align_center">98.5</td>
</tr>
<tr id="S4.T10.1.5.5" class="ltx_tr">
<th id="S4.T10.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T10.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">64</th>
<td id="S4.T10.1.5.5.3" class="ltx_td ltx_align_center">50.9</td>
<td id="S4.T10.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T10.1.5.5.5" class="ltx_td ltx_align_center">48</td>
<td id="S4.T10.1.5.5.6" class="ltx_td ltx_align_center">52.2</td>
</tr>
<tr id="S4.T10.1.6.6" class="ltx_tr">
<th id="S4.T10.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T10.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">128</th>
<td id="S4.T10.1.6.6.3" class="ltx_td ltx_align_center">49</td>
<td id="S4.T10.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T10.1.6.6.5" class="ltx_td ltx_align_center">50.3</td>
<td id="S4.T10.1.6.6.6" class="ltx_td ltx_align_center">52.9</td>
</tr>
<tr id="S4.T10.1.7.7" class="ltx_tr">
<th id="S4.T10.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Healthcare</th>
<th id="S4.T10.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T10.1.7.7.3" class="ltx_td ltx_align_center">77.8</td>
<td id="S4.T10.1.7.7.4" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T10.1.7.7.5" class="ltx_td ltx_align_center">83</td>
<td id="S4.T10.1.7.7.6" class="ltx_td ltx_align_center">80</td>
</tr>
<tr id="S4.T10.1.8.8" class="ltx_tr">
<th id="S4.T10.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Healthcare</th>
<th id="S4.T10.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">35</th>
<td id="S4.T10.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">82.4</td>
<td id="S4.T10.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">79.4</td>
<td id="S4.T10.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">85.7</td>
<td id="S4.T10.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">83.5</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p">In the moderately IID setting, larger batch sizes tended to outperform smaller batch sizes. This was not true for the Healthcare dataset though, where a smaller batch size gave the best performance.</p>
</div>
<figure id="S4.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Aggregator results by batch size across all datasets in moderately IID settings</figcaption>
<table id="S4.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T11.1.1.1" class="ltx_tr">
<td id="S4.T11.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T11.1.1.1.2.1" class="ltx_text ltx_font_bold">Batch</span></th>
<th id="S4.T11.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T11.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></th>
</tr>
<tr id="S4.T11.1.2.2" class="ltx_tr">
<th id="S4.T11.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T11.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T11.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T11.1.2.2.2.1" class="ltx_text ltx_font_bold">size</span></th>
<th id="S4.T11.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T11.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></th>
<th id="S4.T11.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T11.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></th>
<th id="S4.T11.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T11.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></th>
<th id="S4.T11.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T11.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></th>
</tr>
<tr id="S4.T11.1.3.3" class="ltx_tr">
<td id="S4.T11.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">MNIST</td>
<td id="S4.T11.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">10</td>
<td id="S4.T11.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">98.6</td>
<td id="S4.T11.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">98.8</td>
<td id="S4.T11.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">98.4</td>
<td id="S4.T11.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">98.9</td>
</tr>
<tr id="S4.T11.1.4.4" class="ltx_tr">
<td id="S4.T11.1.4.4.1" class="ltx_td ltx_align_center">MNIST</td>
<td id="S4.T11.1.4.4.2" class="ltx_td ltx_align_center">50</td>
<td id="S4.T11.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T11.1.4.4.3.1" class="ltx_text ltx_font_bold">99.1</span></td>
<td id="S4.T11.1.4.4.4" class="ltx_td ltx_align_center">98.7</td>
<td id="S4.T11.1.4.4.5" class="ltx_td ltx_align_center">98.5</td>
<td id="S4.T11.1.4.4.6" class="ltx_td ltx_align_center">99</td>
</tr>
<tr id="S4.T11.1.5.5" class="ltx_tr">
<td id="S4.T11.1.5.5.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T11.1.5.5.2" class="ltx_td ltx_align_center">64</td>
<td id="S4.T11.1.5.5.3" class="ltx_td ltx_align_center">48.6</td>
<td id="S4.T11.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T11.1.5.5.5" class="ltx_td ltx_align_center">50.2</td>
<td id="S4.T11.1.5.5.6" class="ltx_td ltx_align_center">48.3</td>
</tr>
<tr id="S4.T11.1.6.6" class="ltx_tr">
<td id="S4.T11.1.6.6.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T11.1.6.6.2" class="ltx_td ltx_align_center">128</td>
<td id="S4.T11.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T11.1.6.6.3.1" class="ltx_text ltx_font_bold">52</span></td>
<td id="S4.T11.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T11.1.6.6.5" class="ltx_td ltx_align_center">42.7</td>
<td id="S4.T11.1.6.6.6" class="ltx_td ltx_align_center">49.4</td>
</tr>
<tr id="S4.T11.1.7.7" class="ltx_tr">
<td id="S4.T11.1.7.7.1" class="ltx_td ltx_align_center">Healthcare</td>
<td id="S4.T11.1.7.7.2" class="ltx_td ltx_align_center">10</td>
<td id="S4.T11.1.7.7.3" class="ltx_td ltx_align_center">76.1</td>
<td id="S4.T11.1.7.7.4" class="ltx_td ltx_align_center">77.5</td>
<td id="S4.T11.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T11.1.7.7.5.1" class="ltx_text ltx_font_bold">87.8</span></td>
<td id="S4.T11.1.7.7.6" class="ltx_td ltx_align_center">79.4</td>
</tr>
<tr id="S4.T11.1.8.8" class="ltx_tr">
<td id="S4.T11.1.8.8.1" class="ltx_td ltx_align_center ltx_border_bb">Healthcare</td>
<td id="S4.T11.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">35</td>
<td id="S4.T11.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">80</td>
<td id="S4.T11.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">81.3</td>
<td id="S4.T11.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">80.1</td>
<td id="S4.T11.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">83</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p">Using the MNIST and Healthcare datasets in a low IID setting, larger batch sizes performed best. However, there was no clear trend for CIFAR-10, as smaller batch sizes worked best in the low IID setting, but larger batch sizes achieved better results in the moderately IID setting. One potential explanation for this is the outliers observed in aggregator performance with CIFAR-10 in the low IID setting. The best scores for MNIST varied little between settings, although clearly a larger batch size was best. This was similar for Healthcare, as seen in FigureÂ <a href="#S4.F16" title="Figure 16 â€£ 4.3 Batch Size â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>: when using a larger batch size of size 35 (blue line), better performance was achieved than when using a smaller batch size of size 10 (orange line).</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p">Overall, we observed a relationship where a lower batch size is preferred in high IID settings, but a higher batch size is preferred in low IID settings. A notable deviation from this relationship was observed in FedPer and CIFAR-10, where a larger batch size was preferred, regardless of the IID category.</p>
</div>
<figure id="S4.F16" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure17.png" id="S4.F16.g1" class="ltx_graphics ltx_centering ltx_img_square" width="568" height="493" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Comparison of batch size by aggregator Healthcare dataset in low IID settings</figcaption>
</figure>
<figure id="S4.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Aggregator results by batch size across all datasets in low IID settings</figcaption>
<table id="S4.T12.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T12.1.1.1" class="ltx_tr">
<th id="S4.T12.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T12.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T12.1.1.1.2.1" class="ltx_text ltx_font_bold">Batch</span></th>
<td id="S4.T12.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T12.1.1.1.3.1" class="ltx_text ltx_font_bold">Aggregator</span></td>
</tr>
<tr id="S4.T12.1.2.2" class="ltx_tr">
<th id="S4.T12.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T12.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T12.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T12.1.2.2.2.1" class="ltx_text ltx_font_bold">size</span></th>
<td id="S4.T12.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T12.1.2.2.3.1" class="ltx_text ltx_font_bold">FedAvg</span></td>
<td id="S4.T12.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T12.1.2.2.4.1" class="ltx_text ltx_font_bold">FedProx</span></td>
<td id="S4.T12.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T12.1.2.2.5.1" class="ltx_text ltx_font_bold">FedPer</span></td>
<td id="S4.T12.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T12.1.2.2.6.1" class="ltx_text ltx_font_bold">Scaffold</span></td>
</tr>
<tr id="S4.T12.1.3.3" class="ltx_tr">
<th id="S4.T12.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">MNIST</th>
<th id="S4.T12.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">10</th>
<td id="S4.T12.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">99.1</td>
<td id="S4.T12.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">99.1</td>
<td id="S4.T12.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">98.6</td>
<td id="S4.T12.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">99.2</td>
</tr>
<tr id="S4.T12.1.4.4" class="ltx_tr">
<th id="S4.T12.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MNIST</th>
<th id="S4.T12.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S4.T12.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T12.1.4.4.3.1" class="ltx_text ltx_font_bold">99.3</span></td>
<td id="S4.T12.1.4.4.4" class="ltx_td ltx_align_center">99.2</td>
<td id="S4.T12.1.4.4.5" class="ltx_td ltx_align_center">99</td>
<td id="S4.T12.1.4.4.6" class="ltx_td ltx_align_center">99.1</td>
</tr>
<tr id="S4.T12.1.5.5" class="ltx_tr">
<th id="S4.T12.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T12.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">64</th>
<td id="S4.T12.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T12.1.5.5.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T12.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T12.1.5.5.5" class="ltx_td ltx_align_center">57.2</td>
<td id="S4.T12.1.5.5.6" class="ltx_td ltx_align_center">64.3</td>
</tr>
<tr id="S4.T12.1.6.6" class="ltx_tr">
<th id="S4.T12.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">CIFAR-10</th>
<th id="S4.T12.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">128</th>
<td id="S4.T12.1.6.6.3" class="ltx_td ltx_align_center">61.5</td>
<td id="S4.T12.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T12.1.6.6.5" class="ltx_td ltx_align_center">64.4</td>
<td id="S4.T12.1.6.6.6" class="ltx_td ltx_align_center">61.6</td>
</tr>
<tr id="S4.T12.1.7.7" class="ltx_tr">
<th id="S4.T12.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Healthcare</th>
<th id="S4.T12.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T12.1.7.7.3" class="ltx_td ltx_align_center">75.8</td>
<td id="S4.T12.1.7.7.4" class="ltx_td ltx_align_center">72.8</td>
<td id="S4.T12.1.7.7.5" class="ltx_td ltx_align_center">69.7</td>
<td id="S4.T12.1.7.7.6" class="ltx_td ltx_align_center">78.5</td>
</tr>
<tr id="S4.T12.1.8.8" class="ltx_tr">
<th id="S4.T12.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Healthcare</th>
<th id="S4.T12.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">35</th>
<td id="S4.T12.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T12.1.8.8.3.1" class="ltx_text ltx_font_bold">78.7</span></td>
<td id="S4.T12.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">76</td>
<td id="S4.T12.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">63</td>
<td id="S4.T12.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">75.5</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Impact of Data Quantity Variance</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">In addition to the results of the investigation into the variance in data quantity and EMD found in SectionÂ <a href="#S3.SS2" title="3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we provide further experimentation here on the impact of the variance in data quantity on the performance of the aggregator. As shown by FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> andÂ <a href="#S3.F5" title="Figure 5 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> in SectionÂ <a href="#S3.SS2" title="3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the variance of the data quantity had little impact on the calculated EMD scores, however, the increase in the variance of the data quantity was associated with a degradation in the F1 scores. On the contrary, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> found that there was limited impact of the data quantity skew on the aggregator performance. In this section, our aim is to further validate the effects of data quantity variance by testing the best performing aggregators (that is, in terms of best F1 scores per dataset and IID setting combination) found above against different levels of data quantity variance.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">As shown in TableÂ <a href="#S4.T13" title="Table 13 â€£ 4.4 Impact of Data Quantity Variance â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, in the case of the CIFAR-10 and MNIST datasets, a negative relationship was observed between the variance in the data quantity and the resulting F1 scores: increased variance in the data quantity was associated with decreases in the F1 score in all IID settings. However, the results in the Healthcare dataset were more inconsistent. In general, results for the Healthcare dataset became better with increased data quantity variance, although in the highly IID setting, no clear relationship was seen.</p>
</div>
<figure id="S4.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span>Best F1 scores from best aggregators across three levels of data quantity variance</figcaption>
<table id="S4.T13.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T13.1.1.1" class="ltx_tr">
<td id="S4.T13.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T13.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T13.1.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.T13.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T13.1.1.1.4.1" class="ltx_text ltx_font_bold">Variance</span></td>
</tr>
<tr id="S4.T13.1.2.2" class="ltx_tr">
<td id="S4.T13.1.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T13.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T13.1.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T13.1.2.2.2.1" class="ltx_text ltx_font_bold">IID</span></td>
<td id="S4.T13.1.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T13.1.2.2.3.1" class="ltx_text ltx_font_bold">Best Aggregator</span></td>
<td id="S4.T13.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T13.1.2.2.4.1" class="ltx_text ltx_font_bold">0%</span></td>
<td id="S4.T13.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T13.1.2.2.5.1" class="ltx_text ltx_font_bold">50%</span></td>
<td id="S4.T13.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T13.1.2.2.6.1" class="ltx_text ltx_font_bold">90%</span></td>
</tr>
<tr id="S4.T13.1.3.3" class="ltx_tr">
<td id="S4.T13.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">MNIST</td>
<td id="S4.T13.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">High</td>
<td id="S4.T13.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">FedAvg</td>
<td id="S4.T13.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T13.1.3.3.4.1" class="ltx_text ltx_font_bold">98.6</span></td>
<td id="S4.T13.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">98.4</td>
<td id="S4.T13.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">98.1</td>
</tr>
<tr id="S4.T13.1.4.4" class="ltx_tr">
<td id="S4.T13.1.4.4.1" class="ltx_td ltx_align_center">MNIST</td>
<td id="S4.T13.1.4.4.2" class="ltx_td ltx_align_center">Moderate</td>
<td id="S4.T13.1.4.4.3" class="ltx_td ltx_align_center">SCAFFOLD</td>
<td id="S4.T13.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T13.1.4.4.4.1" class="ltx_text ltx_font_bold">99.1</span></td>
<td id="S4.T13.1.4.4.5" class="ltx_td ltx_align_center">98.6</td>
<td id="S4.T13.1.4.4.6" class="ltx_td ltx_align_center">98.4</td>
</tr>
<tr id="S4.T13.1.5.5" class="ltx_tr">
<td id="S4.T13.1.5.5.1" class="ltx_td ltx_align_center">MNIST</td>
<td id="S4.T13.1.5.5.2" class="ltx_td ltx_align_center">Low</td>
<td id="S4.T13.1.5.5.3" class="ltx_td ltx_align_center">FedPer</td>
<td id="S4.T13.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T13.1.5.5.4.1" class="ltx_text ltx_font_bold">98.8</span></td>
<td id="S4.T13.1.5.5.5" class="ltx_td ltx_align_center">98.3</td>
<td id="S4.T13.1.5.5.6" class="ltx_td ltx_align_center">98</td>
</tr>
<tr id="S4.T13.1.6.6" class="ltx_tr">
<td id="S4.T13.1.6.6.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T13.1.6.6.2" class="ltx_td ltx_align_center">High</td>
<td id="S4.T13.1.6.6.3" class="ltx_td ltx_align_center">SCAFFOLD</td>
<td id="S4.T13.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T13.1.6.6.4.1" class="ltx_text ltx_font_bold">52.9</span></td>
<td id="S4.T13.1.6.6.5" class="ltx_td ltx_align_center">41.7</td>
<td id="S4.T13.1.6.6.6" class="ltx_td ltx_align_center">39.2</td>
</tr>
<tr id="S4.T13.1.7.7" class="ltx_tr">
<td id="S4.T13.1.7.7.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T13.1.7.7.2" class="ltx_td ltx_align_center">Moderate</td>
<td id="S4.T13.1.7.7.3" class="ltx_td ltx_align_center">FedPer</td>
<td id="S4.T13.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T13.1.7.7.4.1" class="ltx_text ltx_font_bold">52</span></td>
<td id="S4.T13.1.7.7.5" class="ltx_td ltx_align_center">45.4</td>
<td id="S4.T13.1.7.7.6" class="ltx_td ltx_align_center">41</td>
</tr>
<tr id="S4.T13.1.8.8" class="ltx_tr">
<td id="S4.T13.1.8.8.1" class="ltx_td ltx_align_center">CIFAR-10</td>
<td id="S4.T13.1.8.8.2" class="ltx_td ltx_align_center">Low</td>
<td id="S4.T13.1.8.8.3" class="ltx_td ltx_align_center">SCAFFOLD</td>
<td id="S4.T13.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T13.1.8.8.4.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T13.1.8.8.5" class="ltx_td ltx_align_center">64.1</td>
<td id="S4.T13.1.8.8.6" class="ltx_td ltx_align_center">58.2</td>
</tr>
<tr id="S4.T13.1.9.9" class="ltx_tr">
<td id="S4.T13.1.9.9.1" class="ltx_td ltx_align_center">Healthcare</td>
<td id="S4.T13.1.9.9.2" class="ltx_td ltx_align_center">High</td>
<td id="S4.T13.1.9.9.3" class="ltx_td ltx_align_center">SCAFFOLD</td>
<td id="S4.T13.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T13.1.9.9.4.1" class="ltx_text ltx_font_bold">73.7</span></td>
<td id="S4.T13.1.9.9.5" class="ltx_td ltx_align_center">59.1</td>
<td id="S4.T13.1.9.9.6" class="ltx_td ltx_align_center">66.7</td>
</tr>
<tr id="S4.T13.1.10.10" class="ltx_tr">
<td id="S4.T13.1.10.10.1" class="ltx_td ltx_align_center ltx_border_bb">Healthcare</td>
<td id="S4.T13.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb">Moderate</td>
<td id="S4.T13.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb">FedPer</td>
<td id="S4.T13.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb">59.7</td>
<td id="S4.T13.1.10.10.5" class="ltx_td ltx_align_center ltx_border_bb">58.7</td>
<td id="S4.T13.1.10.10.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T13.1.10.10.6.1" class="ltx_text ltx_font_bold">62.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Active Devices</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">When selected at random, across all IID settings and datasets, selecting fewer devices consistently demonstrated better F1 scores than when selecting more active devices. In the low IID setting, there was a considerably greater gap between the F1 scores of the two device conditions compared to the high IID setting, where the effect was diminished. A moderate gap between the conditions was observed in the moderately IID setting, and provides further evidence to show how the effect of the number of active devices scales with each IID setting.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">When varied by number of active devices, no aggregator clearly performed the best across all IID settings and datasets. However, similar to Hsu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we observed greater volatility in F1 scores in low IID settings, which may have obscured any trends in the best aggregators. Before the removal of outliers and smoothing of volatility in low IID settings, it appeared that FedAvg was capable of scoring the best results. However, after treating outliers and volatility, SCAFFOLD emerged as a strong performer in the low IID setting. SCAFFOLD appeared to have less variance between the results of the two device conditions. This supports two claims from the original SCAFFOLD paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which concluded that the aggregator was more resilient to client sampling and that it may also be a more adequate aggregator for non-convex (i.e., low IID) problems than FedAvg.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">In McMahon et al.â€™s original paper introducing FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, it was reported that increasing the number of active devices corresponded to marginal increases in performance beyond a certain fraction of reporting clients (10% of total clients). Notably, it was also shown that increasing the number of active clients did not always provide a uniform improvement. In certain scenarios, it was reported that increasing the number of active clients would reduce performance relative to smaller fractions of reporting clients. As shown in our results, we almost uniformly saw worse results when using more active devices. Additionally, McMahan et al. also reported that increasing the fraction of active devices resulted in fewer communication rounds required to reach a target level of accuracy. However, our results indicated the opposite, as shown in FigureÂ <a href="#S4.F12" title="Figure 12 â€£ 4.1 Active Devices â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>: we observed fewer devices achieving greater F1 scores before more active devices could.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p">Other than the contribution from McMahan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, testing aggregators with varying numbers of active devices has been infrequently or not extensively tested. Results in literature are often reported when selecting all 10 clients from a total of 10 clients, such as with FedPer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, rather than with random selections of active devices. With FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a related concept of stragglers is tested. A straggler is a device that is active, but has not completed the required amount of training at the time of aggregation. This concept, however, is unique to the FedProx aggregator, and reported results from varied numbers of stragglers did not indicate influence on model loss.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p">One explanation for the results we observed comes from Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, who similarly recorded that better accuracy scores were obtainable by selecting fewer active devices on different aggregators. By selecting a smaller number of active devices, the aggregation of fewer models provides a clearer signal, and there is less capacity for overfitting by each individual model. Therefore, it is apparent that the selection of devices to aggregate is relevant to good global model performance. This is especially clear in lower IID settings, where the consequence of selecting a greater number of active devices can result in markedly worse global model performance.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Epochs</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">In general, it was observed that the optimal number of epochs found for a certain IID setting was influenced by the underlying data distribution. In high IID settings, the aggregators gave better accuracy with more epochs, while in moderate and low IID settings, the accuracy was better with fewer epochs. A quite substantial gap between moderate and low IID settings was observed in the scores recorded for the Healthcare dataset. This finding is also consistent across all aggregators, as shown in Figures 14 and 15. One explanation for this is that a high IID setting is more comparable to that of a centralised learning scenario, as opposed to an FL scenario. In such centralised scenarios, using more epochs typically improves the overall accuracy of a model, whereas using more epochs in a lower IID setting results in the model overfitting the data, and thus negatively impacting its accuracy.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">As shown in the results, different aggregators have different sensitivity to the number of epochs. This can be attributed to the differences in the optimisation algorithms used by each aggregator. For example, aggregators with more complex optimisation algorithms, such as SCAFFOLD, may exhibit sensitivity to the number of epochs due to the iterative nature of their learning process. On the other hand, aggregators with simpler optimization algorithms, such as FedAvg, may be less sensitive to the number of epochs as they rely more on the averaging of model updates rather than fine-tuning with each iteration. Additionally, sensitivity or insensitivity to the number of epochs can also be influenced by the inherent characteristics of the dataset. For example, datasets with a higher degree of complexity or variability may require more epochs for aggregators to converge and achieve optimal performance.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">There was no single aggregator that clearly outperformed other aggregators across all IID settings and datasets. Despite this, it is worth noting that FedAvg performed better than the other aggregators in many of the lower IID settings. This observation supports the findings of Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> in which it is mentioned that the number of local epochs can have a large effect on accuracy and that the optimal value of the number of local epochs is sensitive to non-IID distributions. In lower IID settings, they reported that the accuracy decreased with higher local epoch settings. They suggested that the existing algorithms were not robust enough against large local updates. Furthermore, our findings are in line with the conclusions presented by McMahan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, in which, under similar testing conditions, the experiment with a lower epoch value gave better results than the experiments with a higher epoch value.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p">According to Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and similar to what we observed, the number of local training epochs affects the performance of aggregators such as FedAvg and FedProx as a consequence of model divergence under low IID conditions. It was also reported that only the performance of one aggregator (FedMA) improved with more local training epochs. Potentially then, certain aggregators may perform best with a number of local epochs specific for that aggregator in a given IID setting. Furthermore, the optimal number of local epochs may be variable per communication round, as suggested by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. FedProx attempts to manage this under conditions of systems heterogeneity, and though this was not specifically tested for in our experimentation, FedProx was the second best performing aggregator in lower-IID settings with fewer epochs overall. We also observed that SCAFFOLD under all IID settings always performed better with fewer epochs. This takeaway is similar to the conclusions drawn in the original SCAFFOLD paper by Karimireddy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p">In contrast to the findings on the MNIST and Healthcare datasets, we found that CIFAR-10 consistently had the best results when tested with more epochs rather than fewer. One explanation for this is that CIFAR-10 is a much more complex image recognition problem than MNIST, and that the difficulty of the task posed by the Healthcare dataset is comparatively less complex. Therefore, CIFAR-10 benefitted from more local epochs, and the best number of local epochs may be even greater than the values we were able to test.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p">Lastly, it is important to consider the goal of FL when choosing whether to increase or decrease local epochs. According to the arguments presented by Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, the objective of FL should not be to simply produce a good global model, but rather to increase the personalised performance of local devices. They suggest that, in the case of aggregators such as FedAvg, aggregation attempts to treat decentralised data with the same objective as if it were centralised. Consequently, improved results in lower IID settings when using fewer epochs fails to account for the goal of personalised performance. To this end, they recommend using approaches such as model-agnostic meta-learning, which may involve increasing the number of local epochs even in low IID settings.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Batch Size</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">In high IID settings, we observed the best results from smaller batch sizes. However, in low IID settings, larger batch sizes provided the best results. This is in contrast to Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, where it was reported that batch size has no influence on aggregator performance. Our results instead somewhat support the conclusions of He et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, who found that in a distributed setting, batch size influences a trade-off between communication cost and convergence speed. However, they reported that large batch sizes may increase convergence speed but at the detriment of accuracy, which we did not observe in lower IID settings. Further improvements to convergence speed may come from refining the choice of optimiser, associated learning rates and values of momentum, as suggested by Felbab et al.â€™s work in a distributed setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, though their results were not broken down by different levels of IID. The generalisation of convolutional neural networks has been found to be a function of the ratio between batch size and learning rate, which controls the gradient descent speed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. To this end, it has been reported that decreases in batch size can cause a comparable increase in accuracy as would be possible with a decrease in learning rates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. However, within our experiments, we assumed a constant learning rate that was specific to each data set, and therefore batch size may have played the role of a proxy for learning rate.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">The preference for lower batch sizes in high IID settings can be driven by several factors. Firstly, lower batch sizes reduce communication overhead between the central server and participating devices, promoting more efficient updates. Lower batch sizes help mitigate device heterogeneity, contributing to model consistency across devices. Additionally, the use of smaller batch sizes increases model robustness by minimising the risk of overfitting to specific local patterns. In the other hand, larger batch sizes help mitigate the challenges posed by the non-uniformity and diversity of data across participating devices, facilitating more stable and representative updates to the global model. There are several implications of lower batch sizes in high IDD settings. Resource constraints on participating devices in the FL architecture are accommodated, and privacy preservation is enhanced as smaller batches reduce the potential disclosure of sensitive information during the federated learning process. Compared to in low IID settings where local data distributions vary significantly, larger batch sizes contribute to a more robust averaging of diverse gradients, enhancing the model in capturing and adapting to broader patterns in the data.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p">Nasirigerdeh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, who offered the explanation of a generalisation gap, observed similar trends between batch size and IID settings. The generalisation gap is between local and global models, which becomes larger in low IID settings, but may be improved by using larger batch sizes on the model hosted by local clients. Additionally, our finding that smaller batch sizes achieve higher test accuracy in high IID settings has also been observed in centralised machine learning as in He et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, where a larger batch size instead fails to learn sharp local minima during training.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p">Looking more closely into the trends of aggregator performances and particular datasets, we observed that FedProx was the most robust aggregator to changes in batch size on the Healthcare dataset, as shown in FigureÂ <a href="#S4.F16" title="Figure 16 â€£ 4.3 Batch Size â€£ 4 Results â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. This is potentially due to the choice of value for the proximal term (mu = 0.001), but is in contrast to the view of Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> where it is stated that the regularisation term of FedProx does not have a significant effect when the batch size is small. Typically, we saw similar accuracy regardless of batch size in low IID settings. FedProx algorithm incorporates a proximal term that helps prevent overfitting and promotes stability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. FedProx also considers both model updates and the proximity of local models to the global model, facilitating an effective combination of information from devices with varying batch sizes. The algorithmâ€™s ability to handle data heterogeneity and encourage global consensus further contributes to its stability across devices with different training conditions. Additionally, FedProx allows for flexibility in learning rates, enabling devices to adapt to varying batch sizes during training. In summary, these characteristics make FedProx less sensitive to changes in batch size, ensuring robust performance in federated learning scenarios on the Healthcare dataset.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p">Regardless of the IID setting, we observed that FedPer performed the best on larger batch sizes. This suggests that the personalisation layer of the model needs sufficient batch size to train and update the base layer on each device, and perhaps aggregators such as FedProx and SCAFFOLD require even greater batch sizes to perform similarly well. Lastly, for the more complex dataset CIFAR-10, we saw that a larger batch size was preferred in all IID levels and aggregators. Interestingly, this relationship was also observed by Reyes et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, who found that larger batch sizes were preferable for CIFAR-10, but not for MNIST, where larger batch sizes deteriorated performance. The insight drawn from this work supports the idea that there is greater complexity, or data heterogeneity, in models trained on natural images such as CIFAR-10, as opposed to the greyscale images of handwritten digits in MNIST, and this would be the case even in high IID settings. Hence, this result is sensible and expected for CIFAR-10 and highlights that there are still methods to be explored to delineate between IID levels.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Impact of Data Quantity Variance</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.1" class="ltx_p">We assume that F1 scores would decrease with increased data quantity variance, since the global model would be learning from less reliable data samples at each communication round. In building and validating our data partitioning method, we observed this relationship, though data quantity variance had little impact on the EMD metric when we experimented with the best aggregators, and typically this trend emerged again across all IID settings. In the case of the CIFAR-10 and MNIST datasets, increased data quantity variance was associated with worsened F1 scores. However, with the Healthcare dataset, F1 scores tended to improve with greater variance in the quantity of data. Potentially, this is influenced by the datasetâ€™s tabular nature as opposed to the more complex image recognition problems of CIFAR-10 and MNIST. Given that the results tended to improve with reduced data quantity variance and that the corresponding EMD scores did not decrease with reduced data quantity variance, we conclude that to fully capture the nature of the IID setting, the data quantity variance as a measure should be quoted in conjunction with the EMD metric.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Summary</h3>

<figure id="S5.F17" class="ltx_figure"><img src="/html/2406.06340/assets/figures/Extra/Figure18-updated.png" id="S5.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Summary of Recommended Settings across IID Levels</figcaption>
</figure>
<div id="S5.SS5.p1" class="ltx_para ltx_noindent">
<p id="S5.SS5.p1.1" class="ltx_p">In this study, we extensively tested four FL aggregators in three datasets and levels of IID. In each setting, we investigated three parameters: active devices, local epochs, and batch size, which impacts how devices learn and contribute to a global model in a federated setting. Our contribution is to further the understanding of how each aggregator operates under each level of statistical heterogeneity by each of the parameters, and further testing and validating the claims made by the initial authors of each of the aggregators. By parameter, our experiments indicated that using fewer active devices resulted in the best results regardless of IID setting. We also found that more epochs achieved better results in a high IID setting, but fewer epochs was preferable in a low IID setting, with mixed results in a moderate IID setting. Lastly, we found that best scores in a high IID setting were achieved with a smaller batch size, though in increasingly low IID settings, higher batch size was preferable. This was not true though of the CIFAR-10 dataset, which can be considered as a more complex task than the MNIST and Healthcare datasets.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para ltx_noindent">
<p id="S5.SS5.p2.1" class="ltx_p">Of the four FL aggregators, we observed that in the high IID setting no single aggregator regularly achieved the best F1 scores. FedAvg was consistently a strong performer in this setting, though not the best. Moving towards settings of greater statistical heterogeneity, FedAvg often achieved the best scores, which challenges some of the claims of the other aggregators that they would outperform FedAvg in low IID settings. SCAFFOLD emerged as a reliable alternative particularly in the low IID setting, particularly after removing outliers and volatility from CIFAR-10. Our study draws similar conclusions to Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, who also found that state of the art aggregators did not regularly outperform FedAvg, though they cautioned against the use of SCAFFOLD for it is increased communication cost, since the size of the model parameters that need to be communicated are doubled through the control variates.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para ltx_noindent">
<p id="S5.SS5.p3.1" class="ltx_p">As shown from the results, each aggregator has its strengths and weaknesses depending on the distribution of the dataset and the level of IID. This could be attributed to the nature of each aggregator and the way in which it handles statistical heterogeneity. Additionally, the performance of each aggregator depends on the specific characteristics and distribution of the dataset. The performance difference can also be affected by factors such as the data quantity variance and the robustness of the aggregator to changes in the parameter values, as described in the previous subsections. Based on the results from experimenting with different parameters and aggregators under high, medium and low levels of IID across MNIST, CIFAR-10 and Healthcare datasets, we are able to provide the following recommendations as indicative guidelines for future research studies of similar nature in FigureÂ <a href="#S5.F17" title="Figure 17 â€£ 5.5 Summary â€£ 5 Discussion â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para ltx_noindent">
<p id="S5.SS5.p4.1" class="ltx_p">The scenario below demonstrates the process of determining the optimal parameters and settings to use based on the flow chart: the EMD heat maps shown in Figures 4 and 5 can be replicated by researchers to determine the EMD by the required number of class distributions and data quantity variance. Following this, they can classify the level of IID of their datasets of interest using the EMD score by referring to TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Once the level of IID is determined, a researcher can then refer to the flow chart in FigureÂ <a href="#S5.F17" title="Figure 17 â€£ 5.5 Summary â€£ 5 Discussion â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> to select the optimal parameters that maximises model performance. For example, if an experiment requires using a 10-class, large sized dataset similar to MNIST and distributing 5 classes to the local devices, the IID level can be determined to be moderate, based on TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Taking these into consideration and using the flow chart in FigureÂ <a href="#S5.F17" title="Figure 17 â€£ 5.5 Summary â€£ 5 Discussion â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, we recommend using FedAvg, 1 epoch, 20% of total local devices in global aggregation and a high batch size in the range of 50 to 100 to train the model. Rather than having to run multiple trials to find the suitable training condition that optimises model performance, the researcher can be benefited by having an evidence-based recommended setting as the starting point.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Limitations and Future Works</h3>

<div id="S5.SS6.p1" class="ltx_para ltx_noindent">
<p id="S5.SS6.p1.1" class="ltx_p">We set out to explore the effect of data skewness on overall FL results by controlling label skew and data quantity skew. Through the preliminary experiments conducted to quantify overall skewness using EMD, it was brought to our attention that although label skew had a significant and consistent effect on the EMD metric, the data quantity skewness did not. Such results can be observed in the EMD heatmaps (FiguresÂ <a href="#S3.F4" title="Figure 4 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> andÂ <a href="#S3.F5" title="Figure 5 â€£ 3.2.3 Quantifying Overall Data Skew â€£ 3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), with no distinguishable relationship between data quantity skewness and the EMD score. Therefore, we made a decision to only use label distribution as the determinator for data IID in our primary experiments. An explanation for the behaviour of the data quantity variance could exist in its relationship with the FL training methodology. One of the key features in our FL framework is that devices are selected at random at the beginning of each communication round, therefore this selection may potentially have a large impact on the overall degree of data skewness. For example, in an extremely low IID setting (for example, data quantity variance is 90%), where a smaller proportion of active devices are selected (for instance, 10% of all devices), it is possible that the selected devices do not all exhibit low IID data quantity settings, hence the training results may be similar or even exceed that of higher IID settings in any given communication round.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para ltx_noindent">
<p id="S5.SS6.p2.1" class="ltx_p">An improvement to this measure could potentially explore limiting the data quantity variance within specified ranges for each mode of IID (for example, low IID falls within a range of 70%-90%). This would ensure that the active devices are all within their skewness range regardless of the random active device selection. Due to the exclusion of the data quantity variance in our experiments, a proposal for future work may be to explore the relationship between data quantity skewness and FL aggregator performance while keeping the label skew constant. These experiments would enable the complete analysis of the selected FL aggregators across different modes of skewness to determine their overall usability and effectiveness.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para ltx_noindent">
<p id="S5.SS6.p3.1" class="ltx_p">An evaluation limitation we found through our study was the lack of standard benchmarks for MNIST and CIFAR-10 FL models in the existing literature. Therefore, it was difficult to assess how our results compare to other combinations of parameters that have been reported. Related to this, we see future research benefitting from focussing on a wider range of datasets with different size and label imbalances, as we attempted to do by including the Healthcare dataset in our analysis. This would enable more robust validation of findings and generalisable conclusions to be formed. Another potential future direction involves the development of standardised benchmarks within the field of federated learning. This initiative aims to provide a consistent basis for future research, facilitating fair and meaningful comparisons of performance across different studies.
Lastly, most studies, including ours, optimise accuracy when evaluating model performance. An improvement to our study would be to repeat the experiments with multiple metrics in addition to accuracy, such as stability and communication efficiency, which could provide useful insights for a practical setting.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Our study has two main deliverables that we believe will add to the existing FL knowledge base. The first is our approach to determining the level of IID of a dataset, which has been extensively covered in SectionÂ <a href="#S3.SS2" title="3.2 Data Partitioning â€£ 3 Methodologies â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. The second is our recommended parameter settings for FL models under different levels of IID, which is also one of the research objectives for this study. We have achieved this by providing the flow chart in FigureÂ <a href="#S5.F17" title="Figure 17 â€£ 5.5 Summary â€£ 5 Discussion â€£ Optimisation of Federated Learning Settings under Statistical Heterogeneity Variations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> to generalise our experimental results, which acts as a guidance for researchers who may face similar challenges in the future.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">Our proposed data partitioning approach tells which portion of the data is allocated to each device in a federated learning system, particularly to simulate statistical heterogeneity so that testing the robustness and performance of an aggregation algorithm can be tested better. Our proposed approach supports federated learning to ensure privacy concerns by keeping the raw data locally on each device, and only sharing model updates with the central server, thus preserving data privacy and adhering to the principles of federated learning.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">It is important to note that these recommendations are subject to review and validation by further research, as our study did not perform exhaustive experiments on all types of dataset possible nor with a large variety of datasets in each category. Despite this, we believe that other researchers will find our preliminary results beneficial and informative when designing their research, especially when it comes to the selection of parameters and aggregators that optimise FL model performance.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">We would like to thank Maitreyee Satpathy, Rebecca Grace Johnston, and Jack Zi Jie Ye for their help and fruitful discussions for this study.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jie Xu, BenjaminÂ S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei Wang.

</span>
<span class="ltx_bibblock">Federated learning for healthcare informatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">J. of Healthcare Informatics Research</span>, 5(1):1â€“19, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
SÃ¡ndor Beniczky, Philippa Karoly, Ewan Nurse, Philippe Ryvlin, and Mark Cook.

</span>
<span class="ltx_bibblock">Machine learning and wearable devices of the future.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Epilepsia</span>, 62:S116â€“S124, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Sawsan Abdulrahman, Hanine Tout, Hakima Ould-Slimane, Azzam Mourad, Chamseddine Talhi, and Mohsen Guizani.

</span>
<span class="ltx_bibblock">A survey on federated learning: The journey from centralized to distributed on-site learning and beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE IoTs Journal</span>, 8(7):5476â€“5497, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, RezaÂ M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali Dehghantanha, and Gautam Srivastava.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 115:619â€“640, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Moumita Patra, Rahul Thakur, and CÂ SivaÂ Ram Murthy.

</span>
<span class="ltx_bibblock">Improving delay and energy efficiency of vehicular networks using mobile femto access points.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Vehicular Technology</span>, 66(2):1496â€“1505, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hangyu Zhu, Haoyu Zhang, and Yaochu Jin.

</span>
<span class="ltx_bibblock">From federated learning to federated neural architecture search: a survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Complex &amp; Intelligent Systems</span>, 7(2):639â€“657, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Muhammad Asad, Ahmed Moustafa, and Takayuki Ito.

</span>
<span class="ltx_bibblock">Federated learning versus classical machine learning: A convergence comparison.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Authorea Preprints</span>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and BlaiseÂ Aguera yÂ Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">AI and Statistics</span>, pages 1273â€“1282. PMLR, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tian Li, AnitÂ Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50â€“60, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv:1806.00582</span>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 465:371â€“390, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Mang Ye, Xiuwen Fang, BoÂ Du, PongÂ C Yuen, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Heterogeneous federated learning: State-of-the-art and research challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Liangqiong Qu, Yuyin Zhou, PaulÂ Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, LiÂ Fei-Fei, and Daniel Rubin.

</span>
<span class="ltx_bibblock">Rethinking architecture design for tackling data heterogeneity in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 10061â€“10071, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hai Phan and Anh Nguyen.

</span>
<span class="ltx_bibblock">Deepface-emd: Re-ranking using patch-wise earth moverâ€™s distance improves out-of-distribution face identification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 20259â€“20269, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jiahua Ma, Xinghua Sun, Wenchao Xia, Xijun Wang, Xiang Chen, and Hongbo Zhu.

</span>
<span class="ltx_bibblock">Client selection based on label quantity information for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2021 IEEE 32nd Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)</span>, pages 1â€“6. IEEE, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tzu-MingÂ Harry Hsu, Hang Qi, and Matthew Brown.

</span>
<span class="ltx_bibblock">Measuring the effects of non-identical data distribution for federated visual classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv:1909.06335</span>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Saeed Vahidian, Mahdi Morafah, Mubarak Shah, and Bill Lin.

</span>
<span class="ltx_bibblock">Rethinking data heterogeneity in federated learning: Introducing a new notion and standard benchmarks.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Artificial Intelligence</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv:2102.02079</span>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tian Li, AnitÂ Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine learning and systems</span>, 2:429â€“450, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Zhijin Qin, GeoffreyÂ Ye Li, and Hao Ye.

</span>
<span class="ltx_bibblock">Federated learning and wireless communications.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, 28(5):134â€“140, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
SaiÂ Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, SashankÂ J. Reddi, SebastianÂ U. Stich, and AnandaÂ Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for on-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1910.06378, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jie Zhang, Zhiqi Li, BoÂ Li, Jianghe Xu, Shuang Wu, Shouhong Ding, and Chao Wu.

</span>
<span class="ltx_bibblock">Federated learning with label distribution skew via logits calibration.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 26311â€“26329. PMLR, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
ManojÂ Ghuhan Arivazhagan, Vinay Aggarwal, AadityaÂ Kumar Singh, and Sunav Choudhary.

</span>
<span class="ltx_bibblock">Federated learning with personalization layers.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv:1912.00818</span>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
FabioÂ Mendoza Palechor and Alexis deÂ la HozÂ Manotas.

</span>
<span class="ltx_bibblock">Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from colombia, peru and mexico.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Data in brief</span>, 25:104344, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
YiÂ Liu, Jialiang Peng, Jiawen Kang, AbdullahÂ M Iliyasu, Dusit Niyato, and AhmedÂ A Abd El-Latif.

</span>
<span class="ltx_bibblock">A secure federated learning framework for 5g networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, 27(4):24â€“31, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hao Wang, Zakhary Kaplan, DiÂ Niu, and Baochun Li.

</span>
<span class="ltx_bibblock">Optimizing federated learning on non-iid data with reinforcement learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Conf. on Computer Communications</span>, pages 1698â€“1707. IEEE, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
YaeÂ Jee Cho, Jianyu Wang, and Gauri Joshi.

</span>
<span class="ltx_bibblock">Client selection in federated learning: Convergence analysis and power-of-choice selection strategies.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv:2010.01243</span>, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel.

</span>
<span class="ltx_bibblock">Handwritten digit recognition with a back-propagation network.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Adv. in neural information processing systems</span>, 2, 1989.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.

</span>
<span class="ltx_bibblock">Mobilenetv2: Inverted residuals and linear bottlenecks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">IEEE conf. on computer vision and pattern recognition</span>, pages 4510â€“4520, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
RoelofÂ K Brouwer.

</span>
<span class="ltx_bibblock">A feed-forward network for input that is both categorical and quantitative.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 15(7):881â€“890, 2002.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
GeorgeÂ E Dahl, TaraÂ N Sainath, and GeoffreyÂ E Hinton.

</span>
<span class="ltx_bibblock">Improving deep neural networks for lvcsr using rectified linear units and dropout.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">2013 IEEE international conference on acoustics, speech and signal processing</span>, pages 8609â€“8613. IEEE, 2013.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv:2002.06440</span>, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Yihan Jiang, Jakub KoneÄná»³, Keith Rush, and Sreeram Kannan.

</span>
<span class="ltx_bibblock">Improving federated learning personalization via model agnostic meta learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">arXiv:1909.12488</span>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Fengxiang He, Tongliang Liu, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Control batch size and learning rate to generalize well: Theoretical and empirical evidence.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 32:1143â€“1152, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Vukasin Felbab, PÃ©ter Kiss, and TomÃ¡s HorvÃ¡th.

</span>
<span class="ltx_bibblock">Optimization in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">ITAT</span>, pages 58â€“65, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Reza Nasirigerdeh, Mohammad Bakhtiari, Reihaneh Torkzadehmahani, Amirhossein Bayat, Markus List, DavidÂ B Blumenthal, and Jan Baumbach.

</span>
<span class="ltx_bibblock">Federated multi-mini-batch: An efficient training approach to federated learning in non-iid environments.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv:2011.07006</span>, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Jonatan Reyes, Lisa DiÂ Jorio, Cecile Low-Kam, and Marta Kersten-Oertel.

</span>
<span class="ltx_bibblock">Precision-weighted federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv:2107.09627</span>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.06339" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.06340" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.06340">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.06340" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.06341" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:11:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
