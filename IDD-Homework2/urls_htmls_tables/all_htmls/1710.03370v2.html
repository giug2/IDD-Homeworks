<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1710.03370] iVQA: Inverse Visual Question Answering</title><meta property="og:description" content="We propose the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given ima…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="iVQA: Inverse Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="iVQA: Inverse Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1710.03370">

<!--Generated on Sat Mar 16 04:11:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">iVQA: Inverse Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Feng Liu<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>  Tao Xiang<sup id="id10.10.id2" class="ltx_sup">2</sup>  Timothy M. Hospedales<sup id="id11.11.id3" class="ltx_sup">3</sup>  Wankou Yang<sup id="id12.12.id4" class="ltx_sup">1</sup>  Changyin Sun<sup id="id13.13.id5" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id14.14.id6" class="ltx_sup">1</sup>Southeast University, China

<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup">2</sup>Queen Mary University of London, UK  <sup id="id16.16.id8" class="ltx_sup">3</sup>University of Edinburgh, UK 
<br class="ltx_break"><span id="id17.17.id9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{liufeng,wkyang,cysun}@seu.edu.cn, t.xiang@qmul.ac.uk, t.hospedales@ed.ac.uk</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">We propose the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given image and answer pair. Since the answers are less informative than the questions, and the questions have less learnable bias, an iVQA model needs to better understand the image to be successful than a VQA model. We pose question generation as a multi-modal dynamic inference process and propose an iVQA model that can gradually adjust its focus of attention guided by both a partially generated question and the answer. For evaluation, apart from existing linguistic metrics, we propose a new ranking metric.
This metric compares the ground truth question’s rank among a list of distractors, which allows the drawbacks of different algorithms and sources of error to be studied. Experimental results show that our model can generate diverse, grammatically correct and content correlated questions that match the given answer.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As conventional object detection and recognition approach solved problems, we see a surge of interest in more challenging problems that should require greater ‘understanding’ from computer vision systems. Image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, natural language object retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and ‘visual Turing tests’ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> provide multi-modal AI challenges that are expected to require rich visual and linguistic understanding, as well as knowledge representation and reasoning capabilities. As interest in these grand challenges has grown, so has scrutiny of the benchmarks and models that appear to solve them. Are we making progress towards these challenges, or are good results the latest incarnation of horses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and Potemkin villages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, with neural networks finding unexpected correlates that provide shortcuts to give away the answer?</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent analyses of VQA models and benchmarks have found that the reported VQA success is largely due to making predictions from dataset biases and cues given away in the question, with predictions being minimally dependent on understanding image content. For example it turns out that existing VQA models do not ‘look’ in the same places as humans do to answer the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>; they do not give different answers when the same question is asked of different images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>; and they can perform well given no image at all <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Moreover, VQA model predictions do not depend on more than the first few words of the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and their success depends largely on being able to exploit label bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. These observations have motivated renewed attempts to devise more rigorous VQA benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1710.03370/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="184" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of iVQA task: Input answers and images along with the top questions generated by our model. </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we take a different approach, and explore whether the task of inverse VQA provides an interesting benchmark of multi-modal intelligence. The inverse VQA (iVQA) task is to input a pair of image and answer, and then ask (output) a suitable question for which the given answer holds in the context of the given image. We conjecture that iVQA, as illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is an interesting challenge for several reasons: (i) There may be less scope for an iVQA model to take advantage of question bias than for VQA to score highly through answer bias (there is less question bias, and exploiting it is harder than for categorical answers). (ii) The answers themselves provide a very sparse cue in iVQA compared to questions in VQA. So there may be less opportunity to deduce the question from the answer alone in iVQA than there is to deduce the answer from the question alone in VQA. Thus the iVQA task relies more heavily on understanding image content. (iii) From a knowledge representation and reasoning perspective, iVQA may provide the opportunity to test more complex inference strategies such as counterfactual reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although closely related to VQA, existing VQA models do not provide a solution to the iVQA problem. This is because much less information can be inferred from an answer than from a question. In addition, although an answer is often short consisting of a phrase or even a single word, an iVQA model-generated question is a complete sentence composed of a long sequence of words. The key to effective iVQA is thus to attend selectively and dynamically to different regions of the image as the model progresses to generate the next word. This dynamic attention mechanism has to be conditioned on both the answer and the partial sentence generated so far. To this end, a novel dynamic multi-modal attention-based iVQA model is proposed which is capable of generating diverse, grammatically correct and content correlated questions that match the given answer.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Prior evaluations of question generation methods mainly use standard machine translation metrics, <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p5.1.2" class="ltx_text"></span>, BLEU, METEOR, <em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">etc</em>. These automatic metrics are correlated with human judgements for question generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. However they provide limited power to <em id="S1.p5.1.4" class="ltx_emph ltx_font_italic">diagnose</em> question generation models in terms of when and why they succeed or fail. In this paper, we first propose an alternative and complementary ranking-based evaluation metric which is based on ranking the ground truth question among alternative distractors using an iVQA model, given the image and the answer.
By controlling the types of distractors presented when using this metric, we can better understand the successes and failures of different models. Second, we perform a human study which is robust to iVQA’s one-to-many nature (multiple possible questions can have the same answer). Reassuringly our human study scores turn out to be highly correlated to our proposed new ranking metric.
</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The contributions of this paper are as follows: (1) The novel iVQA problem is introduced as an alternative challenge for high-level multi-modal visuo-linguistic understanding. (2) We propose a multi-modal dynamic attention based iVQA model. (3) We propose a question ranking based evaluation methodology for iVQA that is helpful to diagnose the strengths and weaknesses of different models. (4) As the dual problem of VQA, we show that iVQA has the potential to help improve VQA performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Image captioning</span> Image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> aims to describe, rather than merely recognise objects in images. It encompasses a number of classic vision capabilities as prerequisites including object <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and action <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> recognition, attribute description <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and relationship inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. It further requires natural language generation capabilities to synthesise open-ended linguistic descriptions. Popular benchmarks and competitions have inspired intensive research in this area. Captioning models have explicitly addressed these sub-tasks to varying degrees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, but the most common and successful approaches use neural encoders (of images), and decoders (of captions), with little explicit knowledge representation and reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The iVQA task investigated here is related to captioning in that we aim to produce natural language outputs, but distinct in that the outputs are sharply conditioned on the required answer, as illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">VQA Challenge</span> Like captioning, VQA has gained attention as a synthesis challenge in AI, requiring both computer vision and natural language understanding to succeed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Based on an image, and a natural language question about the image, a VQA system produces an answer. Unlike other vision tasks (recognition, detection, description), the question to be answered in VQA is dynamically specified at runtime. Besides visuo-linguistic grounding, many VQA examples seem to require extra information not contained in the question or image, <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.3" class="ltx_text"></span>, background common sense about the world. Thus VQA is hoped to provide a long term goal for AI-complete multi-modal intelligence. However increasing scrutiny has shown that learning systems excel at finding shortcuts in terms of gaming the biases in answer distributions, and giveaway correlations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, leading to doubts about the level of visuo-linguistic intelligence implied by current results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Although some benchmarks in principle require open-sentence answers, most answers are simple one-word outputs, and therefore the most common approach has been to formalise answer generation as a multi-class classification problem over the most frequent answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Although successful, this is somewhat unsatisfactory as it is no longer an open-world challenge. In this paper we explore a novel iVQA task as an alternative open-world benchmark for visuo-linguistic understanding.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">VQA Models</span> Existing VQA models are commonly based on two-branch neural networks, each consisting of a CNN image encoder, a LSTM question encoder which are merged before feeding to an answer decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Recently they have been enhanced through various mechanisms including better visuo-linguistic merging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, varying degrees of explicit representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, reasoning with external knowledge bases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and improving visual encoding through attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. With most recent models treating answer generation as a classification problem, these models cannot be directly modified for iVQA by simply swapping the answer and question encoder/decoder. The proposed iVQA model is a marriage between captioning and VQA models but with a dynamic and multi-modal attention mechanism developed specificaly for iVQA.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Related Challenges</span> Our proposed challenge is related to emerging task of <span id="S2.p4.1.2" class="ltx_text ltx_font_italic" style="color:#000000;">visual question generation</span> (VQG): to generate a natural question about the content of an image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, VQG is further studied in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> where DenseCap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is used to generate region specific descriptions before being translated into questions. VQG is a pre-specified task unlike VQA and iVQA which are dynamically determined at runtime. Importantly, VQG is easier in terms of required understanding. Since VQG is not required to be answer-conditional, it often generates very general open questions that even humans cannot answer. It does not need to understand the image clearly enough – and ground the two domains richly enough – to correctly condition the generated question on the answer. Another relevant challenge is <span id="S2.p4.1.3" class="ltx_text ltx_font_italic" style="color:#000000;">visually grounded conversation</span> (VGC) which aims to generate natural-sounding conversations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. VGC typically starts with VQG, but the following responses and further questions are generated primarily following conversational patterns mined from social media text, and only loosely grounded on the context of image content. In contrast, the image content grounding is much tighter in iVQA.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/1710.03370/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="275" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall architecture of the proposed iVQA model</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">The problem of inverse visual question answering (iVQA) is to infer a question <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">q</annotation></semantics></math> for which a given answer <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">a</annotation></semantics></math> holds, in the context of a particular image <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">I</annotation></semantics></math>. Formally:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="q^{*}=\max_{q}p(q|I,a;\Theta)," display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msup id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.2.cmml">q</mi><mo id="S3.E1.m1.4.4.1.1.3.3" xref="S3.E1.m1.4.4.1.1.3.3.cmml">∗</mo></msup><mo id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml"><munder id="S3.E1.m1.4.4.1.1.1.3.1" xref="S3.E1.m1.4.4.1.1.1.3.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.3.1.2" xref="S3.E1.m1.4.4.1.1.1.3.1.2.cmml">max</mi><mi id="S3.E1.m1.4.4.1.1.1.3.1.3" xref="S3.E1.m1.4.4.1.1.1.3.1.3.cmml">q</mi></munder><mo lspace="0.167em" id="S3.E1.m1.4.4.1.1.1.3a" xref="S3.E1.m1.4.4.1.1.1.3.cmml">⁡</mo><mi id="S3.E1.m1.4.4.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.cmml">q</mi><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">a</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">;</mo><mi mathvariant="normal" id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></eq><apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3">superscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2">𝑞</ci><times id="S3.E1.m1.4.4.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.3"></times></apply><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2"></times><apply id="S3.E1.m1.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3"><apply id="S3.E1.m1.4.4.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.3.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1">subscript</csymbol><max id="S3.E1.m1.4.4.1.1.1.3.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.2"></max><ci id="S3.E1.m1.4.4.1.1.1.3.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.3">𝑞</ci></apply><ci id="S3.E1.m1.4.4.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2">𝑞</ci><list id="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐼</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑎</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">Θ</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">q^{*}=\max_{q}p(q|I,a;\Theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.6" class="ltx_p">where <math id="S3.SS1.p1.4.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p1.4.m1.1a"><mi id="S3.SS1.p1.4.m1.1.1" xref="S3.SS1.p1.4.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m1.1b"><ci id="S3.SS1.p1.4.m1.1.1.cmml" xref="S3.SS1.p1.4.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m1.1c">q</annotation></semantics></math> is a sentence with words <math id="S3.SS1.p1.5.m2.4" class="ltx_Math" alttext="(w_{1},w_{2},...,w_{n})" display="inline"><semantics id="S3.SS1.p1.5.m2.4a"><mrow id="S3.SS1.p1.5.m2.4.4.3" xref="S3.SS1.p1.5.m2.4.4.4.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m2.4.4.3.4" xref="S3.SS1.p1.5.m2.4.4.4.cmml">(</mo><msub id="S3.SS1.p1.5.m2.2.2.1.1" xref="S3.SS1.p1.5.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.5.m2.2.2.1.1.2" xref="S3.SS1.p1.5.m2.2.2.1.1.2.cmml">w</mi><mn id="S3.SS1.p1.5.m2.2.2.1.1.3" xref="S3.SS1.p1.5.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.5.m2.4.4.3.5" xref="S3.SS1.p1.5.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.5.m2.3.3.2.2" xref="S3.SS1.p1.5.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.5.m2.3.3.2.2.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.cmml">w</mi><mn id="S3.SS1.p1.5.m2.3.3.2.2.3" xref="S3.SS1.p1.5.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.5.m2.4.4.3.6" xref="S3.SS1.p1.5.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.5.m2.1.1" xref="S3.SS1.p1.5.m2.1.1.cmml">…</mi><mo id="S3.SS1.p1.5.m2.4.4.3.7" xref="S3.SS1.p1.5.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.5.m2.4.4.3.3" xref="S3.SS1.p1.5.m2.4.4.3.3.cmml"><mi id="S3.SS1.p1.5.m2.4.4.3.3.2" xref="S3.SS1.p1.5.m2.4.4.3.3.2.cmml">w</mi><mi id="S3.SS1.p1.5.m2.4.4.3.3.3" xref="S3.SS1.p1.5.m2.4.4.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS1.p1.5.m2.4.4.3.8" xref="S3.SS1.p1.5.m2.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m2.4b"><vector id="S3.SS1.p1.5.m2.4.4.4.cmml" xref="S3.SS1.p1.5.m2.4.4.3"><apply id="S3.SS1.p1.5.m2.2.2.1.1.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.2">𝑤</ci><cn type="integer" id="S3.SS1.p1.5.m2.2.2.1.1.3.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.5.m2.3.3.2.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.5.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2">𝑤</ci><cn type="integer" id="S3.SS1.p1.5.m2.3.3.2.2.3.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.5.m2.1.1.cmml" xref="S3.SS1.p1.5.m2.1.1">…</ci><apply id="S3.SS1.p1.5.m2.4.4.3.3.cmml" xref="S3.SS1.p1.5.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.4.4.3.3.1.cmml" xref="S3.SS1.p1.5.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.5.m2.4.4.3.3.2.cmml" xref="S3.SS1.p1.5.m2.4.4.3.3.2">𝑤</ci><ci id="S3.SS1.p1.5.m2.4.4.3.3.3.cmml" xref="S3.SS1.p1.5.m2.4.4.3.3.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m2.4c">(w_{1},w_{2},...,w_{n})</annotation></semantics></math> and <math id="S3.SS1.p1.6.m3.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S3.SS1.p1.6.m3.1a"><mi mathvariant="normal" id="S3.SS1.p1.6.m3.1.1" xref="S3.SS1.p1.6.m3.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m3.1b"><ci id="S3.SS1.p1.6.m3.1.1.cmml" xref="S3.SS1.p1.6.m3.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m3.1c">\Theta</annotation></semantics></math> is the model parameters. As a sequence generation problem, we can use a recurrent neural network language model, to generate the sentence by maximising the likelihood:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="q^{*}=\max_{q}\prod_{t}p(w_{t}|w_{t-1},...,w_{1},I,a)." display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><msup id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml">q</mi><mo id="S3.E2.m1.4.4.1.1.3.3" xref="S3.E2.m1.4.4.1.1.3.3.cmml">∗</mo></msup><mo id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml"><munder id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.3.2.cmml">max</mi><mi id="S3.E2.m1.4.4.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.3.3.cmml">q</mi></munder><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.4.4.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.cmml"><munder id="S3.E2.m1.4.4.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.4.4.1.1.1.1.2.2" xref="S3.E2.m1.4.4.1.1.1.1.2.2.cmml">∏</mo><mi id="S3.E2.m1.4.4.1.1.1.1.2.3" xref="S3.E2.m1.4.4.1.1.1.1.2.3.cmml">t</mi></munder><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2.cmml">w</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3.cmml">t</mi></msub><mo fence="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">w</mi><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">…</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.4" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2.cmml">w</mi><mn id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.5" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">I</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.6" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">a</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"></eq><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3">superscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2">𝑞</ci><times id="S3.E2.m1.4.4.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3"></times></apply><apply id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.3">subscript</csymbol><max id="S3.E2.m1.4.4.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.3.2"></max><ci id="S3.E2.m1.4.4.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3">𝑞</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1"><apply id="S3.E2.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.2">product</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2"></times><ci id="S3.E2.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.3">𝑝</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2">𝑤</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3">𝑡</ci></apply><list id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2">𝑤</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">…</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2">𝑤</ci><cn type="integer" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐼</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑎</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">q^{*}=\max_{q}\prod_{t}p(w_{t}|w_{t-1},...,w_{1},I,a).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.8" class="ltx_p">Since the task is conditioned on both image <math id="S3.SS1.p1.7.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.7.m1.1a"><mi id="S3.SS1.p1.7.m1.1.1" xref="S3.SS1.p1.7.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m1.1b"><ci id="S3.SS1.p1.7.m1.1.1.cmml" xref="S3.SS1.p1.7.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m1.1c">I</annotation></semantics></math> and answer <math id="S3.SS1.p1.8.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p1.8.m2.1a"><mi id="S3.SS1.p1.8.m2.1.1" xref="S3.SS1.p1.8.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m2.1b"><ci id="S3.SS1.p1.8.m2.1.1.cmml" xref="S3.SS1.p1.8.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m2.1c">a</annotation></semantics></math>, the visual information has to be integrated with the answers appropriately to generate questions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model overview</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The architecture of our iVQA model is shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Methodology ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It is a deep neural network with three subnets: an image encoder, an answer encoder, and a question decoder. The two encoders provide inputs for the decoder to generate a sentence which fits to the conditioned answer and image content. A multi-modal attention module (detailed later) is also a key component that directs image attention dynamically given the outputs of both encoders and a partial question encoder. We first describe the three subnets.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p">The image encoder is a CNN that generates a feature representation of the image. Both global and local features are exploited for image representation. The <code id="S3.SS2.p2.6.1" class="ltx_verbatim ltx_font_typewriter">res5c</code> features computed using the ResNet-152 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> are utilised as local features. More specifically, The local feature collection <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\bm{I}=\{\bm{v}_{ij}\}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">𝑰</mi><mo id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.1.m1.1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.2.cmml">𝒗</mi><mrow id="S3.SS2.p2.1.m1.1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"></eq><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑰</ci><set id="S3.SS2.p2.1.m1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1"><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.2">𝒗</ci><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3"><times id="S3.SS2.p2.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\bm{I}=\{\bm{v}_{ij}\}</annotation></semantics></math> is defined as local feature <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\bm{v}_{ij}\in\mathbb{R}^{2048}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><msub id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">𝒗</mi><mrow id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.3.2" xref="S3.SS2.p2.2.m2.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.1" xref="S3.SS2.p2.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.3" xref="S3.SS2.p2.2.m2.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><in id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></in><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">𝒗</ci><apply id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3"><times id="S3.SS2.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2">𝑖</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\bm{v}_{ij}\in\mathbb{R}^{2048}</annotation></semantics></math> over all <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="14\times 14" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mn id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">14</cn><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">14\times 14</annotation></semantics></math> spatial locations. To extract the local features, we resize the image to <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="448\times 448" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">448</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><cn type="integer" id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">448</cn><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">448\times 448</annotation></semantics></math> before feeding it to the feature extractor as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. As for the global feature, the semantic concept <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> feature <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\bm{I}_{s}\in\mathbb{R}^{1000}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><msub id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">𝑰</mi><mi id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml">1000</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><in id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></in><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">𝑰</ci><ci id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">𝑠</ci></apply><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">1000</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\bm{I}_{s}\in\mathbb{R}^{1000}</annotation></semantics></math> is used. These 1,000 semantic concepts are mined from the most frequents words in a set of image captions. A concept classifier is learned to predict <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\bm{I}_{s}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">𝑰</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑰</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\bm{I}_{s}</annotation></semantics></math> as classification scores for the concepts.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For the answer encoder, a long-short memory (LSTM) network with 512 cells <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> is used, and the concatenation of the final hidden state and cell state provides the answer representation <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\bm{a}\in\mathbb{R}^{1024}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">𝒂</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">1024</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><in id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></in><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝒂</ci><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">1024</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\bm{a}\in\mathbb{R}^{1024}</annotation></semantics></math>.
With the described CNN image encoder and LSTM based answer encoder as input, a LSTM question decoder can be used to generate questions conditioned on both the images and answer. The detailed decoding processing together with the proposed attention module will be detailed next.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dynamic multi-modal attention</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">Given the sparse information contained in the answer, having an effective attention model to focus on the right region of the image is critical for iVQA. Attention models have been widely studied in image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. However, our iVQA problem has some unique characteristics, and thus needs a tailor-made attention module. Specifically, compared with VQA, iVQA requires multiple decoding steps and the focus of attention therefore needs to be dynamically changed accordingly. Also unlike image captioning, the generation process has multi-modal conditions: <em id="S3.SS3.p1.6.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS3.p1.6.2" class="ltx_text"></span>, image and answer, both of which need to be integrated in every decoding step in a dynamic manner. Consider the following question-answer pair: “<em id="S3.SS3.p1.6.3" class="ltx_emph ltx_font_italic">Q: What colour is the dress the girl is wearing?</em>”; “<em id="S3.SS3.p1.6.4" class="ltx_emph ltx_font_italic">A: Pink</em>”. Given the answer, the model can infer that the question is about colour. After the model has predicted type specific partial question <math id="S3.SS3.p1.1.m1.1" class="ltx_math_unparsed" alttext="q_{t}=\{" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1b"><msub id="S3.SS3.p1.1.m1.1.1"><mi id="S3.SS3.p1.1.m1.1.1.2">q</mi><mi id="S3.SS3.p1.1.m1.1.1.3">t</mi></msub><mo id="S3.SS3.p1.1.m1.1.2">=</mo><mo stretchy="false" id="S3.SS3.p1.1.m1.1.3">{</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">q_{t}=\{</annotation></semantics></math><em id="S3.SS3.p1.6.5" class="ltx_emph ltx_font_italic">what, colour, is, the</em><math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mo stretchy="false" id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\}</annotation></semantics></math>, the attention network will integrate the partial question <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="q_{t}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">q</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝑞</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">q_{t}</annotation></semantics></math> with the answer <math id="S3.SS3.p1.4.m4.1" class="ltx_math_unparsed" alttext="a=\{" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1b"><mi id="S3.SS3.p1.4.m4.1.1">a</mi><mo id="S3.SS3.p1.4.m4.1.2">=</mo><mo stretchy="false" id="S3.SS3.p1.4.m4.1.3">{</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">a=\{</annotation></semantics></math><em id="S3.SS3.p1.6.6" class="ltx_emph ltx_font_italic">pink</em><math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mo stretchy="false" id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\}</annotation></semantics></math>, and search for all objects with the pink colour and output attended features. Based on these attended features the next word <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="w_{t+1}" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><msub id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">w</mi><mrow id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml"><mi id="S3.SS3.p1.6.m6.1.1.3.2" xref="S3.SS3.p1.6.m6.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p1.6.m6.1.1.3.1" xref="S3.SS3.p1.6.m6.1.1.3.1.cmml">+</mo><mn id="S3.SS3.p1.6.m6.1.1.3.3" xref="S3.SS3.p1.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">𝑤</ci><apply id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3"><plus id="S3.SS3.p1.6.m6.1.1.3.1.cmml" xref="S3.SS3.p1.6.m6.1.1.3.1"></plus><ci id="S3.SS3.p1.6.m6.1.1.3.2.cmml" xref="S3.SS3.p1.6.m6.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS3.p1.6.m6.1.1.3.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">w_{t+1}</annotation></semantics></math> is predicted.
Motivated by these unique characteristics, we propose an attention model that can perform inference based on image, partial question and answer jointly and dynamically. It is composed of the the following sub-modules:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.2" class="ltx_p"><span id="S3.SS3.p2.2.1" class="ltx_text ltx_font_bold">Initial glimpse</span> The initial glimpse should provide an overview cue of the input image-answer pair, to establish a good starting point for the decoding process. We use semantic concept prediction <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\bm{I}_{s}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">𝑰</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑰</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\bm{I}_{s}</annotation></semantics></math> as a global visual cue, which captures 1-gram information that may be relevant to the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The encoded answer <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\bm{a}</annotation></semantics></math> is taken as the textual cue, which determines the set of likely initial words of the target question. The two cues are integrated as</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\bm{h}_{0}=\delta(\bm{W}_{ih}\bm{I}_{s}+\bm{W}_{ah}\bm{a})," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml">𝒉</mi><mn id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml">0</mn></msub><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝑾</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝑰</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">s</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝑾</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.3.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">𝒂</mi></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2">𝒉</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3">0</cn></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">𝛿</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2">𝑾</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.3">ℎ</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2">𝑰</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3">𝑠</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2">𝑾</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.2">𝑎</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.3">ℎ</ci></apply></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3">𝒂</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\bm{h}_{0}=\delta(\bm{W}_{ih}\bm{I}_{s}+\bm{W}_{ah}\bm{a}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.5" class="ltx_p">where <math id="S3.SS3.p2.3.m1.1" class="ltx_Math" alttext="\bm{W}_{ih}" display="inline"><semantics id="S3.SS3.p2.3.m1.1a"><msub id="S3.SS3.p2.3.m1.1.1" xref="S3.SS3.p2.3.m1.1.1.cmml"><mi id="S3.SS3.p2.3.m1.1.1.2" xref="S3.SS3.p2.3.m1.1.1.2.cmml">𝑾</mi><mrow id="S3.SS3.p2.3.m1.1.1.3" xref="S3.SS3.p2.3.m1.1.1.3.cmml"><mi id="S3.SS3.p2.3.m1.1.1.3.2" xref="S3.SS3.p2.3.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m1.1.1.3.1" xref="S3.SS3.p2.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m1.1.1.3.3" xref="S3.SS3.p2.3.m1.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m1.1b"><apply id="S3.SS3.p2.3.m1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m1.1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m1.1.1.2.cmml" xref="S3.SS3.p2.3.m1.1.1.2">𝑾</ci><apply id="S3.SS3.p2.3.m1.1.1.3.cmml" xref="S3.SS3.p2.3.m1.1.1.3"><times id="S3.SS3.p2.3.m1.1.1.3.1.cmml" xref="S3.SS3.p2.3.m1.1.1.3.1"></times><ci id="S3.SS3.p2.3.m1.1.1.3.2.cmml" xref="S3.SS3.p2.3.m1.1.1.3.2">𝑖</ci><ci id="S3.SS3.p2.3.m1.1.1.3.3.cmml" xref="S3.SS3.p2.3.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m1.1c">\bm{W}_{ih}</annotation></semantics></math> and <math id="S3.SS3.p2.4.m2.1" class="ltx_Math" alttext="\bm{W}_{ah}" display="inline"><semantics id="S3.SS3.p2.4.m2.1a"><msub id="S3.SS3.p2.4.m2.1.1" xref="S3.SS3.p2.4.m2.1.1.cmml"><mi id="S3.SS3.p2.4.m2.1.1.2" xref="S3.SS3.p2.4.m2.1.1.2.cmml">𝑾</mi><mrow id="S3.SS3.p2.4.m2.1.1.3" xref="S3.SS3.p2.4.m2.1.1.3.cmml"><mi id="S3.SS3.p2.4.m2.1.1.3.2" xref="S3.SS3.p2.4.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m2.1.1.3.1" xref="S3.SS3.p2.4.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m2.1.1.3.3" xref="S3.SS3.p2.4.m2.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m2.1b"><apply id="S3.SS3.p2.4.m2.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m2.1.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m2.1.1.2.cmml" xref="S3.SS3.p2.4.m2.1.1.2">𝑾</ci><apply id="S3.SS3.p2.4.m2.1.1.3.cmml" xref="S3.SS3.p2.4.m2.1.1.3"><times id="S3.SS3.p2.4.m2.1.1.3.1.cmml" xref="S3.SS3.p2.4.m2.1.1.3.1"></times><ci id="S3.SS3.p2.4.m2.1.1.3.2.cmml" xref="S3.SS3.p2.4.m2.1.1.3.2">𝑎</ci><ci id="S3.SS3.p2.4.m2.1.1.3.3.cmml" xref="S3.SS3.p2.4.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m2.1c">\bm{W}_{ah}</annotation></semantics></math> are embedding weights<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In all equations, we omit the bias term for simplicity.</span></span></span>, and <math id="S3.SS3.p2.5.m3.1" class="ltx_Math" alttext="\delta(\cdot)" display="inline"><semantics id="S3.SS3.p2.5.m3.1a"><mrow id="S3.SS3.p2.5.m3.1.2" xref="S3.SS3.p2.5.m3.1.2.cmml"><mi id="S3.SS3.p2.5.m3.1.2.2" xref="S3.SS3.p2.5.m3.1.2.2.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m3.1.2.1" xref="S3.SS3.p2.5.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.p2.5.m3.1.2.3.2" xref="S3.SS3.p2.5.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.5.m3.1.2.3.2.1" xref="S3.SS3.p2.5.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m3.1.1" xref="S3.SS3.p2.5.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p2.5.m3.1.2.3.2.2" xref="S3.SS3.p2.5.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m3.1b"><apply id="S3.SS3.p2.5.m3.1.2.cmml" xref="S3.SS3.p2.5.m3.1.2"><times id="S3.SS3.p2.5.m3.1.2.1.cmml" xref="S3.SS3.p2.5.m3.1.2.1"></times><ci id="S3.SS3.p2.5.m3.1.2.2.cmml" xref="S3.SS3.p2.5.m3.1.2.2">𝛿</ci><ci id="S3.SS3.p2.5.m3.1.1.cmml" xref="S3.SS3.p2.5.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m3.1c">\delta(\cdot)</annotation></semantics></math> is a tanh activation function. This joint representation is directly used as the initial memory of the decoding network.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.3" class="ltx_p"><span id="S3.SS3.p3.3.1" class="ltx_text ltx_font_bold">Encoding of partial question</span> The partial question encoder sequentially encodes the partial question generated thus far <math id="S3.SS3.p3.1.m1.4" class="ltx_Math" alttext="q_{t}=\{w_{1},w_{2},...,w_{t}\}" display="inline"><semantics id="S3.SS3.p3.1.m1.4a"><mrow id="S3.SS3.p3.1.m1.4.4" xref="S3.SS3.p3.1.m1.4.4.cmml"><msub id="S3.SS3.p3.1.m1.4.4.5" xref="S3.SS3.p3.1.m1.4.4.5.cmml"><mi mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.5.2" xref="S3.SS3.p3.1.m1.4.4.5.2.cmml">q</mi><mi mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.5.3" xref="S3.SS3.p3.1.m1.4.4.5.3.cmml">t</mi></msub><mo mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.4" xref="S3.SS3.p3.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS3.p3.1.m1.4.4.3.3" xref="S3.SS3.p3.1.m1.4.4.3.4.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS3.p3.1.m1.4.4.3.3.4" xref="S3.SS3.p3.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS3.p3.1.m1.2.2.1.1.1" xref="S3.SS3.p3.1.m1.2.2.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS3.p3.1.m1.2.2.1.1.1.2" xref="S3.SS3.p3.1.m1.2.2.1.1.1.2.cmml">w</mi><mn mathcolor="#000000" id="S3.SS3.p3.1.m1.2.2.1.1.1.3" xref="S3.SS3.p3.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.3.3.5" xref="S3.SS3.p3.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS3.p3.1.m1.3.3.2.2.2" xref="S3.SS3.p3.1.m1.3.3.2.2.2.cmml"><mi mathcolor="#000000" id="S3.SS3.p3.1.m1.3.3.2.2.2.2" xref="S3.SS3.p3.1.m1.3.3.2.2.2.2.cmml">w</mi><mn mathcolor="#000000" id="S3.SS3.p3.1.m1.3.3.2.2.2.3" xref="S3.SS3.p3.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.3.3.6" xref="S3.SS3.p3.1.m1.4.4.3.4.cmml">,</mo><mi mathcolor="#000000" mathvariant="normal" id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">…</mi><mo mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.3.3.7" xref="S3.SS3.p3.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS3.p3.1.m1.4.4.3.3.3" xref="S3.SS3.p3.1.m1.4.4.3.3.3.cmml"><mi mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.3.3.3.2" xref="S3.SS3.p3.1.m1.4.4.3.3.3.2.cmml">w</mi><mi mathcolor="#000000" id="S3.SS3.p3.1.m1.4.4.3.3.3.3" xref="S3.SS3.p3.1.m1.4.4.3.3.3.3.cmml">t</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS3.p3.1.m1.4.4.3.3.8" xref="S3.SS3.p3.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.4b"><apply id="S3.SS3.p3.1.m1.4.4.cmml" xref="S3.SS3.p3.1.m1.4.4"><eq id="S3.SS3.p3.1.m1.4.4.4.cmml" xref="S3.SS3.p3.1.m1.4.4.4"></eq><apply id="S3.SS3.p3.1.m1.4.4.5.cmml" xref="S3.SS3.p3.1.m1.4.4.5"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.4.4.5.1.cmml" xref="S3.SS3.p3.1.m1.4.4.5">subscript</csymbol><ci id="S3.SS3.p3.1.m1.4.4.5.2.cmml" xref="S3.SS3.p3.1.m1.4.4.5.2">𝑞</ci><ci id="S3.SS3.p3.1.m1.4.4.5.3.cmml" xref="S3.SS3.p3.1.m1.4.4.5.3">𝑡</ci></apply><set id="S3.SS3.p3.1.m1.4.4.3.4.cmml" xref="S3.SS3.p3.1.m1.4.4.3.3"><apply id="S3.SS3.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p3.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="S3.SS3.p3.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS3.p3.1.m1.3.3.2.2.2.cmml" xref="S3.SS3.p3.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS3.p3.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.3.3.2.2.2.2">𝑤</ci><cn type="integer" id="S3.SS3.p3.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS3.p3.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">…</ci><apply id="S3.SS3.p3.1.m1.4.4.3.3.3.cmml" xref="S3.SS3.p3.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS3.p3.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS3.p3.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS3.p3.1.m1.4.4.3.3.3.2">𝑤</ci><ci id="S3.SS3.p3.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS3.p3.1.m1.4.4.3.3.3.3">𝑡</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.4c">q_{t}=\{w_{1},w_{2},...,w_{t}\}</annotation></semantics></math> to a hidden representation <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\bm{h}_{t}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">𝒉</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝒉</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\bm{h}_{t}</annotation></semantics></math>. A LSTM network with 512 cells is used to encode the partial question to a hidden representation <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\bm{h}_{t}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">𝒉</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝒉</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\bm{h}_{t}</annotation></semantics></math> as</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.28" class="ltx_Math" alttext="\begin{split}&amp;\bm{x}_{t}=\bm{E}\bm{w}_{t},\\
&amp;\bm{h}_{t},\bm{m}_{t}=\operatorname{LSTM}(\bm{x}_{t},\bm{h}_{t-1},\bm{m}_{t-1}),\\
\end{split}" display="block"><semantics id="S3.E4.m1.28a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S3.E4.m1.28.28.3"><mtr id="S3.E4.m1.28.28.3a"><mtd id="S3.E4.m1.28.28.3b"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.28.28.3c"><mrow id="S3.E4.m1.27.27.2.26.8.8.8"><mrow id="S3.E4.m1.27.27.2.26.8.8.8.1"><msub id="S3.E4.m1.27.27.2.26.8.8.8.1.1"><mi id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">𝒙</mi><mi id="S3.E4.m1.2.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.2.1.cmml">t</mi></msub><mo id="S3.E4.m1.3.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E4.m1.27.27.2.26.8.8.8.1.2"><mi id="S3.E4.m1.4.4.4.4.4.4" xref="S3.E4.m1.4.4.4.4.4.4.cmml">𝑬</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.27.27.2.26.8.8.8.1.2.1">​</mo><msub id="S3.E4.m1.27.27.2.26.8.8.8.1.2.2"><mi id="S3.E4.m1.5.5.5.5.5.5" xref="S3.E4.m1.5.5.5.5.5.5.cmml">𝒘</mi><mi id="S3.E4.m1.6.6.6.6.6.6.1" xref="S3.E4.m1.6.6.6.6.6.6.1.cmml">t</mi></msub></mrow></mrow><mo id="S3.E4.m1.7.7.7.7.7.7">,</mo></mrow></mtd></mtr><mtr id="S3.E4.m1.28.28.3d"><mtd id="S3.E4.m1.28.28.3e"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.28.28.3f"><mrow id="S3.E4.m1.28.28.3.27.19.19.19"><mrow id="S3.E4.m1.28.28.3.27.19.19.19.1"><mrow id="S3.E4.m1.28.28.3.27.19.19.19.1.2.2"><msub id="S3.E4.m1.28.28.3.27.19.19.19.1.1.1.1"><mi id="S3.E4.m1.8.8.8.1.1.1" xref="S3.E4.m1.8.8.8.1.1.1.cmml">𝒉</mi><mi id="S3.E4.m1.9.9.9.2.2.2.1" xref="S3.E4.m1.9.9.9.2.2.2.1.cmml">t</mi></msub><mo id="S3.E4.m1.10.10.10.3.3.3">,</mo><msub id="S3.E4.m1.28.28.3.27.19.19.19.1.2.2.2"><mi id="S3.E4.m1.11.11.11.4.4.4" xref="S3.E4.m1.11.11.11.4.4.4.cmml">𝒎</mi><mi id="S3.E4.m1.12.12.12.5.5.5.1" xref="S3.E4.m1.12.12.12.5.5.5.1.cmml">t</mi></msub></mrow><mo id="S3.E4.m1.13.13.13.6.6.6" xref="S3.E4.m1.13.13.13.6.6.6.cmml">=</mo><mrow id="S3.E4.m1.28.28.3.27.19.19.19.1.5.3"><mi id="S3.E4.m1.14.14.14.7.7.7" xref="S3.E4.m1.14.14.14.7.7.7.cmml">LSTM</mi><mo id="S3.E4.m1.28.28.3.27.19.19.19.1.5.3a">⁡</mo><mrow id="S3.E4.m1.28.28.3.27.19.19.19.1.5.3.3"><mo stretchy="false" id="S3.E4.m1.15.15.15.8.8.8">(</mo><msub id="S3.E4.m1.28.28.3.27.19.19.19.1.3.1.1.1"><mi id="S3.E4.m1.16.16.16.9.9.9" xref="S3.E4.m1.16.16.16.9.9.9.cmml">𝒙</mi><mi id="S3.E4.m1.17.17.17.10.10.10.1" xref="S3.E4.m1.17.17.17.10.10.10.1.cmml">t</mi></msub><mo id="S3.E4.m1.18.18.18.11.11.11">,</mo><msub id="S3.E4.m1.28.28.3.27.19.19.19.1.4.2.2.2"><mi id="S3.E4.m1.19.19.19.12.12.12" xref="S3.E4.m1.19.19.19.12.12.12.cmml">𝒉</mi><mrow id="S3.E4.m1.20.20.20.13.13.13.1" xref="S3.E4.m1.20.20.20.13.13.13.1.cmml"><mi id="S3.E4.m1.20.20.20.13.13.13.1.2" xref="S3.E4.m1.20.20.20.13.13.13.1.2.cmml">t</mi><mo id="S3.E4.m1.20.20.20.13.13.13.1.1" xref="S3.E4.m1.20.20.20.13.13.13.1.1.cmml">−</mo><mn id="S3.E4.m1.20.20.20.13.13.13.1.3" xref="S3.E4.m1.20.20.20.13.13.13.1.3.cmml">1</mn></mrow></msub><mo id="S3.E4.m1.21.21.21.14.14.14">,</mo><msub id="S3.E4.m1.28.28.3.27.19.19.19.1.5.3.3.3"><mi id="S3.E4.m1.22.22.22.15.15.15" xref="S3.E4.m1.22.22.22.15.15.15.cmml">𝒎</mi><mrow id="S3.E4.m1.23.23.23.16.16.16.1" xref="S3.E4.m1.23.23.23.16.16.16.1.cmml"><mi id="S3.E4.m1.23.23.23.16.16.16.1.2" xref="S3.E4.m1.23.23.23.16.16.16.1.2.cmml">t</mi><mo id="S3.E4.m1.23.23.23.16.16.16.1.1" xref="S3.E4.m1.23.23.23.16.16.16.1.1.cmml">−</mo><mn id="S3.E4.m1.23.23.23.16.16.16.1.3" xref="S3.E4.m1.23.23.23.16.16.16.1.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.E4.m1.24.24.24.17.17.17">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.25.25.25.18.18.18">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E4.m1.28b"><apply id="S3.E4.m1.26.26.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S3.E4.m1.26.26.1.1.1.1.1.cmml"><eq id="S3.E4.m1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3"></eq><apply id="S3.E4.m1.26.26.1.1.1.1.1.4.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.1.1.4.1.cmml">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">𝒙</ci><ci id="S3.E4.m1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1">𝑡</ci></apply><list id="S3.E4.m1.26.26.1.1.1.1.1.2.3.cmml"><apply id="S3.E4.m1.26.26.1.1.1.1.1.1.1.1.cmml"><times id="S3.E4.m1.26.26.1.1.1.1.1.1.1.1.1.cmml"></times><ci id="S3.E4.m1.4.4.4.4.4.4.cmml" xref="S3.E4.m1.4.4.4.4.4.4">𝑬</ci><apply id="S3.E4.m1.26.26.1.1.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.1.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S3.E4.m1.5.5.5.5.5.5.cmml" xref="S3.E4.m1.5.5.5.5.5.5">𝒘</ci><ci id="S3.E4.m1.6.6.6.6.6.6.1.cmml" xref="S3.E4.m1.6.6.6.6.6.6.1">𝑡</ci></apply></apply><apply id="S3.E4.m1.26.26.1.1.1.1.1.2.2.2.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.1.1.2.2.2.1.cmml">subscript</csymbol><ci id="S3.E4.m1.8.8.8.1.1.1.cmml" xref="S3.E4.m1.8.8.8.1.1.1">𝒉</ci><ci id="S3.E4.m1.9.9.9.2.2.2.1.cmml" xref="S3.E4.m1.9.9.9.2.2.2.1">𝑡</ci></apply></list></apply><apply id="S3.E4.m1.26.26.1.1.1.2.2.cmml"><eq id="S3.E4.m1.13.13.13.6.6.6.cmml" xref="S3.E4.m1.13.13.13.6.6.6"></eq><apply id="S3.E4.m1.26.26.1.1.1.2.2.5.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.2.2.5.1.cmml">subscript</csymbol><ci id="S3.E4.m1.11.11.11.4.4.4.cmml" xref="S3.E4.m1.11.11.11.4.4.4">𝒎</ci><ci id="S3.E4.m1.12.12.12.5.5.5.1.cmml" xref="S3.E4.m1.12.12.12.5.5.5.1">𝑡</ci></apply><apply id="S3.E4.m1.26.26.1.1.1.2.2.3.4.cmml"><ci id="S3.E4.m1.14.14.14.7.7.7.cmml" xref="S3.E4.m1.14.14.14.7.7.7">LSTM</ci><apply id="S3.E4.m1.26.26.1.1.1.2.2.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.2.2.1.1.1.1.1.cmml">subscript</csymbol><ci id="S3.E4.m1.16.16.16.9.9.9.cmml" xref="S3.E4.m1.16.16.16.9.9.9">𝒙</ci><ci id="S3.E4.m1.17.17.17.10.10.10.1.cmml" xref="S3.E4.m1.17.17.17.10.10.10.1">𝑡</ci></apply><apply id="S3.E4.m1.26.26.1.1.1.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.2.2.2.2.2.2.1.cmml">subscript</csymbol><ci id="S3.E4.m1.19.19.19.12.12.12.cmml" xref="S3.E4.m1.19.19.19.12.12.12">𝒉</ci><apply id="S3.E4.m1.20.20.20.13.13.13.1.cmml" xref="S3.E4.m1.20.20.20.13.13.13.1"><minus id="S3.E4.m1.20.20.20.13.13.13.1.1.cmml" xref="S3.E4.m1.20.20.20.13.13.13.1.1"></minus><ci id="S3.E4.m1.20.20.20.13.13.13.1.2.cmml" xref="S3.E4.m1.20.20.20.13.13.13.1.2">𝑡</ci><cn type="integer" id="S3.E4.m1.20.20.20.13.13.13.1.3.cmml" xref="S3.E4.m1.20.20.20.13.13.13.1.3">1</cn></apply></apply><apply id="S3.E4.m1.26.26.1.1.1.2.2.3.3.3.3.cmml"><csymbol cd="ambiguous" id="S3.E4.m1.26.26.1.1.1.2.2.3.3.3.3.1.cmml">subscript</csymbol><ci id="S3.E4.m1.22.22.22.15.15.15.cmml" xref="S3.E4.m1.22.22.22.15.15.15">𝒎</ci><apply id="S3.E4.m1.23.23.23.16.16.16.1.cmml" xref="S3.E4.m1.23.23.23.16.16.16.1"><minus id="S3.E4.m1.23.23.23.16.16.16.1.1.cmml" xref="S3.E4.m1.23.23.23.16.16.16.1.1"></minus><ci id="S3.E4.m1.23.23.23.16.16.16.1.2.cmml" xref="S3.E4.m1.23.23.23.16.16.16.1.2">𝑡</ci><cn type="integer" id="S3.E4.m1.23.23.23.16.16.16.1.3.cmml" xref="S3.E4.m1.23.23.23.16.16.16.1.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.28c">\begin{split}&amp;\bm{x}_{t}=\bm{E}\bm{w}_{t},\\
&amp;\bm{h}_{t},\bm{m}_{t}=\operatorname{LSTM}(\bm{x}_{t},\bm{h}_{t-1},\bm{m}_{t-1}),\\
\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.10" class="ltx_p">where <math id="S3.SS3.p3.4.m1.1" class="ltx_Math" alttext="\bm{w}_{t}" display="inline"><semantics id="S3.SS3.p3.4.m1.1a"><msub id="S3.SS3.p3.4.m1.1.1" xref="S3.SS3.p3.4.m1.1.1.cmml"><mi id="S3.SS3.p3.4.m1.1.1.2" xref="S3.SS3.p3.4.m1.1.1.2.cmml">𝒘</mi><mi id="S3.SS3.p3.4.m1.1.1.3" xref="S3.SS3.p3.4.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m1.1b"><apply id="S3.SS3.p3.4.m1.1.1.cmml" xref="S3.SS3.p3.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m1.1.1.1.cmml" xref="S3.SS3.p3.4.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m1.1.1.2.cmml" xref="S3.SS3.p3.4.m1.1.1.2">𝒘</ci><ci id="S3.SS3.p3.4.m1.1.1.3.cmml" xref="S3.SS3.p3.4.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m1.1c">\bm{w}_{t}</annotation></semantics></math> is the one-hot coding of word <math id="S3.SS3.p3.5.m2.1" class="ltx_Math" alttext="w_{t}" display="inline"><semantics id="S3.SS3.p3.5.m2.1a"><msub id="S3.SS3.p3.5.m2.1.1" xref="S3.SS3.p3.5.m2.1.1.cmml"><mi id="S3.SS3.p3.5.m2.1.1.2" xref="S3.SS3.p3.5.m2.1.1.2.cmml">w</mi><mi id="S3.SS3.p3.5.m2.1.1.3" xref="S3.SS3.p3.5.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m2.1b"><apply id="S3.SS3.p3.5.m2.1.1.cmml" xref="S3.SS3.p3.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m2.1.1.1.cmml" xref="S3.SS3.p3.5.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.m2.1.1.2.cmml" xref="S3.SS3.p3.5.m2.1.1.2">𝑤</ci><ci id="S3.SS3.p3.5.m2.1.1.3.cmml" xref="S3.SS3.p3.5.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m2.1c">w_{t}</annotation></semantics></math>; <math id="S3.SS3.p3.6.m3.1" class="ltx_Math" alttext="\bm{E}" display="inline"><semantics id="S3.SS3.p3.6.m3.1a"><mi id="S3.SS3.p3.6.m3.1.1" xref="S3.SS3.p3.6.m3.1.1.cmml">𝑬</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m3.1b"><ci id="S3.SS3.p3.6.m3.1.1.cmml" xref="S3.SS3.p3.6.m3.1.1">𝑬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m3.1c">\bm{E}</annotation></semantics></math> is the word embedding matrix; <math id="S3.SS3.p3.7.m4.1" class="ltx_Math" alttext="\bm{x}_{t}" display="inline"><semantics id="S3.SS3.p3.7.m4.1a"><msub id="S3.SS3.p3.7.m4.1.1" xref="S3.SS3.p3.7.m4.1.1.cmml"><mi id="S3.SS3.p3.7.m4.1.1.2" xref="S3.SS3.p3.7.m4.1.1.2.cmml">𝒙</mi><mi id="S3.SS3.p3.7.m4.1.1.3" xref="S3.SS3.p3.7.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m4.1b"><apply id="S3.SS3.p3.7.m4.1.1.cmml" xref="S3.SS3.p3.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.7.m4.1.1.1.cmml" xref="S3.SS3.p3.7.m4.1.1">subscript</csymbol><ci id="S3.SS3.p3.7.m4.1.1.2.cmml" xref="S3.SS3.p3.7.m4.1.1.2">𝒙</ci><ci id="S3.SS3.p3.7.m4.1.1.3.cmml" xref="S3.SS3.p3.7.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m4.1c">\bm{x}_{t}</annotation></semantics></math> is the embedded word vector which serves as an input to the LSTM. <math id="S3.SS3.p3.8.m5.2" class="ltx_Math" alttext="\operatorname{LSTM}(\cdot)" display="inline"><semantics id="S3.SS3.p3.8.m5.2a"><mrow id="S3.SS3.p3.8.m5.2.3.2" xref="S3.SS3.p3.8.m5.2.3.1.cmml"><mi id="S3.SS3.p3.8.m5.1.1" xref="S3.SS3.p3.8.m5.1.1.cmml">LSTM</mi><mo id="S3.SS3.p3.8.m5.2.3.2a" xref="S3.SS3.p3.8.m5.2.3.1.cmml">⁡</mo><mrow id="S3.SS3.p3.8.m5.2.3.2.1" xref="S3.SS3.p3.8.m5.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p3.8.m5.2.3.2.1.1" xref="S3.SS3.p3.8.m5.2.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p3.8.m5.2.2" xref="S3.SS3.p3.8.m5.2.2.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p3.8.m5.2.3.2.1.2" xref="S3.SS3.p3.8.m5.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m5.2b"><apply id="S3.SS3.p3.8.m5.2.3.1.cmml" xref="S3.SS3.p3.8.m5.2.3.2"><ci id="S3.SS3.p3.8.m5.1.1.cmml" xref="S3.SS3.p3.8.m5.1.1">LSTM</ci><ci id="S3.SS3.p3.8.m5.2.2.cmml" xref="S3.SS3.p3.8.m5.2.2">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m5.2c">\operatorname{LSTM}(\cdot)</annotation></semantics></math> takes previous states <math id="S3.SS3.p3.9.m6.2" class="ltx_Math" alttext="(\bm{h}_{t-1},\bm{m}_{t-1})" display="inline"><semantics id="S3.SS3.p3.9.m6.2a"><mrow id="S3.SS3.p3.9.m6.2.2.2" xref="S3.SS3.p3.9.m6.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p3.9.m6.2.2.2.3" xref="S3.SS3.p3.9.m6.2.2.3.cmml">(</mo><msub id="S3.SS3.p3.9.m6.1.1.1.1" xref="S3.SS3.p3.9.m6.1.1.1.1.cmml"><mi id="S3.SS3.p3.9.m6.1.1.1.1.2" xref="S3.SS3.p3.9.m6.1.1.1.1.2.cmml">𝒉</mi><mrow id="S3.SS3.p3.9.m6.1.1.1.1.3" xref="S3.SS3.p3.9.m6.1.1.1.1.3.cmml"><mi id="S3.SS3.p3.9.m6.1.1.1.1.3.2" xref="S3.SS3.p3.9.m6.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p3.9.m6.1.1.1.1.3.1" xref="S3.SS3.p3.9.m6.1.1.1.1.3.1.cmml">−</mo><mn id="S3.SS3.p3.9.m6.1.1.1.1.3.3" xref="S3.SS3.p3.9.m6.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS3.p3.9.m6.2.2.2.4" xref="S3.SS3.p3.9.m6.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.9.m6.2.2.2.2" xref="S3.SS3.p3.9.m6.2.2.2.2.cmml"><mi id="S3.SS3.p3.9.m6.2.2.2.2.2" xref="S3.SS3.p3.9.m6.2.2.2.2.2.cmml">𝒎</mi><mrow id="S3.SS3.p3.9.m6.2.2.2.2.3" xref="S3.SS3.p3.9.m6.2.2.2.2.3.cmml"><mi id="S3.SS3.p3.9.m6.2.2.2.2.3.2" xref="S3.SS3.p3.9.m6.2.2.2.2.3.2.cmml">t</mi><mo id="S3.SS3.p3.9.m6.2.2.2.2.3.1" xref="S3.SS3.p3.9.m6.2.2.2.2.3.1.cmml">−</mo><mn id="S3.SS3.p3.9.m6.2.2.2.2.3.3" xref="S3.SS3.p3.9.m6.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.SS3.p3.9.m6.2.2.2.5" xref="S3.SS3.p3.9.m6.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m6.2b"><interval closure="open" id="S3.SS3.p3.9.m6.2.2.3.cmml" xref="S3.SS3.p3.9.m6.2.2.2"><apply id="S3.SS3.p3.9.m6.1.1.1.1.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.9.m6.1.1.1.1.1.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.9.m6.1.1.1.1.2.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1.2">𝒉</ci><apply id="S3.SS3.p3.9.m6.1.1.1.1.3.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1.3"><minus id="S3.SS3.p3.9.m6.1.1.1.1.3.1.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1.3.1"></minus><ci id="S3.SS3.p3.9.m6.1.1.1.1.3.2.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS3.p3.9.m6.1.1.1.1.3.3.cmml" xref="S3.SS3.p3.9.m6.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS3.p3.9.m6.2.2.2.2.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.9.m6.2.2.2.2.1.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.9.m6.2.2.2.2.2.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2.2">𝒎</ci><apply id="S3.SS3.p3.9.m6.2.2.2.2.3.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2.3"><minus id="S3.SS3.p3.9.m6.2.2.2.2.3.1.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2.3.1"></minus><ci id="S3.SS3.p3.9.m6.2.2.2.2.3.2.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2.3.2">𝑡</ci><cn type="integer" id="S3.SS3.p3.9.m6.2.2.2.2.3.3.cmml" xref="S3.SS3.p3.9.m6.2.2.2.2.3.3">1</cn></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m6.2c">(\bm{h}_{t-1},\bm{m}_{t-1})</annotation></semantics></math> and <math id="S3.SS3.p3.10.m7.1" class="ltx_Math" alttext="\bm{x}_{t}" display="inline"><semantics id="S3.SS3.p3.10.m7.1a"><msub id="S3.SS3.p3.10.m7.1.1" xref="S3.SS3.p3.10.m7.1.1.cmml"><mi id="S3.SS3.p3.10.m7.1.1.2" xref="S3.SS3.p3.10.m7.1.1.2.cmml">𝒙</mi><mi id="S3.SS3.p3.10.m7.1.1.3" xref="S3.SS3.p3.10.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.10.m7.1b"><apply id="S3.SS3.p3.10.m7.1.1.cmml" xref="S3.SS3.p3.10.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.10.m7.1.1.1.cmml" xref="S3.SS3.p3.10.m7.1.1">subscript</csymbol><ci id="S3.SS3.p3.10.m7.1.1.2.cmml" xref="S3.SS3.p3.10.m7.1.1.2">𝒙</ci><ci id="S3.SS3.p3.10.m7.1.1.3.cmml" xref="S3.SS3.p3.10.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.10.m7.1c">\bm{x}_{t}</annotation></semantics></math> as input to generate the next states. For the computation of the LSTM, readers are referred to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for details.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.7" class="ltx_p"><span id="S3.SS3.p4.7.2" class="ltx_text ltx_font_bold">Multi-modal attention network</span> The attention network <span id="S3.SS3.p4.1.1" class="ltx_text" style="color:#000000;">takes local features <math id="S3.SS3.p4.1.1.m1.1" class="ltx_Math" alttext="\bm{I}" display="inline"><semantics id="S3.SS3.p4.1.1.m1.1a"><mi mathcolor="#000000" id="S3.SS3.p4.1.1.m1.1.1" xref="S3.SS3.p4.1.1.m1.1.1.cmml">𝑰</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.1.m1.1b"><ci id="S3.SS3.p4.1.1.m1.1.1.cmml" xref="S3.SS3.p4.1.1.m1.1.1">𝑰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.1.m1.1c">\bm{I}</annotation></semantics></math></span>, partial question coding <math id="S3.SS3.p4.2.m1.1" class="ltx_Math" alttext="\bm{h}_{t}" display="inline"><semantics id="S3.SS3.p4.2.m1.1a"><msub id="S3.SS3.p4.2.m1.1.1" xref="S3.SS3.p4.2.m1.1.1.cmml"><mi id="S3.SS3.p4.2.m1.1.1.2" xref="S3.SS3.p4.2.m1.1.1.2.cmml">𝒉</mi><mi id="S3.SS3.p4.2.m1.1.1.3" xref="S3.SS3.p4.2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m1.1b"><apply id="S3.SS3.p4.2.m1.1.1.cmml" xref="S3.SS3.p4.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m1.1.1.1.cmml" xref="S3.SS3.p4.2.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m1.1.1.2.cmml" xref="S3.SS3.p4.2.m1.1.1.2">𝒉</ci><ci id="S3.SS3.p4.2.m1.1.1.3.cmml" xref="S3.SS3.p4.2.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m1.1c">\bm{h}_{t}</annotation></semantics></math>, and answer coding <math id="S3.SS3.p4.3.m2.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS3.p4.3.m2.1a"><mi id="S3.SS3.p4.3.m2.1.1" xref="S3.SS3.p4.3.m2.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m2.1b"><ci id="S3.SS3.p4.3.m2.1.1.cmml" xref="S3.SS3.p4.3.m2.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m2.1c">\bm{a}</annotation></semantics></math> as input, and outputs the joint embedding of attended visual features <math id="S3.SS3.p4.4.m3.1" class="ltx_Math" alttext="\bm{c}_{t}" display="inline"><semantics id="S3.SS3.p4.4.m3.1a"><msub id="S3.SS3.p4.4.m3.1.1" xref="S3.SS3.p4.4.m3.1.1.cmml"><mi id="S3.SS3.p4.4.m3.1.1.2" xref="S3.SS3.p4.4.m3.1.1.2.cmml">𝒄</mi><mi id="S3.SS3.p4.4.m3.1.1.3" xref="S3.SS3.p4.4.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m3.1b"><apply id="S3.SS3.p4.4.m3.1.1.cmml" xref="S3.SS3.p4.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.4.m3.1.1.1.cmml" xref="S3.SS3.p4.4.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.4.m3.1.1.2.cmml" xref="S3.SS3.p4.4.m3.1.1.2">𝒄</ci><ci id="S3.SS3.p4.4.m3.1.1.3.cmml" xref="S3.SS3.p4.4.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m3.1c">\bm{c}_{t}</annotation></semantics></math> specified by the partial question-answer context <math id="S3.SS3.p4.5.m4.1" class="ltx_Math" alttext="\bm{z}_{t}" display="inline"><semantics id="S3.SS3.p4.5.m4.1a"><msub id="S3.SS3.p4.5.m4.1.1" xref="S3.SS3.p4.5.m4.1.1.cmml"><mi id="S3.SS3.p4.5.m4.1.1.2" xref="S3.SS3.p4.5.m4.1.1.2.cmml">𝒛</mi><mi id="S3.SS3.p4.5.m4.1.1.3" xref="S3.SS3.p4.5.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m4.1b"><apply id="S3.SS3.p4.5.m4.1.1.cmml" xref="S3.SS3.p4.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m4.1.1.1.cmml" xref="S3.SS3.p4.5.m4.1.1">subscript</csymbol><ci id="S3.SS3.p4.5.m4.1.1.2.cmml" xref="S3.SS3.p4.5.m4.1.1.2">𝒛</ci><ci id="S3.SS3.p4.5.m4.1.1.3.cmml" xref="S3.SS3.p4.5.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m4.1c">\bm{z}_{t}</annotation></semantics></math>. To obtain the partial question-answer context, the partial question coding <math id="S3.SS3.p4.6.m5.1" class="ltx_Math" alttext="\bm{h}_{t}" display="inline"><semantics id="S3.SS3.p4.6.m5.1a"><msub id="S3.SS3.p4.6.m5.1.1" xref="S3.SS3.p4.6.m5.1.1.cmml"><mi id="S3.SS3.p4.6.m5.1.1.2" xref="S3.SS3.p4.6.m5.1.1.2.cmml">𝒉</mi><mi id="S3.SS3.p4.6.m5.1.1.3" xref="S3.SS3.p4.6.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m5.1b"><apply id="S3.SS3.p4.6.m5.1.1.cmml" xref="S3.SS3.p4.6.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.6.m5.1.1.1.cmml" xref="S3.SS3.p4.6.m5.1.1">subscript</csymbol><ci id="S3.SS3.p4.6.m5.1.1.2.cmml" xref="S3.SS3.p4.6.m5.1.1.2">𝒉</ci><ci id="S3.SS3.p4.6.m5.1.1.3.cmml" xref="S3.SS3.p4.6.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m5.1c">\bm{h}_{t}</annotation></semantics></math> and answer coding <math id="S3.SS3.p4.7.m6.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS3.p4.7.m6.1a"><mi id="S3.SS3.p4.7.m6.1.1" xref="S3.SS3.p4.7.m6.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m6.1b"><ci id="S3.SS3.p4.7.m6.1.1.cmml" xref="S3.SS3.p4.7.m6.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m6.1c">\bm{a}</annotation></semantics></math> are fused as</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\bm{z}_{t}=\operatorname{ReLU}(\bm{W}_{q}\bm{h}_{t}+\bm{W}_{a}\bm{a})\\
" display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><msub id="S3.E5.m1.2.2.3" xref="S3.E5.m1.2.2.3.cmml"><mi id="S3.E5.m1.2.2.3.2" xref="S3.E5.m1.2.2.3.2.cmml">𝒛</mi><mi id="S3.E5.m1.2.2.3.3" xref="S3.E5.m1.2.2.3.3.cmml">t</mi></msub><mo id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.2.cmml"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">ReLU</mi><mo id="S3.E5.m1.2.2.1.1a" xref="S3.E5.m1.2.2.1.2.cmml">⁡</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.2.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.2.cmml"><msub id="S3.E5.m1.2.2.1.1.1.1.2.2" xref="S3.E5.m1.2.2.1.1.1.1.2.2.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.2.2.2" xref="S3.E5.m1.2.2.1.1.1.1.2.2.2.cmml">𝑾</mi><mi id="S3.E5.m1.2.2.1.1.1.1.2.2.3" xref="S3.E5.m1.2.2.1.1.1.1.2.2.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.1.2.1" xref="S3.E5.m1.2.2.1.1.1.1.2.1.cmml">​</mo><msub id="S3.E5.m1.2.2.1.1.1.1.2.3" xref="S3.E5.m1.2.2.1.1.1.1.2.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.2.3.2" xref="S3.E5.m1.2.2.1.1.1.1.2.3.2.cmml">𝒉</mi><mi id="S3.E5.m1.2.2.1.1.1.1.2.3.3" xref="S3.E5.m1.2.2.1.1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="S3.E5.m1.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.cmml">+</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.3.cmml"><msub id="S3.E5.m1.2.2.1.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.3.2.2" xref="S3.E5.m1.2.2.1.1.1.1.3.2.2.cmml">𝑾</mi><mi id="S3.E5.m1.2.2.1.1.1.1.3.2.3" xref="S3.E5.m1.2.2.1.1.1.1.3.2.3.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.1.3.1" xref="S3.E5.m1.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E5.m1.2.2.1.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.1.3.3.cmml">𝒂</mi></mrow></mrow><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"><eq id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2"></eq><apply id="S3.E5.m1.2.2.3.cmml" xref="S3.E5.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.1.cmml" xref="S3.E5.m1.2.2.3">subscript</csymbol><ci id="S3.E5.m1.2.2.3.2.cmml" xref="S3.E5.m1.2.2.3.2">𝒛</ci><ci id="S3.E5.m1.2.2.3.3.cmml" xref="S3.E5.m1.2.2.3.3">𝑡</ci></apply><apply id="S3.E5.m1.2.2.1.2.cmml" xref="S3.E5.m1.2.2.1.1"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ReLU</ci><apply id="S3.E5.m1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1"><plus id="S3.E5.m1.2.2.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1"></plus><apply id="S3.E5.m1.2.2.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2"><times id="S3.E5.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.1"></times><apply id="S3.E5.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.2.2">𝑾</ci><ci id="S3.E5.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.2.3">𝑞</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.3.2">𝒉</ci><ci id="S3.E5.m1.2.2.1.1.1.1.2.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S3.E5.m1.2.2.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3"><times id="S3.E5.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3.1"></times><apply id="S3.E5.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3.2.2">𝑾</ci><ci id="S3.E5.m1.2.2.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3.2.3">𝑎</ci></apply><ci id="S3.E5.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3.3">𝒂</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\bm{z}_{t}=\operatorname{ReLU}(\bm{W}_{q}\bm{h}_{t}+\bm{W}_{a}\bm{a})\\
</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.9" class="ltx_p">where <math id="S3.SS3.p4.8.m1.1" class="ltx_Math" alttext="\bm{W}_{q}" display="inline"><semantics id="S3.SS3.p4.8.m1.1a"><msub id="S3.SS3.p4.8.m1.1.1" xref="S3.SS3.p4.8.m1.1.1.cmml"><mi id="S3.SS3.p4.8.m1.1.1.2" xref="S3.SS3.p4.8.m1.1.1.2.cmml">𝑾</mi><mi id="S3.SS3.p4.8.m1.1.1.3" xref="S3.SS3.p4.8.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m1.1b"><apply id="S3.SS3.p4.8.m1.1.1.cmml" xref="S3.SS3.p4.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m1.1.1.1.cmml" xref="S3.SS3.p4.8.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.8.m1.1.1.2.cmml" xref="S3.SS3.p4.8.m1.1.1.2">𝑾</ci><ci id="S3.SS3.p4.8.m1.1.1.3.cmml" xref="S3.SS3.p4.8.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m1.1c">\bm{W}_{q}</annotation></semantics></math> and <math id="S3.SS3.p4.9.m2.1" class="ltx_Math" alttext="\bm{W}_{a}" display="inline"><semantics id="S3.SS3.p4.9.m2.1a"><msub id="S3.SS3.p4.9.m2.1.1" xref="S3.SS3.p4.9.m2.1.1.cmml"><mi id="S3.SS3.p4.9.m2.1.1.2" xref="S3.SS3.p4.9.m2.1.1.2.cmml">𝑾</mi><mi id="S3.SS3.p4.9.m2.1.1.3" xref="S3.SS3.p4.9.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m2.1b"><apply id="S3.SS3.p4.9.m2.1.1.cmml" xref="S3.SS3.p4.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.9.m2.1.1.1.cmml" xref="S3.SS3.p4.9.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.9.m2.1.1.2.cmml" xref="S3.SS3.p4.9.m2.1.1.2">𝑾</ci><ci id="S3.SS3.p4.9.m2.1.1.3.cmml" xref="S3.SS3.p4.9.m2.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m2.1c">\bm{W}_{a}</annotation></semantics></math> are the embedding weights for question and answer respectively.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.5" class="ltx_p">Then the visual features <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\bm{I}" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">𝑰</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">𝑰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\bm{I}</annotation></semantics></math> and textual context <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="\bm{z}_{t}" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><msub id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.1.1.2" xref="S3.SS3.p5.2.m2.1.1.2.cmml">𝒛</mi><mi id="S3.SS3.p5.2.m2.1.1.3" xref="S3.SS3.p5.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p5.2.m2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.1.1.2">𝒛</ci><ci id="S3.SS3.p5.2.m2.1.1.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\bm{z}_{t}</annotation></semantics></math> are spatially matched via soft attention: The visual features <math id="S3.SS3.p5.3.m3.2" class="ltx_Math" alttext="\bm{v}_{i,j}" display="inline"><semantics id="S3.SS3.p5.3.m3.2a"><msub id="S3.SS3.p5.3.m3.2.3" xref="S3.SS3.p5.3.m3.2.3.cmml"><mi mathcolor="#000000" id="S3.SS3.p5.3.m3.2.3.2" xref="S3.SS3.p5.3.m3.2.3.2.cmml">𝒗</mi><mrow id="S3.SS3.p5.3.m3.2.2.2.4" xref="S3.SS3.p5.3.m3.2.2.2.3.cmml"><mi mathcolor="#000000" id="S3.SS3.p5.3.m3.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.cmml">i</mi><mo mathcolor="#000000" id="S3.SS3.p5.3.m3.2.2.2.4.1" xref="S3.SS3.p5.3.m3.2.2.2.3.cmml">,</mo><mi mathcolor="#000000" id="S3.SS3.p5.3.m3.2.2.2.2" xref="S3.SS3.p5.3.m3.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.2b"><apply id="S3.SS3.p5.3.m3.2.3.cmml" xref="S3.SS3.p5.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.2.3.1.cmml" xref="S3.SS3.p5.3.m3.2.3">subscript</csymbol><ci id="S3.SS3.p5.3.m3.2.3.2.cmml" xref="S3.SS3.p5.3.m3.2.3.2">𝒗</ci><list id="S3.SS3.p5.3.m3.2.2.2.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.4"><ci id="S3.SS3.p5.3.m3.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1">𝑖</ci><ci id="S3.SS3.p5.3.m3.2.2.2.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.2c">\bm{v}_{i,j}</annotation></semantics></math> and context vector <math id="S3.SS3.p5.4.m4.1" class="ltx_Math" alttext="\bm{z}_{t}" display="inline"><semantics id="S3.SS3.p5.4.m4.1a"><msub id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml"><mi id="S3.SS3.p5.4.m4.1.1.2" xref="S3.SS3.p5.4.m4.1.1.2.cmml">𝒛</mi><mi id="S3.SS3.p5.4.m4.1.1.3" xref="S3.SS3.p5.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><apply id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.4.m4.1.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p5.4.m4.1.1.2.cmml" xref="S3.SS3.p5.4.m4.1.1.2">𝒛</ci><ci id="S3.SS3.p5.4.m4.1.1.3.cmml" xref="S3.SS3.p5.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\bm{z}_{t}</annotation></semantics></math> are fused by a multi-modal low rank bilinear pooling (MLB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and then the fused feature <math id="S3.SS3.p5.5.m5.1" class="ltx_Math" alttext="\bm{f}_{ij}" display="inline"><semantics id="S3.SS3.p5.5.m5.1a"><msub id="S3.SS3.p5.5.m5.1.1" xref="S3.SS3.p5.5.m5.1.1.cmml"><mi id="S3.SS3.p5.5.m5.1.1.2" xref="S3.SS3.p5.5.m5.1.1.2.cmml">𝒇</mi><mrow id="S3.SS3.p5.5.m5.1.1.3" xref="S3.SS3.p5.5.m5.1.1.3.cmml"><mi id="S3.SS3.p5.5.m5.1.1.3.2" xref="S3.SS3.p5.5.m5.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.5.m5.1.1.3.1" xref="S3.SS3.p5.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.5.m5.1.1.3.3" xref="S3.SS3.p5.5.m5.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.5.m5.1b"><apply id="S3.SS3.p5.5.m5.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.5.m5.1.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p5.5.m5.1.1.2.cmml" xref="S3.SS3.p5.5.m5.1.1.2">𝒇</ci><apply id="S3.SS3.p5.5.m5.1.1.3.cmml" xref="S3.SS3.p5.5.m5.1.1.3"><times id="S3.SS3.p5.5.m5.1.1.3.1.cmml" xref="S3.SS3.p5.5.m5.1.1.3.1"></times><ci id="S3.SS3.p5.5.m5.1.1.3.2.cmml" xref="S3.SS3.p5.5.m5.1.1.3.2">𝑖</ci><ci id="S3.SS3.p5.5.m5.1.1.3.3.cmml" xref="S3.SS3.p5.5.m5.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.5.m5.1c">\bm{f}_{ij}</annotation></semantics></math> is used for attention map computation as follows:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.50" class="ltx_Math" alttext="\begin{split}&amp;\bm{f}_{ij}=\delta(\bm{U}\delta(\bm{W}_{v}\bm{v}_{i,j})\odot\delta(\bm{W}_{z}\bm{z}_{t}))\\
&amp;\alpha_{ij}^{t}=\operatorname{softmax}(\bm{p}^{T}[\bm{f}_{ij}])\\
&amp;\bm{c}_{t}=\sum_{ij}\alpha_{ij}^{t}\bm{v}_{ij},\end{split}" display="block"><semantics id="S3.E6.m1.50a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S3.E6.m1.50.50.4"><mtr id="S3.E6.m1.50.50.4a"><mtd id="S3.E6.m1.50.50.4b" xref="S3.E6.m1.47.47.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m1.50.50.4c"><mrow id="S3.E6.m1.48.48.2.47.23.23"><msub id="S3.E6.m1.48.48.2.47.23.23.24"><mi id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml">𝒇</mi><mrow id="S3.E6.m1.2.2.2.2.2.2.1" xref="S3.E6.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.1.2" xref="S3.E6.m1.2.2.2.2.2.2.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.2.2.1.1" xref="S3.E6.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E6.m1.2.2.2.2.2.2.1.3" xref="S3.E6.m1.2.2.2.2.2.2.1.3.cmml">j</mi></mrow></msub><mo id="S3.E6.m1.3.3.3.3.3.3" xref="S3.E6.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23"><mi id="S3.E6.m1.4.4.4.4.4.4" xref="S3.E6.m1.4.4.4.4.4.4.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.48.48.2.47.23.23.23.2" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1"><mo stretchy="false" id="S3.E6.m1.5.5.5.5.5.5" xref="S3.E6.m1.47.47.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1"><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1"><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1"><mi id="S3.E6.m1.6.6.6.6.6.6" xref="S3.E6.m1.6.6.6.6.6.6.cmml">𝑼</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.2" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><mi id="S3.E6.m1.7.7.7.7.7.7" xref="S3.E6.m1.7.7.7.7.7.7.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.2a" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.1.1"><mo stretchy="false" id="S3.E6.m1.8.8.8.8.8.8" xref="S3.E6.m1.47.47.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.1.1.1"><msub id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.1.1.1.2"><mi id="S3.E6.m1.9.9.9.9.9.9" xref="S3.E6.m1.9.9.9.9.9.9.cmml">𝑾</mi><mi id="S3.E6.m1.10.10.10.10.10.10.1" xref="S3.E6.m1.10.10.10.10.10.10.1.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><msub id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.1.1.1.1.1.3"><mi id="S3.E6.m1.11.11.11.11.11.11" xref="S3.E6.m1.11.11.11.11.11.11.cmml">𝒗</mi><mrow id="S3.E6.m1.12.12.12.12.12.12.1.4" xref="S3.E6.m1.12.12.12.12.12.12.1.3.cmml"><mi id="S3.E6.m1.12.12.12.12.12.12.1.1" xref="S3.E6.m1.12.12.12.12.12.12.1.1.cmml">i</mi><mo id="S3.E6.m1.12.12.12.12.12.12.1.4.1" xref="S3.E6.m1.12.12.12.12.12.12.1.3.cmml">,</mo><mi id="S3.E6.m1.12.12.12.12.12.12.1.2" xref="S3.E6.m1.12.12.12.12.12.12.1.2.cmml">j</mi></mrow></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.E6.m1.13.13.13.13.13.13" xref="S3.E6.m1.47.47.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E6.m1.14.14.14.14.14.14" xref="S3.E6.m1.14.14.14.14.14.14.cmml">⊙</mo><mi id="S3.E6.m1.15.15.15.15.15.15" xref="S3.E6.m1.15.15.15.15.15.15.cmml">δ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.3" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.2.1"><mo stretchy="false" id="S3.E6.m1.16.16.16.16.16.16" xref="S3.E6.m1.47.47.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.2.1.1"><msub id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.2.1.1.2"><mi id="S3.E6.m1.17.17.17.17.17.17" xref="S3.E6.m1.17.17.17.17.17.17.cmml">𝑾</mi><mi id="S3.E6.m1.18.18.18.18.18.18.1" xref="S3.E6.m1.18.18.18.18.18.18.1.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.2.1.1.1" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><msub id="S3.E6.m1.48.48.2.47.23.23.23.1.1.1.2.1.1.3"><mi id="S3.E6.m1.19.19.19.19.19.19" xref="S3.E6.m1.19.19.19.19.19.19.cmml">𝒛</mi><mi id="S3.E6.m1.20.20.20.20.20.20.1" xref="S3.E6.m1.20.20.20.20.20.20.1.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.E6.m1.21.21.21.21.21.21" xref="S3.E6.m1.47.47.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E6.m1.22.22.22.22.22.22" xref="S3.E6.m1.47.47.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E6.m1.50.50.4d"><mtd id="S3.E6.m1.50.50.4e" xref="S3.E6.m1.47.47.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m1.50.50.4f"><mrow id="S3.E6.m1.49.49.3.48.14.14"><msubsup id="S3.E6.m1.49.49.3.48.14.14.15"><mi id="S3.E6.m1.23.23.23.1.1.1" xref="S3.E6.m1.23.23.23.1.1.1.cmml">α</mi><mrow id="S3.E6.m1.24.24.24.2.2.2.1" xref="S3.E6.m1.24.24.24.2.2.2.1.cmml"><mi id="S3.E6.m1.24.24.24.2.2.2.1.2" xref="S3.E6.m1.24.24.24.2.2.2.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.24.24.24.2.2.2.1.1" xref="S3.E6.m1.24.24.24.2.2.2.1.1.cmml">​</mo><mi id="S3.E6.m1.24.24.24.2.2.2.1.3" xref="S3.E6.m1.24.24.24.2.2.2.1.3.cmml">j</mi></mrow><mi id="S3.E6.m1.25.25.25.3.3.3.1" xref="S3.E6.m1.25.25.25.3.3.3.1.cmml">t</mi></msubsup><mo id="S3.E6.m1.26.26.26.4.4.4" xref="S3.E6.m1.26.26.26.4.4.4.cmml">=</mo><mrow id="S3.E6.m1.49.49.3.48.14.14.14.1"><mi id="S3.E6.m1.27.27.27.5.5.5" xref="S3.E6.m1.27.27.27.5.5.5.cmml">softmax</mi><mo id="S3.E6.m1.49.49.3.48.14.14.14.1a" xref="S3.E6.m1.47.47.1.1.1.cmml">⁡</mo><mrow id="S3.E6.m1.49.49.3.48.14.14.14.1.1"><mo stretchy="false" id="S3.E6.m1.28.28.28.6.6.6" xref="S3.E6.m1.47.47.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.49.49.3.48.14.14.14.1.1.1"><msup id="S3.E6.m1.49.49.3.48.14.14.14.1.1.1.3"><mi id="S3.E6.m1.29.29.29.7.7.7" xref="S3.E6.m1.29.29.29.7.7.7.cmml">𝒑</mi><mi id="S3.E6.m1.30.30.30.8.8.8.1" xref="S3.E6.m1.30.30.30.8.8.8.1.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E6.m1.49.49.3.48.14.14.14.1.1.1.2" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><mrow id="S3.E6.m1.49.49.3.48.14.14.14.1.1.1.1.1"><mo stretchy="false" id="S3.E6.m1.31.31.31.9.9.9" xref="S3.E6.m1.47.47.1.1.1.cmml">[</mo><msub id="S3.E6.m1.49.49.3.48.14.14.14.1.1.1.1.1.1"><mi id="S3.E6.m1.32.32.32.10.10.10" xref="S3.E6.m1.32.32.32.10.10.10.cmml">𝒇</mi><mrow id="S3.E6.m1.33.33.33.11.11.11.1" xref="S3.E6.m1.33.33.33.11.11.11.1.cmml"><mi id="S3.E6.m1.33.33.33.11.11.11.1.2" xref="S3.E6.m1.33.33.33.11.11.11.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.33.33.33.11.11.11.1.1" xref="S3.E6.m1.33.33.33.11.11.11.1.1.cmml">​</mo><mi id="S3.E6.m1.33.33.33.11.11.11.1.3" xref="S3.E6.m1.33.33.33.11.11.11.1.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.E6.m1.34.34.34.12.12.12" xref="S3.E6.m1.47.47.1.1.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S3.E6.m1.35.35.35.13.13.13" xref="S3.E6.m1.47.47.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E6.m1.50.50.4g"><mtd id="S3.E6.m1.50.50.4h" xref="S3.E6.m1.47.47.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m1.50.50.4i"><mrow id="S3.E6.m1.50.50.4.49.12.12.12"><mrow id="S3.E6.m1.50.50.4.49.12.12.12.1"><msub id="S3.E6.m1.50.50.4.49.12.12.12.1.1"><mi id="S3.E6.m1.36.36.36.1.1.1" xref="S3.E6.m1.36.36.36.1.1.1.cmml">𝒄</mi><mi id="S3.E6.m1.37.37.37.2.2.2.1" xref="S3.E6.m1.37.37.37.2.2.2.1.cmml">t</mi></msub><mo rspace="0.111em" id="S3.E6.m1.38.38.38.3.3.3" xref="S3.E6.m1.38.38.38.3.3.3.cmml">=</mo><mrow id="S3.E6.m1.50.50.4.49.12.12.12.1.2"><munder id="S3.E6.m1.50.50.4.49.12.12.12.1.2.1"><mo movablelimits="false" id="S3.E6.m1.39.39.39.4.4.4" xref="S3.E6.m1.39.39.39.4.4.4.cmml">∑</mo><mrow id="S3.E6.m1.40.40.40.5.5.5.1" xref="S3.E6.m1.40.40.40.5.5.5.1.cmml"><mi id="S3.E6.m1.40.40.40.5.5.5.1.2" xref="S3.E6.m1.40.40.40.5.5.5.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.40.40.40.5.5.5.1.1" xref="S3.E6.m1.40.40.40.5.5.5.1.1.cmml">​</mo><mi id="S3.E6.m1.40.40.40.5.5.5.1.3" xref="S3.E6.m1.40.40.40.5.5.5.1.3.cmml">j</mi></mrow></munder><mrow id="S3.E6.m1.50.50.4.49.12.12.12.1.2.2"><msubsup id="S3.E6.m1.50.50.4.49.12.12.12.1.2.2.2"><mi id="S3.E6.m1.41.41.41.6.6.6" xref="S3.E6.m1.41.41.41.6.6.6.cmml">α</mi><mrow id="S3.E6.m1.42.42.42.7.7.7.1" xref="S3.E6.m1.42.42.42.7.7.7.1.cmml"><mi id="S3.E6.m1.42.42.42.7.7.7.1.2" xref="S3.E6.m1.42.42.42.7.7.7.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.42.42.42.7.7.7.1.1" xref="S3.E6.m1.42.42.42.7.7.7.1.1.cmml">​</mo><mi id="S3.E6.m1.42.42.42.7.7.7.1.3" xref="S3.E6.m1.42.42.42.7.7.7.1.3.cmml">j</mi></mrow><mi id="S3.E6.m1.43.43.43.8.8.8.1" xref="S3.E6.m1.43.43.43.8.8.8.1.cmml">t</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.E6.m1.50.50.4.49.12.12.12.1.2.2.1" xref="S3.E6.m1.47.47.1.1.1.cmml">​</mo><msub id="S3.E6.m1.50.50.4.49.12.12.12.1.2.2.3"><mi id="S3.E6.m1.44.44.44.9.9.9" xref="S3.E6.m1.44.44.44.9.9.9.cmml">𝒗</mi><mrow id="S3.E6.m1.45.45.45.10.10.10.1" xref="S3.E6.m1.45.45.45.10.10.10.1.cmml"><mi id="S3.E6.m1.45.45.45.10.10.10.1.2" xref="S3.E6.m1.45.45.45.10.10.10.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.45.45.45.10.10.10.1.1" xref="S3.E6.m1.45.45.45.10.10.10.1.1.cmml">​</mo><mi id="S3.E6.m1.45.45.45.10.10.10.1.3" xref="S3.E6.m1.45.45.45.10.10.10.1.3.cmml">j</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.E6.m1.46.46.46.11.11.11" xref="S3.E6.m1.47.47.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E6.m1.50b"><apply id="S3.E6.m1.47.47.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><and id="S3.E6.m1.47.47.1.1.1a.cmml" xref="S3.E6.m1.50.50.4b"></and><apply id="S3.E6.m1.47.47.1.1.1b.cmml" xref="S3.E6.m1.50.50.4b"><eq id="S3.E6.m1.3.3.3.3.3.3.cmml" xref="S3.E6.m1.3.3.3.3.3.3"></eq><apply id="S3.E6.m1.47.47.1.1.1.4.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.4.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1">𝒇</ci><apply id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1"><times id="S3.E6.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.E6.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1.2">𝑖</ci><ci id="S3.E6.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1.3">𝑗</ci></apply></apply><apply id="S3.E6.m1.47.47.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"></times><ci id="S3.E6.m1.4.4.4.4.4.4.cmml" xref="S3.E6.m1.4.4.4.4.4.4">𝛿</ci><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.50.50.4b"></times><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="latexml" id="S3.E6.m1.14.14.14.14.14.14.cmml" xref="S3.E6.m1.14.14.14.14.14.14">direct-product</csymbol><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"></times><ci id="S3.E6.m1.6.6.6.6.6.6.cmml" xref="S3.E6.m1.6.6.6.6.6.6">𝑼</ci><ci id="S3.E6.m1.7.7.7.7.7.7.cmml" xref="S3.E6.m1.7.7.7.7.7.7">𝛿</ci><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"></times><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.9.9.9.9.9.9.cmml" xref="S3.E6.m1.9.9.9.9.9.9">𝑾</ci><ci id="S3.E6.m1.10.10.10.10.10.10.1.cmml" xref="S3.E6.m1.10.10.10.10.10.10.1">𝑣</ci></apply><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.11.11.11.11.11.11.cmml" xref="S3.E6.m1.11.11.11.11.11.11">𝒗</ci><list id="S3.E6.m1.12.12.12.12.12.12.1.3.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.4"><ci id="S3.E6.m1.12.12.12.12.12.12.1.1.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.1">𝑖</ci><ci id="S3.E6.m1.12.12.12.12.12.12.1.2.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.2">𝑗</ci></list></apply></apply></apply><ci id="S3.E6.m1.15.15.15.15.15.15.cmml" xref="S3.E6.m1.15.15.15.15.15.15">𝛿</ci></apply><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"></times><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.17.17.17.17.17.17.cmml" xref="S3.E6.m1.17.17.17.17.17.17">𝑾</ci><ci id="S3.E6.m1.18.18.18.18.18.18.1.cmml" xref="S3.E6.m1.18.18.18.18.18.18.1">𝑧</ci></apply><apply id="S3.E6.m1.47.47.1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.1.1.1.1.2.1.1.3.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.19.19.19.19.19.19.cmml" xref="S3.E6.m1.19.19.19.19.19.19">𝒛</ci><ci id="S3.E6.m1.20.20.20.20.20.20.1.cmml" xref="S3.E6.m1.20.20.20.20.20.20.1">𝑡</ci></apply></apply></apply><apply id="S3.E6.m1.47.47.1.1.1.1.4.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.1.4.1.cmml" xref="S3.E6.m1.50.50.4b">superscript</csymbol><apply id="S3.E6.m1.47.47.1.1.1.1.4.2.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.1.4.2.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.23.23.23.1.1.1.cmml" xref="S3.E6.m1.23.23.23.1.1.1">𝛼</ci><apply id="S3.E6.m1.24.24.24.2.2.2.1.cmml" xref="S3.E6.m1.24.24.24.2.2.2.1"><times id="S3.E6.m1.24.24.24.2.2.2.1.1.cmml" xref="S3.E6.m1.24.24.24.2.2.2.1.1"></times><ci id="S3.E6.m1.24.24.24.2.2.2.1.2.cmml" xref="S3.E6.m1.24.24.24.2.2.2.1.2">𝑖</ci><ci id="S3.E6.m1.24.24.24.2.2.2.1.3.cmml" xref="S3.E6.m1.24.24.24.2.2.2.1.3">𝑗</ci></apply></apply><ci id="S3.E6.m1.25.25.25.3.3.3.1.cmml" xref="S3.E6.m1.25.25.25.3.3.3.1">𝑡</ci></apply></apply></apply><apply id="S3.E6.m1.47.47.1.1.1c.cmml" xref="S3.E6.m1.50.50.4b"><eq id="S3.E6.m1.26.26.26.4.4.4.cmml" xref="S3.E6.m1.26.26.26.4.4.4"></eq><share href="#S3.E6.m1.47.47.1.1.1.1.cmml" id="S3.E6.m1.47.47.1.1.1d.cmml" xref="S3.E6.m1.50.50.4b"></share><apply id="S3.E6.m1.47.47.1.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.2.2.cmml" xref="S3.E6.m1.50.50.4b"></times><apply id="S3.E6.m1.47.47.1.1.1.2.1.2.cmml" xref="S3.E6.m1.50.50.4b"><ci id="S3.E6.m1.27.27.27.5.5.5.cmml" xref="S3.E6.m1.27.27.27.5.5.5">softmax</ci><apply id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"></times><apply id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.3.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.3.1.cmml" xref="S3.E6.m1.50.50.4b">superscript</csymbol><ci id="S3.E6.m1.29.29.29.7.7.7.cmml" xref="S3.E6.m1.29.29.29.7.7.7">𝒑</ci><ci id="S3.E6.m1.30.30.30.8.8.8.1.cmml" xref="S3.E6.m1.30.30.30.8.8.8.1">𝑇</ci></apply><apply id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="latexml" id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.50.50.4b">delimited-[]</csymbol><apply id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.32.32.32.10.10.10.cmml" xref="S3.E6.m1.32.32.32.10.10.10">𝒇</ci><apply id="S3.E6.m1.33.33.33.11.11.11.1.cmml" xref="S3.E6.m1.33.33.33.11.11.11.1"><times id="S3.E6.m1.33.33.33.11.11.11.1.1.cmml" xref="S3.E6.m1.33.33.33.11.11.11.1.1"></times><ci id="S3.E6.m1.33.33.33.11.11.11.1.2.cmml" xref="S3.E6.m1.33.33.33.11.11.11.1.2">𝑖</ci><ci id="S3.E6.m1.33.33.33.11.11.11.1.3.cmml" xref="S3.E6.m1.33.33.33.11.11.11.1.3">𝑗</ci></apply></apply></apply></apply></apply><apply id="S3.E6.m1.47.47.1.1.1.2.3.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.2.3.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.36.36.36.1.1.1.cmml" xref="S3.E6.m1.36.36.36.1.1.1">𝒄</ci><ci id="S3.E6.m1.37.37.37.2.2.2.1.cmml" xref="S3.E6.m1.37.37.37.2.2.2.1">𝑡</ci></apply></apply></apply><apply id="S3.E6.m1.47.47.1.1.1e.cmml" xref="S3.E6.m1.50.50.4b"><eq id="S3.E6.m1.38.38.38.3.3.3.cmml" xref="S3.E6.m1.38.38.38.3.3.3"></eq><share href="#S3.E6.m1.47.47.1.1.1.2.cmml" id="S3.E6.m1.47.47.1.1.1f.cmml" xref="S3.E6.m1.50.50.4b"></share><apply id="S3.E6.m1.47.47.1.1.1.8.cmml" xref="S3.E6.m1.50.50.4b"><apply id="S3.E6.m1.47.47.1.1.1.8.1.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.8.1.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><sum id="S3.E6.m1.39.39.39.4.4.4.cmml" xref="S3.E6.m1.39.39.39.4.4.4"></sum><apply id="S3.E6.m1.40.40.40.5.5.5.1.cmml" xref="S3.E6.m1.40.40.40.5.5.5.1"><times id="S3.E6.m1.40.40.40.5.5.5.1.1.cmml" xref="S3.E6.m1.40.40.40.5.5.5.1.1"></times><ci id="S3.E6.m1.40.40.40.5.5.5.1.2.cmml" xref="S3.E6.m1.40.40.40.5.5.5.1.2">𝑖</ci><ci id="S3.E6.m1.40.40.40.5.5.5.1.3.cmml" xref="S3.E6.m1.40.40.40.5.5.5.1.3">𝑗</ci></apply></apply><apply id="S3.E6.m1.47.47.1.1.1.8.2.cmml" xref="S3.E6.m1.50.50.4b"><times id="S3.E6.m1.47.47.1.1.1.8.2.1.cmml" xref="S3.E6.m1.50.50.4b"></times><apply id="S3.E6.m1.47.47.1.1.1.8.2.2.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.8.2.2.1.cmml" xref="S3.E6.m1.50.50.4b">superscript</csymbol><apply id="S3.E6.m1.47.47.1.1.1.8.2.2.2.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.8.2.2.2.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.41.41.41.6.6.6.cmml" xref="S3.E6.m1.41.41.41.6.6.6">𝛼</ci><apply id="S3.E6.m1.42.42.42.7.7.7.1.cmml" xref="S3.E6.m1.42.42.42.7.7.7.1"><times id="S3.E6.m1.42.42.42.7.7.7.1.1.cmml" xref="S3.E6.m1.42.42.42.7.7.7.1.1"></times><ci id="S3.E6.m1.42.42.42.7.7.7.1.2.cmml" xref="S3.E6.m1.42.42.42.7.7.7.1.2">𝑖</ci><ci id="S3.E6.m1.42.42.42.7.7.7.1.3.cmml" xref="S3.E6.m1.42.42.42.7.7.7.1.3">𝑗</ci></apply></apply><ci id="S3.E6.m1.43.43.43.8.8.8.1.cmml" xref="S3.E6.m1.43.43.43.8.8.8.1">𝑡</ci></apply><apply id="S3.E6.m1.47.47.1.1.1.8.2.3.cmml" xref="S3.E6.m1.50.50.4b"><csymbol cd="ambiguous" id="S3.E6.m1.47.47.1.1.1.8.2.3.1.cmml" xref="S3.E6.m1.50.50.4b">subscript</csymbol><ci id="S3.E6.m1.44.44.44.9.9.9.cmml" xref="S3.E6.m1.44.44.44.9.9.9">𝒗</ci><apply id="S3.E6.m1.45.45.45.10.10.10.1.cmml" xref="S3.E6.m1.45.45.45.10.10.10.1"><times id="S3.E6.m1.45.45.45.10.10.10.1.1.cmml" xref="S3.E6.m1.45.45.45.10.10.10.1.1"></times><ci id="S3.E6.m1.45.45.45.10.10.10.1.2.cmml" xref="S3.E6.m1.45.45.45.10.10.10.1.2">𝑖</ci><ci id="S3.E6.m1.45.45.45.10.10.10.1.3.cmml" xref="S3.E6.m1.45.45.45.10.10.10.1.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.50c">\begin{split}&amp;\bm{f}_{ij}=\delta(\bm{U}\delta(\bm{W}_{v}\bm{v}_{i,j})\odot\delta(\bm{W}_{z}\bm{z}_{t}))\\
&amp;\alpha_{ij}^{t}=\operatorname{softmax}(\bm{p}^{T}[\bm{f}_{ij}])\\
&amp;\bm{c}_{t}=\sum_{ij}\alpha_{ij}^{t}\bm{v}_{ij},\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p5.11" class="ltx_p">where <math id="S3.SS3.p5.6.m1.1" class="ltx_Math" alttext="\bm{c}_{t}" display="inline"><semantics id="S3.SS3.p5.6.m1.1a"><msub id="S3.SS3.p5.6.m1.1.1" xref="S3.SS3.p5.6.m1.1.1.cmml"><mi id="S3.SS3.p5.6.m1.1.1.2" xref="S3.SS3.p5.6.m1.1.1.2.cmml">𝒄</mi><mi id="S3.SS3.p5.6.m1.1.1.3" xref="S3.SS3.p5.6.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.6.m1.1b"><apply id="S3.SS3.p5.6.m1.1.1.cmml" xref="S3.SS3.p5.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.6.m1.1.1.1.cmml" xref="S3.SS3.p5.6.m1.1.1">subscript</csymbol><ci id="S3.SS3.p5.6.m1.1.1.2.cmml" xref="S3.SS3.p5.6.m1.1.1.2">𝒄</ci><ci id="S3.SS3.p5.6.m1.1.1.3.cmml" xref="S3.SS3.p5.6.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.6.m1.1c">\bm{c}_{t}</annotation></semantics></math> is the attended visual feature; <math id="S3.SS3.p5.7.m2.1" class="ltx_Math" alttext="\bm{\alpha}_{t}=[\alpha_{ij}^{t}]" display="inline"><semantics id="S3.SS3.p5.7.m2.1a"><mrow id="S3.SS3.p5.7.m2.1.1" xref="S3.SS3.p5.7.m2.1.1.cmml"><msub id="S3.SS3.p5.7.m2.1.1.3" xref="S3.SS3.p5.7.m2.1.1.3.cmml"><mi id="S3.SS3.p5.7.m2.1.1.3.2" xref="S3.SS3.p5.7.m2.1.1.3.2.cmml">𝜶</mi><mi id="S3.SS3.p5.7.m2.1.1.3.3" xref="S3.SS3.p5.7.m2.1.1.3.3.cmml">t</mi></msub><mo id="S3.SS3.p5.7.m2.1.1.2" xref="S3.SS3.p5.7.m2.1.1.2.cmml">=</mo><mrow id="S3.SS3.p5.7.m2.1.1.1.1" xref="S3.SS3.p5.7.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p5.7.m2.1.1.1.1.2" xref="S3.SS3.p5.7.m2.1.1.1.2.1.cmml">[</mo><msubsup id="S3.SS3.p5.7.m2.1.1.1.1.1" xref="S3.SS3.p5.7.m2.1.1.1.1.1.cmml"><mi id="S3.SS3.p5.7.m2.1.1.1.1.1.2.2" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.2.cmml">α</mi><mrow id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.cmml"><mi id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.2" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.1" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.3" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.3.cmml">j</mi></mrow><mi id="S3.SS3.p5.7.m2.1.1.1.1.1.3" xref="S3.SS3.p5.7.m2.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S3.SS3.p5.7.m2.1.1.1.1.3" xref="S3.SS3.p5.7.m2.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.7.m2.1b"><apply id="S3.SS3.p5.7.m2.1.1.cmml" xref="S3.SS3.p5.7.m2.1.1"><eq id="S3.SS3.p5.7.m2.1.1.2.cmml" xref="S3.SS3.p5.7.m2.1.1.2"></eq><apply id="S3.SS3.p5.7.m2.1.1.3.cmml" xref="S3.SS3.p5.7.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p5.7.m2.1.1.3.1.cmml" xref="S3.SS3.p5.7.m2.1.1.3">subscript</csymbol><ci id="S3.SS3.p5.7.m2.1.1.3.2.cmml" xref="S3.SS3.p5.7.m2.1.1.3.2">𝜶</ci><ci id="S3.SS3.p5.7.m2.1.1.3.3.cmml" xref="S3.SS3.p5.7.m2.1.1.3.3">𝑡</ci></apply><apply id="S3.SS3.p5.7.m2.1.1.1.2.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p5.7.m2.1.1.1.2.1.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS3.p5.7.m2.1.1.1.1.1.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.7.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p5.7.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.7.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p5.7.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.2">𝛼</ci><apply id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3"><times id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.1"></times><ci id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.2.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><ci id="S3.SS3.p5.7.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p5.7.m2.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.7.m2.1c">\bm{\alpha}_{t}=[\alpha_{ij}^{t}]</annotation></semantics></math> is the attention map; and <math id="S3.SS3.p5.8.m3.1" class="ltx_Math" alttext="\bm{U}" display="inline"><semantics id="S3.SS3.p5.8.m3.1a"><mi id="S3.SS3.p5.8.m3.1.1" xref="S3.SS3.p5.8.m3.1.1.cmml">𝑼</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.8.m3.1b"><ci id="S3.SS3.p5.8.m3.1.1.cmml" xref="S3.SS3.p5.8.m3.1.1">𝑼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.8.m3.1c">\bm{U}</annotation></semantics></math>, <math id="S3.SS3.p5.9.m4.1" class="ltx_Math" alttext="\bm{W}_{v}" display="inline"><semantics id="S3.SS3.p5.9.m4.1a"><msub id="S3.SS3.p5.9.m4.1.1" xref="S3.SS3.p5.9.m4.1.1.cmml"><mi id="S3.SS3.p5.9.m4.1.1.2" xref="S3.SS3.p5.9.m4.1.1.2.cmml">𝑾</mi><mi id="S3.SS3.p5.9.m4.1.1.3" xref="S3.SS3.p5.9.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.9.m4.1b"><apply id="S3.SS3.p5.9.m4.1.1.cmml" xref="S3.SS3.p5.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.9.m4.1.1.1.cmml" xref="S3.SS3.p5.9.m4.1.1">subscript</csymbol><ci id="S3.SS3.p5.9.m4.1.1.2.cmml" xref="S3.SS3.p5.9.m4.1.1.2">𝑾</ci><ci id="S3.SS3.p5.9.m4.1.1.3.cmml" xref="S3.SS3.p5.9.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.9.m4.1c">\bm{W}_{v}</annotation></semantics></math>, <math id="S3.SS3.p5.10.m5.1" class="ltx_Math" alttext="\bm{W}_{z}" display="inline"><semantics id="S3.SS3.p5.10.m5.1a"><msub id="S3.SS3.p5.10.m5.1.1" xref="S3.SS3.p5.10.m5.1.1.cmml"><mi id="S3.SS3.p5.10.m5.1.1.2" xref="S3.SS3.p5.10.m5.1.1.2.cmml">𝑾</mi><mi id="S3.SS3.p5.10.m5.1.1.3" xref="S3.SS3.p5.10.m5.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.10.m5.1b"><apply id="S3.SS3.p5.10.m5.1.1.cmml" xref="S3.SS3.p5.10.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.10.m5.1.1.1.cmml" xref="S3.SS3.p5.10.m5.1.1">subscript</csymbol><ci id="S3.SS3.p5.10.m5.1.1.2.cmml" xref="S3.SS3.p5.10.m5.1.1.2">𝑾</ci><ci id="S3.SS3.p5.10.m5.1.1.3.cmml" xref="S3.SS3.p5.10.m5.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.10.m5.1c">\bm{W}_{z}</annotation></semantics></math>, <math id="S3.SS3.p5.11.m6.1" class="ltx_Math" alttext="\bm{p}" display="inline"><semantics id="S3.SS3.p5.11.m6.1a"><mi id="S3.SS3.p5.11.m6.1.1" xref="S3.SS3.p5.11.m6.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.11.m6.1b"><ci id="S3.SS3.p5.11.m6.1.1.cmml" xref="S3.SS3.p5.11.m6.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.11.m6.1c">\bm{p}</annotation></semantics></math> are the corresponding embedding weights.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.2" class="ltx_p">The attended visual feature <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\bm{c}_{t}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><msub id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">𝒄</mi><mi id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">𝒄</ci><ci id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\bm{c}_{t}</annotation></semantics></math> is further fused with the textual context <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="\bm{z}_{t}" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><msub id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml"><mi id="S3.SS3.p6.2.m2.1.1.2" xref="S3.SS3.p6.2.m2.1.1.2.cmml">𝒛</mi><mi id="S3.SS3.p6.2.m2.1.1.3" xref="S3.SS3.p6.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><apply id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.2.m2.1.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p6.2.m2.1.1.2.cmml" xref="S3.SS3.p6.2.m2.1.1.2">𝒛</ci><ci id="S3.SS3.p6.2.m2.1.1.3.cmml" xref="S3.SS3.p6.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">\bm{z}_{t}</annotation></semantics></math> via MLB, which can be interpreted as co-attention between vision and language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\bm{g}_{t}=\delta(\bm{U^{\prime}}\delta(\bm{W}_{c}\bm{c}_{t})\odot\delta(\bm{W}^{\prime}_{z}\bm{z}_{t}))," display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><msub id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.3.2.cmml">𝒈</mi><mi id="S3.E7.m1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.3.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝑼</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.4.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2a" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝑾</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝒄</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.2.cmml">⊙</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml">δ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.cmml"><msubsup id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.2.cmml">𝑾</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml">z</mi><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.3.cmml">′</mo></msubsup><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">​</mo><msub id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.2.cmml">𝒛</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><eq id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2"></eq><apply id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2">𝒈</ci><ci id="S3.E7.m1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E7.m1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.2"></times><ci id="S3.E7.m1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.3">𝛿</ci><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.3"></times><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.2">direct-product</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝑼</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3">bold-′</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.4">𝛿</ci><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑾</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝒄</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.3">𝛿</ci></apply><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.1"></times><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2">subscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.2">𝑾</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.3">′</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.2.3">𝑧</ci></apply><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.2">𝒛</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.1.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\bm{g}_{t}=\delta(\bm{U^{\prime}}\delta(\bm{W}_{c}\bm{c}_{t})\odot\delta(\bm{W}^{\prime}_{z}\bm{z}_{t})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p6.3" class="ltx_p">where <math id="S3.SS3.p6.3.m1.3" class="ltx_Math" alttext="\bm{U^{\prime}},\bm{W}_{c},\bm{W}^{\prime}_{z}" display="inline"><semantics id="S3.SS3.p6.3.m1.3a"><mrow id="S3.SS3.p6.3.m1.3.3.3" xref="S3.SS3.p6.3.m1.3.3.4.cmml"><msup id="S3.SS3.p6.3.m1.1.1.1.1" xref="S3.SS3.p6.3.m1.1.1.1.1.cmml"><mi id="S3.SS3.p6.3.m1.1.1.1.1.2" xref="S3.SS3.p6.3.m1.1.1.1.1.2.cmml">𝑼</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS3.p6.3.m1.1.1.1.1.3" xref="S3.SS3.p6.3.m1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.SS3.p6.3.m1.3.3.3.4" xref="S3.SS3.p6.3.m1.3.3.4.cmml">,</mo><msub id="S3.SS3.p6.3.m1.2.2.2.2" xref="S3.SS3.p6.3.m1.2.2.2.2.cmml"><mi id="S3.SS3.p6.3.m1.2.2.2.2.2" xref="S3.SS3.p6.3.m1.2.2.2.2.2.cmml">𝑾</mi><mi id="S3.SS3.p6.3.m1.2.2.2.2.3" xref="S3.SS3.p6.3.m1.2.2.2.2.3.cmml">c</mi></msub><mo id="S3.SS3.p6.3.m1.3.3.3.5" xref="S3.SS3.p6.3.m1.3.3.4.cmml">,</mo><msubsup id="S3.SS3.p6.3.m1.3.3.3.3" xref="S3.SS3.p6.3.m1.3.3.3.3.cmml"><mi id="S3.SS3.p6.3.m1.3.3.3.3.2.2" xref="S3.SS3.p6.3.m1.3.3.3.3.2.2.cmml">𝑾</mi><mi id="S3.SS3.p6.3.m1.3.3.3.3.3" xref="S3.SS3.p6.3.m1.3.3.3.3.3.cmml">z</mi><mo id="S3.SS3.p6.3.m1.3.3.3.3.2.3" xref="S3.SS3.p6.3.m1.3.3.3.3.2.3.cmml">′</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m1.3b"><list id="S3.SS3.p6.3.m1.3.3.4.cmml" xref="S3.SS3.p6.3.m1.3.3.3"><apply id="S3.SS3.p6.3.m1.1.1.1.1.cmml" xref="S3.SS3.p6.3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m1.1.1.1.1.1.cmml" xref="S3.SS3.p6.3.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p6.3.m1.1.1.1.1.2.cmml" xref="S3.SS3.p6.3.m1.1.1.1.1.2">𝑼</ci><ci id="S3.SS3.p6.3.m1.1.1.1.1.3.cmml" xref="S3.SS3.p6.3.m1.1.1.1.1.3">bold-′</ci></apply><apply id="S3.SS3.p6.3.m1.2.2.2.2.cmml" xref="S3.SS3.p6.3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m1.2.2.2.2.1.cmml" xref="S3.SS3.p6.3.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p6.3.m1.2.2.2.2.2.cmml" xref="S3.SS3.p6.3.m1.2.2.2.2.2">𝑾</ci><ci id="S3.SS3.p6.3.m1.2.2.2.2.3.cmml" xref="S3.SS3.p6.3.m1.2.2.2.2.3">𝑐</ci></apply><apply id="S3.SS3.p6.3.m1.3.3.3.3.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m1.3.3.3.3.1.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3">subscript</csymbol><apply id="S3.SS3.p6.3.m1.3.3.3.3.2.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m1.3.3.3.3.2.1.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3">superscript</csymbol><ci id="S3.SS3.p6.3.m1.3.3.3.3.2.2.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3.2.2">𝑾</ci><ci id="S3.SS3.p6.3.m1.3.3.3.3.2.3.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3.2.3">′</ci></apply><ci id="S3.SS3.p6.3.m1.3.3.3.3.3.cmml" xref="S3.SS3.p6.3.m1.3.3.3.3.3">𝑧</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m1.3c">\bm{U^{\prime}},\bm{W}_{c},\bm{W}^{\prime}_{z}</annotation></semantics></math> are embedding weights of the pooling.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.1" class="ltx_p"><span id="S3.SS3.p7.1.1" class="ltx_text ltx_font_bold">Word predictor</span> The next-word predictor is a softmax classifier, which generates a distribution over the next words, leveraging the multi-modal attention network’s output <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="\bm{g}_{t}" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><msub id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mi id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml">𝒈</mi><mi id="S3.SS3.p7.1.m1.1.1.3" xref="S3.SS3.p7.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2">𝒈</ci><ci id="S3.SS3.p7.1.m1.1.1.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">\bm{g}_{t}</annotation></semantics></math>:</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.2" class="ltx_Math" alttext="w_{t+1}\sim\operatorname{softmax}(\bm{W}_{o}\bm{g}_{t})," display="block"><semantics id="S3.E8.m1.2a"><mrow id="S3.E8.m1.2.2.1" xref="S3.E8.m1.2.2.1.1.cmml"><mrow id="S3.E8.m1.2.2.1.1" xref="S3.E8.m1.2.2.1.1.cmml"><msub id="S3.E8.m1.2.2.1.1.3" xref="S3.E8.m1.2.2.1.1.3.cmml"><mi id="S3.E8.m1.2.2.1.1.3.2" xref="S3.E8.m1.2.2.1.1.3.2.cmml">w</mi><mrow id="S3.E8.m1.2.2.1.1.3.3" xref="S3.E8.m1.2.2.1.1.3.3.cmml"><mi id="S3.E8.m1.2.2.1.1.3.3.2" xref="S3.E8.m1.2.2.1.1.3.3.2.cmml">t</mi><mo id="S3.E8.m1.2.2.1.1.3.3.1" xref="S3.E8.m1.2.2.1.1.3.3.1.cmml">+</mo><mn id="S3.E8.m1.2.2.1.1.3.3.3" xref="S3.E8.m1.2.2.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E8.m1.2.2.1.1.2" xref="S3.E8.m1.2.2.1.1.2.cmml">∼</mo><mrow id="S3.E8.m1.2.2.1.1.1.1" xref="S3.E8.m1.2.2.1.1.1.2.cmml"><mi id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml">softmax</mi><mo id="S3.E8.m1.2.2.1.1.1.1a" xref="S3.E8.m1.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E8.m1.2.2.1.1.1.1.1" xref="S3.E8.m1.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E8.m1.2.2.1.1.1.1.1.2" xref="S3.E8.m1.2.2.1.1.1.2.cmml">(</mo><mrow id="S3.E8.m1.2.2.1.1.1.1.1.1" xref="S3.E8.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E8.m1.2.2.1.1.1.1.1.1.2" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E8.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2.2.cmml">𝑾</mi><mi id="S3.E8.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.1.1.1.1.1.1.1" xref="S3.E8.m1.2.2.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E8.m1.2.2.1.1.1.1.1.1.3" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3.2.cmml">𝒈</mi><mi id="S3.E8.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.E8.m1.2.2.1.1.1.1.1.3" xref="S3.E8.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E8.m1.2.2.1.2" xref="S3.E8.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.2b"><apply id="S3.E8.m1.2.2.1.1.cmml" xref="S3.E8.m1.2.2.1"><csymbol cd="latexml" id="S3.E8.m1.2.2.1.1.2.cmml" xref="S3.E8.m1.2.2.1.1.2">similar-to</csymbol><apply id="S3.E8.m1.2.2.1.1.3.cmml" xref="S3.E8.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.1.3.1.cmml" xref="S3.E8.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.1.3.2.cmml" xref="S3.E8.m1.2.2.1.1.3.2">𝑤</ci><apply id="S3.E8.m1.2.2.1.1.3.3.cmml" xref="S3.E8.m1.2.2.1.1.3.3"><plus id="S3.E8.m1.2.2.1.1.3.3.1.cmml" xref="S3.E8.m1.2.2.1.1.3.3.1"></plus><ci id="S3.E8.m1.2.2.1.1.3.3.2.cmml" xref="S3.E8.m1.2.2.1.1.3.3.2">𝑡</ci><cn type="integer" id="S3.E8.m1.2.2.1.1.3.3.3.cmml" xref="S3.E8.m1.2.2.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E8.m1.2.2.1.1.1.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1"><ci id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1">softmax</ci><apply id="S3.E8.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1"><times id="S3.E8.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.1"></times><apply id="S3.E8.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2.2">𝑾</ci><ci id="S3.E8.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.2.3">𝑜</ci></apply><apply id="S3.E8.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3.2">𝒈</ci><ci id="S3.E8.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.2c">w_{t+1}\sim\operatorname{softmax}(\bm{W}_{o}\bm{g}_{t}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p7.3" class="ltx_p">where <math id="S3.SS3.p7.2.m1.1" class="ltx_Math" alttext="\bm{W}_{o}" display="inline"><semantics id="S3.SS3.p7.2.m1.1a"><msub id="S3.SS3.p7.2.m1.1.1" xref="S3.SS3.p7.2.m1.1.1.cmml"><mi id="S3.SS3.p7.2.m1.1.1.2" xref="S3.SS3.p7.2.m1.1.1.2.cmml">𝑾</mi><mi id="S3.SS3.p7.2.m1.1.1.3" xref="S3.SS3.p7.2.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m1.1b"><apply id="S3.SS3.p7.2.m1.1.1.cmml" xref="S3.SS3.p7.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m1.1.1.1.cmml" xref="S3.SS3.p7.2.m1.1.1">subscript</csymbol><ci id="S3.SS3.p7.2.m1.1.1.2.cmml" xref="S3.SS3.p7.2.m1.1.1.2">𝑾</ci><ci id="S3.SS3.p7.2.m1.1.1.3.cmml" xref="S3.SS3.p7.2.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m1.1c">\bm{W}_{o}</annotation></semantics></math> are the classifier weights. The next word <math id="S3.SS3.p7.3.m2.1" class="ltx_Math" alttext="w_{t+1}" display="inline"><semantics id="S3.SS3.p7.3.m2.1a"><msub id="S3.SS3.p7.3.m2.1.1" xref="S3.SS3.p7.3.m2.1.1.cmml"><mi id="S3.SS3.p7.3.m2.1.1.2" xref="S3.SS3.p7.3.m2.1.1.2.cmml">w</mi><mrow id="S3.SS3.p7.3.m2.1.1.3" xref="S3.SS3.p7.3.m2.1.1.3.cmml"><mi id="S3.SS3.p7.3.m2.1.1.3.2" xref="S3.SS3.p7.3.m2.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p7.3.m2.1.1.3.1" xref="S3.SS3.p7.3.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS3.p7.3.m2.1.1.3.3" xref="S3.SS3.p7.3.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.3.m2.1b"><apply id="S3.SS3.p7.3.m2.1.1.cmml" xref="S3.SS3.p7.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.3.m2.1.1.1.cmml" xref="S3.SS3.p7.3.m2.1.1">subscript</csymbol><ci id="S3.SS3.p7.3.m2.1.1.2.cmml" xref="S3.SS3.p7.3.m2.1.1.2">𝑤</ci><apply id="S3.SS3.p7.3.m2.1.1.3.cmml" xref="S3.SS3.p7.3.m2.1.1.3"><plus id="S3.SS3.p7.3.m2.1.1.3.1.cmml" xref="S3.SS3.p7.3.m2.1.1.3.1"></plus><ci id="S3.SS3.p7.3.m2.1.1.3.2.cmml" xref="S3.SS3.p7.3.m2.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS3.p7.3.m2.1.1.3.3.cmml" xref="S3.SS3.p7.3.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.3.m2.1c">w_{t+1}</annotation></semantics></math> is sampled from the softmax classifier’s distribution.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>iVQA Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We explore three of iVQA evaluation metrics including standard language-generation metrics, a new ranking-based metric, and a human validation study.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Linguistic Metrics</span> Standard linguistic measures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> including CIDEr, BLEU, METEOR and ROGUE-L can be used to evaluate the generated questions. Given an image, question, answer tuple, we use the ground-truth question as the reference sentence, and compare the generated question based on the given image and answer. The similarity between the machine generated questions and the reference questions can be measured by these metrics. Even though generating humanlike questions is relatively easy, doing so in a correct image+answer conditional way to get a high score is challenging, since the model has to capture all the semantic concepts and high-order interactions.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.3" class="ltx_p"><span id="S4.p3.3.1" class="ltx_text ltx_font_bold">Ranking Metric</span> We also develop a ranking based evaluation metric for the iVQA problem. For an image-answer pair <math id="S4.p3.1.m1.2" class="ltx_Math" alttext="(I,a)" display="inline"><semantics id="S4.p3.1.m1.2a"><mrow id="S4.p3.1.m1.2.3.2" xref="S4.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.p3.1.m1.2.3.2.1" xref="S4.p3.1.m1.2.3.1.cmml">(</mo><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">I</mi><mo id="S4.p3.1.m1.2.3.2.2" xref="S4.p3.1.m1.2.3.1.cmml">,</mo><mi id="S4.p3.1.m1.2.2" xref="S4.p3.1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S4.p3.1.m1.2.3.2.3" xref="S4.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.2b"><interval closure="open" id="S4.p3.1.m1.2.3.1.cmml" xref="S4.p3.1.m1.2.3.2"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝐼</ci><ci id="S4.p3.1.m1.2.2.cmml" xref="S4.p3.1.m1.2.2">𝑎</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.2c">(I,a)</annotation></semantics></math>, and a candidate question <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">q</annotation></semantics></math>, the conditioning score <math id="S4.p3.3.m3.4" class="ltx_Math" alttext="p(q|I,a;\Theta)" display="inline"><semantics id="S4.p3.3.m3.4a"><mrow id="S4.p3.3.m3.4.4" xref="S4.p3.3.m3.4.4.cmml"><mi id="S4.p3.3.m3.4.4.3" xref="S4.p3.3.m3.4.4.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.4.4.2" xref="S4.p3.3.m3.4.4.2.cmml">​</mo><mrow id="S4.p3.3.m3.4.4.1.1" xref="S4.p3.3.m3.4.4.1.1.1.cmml"><mo stretchy="false" id="S4.p3.3.m3.4.4.1.1.2" xref="S4.p3.3.m3.4.4.1.1.1.cmml">(</mo><mrow id="S4.p3.3.m3.4.4.1.1.1" xref="S4.p3.3.m3.4.4.1.1.1.cmml"><mi id="S4.p3.3.m3.4.4.1.1.1.2" xref="S4.p3.3.m3.4.4.1.1.1.2.cmml">q</mi><mo fence="false" id="S4.p3.3.m3.4.4.1.1.1.1" xref="S4.p3.3.m3.4.4.1.1.1.1.cmml">|</mo><mrow id="S4.p3.3.m3.4.4.1.1.1.3.2" xref="S4.p3.3.m3.4.4.1.1.1.3.1.cmml"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">I</mi><mo id="S4.p3.3.m3.4.4.1.1.1.3.2.1" xref="S4.p3.3.m3.4.4.1.1.1.3.1.cmml">,</mo><mi id="S4.p3.3.m3.2.2" xref="S4.p3.3.m3.2.2.cmml">a</mi><mo id="S4.p3.3.m3.4.4.1.1.1.3.2.2" xref="S4.p3.3.m3.4.4.1.1.1.3.1.cmml">;</mo><mi mathvariant="normal" id="S4.p3.3.m3.3.3" xref="S4.p3.3.m3.3.3.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S4.p3.3.m3.4.4.1.1.3" xref="S4.p3.3.m3.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.4b"><apply id="S4.p3.3.m3.4.4.cmml" xref="S4.p3.3.m3.4.4"><times id="S4.p3.3.m3.4.4.2.cmml" xref="S4.p3.3.m3.4.4.2"></times><ci id="S4.p3.3.m3.4.4.3.cmml" xref="S4.p3.3.m3.4.4.3">𝑝</ci><apply id="S4.p3.3.m3.4.4.1.1.1.cmml" xref="S4.p3.3.m3.4.4.1.1"><csymbol cd="latexml" id="S4.p3.3.m3.4.4.1.1.1.1.cmml" xref="S4.p3.3.m3.4.4.1.1.1.1">conditional</csymbol><ci id="S4.p3.3.m3.4.4.1.1.1.2.cmml" xref="S4.p3.3.m3.4.4.1.1.1.2">𝑞</ci><list id="S4.p3.3.m3.4.4.1.1.1.3.1.cmml" xref="S4.p3.3.m3.4.4.1.1.1.3.2"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝐼</ci><ci id="S4.p3.3.m3.2.2.cmml" xref="S4.p3.3.m3.2.2">𝑎</ci><ci id="S4.p3.3.m3.3.3.cmml" xref="S4.p3.3.m3.3.3">Θ</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.4c">p(q|I,a;\Theta)</annotation></semantics></math> is used for ranking. If one of the correct (ground truth) questions is ranked the highest then this image-answer pair is regard as correct at Rank-1. In this way, accuracy over a testing set can be computed as the percentage of the times that correct questions are ranked at the top (denoted Acc.@1). Similarly, we can measure cumulative ranking accuracy at other ranks, e.g., Acc.@3 measures the percentage of times correct questions are ranked in top 3. This is related to the
multiple-choice setting of VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, the difference is that we can explore specific choices of candidate question subsets to form a question pool in order to reveal insights about model strengths and weaknesses.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.7" class="ltx_p"><span id="S4.p4.7.1" class="ltx_text ltx_font_bold">Question Pool</span> For a particular image-answer pair, the candidate ranking questions are collected from the following subsets. <span id="S4.p4.7.2" class="ltx_text ltx_font_bold">Correct questions (GT)</span>: given image-answer pair <math id="S4.p4.1.m1.2" class="ltx_Math" alttext="(I,a)" display="inline"><semantics id="S4.p4.1.m1.2a"><mrow id="S4.p4.1.m1.2.3.2" xref="S4.p4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.p4.1.m1.2.3.2.1" xref="S4.p4.1.m1.2.3.1.cmml">(</mo><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">I</mi><mo id="S4.p4.1.m1.2.3.2.2" xref="S4.p4.1.m1.2.3.1.cmml">,</mo><mi id="S4.p4.1.m1.2.2" xref="S4.p4.1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S4.p4.1.m1.2.3.2.3" xref="S4.p4.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.2b"><interval closure="open" id="S4.p4.1.m1.2.3.1.cmml" xref="S4.p4.1.m1.2.3.2"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝐼</ci><ci id="S4.p4.1.m1.2.2.cmml" xref="S4.p4.1.m1.2.2">𝑎</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.2c">(I,a)</annotation></semantics></math>, the correct (ground truth) questions are defined as all the questions with answer <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.p4.2.m2.1a"><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">a</annotation></semantics></math> in image <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">I</annotation></semantics></math><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>There can be multiple correct questions corresponding to a given answer, since multiple questions may have the same answer for one image.</span></span></span>. <span id="S4.p4.7.3" class="ltx_text ltx_font_bold">Contrastive questions (CT)</span>: these are questions associated with visually similar images to <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.p4.4.m4.1a"><mi id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><ci id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">I</annotation></semantics></math> (including <math id="S4.p4.5.m5.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.p4.5.m5.1a"><mi id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">I</annotation></semantics></math>) but having different answers. The similarity of the images is measured using the image CNN feature. <span id="S4.p4.7.4" class="ltx_text ltx_font_bold">Plausible questions (PS):</span> These test whether the model can tell the subtle difference between questions and maintain grammar correctness. They are obtained by randomly replacing one of the key words (e.g., verbs, nouns, adjective, and adverb) in the ground truth question. <span id="S4.p4.7.5" class="ltx_text ltx_font_bold">Popular questions (PP):</span> Popular questions are chosen to be the most popular questions with the same answer type as <math id="S4.p4.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.p4.6.m6.1a"><mi id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><ci id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">a</annotation></semantics></math> across the whole dataset. These diagnose the extent to which the model is relying on label-bias. <span id="S4.p4.7.6" class="ltx_text ltx_font_bold">Answer-related (RN):</span> These are chosen to be the random questions having answer <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.p4.7.m7.1a"><mi id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><ci id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">a</annotation></semantics></math> but from other images. These diagnose the extent to which the model is relying on visual features, which did not always happen in VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. We manually checked all generated distractor questions, and removed any which were also correct for their corresponding image answer.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Human study</span>  iVQA is open-ended and one-to-many in that there can be many correct questions for one image and answer. Therefore, given an image and an answer, the correct questions may not be annotated exhaustively in existing datasets. The proposed ranking metric computed on the selected question pool ameliorates the effects of the open-ended/one-to-many questions generated by an iVQA model. However, it does not measure directly how ‘correct’ the generated questions are, when they differ from the human annotations originally provided. To evaluate iVQA in a way that awards ‘credit’ for correct questions that are not annotated originally, we perform a human evaluation study. Image-answer pairs are randomly selected from the test set, and annotators assess the generated questions, scoring them from 0 (complete nonsense) to 4 (perfect). The mean score is used as the metric.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets and settings</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset:</span>  We repurpose the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to investigate the iVQA task. The VQA dataset uses images from MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, including 82,783 training, 40,504 validation and 81,434 test images. Three question-answer pairs are collected for each image. Since the test set answers are not available, we adopt the commonly used off-line data split in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for image captioning: 82,783 images are used for training, and 5,000 for validation and test each.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Training:</span>  The model is trained by minimising the cross entropy loss between machine generated questions and ground truth questions. The Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> optimiser is employed with a batch size 32 for 30 epochs. The initial learning rate is set to be 5e-4, and it is annealed 0.83 times per epoch with an exponential decay.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Inference:</span>  Beam search with max sentence length 20 is used to generate questions. We use beam size 3 for quantitative and 10 for qualitative results.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Question Pool:</span>  For the proposed ranking accuracy metric, given each image-answer pair, the question pool contains 24 questions, of which 1-3 are GT, 3-5 are CT (so that the total of GT+CT is 6), 6 are PP, 6 are PS, and 6 are RN .</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Baseline models</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Answer only (A):</span>  It uses a LSTM encoder to encode tokenised answers to a fixed 512-dimensional representation, then a LSTM decoder to generate questions.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Image only (I, VQG):</span>  The visual only model is similar to the GRNN model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, however, we use a more powerful image feature: the same <code id="S5.SS2.p2.1.2" class="ltx_verbatim ltx_font_typewriter">res5c</code> feature of ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> used in our model. This feature is fed into a LSTM decoder as the initial state.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Image+Answer Type (I+AT):</span>  VQG models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> generate questions purely based on visual cues. To make VQG more competitive in our answer-conditional iVQA setting, we also provide one-hot encoding of the answer <em id="S5.SS2.p3.1.2" class="ltx_emph ltx_font_italic">type</em>. This hint helps a VQG model generate the right question type (e.g.,‘is…’, ‘what…’).</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">NN:</span>  We adapt the nearest neighbour (NN) image captioning method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to our problem. As iVQA is conditioned on both image and answer, we averaged the distance computed from both modalities for NN computation.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">SAT:</span>  Show attend and tell (SAT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> is a strong attentional captioning method. To provide a strong competitor to our approach, we modify SAT to take input from both modalities by setting the initial state of the decoding LSTM as the joint embedding of image and answer.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">VQG+VQA:</span>  The VQG+VQA baseline uses the VQG model above to generate question proposals from the image, and then uses VQA to select the question with maximum conditioning score. We use VQG to generate 10 candidates for each image for VQA re-ranking, and the retrained multi-modal low-rank bilinear attention network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is used as the VQA model.
<br class="ltx_break"><span id="S5.SS2.p6.1.2" class="ltx_text ltx_font_bold">Ours:</span>  Our model processes images with local and global semantic features, and dynamic multi-modal attention (I+A+Att+<math id="S5.SS2.p6.1.m1.1" class="ltx_Math" alttext="I_{s}" display="inline"><semantics id="S5.SS2.p6.1.m1.1a"><msub id="S5.SS2.p6.1.m1.1.1" xref="S5.SS2.p6.1.m1.1.1.cmml"><mi id="S5.SS2.p6.1.m1.1.1.2" xref="S5.SS2.p6.1.m1.1.1.2.cmml">I</mi><mi id="S5.SS2.p6.1.m1.1.1.3" xref="S5.SS2.p6.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><apply id="S5.SS2.p6.1.m1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.1.m1.1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p6.1.m1.1.1.2.cmml" xref="S5.SS2.p6.1.m1.1.1.2">𝐼</ci><ci id="S5.SS2.p6.1.m1.1.1.3.cmml" xref="S5.SS2.p6.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">I_{s}</annotation></semantics></math>). The global semantic feature is obtained following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> by learning a concept predictor on the training split using the 1,000 most frequent caption words.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/1710.03370/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_img_landscape" width="502" height="229" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualitative results of iVQA. Larger numbers in brackets mean higher confidence. The attention of generating the questions in purple is further visualised in Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</figcaption>
</figure>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.2.1" class="ltx_text" style="font-size:80%;">CIDEr</span></td>
<td id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.3.1" class="ltx_text" style="font-size:80%;">BLEU-4</span></td>
<td id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.4.1" class="ltx_text" style="font-size:80%;">BLEU-3</span></td>
<td id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.5.1" class="ltx_text" style="font-size:80%;">BLEU-2</span></td>
<td id="S5.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.6.1" class="ltx_text" style="font-size:80%;">BLEU-1</span></td>
<td id="S5.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.7.1" class="ltx_text" style="font-size:80%;">ROUGE-L</span></td>
<td id="S5.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.8.1" class="ltx_text" style="font-size:80%;">METEOR</span></td>
<td id="S5.T1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.9.1" class="ltx_text" style="font-size:80%;">Acc@1</span></td>
<td id="S5.T1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.10.1" class="ltx_text" style="font-size:80%;">Acc@3</span></td>
<td id="S5.T1.1.1.1.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.11.1" class="ltx_text" style="font-size:80%;">Human</span></td>
</tr>
<tr id="S5.T1.1.2.2" class="ltx_tr">
<td id="S5.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.1.1" class="ltx_text" style="font-size:80%;">A</span></td>
<td id="S5.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.2.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.2.2.3.1" class="ltx_text" style="font-size:80%;">0.146</span></td>
<td id="S5.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.2.2.4.1" class="ltx_text" style="font-size:80%;">0.192</span></td>
<td id="S5.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.2.2.5.1" class="ltx_text" style="font-size:80%;">0.265</span></td>
<td id="S5.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.6.1" class="ltx_text" style="font-size:80%;">0.371</span></td>
<td id="S5.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.7.1" class="ltx_text" style="font-size:80%;">0.408</span></td>
<td id="S5.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.8.1" class="ltx_text" style="font-size:80%;">0.161</span></td>
<td id="S5.T1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.2.2.9.1" class="ltx_text" style="font-size:80%;">14.589</span></td>
<td id="S5.T1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.10.1" class="ltx_text" style="font-size:80%;">28.795</span></td>
<td id="S5.T1.1.2.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T1.1.2.2.11.1" class="ltx_text" style="font-size:80%;">1.92</span></td>
</tr>
<tr id="S5.T1.1.3.3" class="ltx_tr">
<td id="S5.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T1.1.3.3.1.1" class="ltx_text" style="font-size:80%;">I</span></td>
<td id="S5.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.3.3.2.1" class="ltx_text" style="font-size:80%;">0.652</span></td>
<td id="S5.T1.1.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.3.3.3.1" class="ltx_text" style="font-size:80%;">0.086</span></td>
<td id="S5.T1.1.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.3.3.4.1" class="ltx_text" style="font-size:80%;">0.121</span></td>
<td id="S5.T1.1.3.3.5" class="ltx_td ltx_align_center"><span id="S5.T1.1.3.3.5.1" class="ltx_text" style="font-size:80%;">0.179</span></td>
<td id="S5.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.3.3.6.1" class="ltx_text" style="font-size:80%;">0.280</span></td>
<td id="S5.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.3.3.7.1" class="ltx_text" style="font-size:80%;">0.310</span></td>
<td id="S5.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.3.3.8.1" class="ltx_text" style="font-size:80%;">0.117</span></td>
<td id="S5.T1.1.3.3.9" class="ltx_td ltx_align_center"><span id="S5.T1.1.3.3.9.1" class="ltx_text" style="font-size:80%;">13.012</span></td>
<td id="S5.T1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.3.3.10.1" class="ltx_text" style="font-size:80%;">28.644</span></td>
<td id="S5.T1.1.3.3.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.3.3.11.1" class="ltx_text" style="font-size:80%;">2.04</span></td>
</tr>
<tr id="S5.T1.1.4.4" class="ltx_tr">
<td id="S5.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T1.1.4.4.1.1" class="ltx_text" style="font-size:80%;">I+AT</span></td>
<td id="S5.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.4.4.2.1" class="ltx_text" style="font-size:80%;">0.904</span></td>
<td id="S5.T1.1.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.4.4.3.1" class="ltx_text" style="font-size:80%;">0.122</span></td>
<td id="S5.T1.1.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.4.4.4.1" class="ltx_text" style="font-size:80%;">0.164</span></td>
<td id="S5.T1.1.4.4.5" class="ltx_td ltx_align_center"><span id="S5.T1.1.4.4.5.1" class="ltx_text" style="font-size:80%;">0.234</span></td>
<td id="S5.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.4.4.6.1" class="ltx_text" style="font-size:80%;">0.350</span></td>
<td id="S5.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.4.4.7.1" class="ltx_text" style="font-size:80%;">0.397</span></td>
<td id="S5.T1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.4.4.8.1" class="ltx_text" style="font-size:80%;">0.151</span></td>
<td id="S5.T1.1.4.4.9" class="ltx_td ltx_align_center"><span id="S5.T1.1.4.4.9.1" class="ltx_text" style="font-size:80%;">20.277</span></td>
<td id="S5.T1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.4.4.10.1" class="ltx_text" style="font-size:80%;">36.134</span></td>
<td id="S5.T1.1.4.4.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.4.4.11.1" class="ltx_text" style="font-size:80%;">2.65</span></td>
</tr>
<tr id="S5.T1.1.5.5" class="ltx_tr">
<td id="S5.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T1.1.5.5.1.1" class="ltx_text" style="font-size:80%;">NN</span></td>
<td id="S5.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.5.2.1" class="ltx_text" style="font-size:80%;">1.372</span></td>
<td id="S5.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.5.5.3.1" class="ltx_text" style="font-size:80%;">0.175</span></td>
<td id="S5.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.5.5.4.1" class="ltx_text" style="font-size:80%;">0.223</span></td>
<td id="S5.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.5.5.5.1" class="ltx_text" style="font-size:80%;">0.294</span></td>
<td id="S5.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.5.6.1" class="ltx_text" style="font-size:80%;">0.404</span></td>
<td id="S5.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.5.7.1" class="ltx_text" style="font-size:80%;">0.428</span></td>
<td id="S5.T1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.5.8.1" class="ltx_text" style="font-size:80%;">0.183</span></td>
<td id="S5.T1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.5.5.9.1" class="ltx_text" style="font-size:80%;">26.783</span></td>
<td id="S5.T1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.5.10.1" class="ltx_text" style="font-size:80%;">48.755</span></td>
<td id="S5.T1.1.5.5.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T1.1.5.5.11.1" class="ltx_text" style="font-size:80%;">3.01</span></td>
</tr>
<tr id="S5.T1.1.6.6" class="ltx_tr">
<td id="S5.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T1.1.6.6.1.1" class="ltx_text" style="font-size:80%;">SAT</span></td>
<td id="S5.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.6.6.2.1" class="ltx_text" style="font-size:80%;">1.533</span></td>
<td id="S5.T1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.6.6.3.1" class="ltx_text" style="font-size:80%;">0.192</span></td>
<td id="S5.T1.1.6.6.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.6.6.4.1" class="ltx_text" style="font-size:80%;">0.241</span></td>
<td id="S5.T1.1.6.6.5" class="ltx_td ltx_align_center"><span id="S5.T1.1.6.6.5.1" class="ltx_text" style="font-size:80%;">0.311</span></td>
<td id="S5.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.6.6.6.1" class="ltx_text" style="font-size:80%;">0.417</span></td>
<td id="S5.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.6.6.7.1" class="ltx_text" style="font-size:80%;">0.456</span></td>
<td id="S5.T1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.6.6.8.1" class="ltx_text" style="font-size:80%;">0.195</span></td>
<td id="S5.T1.1.6.6.9" class="ltx_td ltx_align_center"><span id="S5.T1.1.6.6.9.1" class="ltx_text" style="font-size:80%;">29.722</span></td>
<td id="S5.T1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.6.6.10.1" class="ltx_text" style="font-size:80%;">48.118</span></td>
<td id="S5.T1.1.6.6.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.6.6.11.1" class="ltx_text" style="font-size:80%;">3.19</span></td>
</tr>
<tr id="S5.T1.1.7.7" class="ltx_tr">
<td id="S5.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T1.1.7.7.1.1" class="ltx_text" style="font-size:80%;">VQG+VQA</span></td>
<td id="S5.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.7.7.2.1" class="ltx_text" style="font-size:80%;">1.110</span></td>
<td id="S5.T1.1.7.7.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.7.7.3.1" class="ltx_text" style="font-size:80%;">0.147</span></td>
<td id="S5.T1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.7.7.4.1" class="ltx_text" style="font-size:80%;">0.193</span></td>
<td id="S5.T1.1.7.7.5" class="ltx_td ltx_align_center"><span id="S5.T1.1.7.7.5.1" class="ltx_text" style="font-size:80%;">0.261</span></td>
<td id="S5.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.7.7.6.1" class="ltx_text" style="font-size:80%;">0.371</span></td>
<td id="S5.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.7.7.7.1" class="ltx_text" style="font-size:80%;">0.396</span></td>
<td id="S5.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.7.7.8.1" class="ltx_text" style="font-size:80%;">0.165</span></td>
<td id="S5.T1.1.7.7.9" class="ltx_td ltx_align_center"><span id="S5.T1.1.7.7.9.1" class="ltx_text" style="font-size:80%;">16.529</span></td>
<td id="S5.T1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.7.7.10.1" class="ltx_text" style="font-size:80%;">41.655</span></td>
<td id="S5.T1.1.7.7.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.7.7.11.1" class="ltx_text" style="font-size:80%;">2.79</span></td>
</tr>
<tr id="S5.T1.1.8.8" class="ltx_tr">
<td id="S5.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.1.8.8.1.1" class="ltx_text" style="font-size:80%;">Ours</span></td>
<td id="S5.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.1.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1.714</span></td>
<td id="S5.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.1.8.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.208</span></td>
<td id="S5.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.1.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.256</span></td>
<td id="S5.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.1.8.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.326</span></td>
<td id="S5.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.1.8.8.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.430</span></td>
<td id="S5.T1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.1.8.8.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.468</span></td>
<td id="S5.T1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.1.8.8.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.205</span></td>
<td id="S5.T1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.1.8.8.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">32.899</span></td>
<td id="S5.T1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.1.8.8.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">51.418</span></td>
<td id="S5.T1.1.8.8.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.1.8.8.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">3.31</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overall question generation performance on the testing set.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure">
<table id="S5.F4.7.7" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F4.7.7.7" class="ltx_tr">
<td id="S5.F4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><img src="/html/1710.03370/assets/x4.png" id="S5.F4.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="73" height="81" alt="Refer to caption"></td>
<td id="S5.F4.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><img src="/html/1710.03370/assets/x5.png" id="S5.F4.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="73" height="81" alt="Refer to caption"></td>
<td id="S5.F4.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><img src="/html/1710.03370/assets/x6.png" id="S5.F4.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="73" height="81" alt="Refer to caption"></td>
<td id="S5.F4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><img src="/html/1710.03370/assets/x7.png" id="S5.F4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="73" height="81" alt="Refer to caption"></td>
<td id="S5.F4.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><img src="/html/1710.03370/assets/x8.png" id="S5.F4.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="73" height="81" alt="Refer to caption"></td>
<td id="S5.F4.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><img src="/html/1710.03370/assets/x9.png" id="S5.F4.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="73" height="81" alt="Refer to caption"></td>
<td id="S5.F4.7.7.7.7" class="ltx_td ltx_align_left"><img src="/html/1710.03370/assets/x10.png" id="S5.F4.7.7.7.7.g1" class="ltx_graphics ltx_img_portrait" width="23" height="84" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of effects of different distractors on different models on the test set.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.F5.12.12" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F5.6.6.6" class="ltx_tr">
<td id="S5.F5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1710.03370/assets/figs/tie_attention/01.jpg" id="S5.F5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption"></td>
<td id="S5.F5.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.2.2.2.2.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/tie_attention/02.jpg" id="S5.F5.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.3.3.3.3.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/tie_attention/03.jpg" id="S5.F5.3.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.4.4.4.4.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/tie_attention/04.jpg" id="S5.F5.4.4.4.4.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.5.5.5.5.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/tie_attention/05.jpg" id="S5.F5.5.5.5.5.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.6.6.6.6.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/tie_attention/06.jpg" id="S5.F5.6.6.6.6.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
</tr>
<tr id="S5.F5.12.12.13.1" class="ltx_tr">
<td id="S5.F5.12.12.13.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.13.1.1.1" class="ltx_text" style="font-size:70%;">what</span></td>
<td id="S5.F5.12.12.13.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.13.1.2.1" class="ltx_text" style="font-size:70%;">is</span></td>
<td id="S5.F5.12.12.13.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.13.1.3.1" class="ltx_text" style="font-size:70%;">the</span></td>
<td id="S5.F5.12.12.13.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.13.1.4.1" class="ltx_text" style="font-size:70%;">bear</span></td>
<td id="S5.F5.12.12.13.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.13.1.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">wearing</span></td>
<td id="S5.F5.12.12.13.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.13.1.6.1" class="ltx_text" style="font-size:70%;">?</span></td>
</tr>
<tr id="S5.F5.12.12.12" class="ltx_tr">
<td id="S5.F5.7.7.7.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1710.03370/assets/figs/couch_attention/01.jpg" id="S5.F5.7.7.7.1.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption"></td>
<td id="S5.F5.8.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.8.8.8.2.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/couch_attention/02.jpg" id="S5.F5.8.8.8.2.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.9.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.9.9.9.3.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/couch_attention/03.jpg" id="S5.F5.9.9.9.3.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.10.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.10.10.10.4.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/couch_attention/04.jpg" id="S5.F5.10.10.10.4.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.11.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.11.11.11.5.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/couch_attention/05.jpg" id="S5.F5.11.11.11.5.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
<td id="S5.F5.12.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="S5.F5.12.12.12.6.1" class="ltx_text" style="font-size:70%;"> </span><img src="/html/1710.03370/assets/figs/couch_attention/06.jpg" id="S5.F5.12.12.12.6.g1" class="ltx_graphics ltx_img_landscape" width="96" height="72" alt="Refer to caption">
</td>
</tr>
<tr id="S5.F5.12.12.14.2" class="ltx_tr">
<td id="S5.F5.12.12.14.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.14.2.1.1" class="ltx_text" style="font-size:70%;">what</span></td>
<td id="S5.F5.12.12.14.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.14.2.2.1" class="ltx_text" style="font-size:70%;">is</span></td>
<td id="S5.F5.12.12.14.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.14.2.3.1" class="ltx_text" style="font-size:70%;">the</span></td>
<td id="S5.F5.12.12.14.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.14.2.4.1" class="ltx_text" style="font-size:70%;">bear</span></td>
<td id="S5.F5.12.12.14.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.14.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">sitting</span></td>
<td id="S5.F5.12.12.14.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.12.12.14.2.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">on</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Dynamic attention maps generated by the proposed model. Input answer: ‘<em id="S5.F5.19.1" class="ltx_emph ltx_font_italic">Bowtie</em>’ (top), ‘<em id="S5.F5.20.2" class="ltx_emph ltx_font_italic">Couch</em>’ (bottom). Because the conditioning answer is different the model generates totally different attention maps in producing the output question.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F5.21" class="ltx_p ltx_figure_panel"><span id="S5.F5.21.1" class="ltx_text" style="font-size:70%;">.</span></p>
</div>
</div>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">Overall</span>  In the first experiment we report the overall iVQA performance on the test split. The results are shown in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with both the standard linguistic metrics, as well as our ranking accuracy metric. From the table, we can make the following observations: (i) Unlike VQA the margin between the no-image case (A), and the full model (Ours) is dramatic. The ranking accuracies are more than doubled, and the language metrics show similarly striking improvements. This demonstrates that unlike conventional VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the ‘V’ does matter in iVQA.  (ii) The margin between the image + answer type (I+AT) and image-only (I) setting exists, but is less significant. This shows that while it is a useful hint for an iVQA model to know the question type, it still really needs the actual semantic answer to generate the right questions. E.g., rather than just knowing that it was counting something (answer type), the model does need to know <em id="S5.SS3.p1.1.2" class="ltx_emph ltx_font_italic">how many objects were counted</em> (answer) in order to generate the right question specifying what object type needs to be counted – as there may be other objects that could be counted.
(iii) The margins between (I) and (I+AT) and the full model are also striking. This demonstrates that as a test of multi-modal intelligence, iVQA reassuringly requires both modalities in order to do well.
(iv) VQG+VQA indeed performs better than the vanilla VQG model by making the generated question more answer conditional, but it is still weaker than the captioning adapted models (NN and SAT) or our proposed model. The reason is that <span id="S5.SS3.p1.1.3" class="ltx_text">the VQG model has too low <span id="S5.SS3.p1.1.3.1" class="ltx_text ltx_font_italic" style="color:#000000;">sensitivity</span>, thus is unable to provide the right question candidates for VQA model to select. Also, VQA model can be easily fooled by similar questions not relevant to the image because VQA models are trained to distinguish different answers rather than questions.<span id="S5.SS3.p1.1.3.2" class="ltx_text">
(v) The captioning adapted models (NN and SAT) perform well, but are still inferior to the proposed model which is specifically designed for iVQA.</span></span></p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Human study</span>  The human study is applied on a subset of 1000 samples, and evaluates the models in a way that is robust to open-ended question generation. Results in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> show that (i) our model performs the best among all competitors; (ii) the human study scores are highly correlated with the proposed ranking metric. Specifically, the Pearson correlation coefficient between manually labelled scores and the proposed acc@1 and acc@3 metrics are 0.917 and 0.981 separately, while the best performing linguistic measure (CIDEr) can only reach 0.898, which demonstrates the effectiveness of the proposed ranking metric. Since human evaluation is expensive, the proposed ranking metric is a reasonable and cost-effective alternative.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Qualitative Results</span>  Examples of questions generated by our model are shown in Fig. <a href="#S5.F3" title="Figure 3 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The results illustrate a few interesting points: (i) The generated questions are highly conditional on both images and answers. Particularly, the same answers generate different questions for different images, unlike the situation in VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>; and the same images generate very different questions when paired with different answers, showing richer reasoning than in VQG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. (ii) Unlike VQA, there are multiple reasonable questions that correspond to one image-answer pair. This is both due to alternative phrasing of the same question (‘<em id="S5.SS3.p3.1.2" class="ltx_emph ltx_font_italic">where is the bear?</em>’,’<em id="S5.SS3.p3.1.3" class="ltx_emph ltx_font_italic">where is the teddy bear?</em>’,‘<em id="S5.SS3.p3.1.4" class="ltx_emph ltx_font_italic">what is the teddy bear sitting on?</em>’), as well as multiple semantically distinct questions having the same answer (e.g., ‘<em id="S5.SS3.p3.1.5" class="ltx_emph ltx_font_italic">are the children eating?</em>’,‘<em id="S5.SS3.p3.1.6" class="ltx_emph ltx_font_italic">is the child wearing a shirt?</em>’,‘<em id="S5.SS3.p3.1.7" class="ltx_emph ltx_font_italic">is the child wearing a hat?</em>’). Since the annotation is not exhaustive, the standard linguistic metrics could be misleading: the generated questions can be correct but just have never been asked by the annotators. Our proposed ranking metric is more robust to this, as models are only scored according to how plausibly they rate the true question, rather than whether their open-world estimate of the question matches annotated ground-truth. Our human study evaluates the methods in a way that credits open-world question generation. The open-ended question generation formulation of our model means it is straightforward to <em id="S5.SS3.p3.1.8" class="ltx_emph ltx_font_italic">sample</em> the distribution over questions given images and answers, in order to explore the model’s beliefs.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Further analysis</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.2" class="ltx_p"><span id="S5.SS4.p1.2.1" class="ltx_text ltx_font_bold">Analysis by Failure Type</span> 
The proposed new evaluation metric enables us to understand the mistakes each model makes. The results in Fig. <a href="#S5.F4" title="Figure 4 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show the Rank-1 predictions of each model (highest scoring question) broken down according to the category of that prediction. It again shows the much superior performance of our iVQA model (I+A+Att+I<sub id="S5.SS4.p1.2.2" class="ltx_sub"><span id="S5.SS4.p1.2.2.1" class="ltx_text ltx_font_italic">s</span></sub>): 32.67% of the top ranked prediction is correct, which almost doubles that of the VQA model (16.4%). But the main objective here is to analyse which distractor types are mistakenly ranked high:
(i) Without being able to condition on the answers, the image-only method (I) makes predictions dominated by contrastive (CT) distractor questions taken from similar looking images.
(ii) The answer-only (A) and image+answer type (I+AT) methods make predictions that are dominated by distractor questions of the popular (PP) type. This suggests that these models are trying to rely on unconditional distribution of label statistics in order to solve iVQA.
(iii) The VQA-based baseline instead is dominated by answer-related (RN) distractor. This suggests that the VQA model fails to correctly take into account image context (the ‘V’ is not being accounted for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>), and is simply picking questions that generate the corresponding answer independently of the required conditioning on image context.
(iv) The nearest neighbour (NN) approach performs well and has an evenly distributed set of error types, but it is weaker than the proposed in correctly capturing the visual and answer conditions. It is reflected on the larger portion of CT and RN errors.
(v) Our full model (I+A+Att+I<sub id="S5.SS4.p1.2.3" class="ltx_sub"><span id="S5.SS4.p1.2.3.1" class="ltx_text ltx_font_italic">s</span></sub>) has the largest fraction of correct predictions and manage to suppress the plausible (PS) and contrastive (CT) errors. It still makes some mistakes of ranking the popular (PP) questions at the top, but still fewer than answer (A) and image+answer type (I+AT) alternatives.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Ablation study</span> 
We evaluate the contributions of our key technical components: dynamic attention, and multi-modal inference with both local and global semantic image features. The results in Table <a href="#S5.T2" title="Table 2 ‣ 5.4 Further analysis ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> verify that each of these components contributes to the final result, and the biggest contribution comes from the proposed dynamic attention model.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.2.1" class="ltx_tr">
<th id="S5.T2.1.1.2.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">CIDEr</span></th>
<th id="S5.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T2.1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">BLEU-4</span></th>
<th id="S5.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.2.1.4.1" class="ltx_text" style="font-size:80%;">Acc@1</span></th>
<th id="S5.T2.1.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.2.1.5.1" class="ltx_text" style="font-size:80%;">Acc@3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.3.1" class="ltx_tr">
<th id="S5.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T2.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">I+A</span></th>
<td id="S5.T2.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.3.1.2.1" class="ltx_text" style="font-size:80%;">1.541</span></td>
<td id="S5.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.1.1.3.1.3.1" class="ltx_text" style="font-size:80%;">0.196</span></td>
<td id="S5.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.3.1.4.1" class="ltx_text" style="font-size:80%;">29.929</span></td>
<td id="S5.T2.1.1.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T2.1.1.3.1.5.1" class="ltx_text" style="font-size:80%;">48.356</span></td>
</tr>
<tr id="S5.T2.1.1.4.2" class="ltx_tr">
<th id="S5.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">I+Att+A</span></th>
<td id="S5.T2.1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.4.2.2.1" class="ltx_text" style="font-size:80%;">1.698</span></td>
<td id="S5.T2.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.1.4.2.3.1" class="ltx_text" style="font-size:80%;">0.208</span></td>
<td id="S5.T2.1.1.4.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.4.2.4.1" class="ltx_text" style="font-size:80%;">32.547</span></td>
<td id="S5.T2.1.1.4.2.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.4.2.5.1" class="ltx_text" style="font-size:80%;">51.024</span></td>
</tr>
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">
<span id="S5.T2.1.1.1.1.1" class="ltx_text" style="font-size:80%;">I+Att+A+I</span><sub id="S5.T2.1.1.1.1.2" class="ltx_sub"><span id="S5.T2.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">s</span></sub>
</th>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1.750</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.214</span></td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">33.636</span></td>
<td id="S5.T2.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S5.T2.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">52.344</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study on the contributions of key model components. The results are obtained on the validation set.</figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">How dynamic attention helps</span>  To illustrate our dynamic attention in iVQA, we visualise attention maps computed during question generation in Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. From the examples, we see that focus of attention varies over time in an appropriate way according to the partially generated question as well as the answer. For example, Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.2 Baseline models ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that the model focuses on regions on and below the bear at the point of generating the words ‘<em id="S5.SS4.p3.1.2" class="ltx_emph ltx_font_italic">on</em>’ and ‘<em id="S5.SS4.p3.1.3" class="ltx_emph ltx_font_italic">wearing</em>’ respectively. These examples also demonstrate that the learned iVQA model has achieved some degree of multi-modal visuo-linguistic understanding and implicit reasoning capability.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para ltx_noindent">
<p id="S5.SS4.p4.1" class="ltx_p"><span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_bold">Can iVQA help VQA?</span> 
The problem of iVQA can be seen as a dual problem of VQA, so we investigate whether VQA performance can be boosted by applying iVQA to obtain a second opinion. Specifically, we conduct the experiments on the more challenging VQA 2.0 dataset and MLB-att <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> trained on the training set is employed as the VQA model. During testing, the top 3 answers with the largest VQA score are served as answer candidates, then a second score is computed from the iVQA model. They are further combined by a score fusion network, whose output is utilised as the confidence of final prediction. Before the VQA-iVQA fusion, the VQA model alone can achieve a validation accuracy of 57.85, while after the final model reaches an accuracy of 58.86, where the performance gain is mainly from the challenging number type (improved from 34.94 to 38.71). These results thus show that iVQA can indeed assist VQA. Actually iVQA can also be used as a diagnosis tool to extract the belief set of an existing VQA model, which is part of the ongoing work.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para ltx_noindent">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_bold">Contrasting VQA and iVQA as benchmarks</span>  Finally, we discuss iVQA’s interest as a benchmark compared to the conventional VQA. Two of the main kinds of bias that a VQA/iVQA model could use to cheat the benchmark are the output <em id="S5.SS4.p5.1.2" class="ltx_emph ltx_font_italic">Prior bias</em> (Ignore both inputs and predict only the most likely answer on VQA; use question frequency in iVQA), and <em id="S5.SS4.p5.1.3" class="ltx_emph ltx_font_italic">Language bias</em> (ignore the image and use only the input – question for VQA, answer for iVQA – to predict the output). A good multimodal intelligence benchmark should require understanding and mutual grounding of both modalities, and should be hard to game by exploiting those biases. To analyses these issues we compare performance on iVQA and VQA benchmarks using Prior-alone and Language-alone (LSTM Q for VQA and Answer only for iVQA) baselines versus the full multi-modal model in each case (DeeperLSTM+Norm I for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and I+A model for iVQA).
The results in Table <a href="#S5.T3" title="Table 3 ‣ 5.4 Further analysis ‣ 5 Experiments ‣ iVQA: Inverse Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show that for VQA the bias-based baselines approach the performance of a full multi-modal model much more closely than the corresponding baselines do for iVQA. This suggests that VQA is easier to ‘game’ (achieve an apparently high score without any image understanding or multimodal grounding), compared to iVQA. Thus we propose that iVQA makes a distinct and interesting benchmark for multimodal intelligence.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">split</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Prior</span></th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Language</span></th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Language+Visual</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.1.1" class="ltx_text" style="font-size:80%;">iVQA (acc@1)</span></th>
<th id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.2.1" class="ltx_text" style="font-size:80%;">test</span></th>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.1.3.1" class="ltx_text" style="font-size:80%;">3.94</span></td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.1.4.1" class="ltx_text" style="font-size:80%;">14.59</span></td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T3.1.2.1.5.1" class="ltx_text" style="font-size:80%;">28.44</span></td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">VQA (accuracy)</span></th>
<th id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">test-dev</span></th>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.3.2.3.1" class="ltx_text" style="font-size:80%;">29.66</span></td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.3.2.4.1" class="ltx_text" style="font-size:80%;">48.76</span></td>
<td id="S5.T3.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S5.T3.1.3.2.5.1" class="ltx_text" style="font-size:80%;">57.75</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>VQA vs. iVQA on bias-based gameability.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We have introduced the novel task of inverse VQA as an alternative multi-modal visual intelligence challenge to the popular VQA paradigm. The analyses suggest that iVQA is appealing in terms of being less game-able via exploiting label-bias, more clearly requiring the mutual grounding and understanding of both visual and linguistic modalities, and naturally providing an open-world prediction setting.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements:</span> This project received support from Natural Science Foundation of China (NSFC) grant #61773117, #61473086, #61520106009, #61533008, and #U1713209.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Analyzing the behavior of visual question answering models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Parikh, and
D. Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Neural module networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
L. Bottou, J. Peters, J. Quiñonero Candela, D. X. Charles, D. M.
Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Counterfactual reasoning and learning systems: The example of
computational advertising.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">JMLR</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
X. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L.
Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, abs/1504.00325, 2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Human attention in visual question answering: Do humans and deep
networks look at the same regions?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and
D. Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Visual dialog.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
J. Devlin, S. Gupta, R. Girshick, M. Mitchell, and C. L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Exploring nearest neighbor approaches for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1505.04467</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Describing objects by their attributes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
D. Geman, S. Geman, N. Hallonquist, and L. Younes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Visual turing test for computer vision systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">PNAS</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
I. J. Goodfellow, J. Shlens, and C. Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Explaining and harnessing adversarial examples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
A. Graves, A. Mohamed, and G. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Speech recognition with deep recurrent neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
S. Hochreiter and J. Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 1997.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
A. Jabri, A. Joulin, and L. van der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Revisiting visual question answering baselines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, A. Karpathy, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Densecap: Fully convolutional localization networks for dense
captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
A. Karpathy and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Deep visual-semantic alignments for generating image descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Referit game: Referring to objects in photographs of natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
J.-H. Kim, K. W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Hadamard product for low-rank bilinear pooling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
D. Kingma and J. Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
F. Liu, T. Xiang, T. M. Hospedales, W. Yang, and C. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Semantic regularisation for recurrent image annotation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Visual relationship detection with language priors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
J. Lu, C. Xiong, D. Parikh, and R. Socher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Knowing when to look: Adaptive attention via a visual sentinel for
image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski, M. Rohrbach, and M. Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Ask your neurons: A neural-based approach to answering questions
about images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
N. Mostafazadeh, C. Brockett, B. Dolan, M. Galley, J. Gao, G. P. Spithourakis,
and L. Vanderwende.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Image-grounded conversations: Multimodal context for natural question
and response generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCNLP</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
N. Mostafazadeh, I. Misra, J. Devlin, M. Mitchell, X. He, and L. Vanderwende.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Generating natural questions about an image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
O. Pfungst.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Clever Hans (The Horse of Mr. Von Osten): A Contribution to
Experimental Animal and Human Psychology</span><span id="bib.bib29.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">H. Holt and company, New York, 1911.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
B. L. Sturm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">A simple method to determine if a music information retrieval system
is a horse.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TMM</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Show and tell: A neural image caption generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
P. Wang, Q. Wu, C. Shen, A. Dick, and A. van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Fvqa: Fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
H. Xu and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and
Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Show, attend and tell: Neural image caption generation with visual
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
B. Yao and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Recognizing human-object interactions in still images by modeling the
mutual context of objects and human poses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
S. Zhang, L. Qu, S. You, Z. Yang, and J. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Automatic generation of grounded visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCAI</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
L. Zhou, C. Xu, P. Koch, and J. J. Corso.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Image caption generation with text-conditional semantic attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.04621</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1710.03369" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1710.03370" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1710.03370">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1710.03370" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1710.03371" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 04:11:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
