<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.11976] Visual Question Answering on Image Sets</title><meta property="og:description" content="We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering on Image Sets">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering on Image Sets">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.11976">

<!--Generated on Wed Feb 28 07:04:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Maryland, College Park,  <span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{ankan,rama}@umd.edu</span></span></span> </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Amazon Web Services (AWS),   <span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>yutingzh@amazon.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Visual Question Answering on Image Sets</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ankan Bansal 
</span><span class="ltx_author_notes">This work was done when Ankan Bansal was an intern at AWS.11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuting Zhang
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rama Chellappa
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets – indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human-annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline
models to investigate new research challenges in ISVQA.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Answering natural-language questions about images requires understanding both linguistic and visual data.
Since its introduction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, Visual Question Answering (VQA) has attracted
significant attention.
Several related datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> have been proposed.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we introduce the new task of Image Set Visual Question Answering (ISVQA)
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Project page: <a target="_blank" href="https://ankanbansal.com/isvqa.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ankanbansal.com/isvqa.html</a></span></span></span>. It aims to
answer a free-form natural-language question based on a set of images. The proposed ISVQA task
requires reasoning over objects and concepts in different images to predict the correct answer.
For example, for figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (Left), a model has to find the relationship between the
<span id="S1.p2.1.1" class="ltx_text ltx_font_typewriter">bed</span> in the top-left image and the <span id="S1.p2.1.2" class="ltx_text ltx_font_typewriter">mirror</span> in the top-right, via <span id="S1.p2.1.3" class="ltx_text ltx_font_typewriter">pillows</span>
which are common to both the images. This example shows the unique challenges associated with
image-set VQA. A model for solving this type of problems has to understand the question, find the
connections between the images, and use those connections to relate objects across images.
Similarly, in figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (Right), the model has to avoid double-counting recurring objects in
multiple images.
These challenges associated with scene understanding have not been explored in existing single-image
VQA settings but frequently happen in the real world.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2008.11976/assets/images/gibson_4000.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="179" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2008.11976/assets/images/gibson_3500.png" id="S1.F1.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="179" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">(Left) Given the set of images above, and the question “What is hanging above the bed?”, it
is necessary to connect the bed in the top-left image to the mirror in the top-right image. To
answer this question a model needs to understand the concepts of “bed”, “mirror”, “above”,
“hanging”, etc. and be able to relate the bed in the first image with the headrest and pillows in
the third image. (Right) When asked the question “How many rectangles are on the interior doors?”, the model
should be able to provide the answer (“four”) and avoid counting the same rectangles multiple
times. </span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">ISVQA reflects information retrieval from multiple images of relevance but with no obvious
continuous correspondences. Such image sets can be any albums and images captured by multiple
devices, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, images under the same story on Facebook/Instagram, images of the same product
on Craigslist and Amazon, pictures of the same house on real estate websites, and images from
different car-mounted cameras. Other instances of the ISVQA task include answering questions about
images taken at different times (<em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em> like in camera trap photography), at different
locations (<em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">e.g.</em> multiple cameras from an indoor or outdoor location), or from different
viewpoints (<em id="S1.p3.1.4" class="ltx_emph ltx_font_italic">e.g.</em> live sports coverage). Some of these settings contain images taken from the
same scene, while others might involve images of a larger span.
While ISVQA can be generally applied to any type of images, in this paper, we focus on images from
multiple views of an environment, especially street and indoor scenes. </p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">ISVQA may require finding the same objects in different images or determining the
relationships between different objects within or across images. It can also entail determining
which images are the most relevant for the question and then answering based only on
them, ignoring the other images.
ISVQA leads to new research challenges, including: a) How
to use natural language to guide scene understanding across multiple views/images; and b) how to
fuse information from relevant images to reason about relationships among entities.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To enable research into these problems, we built two datasets for ISVQA - one for indoor scenes and
the other for outdoor scenes. The indoor scenes dataset comes from Gibson Environment
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and contains 91,479 human-generated questions, each for a set of images - for a
total of 48,138 image sets. Similarly, the outdoor scenes dataset comprises of 49,617 questions for
12,746 image sets. The images in the outdoor scenes dataset come from the nuScenes dataset
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. We explain the data collection methodology and statistics in section
<a href="#S3" title="3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The indoor ISVQA dataset contains two parts: 1.) Gibson-Room - containing images from the
same room; and 2.) Gibson-Building - containing images from different places in the same building.
This is to facilitate spatial and semantic reasoning both in localized and extended regions
in the same scene. The outdoor dataset contains image sets taken from mostly urban
environments.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We propose two extensions of single-image VQA methods as baseline approaches to investigate the
ISVQA task and the datasets. In addition, we also use an existing Video VQA approach as a simple
baseline. Finally, we also propose to use use a transformer-based approach which can specifically
target ISVQA.
Such baselines meet significant difficulties in solving the ISVQA problem, and they reflect the
particular challenges of the ISVQA task. We also present the statistics of the datasets, by
analyzing the types of question, distributions of answers for different types of questions, and
biases present in the dataset.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In summary, we make the following major contributions in this work.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">We propose ISVQA as a new setting for scene understanding via Q&amp;A;</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">We introduce two large-scale datasets for targeting the ISVQA problem. In total, these
datasets contain 141,096 questions for 60,884 sets of images. </p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">We establish baseline methods on ISVQA tasks to recognize the challenges and encourage
future research. </p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">VQA settings</span>. The basic VQA setting
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> involves answering natural language questions about images. The VisualGenome dataset
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> also contains annotations for visual question-answer pairs at both image
and region levels. Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> built upon the basic VQA setting and introduced visual
grounding to VQA. Several other VQA settings target specific problems or applications. For example,
VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> was designed to help answer questions asked by blind people.
RecipeQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is targeted for answering questions about recipes from
multi-modal cues. TallyQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and HowMany-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
specifically target counting questions for single images. Unlike these, the CLEVR
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> benchmark and dataset uses synthetically generated images of rendered 3D
shapes and is aimed towards understanding the geometric relationships between objects. IQA
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is also a synthetic setting where an agent is required to navigate a scene and
reach the desired location in order to answer the question.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Unlike existing work, ISVQA targets scene understanding by answering questions which
might require multiple images to answer. This important setting has not been studied before and
necessitates a specialized dataset. Additionally, answering most of the questions requires a model
to ignore some of the images in the set. This capability is absent from many state-of-the-art VQA
models.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">We also distinguish our work from video VQA. Unlike many such datasets (<em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">e.g.</em>
TVQA+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, MovieQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>),
our datasets do not contain any textual cues like scripts or subtitles. Also, videos are temporally continuous and are
usually taken from a stationary view-point. This makes finding associations between objects
across frames easy, even if datasets do not provide textual cues (<em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em> tGIF-QA
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>). The image sets in ISVQA dataset are not akin to video frames.
Also, unlike embodied QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, ISVQA does not have an agent interacting with the
environment. ISVQA algorithms can use only the few given images, which resembles real-world
applications. Embodied QA does not require sophisticated inference of
the correspondence between images, as the frames that an agent sees are continuous. The agent can
reach the desired location, and answer the question using only the final frame. In contrast, ISVQA
often needs reasoning across images and an implicit understanding of a scene.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">VQA methods</span>. Most of the recent VQA methods use attention mechanisms to focus on the most
relevant image regions. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed a bottom-up and top-down attention mechanism for answering
visual questions. In addition, several methods which use co-attention (or bi-directional)
attention over questions and images have been proposed. Such methods include
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, all of which use the information from one
modality (text or image) to attend to the other. Somewhat different from these is the work from
Gao <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which proposed the multi-modality latent interaction module
which can model the relationships between visual and language summaries in the form of latent
vectors.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Unlike these, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> used reasoning modules over detected objects to answer questions
about geometric relationships between objects. Similarly, Santoro <em id="S2.p5.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
proposed using Relation Networks to solve specific relational reasoning problems. Neither of these
approaches used attention mechanisms. In this paper, we mostly focus on attention-based mechanisms
to design the baseline models.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The main goal of our data collection effort is to aid multi-image scene understanding via question
answering. We use two publicly available datasets (Gibson <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>)
as the source of images to build our datasets.
Gibson provides navigable 3D-indoor scenes. We use the Habitat API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to extract
images from multiple locations and viewpoints from it. nuScences contains sets of images taken
simultaneously from multiple cameras on a car.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Annotation Collection</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Indoor Scenes.</span>
Gibson is a collection of 3D scans of indoor
spaces, particularly houses and offices. It provides virtualized scans of real indoor spaces like
houses and offices. Using the Habitat platform, we place an agent at different locations and orientations in a scene and
store the views visible to the agent. We generate a set of images by obtaining several views from
the same scene. Therefore, together, each image set can be considered to represent the scene. </p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We collect two types of indoor scenes: 1.) Gibson-Building; and 2.) Gibson-Room.
Gibson-Building contains multiple images taken from the same building by placing the agent
at random locations and recording its viewpoint while Gibson-Room is
collected by obtaining several views from the same room.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">For Gibson-Building, we sample image sets by placing an agent at random locations in the scene.
We show images from Gibson-Building sets to annotators and request them to ask questions about the
scene.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">We obtain question-answer annotations for a scene from several annotators using Amazon Mechanical
Turk. We let each annotator ask a question about the scene and also provide the corresponding
answer. We request that the annotators should ensure that their question can be answered using only
the scene shown and no additional knowledge should be required.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">From a pilot study, we observed that it is easier for humans to frame questions if they are shown
the full 3D view of a scene, simulating the situation of them being present in the scene. Humans are
able to frame better questions about locations of objects, and their relationships in such a
setting. For Gibson-Room, we simulate such immersion by creating a 360<sup id="S3.SS1.p5.1.1" class="ltx_sup">∘</sup> video from a room. We
show these videos (see supplementary material for examples of how these videos are created) to the
annotators and ask them to provide questions and answers about the scenes. This process helped
annotators understand the entire scene more easily and enabled us to collect more questions
requiring across-image reasoning. Videos are not used for annotating nuScene and Gibson-Building.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Note that the ISVQA problem and datasets do not have videos. The images for Gibson-Room still came
from random views as previously described. It is possible that the image set has less coverage of
the scene than the video. Just using the image set, it might not be possible to answer the questions
collected on the video. We prune out those cases by by asking other human-annotators to verify if
the question can be answered using the provided image set.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p"><span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_bold">Outdoor Scenes.</span>
We collect annotations for the nuScenes dataset similar to the Gibson-Building setting. We show the
annotators images from an image set. These represent a 360 degree view of a scene. We, again, ask
them to write questions and answers about the scene as before.</p>
</div>
<div id="S3.SS1.p8" class="ltx_para ltx_noindent">
<p id="S3.SS1.p8.1" class="ltx_p"><span id="S3.SS1.p8.1.1" class="ltx_text ltx_font_bold">Refining Annotations.</span>
We showed all the image sets in our datasets
and the associated questions obtained from the previous step to up to three other annotators. We
asked them to provide an answer to the question based only on the image set shown. We also asked
them to say “Not possible to answer” if the question cannot be answered. This step increases
confidence about an answer if there is a consensus among the annotators. This step has the added
benefit of ensuring that the question can be answered using the image set. </p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">In addition, we also asked the annotators at this stage to mark the images which are required to
answer the given question. This provides us information about which images are the most salient for
answering a question.</p>
</div>
<div id="S3.SS1.p10" class="ltx_para ltx_noindent">
<p id="S3.SS1.p10.1" class="ltx_p"><span id="S3.SS1.p10.1.1" class="ltx_text ltx_font_bold">Train and Test Splits.</span>
After refinement, we divided the datasets into train and test splits. The statistics of
these splits are given in table <a href="#S3.T1" title="Table 1 ‣ 3.1 Annotation Collection ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For test splits, we have select
samples for which at least two annotators agreed on the answer. We also ensured that the
train and test sets have the same set of answers.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Statistics of train and test splits of the datasets.</span></figcaption>
<div id="S3.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:42.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.0pt,5.8pt) scale(0.783337073013869,0.783337073013869) ;">
<table id="S3.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S3.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Train sets</th>
<th id="S3.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Test sets</th>
<th id="S3.T1.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Unique answers</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.1.2.1" class="ltx_tr">
<td id="S3.T1.4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Indoor - Gibson (Room + Building)</td>
<td id="S3.T1.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">69,207</td>
<td id="S3.T1.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">22,272</td>
<td id="S3.T1.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">961</td>
</tr>
<tr id="S3.T1.4.1.3.2" class="ltx_tr">
<td id="S3.T1.4.1.3.2.1" class="ltx_td ltx_align_left ltx_border_bb">Outdoor - nuScenes</td>
<td id="S3.T1.4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">33,973</td>
<td id="S3.T1.4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">15,644</td>
<td id="S3.T1.4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">650</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Analysis</h3>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2008.11976/assets/x1.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="161" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2008.11976/assets/x2.png" id="S3.F2.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="161" height="97" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.5.2" class="ltx_text" style="font-size:90%;">Question wordclouds for Gibson (left) and nuScenes (right) datasets.</span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Question word distributions.</span>
The question word clouds for datasets are shown in figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We have removed the first few words from each question before plotting these. This gives us a better
picture of which objects people are interested in. Clearly, for outdoor scenes, people are most interested in objects commonly found on the streets
and their properties (types, colors, numbers). On the other hand, for
indoor scenes, the most frequent questions are about objects hanging on walls and kept on beds, and
the room layouts in general.
</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Types of Questions.</span>
Figure <a href="#S3.F3.sf1" title="In Figure 3 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> shows the distribution of question lengths for the
dataset. We observe that a large chunk of the questions contain between
5 and 10 words. Further, figure <a href="#S3.F3.sf2" title="In Figure 3 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> shows the numbers of the
most frequent types of questions for the dataset. We observe that the most frequent questions are
about properties of objects, and spatial relationships between
different entities.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To understand the types of questions in the dataset, we plot the distribution of the most frequent
first five words of the questions in the whole dataset in figure <a href="#S3.F3.sf3" title="In Figure 3 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a>. Note that a
large portion of the questions are about the numbers of different kinds of objects. Another major
subset of the questions are about geometric relationships between objects in a scene. A third big
part of the dataset contains questions about colors of objects in scenes. Answering questions about
the colors of things in a scene requires localization of the object of interest. Depending on the
question, this might require reasoning about the relationships between objects in different images.
Similarly, counting the number of a particular type of object requires keeping track of previously
counted objects to avoid double counting if the same object appears in different images.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2008.11976/assets/x3.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="330" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Question lengths</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2008.11976/assets/x4.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="461" height="392" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Frequent questions</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2008.11976/assets/x5.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_square" width="298" height="275" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">Types of questions</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">(Left) Distribution of questions over no. of words. (Middle) Most frequent types
of questions in the dataset. (Right) First five words of the questions.
</span></figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Answer Distributions.</span>
Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the distribution of answers in the dataset for
frequently occurring questions types. On one hand, due to human bias in asking questions, dominant
answers exist for a few types of questions, such as “can you see the” (usually for an object
in the image) and “what is this” (usually a large object). In ISVQA and other VQA
datasets humans’ tendency to only ask questions about objects that they can see leads to a higher
frequency of “yes” answers. On the other hand, most question types do not have a dominant
answer. Of particular note are the questions about relative locations and orientations of objects,
<em id="S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.</em> “What is next to”, and questions about the numbers of objects
<em id="S3.SS2.p4.1.3" class="ltx_emph ltx_font_italic">e.g.</em> “How many chairs are” etc. This means that it is difficult for a
model to perform well by lazily exploiting the statistics of question types.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2008.11976/assets/images/train_answer_dis_by_question_type.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Answer distributions for several types of questions in the whole dataset.</span></figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Number of Images Required.</span>
While refining the annotations, we also collect annotations for which images are required to answer
the given question. In figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we plot the number of images
required to answer each question for all datasets. For the plot in
figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 Dataset Analysis ‣ 3 Dataset ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we only consider those image sets for which at least 2 annotators
agree about the images which are needed. We observe that one-third of the samples (about 7,000/21,000) in
Gibson-Room require at least two images to answer the question. As expected, this ratio is
lower for Gibson-Building dataset. However, for all three datasets, we have a large number of
questions which require more than one image to answer. The large number of samples in both cases enable the study of both across-image reasoning and
image-level focusing. In particular, the latter case also involves rejecting most of the images in
the image set and focusing only on one image. In theory, such questions can potentially be answered
by using existing single-image VQA models. However, this would require the single-image VQA model to
say “Not possible to answer” for all the irrelevant images and finding only the most relevant one.
Current VQA models do not have the ability to do this in many cases. (see supplementary material for
examples).
</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2008.11976/assets/x6.png" id="S3.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="223" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Gibson-Room</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2008.11976/assets/x7.png" id="S3.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="332" height="223" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Gibson-Building</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2008.11976/assets/x8.png" id="S3.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="332" height="223" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">nuScenes</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Number of images required to answer different types of questions.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>ISVQA Problem Formulation and Baselines</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Problem Definition</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.10" class="ltx_p">Refer to figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 Problem Definition ‣ 4 ISVQA Problem Formulation and Baselines ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for some examples of the ISVQA setting. Given a set of
images, <math id="S4.SS1.p1.1.m1.4" class="ltx_Math" alttext="S=\{I_{1},I_{2},\dots,I_{n}\}" display="inline"><semantics id="S4.SS1.p1.1.m1.4a"><mrow id="S4.SS1.p1.1.m1.4.4" xref="S4.SS1.p1.1.m1.4.4.cmml"><mi id="S4.SS1.p1.1.m1.4.4.5" xref="S4.SS1.p1.1.m1.4.4.5.cmml">S</mi><mo id="S4.SS1.p1.1.m1.4.4.4" xref="S4.SS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS1.p1.1.m1.4.4.3.3" xref="S4.SS1.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS1.p1.1.m1.4.4.3.3.4" xref="S4.SS1.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S4.SS1.p1.1.m1.2.2.1.1.1" xref="S4.SS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.2.2.1.1.1.2" xref="S4.SS1.p1.1.m1.2.2.1.1.1.2.cmml">I</mi><mn id="S4.SS1.p1.1.m1.2.2.1.1.1.3" xref="S4.SS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.1.m1.4.4.3.3.5" xref="S4.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.1.m1.3.3.2.2.2" xref="S4.SS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.1.m1.3.3.2.2.2.2" xref="S4.SS1.p1.1.m1.3.3.2.2.2.2.cmml">I</mi><mn id="S4.SS1.p1.1.m1.3.3.2.2.2.3" xref="S4.SS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.1.m1.4.4.3.3.6" xref="S4.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">…</mi><mo id="S4.SS1.p1.1.m1.4.4.3.3.7" xref="S4.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.1.m1.4.4.3.3.3" xref="S4.SS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS1.p1.1.m1.4.4.3.3.3.2" xref="S4.SS1.p1.1.m1.4.4.3.3.3.2.cmml">I</mi><mi id="S4.SS1.p1.1.m1.4.4.3.3.3.3" xref="S4.SS1.p1.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.SS1.p1.1.m1.4.4.3.3.8" xref="S4.SS1.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.4b"><apply id="S4.SS1.p1.1.m1.4.4.cmml" xref="S4.SS1.p1.1.m1.4.4"><eq id="S4.SS1.p1.1.m1.4.4.4.cmml" xref="S4.SS1.p1.1.m1.4.4.4"></eq><ci id="S4.SS1.p1.1.m1.4.4.5.cmml" xref="S4.SS1.p1.1.m1.4.4.5">𝑆</ci><set id="S4.SS1.p1.1.m1.4.4.3.4.cmml" xref="S4.SS1.p1.1.m1.4.4.3.3"><apply id="S4.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.2.2.1.1.1.2">𝐼</ci><cn type="integer" id="S4.SS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.1.m1.3.3.2.2.2.2">𝐼</ci><cn type="integer" id="S4.SS1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">…</ci><apply id="S4.SS1.p1.1.m1.4.4.3.3.3.cmml" xref="S4.SS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS1.p1.1.m1.4.4.3.3.3.2">𝐼</ci><ci id="S4.SS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS1.p1.1.m1.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.4c">S=\{I_{1},I_{2},\dots,I_{n}\}</annotation></semantics></math>, and a natural language question, <math id="S4.SS1.p1.2.m2.4" class="ltx_Math" alttext="Q=\{v_{1},v_{2},\dots,v_{T}\}" display="inline"><semantics id="S4.SS1.p1.2.m2.4a"><mrow id="S4.SS1.p1.2.m2.4.4" xref="S4.SS1.p1.2.m2.4.4.cmml"><mi id="S4.SS1.p1.2.m2.4.4.5" xref="S4.SS1.p1.2.m2.4.4.5.cmml">Q</mi><mo id="S4.SS1.p1.2.m2.4.4.4" xref="S4.SS1.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S4.SS1.p1.2.m2.4.4.3.3" xref="S4.SS1.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS1.p1.2.m2.4.4.3.3.4" xref="S4.SS1.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S4.SS1.p1.2.m2.2.2.1.1.1" xref="S4.SS1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.2.m2.2.2.1.1.1.2" xref="S4.SS1.p1.2.m2.2.2.1.1.1.2.cmml">v</mi><mn id="S4.SS1.p1.2.m2.2.2.1.1.1.3" xref="S4.SS1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.2.m2.4.4.3.3.5" xref="S4.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.2.m2.3.3.2.2.2" xref="S4.SS1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.2.m2.3.3.2.2.2.2" xref="S4.SS1.p1.2.m2.3.3.2.2.2.2.cmml">v</mi><mn id="S4.SS1.p1.2.m2.3.3.2.2.2.3" xref="S4.SS1.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.2.m2.4.4.3.3.6" xref="S4.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S4.SS1.p1.2.m2.4.4.3.3.7" xref="S4.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.2.m2.4.4.3.3.3" xref="S4.SS1.p1.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS1.p1.2.m2.4.4.3.3.3.2" xref="S4.SS1.p1.2.m2.4.4.3.3.3.2.cmml">v</mi><mi id="S4.SS1.p1.2.m2.4.4.3.3.3.3" xref="S4.SS1.p1.2.m2.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S4.SS1.p1.2.m2.4.4.3.3.8" xref="S4.SS1.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.4b"><apply id="S4.SS1.p1.2.m2.4.4.cmml" xref="S4.SS1.p1.2.m2.4.4"><eq id="S4.SS1.p1.2.m2.4.4.4.cmml" xref="S4.SS1.p1.2.m2.4.4.4"></eq><ci id="S4.SS1.p1.2.m2.4.4.5.cmml" xref="S4.SS1.p1.2.m2.4.4.5">𝑄</ci><set id="S4.SS1.p1.2.m2.4.4.3.4.cmml" xref="S4.SS1.p1.2.m2.4.4.3.3"><apply id="S4.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.2.m2.2.2.1.1.1.2">𝑣</ci><cn type="integer" id="S4.SS1.p1.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.2.m2.3.3.2.2.2.2">𝑣</ci><cn type="integer" id="S4.SS1.p1.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS1.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">…</ci><apply id="S4.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S4.SS1.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS1.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS1.p1.2.m2.4.4.3.3.3.2">𝑣</ci><ci id="S4.SS1.p1.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS1.p1.2.m2.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.4c">Q=\{v_{1},v_{2},\dots,v_{T}\}</annotation></semantics></math>, where <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><msub id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">v</mi><mi id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">𝑣</ci><ci id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">v_{i}</annotation></semantics></math> is the <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><msup id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">i</mi><mrow id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">superscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">𝑖</ci><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><times id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.2">𝑡</ci><ci id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">i^{th}</annotation></semantics></math> word in the question, the task is to provide an answer, <math id="S4.SS1.p1.5.m5.2" class="ltx_Math" alttext="a=f(S,Q)" display="inline"><semantics id="S4.SS1.p1.5.m5.2a"><mrow id="S4.SS1.p1.5.m5.2.3" xref="S4.SS1.p1.5.m5.2.3.cmml"><mi id="S4.SS1.p1.5.m5.2.3.2" xref="S4.SS1.p1.5.m5.2.3.2.cmml">a</mi><mo id="S4.SS1.p1.5.m5.2.3.1" xref="S4.SS1.p1.5.m5.2.3.1.cmml">=</mo><mrow id="S4.SS1.p1.5.m5.2.3.3" xref="S4.SS1.p1.5.m5.2.3.3.cmml"><mi id="S4.SS1.p1.5.m5.2.3.3.2" xref="S4.SS1.p1.5.m5.2.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.2.3.3.1" xref="S4.SS1.p1.5.m5.2.3.3.1.cmml">​</mo><mrow id="S4.SS1.p1.5.m5.2.3.3.3.2" xref="S4.SS1.p1.5.m5.2.3.3.3.1.cmml"><mo stretchy="false" id="S4.SS1.p1.5.m5.2.3.3.3.2.1" xref="S4.SS1.p1.5.m5.2.3.3.3.1.cmml">(</mo><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">S</mi><mo id="S4.SS1.p1.5.m5.2.3.3.3.2.2" xref="S4.SS1.p1.5.m5.2.3.3.3.1.cmml">,</mo><mi id="S4.SS1.p1.5.m5.2.2" xref="S4.SS1.p1.5.m5.2.2.cmml">Q</mi><mo stretchy="false" id="S4.SS1.p1.5.m5.2.3.3.3.2.3" xref="S4.SS1.p1.5.m5.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.2b"><apply id="S4.SS1.p1.5.m5.2.3.cmml" xref="S4.SS1.p1.5.m5.2.3"><eq id="S4.SS1.p1.5.m5.2.3.1.cmml" xref="S4.SS1.p1.5.m5.2.3.1"></eq><ci id="S4.SS1.p1.5.m5.2.3.2.cmml" xref="S4.SS1.p1.5.m5.2.3.2">𝑎</ci><apply id="S4.SS1.p1.5.m5.2.3.3.cmml" xref="S4.SS1.p1.5.m5.2.3.3"><times id="S4.SS1.p1.5.m5.2.3.3.1.cmml" xref="S4.SS1.p1.5.m5.2.3.3.1"></times><ci id="S4.SS1.p1.5.m5.2.3.3.2.cmml" xref="S4.SS1.p1.5.m5.2.3.3.2">𝑓</ci><interval closure="open" id="S4.SS1.p1.5.m5.2.3.3.3.1.cmml" xref="S4.SS1.p1.5.m5.2.3.3.3.2"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑆</ci><ci id="S4.SS1.p1.5.m5.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2">𝑄</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.2c">a=f(S,Q)</annotation></semantics></math>, which is true for the given question and image set. The function <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">f</annotation></semantics></math> can either output a
probability distribution over a pre-defined set of possible answers, <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">\mathcal{A}</annotation></semantics></math>, or select the
best answer from several choices which are input along with the question, i.e., <math id="S4.SS1.p1.8.m8.3" class="ltx_Math" alttext="a=f(S,Q,C_{Q})" display="inline"><semantics id="S4.SS1.p1.8.m8.3a"><mrow id="S4.SS1.p1.8.m8.3.3" xref="S4.SS1.p1.8.m8.3.3.cmml"><mi id="S4.SS1.p1.8.m8.3.3.3" xref="S4.SS1.p1.8.m8.3.3.3.cmml">a</mi><mo id="S4.SS1.p1.8.m8.3.3.2" xref="S4.SS1.p1.8.m8.3.3.2.cmml">=</mo><mrow id="S4.SS1.p1.8.m8.3.3.1" xref="S4.SS1.p1.8.m8.3.3.1.cmml"><mi id="S4.SS1.p1.8.m8.3.3.1.3" xref="S4.SS1.p1.8.m8.3.3.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.8.m8.3.3.1.2" xref="S4.SS1.p1.8.m8.3.3.1.2.cmml">​</mo><mrow id="S4.SS1.p1.8.m8.3.3.1.1.1" xref="S4.SS1.p1.8.m8.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.8.m8.3.3.1.1.1.2" xref="S4.SS1.p1.8.m8.3.3.1.1.2.cmml">(</mo><mi id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">S</mi><mo id="S4.SS1.p1.8.m8.3.3.1.1.1.3" xref="S4.SS1.p1.8.m8.3.3.1.1.2.cmml">,</mo><mi id="S4.SS1.p1.8.m8.2.2" xref="S4.SS1.p1.8.m8.2.2.cmml">Q</mi><mo id="S4.SS1.p1.8.m8.3.3.1.1.1.4" xref="S4.SS1.p1.8.m8.3.3.1.1.2.cmml">,</mo><msub id="S4.SS1.p1.8.m8.3.3.1.1.1.1" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1.cmml"><mi id="S4.SS1.p1.8.m8.3.3.1.1.1.1.2" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1.2.cmml">C</mi><mi id="S4.SS1.p1.8.m8.3.3.1.1.1.1.3" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1.3.cmml">Q</mi></msub><mo stretchy="false" id="S4.SS1.p1.8.m8.3.3.1.1.1.5" xref="S4.SS1.p1.8.m8.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.3b"><apply id="S4.SS1.p1.8.m8.3.3.cmml" xref="S4.SS1.p1.8.m8.3.3"><eq id="S4.SS1.p1.8.m8.3.3.2.cmml" xref="S4.SS1.p1.8.m8.3.3.2"></eq><ci id="S4.SS1.p1.8.m8.3.3.3.cmml" xref="S4.SS1.p1.8.m8.3.3.3">𝑎</ci><apply id="S4.SS1.p1.8.m8.3.3.1.cmml" xref="S4.SS1.p1.8.m8.3.3.1"><times id="S4.SS1.p1.8.m8.3.3.1.2.cmml" xref="S4.SS1.p1.8.m8.3.3.1.2"></times><ci id="S4.SS1.p1.8.m8.3.3.1.3.cmml" xref="S4.SS1.p1.8.m8.3.3.1.3">𝑓</ci><vector id="S4.SS1.p1.8.m8.3.3.1.1.2.cmml" xref="S4.SS1.p1.8.m8.3.3.1.1.1"><ci id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">𝑆</ci><ci id="S4.SS1.p1.8.m8.2.2.cmml" xref="S4.SS1.p1.8.m8.2.2">𝑄</ci><apply id="S4.SS1.p1.8.m8.3.3.1.1.1.1.cmml" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.8.m8.3.3.1.1.1.1.1.cmml" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.8.m8.3.3.1.1.1.1.2.cmml" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1.2">𝐶</ci><ci id="S4.SS1.p1.8.m8.3.3.1.1.1.1.3.cmml" xref="S4.SS1.p1.8.m8.3.3.1.1.1.1.3">𝑄</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.3c">a=f(S,Q,C_{Q})</annotation></semantics></math>,
where <math id="S4.SS1.p1.9.m9.1" class="ltx_Math" alttext="C_{Q}" display="inline"><semantics id="S4.SS1.p1.9.m9.1a"><msub id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml"><mi id="S4.SS1.p1.9.m9.1.1.2" xref="S4.SS1.p1.9.m9.1.1.2.cmml">C</mi><mi id="S4.SS1.p1.9.m9.1.1.3" xref="S4.SS1.p1.9.m9.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><apply id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.9.m9.1.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.p1.9.m9.1.1.2.cmml" xref="S4.SS1.p1.9.m9.1.1.2">𝐶</ci><ci id="S4.SS1.p1.9.m9.1.1.3.cmml" xref="S4.SS1.p1.9.m9.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">C_{Q}</annotation></semantics></math> is the list of choices associated with <math id="S4.SS1.p1.10.m10.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.SS1.p1.10.m10.1a"><mi id="S4.SS1.p1.10.m10.1.1" xref="S4.SS1.p1.10.m10.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m10.1b"><ci id="S4.SS1.p1.10.m10.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m10.1c">Q</annotation></semantics></math>. The former is usually called open-ended QA
and the latter is called multiple-choice QA. In this work, we mainly deal with the open-ended
setting. Another possible setting is to actually generate the answer using a text generation method
similar to image-captioning. But, most existing VQA works focus on either of the first two settings
and therefore, we also consider the open-ended setting in this work. We leave the harder problem of
generating answers to future work.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2008.11976/assets/x9.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="631" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Some examples from our dataset which demonstrate the ISVQA problem setting. In each
case, input is a set of images and a question.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Definitions</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Now, we describe some baselines for the ISVQA problem. These baselines directly adapt single image
VQA models. The first of these processes each image separately and concatenates the features
obtained from each image to predict the answer. The second baseline directly adapts VQA methods by
simply stitching the images and using single image VQA methods to predict the answer.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We also propose an approach to address the special challenges in ISVQA.
A fundamental direction to solve ISVQA problem is to enable finer-grained and across-image interactions in a VQA model, where self-attention-based transformers can fit well.
In particular, we adapt LXMERT
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which is designed for cross-modality learning, to both cross-modality and cross-image scenarios.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.14" class="ltx_p"><span id="S4.SS2.p3.14.1" class="ltx_text ltx_font_bold">Concatenate-Feature.</span>
Starting from a given set of <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">n</annotation></semantics></math> images <math id="S4.SS2.p3.2.m2.4" class="ltx_Math" alttext="S=\{I_{1},I_{2},\dots,I_{n}\}" display="inline"><semantics id="S4.SS2.p3.2.m2.4a"><mrow id="S4.SS2.p3.2.m2.4.4" xref="S4.SS2.p3.2.m2.4.4.cmml"><mi id="S4.SS2.p3.2.m2.4.4.5" xref="S4.SS2.p3.2.m2.4.4.5.cmml">S</mi><mo id="S4.SS2.p3.2.m2.4.4.4" xref="S4.SS2.p3.2.m2.4.4.4.cmml">=</mo><mrow id="S4.SS2.p3.2.m2.4.4.3.3" xref="S4.SS2.p3.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p3.2.m2.4.4.3.3.4" xref="S4.SS2.p3.2.m2.4.4.3.4.cmml">{</mo><msub id="S4.SS2.p3.2.m2.2.2.1.1.1" xref="S4.SS2.p3.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS2.p3.2.m2.2.2.1.1.1.2" xref="S4.SS2.p3.2.m2.2.2.1.1.1.2.cmml">I</mi><mn id="S4.SS2.p3.2.m2.2.2.1.1.1.3" xref="S4.SS2.p3.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p3.2.m2.4.4.3.3.5" xref="S4.SS2.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p3.2.m2.3.3.2.2.2" xref="S4.SS2.p3.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS2.p3.2.m2.3.3.2.2.2.2" xref="S4.SS2.p3.2.m2.3.3.2.2.2.2.cmml">I</mi><mn id="S4.SS2.p3.2.m2.3.3.2.2.2.3" xref="S4.SS2.p3.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p3.2.m2.4.4.3.3.6" xref="S4.SS2.p3.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">…</mi><mo id="S4.SS2.p3.2.m2.4.4.3.3.7" xref="S4.SS2.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p3.2.m2.4.4.3.3.3" xref="S4.SS2.p3.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS2.p3.2.m2.4.4.3.3.3.2" xref="S4.SS2.p3.2.m2.4.4.3.3.3.2.cmml">I</mi><mi id="S4.SS2.p3.2.m2.4.4.3.3.3.3" xref="S4.SS2.p3.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.SS2.p3.2.m2.4.4.3.3.8" xref="S4.SS2.p3.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.4b"><apply id="S4.SS2.p3.2.m2.4.4.cmml" xref="S4.SS2.p3.2.m2.4.4"><eq id="S4.SS2.p3.2.m2.4.4.4.cmml" xref="S4.SS2.p3.2.m2.4.4.4"></eq><ci id="S4.SS2.p3.2.m2.4.4.5.cmml" xref="S4.SS2.p3.2.m2.4.4.5">𝑆</ci><set id="S4.SS2.p3.2.m2.4.4.3.4.cmml" xref="S4.SS2.p3.2.m2.4.4.3.3"><apply id="S4.SS2.p3.2.m2.2.2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS2.p3.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS2.p3.2.m2.2.2.1.1.1.2">𝐼</ci><cn type="integer" id="S4.SS2.p3.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS2.p3.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p3.2.m2.3.3.2.2.2.cmml" xref="S4.SS2.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS2.p3.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p3.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS2.p3.2.m2.3.3.2.2.2.2">𝐼</ci><cn type="integer" id="S4.SS2.p3.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS2.p3.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">…</ci><apply id="S4.SS2.p3.2.m2.4.4.3.3.3.cmml" xref="S4.SS2.p3.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS2.p3.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p3.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS2.p3.2.m2.4.4.3.3.3.2">𝐼</ci><ci id="S4.SS2.p3.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS2.p3.2.m2.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.4c">S=\{I_{1},I_{2},\dots,I_{n}\}</annotation></semantics></math>, we use a region proposal
network (RPN) to extract region proposals <math id="S4.SS2.p3.3.m3.6" class="ltx_Math" alttext="R_{i},i=1,2,\dots,n" display="inline"><semantics id="S4.SS2.p3.3.m3.6a"><mrow id="S4.SS2.p3.3.m3.6.6.2" xref="S4.SS2.p3.3.m3.6.6.3.cmml"><mrow id="S4.SS2.p3.3.m3.5.5.1.1" xref="S4.SS2.p3.3.m3.5.5.1.1.cmml"><mrow id="S4.SS2.p3.3.m3.5.5.1.1.1.1" xref="S4.SS2.p3.3.m3.5.5.1.1.1.2.cmml"><msub id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.cmml"><mi id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.2" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.2.cmml">R</mi><mi id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.3" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p3.3.m3.5.5.1.1.1.1.2" xref="S4.SS2.p3.3.m3.5.5.1.1.1.2.cmml">,</mo><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">i</mi></mrow><mo id="S4.SS2.p3.3.m3.5.5.1.1.2" xref="S4.SS2.p3.3.m3.5.5.1.1.2.cmml">=</mo><mn id="S4.SS2.p3.3.m3.5.5.1.1.3" xref="S4.SS2.p3.3.m3.5.5.1.1.3.cmml">1</mn></mrow><mo id="S4.SS2.p3.3.m3.6.6.2.3" xref="S4.SS2.p3.3.m3.6.6.3a.cmml">,</mo><mrow id="S4.SS2.p3.3.m3.6.6.2.2.2" xref="S4.SS2.p3.3.m3.6.6.2.2.1.cmml"><mn id="S4.SS2.p3.3.m3.2.2" xref="S4.SS2.p3.3.m3.2.2.cmml">2</mn><mo id="S4.SS2.p3.3.m3.6.6.2.2.2.1" xref="S4.SS2.p3.3.m3.6.6.2.2.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p3.3.m3.3.3" xref="S4.SS2.p3.3.m3.3.3.cmml">…</mi><mo id="S4.SS2.p3.3.m3.6.6.2.2.2.2" xref="S4.SS2.p3.3.m3.6.6.2.2.1.cmml">,</mo><mi id="S4.SS2.p3.3.m3.4.4" xref="S4.SS2.p3.3.m3.4.4.cmml">n</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.6b"><apply id="S4.SS2.p3.3.m3.6.6.3.cmml" xref="S4.SS2.p3.3.m3.6.6.2"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.6.6.3a.cmml" xref="S4.SS2.p3.3.m3.6.6.2.3">formulae-sequence</csymbol><apply id="S4.SS2.p3.3.m3.5.5.1.1.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1"><eq id="S4.SS2.p3.3.m3.5.5.1.1.2.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.2"></eq><list id="S4.SS2.p3.3.m3.5.5.1.1.1.2.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1"><apply id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.2">𝑅</ci><ci id="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.1.1.1.3">𝑖</ci></apply><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">𝑖</ci></list><cn type="integer" id="S4.SS2.p3.3.m3.5.5.1.1.3.cmml" xref="S4.SS2.p3.3.m3.5.5.1.1.3">1</cn></apply><list id="S4.SS2.p3.3.m3.6.6.2.2.1.cmml" xref="S4.SS2.p3.3.m3.6.6.2.2.2"><cn type="integer" id="S4.SS2.p3.3.m3.2.2.cmml" xref="S4.SS2.p3.3.m3.2.2">2</cn><ci id="S4.SS2.p3.3.m3.3.3.cmml" xref="S4.SS2.p3.3.m3.3.3">…</ci><ci id="S4.SS2.p3.3.m3.4.4.cmml" xref="S4.SS2.p3.3.m3.4.4">𝑛</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.6c">R_{i},i=1,2,\dots,n</annotation></semantics></math> and the
corresponding RoI-pooled features (fc6). With some abuse of notation, we denote the region features
obtained from each image as <math id="S4.SS2.p3.4.m4.6" class="ltx_Math" alttext="R_{i}\in\mathbb{R}^{p\times d},i=1,2,\dots,n" display="inline"><semantics id="S4.SS2.p3.4.m4.6a"><mrow id="S4.SS2.p3.4.m4.6.6.2" xref="S4.SS2.p3.4.m4.6.6.3.cmml"><mrow id="S4.SS2.p3.4.m4.5.5.1.1" xref="S4.SS2.p3.4.m4.5.5.1.1.cmml"><msub id="S4.SS2.p3.4.m4.5.5.1.1.2" xref="S4.SS2.p3.4.m4.5.5.1.1.2.cmml"><mi id="S4.SS2.p3.4.m4.5.5.1.1.2.2" xref="S4.SS2.p3.4.m4.5.5.1.1.2.2.cmml">R</mi><mi id="S4.SS2.p3.4.m4.5.5.1.1.2.3" xref="S4.SS2.p3.4.m4.5.5.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.p3.4.m4.5.5.1.1.1" xref="S4.SS2.p3.4.m4.5.5.1.1.1.cmml">∈</mo><msup id="S4.SS2.p3.4.m4.5.5.1.1.3" xref="S4.SS2.p3.4.m4.5.5.1.1.3.cmml"><mi id="S4.SS2.p3.4.m4.5.5.1.1.3.2" xref="S4.SS2.p3.4.m4.5.5.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p3.4.m4.5.5.1.1.3.3" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.cmml"><mi id="S4.SS2.p3.4.m4.5.5.1.1.3.3.2" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.4.m4.5.5.1.1.3.3.1" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.p3.4.m4.5.5.1.1.3.3.3" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><mo id="S4.SS2.p3.4.m4.6.6.2.3" xref="S4.SS2.p3.4.m4.6.6.3a.cmml">,</mo><mrow id="S4.SS2.p3.4.m4.6.6.2.2" xref="S4.SS2.p3.4.m4.6.6.2.2.cmml"><mi id="S4.SS2.p3.4.m4.6.6.2.2.2" xref="S4.SS2.p3.4.m4.6.6.2.2.2.cmml">i</mi><mo id="S4.SS2.p3.4.m4.6.6.2.2.1" xref="S4.SS2.p3.4.m4.6.6.2.2.1.cmml">=</mo><mrow id="S4.SS2.p3.4.m4.6.6.2.2.3.2" xref="S4.SS2.p3.4.m4.6.6.2.2.3.1.cmml"><mn id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">1</mn><mo id="S4.SS2.p3.4.m4.6.6.2.2.3.2.1" xref="S4.SS2.p3.4.m4.6.6.2.2.3.1.cmml">,</mo><mn id="S4.SS2.p3.4.m4.2.2" xref="S4.SS2.p3.4.m4.2.2.cmml">2</mn><mo id="S4.SS2.p3.4.m4.6.6.2.2.3.2.2" xref="S4.SS2.p3.4.m4.6.6.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p3.4.m4.3.3" xref="S4.SS2.p3.4.m4.3.3.cmml">…</mi><mo id="S4.SS2.p3.4.m4.6.6.2.2.3.2.3" xref="S4.SS2.p3.4.m4.6.6.2.2.3.1.cmml">,</mo><mi id="S4.SS2.p3.4.m4.4.4" xref="S4.SS2.p3.4.m4.4.4.cmml">n</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.6b"><apply id="S4.SS2.p3.4.m4.6.6.3.cmml" xref="S4.SS2.p3.4.m4.6.6.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.6.6.3a.cmml" xref="S4.SS2.p3.4.m4.6.6.2.3">formulae-sequence</csymbol><apply id="S4.SS2.p3.4.m4.5.5.1.1.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1"><in id="S4.SS2.p3.4.m4.5.5.1.1.1.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.1"></in><apply id="S4.SS2.p3.4.m4.5.5.1.1.2.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.5.5.1.1.2.1.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.4.m4.5.5.1.1.2.2.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.2.2">𝑅</ci><ci id="S4.SS2.p3.4.m4.5.5.1.1.2.3.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.p3.4.m4.5.5.1.1.3.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.5.5.1.1.3.1.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.4.m4.5.5.1.1.3.2.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3.2">ℝ</ci><apply id="S4.SS2.p3.4.m4.5.5.1.1.3.3.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3"><times id="S4.SS2.p3.4.m4.5.5.1.1.3.3.1.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.1"></times><ci id="S4.SS2.p3.4.m4.5.5.1.1.3.3.2.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.2">𝑝</ci><ci id="S4.SS2.p3.4.m4.5.5.1.1.3.3.3.cmml" xref="S4.SS2.p3.4.m4.5.5.1.1.3.3.3">𝑑</ci></apply></apply></apply><apply id="S4.SS2.p3.4.m4.6.6.2.2.cmml" xref="S4.SS2.p3.4.m4.6.6.2.2"><eq id="S4.SS2.p3.4.m4.6.6.2.2.1.cmml" xref="S4.SS2.p3.4.m4.6.6.2.2.1"></eq><ci id="S4.SS2.p3.4.m4.6.6.2.2.2.cmml" xref="S4.SS2.p3.4.m4.6.6.2.2.2">𝑖</ci><list id="S4.SS2.p3.4.m4.6.6.2.2.3.1.cmml" xref="S4.SS2.p3.4.m4.6.6.2.2.3.2"><cn type="integer" id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">1</cn><cn type="integer" id="S4.SS2.p3.4.m4.2.2.cmml" xref="S4.SS2.p3.4.m4.2.2">2</cn><ci id="S4.SS2.p3.4.m4.3.3.cmml" xref="S4.SS2.p3.4.m4.3.3">…</ci><ci id="S4.SS2.p3.4.m4.4.4.cmml" xref="S4.SS2.p3.4.m4.4.4">𝑛</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.6c">R_{i}\in\mathbb{R}^{p\times d},i=1,2,\dots,n</annotation></semantics></math>, where <math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mi id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><ci id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">p</annotation></semantics></math> is the
number of region features obtained from each image and <math id="S4.SS2.p3.6.m6.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS2.p3.6.m6.1a"><mi id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><ci id="S4.SS2.p3.6.m6.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">d</annotation></semantics></math> is the dimension of the features. We are
also given a natural language question <math id="S4.SS2.p3.7.m7.4" class="ltx_Math" alttext="Q=\{v_{1},v_{2},\dots,v_{T}\}" display="inline"><semantics id="S4.SS2.p3.7.m7.4a"><mrow id="S4.SS2.p3.7.m7.4.4" xref="S4.SS2.p3.7.m7.4.4.cmml"><mi id="S4.SS2.p3.7.m7.4.4.5" xref="S4.SS2.p3.7.m7.4.4.5.cmml">Q</mi><mo id="S4.SS2.p3.7.m7.4.4.4" xref="S4.SS2.p3.7.m7.4.4.4.cmml">=</mo><mrow id="S4.SS2.p3.7.m7.4.4.3.3" xref="S4.SS2.p3.7.m7.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p3.7.m7.4.4.3.3.4" xref="S4.SS2.p3.7.m7.4.4.3.4.cmml">{</mo><msub id="S4.SS2.p3.7.m7.2.2.1.1.1" xref="S4.SS2.p3.7.m7.2.2.1.1.1.cmml"><mi id="S4.SS2.p3.7.m7.2.2.1.1.1.2" xref="S4.SS2.p3.7.m7.2.2.1.1.1.2.cmml">v</mi><mn id="S4.SS2.p3.7.m7.2.2.1.1.1.3" xref="S4.SS2.p3.7.m7.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p3.7.m7.4.4.3.3.5" xref="S4.SS2.p3.7.m7.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p3.7.m7.3.3.2.2.2" xref="S4.SS2.p3.7.m7.3.3.2.2.2.cmml"><mi id="S4.SS2.p3.7.m7.3.3.2.2.2.2" xref="S4.SS2.p3.7.m7.3.3.2.2.2.2.cmml">v</mi><mn id="S4.SS2.p3.7.m7.3.3.2.2.2.3" xref="S4.SS2.p3.7.m7.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p3.7.m7.4.4.3.3.6" xref="S4.SS2.p3.7.m7.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p3.7.m7.1.1" xref="S4.SS2.p3.7.m7.1.1.cmml">…</mi><mo id="S4.SS2.p3.7.m7.4.4.3.3.7" xref="S4.SS2.p3.7.m7.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p3.7.m7.4.4.3.3.3" xref="S4.SS2.p3.7.m7.4.4.3.3.3.cmml"><mi id="S4.SS2.p3.7.m7.4.4.3.3.3.2" xref="S4.SS2.p3.7.m7.4.4.3.3.3.2.cmml">v</mi><mi id="S4.SS2.p3.7.m7.4.4.3.3.3.3" xref="S4.SS2.p3.7.m7.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S4.SS2.p3.7.m7.4.4.3.3.8" xref="S4.SS2.p3.7.m7.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m7.4b"><apply id="S4.SS2.p3.7.m7.4.4.cmml" xref="S4.SS2.p3.7.m7.4.4"><eq id="S4.SS2.p3.7.m7.4.4.4.cmml" xref="S4.SS2.p3.7.m7.4.4.4"></eq><ci id="S4.SS2.p3.7.m7.4.4.5.cmml" xref="S4.SS2.p3.7.m7.4.4.5">𝑄</ci><set id="S4.SS2.p3.7.m7.4.4.3.4.cmml" xref="S4.SS2.p3.7.m7.4.4.3.3"><apply id="S4.SS2.p3.7.m7.2.2.1.1.1.cmml" xref="S4.SS2.p3.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m7.2.2.1.1.1.1.cmml" xref="S4.SS2.p3.7.m7.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.7.m7.2.2.1.1.1.2.cmml" xref="S4.SS2.p3.7.m7.2.2.1.1.1.2">𝑣</ci><cn type="integer" id="S4.SS2.p3.7.m7.2.2.1.1.1.3.cmml" xref="S4.SS2.p3.7.m7.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p3.7.m7.3.3.2.2.2.cmml" xref="S4.SS2.p3.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m7.3.3.2.2.2.1.cmml" xref="S4.SS2.p3.7.m7.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p3.7.m7.3.3.2.2.2.2.cmml" xref="S4.SS2.p3.7.m7.3.3.2.2.2.2">𝑣</ci><cn type="integer" id="S4.SS2.p3.7.m7.3.3.2.2.2.3.cmml" xref="S4.SS2.p3.7.m7.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p3.7.m7.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1">…</ci><apply id="S4.SS2.p3.7.m7.4.4.3.3.3.cmml" xref="S4.SS2.p3.7.m7.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m7.4.4.3.3.3.1.cmml" xref="S4.SS2.p3.7.m7.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p3.7.m7.4.4.3.3.3.2.cmml" xref="S4.SS2.p3.7.m7.4.4.3.3.3.2">𝑣</ci><ci id="S4.SS2.p3.7.m7.4.4.3.3.3.3.cmml" xref="S4.SS2.p3.7.m7.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m7.4c">Q=\{v_{1},v_{2},\dots,v_{T}\}</annotation></semantics></math>, where <math id="S4.SS2.p3.8.m8.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S4.SS2.p3.8.m8.1a"><msub id="S4.SS2.p3.8.m8.1.1" xref="S4.SS2.p3.8.m8.1.1.cmml"><mi id="S4.SS2.p3.8.m8.1.1.2" xref="S4.SS2.p3.8.m8.1.1.2.cmml">v</mi><mi id="S4.SS2.p3.8.m8.1.1.3" xref="S4.SS2.p3.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m8.1b"><apply id="S4.SS2.p3.8.m8.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m8.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.p3.8.m8.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.2">𝑣</ci><ci id="S4.SS2.p3.8.m8.1.1.3.cmml" xref="S4.SS2.p3.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m8.1c">v_{i}</annotation></semantics></math> is the <math id="S4.SS2.p3.9.m9.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S4.SS2.p3.9.m9.1a"><msup id="S4.SS2.p3.9.m9.1.1" xref="S4.SS2.p3.9.m9.1.1.cmml"><mi id="S4.SS2.p3.9.m9.1.1.2" xref="S4.SS2.p3.9.m9.1.1.2.cmml">i</mi><mrow id="S4.SS2.p3.9.m9.1.1.3" xref="S4.SS2.p3.9.m9.1.1.3.cmml"><mi id="S4.SS2.p3.9.m9.1.1.3.2" xref="S4.SS2.p3.9.m9.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.9.m9.1.1.3.1" xref="S4.SS2.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p3.9.m9.1.1.3.3" xref="S4.SS2.p3.9.m9.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.9.m9.1b"><apply id="S4.SS2.p3.9.m9.1.1.cmml" xref="S4.SS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.9.m9.1.1.1.cmml" xref="S4.SS2.p3.9.m9.1.1">superscript</csymbol><ci id="S4.SS2.p3.9.m9.1.1.2.cmml" xref="S4.SS2.p3.9.m9.1.1.2">𝑖</ci><apply id="S4.SS2.p3.9.m9.1.1.3.cmml" xref="S4.SS2.p3.9.m9.1.1.3"><times id="S4.SS2.p3.9.m9.1.1.3.1.cmml" xref="S4.SS2.p3.9.m9.1.1.3.1"></times><ci id="S4.SS2.p3.9.m9.1.1.3.2.cmml" xref="S4.SS2.p3.9.m9.1.1.3.2">𝑡</ci><ci id="S4.SS2.p3.9.m9.1.1.3.3.cmml" xref="S4.SS2.p3.9.m9.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.9.m9.1c">i^{th}</annotation></semantics></math>
word, encoded as a one-hot vector over a fixed vocabulary <math id="S4.SS2.p3.10.m10.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S4.SS2.p3.10.m10.1a"><mi id="S4.SS2.p3.10.m10.1.1" xref="S4.SS2.p3.10.m10.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.10.m10.1b"><ci id="S4.SS2.p3.10.m10.1.1.cmml" xref="S4.SS2.p3.10.m10.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.10.m10.1c">V</annotation></semantics></math> of size <math id="S4.SS2.p3.11.m11.1" class="ltx_Math" alttext="d_{V}" display="inline"><semantics id="S4.SS2.p3.11.m11.1a"><msub id="S4.SS2.p3.11.m11.1.1" xref="S4.SS2.p3.11.m11.1.1.cmml"><mi id="S4.SS2.p3.11.m11.1.1.2" xref="S4.SS2.p3.11.m11.1.1.2.cmml">d</mi><mi id="S4.SS2.p3.11.m11.1.1.3" xref="S4.SS2.p3.11.m11.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.11.m11.1b"><apply id="S4.SS2.p3.11.m11.1.1.cmml" xref="S4.SS2.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m11.1.1.1.cmml" xref="S4.SS2.p3.11.m11.1.1">subscript</csymbol><ci id="S4.SS2.p3.11.m11.1.1.2.cmml" xref="S4.SS2.p3.11.m11.1.1.2">𝑑</ci><ci id="S4.SS2.p3.11.m11.1.1.3.cmml" xref="S4.SS2.p3.11.m11.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.11.m11.1c">d_{V}</annotation></semantics></math>. For all the models, we
first obtain question token embeddings <math id="S4.SS2.p3.12.m12.1" class="ltx_Math" alttext="E=\{W_{w}^{T}v_{i}\}_{i=1}^{T}" display="inline"><semantics id="S4.SS2.p3.12.m12.1a"><mrow id="S4.SS2.p3.12.m12.1.1" xref="S4.SS2.p3.12.m12.1.1.cmml"><mi id="S4.SS2.p3.12.m12.1.1.3" xref="S4.SS2.p3.12.m12.1.1.3.cmml">E</mi><mo id="S4.SS2.p3.12.m12.1.1.2" xref="S4.SS2.p3.12.m12.1.1.2.cmml">=</mo><msubsup id="S4.SS2.p3.12.m12.1.1.1" xref="S4.SS2.p3.12.m12.1.1.1.cmml"><mrow id="S4.SS2.p3.12.m12.1.1.1.1.1.1" xref="S4.SS2.p3.12.m12.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p3.12.m12.1.1.1.1.1.1.2" xref="S4.SS2.p3.12.m12.1.1.1.1.1.2.cmml">{</mo><mrow id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.cmml"><msubsup id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.2" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.2.cmml">W</mi><mi id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.3" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.3.cmml">w</mi><mi id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.3" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.1" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.2" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.2.cmml">v</mi><mi id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.3" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.SS2.p3.12.m12.1.1.1.1.1.1.3" xref="S4.SS2.p3.12.m12.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p3.12.m12.1.1.1.1.3" xref="S4.SS2.p3.12.m12.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.12.m12.1.1.1.1.3.2" xref="S4.SS2.p3.12.m12.1.1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.12.m12.1.1.1.1.3.1" xref="S4.SS2.p3.12.m12.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.12.m12.1.1.1.1.3.3" xref="S4.SS2.p3.12.m12.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.12.m12.1.1.1.3" xref="S4.SS2.p3.12.m12.1.1.1.3.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.12.m12.1b"><apply id="S4.SS2.p3.12.m12.1.1.cmml" xref="S4.SS2.p3.12.m12.1.1"><eq id="S4.SS2.p3.12.m12.1.1.2.cmml" xref="S4.SS2.p3.12.m12.1.1.2"></eq><ci id="S4.SS2.p3.12.m12.1.1.3.cmml" xref="S4.SS2.p3.12.m12.1.1.3">𝐸</ci><apply id="S4.SS2.p3.12.m12.1.1.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.12.m12.1.1.1.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1">superscript</csymbol><apply id="S4.SS2.p3.12.m12.1.1.1.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.12.m12.1.1.1.1.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1">subscript</csymbol><set id="S4.SS2.p3.12.m12.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1"><apply id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1"><times id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.1"></times><apply id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.2">𝑊</ci><ci id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.2.3">𝑤</ci></apply><ci id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.2">𝑣</ci><ci id="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></set><apply id="S4.SS2.p3.12.m12.1.1.1.1.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.3"><eq id="S4.SS2.p3.12.m12.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.3.1"></eq><ci id="S4.SS2.p3.12.m12.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S4.SS2.p3.12.m12.1.1.1.1.3.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.12.m12.1.1.1.3.cmml" xref="S4.SS2.p3.12.m12.1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.12.m12.1c">E=\{W_{w}^{T}v_{i}\}_{i=1}^{T}</annotation></semantics></math>, where <math id="S4.SS2.p3.13.m13.1" class="ltx_Math" alttext="W_{w}\in\mathbb{R}^{d_{V}\times d_{q}}" display="inline"><semantics id="S4.SS2.p3.13.m13.1a"><mrow id="S4.SS2.p3.13.m13.1.1" xref="S4.SS2.p3.13.m13.1.1.cmml"><msub id="S4.SS2.p3.13.m13.1.1.2" xref="S4.SS2.p3.13.m13.1.1.2.cmml"><mi id="S4.SS2.p3.13.m13.1.1.2.2" xref="S4.SS2.p3.13.m13.1.1.2.2.cmml">W</mi><mi id="S4.SS2.p3.13.m13.1.1.2.3" xref="S4.SS2.p3.13.m13.1.1.2.3.cmml">w</mi></msub><mo id="S4.SS2.p3.13.m13.1.1.1" xref="S4.SS2.p3.13.m13.1.1.1.cmml">∈</mo><msup id="S4.SS2.p3.13.m13.1.1.3" xref="S4.SS2.p3.13.m13.1.1.3.cmml"><mi id="S4.SS2.p3.13.m13.1.1.3.2" xref="S4.SS2.p3.13.m13.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p3.13.m13.1.1.3.3" xref="S4.SS2.p3.13.m13.1.1.3.3.cmml"><msub id="S4.SS2.p3.13.m13.1.1.3.3.2" xref="S4.SS2.p3.13.m13.1.1.3.3.2.cmml"><mi id="S4.SS2.p3.13.m13.1.1.3.3.2.2" xref="S4.SS2.p3.13.m13.1.1.3.3.2.2.cmml">d</mi><mi id="S4.SS2.p3.13.m13.1.1.3.3.2.3" xref="S4.SS2.p3.13.m13.1.1.3.3.2.3.cmml">V</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.13.m13.1.1.3.3.1" xref="S4.SS2.p3.13.m13.1.1.3.3.1.cmml">×</mo><msub id="S4.SS2.p3.13.m13.1.1.3.3.3" xref="S4.SS2.p3.13.m13.1.1.3.3.3.cmml"><mi id="S4.SS2.p3.13.m13.1.1.3.3.3.2" xref="S4.SS2.p3.13.m13.1.1.3.3.3.2.cmml">d</mi><mi id="S4.SS2.p3.13.m13.1.1.3.3.3.3" xref="S4.SS2.p3.13.m13.1.1.3.3.3.3.cmml">q</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.13.m13.1b"><apply id="S4.SS2.p3.13.m13.1.1.cmml" xref="S4.SS2.p3.13.m13.1.1"><in id="S4.SS2.p3.13.m13.1.1.1.cmml" xref="S4.SS2.p3.13.m13.1.1.1"></in><apply id="S4.SS2.p3.13.m13.1.1.2.cmml" xref="S4.SS2.p3.13.m13.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m13.1.1.2.1.cmml" xref="S4.SS2.p3.13.m13.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.13.m13.1.1.2.2.cmml" xref="S4.SS2.p3.13.m13.1.1.2.2">𝑊</ci><ci id="S4.SS2.p3.13.m13.1.1.2.3.cmml" xref="S4.SS2.p3.13.m13.1.1.2.3">𝑤</ci></apply><apply id="S4.SS2.p3.13.m13.1.1.3.cmml" xref="S4.SS2.p3.13.m13.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m13.1.1.3.1.cmml" xref="S4.SS2.p3.13.m13.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.13.m13.1.1.3.2.cmml" xref="S4.SS2.p3.13.m13.1.1.3.2">ℝ</ci><apply id="S4.SS2.p3.13.m13.1.1.3.3.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3"><times id="S4.SS2.p3.13.m13.1.1.3.3.1.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.1"></times><apply id="S4.SS2.p3.13.m13.1.1.3.3.2.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m13.1.1.3.3.2.1.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.2">subscript</csymbol><ci id="S4.SS2.p3.13.m13.1.1.3.3.2.2.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.2.2">𝑑</ci><ci id="S4.SS2.p3.13.m13.1.1.3.3.2.3.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.2.3">𝑉</ci></apply><apply id="S4.SS2.p3.13.m13.1.1.3.3.3.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m13.1.1.3.3.3.1.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.3">subscript</csymbol><ci id="S4.SS2.p3.13.m13.1.1.3.3.3.2.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.3.2">𝑑</ci><ci id="S4.SS2.p3.13.m13.1.1.3.3.3.3.cmml" xref="S4.SS2.p3.13.m13.1.1.3.3.3.3">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.13.m13.1c">W_{w}\in\mathbb{R}^{d_{V}\times d_{q}}</annotation></semantics></math> is a continuous word-vector embedding matrix. We obtain the question embedding feature
using an LSTM-attention module, i.e., <math id="S4.SS2.p3.14.m14.2" class="ltx_Math" alttext="q=\text{AttentionPool}(\text{LSTM}(E))\in\mathbb{R}^{d_{q}}" display="inline"><semantics id="S4.SS2.p3.14.m14.2a"><mrow id="S4.SS2.p3.14.m14.2.2" xref="S4.SS2.p3.14.m14.2.2.cmml"><mi id="S4.SS2.p3.14.m14.2.2.3" xref="S4.SS2.p3.14.m14.2.2.3.cmml">q</mi><mo id="S4.SS2.p3.14.m14.2.2.4" xref="S4.SS2.p3.14.m14.2.2.4.cmml">=</mo><mrow id="S4.SS2.p3.14.m14.2.2.1" xref="S4.SS2.p3.14.m14.2.2.1.cmml"><mtext id="S4.SS2.p3.14.m14.2.2.1.3" xref="S4.SS2.p3.14.m14.2.2.1.3a.cmml">AttentionPool</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p3.14.m14.2.2.1.2" xref="S4.SS2.p3.14.m14.2.2.1.2.cmml">​</mo><mrow id="S4.SS2.p3.14.m14.2.2.1.1.1" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.14.m14.2.2.1.1.1.2" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.14.m14.2.2.1.1.1.1" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml"><mtext id="S4.SS2.p3.14.m14.2.2.1.1.1.1.2" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.2a.cmml">LSTM</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p3.14.m14.2.2.1.1.1.1.1" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.1.cmml">​</mo><mrow id="S4.SS2.p3.14.m14.2.2.1.1.1.1.3.2" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.14.m14.2.2.1.1.1.1.3.2.1" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml">(</mo><mi id="S4.SS2.p3.14.m14.1.1" xref="S4.SS2.p3.14.m14.1.1.cmml">E</mi><mo stretchy="false" id="S4.SS2.p3.14.m14.2.2.1.1.1.1.3.2.2" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p3.14.m14.2.2.1.1.1.3" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.14.m14.2.2.5" xref="S4.SS2.p3.14.m14.2.2.5.cmml">∈</mo><msup id="S4.SS2.p3.14.m14.2.2.6" xref="S4.SS2.p3.14.m14.2.2.6.cmml"><mi id="S4.SS2.p3.14.m14.2.2.6.2" xref="S4.SS2.p3.14.m14.2.2.6.2.cmml">ℝ</mi><msub id="S4.SS2.p3.14.m14.2.2.6.3" xref="S4.SS2.p3.14.m14.2.2.6.3.cmml"><mi id="S4.SS2.p3.14.m14.2.2.6.3.2" xref="S4.SS2.p3.14.m14.2.2.6.3.2.cmml">d</mi><mi id="S4.SS2.p3.14.m14.2.2.6.3.3" xref="S4.SS2.p3.14.m14.2.2.6.3.3.cmml">q</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.14.m14.2b"><apply id="S4.SS2.p3.14.m14.2.2.cmml" xref="S4.SS2.p3.14.m14.2.2"><and id="S4.SS2.p3.14.m14.2.2a.cmml" xref="S4.SS2.p3.14.m14.2.2"></and><apply id="S4.SS2.p3.14.m14.2.2b.cmml" xref="S4.SS2.p3.14.m14.2.2"><eq id="S4.SS2.p3.14.m14.2.2.4.cmml" xref="S4.SS2.p3.14.m14.2.2.4"></eq><ci id="S4.SS2.p3.14.m14.2.2.3.cmml" xref="S4.SS2.p3.14.m14.2.2.3">𝑞</ci><apply id="S4.SS2.p3.14.m14.2.2.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1"><times id="S4.SS2.p3.14.m14.2.2.1.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.2"></times><ci id="S4.SS2.p3.14.m14.2.2.1.3a.cmml" xref="S4.SS2.p3.14.m14.2.2.1.3"><mtext id="S4.SS2.p3.14.m14.2.2.1.3.cmml" xref="S4.SS2.p3.14.m14.2.2.1.3">AttentionPool</mtext></ci><apply id="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1"><times id="S4.SS2.p3.14.m14.2.2.1.1.1.1.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.1"></times><ci id="S4.SS2.p3.14.m14.2.2.1.1.1.1.2a.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.2"><mtext id="S4.SS2.p3.14.m14.2.2.1.1.1.1.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.2">LSTM</mtext></ci><ci id="S4.SS2.p3.14.m14.1.1.cmml" xref="S4.SS2.p3.14.m14.1.1">𝐸</ci></apply></apply></apply><apply id="S4.SS2.p3.14.m14.2.2c.cmml" xref="S4.SS2.p3.14.m14.2.2"><in id="S4.SS2.p3.14.m14.2.2.5.cmml" xref="S4.SS2.p3.14.m14.2.2.5"></in><share href="#S4.SS2.p3.14.m14.2.2.1.cmml" id="S4.SS2.p3.14.m14.2.2d.cmml" xref="S4.SS2.p3.14.m14.2.2"></share><apply id="S4.SS2.p3.14.m14.2.2.6.cmml" xref="S4.SS2.p3.14.m14.2.2.6"><csymbol cd="ambiguous" id="S4.SS2.p3.14.m14.2.2.6.1.cmml" xref="S4.SS2.p3.14.m14.2.2.6">superscript</csymbol><ci id="S4.SS2.p3.14.m14.2.2.6.2.cmml" xref="S4.SS2.p3.14.m14.2.2.6.2">ℝ</ci><apply id="S4.SS2.p3.14.m14.2.2.6.3.cmml" xref="S4.SS2.p3.14.m14.2.2.6.3"><csymbol cd="ambiguous" id="S4.SS2.p3.14.m14.2.2.6.3.1.cmml" xref="S4.SS2.p3.14.m14.2.2.6.3">subscript</csymbol><ci id="S4.SS2.p3.14.m14.2.2.6.3.2.cmml" xref="S4.SS2.p3.14.m14.2.2.6.3.2">𝑑</ci><ci id="S4.SS2.p3.14.m14.2.2.6.3.3.cmml" xref="S4.SS2.p3.14.m14.2.2.6.3.3">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.14.m14.2c">q=\text{AttentionPool}(\text{LSTM}(E))\in\mathbb{R}^{d_{q}}</annotation></semantics></math>.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2008.11976/assets/images/concat_baseline.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="69" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.17.8.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.14.7" class="ltx_text ltx_font_bold" style="font-size:90%;">Concatenate-Feature Baseline<span id="S4.F7.14.7.7" class="ltx_text ltx_font_medium">. This method adapts a single-image VQA model
to an image set <math id="S4.F7.8.1.1.m1.1" class="ltx_Math" alttext="S=\{I_{1}\dots I_{n}\}" display="inline"><semantics id="S4.F7.8.1.1.m1.1b"><mrow id="S4.F7.8.1.1.m1.1.1" xref="S4.F7.8.1.1.m1.1.1.cmml"><mi id="S4.F7.8.1.1.m1.1.1.3" xref="S4.F7.8.1.1.m1.1.1.3.cmml">S</mi><mo id="S4.F7.8.1.1.m1.1.1.2" xref="S4.F7.8.1.1.m1.1.1.2.cmml">=</mo><mrow id="S4.F7.8.1.1.m1.1.1.1.1" xref="S4.F7.8.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.F7.8.1.1.m1.1.1.1.1.2" xref="S4.F7.8.1.1.m1.1.1.1.2.cmml">{</mo><mrow id="S4.F7.8.1.1.m1.1.1.1.1.1" xref="S4.F7.8.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.F7.8.1.1.m1.1.1.1.1.1.2" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.F7.8.1.1.m1.1.1.1.1.1.2.2" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2.2.cmml">I</mi><mn id="S4.F7.8.1.1.m1.1.1.1.1.1.2.3" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S4.F7.8.1.1.m1.1.1.1.1.1.1" xref="S4.F7.8.1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.F7.8.1.1.m1.1.1.1.1.1.3" xref="S4.F7.8.1.1.m1.1.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S4.F7.8.1.1.m1.1.1.1.1.1.1b" xref="S4.F7.8.1.1.m1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.F7.8.1.1.m1.1.1.1.1.1.4" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4.cmml"><mi id="S4.F7.8.1.1.m1.1.1.1.1.1.4.2" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4.2.cmml">I</mi><mi id="S4.F7.8.1.1.m1.1.1.1.1.1.4.3" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S4.F7.8.1.1.m1.1.1.1.1.3" xref="S4.F7.8.1.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.8.1.1.m1.1c"><apply id="S4.F7.8.1.1.m1.1.1.cmml" xref="S4.F7.8.1.1.m1.1.1"><eq id="S4.F7.8.1.1.m1.1.1.2.cmml" xref="S4.F7.8.1.1.m1.1.1.2"></eq><ci id="S4.F7.8.1.1.m1.1.1.3.cmml" xref="S4.F7.8.1.1.m1.1.1.3">𝑆</ci><set id="S4.F7.8.1.1.m1.1.1.1.2.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1"><apply id="S4.F7.8.1.1.m1.1.1.1.1.1.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1"><times id="S4.F7.8.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.1"></times><apply id="S4.F7.8.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.F7.8.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.F7.8.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2.2">𝐼</ci><cn type="integer" id="S4.F7.8.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.2.3">1</cn></apply><ci id="S4.F7.8.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.3">…</ci><apply id="S4.F7.8.1.1.m1.1.1.1.1.1.4.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.F7.8.1.1.m1.1.1.1.1.1.4.1.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.F7.8.1.1.m1.1.1.1.1.1.4.2.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4.2">𝐼</ci><ci id="S4.F7.8.1.1.m1.1.1.1.1.1.4.3.cmml" xref="S4.F7.8.1.1.m1.1.1.1.1.1.4.3">𝑛</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.8.1.1.m1.1d">S=\{I_{1}\dots I_{n}\}</annotation></semantics></math>. We first extract region proposals, <math id="S4.F7.9.2.2.m2.1" class="ltx_Math" alttext="R_{i}" display="inline"><semantics id="S4.F7.9.2.2.m2.1b"><msub id="S4.F7.9.2.2.m2.1.1" xref="S4.F7.9.2.2.m2.1.1.cmml"><mi id="S4.F7.9.2.2.m2.1.1.2" xref="S4.F7.9.2.2.m2.1.1.2.cmml">R</mi><mi id="S4.F7.9.2.2.m2.1.1.3" xref="S4.F7.9.2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F7.9.2.2.m2.1c"><apply id="S4.F7.9.2.2.m2.1.1.cmml" xref="S4.F7.9.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.F7.9.2.2.m2.1.1.1.cmml" xref="S4.F7.9.2.2.m2.1.1">subscript</csymbol><ci id="S4.F7.9.2.2.m2.1.1.2.cmml" xref="S4.F7.9.2.2.m2.1.1.2">𝑅</ci><ci id="S4.F7.9.2.2.m2.1.1.3.cmml" xref="S4.F7.9.2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.9.2.2.m2.1d">R_{i}</annotation></semantics></math> from each
image <math id="S4.F7.10.3.3.m3.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S4.F7.10.3.3.m3.1b"><msub id="S4.F7.10.3.3.m3.1.1" xref="S4.F7.10.3.3.m3.1.1.cmml"><mi id="S4.F7.10.3.3.m3.1.1.2" xref="S4.F7.10.3.3.m3.1.1.2.cmml">I</mi><mi id="S4.F7.10.3.3.m3.1.1.3" xref="S4.F7.10.3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F7.10.3.3.m3.1c"><apply id="S4.F7.10.3.3.m3.1.1.cmml" xref="S4.F7.10.3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.F7.10.3.3.m3.1.1.1.cmml" xref="S4.F7.10.3.3.m3.1.1">subscript</csymbol><ci id="S4.F7.10.3.3.m3.1.1.2.cmml" xref="S4.F7.10.3.3.m3.1.1.2">𝐼</ci><ci id="S4.F7.10.3.3.m3.1.1.3.cmml" xref="S4.F7.10.3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.10.3.3.m3.1d">I_{i}</annotation></semantics></math>. The model attends over the regions in each image separately using the question
embedding <math id="S4.F7.11.4.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.F7.11.4.4.m4.1b"><mi id="S4.F7.11.4.4.m4.1.1" xref="S4.F7.11.4.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.F7.11.4.4.m4.1c"><ci id="S4.F7.11.4.4.m4.1.1.cmml" xref="S4.F7.11.4.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.11.4.4.m4.1d">q</annotation></semantics></math>. Pooling the region features gives a representation of an image as <math id="S4.F7.12.5.5.m5.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.F7.12.5.5.m5.1b"><msub id="S4.F7.12.5.5.m5.1.1" xref="S4.F7.12.5.5.m5.1.1.cmml"><mi id="S4.F7.12.5.5.m5.1.1.2" xref="S4.F7.12.5.5.m5.1.1.2.cmml">x</mi><mi id="S4.F7.12.5.5.m5.1.1.3" xref="S4.F7.12.5.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F7.12.5.5.m5.1c"><apply id="S4.F7.12.5.5.m5.1.1.cmml" xref="S4.F7.12.5.5.m5.1.1"><csymbol cd="ambiguous" id="S4.F7.12.5.5.m5.1.1.1.cmml" xref="S4.F7.12.5.5.m5.1.1">subscript</csymbol><ci id="S4.F7.12.5.5.m5.1.1.2.cmml" xref="S4.F7.12.5.5.m5.1.1.2">𝑥</ci><ci id="S4.F7.12.5.5.m5.1.1.3.cmml" xref="S4.F7.12.5.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.12.5.5.m5.1d">x_{i}</annotation></semantics></math>. These are
concatenated and combined (element-wise multiplied) by the question embedding to give the joint
scene representation <math id="S4.F7.13.6.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.F7.13.6.6.m6.1b"><mi id="S4.F7.13.6.6.m6.1.1" xref="S4.F7.13.6.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.F7.13.6.6.m6.1c"><ci id="S4.F7.13.6.6.m6.1.1.cmml" xref="S4.F7.13.6.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.13.6.6.m6.1d">x</annotation></semantics></math>. We use fully-connected layers to predict the final answer <math id="S4.F7.14.7.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.F7.14.7.7.m7.1b"><mi id="S4.F7.14.7.7.m7.1.1" xref="S4.F7.14.7.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.F7.14.7.7.m7.1c"><ci id="S4.F7.14.7.7.m7.1.1.cmml" xref="S4.F7.14.7.7.m7.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.14.7.7.m7.1d">a</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.4" class="ltx_p">Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 Model Definitions ‣ 4 ISVQA Problem Formulation and Baselines ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows an outline of the model. For each image, <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><msub id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">I</mi><mi id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">𝐼</ci><ci id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">I_{i}</annotation></semantics></math>, we obtain the image
embedding, <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><msub id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">x</mi><mi id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">𝑥</ci><ci id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">x_{i}</annotation></semantics></math> by attending over the corresponding region features <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="R_{i}" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><msub id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mi id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">R</mi><mi id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">𝑅</ci><ci id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">R_{i}</annotation></semantics></math> using the question
embedding <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mi id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><ci id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">q</annotation></semantics></math>.</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.2" class="ltx_Math" alttext="x_{i}=\text{AttentionPool}(\text{Combine}(R_{i},q))" display="block"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><msub id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.3.2" xref="S4.E1.m1.2.2.3.2.cmml">x</mi><mi id="S4.E1.m1.2.2.3.3" xref="S4.E1.m1.2.2.3.3.cmml">i</mi></msub><mo id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml">=</mo><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.cmml"><mtext id="S4.E1.m1.2.2.1.3" xref="S4.E1.m1.2.2.1.3a.cmml">AttentionPool</mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.2.cmml">​</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><mtext id="S4.E1.m1.2.2.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.3a.cmml">Combine</mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">(</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">R</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">,</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">q</mi><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><eq id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"></eq><apply id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.3">subscript</csymbol><ci id="S4.E1.m1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.3.2">𝑥</ci><ci id="S4.E1.m1.2.2.3.3.cmml" xref="S4.E1.m1.2.2.3.3">𝑖</ci></apply><apply id="S4.E1.m1.2.2.1.cmml" xref="S4.E1.m1.2.2.1"><times id="S4.E1.m1.2.2.1.2.cmml" xref="S4.E1.m1.2.2.1.2"></times><ci id="S4.E1.m1.2.2.1.3a.cmml" xref="S4.E1.m1.2.2.1.3"><mtext id="S4.E1.m1.2.2.1.3.cmml" xref="S4.E1.m1.2.2.1.3">AttentionPool</mtext></ci><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"></times><ci id="S4.E1.m1.2.2.1.1.1.1.3a.cmml" xref="S4.E1.m1.2.2.1.1.1.1.3"><mtext id="S4.E1.m1.2.2.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.3">Combine</mtext></ci><interval closure="open" id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2">𝑅</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝑞</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">x_{i}=\text{AttentionPool}(\text{Combine}(R_{i},q))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p4.9" class="ltx_p">where, we use element-wise multiplication (after projecting to suitable dimensions) as the
<span id="S4.SS2.p4.9.1" class="ltx_text ltx_markedasmath">Combine</span> layer and <span id="S4.SS2.p4.9.2" class="ltx_text ltx_markedasmath">AttentionPool</span> is a combination of an <span id="S4.SS2.p4.9.3" class="ltx_text ltx_markedasmath">Attention</span> module
over the region features which is calculated through a softmax operation and a <span id="S4.SS2.p4.9.4" class="ltx_text ltx_markedasmath">Pool</span>
operation. The region features are multiplied by the attention and added to obtain the pooled image
representation. For a single image, this model is an adaptation of the recent Pythia model
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> without its OCR functionality. We concatenate the image features <math id="S4.SS2.p4.9.m5.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS2.p4.9.m5.1a"><msub id="S4.SS2.p4.9.m5.1.1" xref="S4.SS2.p4.9.m5.1.1.cmml"><mi id="S4.SS2.p4.9.m5.1.1.2" xref="S4.SS2.p4.9.m5.1.1.2.cmml">x</mi><mi id="S4.SS2.p4.9.m5.1.1.3" xref="S4.SS2.p4.9.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.9.m5.1b"><apply id="S4.SS2.p4.9.m5.1.1.cmml" xref="S4.SS2.p4.9.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.9.m5.1.1.1.cmml" xref="S4.SS2.p4.9.m5.1.1">subscript</csymbol><ci id="S4.SS2.p4.9.m5.1.1.2.cmml" xref="S4.SS2.p4.9.m5.1.1.2">𝑥</ci><ci id="S4.SS2.p4.9.m5.1.1.3.cmml" xref="S4.SS2.p4.9.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.9.m5.1c">x_{i}</annotation></semantics></math> and
element-wise multiply by the question embedding to obtain the joint embedding</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.3" class="ltx_Math" alttext="x=\text{Combine}(\text{Concat}(x_{1},x_{2},\dots,x_{n}),q)" display="block"><semantics id="S4.E2.m1.3a"><mrow id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml"><mi id="S4.E2.m1.3.3.3" xref="S4.E2.m1.3.3.3.cmml">x</mi><mo id="S4.E2.m1.3.3.2" xref="S4.E2.m1.3.3.2.cmml">=</mo><mrow id="S4.E2.m1.3.3.1" xref="S4.E2.m1.3.3.1.cmml"><mtext id="S4.E2.m1.3.3.1.3" xref="S4.E2.m1.3.3.1.3a.cmml">Combine</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.2" xref="S4.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S4.E2.m1.3.3.1.1.1" xref="S4.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.2" xref="S4.E2.m1.3.3.1.1.2.cmml">(</mo><mrow id="S4.E2.m1.3.3.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.cmml"><mtext id="S4.E2.m1.3.3.1.1.1.1.5" xref="S4.E2.m1.3.3.1.1.1.1.5a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.4" xref="S4.E2.m1.3.3.1.1.1.1.4.cmml">​</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.3.3" xref="S4.E2.m1.3.3.1.1.1.1.3.4.cmml"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.1.3.3.4" xref="S4.E2.m1.3.3.1.1.1.1.3.4.cmml">(</mo><msub id="S4.E2.m1.3.3.1.1.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.E2.m1.3.3.1.1.1.1.3.3.5" xref="S4.E2.m1.3.3.1.1.1.1.3.4.cmml">,</mo><msub id="S4.E2.m1.3.3.1.1.1.1.2.2.2" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.2.2.2.2" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S4.E2.m1.3.3.1.1.1.1.2.2.2.3" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S4.E2.m1.3.3.1.1.1.1.3.3.6" xref="S4.E2.m1.3.3.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">…</mi><mo id="S4.E2.m1.3.3.1.1.1.1.3.3.7" xref="S4.E2.m1.3.3.1.1.1.1.3.4.cmml">,</mo><msub id="S4.E2.m1.3.3.1.1.1.1.3.3.3" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.3.3.3.2" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3.2.cmml">x</mi><mi id="S4.E2.m1.3.3.1.1.1.1.3.3.3.3" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.1.3.3.8" xref="S4.E2.m1.3.3.1.1.1.1.3.4.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.3.3.1.1.1.3" xref="S4.E2.m1.3.3.1.1.2.cmml">,</mo><mi id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml">q</mi><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.4" xref="S4.E2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.3b"><apply id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3"><eq id="S4.E2.m1.3.3.2.cmml" xref="S4.E2.m1.3.3.2"></eq><ci id="S4.E2.m1.3.3.3.cmml" xref="S4.E2.m1.3.3.3">𝑥</ci><apply id="S4.E2.m1.3.3.1.cmml" xref="S4.E2.m1.3.3.1"><times id="S4.E2.m1.3.3.1.2.cmml" xref="S4.E2.m1.3.3.1.2"></times><ci id="S4.E2.m1.3.3.1.3a.cmml" xref="S4.E2.m1.3.3.1.3"><mtext id="S4.E2.m1.3.3.1.3.cmml" xref="S4.E2.m1.3.3.1.3">Combine</mtext></ci><interval closure="open" id="S4.E2.m1.3.3.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1"><apply id="S4.E2.m1.3.3.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1"><times id="S4.E2.m1.3.3.1.1.1.1.4.cmml" xref="S4.E2.m1.3.3.1.1.1.1.4"></times><ci id="S4.E2.m1.3.3.1.1.1.1.5a.cmml" xref="S4.E2.m1.3.3.1.1.1.1.5"><mtext id="S4.E2.m1.3.3.1.1.1.1.5.cmml" xref="S4.E2.m1.3.3.1.1.1.1.5">Concat</mtext></ci><vector id="S4.E2.m1.3.3.1.1.1.1.3.4.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.3"><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S4.E2.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.2.2.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.2.2.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.2">𝑥</ci><cn type="integer" id="S4.E2.m1.3.3.1.1.1.1.2.2.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.3">2</cn></apply><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">…</ci><apply id="S4.E2.m1.3.3.1.1.1.1.3.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3.2">𝑥</ci><ci id="S4.E2.m1.3.3.1.1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.3.3.3">𝑛</ci></apply></vector></apply><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝑞</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.3c">x=\text{Combine}(\text{Concat}(x_{1},x_{2},\dots,x_{n}),q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p4.11" class="ltx_p">where the <span id="S4.SS2.p4.11.1" class="ltx_text ltx_markedasmath">Combine</span> layer is again an element-wise multiplication. This is passed through a
small MLP to obtain the distribution over answers, <math id="S4.SS2.p4.11.m2.1" class="ltx_Math" alttext="P_{A}=\text{MLP}(x)" display="inline"><semantics id="S4.SS2.p4.11.m2.1a"><mrow id="S4.SS2.p4.11.m2.1.2" xref="S4.SS2.p4.11.m2.1.2.cmml"><msub id="S4.SS2.p4.11.m2.1.2.2" xref="S4.SS2.p4.11.m2.1.2.2.cmml"><mi id="S4.SS2.p4.11.m2.1.2.2.2" xref="S4.SS2.p4.11.m2.1.2.2.2.cmml">P</mi><mi id="S4.SS2.p4.11.m2.1.2.2.3" xref="S4.SS2.p4.11.m2.1.2.2.3.cmml">A</mi></msub><mo id="S4.SS2.p4.11.m2.1.2.1" xref="S4.SS2.p4.11.m2.1.2.1.cmml">=</mo><mrow id="S4.SS2.p4.11.m2.1.2.3" xref="S4.SS2.p4.11.m2.1.2.3.cmml"><mtext id="S4.SS2.p4.11.m2.1.2.3.2" xref="S4.SS2.p4.11.m2.1.2.3.2a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p4.11.m2.1.2.3.1" xref="S4.SS2.p4.11.m2.1.2.3.1.cmml">​</mo><mrow id="S4.SS2.p4.11.m2.1.2.3.3.2" xref="S4.SS2.p4.11.m2.1.2.3.cmml"><mo stretchy="false" id="S4.SS2.p4.11.m2.1.2.3.3.2.1" xref="S4.SS2.p4.11.m2.1.2.3.cmml">(</mo><mi id="S4.SS2.p4.11.m2.1.1" xref="S4.SS2.p4.11.m2.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS2.p4.11.m2.1.2.3.3.2.2" xref="S4.SS2.p4.11.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.11.m2.1b"><apply id="S4.SS2.p4.11.m2.1.2.cmml" xref="S4.SS2.p4.11.m2.1.2"><eq id="S4.SS2.p4.11.m2.1.2.1.cmml" xref="S4.SS2.p4.11.m2.1.2.1"></eq><apply id="S4.SS2.p4.11.m2.1.2.2.cmml" xref="S4.SS2.p4.11.m2.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p4.11.m2.1.2.2.1.cmml" xref="S4.SS2.p4.11.m2.1.2.2">subscript</csymbol><ci id="S4.SS2.p4.11.m2.1.2.2.2.cmml" xref="S4.SS2.p4.11.m2.1.2.2.2">𝑃</ci><ci id="S4.SS2.p4.11.m2.1.2.2.3.cmml" xref="S4.SS2.p4.11.m2.1.2.2.3">𝐴</ci></apply><apply id="S4.SS2.p4.11.m2.1.2.3.cmml" xref="S4.SS2.p4.11.m2.1.2.3"><times id="S4.SS2.p4.11.m2.1.2.3.1.cmml" xref="S4.SS2.p4.11.m2.1.2.3.1"></times><ci id="S4.SS2.p4.11.m2.1.2.3.2a.cmml" xref="S4.SS2.p4.11.m2.1.2.3.2"><mtext id="S4.SS2.p4.11.m2.1.2.3.2.cmml" xref="S4.SS2.p4.11.m2.1.2.3.2">MLP</mtext></ci><ci id="S4.SS2.p4.11.m2.1.1.cmml" xref="S4.SS2.p4.11.m2.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.11.m2.1c">P_{A}=\text{MLP}(x)</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Stitched Image.</span>
Our next baseline is also an adaptation of existing single-image VQA methods. We start by
stitching all the images in an image set into a mosaic, similar to the ones shown in figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 Problem Definition ‣ 4 ISVQA Problem Formulation and Baselines ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Note that the ISVQA setting does not require the images in an image set to
follow an order. Therefore, the stitched image obtained need not be panoramic. We train the recent
Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> model on the stitched images and report performance in table <a href="#S5.T2" title="Table 2 ‣ 5.3 Results ‣ 5 Experiments ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Video VQA.</span>
To highlight the differences between Video VQA and ISVQA, we adapt the recent state-of-the-art
method HME-VideoQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This model consists of heterogeneous memory module which can
potentially learn global context information. We consider images in the image set as frames of a
video. Note that, the images in an image set in ISVQA do not necessarily constitute the frames of a
video. Therefore, it is reasonable to expect such Video VQA methods to not provide any advantages
over our baselines.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">Using these baselines, we show that ISVQA is not a trivial extension of VQA. Solving ISVQA requires development of specialized methods.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para ltx_noindent">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">Transformer-based Method.</span>
We utilize the power of transformers and adapt the LXMERT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> to both
cross-modality and cross-image scenarios. The transformer can summarize the relevant information
within an image set and also model the across-image finer-grained dependencies. Here, we briefly
described the original LXMERT model and then describe our modifications.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">LXMERT learns cross-modality representations between regions in an image and sentences. It first
uses separate visual and language encoders to obtain visual and semantic embeddings. The visual
encoder consists of several self-attention sub-layers which help in encoding the relationships
between objects. Similarly, the language encoder consists of multiple self-attention sub-layers and
feed-forward sub-layers which provide a semantic embedding for the sentence or question. The visual
and semantic embeddings are then used to attend to each other via cross-attention sub-layers. This
helps the LXMERT model learn final visual and language embeddings which can tightly couple the
information from visual and semantic domains. These coupled embeddings can be seen as the joint
representations of the image and sentence and are used for inference.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.3" class="ltx_p">Instead of using features from only a single image as input to the object-relationship encoder, we
propose to use the region features from each image in our image-set. As described above, we start by
extracting <math id="S4.SS2.p10.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p10.1.m1.1a"><mi id="S4.SS2.p10.1.m1.1.1" xref="S4.SS2.p10.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.1.m1.1b"><ci id="S4.SS2.p10.1.m1.1.1.cmml" xref="S4.SS2.p10.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.1.m1.1c">p</annotation></semantics></math> region proposals and the corresponding features from each of the <math id="S4.SS2.p10.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.p10.2.m2.1a"><mi id="S4.SS2.p10.2.m2.1.1" xref="S4.SS2.p10.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.2.m2.1b"><ci id="S4.SS2.p10.2.m2.1.1.cmml" xref="S4.SS2.p10.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.2.m2.1c">n</annotation></semantics></math> images in the
image set. We pass the <math id="S4.SS2.p10.3.m3.1" class="ltx_Math" alttext="p\times n" display="inline"><semantics id="S4.SS2.p10.3.m3.1a"><mrow id="S4.SS2.p10.3.m3.1.1" xref="S4.SS2.p10.3.m3.1.1.cmml"><mi id="S4.SS2.p10.3.m3.1.1.2" xref="S4.SS2.p10.3.m3.1.1.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p10.3.m3.1.1.1" xref="S4.SS2.p10.3.m3.1.1.1.cmml">×</mo><mi id="S4.SS2.p10.3.m3.1.1.3" xref="S4.SS2.p10.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.3.m3.1b"><apply id="S4.SS2.p10.3.m3.1.1.cmml" xref="S4.SS2.p10.3.m3.1.1"><times id="S4.SS2.p10.3.m3.1.1.1.cmml" xref="S4.SS2.p10.3.m3.1.1.1"></times><ci id="S4.SS2.p10.3.m3.1.1.2.cmml" xref="S4.SS2.p10.3.m3.1.1.2">𝑝</ci><ci id="S4.SS2.p10.3.m3.1.1.3.cmml" xref="S4.SS2.p10.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.3.m3.1c">p\times n</annotation></semantics></math> region features as inputs to the object-relationship encoder in
LXMERT. We note that this enables the our model to encode relationships between objects across
different images.</p>
</div>
<div id="S4.SS2.p11" class="ltx_para">
<p id="S4.SS2.p11.10" class="ltx_p">Let us denote the image features as <math id="S4.SS2.p11.1.m1.4" class="ltx_Math" alttext="R=[R_{1};R_{2};\dots;R_{n}]\in\mathbb{R}^{pn\times d}" display="inline"><semantics id="S4.SS2.p11.1.m1.4a"><mrow id="S4.SS2.p11.1.m1.4.4" xref="S4.SS2.p11.1.m1.4.4.cmml"><mi id="S4.SS2.p11.1.m1.4.4.5" xref="S4.SS2.p11.1.m1.4.4.5.cmml">R</mi><mo id="S4.SS2.p11.1.m1.4.4.6" xref="S4.SS2.p11.1.m1.4.4.6.cmml">=</mo><mrow id="S4.SS2.p11.1.m1.4.4.3.3" xref="S4.SS2.p11.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p11.1.m1.4.4.3.3.4" xref="S4.SS2.p11.1.m1.4.4.3.4.cmml">[</mo><msub id="S4.SS2.p11.1.m1.2.2.1.1.1" xref="S4.SS2.p11.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.p11.1.m1.2.2.1.1.1.2" xref="S4.SS2.p11.1.m1.2.2.1.1.1.2.cmml">R</mi><mn id="S4.SS2.p11.1.m1.2.2.1.1.1.3" xref="S4.SS2.p11.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p11.1.m1.4.4.3.3.5" xref="S4.SS2.p11.1.m1.4.4.3.4.cmml">;</mo><msub id="S4.SS2.p11.1.m1.3.3.2.2.2" xref="S4.SS2.p11.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.p11.1.m1.3.3.2.2.2.2" xref="S4.SS2.p11.1.m1.3.3.2.2.2.2.cmml">R</mi><mn id="S4.SS2.p11.1.m1.3.3.2.2.2.3" xref="S4.SS2.p11.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p11.1.m1.4.4.3.3.6" xref="S4.SS2.p11.1.m1.4.4.3.4.cmml">;</mo><mi mathvariant="normal" id="S4.SS2.p11.1.m1.1.1" xref="S4.SS2.p11.1.m1.1.1.cmml">…</mi><mo id="S4.SS2.p11.1.m1.4.4.3.3.7" xref="S4.SS2.p11.1.m1.4.4.3.4.cmml">;</mo><msub id="S4.SS2.p11.1.m1.4.4.3.3.3" xref="S4.SS2.p11.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.p11.1.m1.4.4.3.3.3.2" xref="S4.SS2.p11.1.m1.4.4.3.3.3.2.cmml">R</mi><mi id="S4.SS2.p11.1.m1.4.4.3.3.3.3" xref="S4.SS2.p11.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.SS2.p11.1.m1.4.4.3.3.8" xref="S4.SS2.p11.1.m1.4.4.3.4.cmml">]</mo></mrow><mo id="S4.SS2.p11.1.m1.4.4.7" xref="S4.SS2.p11.1.m1.4.4.7.cmml">∈</mo><msup id="S4.SS2.p11.1.m1.4.4.8" xref="S4.SS2.p11.1.m1.4.4.8.cmml"><mi id="S4.SS2.p11.1.m1.4.4.8.2" xref="S4.SS2.p11.1.m1.4.4.8.2.cmml">ℝ</mi><mrow id="S4.SS2.p11.1.m1.4.4.8.3" xref="S4.SS2.p11.1.m1.4.4.8.3.cmml"><mrow id="S4.SS2.p11.1.m1.4.4.8.3.2" xref="S4.SS2.p11.1.m1.4.4.8.3.2.cmml"><mi id="S4.SS2.p11.1.m1.4.4.8.3.2.2" xref="S4.SS2.p11.1.m1.4.4.8.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p11.1.m1.4.4.8.3.2.1" xref="S4.SS2.p11.1.m1.4.4.8.3.2.1.cmml">​</mo><mi id="S4.SS2.p11.1.m1.4.4.8.3.2.3" xref="S4.SS2.p11.1.m1.4.4.8.3.2.3.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p11.1.m1.4.4.8.3.1" xref="S4.SS2.p11.1.m1.4.4.8.3.1.cmml">×</mo><mi id="S4.SS2.p11.1.m1.4.4.8.3.3" xref="S4.SS2.p11.1.m1.4.4.8.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.1.m1.4b"><apply id="S4.SS2.p11.1.m1.4.4.cmml" xref="S4.SS2.p11.1.m1.4.4"><and id="S4.SS2.p11.1.m1.4.4a.cmml" xref="S4.SS2.p11.1.m1.4.4"></and><apply id="S4.SS2.p11.1.m1.4.4b.cmml" xref="S4.SS2.p11.1.m1.4.4"><eq id="S4.SS2.p11.1.m1.4.4.6.cmml" xref="S4.SS2.p11.1.m1.4.4.6"></eq><ci id="S4.SS2.p11.1.m1.4.4.5.cmml" xref="S4.SS2.p11.1.m1.4.4.5">𝑅</ci><list id="S4.SS2.p11.1.m1.4.4.3.4.cmml" xref="S4.SS2.p11.1.m1.4.4.3.3"><apply id="S4.SS2.p11.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p11.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.p11.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p11.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.p11.1.m1.2.2.1.1.1.2">𝑅</ci><cn type="integer" id="S4.SS2.p11.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS2.p11.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p11.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.p11.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p11.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.p11.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p11.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.p11.1.m1.3.3.2.2.2.2">𝑅</ci><cn type="integer" id="S4.SS2.p11.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS2.p11.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p11.1.m1.1.1.cmml" xref="S4.SS2.p11.1.m1.1.1">…</ci><apply id="S4.SS2.p11.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.p11.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p11.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.p11.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p11.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.p11.1.m1.4.4.3.3.3.2">𝑅</ci><ci id="S4.SS2.p11.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.p11.1.m1.4.4.3.3.3.3">𝑛</ci></apply></list></apply><apply id="S4.SS2.p11.1.m1.4.4c.cmml" xref="S4.SS2.p11.1.m1.4.4"><in id="S4.SS2.p11.1.m1.4.4.7.cmml" xref="S4.SS2.p11.1.m1.4.4.7"></in><share href="#S4.SS2.p11.1.m1.4.4.3.cmml" id="S4.SS2.p11.1.m1.4.4d.cmml" xref="S4.SS2.p11.1.m1.4.4"></share><apply id="S4.SS2.p11.1.m1.4.4.8.cmml" xref="S4.SS2.p11.1.m1.4.4.8"><csymbol cd="ambiguous" id="S4.SS2.p11.1.m1.4.4.8.1.cmml" xref="S4.SS2.p11.1.m1.4.4.8">superscript</csymbol><ci id="S4.SS2.p11.1.m1.4.4.8.2.cmml" xref="S4.SS2.p11.1.m1.4.4.8.2">ℝ</ci><apply id="S4.SS2.p11.1.m1.4.4.8.3.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3"><times id="S4.SS2.p11.1.m1.4.4.8.3.1.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3.1"></times><apply id="S4.SS2.p11.1.m1.4.4.8.3.2.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3.2"><times id="S4.SS2.p11.1.m1.4.4.8.3.2.1.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3.2.1"></times><ci id="S4.SS2.p11.1.m1.4.4.8.3.2.2.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3.2.2">𝑝</ci><ci id="S4.SS2.p11.1.m1.4.4.8.3.2.3.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3.2.3">𝑛</ci></apply><ci id="S4.SS2.p11.1.m1.4.4.8.3.3.cmml" xref="S4.SS2.p11.1.m1.4.4.8.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.1.m1.4c">R=[R_{1};R_{2};\dots;R_{n}]\in\mathbb{R}^{pn\times d}</annotation></semantics></math>, where <math id="S4.SS2.p11.2.m2.1" class="ltx_Math" alttext="R_{i}" display="inline"><semantics id="S4.SS2.p11.2.m2.1a"><msub id="S4.SS2.p11.2.m2.1.1" xref="S4.SS2.p11.2.m2.1.1.cmml"><mi id="S4.SS2.p11.2.m2.1.1.2" xref="S4.SS2.p11.2.m2.1.1.2.cmml">R</mi><mi id="S4.SS2.p11.2.m2.1.1.3" xref="S4.SS2.p11.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.2.m2.1b"><apply id="S4.SS2.p11.2.m2.1.1.cmml" xref="S4.SS2.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.2.m2.1.1.1.cmml" xref="S4.SS2.p11.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p11.2.m2.1.1.2.cmml" xref="S4.SS2.p11.2.m2.1.1.2">𝑅</ci><ci id="S4.SS2.p11.2.m2.1.1.3.cmml" xref="S4.SS2.p11.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.2.m2.1c">R_{i}</annotation></semantics></math> are the region features obtained from <math id="S4.SS2.p11.3.m3.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S4.SS2.p11.3.m3.1a"><msub id="S4.SS2.p11.3.m3.1.1" xref="S4.SS2.p11.3.m3.1.1.cmml"><mi id="S4.SS2.p11.3.m3.1.1.2" xref="S4.SS2.p11.3.m3.1.1.2.cmml">I</mi><mi id="S4.SS2.p11.3.m3.1.1.3" xref="S4.SS2.p11.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.3.m3.1b"><apply id="S4.SS2.p11.3.m3.1.1.cmml" xref="S4.SS2.p11.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.3.m3.1.1.1.cmml" xref="S4.SS2.p11.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p11.3.m3.1.1.2.cmml" xref="S4.SS2.p11.3.m3.1.1.2">𝐼</ci><ci id="S4.SS2.p11.3.m3.1.1.3.cmml" xref="S4.SS2.p11.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.3.m3.1c">I_{i}</annotation></semantics></math>. We also have the corresponding
position encodings of each region in the images <math id="S4.SS2.p11.4.m4.4" class="ltx_Math" alttext="P=[P_{1};P_{2};\dots;P_{n}]\in\mathbb{R}^{pn\times 4}" display="inline"><semantics id="S4.SS2.p11.4.m4.4a"><mrow id="S4.SS2.p11.4.m4.4.4" xref="S4.SS2.p11.4.m4.4.4.cmml"><mi id="S4.SS2.p11.4.m4.4.4.5" xref="S4.SS2.p11.4.m4.4.4.5.cmml">P</mi><mo id="S4.SS2.p11.4.m4.4.4.6" xref="S4.SS2.p11.4.m4.4.4.6.cmml">=</mo><mrow id="S4.SS2.p11.4.m4.4.4.3.3" xref="S4.SS2.p11.4.m4.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p11.4.m4.4.4.3.3.4" xref="S4.SS2.p11.4.m4.4.4.3.4.cmml">[</mo><msub id="S4.SS2.p11.4.m4.2.2.1.1.1" xref="S4.SS2.p11.4.m4.2.2.1.1.1.cmml"><mi id="S4.SS2.p11.4.m4.2.2.1.1.1.2" xref="S4.SS2.p11.4.m4.2.2.1.1.1.2.cmml">P</mi><mn id="S4.SS2.p11.4.m4.2.2.1.1.1.3" xref="S4.SS2.p11.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p11.4.m4.4.4.3.3.5" xref="S4.SS2.p11.4.m4.4.4.3.4.cmml">;</mo><msub id="S4.SS2.p11.4.m4.3.3.2.2.2" xref="S4.SS2.p11.4.m4.3.3.2.2.2.cmml"><mi id="S4.SS2.p11.4.m4.3.3.2.2.2.2" xref="S4.SS2.p11.4.m4.3.3.2.2.2.2.cmml">P</mi><mn id="S4.SS2.p11.4.m4.3.3.2.2.2.3" xref="S4.SS2.p11.4.m4.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p11.4.m4.4.4.3.3.6" xref="S4.SS2.p11.4.m4.4.4.3.4.cmml">;</mo><mi mathvariant="normal" id="S4.SS2.p11.4.m4.1.1" xref="S4.SS2.p11.4.m4.1.1.cmml">…</mi><mo id="S4.SS2.p11.4.m4.4.4.3.3.7" xref="S4.SS2.p11.4.m4.4.4.3.4.cmml">;</mo><msub id="S4.SS2.p11.4.m4.4.4.3.3.3" xref="S4.SS2.p11.4.m4.4.4.3.3.3.cmml"><mi id="S4.SS2.p11.4.m4.4.4.3.3.3.2" xref="S4.SS2.p11.4.m4.4.4.3.3.3.2.cmml">P</mi><mi id="S4.SS2.p11.4.m4.4.4.3.3.3.3" xref="S4.SS2.p11.4.m4.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.SS2.p11.4.m4.4.4.3.3.8" xref="S4.SS2.p11.4.m4.4.4.3.4.cmml">]</mo></mrow><mo id="S4.SS2.p11.4.m4.4.4.7" xref="S4.SS2.p11.4.m4.4.4.7.cmml">∈</mo><msup id="S4.SS2.p11.4.m4.4.4.8" xref="S4.SS2.p11.4.m4.4.4.8.cmml"><mi id="S4.SS2.p11.4.m4.4.4.8.2" xref="S4.SS2.p11.4.m4.4.4.8.2.cmml">ℝ</mi><mrow id="S4.SS2.p11.4.m4.4.4.8.3" xref="S4.SS2.p11.4.m4.4.4.8.3.cmml"><mrow id="S4.SS2.p11.4.m4.4.4.8.3.2" xref="S4.SS2.p11.4.m4.4.4.8.3.2.cmml"><mi id="S4.SS2.p11.4.m4.4.4.8.3.2.2" xref="S4.SS2.p11.4.m4.4.4.8.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p11.4.m4.4.4.8.3.2.1" xref="S4.SS2.p11.4.m4.4.4.8.3.2.1.cmml">​</mo><mi id="S4.SS2.p11.4.m4.4.4.8.3.2.3" xref="S4.SS2.p11.4.m4.4.4.8.3.2.3.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p11.4.m4.4.4.8.3.1" xref="S4.SS2.p11.4.m4.4.4.8.3.1.cmml">×</mo><mn id="S4.SS2.p11.4.m4.4.4.8.3.3" xref="S4.SS2.p11.4.m4.4.4.8.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.4.m4.4b"><apply id="S4.SS2.p11.4.m4.4.4.cmml" xref="S4.SS2.p11.4.m4.4.4"><and id="S4.SS2.p11.4.m4.4.4a.cmml" xref="S4.SS2.p11.4.m4.4.4"></and><apply id="S4.SS2.p11.4.m4.4.4b.cmml" xref="S4.SS2.p11.4.m4.4.4"><eq id="S4.SS2.p11.4.m4.4.4.6.cmml" xref="S4.SS2.p11.4.m4.4.4.6"></eq><ci id="S4.SS2.p11.4.m4.4.4.5.cmml" xref="S4.SS2.p11.4.m4.4.4.5">𝑃</ci><list id="S4.SS2.p11.4.m4.4.4.3.4.cmml" xref="S4.SS2.p11.4.m4.4.4.3.3"><apply id="S4.SS2.p11.4.m4.2.2.1.1.1.cmml" xref="S4.SS2.p11.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.4.m4.2.2.1.1.1.1.cmml" xref="S4.SS2.p11.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p11.4.m4.2.2.1.1.1.2.cmml" xref="S4.SS2.p11.4.m4.2.2.1.1.1.2">𝑃</ci><cn type="integer" id="S4.SS2.p11.4.m4.2.2.1.1.1.3.cmml" xref="S4.SS2.p11.4.m4.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p11.4.m4.3.3.2.2.2.cmml" xref="S4.SS2.p11.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p11.4.m4.3.3.2.2.2.1.cmml" xref="S4.SS2.p11.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p11.4.m4.3.3.2.2.2.2.cmml" xref="S4.SS2.p11.4.m4.3.3.2.2.2.2">𝑃</ci><cn type="integer" id="S4.SS2.p11.4.m4.3.3.2.2.2.3.cmml" xref="S4.SS2.p11.4.m4.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p11.4.m4.1.1.cmml" xref="S4.SS2.p11.4.m4.1.1">…</ci><apply id="S4.SS2.p11.4.m4.4.4.3.3.3.cmml" xref="S4.SS2.p11.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p11.4.m4.4.4.3.3.3.1.cmml" xref="S4.SS2.p11.4.m4.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p11.4.m4.4.4.3.3.3.2.cmml" xref="S4.SS2.p11.4.m4.4.4.3.3.3.2">𝑃</ci><ci id="S4.SS2.p11.4.m4.4.4.3.3.3.3.cmml" xref="S4.SS2.p11.4.m4.4.4.3.3.3.3">𝑛</ci></apply></list></apply><apply id="S4.SS2.p11.4.m4.4.4c.cmml" xref="S4.SS2.p11.4.m4.4.4"><in id="S4.SS2.p11.4.m4.4.4.7.cmml" xref="S4.SS2.p11.4.m4.4.4.7"></in><share href="#S4.SS2.p11.4.m4.4.4.3.cmml" id="S4.SS2.p11.4.m4.4.4d.cmml" xref="S4.SS2.p11.4.m4.4.4"></share><apply id="S4.SS2.p11.4.m4.4.4.8.cmml" xref="S4.SS2.p11.4.m4.4.4.8"><csymbol cd="ambiguous" id="S4.SS2.p11.4.m4.4.4.8.1.cmml" xref="S4.SS2.p11.4.m4.4.4.8">superscript</csymbol><ci id="S4.SS2.p11.4.m4.4.4.8.2.cmml" xref="S4.SS2.p11.4.m4.4.4.8.2">ℝ</ci><apply id="S4.SS2.p11.4.m4.4.4.8.3.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3"><times id="S4.SS2.p11.4.m4.4.4.8.3.1.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3.1"></times><apply id="S4.SS2.p11.4.m4.4.4.8.3.2.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3.2"><times id="S4.SS2.p11.4.m4.4.4.8.3.2.1.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3.2.1"></times><ci id="S4.SS2.p11.4.m4.4.4.8.3.2.2.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3.2.2">𝑝</ci><ci id="S4.SS2.p11.4.m4.4.4.8.3.2.3.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3.2.3">𝑛</ci></apply><cn type="integer" id="S4.SS2.p11.4.m4.4.4.8.3.3.cmml" xref="S4.SS2.p11.4.m4.4.4.8.3.3">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.4.m4.4c">P=[P_{1};P_{2};\dots;P_{n}]\in\mathbb{R}^{pn\times 4}</annotation></semantics></math>, where <math id="S4.SS2.p11.5.m5.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S4.SS2.p11.5.m5.1a"><msub id="S4.SS2.p11.5.m5.1.1" xref="S4.SS2.p11.5.m5.1.1.cmml"><mi id="S4.SS2.p11.5.m5.1.1.2" xref="S4.SS2.p11.5.m5.1.1.2.cmml">P</mi><mi id="S4.SS2.p11.5.m5.1.1.3" xref="S4.SS2.p11.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.5.m5.1b"><apply id="S4.SS2.p11.5.m5.1.1.cmml" xref="S4.SS2.p11.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.5.m5.1.1.1.cmml" xref="S4.SS2.p11.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p11.5.m5.1.1.2.cmml" xref="S4.SS2.p11.5.m5.1.1.2">𝑃</ci><ci id="S4.SS2.p11.5.m5.1.1.3.cmml" xref="S4.SS2.p11.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.5.m5.1c">P_{i}</annotation></semantics></math> contains the bounding box co-ordinates of the regions in <math id="S4.SS2.p11.6.m6.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S4.SS2.p11.6.m6.1a"><msub id="S4.SS2.p11.6.m6.1.1" xref="S4.SS2.p11.6.m6.1.1.cmml"><mi id="S4.SS2.p11.6.m6.1.1.2" xref="S4.SS2.p11.6.m6.1.1.2.cmml">I</mi><mi id="S4.SS2.p11.6.m6.1.1.3" xref="S4.SS2.p11.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.6.m6.1b"><apply id="S4.SS2.p11.6.m6.1.1.cmml" xref="S4.SS2.p11.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.6.m6.1.1.1.cmml" xref="S4.SS2.p11.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p11.6.m6.1.1.2.cmml" xref="S4.SS2.p11.6.m6.1.1.2">𝐼</ci><ci id="S4.SS2.p11.6.m6.1.1.3.cmml" xref="S4.SS2.p11.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.6.m6.1c">I_{i}</annotation></semantics></math>. We
combine the region features and position encodings to obtain position-aware embeddings, <math id="S4.SS2.p11.7.m7.1" class="ltx_Math" alttext="S\in\mathbb{R}^{pn\times d^{\prime}}" display="inline"><semantics id="S4.SS2.p11.7.m7.1a"><mrow id="S4.SS2.p11.7.m7.1.1" xref="S4.SS2.p11.7.m7.1.1.cmml"><mi id="S4.SS2.p11.7.m7.1.1.2" xref="S4.SS2.p11.7.m7.1.1.2.cmml">S</mi><mo id="S4.SS2.p11.7.m7.1.1.1" xref="S4.SS2.p11.7.m7.1.1.1.cmml">∈</mo><msup id="S4.SS2.p11.7.m7.1.1.3" xref="S4.SS2.p11.7.m7.1.1.3.cmml"><mi id="S4.SS2.p11.7.m7.1.1.3.2" xref="S4.SS2.p11.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p11.7.m7.1.1.3.3" xref="S4.SS2.p11.7.m7.1.1.3.3.cmml"><mrow id="S4.SS2.p11.7.m7.1.1.3.3.2" xref="S4.SS2.p11.7.m7.1.1.3.3.2.cmml"><mi id="S4.SS2.p11.7.m7.1.1.3.3.2.2" xref="S4.SS2.p11.7.m7.1.1.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p11.7.m7.1.1.3.3.2.1" xref="S4.SS2.p11.7.m7.1.1.3.3.2.1.cmml">​</mo><mi id="S4.SS2.p11.7.m7.1.1.3.3.2.3" xref="S4.SS2.p11.7.m7.1.1.3.3.2.3.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p11.7.m7.1.1.3.3.1" xref="S4.SS2.p11.7.m7.1.1.3.3.1.cmml">×</mo><msup id="S4.SS2.p11.7.m7.1.1.3.3.3" xref="S4.SS2.p11.7.m7.1.1.3.3.3.cmml"><mi id="S4.SS2.p11.7.m7.1.1.3.3.3.2" xref="S4.SS2.p11.7.m7.1.1.3.3.3.2.cmml">d</mi><mo id="S4.SS2.p11.7.m7.1.1.3.3.3.3" xref="S4.SS2.p11.7.m7.1.1.3.3.3.3.cmml">′</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.7.m7.1b"><apply id="S4.SS2.p11.7.m7.1.1.cmml" xref="S4.SS2.p11.7.m7.1.1"><in id="S4.SS2.p11.7.m7.1.1.1.cmml" xref="S4.SS2.p11.7.m7.1.1.1"></in><ci id="S4.SS2.p11.7.m7.1.1.2.cmml" xref="S4.SS2.p11.7.m7.1.1.2">𝑆</ci><apply id="S4.SS2.p11.7.m7.1.1.3.cmml" xref="S4.SS2.p11.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p11.7.m7.1.1.3.1.cmml" xref="S4.SS2.p11.7.m7.1.1.3">superscript</csymbol><ci id="S4.SS2.p11.7.m7.1.1.3.2.cmml" xref="S4.SS2.p11.7.m7.1.1.3.2">ℝ</ci><apply id="S4.SS2.p11.7.m7.1.1.3.3.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3"><times id="S4.SS2.p11.7.m7.1.1.3.3.1.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.1"></times><apply id="S4.SS2.p11.7.m7.1.1.3.3.2.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.2"><times id="S4.SS2.p11.7.m7.1.1.3.3.2.1.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.2.1"></times><ci id="S4.SS2.p11.7.m7.1.1.3.3.2.2.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.2.2">𝑝</ci><ci id="S4.SS2.p11.7.m7.1.1.3.3.2.3.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.2.3">𝑛</ci></apply><apply id="S4.SS2.p11.7.m7.1.1.3.3.3.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p11.7.m7.1.1.3.3.3.1.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.3">superscript</csymbol><ci id="S4.SS2.p11.7.m7.1.1.3.3.3.2.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.3.2">𝑑</ci><ci id="S4.SS2.p11.7.m7.1.1.3.3.3.3.cmml" xref="S4.SS2.p11.7.m7.1.1.3.3.3.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.7.m7.1c">S\in\mathbb{R}^{pn\times d^{\prime}}</annotation></semantics></math>, where <math id="S4.SS2.p11.8.m8.4" class="ltx_Math" alttext="S=\text{LayerNorm}(\text{FC}(R))+\text{LayerNorm}(\text{FC}(P))" display="inline"><semantics id="S4.SS2.p11.8.m8.4a"><mrow id="S4.SS2.p11.8.m8.4.4" xref="S4.SS2.p11.8.m8.4.4.cmml"><mi id="S4.SS2.p11.8.m8.4.4.4" xref="S4.SS2.p11.8.m8.4.4.4.cmml">S</mi><mo id="S4.SS2.p11.8.m8.4.4.3" xref="S4.SS2.p11.8.m8.4.4.3.cmml">=</mo><mrow id="S4.SS2.p11.8.m8.4.4.2" xref="S4.SS2.p11.8.m8.4.4.2.cmml"><mrow id="S4.SS2.p11.8.m8.3.3.1.1" xref="S4.SS2.p11.8.m8.3.3.1.1.cmml"><mtext id="S4.SS2.p11.8.m8.3.3.1.1.3" xref="S4.SS2.p11.8.m8.3.3.1.1.3a.cmml">LayerNorm</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p11.8.m8.3.3.1.1.2" xref="S4.SS2.p11.8.m8.3.3.1.1.2.cmml">​</mo><mrow id="S4.SS2.p11.8.m8.3.3.1.1.1.1" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p11.8.m8.3.3.1.1.1.1.2" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml"><mtext id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.2" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.1" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.1.cmml">​</mo><mrow id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.3.2" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.3.2.1" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml">(</mo><mi id="S4.SS2.p11.8.m8.1.1" xref="S4.SS2.p11.8.m8.1.1.cmml">R</mi><mo stretchy="false" id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.3.2.2" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p11.8.m8.3.3.1.1.1.1.3" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p11.8.m8.4.4.2.3" xref="S4.SS2.p11.8.m8.4.4.2.3.cmml">+</mo><mrow id="S4.SS2.p11.8.m8.4.4.2.2" xref="S4.SS2.p11.8.m8.4.4.2.2.cmml"><mtext id="S4.SS2.p11.8.m8.4.4.2.2.3" xref="S4.SS2.p11.8.m8.4.4.2.2.3a.cmml">LayerNorm</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p11.8.m8.4.4.2.2.2" xref="S4.SS2.p11.8.m8.4.4.2.2.2.cmml">​</mo><mrow id="S4.SS2.p11.8.m8.4.4.2.2.1.1" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p11.8.m8.4.4.2.2.1.1.2" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml">(</mo><mrow id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml"><mtext id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.2" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.1" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.1.cmml">​</mo><mrow id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.3.2" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.3.2.1" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml">(</mo><mi id="S4.SS2.p11.8.m8.2.2" xref="S4.SS2.p11.8.m8.2.2.cmml">P</mi><mo stretchy="false" id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.3.2.2" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p11.8.m8.4.4.2.2.1.1.3" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.8.m8.4b"><apply id="S4.SS2.p11.8.m8.4.4.cmml" xref="S4.SS2.p11.8.m8.4.4"><eq id="S4.SS2.p11.8.m8.4.4.3.cmml" xref="S4.SS2.p11.8.m8.4.4.3"></eq><ci id="S4.SS2.p11.8.m8.4.4.4.cmml" xref="S4.SS2.p11.8.m8.4.4.4">𝑆</ci><apply id="S4.SS2.p11.8.m8.4.4.2.cmml" xref="S4.SS2.p11.8.m8.4.4.2"><plus id="S4.SS2.p11.8.m8.4.4.2.3.cmml" xref="S4.SS2.p11.8.m8.4.4.2.3"></plus><apply id="S4.SS2.p11.8.m8.3.3.1.1.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1"><times id="S4.SS2.p11.8.m8.3.3.1.1.2.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.2"></times><ci id="S4.SS2.p11.8.m8.3.3.1.1.3a.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.3"><mtext id="S4.SS2.p11.8.m8.3.3.1.1.3.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.3">LayerNorm</mtext></ci><apply id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1"><times id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.1.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.1"></times><ci id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.2a.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.2"><mtext id="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.2.cmml" xref="S4.SS2.p11.8.m8.3.3.1.1.1.1.1.2">FC</mtext></ci><ci id="S4.SS2.p11.8.m8.1.1.cmml" xref="S4.SS2.p11.8.m8.1.1">𝑅</ci></apply></apply><apply id="S4.SS2.p11.8.m8.4.4.2.2.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2"><times id="S4.SS2.p11.8.m8.4.4.2.2.2.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.2"></times><ci id="S4.SS2.p11.8.m8.4.4.2.2.3a.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.3"><mtext id="S4.SS2.p11.8.m8.4.4.2.2.3.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.3">LayerNorm</mtext></ci><apply id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1"><times id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.1.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.1"></times><ci id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.2a.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.2"><mtext id="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.2.cmml" xref="S4.SS2.p11.8.m8.4.4.2.2.1.1.1.2">FC</mtext></ci><ci id="S4.SS2.p11.8.m8.2.2.cmml" xref="S4.SS2.p11.8.m8.2.2">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.8.m8.4c">S=\text{LayerNorm}(\text{FC}(R))+\text{LayerNorm}(\text{FC}(P))</annotation></semantics></math>. Within- and across-image object relationships are encoded by applying <math id="S4.SS2.p11.9.m9.1" class="ltx_Math" alttext="N_{R}" display="inline"><semantics id="S4.SS2.p11.9.m9.1a"><msub id="S4.SS2.p11.9.m9.1.1" xref="S4.SS2.p11.9.m9.1.1.cmml"><mi id="S4.SS2.p11.9.m9.1.1.2" xref="S4.SS2.p11.9.m9.1.1.2.cmml">N</mi><mi id="S4.SS2.p11.9.m9.1.1.3" xref="S4.SS2.p11.9.m9.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.9.m9.1b"><apply id="S4.SS2.p11.9.m9.1.1.cmml" xref="S4.SS2.p11.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.9.m9.1.1.1.cmml" xref="S4.SS2.p11.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p11.9.m9.1.1.2.cmml" xref="S4.SS2.p11.9.m9.1.1.2">𝑁</ci><ci id="S4.SS2.p11.9.m9.1.1.3.cmml" xref="S4.SS2.p11.9.m9.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.9.m9.1c">N_{R}</annotation></semantics></math> layers of the object relationship encoder. The <math id="S4.SS2.p11.10.m10.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.SS2.p11.10.m10.1a"><mi id="S4.SS2.p11.10.m10.1.1" xref="S4.SS2.p11.10.m10.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.10.m10.1b"><ci id="S4.SS2.p11.10.m10.1.1.cmml" xref="S4.SS2.p11.10.m10.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.10.m10.1c">l</annotation></semantics></math>-th layer can be represented as</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="x_{l}=\text{FC}(\text{FC}(\text{SelfAttention}(x_{l-1})))" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><msub id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2.cmml">x</mi><mi id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml">l</mi></msub><mo id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml"><mtext id="S4.E3.m1.1.1.1.3" xref="S4.E3.m1.1.1.1.3a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.2" xref="S4.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><mtext id="S4.E3.m1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.3a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.3a.cmml">SelfAttention</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"></eq><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2">𝑥</ci><ci id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3">𝑙</ci></apply><apply id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><times id="S4.E3.m1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.2"></times><ci id="S4.E3.m1.1.1.1.3a.cmml" xref="S4.E3.m1.1.1.1.3"><mtext id="S4.E3.m1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.3">FC</mtext></ci><apply id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"><times id="S4.E3.m1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2"></times><ci id="S4.E3.m1.1.1.1.1.1.1.3a.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3"><mtext id="S4.E3.m1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3">FC</mtext></ci><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1"><times id="S4.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.3"><mtext id="S4.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.3">SelfAttention</mtext></ci><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">x_{l}=\text{FC}(\text{FC}(\text{SelfAttention}(x_{l-1})))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p11.12" class="ltx_p">where, <math id="S4.SS2.p11.11.m1.1" class="ltx_Math" alttext="x_{0}=S" display="inline"><semantics id="S4.SS2.p11.11.m1.1a"><mrow id="S4.SS2.p11.11.m1.1.1" xref="S4.SS2.p11.11.m1.1.1.cmml"><msub id="S4.SS2.p11.11.m1.1.1.2" xref="S4.SS2.p11.11.m1.1.1.2.cmml"><mi id="S4.SS2.p11.11.m1.1.1.2.2" xref="S4.SS2.p11.11.m1.1.1.2.2.cmml">x</mi><mn id="S4.SS2.p11.11.m1.1.1.2.3" xref="S4.SS2.p11.11.m1.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS2.p11.11.m1.1.1.1" xref="S4.SS2.p11.11.m1.1.1.1.cmml">=</mo><mi id="S4.SS2.p11.11.m1.1.1.3" xref="S4.SS2.p11.11.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.11.m1.1b"><apply id="S4.SS2.p11.11.m1.1.1.cmml" xref="S4.SS2.p11.11.m1.1.1"><eq id="S4.SS2.p11.11.m1.1.1.1.cmml" xref="S4.SS2.p11.11.m1.1.1.1"></eq><apply id="S4.SS2.p11.11.m1.1.1.2.cmml" xref="S4.SS2.p11.11.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p11.11.m1.1.1.2.1.cmml" xref="S4.SS2.p11.11.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p11.11.m1.1.1.2.2.cmml" xref="S4.SS2.p11.11.m1.1.1.2.2">𝑥</ci><cn type="integer" id="S4.SS2.p11.11.m1.1.1.2.3.cmml" xref="S4.SS2.p11.11.m1.1.1.2.3">0</cn></apply><ci id="S4.SS2.p11.11.m1.1.1.3.cmml" xref="S4.SS2.p11.11.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.11.m1.1c">x_{0}=S</annotation></semantics></math>, and <math id="S4.SS2.p11.12.m2.1" class="ltx_Math" alttext="X(=x_{N_{R}})" display="inline"><semantics id="S4.SS2.p11.12.m2.1a"><mrow id="S4.SS2.p11.12.m2.1.1" xref="S4.SS2.p11.12.m2.1.1.cmml"><mi id="S4.SS2.p11.12.m2.1.1.3" xref="S4.SS2.p11.12.m2.1.1.3.cmml">X</mi><mspace width="0.3888888888888889em" id="S4.SS2.p11.12.m2.1.1a" xref="S4.SS2.p11.12.m2.1.1.cmml"></mspace><mrow id="S4.SS2.p11.12.m2.1.1.1.1" xref="S4.SS2.p11.12.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p11.12.m2.1.1.1.1.2" xref="S4.SS2.p11.12.m2.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p11.12.m2.1.1.1.1.1" xref="S4.SS2.p11.12.m2.1.1.1.1.1.cmml"><mi id="S4.SS2.p11.12.m2.1.1.1.1.1.2" xref="S4.SS2.p11.12.m2.1.1.1.1.1.2.cmml"></mi><mo id="S4.SS2.p11.12.m2.1.1.1.1.1.1" xref="S4.SS2.p11.12.m2.1.1.1.1.1.1.cmml">=</mo><msub id="S4.SS2.p11.12.m2.1.1.1.1.1.3" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p11.12.m2.1.1.1.1.1.3.2" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.2.cmml">x</mi><msub id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.cmml"><mi id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.2" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.2.cmml">N</mi><mi id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.3" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.3.cmml">R</mi></msub></msub></mrow><mo stretchy="false" id="S4.SS2.p11.12.m2.1.1.1.1.3" xref="S4.SS2.p11.12.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.12.m2.1b"><apply id="S4.SS2.p11.12.m2.1.1.cmml" xref="S4.SS2.p11.12.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p11.12.m2.1.1.2.cmml" xref="S4.SS2.p11.12.m2.1.1">annotated</csymbol><ci id="S4.SS2.p11.12.m2.1.1.3.cmml" xref="S4.SS2.p11.12.m2.1.1.3">𝑋</ci><apply id="S4.SS2.p11.12.m2.1.1.1.1.1.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1"><eq id="S4.SS2.p11.12.m2.1.1.1.1.1.1.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.1"></eq><csymbol cd="latexml" id="S4.SS2.p11.12.m2.1.1.1.1.1.2.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.2">absent</csymbol><apply id="S4.SS2.p11.12.m2.1.1.1.1.1.3.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p11.12.m2.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS2.p11.12.m2.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.2">𝑥</ci><apply id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.1.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.2.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.2">𝑁</ci><ci id="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.3.cmml" xref="S4.SS2.p11.12.m2.1.1.1.1.1.3.3.3">𝑅</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.12.m2.1c">X(=x_{N_{R}})</annotation></semantics></math> is the final visual embedding of the object-relationship
encoder.</p>
</div>
<div id="S4.SS2.p12" class="ltx_para">
<p id="S4.SS2.p12.8" class="ltx_p">Similarly, given the word embeddings of the question, <math id="S4.SS2.p12.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS2.p12.1.m1.1a"><mi id="S4.SS2.p12.1.m1.1.1" xref="S4.SS2.p12.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.1.m1.1b"><ci id="S4.SS2.p12.1.m1.1.1.cmml" xref="S4.SS2.p12.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.1.m1.1c">E</annotation></semantics></math>, and the index embeddings of each word in
the question, <math id="S4.SS2.p12.2.m2.5" class="ltx_Math" alttext="E^{\prime}=\{\text{IdxEmbed}(1),\dots,\text{IdxEmbed}(T)\}" display="inline"><semantics id="S4.SS2.p12.2.m2.5a"><mrow id="S4.SS2.p12.2.m2.5.5" xref="S4.SS2.p12.2.m2.5.5.cmml"><msup id="S4.SS2.p12.2.m2.5.5.4" xref="S4.SS2.p12.2.m2.5.5.4.cmml"><mi id="S4.SS2.p12.2.m2.5.5.4.2" xref="S4.SS2.p12.2.m2.5.5.4.2.cmml">E</mi><mo id="S4.SS2.p12.2.m2.5.5.4.3" xref="S4.SS2.p12.2.m2.5.5.4.3.cmml">′</mo></msup><mo id="S4.SS2.p12.2.m2.5.5.3" xref="S4.SS2.p12.2.m2.5.5.3.cmml">=</mo><mrow id="S4.SS2.p12.2.m2.5.5.2.2" xref="S4.SS2.p12.2.m2.5.5.2.3.cmml"><mo stretchy="false" id="S4.SS2.p12.2.m2.5.5.2.2.3" xref="S4.SS2.p12.2.m2.5.5.2.3.cmml">{</mo><mrow id="S4.SS2.p12.2.m2.4.4.1.1.1" xref="S4.SS2.p12.2.m2.4.4.1.1.1.cmml"><mtext id="S4.SS2.p12.2.m2.4.4.1.1.1.2" xref="S4.SS2.p12.2.m2.4.4.1.1.1.2a.cmml">IdxEmbed</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p12.2.m2.4.4.1.1.1.1" xref="S4.SS2.p12.2.m2.4.4.1.1.1.1.cmml">​</mo><mrow id="S4.SS2.p12.2.m2.4.4.1.1.1.3.2" xref="S4.SS2.p12.2.m2.4.4.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p12.2.m2.4.4.1.1.1.3.2.1" xref="S4.SS2.p12.2.m2.4.4.1.1.1.cmml">(</mo><mn id="S4.SS2.p12.2.m2.1.1" xref="S4.SS2.p12.2.m2.1.1.cmml">1</mn><mo stretchy="false" id="S4.SS2.p12.2.m2.4.4.1.1.1.3.2.2" xref="S4.SS2.p12.2.m2.4.4.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p12.2.m2.5.5.2.2.4" xref="S4.SS2.p12.2.m2.5.5.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p12.2.m2.3.3" xref="S4.SS2.p12.2.m2.3.3.cmml">…</mi><mo id="S4.SS2.p12.2.m2.5.5.2.2.5" xref="S4.SS2.p12.2.m2.5.5.2.3.cmml">,</mo><mrow id="S4.SS2.p12.2.m2.5.5.2.2.2" xref="S4.SS2.p12.2.m2.5.5.2.2.2.cmml"><mtext id="S4.SS2.p12.2.m2.5.5.2.2.2.2" xref="S4.SS2.p12.2.m2.5.5.2.2.2.2a.cmml">IdxEmbed</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p12.2.m2.5.5.2.2.2.1" xref="S4.SS2.p12.2.m2.5.5.2.2.2.1.cmml">​</mo><mrow id="S4.SS2.p12.2.m2.5.5.2.2.2.3.2" xref="S4.SS2.p12.2.m2.5.5.2.2.2.cmml"><mo stretchy="false" id="S4.SS2.p12.2.m2.5.5.2.2.2.3.2.1" xref="S4.SS2.p12.2.m2.5.5.2.2.2.cmml">(</mo><mi id="S4.SS2.p12.2.m2.2.2" xref="S4.SS2.p12.2.m2.2.2.cmml">T</mi><mo stretchy="false" id="S4.SS2.p12.2.m2.5.5.2.2.2.3.2.2" xref="S4.SS2.p12.2.m2.5.5.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p12.2.m2.5.5.2.2.6" xref="S4.SS2.p12.2.m2.5.5.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.2.m2.5b"><apply id="S4.SS2.p12.2.m2.5.5.cmml" xref="S4.SS2.p12.2.m2.5.5"><eq id="S4.SS2.p12.2.m2.5.5.3.cmml" xref="S4.SS2.p12.2.m2.5.5.3"></eq><apply id="S4.SS2.p12.2.m2.5.5.4.cmml" xref="S4.SS2.p12.2.m2.5.5.4"><csymbol cd="ambiguous" id="S4.SS2.p12.2.m2.5.5.4.1.cmml" xref="S4.SS2.p12.2.m2.5.5.4">superscript</csymbol><ci id="S4.SS2.p12.2.m2.5.5.4.2.cmml" xref="S4.SS2.p12.2.m2.5.5.4.2">𝐸</ci><ci id="S4.SS2.p12.2.m2.5.5.4.3.cmml" xref="S4.SS2.p12.2.m2.5.5.4.3">′</ci></apply><set id="S4.SS2.p12.2.m2.5.5.2.3.cmml" xref="S4.SS2.p12.2.m2.5.5.2.2"><apply id="S4.SS2.p12.2.m2.4.4.1.1.1.cmml" xref="S4.SS2.p12.2.m2.4.4.1.1.1"><times id="S4.SS2.p12.2.m2.4.4.1.1.1.1.cmml" xref="S4.SS2.p12.2.m2.4.4.1.1.1.1"></times><ci id="S4.SS2.p12.2.m2.4.4.1.1.1.2a.cmml" xref="S4.SS2.p12.2.m2.4.4.1.1.1.2"><mtext id="S4.SS2.p12.2.m2.4.4.1.1.1.2.cmml" xref="S4.SS2.p12.2.m2.4.4.1.1.1.2">IdxEmbed</mtext></ci><cn type="integer" id="S4.SS2.p12.2.m2.1.1.cmml" xref="S4.SS2.p12.2.m2.1.1">1</cn></apply><ci id="S4.SS2.p12.2.m2.3.3.cmml" xref="S4.SS2.p12.2.m2.3.3">…</ci><apply id="S4.SS2.p12.2.m2.5.5.2.2.2.cmml" xref="S4.SS2.p12.2.m2.5.5.2.2.2"><times id="S4.SS2.p12.2.m2.5.5.2.2.2.1.cmml" xref="S4.SS2.p12.2.m2.5.5.2.2.2.1"></times><ci id="S4.SS2.p12.2.m2.5.5.2.2.2.2a.cmml" xref="S4.SS2.p12.2.m2.5.5.2.2.2.2"><mtext id="S4.SS2.p12.2.m2.5.5.2.2.2.2.cmml" xref="S4.SS2.p12.2.m2.5.5.2.2.2.2">IdxEmbed</mtext></ci><ci id="S4.SS2.p12.2.m2.2.2.cmml" xref="S4.SS2.p12.2.m2.2.2">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.2.m2.5c">E^{\prime}=\{\text{IdxEmbed}(1),\dots,\text{IdxEmbed}(T)\}</annotation></semantics></math>, the
index-aware word embedding of the <math id="S4.SS2.p12.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p12.3.m3.1a"><mi id="S4.SS2.p12.3.m3.1.1" xref="S4.SS2.p12.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.3.m3.1b"><ci id="S4.SS2.p12.3.m3.1.1.cmml" xref="S4.SS2.p12.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.3.m3.1c">i</annotation></semantics></math>-th word is obtained as <math id="S4.SS2.p12.4.m4.1" class="ltx_Math" alttext="H_{i}=\\
\text{LayerNorm}(E_{i}+E_{i}^{{}^{\prime}})" display="inline"><semantics id="S4.SS2.p12.4.m4.1a"><mrow id="S4.SS2.p12.4.m4.1.1" xref="S4.SS2.p12.4.m4.1.1.cmml"><msub id="S4.SS2.p12.4.m4.1.1.3" xref="S4.SS2.p12.4.m4.1.1.3.cmml"><mi id="S4.SS2.p12.4.m4.1.1.3.2" xref="S4.SS2.p12.4.m4.1.1.3.2.cmml">H</mi><mi id="S4.SS2.p12.4.m4.1.1.3.3" xref="S4.SS2.p12.4.m4.1.1.3.3.cmml">i</mi></msub><mo id="S4.SS2.p12.4.m4.1.1.2" xref="S4.SS2.p12.4.m4.1.1.2.cmml">=</mo><mrow id="S4.SS2.p12.4.m4.1.1.1" xref="S4.SS2.p12.4.m4.1.1.1.cmml"><mtext id="S4.SS2.p12.4.m4.1.1.1.3" xref="S4.SS2.p12.4.m4.1.1.1.3a.cmml">LayerNorm</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p12.4.m4.1.1.1.2" xref="S4.SS2.p12.4.m4.1.1.1.2.cmml">​</mo><mrow id="S4.SS2.p12.4.m4.1.1.1.1.1" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p12.4.m4.1.1.1.1.1.2" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p12.4.m4.1.1.1.1.1.1" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.cmml"><msub id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.2" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.2.cmml">E</mi><mi id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.3" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.p12.4.m4.1.1.1.1.1.1.1" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.1.cmml">+</mo><msubsup id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.2" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.2.cmml">E</mi><mi id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.3" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.3.cmml">i</mi><msup id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.cmml"><mi id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3a" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.cmml"></mi><mo id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.1" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.1.cmml">′</mo></msup></msubsup></mrow><mo stretchy="false" id="S4.SS2.p12.4.m4.1.1.1.1.1.3" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.4.m4.1b"><apply id="S4.SS2.p12.4.m4.1.1.cmml" xref="S4.SS2.p12.4.m4.1.1"><eq id="S4.SS2.p12.4.m4.1.1.2.cmml" xref="S4.SS2.p12.4.m4.1.1.2"></eq><apply id="S4.SS2.p12.4.m4.1.1.3.cmml" xref="S4.SS2.p12.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p12.4.m4.1.1.3.1.cmml" xref="S4.SS2.p12.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS2.p12.4.m4.1.1.3.2.cmml" xref="S4.SS2.p12.4.m4.1.1.3.2">𝐻</ci><ci id="S4.SS2.p12.4.m4.1.1.3.3.cmml" xref="S4.SS2.p12.4.m4.1.1.3.3">𝑖</ci></apply><apply id="S4.SS2.p12.4.m4.1.1.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1"><times id="S4.SS2.p12.4.m4.1.1.1.2.cmml" xref="S4.SS2.p12.4.m4.1.1.1.2"></times><ci id="S4.SS2.p12.4.m4.1.1.1.3a.cmml" xref="S4.SS2.p12.4.m4.1.1.1.3"><mtext id="S4.SS2.p12.4.m4.1.1.1.3.cmml" xref="S4.SS2.p12.4.m4.1.1.1.3">LayerNorm</mtext></ci><apply id="S4.SS2.p12.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1"><plus id="S4.SS2.p12.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.1"></plus><apply id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.2">𝐸</ci><ci id="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.2.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.2">𝐸</ci><ci id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.3.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3"><ci id="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.1.cmml" xref="S4.SS2.p12.4.m4.1.1.1.1.1.1.3.3.1">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.4.m4.1c">H_{i}=\\
\text{LayerNorm}(E_{i}+E_{i}^{{}^{\prime}})</annotation></semantics></math>. Note that the index embedding, <span id="S4.SS2.p12.8.1" class="ltx_text ltx_markedasmath">IdxEmbed</span>, is just a
projection of the position of the word to a vector using fully-connected layers. We apply a
similar operation as equation <a href="#S4.E3" title="In 4.2 Model Definitions ‣ 4 ISVQA Problem Formulation and Baselines ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> <math id="S4.SS2.p12.6.m6.1" class="ltx_Math" alttext="N_{L}" display="inline"><semantics id="S4.SS2.p12.6.m6.1a"><msub id="S4.SS2.p12.6.m6.1.1" xref="S4.SS2.p12.6.m6.1.1.cmml"><mi id="S4.SS2.p12.6.m6.1.1.2" xref="S4.SS2.p12.6.m6.1.1.2.cmml">N</mi><mi id="S4.SS2.p12.6.m6.1.1.3" xref="S4.SS2.p12.6.m6.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.6.m6.1b"><apply id="S4.SS2.p12.6.m6.1.1.cmml" xref="S4.SS2.p12.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p12.6.m6.1.1.1.cmml" xref="S4.SS2.p12.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p12.6.m6.1.1.2.cmml" xref="S4.SS2.p12.6.m6.1.1.2">𝑁</ci><ci id="S4.SS2.p12.6.m6.1.1.3.cmml" xref="S4.SS2.p12.6.m6.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.6.m6.1c">N_{L}</annotation></semantics></math> times to the word embeddings <math id="S4.SS2.p12.7.m7.4" class="ltx_Math" alttext="H=[H_{1};H_{2};\dots;H_{T}]\in\mathbb{R}^{T\times d_{q}}" display="inline"><semantics id="S4.SS2.p12.7.m7.4a"><mrow id="S4.SS2.p12.7.m7.4.4" xref="S4.SS2.p12.7.m7.4.4.cmml"><mi id="S4.SS2.p12.7.m7.4.4.5" xref="S4.SS2.p12.7.m7.4.4.5.cmml">H</mi><mo id="S4.SS2.p12.7.m7.4.4.6" xref="S4.SS2.p12.7.m7.4.4.6.cmml">=</mo><mrow id="S4.SS2.p12.7.m7.4.4.3.3" xref="S4.SS2.p12.7.m7.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p12.7.m7.4.4.3.3.4" xref="S4.SS2.p12.7.m7.4.4.3.4.cmml">[</mo><msub id="S4.SS2.p12.7.m7.2.2.1.1.1" xref="S4.SS2.p12.7.m7.2.2.1.1.1.cmml"><mi id="S4.SS2.p12.7.m7.2.2.1.1.1.2" xref="S4.SS2.p12.7.m7.2.2.1.1.1.2.cmml">H</mi><mn id="S4.SS2.p12.7.m7.2.2.1.1.1.3" xref="S4.SS2.p12.7.m7.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p12.7.m7.4.4.3.3.5" xref="S4.SS2.p12.7.m7.4.4.3.4.cmml">;</mo><msub id="S4.SS2.p12.7.m7.3.3.2.2.2" xref="S4.SS2.p12.7.m7.3.3.2.2.2.cmml"><mi id="S4.SS2.p12.7.m7.3.3.2.2.2.2" xref="S4.SS2.p12.7.m7.3.3.2.2.2.2.cmml">H</mi><mn id="S4.SS2.p12.7.m7.3.3.2.2.2.3" xref="S4.SS2.p12.7.m7.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p12.7.m7.4.4.3.3.6" xref="S4.SS2.p12.7.m7.4.4.3.4.cmml">;</mo><mi mathvariant="normal" id="S4.SS2.p12.7.m7.1.1" xref="S4.SS2.p12.7.m7.1.1.cmml">…</mi><mo id="S4.SS2.p12.7.m7.4.4.3.3.7" xref="S4.SS2.p12.7.m7.4.4.3.4.cmml">;</mo><msub id="S4.SS2.p12.7.m7.4.4.3.3.3" xref="S4.SS2.p12.7.m7.4.4.3.3.3.cmml"><mi id="S4.SS2.p12.7.m7.4.4.3.3.3.2" xref="S4.SS2.p12.7.m7.4.4.3.3.3.2.cmml">H</mi><mi id="S4.SS2.p12.7.m7.4.4.3.3.3.3" xref="S4.SS2.p12.7.m7.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S4.SS2.p12.7.m7.4.4.3.3.8" xref="S4.SS2.p12.7.m7.4.4.3.4.cmml">]</mo></mrow><mo id="S4.SS2.p12.7.m7.4.4.7" xref="S4.SS2.p12.7.m7.4.4.7.cmml">∈</mo><msup id="S4.SS2.p12.7.m7.4.4.8" xref="S4.SS2.p12.7.m7.4.4.8.cmml"><mi id="S4.SS2.p12.7.m7.4.4.8.2" xref="S4.SS2.p12.7.m7.4.4.8.2.cmml">ℝ</mi><mrow id="S4.SS2.p12.7.m7.4.4.8.3" xref="S4.SS2.p12.7.m7.4.4.8.3.cmml"><mi id="S4.SS2.p12.7.m7.4.4.8.3.2" xref="S4.SS2.p12.7.m7.4.4.8.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p12.7.m7.4.4.8.3.1" xref="S4.SS2.p12.7.m7.4.4.8.3.1.cmml">×</mo><msub id="S4.SS2.p12.7.m7.4.4.8.3.3" xref="S4.SS2.p12.7.m7.4.4.8.3.3.cmml"><mi id="S4.SS2.p12.7.m7.4.4.8.3.3.2" xref="S4.SS2.p12.7.m7.4.4.8.3.3.2.cmml">d</mi><mi id="S4.SS2.p12.7.m7.4.4.8.3.3.3" xref="S4.SS2.p12.7.m7.4.4.8.3.3.3.cmml">q</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.7.m7.4b"><apply id="S4.SS2.p12.7.m7.4.4.cmml" xref="S4.SS2.p12.7.m7.4.4"><and id="S4.SS2.p12.7.m7.4.4a.cmml" xref="S4.SS2.p12.7.m7.4.4"></and><apply id="S4.SS2.p12.7.m7.4.4b.cmml" xref="S4.SS2.p12.7.m7.4.4"><eq id="S4.SS2.p12.7.m7.4.4.6.cmml" xref="S4.SS2.p12.7.m7.4.4.6"></eq><ci id="S4.SS2.p12.7.m7.4.4.5.cmml" xref="S4.SS2.p12.7.m7.4.4.5">𝐻</ci><list id="S4.SS2.p12.7.m7.4.4.3.4.cmml" xref="S4.SS2.p12.7.m7.4.4.3.3"><apply id="S4.SS2.p12.7.m7.2.2.1.1.1.cmml" xref="S4.SS2.p12.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p12.7.m7.2.2.1.1.1.1.cmml" xref="S4.SS2.p12.7.m7.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p12.7.m7.2.2.1.1.1.2.cmml" xref="S4.SS2.p12.7.m7.2.2.1.1.1.2">𝐻</ci><cn type="integer" id="S4.SS2.p12.7.m7.2.2.1.1.1.3.cmml" xref="S4.SS2.p12.7.m7.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p12.7.m7.3.3.2.2.2.cmml" xref="S4.SS2.p12.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p12.7.m7.3.3.2.2.2.1.cmml" xref="S4.SS2.p12.7.m7.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p12.7.m7.3.3.2.2.2.2.cmml" xref="S4.SS2.p12.7.m7.3.3.2.2.2.2">𝐻</ci><cn type="integer" id="S4.SS2.p12.7.m7.3.3.2.2.2.3.cmml" xref="S4.SS2.p12.7.m7.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p12.7.m7.1.1.cmml" xref="S4.SS2.p12.7.m7.1.1">…</ci><apply id="S4.SS2.p12.7.m7.4.4.3.3.3.cmml" xref="S4.SS2.p12.7.m7.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p12.7.m7.4.4.3.3.3.1.cmml" xref="S4.SS2.p12.7.m7.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p12.7.m7.4.4.3.3.3.2.cmml" xref="S4.SS2.p12.7.m7.4.4.3.3.3.2">𝐻</ci><ci id="S4.SS2.p12.7.m7.4.4.3.3.3.3.cmml" xref="S4.SS2.p12.7.m7.4.4.3.3.3.3">𝑇</ci></apply></list></apply><apply id="S4.SS2.p12.7.m7.4.4c.cmml" xref="S4.SS2.p12.7.m7.4.4"><in id="S4.SS2.p12.7.m7.4.4.7.cmml" xref="S4.SS2.p12.7.m7.4.4.7"></in><share href="#S4.SS2.p12.7.m7.4.4.3.cmml" id="S4.SS2.p12.7.m7.4.4d.cmml" xref="S4.SS2.p12.7.m7.4.4"></share><apply id="S4.SS2.p12.7.m7.4.4.8.cmml" xref="S4.SS2.p12.7.m7.4.4.8"><csymbol cd="ambiguous" id="S4.SS2.p12.7.m7.4.4.8.1.cmml" xref="S4.SS2.p12.7.m7.4.4.8">superscript</csymbol><ci id="S4.SS2.p12.7.m7.4.4.8.2.cmml" xref="S4.SS2.p12.7.m7.4.4.8.2">ℝ</ci><apply id="S4.SS2.p12.7.m7.4.4.8.3.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3"><times id="S4.SS2.p12.7.m7.4.4.8.3.1.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3.1"></times><ci id="S4.SS2.p12.7.m7.4.4.8.3.2.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3.2">𝑇</ci><apply id="S4.SS2.p12.7.m7.4.4.8.3.3.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3.3"><csymbol cd="ambiguous" id="S4.SS2.p12.7.m7.4.4.8.3.3.1.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3.3">subscript</csymbol><ci id="S4.SS2.p12.7.m7.4.4.8.3.3.2.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3.3.2">𝑑</ci><ci id="S4.SS2.p12.7.m7.4.4.8.3.3.3.cmml" xref="S4.SS2.p12.7.m7.4.4.8.3.3.3">𝑞</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.7.m7.4c">H=[H_{1};H_{2};\dots;H_{T}]\in\mathbb{R}^{T\times d_{q}}</annotation></semantics></math> to give the question embedding, <math id="S4.SS2.p12.8.m8.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS2.p12.8.m8.1a"><mi id="S4.SS2.p12.8.m8.1.1" xref="S4.SS2.p12.8.m8.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p12.8.m8.1b"><ci id="S4.SS2.p12.8.m8.1.1.cmml" xref="S4.SS2.p12.8.m8.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p12.8.m8.1c">L</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p13" class="ltx_para">
<p id="S4.SS2.p13.4" class="ltx_p">Finally, LXMERT consists of <math id="S4.SS2.p13.1.m1.1" class="ltx_Math" alttext="N_{X}" display="inline"><semantics id="S4.SS2.p13.1.m1.1a"><msub id="S4.SS2.p13.1.m1.1.1" xref="S4.SS2.p13.1.m1.1.1.cmml"><mi id="S4.SS2.p13.1.m1.1.1.2" xref="S4.SS2.p13.1.m1.1.1.2.cmml">N</mi><mi id="S4.SS2.p13.1.m1.1.1.3" xref="S4.SS2.p13.1.m1.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p13.1.m1.1b"><apply id="S4.SS2.p13.1.m1.1.1.cmml" xref="S4.SS2.p13.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p13.1.m1.1.1.1.cmml" xref="S4.SS2.p13.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p13.1.m1.1.1.2.cmml" xref="S4.SS2.p13.1.m1.1.1.2">𝑁</ci><ci id="S4.SS2.p13.1.m1.1.1.3.cmml" xref="S4.SS2.p13.1.m1.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p13.1.m1.1c">N_{X}</annotation></semantics></math> cross-modality encoders stacked one-after-another. Each
encoder consists of two operations: 1.) language to vision cross attention,
<math id="S4.SS2.p13.2.m2.3" class="ltx_Math" alttext="X=\text{FC}(\text{SelfAttention}(\text{CrossAttention}_{LV}(X,L)))" display="inline"><semantics id="S4.SS2.p13.2.m2.3a"><mrow id="S4.SS2.p13.2.m2.3.3" xref="S4.SS2.p13.2.m2.3.3.cmml"><mi id="S4.SS2.p13.2.m2.3.3.3" xref="S4.SS2.p13.2.m2.3.3.3.cmml">X</mi><mo id="S4.SS2.p13.2.m2.3.3.2" xref="S4.SS2.p13.2.m2.3.3.2.cmml">=</mo><mrow id="S4.SS2.p13.2.m2.3.3.1" xref="S4.SS2.p13.2.m2.3.3.1.cmml"><mtext id="S4.SS2.p13.2.m2.3.3.1.3" xref="S4.SS2.p13.2.m2.3.3.1.3a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p13.2.m2.3.3.1.2" xref="S4.SS2.p13.2.m2.3.3.1.2.cmml">​</mo><mrow id="S4.SS2.p13.2.m2.3.3.1.1.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p13.2.m2.3.3.1.1.1.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p13.2.m2.3.3.1.1.1.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.cmml"><mtext id="S4.SS2.p13.2.m2.3.3.1.1.1.1.3" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.3a.cmml">SelfAttention</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.cmml"><msub id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.cmml"><mtext id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.2a.cmml">CrossAttention</mtext><mrow id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.3" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.3.cmml">V</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.2.1" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S4.SS2.p13.2.m2.1.1" xref="S4.SS2.p13.2.m2.1.1.cmml">X</mi><mo id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.2.2" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.SS2.p13.2.m2.2.2" xref="S4.SS2.p13.2.m2.2.2.cmml">L</mi><mo stretchy="false" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.2.3" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.3" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p13.2.m2.3.3.1.1.1.3" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p13.2.m2.3b"><apply id="S4.SS2.p13.2.m2.3.3.cmml" xref="S4.SS2.p13.2.m2.3.3"><eq id="S4.SS2.p13.2.m2.3.3.2.cmml" xref="S4.SS2.p13.2.m2.3.3.2"></eq><ci id="S4.SS2.p13.2.m2.3.3.3.cmml" xref="S4.SS2.p13.2.m2.3.3.3">𝑋</ci><apply id="S4.SS2.p13.2.m2.3.3.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1"><times id="S4.SS2.p13.2.m2.3.3.1.2.cmml" xref="S4.SS2.p13.2.m2.3.3.1.2"></times><ci id="S4.SS2.p13.2.m2.3.3.1.3a.cmml" xref="S4.SS2.p13.2.m2.3.3.1.3"><mtext id="S4.SS2.p13.2.m2.3.3.1.3.cmml" xref="S4.SS2.p13.2.m2.3.3.1.3">FC</mtext></ci><apply id="S4.SS2.p13.2.m2.3.3.1.1.1.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1"><times id="S4.SS2.p13.2.m2.3.3.1.1.1.1.2.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.2"></times><ci id="S4.SS2.p13.2.m2.3.3.1.1.1.1.3a.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.3"><mtext id="S4.SS2.p13.2.m2.3.3.1.1.1.1.3.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.3">SelfAttention</mtext></ci><apply id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1"><times id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.1"></times><apply id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.2a.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.2"><mtext id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.2">CrossAttention</mtext></ci><apply id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3"><times id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.1"></times><ci id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.2">𝐿</ci><ci id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.2.3.3">𝑉</ci></apply></apply><interval closure="open" id="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p13.2.m2.3.3.1.1.1.1.1.1.1.3.2"><ci id="S4.SS2.p13.2.m2.1.1.cmml" xref="S4.SS2.p13.2.m2.1.1">𝑋</ci><ci id="S4.SS2.p13.2.m2.2.2.cmml" xref="S4.SS2.p13.2.m2.2.2">𝐿</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p13.2.m2.3c">X=\text{FC}(\text{SelfAttention}(\text{CrossAttention}_{LV}(X,L)))</annotation></semantics></math>; and 2.) vision to
language cross attention, <math id="S4.SS2.p13.3.m3.3" class="ltx_Math" alttext="L=\text{FC}(\text{SelfAttention}(\text{CrossAttention}_{VL}(L,\allowbreak X)))" display="inline"><semantics id="S4.SS2.p13.3.m3.3a"><mrow id="S4.SS2.p13.3.m3.3.3" xref="S4.SS2.p13.3.m3.3.3.cmml"><mi id="S4.SS2.p13.3.m3.3.3.3" xref="S4.SS2.p13.3.m3.3.3.3.cmml">L</mi><mo id="S4.SS2.p13.3.m3.3.3.2" xref="S4.SS2.p13.3.m3.3.3.2.cmml">=</mo><mrow id="S4.SS2.p13.3.m3.3.3.1" xref="S4.SS2.p13.3.m3.3.3.1.cmml"><mtext id="S4.SS2.p13.3.m3.3.3.1.3" xref="S4.SS2.p13.3.m3.3.3.1.3a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p13.3.m3.3.3.1.2" xref="S4.SS2.p13.3.m3.3.3.1.2.cmml">​</mo><mrow id="S4.SS2.p13.3.m3.3.3.1.1.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p13.3.m3.3.3.1.1.1.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p13.3.m3.3.3.1.1.1.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.cmml"><mtext id="S4.SS2.p13.3.m3.3.3.1.1.1.1.3" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.3a.cmml">SelfAttention</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.cmml"><msub id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.cmml"><mtext id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.2a.cmml">CrossAttention</mtext><mrow id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.3" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.3.cmml">L</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.2.1" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S4.SS2.p13.3.m3.1.1" xref="S4.SS2.p13.3.m3.1.1.cmml">L</mi><mo id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.2.2" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.SS2.p13.3.m3.2.2" xref="S4.SS2.p13.3.m3.2.2.cmml">X</mi><mo stretchy="false" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.2.3" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.3" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p13.3.m3.3.3.1.1.1.3" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p13.3.m3.3b"><apply id="S4.SS2.p13.3.m3.3.3.cmml" xref="S4.SS2.p13.3.m3.3.3"><eq id="S4.SS2.p13.3.m3.3.3.2.cmml" xref="S4.SS2.p13.3.m3.3.3.2"></eq><ci id="S4.SS2.p13.3.m3.3.3.3.cmml" xref="S4.SS2.p13.3.m3.3.3.3">𝐿</ci><apply id="S4.SS2.p13.3.m3.3.3.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1"><times id="S4.SS2.p13.3.m3.3.3.1.2.cmml" xref="S4.SS2.p13.3.m3.3.3.1.2"></times><ci id="S4.SS2.p13.3.m3.3.3.1.3a.cmml" xref="S4.SS2.p13.3.m3.3.3.1.3"><mtext id="S4.SS2.p13.3.m3.3.3.1.3.cmml" xref="S4.SS2.p13.3.m3.3.3.1.3">FC</mtext></ci><apply id="S4.SS2.p13.3.m3.3.3.1.1.1.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1"><times id="S4.SS2.p13.3.m3.3.3.1.1.1.1.2.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.2"></times><ci id="S4.SS2.p13.3.m3.3.3.1.1.1.1.3a.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.3"><mtext id="S4.SS2.p13.3.m3.3.3.1.1.1.1.3.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.3">SelfAttention</mtext></ci><apply id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1"><times id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.1"></times><apply id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.2a.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.2"><mtext id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.2">CrossAttention</mtext></ci><apply id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3"><times id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.1"></times><ci id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.2">𝑉</ci><ci id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.2.3.3">𝐿</ci></apply></apply><interval closure="open" id="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p13.3.m3.3.3.1.1.1.1.1.1.1.3.2"><ci id="S4.SS2.p13.3.m3.1.1.cmml" xref="S4.SS2.p13.3.m3.1.1">𝐿</ci><ci id="S4.SS2.p13.3.m3.2.2.cmml" xref="S4.SS2.p13.3.m3.2.2">𝑋</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p13.3.m3.3c">L=\text{FC}(\text{SelfAttention}(\text{CrossAttention}_{VL}(L,\allowbreak X)))</annotation></semantics></math>.
The final output of the <math id="S4.SS2.p13.4.m4.1" class="ltx_Math" alttext="N_{X}" display="inline"><semantics id="S4.SS2.p13.4.m4.1a"><msub id="S4.SS2.p13.4.m4.1.1" xref="S4.SS2.p13.4.m4.1.1.cmml"><mi id="S4.SS2.p13.4.m4.1.1.2" xref="S4.SS2.p13.4.m4.1.1.2.cmml">N</mi><mi id="S4.SS2.p13.4.m4.1.1.3" xref="S4.SS2.p13.4.m4.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p13.4.m4.1b"><apply id="S4.SS2.p13.4.m4.1.1.cmml" xref="S4.SS2.p13.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p13.4.m4.1.1.1.cmml" xref="S4.SS2.p13.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p13.4.m4.1.1.2.cmml" xref="S4.SS2.p13.4.m4.1.1.2">𝑁</ci><ci id="S4.SS2.p13.4.m4.1.1.3.cmml" xref="S4.SS2.p13.4.m4.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p13.4.m4.1c">N_{X}</annotation></semantics></math> encoders are used to predict the answer.</p>
</div>
<div id="S4.SS2.p14" class="ltx_para ltx_noindent">
<p id="S4.SS2.p14.1" class="ltx_p"><span id="S4.SS2.p14.1.1" class="ltx_text ltx_font_bold">Evaluating Biases in the Datasets.</span>
We also evaluate the following prior-based baselines to reveal and understand the biases present in
the datasets.</p>
</div>
<div id="S4.SS2.p15" class="ltx_para ltx_noindent">
<p id="S4.SS2.p15.1" class="ltx_p"><span id="S4.SS2.p15.1.1" class="ltx_text ltx_framed ltx_framed_underline">Naïve Baseline</span>. The model always predicts the most frequent answer from the training set. For nuScenes, it always
predicts “yes”, while for Gibson it predicts “white”. Ideally, this should set a minimum
performance bar.</p>
</div>
<div id="S4.SS2.p16" class="ltx_para ltx_noindent">
<p id="S4.SS2.p16.1" class="ltx_p"><span id="S4.SS2.p16.1.1" class="ltx_text ltx_framed ltx_framed_underline">Hasty-Student Baseline</span>. In this baseline, a model simply finds the most frequent answer
for each type of question. In this case, we define a “question type” as the first two words of a
question. For example, a hasty-student might always answer “one” for all “How many” questions.
This is similar to the hasty-student baseline used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (MovieQA).</p>
</div>
<div id="S4.SS2.p17" class="ltx_para ltx_noindent">
<p id="S4.SS2.p17.1" class="ltx_p"><span id="S4.SS2.p17.1.1" class="ltx_text ltx_framed ltx_framed_underline">Question-Only Baseline</span>. In this model, we ignore the visual information and only use
question text to train a model. Our implementation takes as input only the question embedding, <math id="S4.SS2.p17.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS2.p17.1.m1.1a"><mi id="S4.SS2.p17.1.m1.1.1" xref="S4.SS2.p17.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p17.1.m1.1b"><ci id="S4.SS2.p17.1.m1.1.1.cmml" xref="S4.SS2.p17.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p17.1.m1.1c">q</annotation></semantics></math>
which is passed through several fully-connected layers to predict the answer distribution. This
baseline is meant to reveal the language-bias present in the dataset.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Human Performance</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">An ideal image-set question answering system should be able to reach at least the accuracy achieved
by humans. We evaluate the human performance using the annotations with the standard VQA-accuracy
metric described below. For the outdoor scenes dataset, humans obtain a VQA-accuracy of 91.88% and
for the indoor scenes they obtain 88.80%. Comparing this with table <a href="#S5.T2" title="Table 2 ‣ 5.3 Results ‣ 5 Experiments ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
shows that ISVQA is extremely challenging and requires specialized methods. The reason for
the human performance being lower than 100% is that, in many cases an annotator has given an answer
which is not exactly similar to the other two but is still semantically similar. For example, the
majority answer might have been “black and white” but the third annotator answered “white and
black”.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Implementation Details</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.4" class="ltx_p">We start by using Faster R-CNN in Detectron to extract the region proposals and features <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="R_{i}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">R</mi><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑅</ci><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">R_{i}</annotation></semantics></math>
for each image. Each region feature is 2048-D and we use the top 100 region proposals from each
image. To obtain the word-vector embeddings we use 300-D GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> vectors.
The joint visual-question embedding, <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">x</annotation></semantics></math> is taken to be 5000-D. For evaluation, we use the VQA-Accuracy
metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A predicted answer is given a score of one if it matches at least two out
of the three annotations. If it matches only one annotations, it is given a score of 0.5.
All of our VQA models are implemented in the Pythia framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and are trained
on two NVIDIA V100 GPUs for 22,000 iterations with a batch size of 32. The initial learning rate is
warmed up to <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mn id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><cn type="float" id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">0.01</annotation></semantics></math> in the first 1,000 iterations. The learning rate is dropped by a factor of 10 at
iterations 12,000 and 18,000. For the HME-VideoQA model, we use the implementation provided by the
authors. We train the model for 22,000 iterations with a batch size of 32 and a starting learning
rate of 0.001. For the transformer-based model, we use <math id="S5.SS2.p1.4.m4.2" class="ltx_Math" alttext="N_{L}=9,N_{R}=5,N_{X}=5" display="inline"><semantics id="S5.SS2.p1.4.m4.2a"><mrow id="S5.SS2.p1.4.m4.2.2.2" xref="S5.SS2.p1.4.m4.2.2.3.cmml"><mrow id="S5.SS2.p1.4.m4.1.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.1.cmml"><msub id="S5.SS2.p1.4.m4.1.1.1.1.2" xref="S5.SS2.p1.4.m4.1.1.1.1.2.cmml"><mi id="S5.SS2.p1.4.m4.1.1.1.1.2.2" xref="S5.SS2.p1.4.m4.1.1.1.1.2.2.cmml">N</mi><mi id="S5.SS2.p1.4.m4.1.1.1.1.2.3" xref="S5.SS2.p1.4.m4.1.1.1.1.2.3.cmml">L</mi></msub><mo id="S5.SS2.p1.4.m4.1.1.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.4.m4.1.1.1.1.3" xref="S5.SS2.p1.4.m4.1.1.1.1.3.cmml">9</mn></mrow><mo id="S5.SS2.p1.4.m4.2.2.2.3" xref="S5.SS2.p1.4.m4.2.2.3a.cmml">,</mo><mrow id="S5.SS2.p1.4.m4.2.2.2.2.2" xref="S5.SS2.p1.4.m4.2.2.2.2.3.cmml"><mrow id="S5.SS2.p1.4.m4.2.2.2.2.1.1" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.cmml"><msub id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.cmml"><mi id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.2" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.2.cmml">N</mi><mi id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.3" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.3.cmml">R</mi></msub><mo id="S5.SS2.p1.4.m4.2.2.2.2.1.1.1" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.4.m4.2.2.2.2.1.1.3" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.3.cmml">5</mn></mrow><mo id="S5.SS2.p1.4.m4.2.2.2.2.2.3" xref="S5.SS2.p1.4.m4.2.2.2.2.3a.cmml">,</mo><mrow id="S5.SS2.p1.4.m4.2.2.2.2.2.2" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.cmml"><msub id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.cmml"><mi id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.2" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.2.cmml">N</mi><mi id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.3" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.3.cmml">X</mi></msub><mo id="S5.SS2.p1.4.m4.2.2.2.2.2.2.1" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.1.cmml">=</mo><mn id="S5.SS2.p1.4.m4.2.2.2.2.2.2.3" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.3.cmml">5</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.2b"><apply id="S5.SS2.p1.4.m4.2.2.3.cmml" xref="S5.SS2.p1.4.m4.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.2.2.3a.cmml" xref="S5.SS2.p1.4.m4.2.2.2.3">formulae-sequence</csymbol><apply id="S5.SS2.p1.4.m4.1.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1"><eq id="S5.SS2.p1.4.m4.1.1.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1.1"></eq><apply id="S5.SS2.p1.4.m4.1.1.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.1.1.1.1.2.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.4.m4.1.1.1.1.2.2.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1.2.2">𝑁</ci><ci id="S5.SS2.p1.4.m4.1.1.1.1.2.3.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1.2.3">𝐿</ci></apply><cn type="integer" id="S5.SS2.p1.4.m4.1.1.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.1.1.3">9</cn></apply><apply id="S5.SS2.p1.4.m4.2.2.2.2.3.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.2.2.2.2.3a.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S5.SS2.p1.4.m4.2.2.2.2.1.1.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1"><eq id="S5.SS2.p1.4.m4.2.2.2.2.1.1.1.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.1"></eq><apply id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.1.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.2.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.2">𝑁</ci><ci id="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.3.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.2.3">𝑅</ci></apply><cn type="integer" id="S5.SS2.p1.4.m4.2.2.2.2.1.1.3.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.1.1.3">5</cn></apply><apply id="S5.SS2.p1.4.m4.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2"><eq id="S5.SS2.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.1"></eq><apply id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.1.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2">subscript</csymbol><ci id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.2">𝑁</ci><ci id="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.3.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.2.3">𝑋</ci></apply><cn type="integer" id="S5.SS2.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.3">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.2c">N_{L}=9,N_{R}=5,N_{X}=5</annotation></semantics></math>. We use a
batch-size of 32, learning rate of 0.00005, and we train the model for 20 epochs. All feature
dimensions are kept the same as LXMERT.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">We report the VQA-accuracy for all methods in table <a href="#S5.T2" title="Table 2 ‣ 5.3 Results ‣ 5 Experiments ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The accuracy achieved by
both of the VQA-based baselines is only around <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="50-54\%" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">50</mn><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml"><mn id="S5.SS3.p1.1.m1.1.1.3.2" xref="S5.SS3.p1.1.m1.1.1.3.2.cmml">54</mn><mo id="S5.SS3.p1.1.m1.1.1.3.1" xref="S5.SS3.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><minus id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></minus><cn type="integer" id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">50</cn><apply id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS3.p1.1.m1.1.1.3.1.cmml" xref="S5.SS3.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS3.p1.1.m1.1.1.3.2.cmml" xref="S5.SS3.p1.1.m1.1.1.3.2">54</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">50-54\%</annotation></semantics></math> and the Video VQA model achieves only
<math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="39.88\%" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">39.88</mn><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">39.88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">39.88\%</annotation></semantics></math> for the indoor dataset and <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="52.14\%" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">52.14</mn><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">52.14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">52.14\%</annotation></semantics></math> for the outdoor dataset. This highlights the need for
advanced models for ISVQA.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Comparison between Baselines.</span>
Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Results ‣ 5 Experiments ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the naïve baseline reaches a VQA-Accuracy of only 8.6% for the
indoor scenes dataset compared to 47.57% given by the Concatenate-Feature baseline and 50.53%
given by the Stitched-Image baseline model. This shows that single-image VQA methods are not enough
to overcome the challenges presented by ISVQA. On the other hand, our proposed transformer-based
model performs the best for both indoor and outdoor scenes out-performing other models by over
10%.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Language Biases.</span>
Recent works (<em id="S5.SS3.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>) show that high performance in VQA could be achieved
using only the language components. Deep networks can easily exploit biases in the datasets to find
short-cuts for answering questions. We observe that most VQA-based
baselines perform much better than the question-only baseline. This shows that the ISVQA datasets
are less biased compared to many existing VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and validates the
utility of developing ISVQA models that can utilize both the visual and language components
simultaneously.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.3.2" class="ltx_text" style="font-size:90%;">Results for both indoor and outdoor datasets.</span></figcaption>
<div id="S5.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:281.9pt;height:163.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.3pt,8.3pt) scale(0.908094897517124,0.908094897517124) ;">
<table id="S5.T2.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.4.1.1.1" class="ltx_tr">
<td id="S5.T2.4.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S5.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S5.T2.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="S5.T2.4.1.1.1.3.1" class="ltx_text ltx_font_bold">VQA-Accuracy</span> (%)</td>
</tr>
<tr id="S5.T2.4.1.2.2" class="ltx_tr">
<td id="S5.T2.4.1.2.2.1" class="ltx_td"></td>
<td id="S5.T2.4.1.2.2.2" class="ltx_td"></td>
<td id="S5.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">Gibson</td>
<td id="S5.T2.4.1.2.2.4" class="ltx_td ltx_align_center">nuScenes</td>
</tr>
<tr id="S5.T2.4.1.3.3" class="ltx_tr">
<td id="S5.T2.4.1.3.3.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">Naïve</td>
<td id="S5.T2.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.61</td>
<td id="S5.T2.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">22.46</td>
</tr>
<tr id="S5.T2.4.1.4.4" class="ltx_tr">
<td id="S5.T2.4.1.4.4.1" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.4.4.1.1" class="ltx_text ltx_font_bold">Prior-Based Baselines</span></td>
<td id="S5.T2.4.1.4.4.2" class="ltx_td ltx_align_center">Hasty-Student</td>
<td id="S5.T2.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">27.22</td>
<td id="S5.T2.4.1.4.4.4" class="ltx_td ltx_align_center">41.65</td>
</tr>
<tr id="S5.T2.4.1.5.5" class="ltx_tr">
<td id="S5.T2.4.1.5.5.1" class="ltx_td"></td>
<td id="S5.T2.4.1.5.5.2" class="ltx_td ltx_align_center">Question-Only</td>
<td id="S5.T2.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">40.26</td>
<td id="S5.T2.4.1.5.5.4" class="ltx_td ltx_align_center">46.06</td>
</tr>
<tr id="S5.T2.4.1.6.6" class="ltx_tr">
<td id="S5.T2.4.1.6.6.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">Video-VQA</td>
<td id="S5.T2.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.88</td>
<td id="S5.T2.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">52.14</td>
</tr>
<tr id="S5.T2.4.1.7.7" class="ltx_tr">
<td id="S5.T2.4.1.7.7.1" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.7.7.1.1" class="ltx_text ltx_font_bold">Approaches</span></td>
<td id="S5.T2.4.1.7.7.2" class="ltx_td ltx_align_center">Concatenate-Feature</td>
<td id="S5.T2.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">47.57</td>
<td id="S5.T2.4.1.7.7.4" class="ltx_td ltx_align_center">53.66</td>
</tr>
<tr id="S5.T2.4.1.8.8" class="ltx_tr">
<td id="S5.T2.4.1.8.8.1" class="ltx_td"></td>
<td id="S5.T2.4.1.8.8.2" class="ltx_td ltx_align_center">Stitched-Image</td>
<td id="S5.T2.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">50.53</td>
<td id="S5.T2.4.1.8.8.4" class="ltx_td ltx_align_center">54.32</td>
</tr>
<tr id="S5.T2.4.1.9.9" class="ltx_tr">
<td id="S5.T2.4.1.9.9.1" class="ltx_td"></td>
<td id="S5.T2.4.1.9.9.2" class="ltx_td ltx_align_center">Transformer-Based</td>
<td id="S5.T2.4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">61.58</td>
<td id="S5.T2.4.1.9.9.4" class="ltx_td ltx_align_center">64.91</td>
</tr>
<tr id="S5.T2.4.1.10.10" class="ltx_tr">
<td id="S5.T2.4.1.10.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T2.4.1.10.10.1.1" class="ltx_text ltx_font_bold">Human Performance</span></td>
<td id="S5.T2.4.1.10.10.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S5.T2.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">88.80</td>
<td id="S5.T2.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">91.88</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p"><span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_bold">Performance by Question Type.</span>
Figure <a href="#S5.F8" title="Figure 8 ‣ 5.3 Results ‣ 5 Experiments ‣ Visual Question Answering on Image Sets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the accuracy bar-chart of our single-image VQA-based
baselines for various types of questions. Using this chart, we have the following observations and
hypotheses:</p>
</div>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">Single-image VQA baselines can predict single-object attributes.</span>
Both baseline models can answer questions about colors of single objects well (black and gray bars).
This is expected because no cross-image dependency is needed.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para ltx_noindent">
<p id="S5.SS3.p6.1" class="ltx_p"><span id="S5.SS3.p6.1.1" class="ltx_text ltx_font_bold">General cases may need cross-image inference. </span>
A large portion of questions involve multiple objects, which may appear in different images. The two VQA baselines using simple attention do not perform well on such questions.
The across-image transformer-based approach performs much better.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para ltx_noindent">
<p id="S5.SS3.p7.1" class="ltx_p"><span id="S5.SS3.p7.1.1" class="ltx_text ltx_font_bold">Stitched-Image captures cross-image dependency better.</span>
The Stitched-Image baseline allows direct pooling from regions in all images,
which may capture across-image dependency better.
It also outperforms the
Concatenate-Feature baseline for most question types, except for the counting questions.
The Stitched-Image cannot avoid double counting.
The transformer-based approach has all the advantages of the Stitched-Image and can do more sophisticated inference.
</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2008.11976/assets/x10.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="249" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.6.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.7.2" class="ltx_text" style="font-size:90%;">Performance of the two VQA-based baselines for different types of questions for the
combined Gibson test set. Dark
colors represent the performance for Concatenate-Feature baseline and light colors for Stitched Image
baseline. <span id="S5.F8.7.2.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Blue</span> is used for geometric relationship questions,
<span id="S5.F8.7.2.2" class="ltx_text ltx_font_bold" style="color:#00E000;">green</span> for counting questions,
<span id="S5.F8.7.2.3" class="ltx_text ltx_font_bold" style="color:#FF0000;">red</span> for location, and <span id="S5.F8.7.2.4" class="ltx_text ltx_font_bold">black</span> for color questions. We
notice that the VQA-based baselines are able to answer simple questions like those about
colors of single objects very well. However, questions involving spatial reasoning between
objects in one image or across images are extremely challenging for such methods.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We proposed the new task of image-set visual question answering (ISVQA).
This task can lead to new research challenges, such as language-guided cross-image attentions and reasoning.
To establish the ISVQA problem and enable its research, we introduced two ISVQA datasets for indoor
and outdoor scenes. Large-scale annotations were collected for questions and answers with novel
ways to present the scene to the annotators. We performed bias analysis of the datasets to set up
performance lower bounds. We also extended a single-image VQA method to two simple attention-based
baseline models and showed the performance of state-of-the-art Video VQA model. Their limited
performance reflects the unique challenges of ISVQA, which cannot be solved trivially by the
capabilities of existing models. Approaches for solving the ISVQA problem may need to pass
information across images in a sophisticated way, understand the scene behind the image set, and
attend the relevant images. Another potential direction could be to create explicit maps of the
scenes. However, depending on the complexity of the scene, different techniques might be required to
explicitly construct a coherent map. Where such maps can be obtained accurately,
reconstruction-based ISVQA solutions can be more accurate than the baselines. Meanwhile,
humans do not have to do exact scene reconstruction to answer questions. So, in this
paper, we have focused on methods that can model across-image dependencies implicitly.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
Acharya, M., Kafle, K., Kanan, C.: TallyQA: Answering Complex Counting
Questions. AAAI Conference on Artificial Intelligence (2019)
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
Agrawal, A., Batra, D., Parikh, D.: Analyzing the Behavior of Visual Question
Answering Models. Empirical Methods in Natural Language Processing (2016)
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-Up and Top-Down Attention for Image Captioning and Visual
Question Answering. Conference on Computer Vision and Pattern Recognition
(2018)
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
D.: VQA: Visual Question Answering. International Conference on Computer
Vision (2015)
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan,
A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes: A multimodal dataset for
autonomous driving. arXiv preprint arXiv:1903.11027 (2019)
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied
Question Answering. Conference on Computer Vision and Pattern Recognition
(2018)
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
Desta, M.T., Chen, L., Kornuta, T.: Object-based reasoning in VQA. Winter
Conference on Applications of Computer Vision (2018)
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
Fan, C., Zhang, X., Zhang, S., Wang, W., Zhang, C., Huang, H.: Heterogeneous
Memory Enhanced Multimodal Attention Model for Video Question Answering. In:
Conference on Computer Vision and Pattern Recognition (2019)
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
Gao, P., Jiang, Z., You, H., Lu, P., Hoi, S., Wang, X., Li, H.: Dynamic Fusion
with Intra- and Inter-modality Attention Flow for Visual Question Answering.
Conference on Computer Vision and Pattern Recognition (2019)
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
Gao, P., You, H., Zhang, Z., Wang, X., Li, H.: Multi-modality Latent
Interaction Network for Visual Question Answering. International Conference
on Computer Vision (2019)
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.:
IQA: Visual Question Answering in Interactive Environments. Conference on
Computer Vision and Pattern Recognition (2018)
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
Goyal, Y., Khot, T., Agrawal, A., Summers-Stay, D., Batra, D., Parikh, D.:
Making the V in VQA Matter: Elevating the Role of Image Understanding in
Visual Question Answering. International Journal of Computer Vision (2019)
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J.,
Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind
people. In: Conference on Computer Vision and Pattern Recognition (2018)
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
Hudson, D.A., Manning, C.D.: GQA: A New Dataset for Real-World Visual
Reasoning and Compositional Question Answering. Conference on Computer
Vision and Pattern Recognition (2019)
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
Jang, Y., Song, Y., Yu, Y., Kim, Y., Kim, G.: TGIF-QA: Toward spatio-temporal
reasoning in visual question answering. In: Conference on Computer Vision
and Pattern Recognition. Institute of Electrical and Electronics Engineers
Inc. (2017)
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick,
C., Girshick, R.: Clevr: A diagnostic dataset for compositional language and
elementary visual reasoning. In: Conference on Computer Vision and Pattern
Recognition (2017)
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting
language and vision using crowdsourced dense image annotations. International
Journal of Computer Vision (2017)
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
Lei, J., Yu, L., Berg, T.L., Bansal, M.: TVQA+: Spatio-Temporal Grounding for
Video Question Answering. arXiv preprint arXiv:1904.11574 (2019)
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
Liang, J., Jiang, L., Cao, L., Li, L.J., Hauptmann, A.: Focal Visual-Text
Attention for Visual Question Answering. Conference on Computer Vision and
Pattern Recognition (2018)
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
Lin, X., Parikh, D.: Don’t Just Listen, Use Your Imagination: Leveraging
Visual Common Sense for Non-Visual Tasks. Conference on Computer Vision and
Pattern Recognition (2015)
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image
co-attention for visual question answering. In: Advances In Neural
Information Processing Systems (2016)
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: OK-VQA : A Visual
Question Answering Benchmark Requiring External Knowledge. Conference on
Computer Vision and Pattern Recognition (2019)
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word
representation. In: Conference on empirical methods in natural language
processing (2014)
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia,
P., Lillicrap, T.: A simple neural network module for relational reasoning.
Advances in Neural Information Processing Systems (2017)
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub,
J., Liu, J., Koltun, V., Malik, J., Parikh, D., Batra, D.: Habitat: A
Platform for Embodied AI Research. arXiv preprint arXiv:1904.01201 (2019)
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
Singh, A., Natarajan, V., Jiang, Y., Chen, X., Shah, M., Rohrbach, M., Batra,
D., Parikh, D.: Pythia-a platform for vision &amp; language research. In: SysML
Workshop, NeurIPS (2018)
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,
Rohrbach, M.: Towards vqa models that can read. In: Conference on Computer
Vision and Pattern Recognition (2019)
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations
from transformers. In: Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language
Processing (2019)
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.:
MovieQA: Understanding Stories in Movies through Question-Answering.
Conference on Computer Vision and Pattern Recognition (2016)
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
Trott, A., Xiong, C., Socher, R.: Interpretable counting for visual question
answering. In: International Conference on Learning Representations (2018)
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
Xia, F., Zamir, A.R., He, Z., Sax, A., Malik, J., Savarese, S.: Gibson env:
Real-world perception for embodied agents. In: Conference on Computer Vision
and Pattern Recognition (2018)
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
Yagcioglu, S., Erdem, A., Erdem, E., Ikizler-Cinbis, N.: Recipeqa: A challenge
dataset for multimodal comprehension of cooking recipes. arXiv preprint
arXiv:1809.00812 (2018)
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7W: Grounded Question
Answering in Images. Conference on Computer Vision and Pattern Recognition
(2016)
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.11975" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.11976" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.11976">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.11976" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.11977" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:04:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
