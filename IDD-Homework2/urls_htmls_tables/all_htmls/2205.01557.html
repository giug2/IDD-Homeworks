<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.01557] Training Mixed-Domain Translation Models via Federated Learning</title><meta property="og:description" content="Training mixed-domain translation models is a complex task that demands tailored architectures and costly data preparation techniques. In this work, we leverage federated learning (FL) in order to tackle the problem. O…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Mixed-Domain Translation Models via Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Mixed-Domain Translation Models via Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.01557">

<!--Generated on Mon Mar 11 14:13:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Training Mixed-Domain Translation Models via Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peyman Passban       Tanya Roosta<sup id="id2.2.id1" class="ltx_sup">∗</sup>      
<br class="ltx_break"><span id="id3.3.id2" class="ltx_text ltx_font_bold">Rahul Gupta       Ankit Chadha       Clement Chung
<br class="ltx_break"></span>Amazon
<br class="ltx_break"><span id="id4.4.id3" class="ltx_text ltx_font_typewriter">{peymp,troosta,gupra,ankitrc,chungcle}@amazon.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">  Equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Training mixed-domain translation models is a complex task that demands tailored architectures and costly data preparation techniques. In this work, we leverage federated learning (FL) in order to tackle the problem. Our investigation demonstrates that with slight modifications in the training process, neural machine translation (NMT) engines can be easily adapted when an FL-based aggregation is applied to fuse different domains. Experimental results also show that engines built via FL are able to perform on par with state-of-the-art baselines that rely on centralized training techniques.</p>
<p id="id6.id2" class="ltx_p">We evaluate our hypothesis in the presence of five datasets with different sizes, from different domains, to translate from German into English and discuss how FL and NMT can mutually benefit from each other. In addition to providing benchmarking results on the union of FL and NMT, we also propose a novel technique to dynamically control the communication bandwidth by selecting impactful parameters during FL updates. This is a significant achievement considering the large size of NMT engines that need to be exchanged between FL parties.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a rapidly growing field in the machine learning community. The reason for this popularity is because of its <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">decentralized</span> and <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">private</span> nature. Model training in FL is distributed over multiple nodes where each node could be an independent piece of hardware with its own isolated data. This unique feature enables building high-quality models that benefit from external resources without requiring access to them.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although FL is a relatively new field <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>, it has drawn researchers’ attention and the community has witnessed a rapid growth. Fields such as computer vision have adapted quickly to the FL framework <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al., <a href="#bib.bib10" title="" class="ltx_ref">2017</a>; Lin et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Hardy et al., <a href="#bib.bib12" title="" class="ltx_ref">2019</a>; Geiping et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Ren et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, but others, such as natural language processing (NLP), have not been as quick and only recently have begun to explore <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We believe that the reason for this slow(er) integration of NLP and FL is because representation learning in NLP is complicated and downstream tasks require large and data-hungry models. These requirements are heavy-handed for any FL model and slow down the unification. However, real-world NLP problems necessitate the use of distributed solutions with privacy-preserving characteristics <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>. Our work is an effort towards leveraging FL-based solutions in NLP.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we focus on NMT to combine it with FL. A review of the NLP literature shows that almost all recent groundbreaking architectures have been first proposed, or at least evaluated, for translation <cite class="ltx_cite ltx_citemacro_cite">Bahdanau et al. (<a href="#bib.bib1" title="" class="ltx_ref">2014</a>); Sutskever et al. (<a href="#bib.bib36" title="" class="ltx_ref">2014</a>); Gehring et al. (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>); Vaswani et al. (<a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite>. This is an indication of the intricacy of the NMT task. Therefore, it is fair to claim that any FL technique that is capable of training high-quality NMT models could also be considered as a trustworthy alternative for other NLP tasks. Thus, NMT could be a strong candidate for FL benchmarking purposes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">NMT also has other unique features that can directly benefit FL. One key factor in a fair simulation of FL is to mimic data heterogeneity (also referred to as non-IIDness) in experimental environments <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>. Different sampling techniques have been proposed to model such data distribution processes <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>, however there is no guarantee that what we simulate is what we encounter in real-world applications. NMT, to some extent, solves this problem since parallel datasets by nature have such heterogeneity. There are only a few NLP tasks that can provide as large and diverse training corpora as NMT. For some language pairs, there exist multiple datasets with hundreds of thousands or even millions of parallel sentences.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://opus.nlpl.eu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://opus.nlpl.eu/</a></span></span></span> This means, we should have enough data for each FL node. Moreover, each node can naturally pick one dataset/language, so we do not have to artificially distribute data. NMT, together with offering rich training data, also has a bi-lingual setting which is a compelling testbed for FL. Coping with the complexity of not only different domains, but also different languages, at the same time in a distributed platform is worth investigating.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">While NMT offers a natural testbed for FL, we argue that the task itself benefits from the offerings of FL. Training a mixed-domain translation model is a challenging task. We show that the aggregation phase of FL can greatly help with this challenge, as it efficiently fuses information from different domains. From the privacy perspective, NMT can also benefit from FL. In fact, the necessity of having a private training pipeline is relatively understudied in NMT. It is mostly assumed that all training datasets are available in a homogeneous and compatible format through a central repository, which is usually not attainable in real world. All these reasons make NMT a compelling case to study in the context of FL.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Research Scope</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The goal of this paper is to provide preliminary results on the combination of FL and NMT, as opposed to running a comprehensive FL research or comparing different algorithms. We are also interested in studying the feasibility of training complex and deep NMT models in <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">decentralized</span> and <span id="S1.SS1.p1.1.2" class="ltx_text ltx_font_italic">private</span> settings. Besides providing a set of benchmarking results, this paper’s other two contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We show that FL aggregation techniques are reasonable alternatives to fuse information from multiple domains, thus FL-based training could be considered as an approach to build mixed-domain NMT engines.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that large NMT models are hard to distribute within the FL network. Therefore, we propose a novel and cost-efficient solution to reduce the communication bandwidth.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">FL is an approach to train models in a distributed fashion where nodes do not (and are not allowed to) access each other’s data <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>. Any node by itself is not powerful enough to deliver high-quality services due to the small size of its local data. It can perform well on in-domain instances, but it might fail to respond to requests from other domains. FL establishes a communication methodology and a platform that allows participating nodes to exchange parameters (but not data) to help boost each other’s performance.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this work, we follow the <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">cross-silo</span> FL setting as outlined in <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. Algorithm <a href="#algorithm1" title="In 2 Federated Learning ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the entire model training pipeline and explains what we mean by being cross-silo.</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.7" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>



<span id="algorithm1.2.2.1" class="ltx_text ltx_font_bold">for</span> <math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="r\leftarrow 0" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mrow id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml"><mi id="algorithm1.1.1.m1.1.1.2" xref="algorithm1.1.1.m1.1.1.2.cmml">r</mi><mo stretchy="false" id="algorithm1.1.1.m1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.cmml">←</mo><mn id="algorithm1.1.1.m1.1.1.3" xref="algorithm1.1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1"><ci id="algorithm1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1">←</ci><ci id="algorithm1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.2">𝑟</ci><cn type="integer" id="algorithm1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">r\leftarrow 0</annotation></semantics></math> <span id="algorithm1.2.2.2" class="ltx_text ltx_font_bold">to</span> <span id="algorithm1.2.2.3" class="ltx_text ltx_font_italic">rounds</span> <span id="algorithm1.2.2.4" class="ltx_text ltx_font_bold">by</span> <math id="algorithm1.2.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="algorithm1.2.2.m2.1a"><mn id="algorithm1.2.2.m2.1.1" xref="algorithm1.2.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m2.1b"><cn type="integer" id="algorithm1.2.2.m2.1.1.cmml" xref="algorithm1.2.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m2.1c">1</annotation></semantics></math> <span id="algorithm1.2.2.5" class="ltx_text ltx_font_bold">do</span>
</div>
<div id="algorithm1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
updates = <span id="algorithm1.3.3.1" class="ltx_text ltx_font_typewriter">Pull(</span> <math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="algorithm1.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><ci id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">\mathcal{C}</annotation></semantics></math><span id="algorithm1.3.3.2" class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="algorithm1.7.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="algorithm1.7.8.1" class="ltx_text ltx_font_bold">for</span>  <span id="algorithm1.7.8.2" class="ltx_text ltx_font_italic">upd</span> <span id="algorithm1.7.8.3" class="ltx_text ltx_font_bold">in</span> <span id="algorithm1.7.8.4" class="ltx_text ltx_font_italic">updates</span> <span id="algorithm1.7.8.5" class="ltx_text ltx_font_bold">do</span>
</div>
<div id="algorithm1.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.4.4.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="algorithm1.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.4.4.m1.1.1" xref="algorithm1.4.4.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m1.1b"><ci id="algorithm1.4.4.m1.1.1.cmml" xref="algorithm1.4.4.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m1.1c">\mathcal{S}</annotation></semantics></math> = <span id="algorithm1.5.5.1" class="ltx_text ltx_font_italic">aggregate</span> (<math id="algorithm1.5.5.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="algorithm1.5.5.m2.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.5.5.m2.1.1" xref="algorithm1.5.5.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m2.1b"><ci id="algorithm1.5.5.m2.1.1.cmml" xref="algorithm1.5.5.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m2.1c">\mathcal{S}</annotation></semantics></math>,<span id="algorithm1.5.5.2" class="ltx_text ltx_font_italic">upd</span>)
</div>
<div id="algorithm1.7.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.7.7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <math id="algorithm1.6.6.m1.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="algorithm1.6.6.m1.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><ci id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">\mathcal{C}</annotation></semantics></math> = <span id="algorithm1.7.7.1" class="ltx_text ltx_font_typewriter">Push(</span> <math id="algorithm1.7.7.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="algorithm1.7.7.m2.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.7.7.m2.1.1" xref="algorithm1.7.7.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m2.1b"><ci id="algorithm1.7.7.m2.1.1.cmml" xref="algorithm1.7.7.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m2.1c">\mathcal{S}</annotation></semantics></math><span id="algorithm1.7.7.2" class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="algorithm1.7.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.7.11" class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.9.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Cross-Silo FL</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.4" class="ltx_p">In this setup, there is a central node <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\mathcal{S}</annotation></semantics></math> that orchestrates training. In each round <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">r</annotation></semantics></math>, the server pulls local updates (i.e. a set of parameters) from different nodes (also known as clients) and updates the parameters of the central model. <math id="S2.p3.3.m3.3" class="ltx_Math" alttext="\mathcal{C}=\{c_{1},...,c_{K}\}" display="inline"><semantics id="S2.p3.3.m3.3a"><mrow id="S2.p3.3.m3.3.3" xref="S2.p3.3.m3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.3.m3.3.3.4" xref="S2.p3.3.m3.3.3.4.cmml">𝒞</mi><mo id="S2.p3.3.m3.3.3.3" xref="S2.p3.3.m3.3.3.3.cmml">=</mo><mrow id="S2.p3.3.m3.3.3.2.2" xref="S2.p3.3.m3.3.3.2.3.cmml"><mo stretchy="false" id="S2.p3.3.m3.3.3.2.2.3" xref="S2.p3.3.m3.3.3.2.3.cmml">{</mo><msub id="S2.p3.3.m3.2.2.1.1.1" xref="S2.p3.3.m3.2.2.1.1.1.cmml"><mi id="S2.p3.3.m3.2.2.1.1.1.2" xref="S2.p3.3.m3.2.2.1.1.1.2.cmml">c</mi><mn id="S2.p3.3.m3.2.2.1.1.1.3" xref="S2.p3.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p3.3.m3.3.3.2.2.4" xref="S2.p3.3.m3.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">…</mi><mo id="S2.p3.3.m3.3.3.2.2.5" xref="S2.p3.3.m3.3.3.2.3.cmml">,</mo><msub id="S2.p3.3.m3.3.3.2.2.2" xref="S2.p3.3.m3.3.3.2.2.2.cmml"><mi id="S2.p3.3.m3.3.3.2.2.2.2" xref="S2.p3.3.m3.3.3.2.2.2.2.cmml">c</mi><mi id="S2.p3.3.m3.3.3.2.2.2.3" xref="S2.p3.3.m3.3.3.2.2.2.3.cmml">K</mi></msub><mo stretchy="false" id="S2.p3.3.m3.3.3.2.2.6" xref="S2.p3.3.m3.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.3b"><apply id="S2.p3.3.m3.3.3.cmml" xref="S2.p3.3.m3.3.3"><eq id="S2.p3.3.m3.3.3.3.cmml" xref="S2.p3.3.m3.3.3.3"></eq><ci id="S2.p3.3.m3.3.3.4.cmml" xref="S2.p3.3.m3.3.3.4">𝒞</ci><set id="S2.p3.3.m3.3.3.2.3.cmml" xref="S2.p3.3.m3.3.3.2.2"><apply id="S2.p3.3.m3.2.2.1.1.1.cmml" xref="S2.p3.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p3.3.m3.2.2.1.1.1.1.cmml" xref="S2.p3.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.p3.3.m3.2.2.1.1.1.2.cmml" xref="S2.p3.3.m3.2.2.1.1.1.2">𝑐</ci><cn type="integer" id="S2.p3.3.m3.2.2.1.1.1.3.cmml" xref="S2.p3.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">…</ci><apply id="S2.p3.3.m3.3.3.2.2.2.cmml" xref="S2.p3.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p3.3.m3.3.3.2.2.2.1.cmml" xref="S2.p3.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.p3.3.m3.3.3.2.2.2.2.cmml" xref="S2.p3.3.m3.3.3.2.2.2.2">𝑐</ci><ci id="S2.p3.3.m3.3.3.2.2.2.3.cmml" xref="S2.p3.3.m3.3.3.2.2.2.3">𝐾</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.3c">\mathcal{C}=\{c_{1},...,c_{K}\}</annotation></semantics></math> indicates the set of <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">K</annotation></semantics></math> clients. Once all information is aggregated, parameters of the central model are pushed back to clients so that they can also benefit from global/community knowledge.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">One key factor in FL is <span id="S2.p4.1.1" class="ltx_text ltx_font_italic">communication</span>, which is defined by the <span id="S2.p4.1.2" class="ltx_text ltx_font_typewriter">Pull</span> and <span id="S2.p4.1.3" class="ltx_text ltx_font_typewriter">Push</span> steps in this algorithm. Due to the distributed nature of FL, nodes need to connect and exchange information and this needs to be carried out in an efficient fashion. The communication cost becomes even more critical when exchanging large models, such as in NMT. In the next sections, we discuss how communication directly affect the feasibility and performance of any FL setting, and how we improve it by our <span id="S2.p4.1.4" class="ltx_text ltx_font_italic">dynamic pulling</span> technique.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Algorithm <a href="#algorithm1" title="In 2 Federated Learning ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> only shows the computation that occurs on the server side. It should be noted that each client is an independent silo that updates its internal model with local data. This algorithm only illustrates the main skeleton of the cross-silo setting and does not entail all the details of each step.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.4" class="ltx_p">In our experiments, we use the well-known <span id="S2.p6.4.1" class="ltx_text ltx_font_typewriter">FedAVG</span> algorithm <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> for <span id="S2.p6.4.2" class="ltx_text ltx_font_italic">aggregation</span>. Therefor, Line 4 can be formulated as in Equation <a href="#S2.E1" title="In 2 Federated Learning ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="w_{r}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}w^{k}_{r}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">w</mi><mi id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml">r</mi></msub><mo rspace="0.111em" stretchy="false" id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">←</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><munderover id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.1.1.3.1.2.2" xref="S2.E1.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.1.1.3.1.2.3" xref="S2.E1.m1.1.1.3.1.2.3.cmml"><mi id="S2.E1.m1.1.1.3.1.2.3.2" xref="S2.E1.m1.1.1.3.1.2.3.2.cmml">k</mi><mo id="S2.E1.m1.1.1.3.1.2.3.1" xref="S2.E1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.1.1.3.1.2.3.3" xref="S2.E1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.1.1.3.1.3" xref="S2.E1.m1.1.1.3.1.3.cmml">K</mi></munderover><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mfrac id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml"><msub id="S2.E1.m1.1.1.3.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.2.cmml">n</mi><mi id="S2.E1.m1.1.1.3.2.2.2.3" xref="S2.E1.m1.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="S2.E1.m1.1.1.3.2.2.3" xref="S2.E1.m1.1.1.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">​</mo><msubsup id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.1.1.3.2.3.2.2" xref="S2.E1.m1.1.1.3.2.3.2.2.cmml">w</mi><mi id="S2.E1.m1.1.1.3.2.3.3" xref="S2.E1.m1.1.1.3.2.3.3.cmml">r</mi><mi id="S2.E1.m1.1.1.3.2.3.2.3" xref="S2.E1.m1.1.1.3.2.3.2.3.cmml">k</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><ci id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">←</ci><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">𝑟</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><apply id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.1.1.cmml" xref="S2.E1.m1.1.1.3.1">superscript</csymbol><apply id="S2.E1.m1.1.1.3.1.2.cmml" xref="S2.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.1.2.1.cmml" xref="S2.E1.m1.1.1.3.1">subscript</csymbol><sum id="S2.E1.m1.1.1.3.1.2.2.cmml" xref="S2.E1.m1.1.1.3.1.2.2"></sum><apply id="S2.E1.m1.1.1.3.1.2.3.cmml" xref="S2.E1.m1.1.1.3.1.2.3"><eq id="S2.E1.m1.1.1.3.1.2.3.1.cmml" xref="S2.E1.m1.1.1.3.1.2.3.1"></eq><ci id="S2.E1.m1.1.1.3.1.2.3.2.cmml" xref="S2.E1.m1.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="S2.E1.m1.1.1.3.1.2.3.3.cmml" xref="S2.E1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.1.1.3.1.3.cmml" xref="S2.E1.m1.1.1.3.1.3">𝐾</ci></apply><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><times id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></times><apply id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2"><divide id="S2.E1.m1.1.1.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2.2"></divide><apply id="S2.E1.m1.1.1.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2.2">𝑛</ci><ci id="S2.E1.m1.1.1.3.2.2.2.3.cmml" xref="S2.E1.m1.1.1.3.2.2.2.3">𝑘</ci></apply><ci id="S2.E1.m1.1.1.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.2.2.3">𝑛</ci></apply><apply id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.2.3">subscript</csymbol><apply id="S2.E1.m1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.3">superscript</csymbol><ci id="S2.E1.m1.1.1.3.2.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.3.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.2.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3.2.3">𝑘</ci></apply><ci id="S2.E1.m1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.1.1.3.2.3.3">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">w_{r}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}w^{k}_{r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p6.3" class="ltx_p">where <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="w_{r}" display="inline"><semantics id="S2.p6.1.m1.1a"><msub id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml"><mi id="S2.p6.1.m1.1.1.2" xref="S2.p6.1.m1.1.1.2.cmml">w</mi><mi id="S2.p6.1.m1.1.1.3" xref="S2.p6.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><apply id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.1.1.1.cmml" xref="S2.p6.1.m1.1.1">subscript</csymbol><ci id="S2.p6.1.m1.1.1.2.cmml" xref="S2.p6.1.m1.1.1.2">𝑤</ci><ci id="S2.p6.1.m1.1.1.3.cmml" xref="S2.p6.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">w_{r}</annotation></semantics></math> is the set of all parameters of the central model in the <span id="S2.p6.3.1" class="ltx_text ltx_font_italic">r</span>-<span id="S2.p6.3.2" class="ltx_text ltx_font_italic">th</span> round, <math id="S2.p6.2.m2.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S2.p6.2.m2.1a"><msub id="S2.p6.2.m2.1.1" xref="S2.p6.2.m2.1.1.cmml"><mi id="S2.p6.2.m2.1.1.2" xref="S2.p6.2.m2.1.1.2.cmml">n</mi><mi id="S2.p6.2.m2.1.1.3" xref="S2.p6.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.1b"><apply id="S2.p6.2.m2.1.1.cmml" xref="S2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m2.1.1.1.cmml" xref="S2.p6.2.m2.1.1">subscript</csymbol><ci id="S2.p6.2.m2.1.1.2.cmml" xref="S2.p6.2.m2.1.1.2">𝑛</ci><ci id="S2.p6.2.m2.1.1.3.cmml" xref="S2.p6.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.1c">n_{k}</annotation></semantics></math> is the number of data points in the <span id="S2.p6.3.3" class="ltx_text ltx_font_italic">k</span>-th client’s dataset, and <math id="S2.p6.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p6.3.m3.1a"><mi id="S2.p6.3.m3.1.1" xref="S2.p6.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p6.3.m3.1b"><ci id="S2.p6.3.m3.1.1.cmml" xref="S2.p6.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m3.1c">n</annotation></semantics></math> is the total number of all training samples. There exist multiple extensions to <span id="S2.p6.3.4" class="ltx_text ltx_font_typewriter">FedAVG</span>, but since it is a widely-acceptable baseline for benchmarking purposes we also use it in our experiments. This choice allows us to minimize the impact of different factors introduced by other FL algorithms and only focus on the relation between NMT and FL and their mutual impact on each other.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Federated Learning for NLP</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">There are several models in the field that have been proposed to leverage FL for NLP. <cite class="ltx_cite ltx_citemacro_citet">Hartmann et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> studied whether they could improve the ranking of suggestions in the Firefox URL bar and train a model on user interactions without violating user privacy. They incorporated feedback received from different clients using <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">FedAVG</span> which resulted in significant improvements. <cite class="ltx_cite ltx_citemacro_citet">Ji et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> suggested that the simple averaging strategy used in <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">FedAVG</span> might not be sufficient enough, so they improved the aggregation phase by incorporating the significance of each client by using an attention mask to weigh clients. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> focused on language modelling and addressed the problem of out-of-vocabulary entries when working with different clients.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Bui et al. (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> investigated the effect of FL in training better and more personalized user/data representations. Their results show that when aggregating information via FL, the model quality increases significantly; at the same time, the training pipeline is distributed and private. We also make a similar observation in our experiments, though not at the representation level, but in terms of the final translation quality.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Ge et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> proposed a named-entity recognition (NER) model that is trained with FL to work on medical data. Their results demonstrate that not only FL preserves privacy, but also outperforms models trained in a centralized fashion. Apart from NER, models with similar concerns have been proposed for mobile keyword prediction <cite class="ltx_cite ltx_citemacro_citep">(Hard et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, keyword spotting <cite class="ltx_cite ltx_citemacro_citep">(Leroy et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, and next word prediction <cite class="ltx_cite ltx_citemacro_citep">(Stremmel and Singh, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Liu and Miller (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> utilized FL to pre-train and/or fine-tune BERT models <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. From a research standpoint, it is worthwhile to understand if it is even feasible to handle such deep models in an FL framework, and whether a simple averaging-based aggregation is enough. They attempted to address these questions and provided supporting results. In that sense, our work is similar to theirs as we also work with complex models. In addition, we also discuss the bandwidth problem to facilitate exchanging large sets of parameters.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Domain Adaptation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Domain adaptation covers a wide range of problems from adjusting a model to work in a new domain/genre <cite class="ltx_cite ltx_citemacro_citep">(Chu and Wang, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> to fine-tuning for noisier conditions <cite class="ltx_cite ltx_citemacro_citep">(Passban et al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, or even transferring a model to a different environment for a different task <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>. Domain adaptation has recently attracted more attention due to advances introduced by models such as ELMO <cite class="ltx_cite ltx_citemacro_citep">(Peters et al., <a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite> and BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. These models provide general-purpose representations which are easily adaptable to other tasks. In these models, <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">all</span> network parameters are fine-tuned during adaptation, which might not be necessary. <cite class="ltx_cite ltx_citemacro_citet">Houlsby et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Rücklé et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> proposed a new set of architectures, known as Adapters, to tackle this problem. Adapters are low-cost plug-ins that are mounted on pre-trained models, so when adapting the model only these small sets of parameters are updated.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Bapna et al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> proposed an NMT variation of Adapters. In their model, a dedicated component is added inside each layer that is responsible for transitioning in-between domains. However, all these solutions perform in centralized settings. <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> studied this problem in the context of FL and showed that Adapters might not be aligned well with the distributed nature of FL. As they reported, Adapters seem to be suitable to connect two domains but when exposed to several domains in FL, they diverge too much from their main distribution, such that using them in the body of clients drastically downgrades performance.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">To address the problem, they introduced additional and dedicated layers (as opposed to intra-layer modules in Adapters), called Controllers, that are designed to be exchanged between the server and clients. These new layers are placed in-between client model’s original layers and deal with external information sent/received to/from the server. Since they only exchange Controllers, they are able to reduce the communication bandwidth. This work is the closest to ours so we use it as our main baseline.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The model proposed by <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> suffers from two issues. They randomly introduce new layers (Controllers) but there is no mechanism defined to determine the number those layers, i.e. it is not investigated that how many Controllers are required under different conditions. It is also not clear where these layers need to be placed, and it is only discovered through experimental explorations. On the contrary, we introduce a simple yet effective heuristic to select a subset of impactful parameters during communication. In our solution, we do not need to deal with the aforementioned issues.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Low-Cost Domain Adaptation in FL</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In our FL setting, we initialize each client with a high-quality but generic NMT engine. Clients use local data to fine-tune their internal model and the combination of a pre-loaded model with local data should lead to better quality. Clients also connect with the server regularly to transfer local knowledge and contribute to the aggregation phase. In such a process, domain adaptation happens naturally. Inspired by <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, we implemented this idea and observed a substantial boost in our translation engines. Not only are we able to deliver better results but also we train NMT models in a distributed and private fashion. However, we noticed that <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">communication</span> could be quite costly.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.6" class="ltx_p">In the default configuration, for every <span id="S4.p2.6.1" class="ltx_text ltx_font_typewriter">Pull</span> (in Algorithm <a href="#algorithm1" title="In 2 Federated Learning ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) a large NMT engine has to be exchanged, which might not be necessary. In order to clarify why, we designed an experiment whose information is illustrated in Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In this experiment, we pick three of our engines and train them for <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">120</annotation></semantics></math>K steps within the FL pipeline. For each model, we compare tensors from the <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="integer" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">120</annotation></semantics></math>K checkpoint to their <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="110" display="inline"><semantics id="S4.p2.3.m3.1a"><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">110</mn><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><cn type="integer" id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">110</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">110</annotation></semantics></math>K peers and measure how much they changed in-between these two checkpoints. More specifically, for a given tensor <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">w</annotation></semantics></math>, we compute the pair-wise difference between values from the two checkpoints (<math id="S4.p2.5.m5.1" class="ltx_Math" alttext="w_{d}=w_{120K}-w_{110K}" display="inline"><semantics id="S4.p2.5.m5.1a"><mrow id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><msub id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml"><mi id="S4.p2.5.m5.1.1.2.2" xref="S4.p2.5.m5.1.1.2.2.cmml">w</mi><mi id="S4.p2.5.m5.1.1.2.3" xref="S4.p2.5.m5.1.1.2.3.cmml">d</mi></msub><mo id="S4.p2.5.m5.1.1.1" xref="S4.p2.5.m5.1.1.1.cmml">=</mo><mrow id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml"><msub id="S4.p2.5.m5.1.1.3.2" xref="S4.p2.5.m5.1.1.3.2.cmml"><mi id="S4.p2.5.m5.1.1.3.2.2" xref="S4.p2.5.m5.1.1.3.2.2.cmml">w</mi><mrow id="S4.p2.5.m5.1.1.3.2.3" xref="S4.p2.5.m5.1.1.3.2.3.cmml"><mn id="S4.p2.5.m5.1.1.3.2.3.2" xref="S4.p2.5.m5.1.1.3.2.3.2.cmml">120</mn><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.2.3.1" xref="S4.p2.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.2.3.3" xref="S4.p2.5.m5.1.1.3.2.3.3.cmml">K</mi></mrow></msub><mo id="S4.p2.5.m5.1.1.3.1" xref="S4.p2.5.m5.1.1.3.1.cmml">−</mo><msub id="S4.p2.5.m5.1.1.3.3" xref="S4.p2.5.m5.1.1.3.3.cmml"><mi id="S4.p2.5.m5.1.1.3.3.2" xref="S4.p2.5.m5.1.1.3.3.2.cmml">w</mi><mrow id="S4.p2.5.m5.1.1.3.3.3" xref="S4.p2.5.m5.1.1.3.3.3.cmml"><mn id="S4.p2.5.m5.1.1.3.3.3.2" xref="S4.p2.5.m5.1.1.3.3.3.2.cmml">110</mn><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.3.3.1" xref="S4.p2.5.m5.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.3.3.3" xref="S4.p2.5.m5.1.1.3.3.3.3.cmml">K</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><eq id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1.1"></eq><apply id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.2.1.cmml" xref="S4.p2.5.m5.1.1.2">subscript</csymbol><ci id="S4.p2.5.m5.1.1.2.2.cmml" xref="S4.p2.5.m5.1.1.2.2">𝑤</ci><ci id="S4.p2.5.m5.1.1.2.3.cmml" xref="S4.p2.5.m5.1.1.2.3">𝑑</ci></apply><apply id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"><minus id="S4.p2.5.m5.1.1.3.1.cmml" xref="S4.p2.5.m5.1.1.3.1"></minus><apply id="S4.p2.5.m5.1.1.3.2.cmml" xref="S4.p2.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.3.2.1.cmml" xref="S4.p2.5.m5.1.1.3.2">subscript</csymbol><ci id="S4.p2.5.m5.1.1.3.2.2.cmml" xref="S4.p2.5.m5.1.1.3.2.2">𝑤</ci><apply id="S4.p2.5.m5.1.1.3.2.3.cmml" xref="S4.p2.5.m5.1.1.3.2.3"><times id="S4.p2.5.m5.1.1.3.2.3.1.cmml" xref="S4.p2.5.m5.1.1.3.2.3.1"></times><cn type="integer" id="S4.p2.5.m5.1.1.3.2.3.2.cmml" xref="S4.p2.5.m5.1.1.3.2.3.2">120</cn><ci id="S4.p2.5.m5.1.1.3.2.3.3.cmml" xref="S4.p2.5.m5.1.1.3.2.3.3">𝐾</ci></apply></apply><apply id="S4.p2.5.m5.1.1.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.3.3.1.cmml" xref="S4.p2.5.m5.1.1.3.3">subscript</csymbol><ci id="S4.p2.5.m5.1.1.3.3.2.cmml" xref="S4.p2.5.m5.1.1.3.3.2">𝑤</ci><apply id="S4.p2.5.m5.1.1.3.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3.3"><times id="S4.p2.5.m5.1.1.3.3.3.1.cmml" xref="S4.p2.5.m5.1.1.3.3.3.1"></times><cn type="integer" id="S4.p2.5.m5.1.1.3.3.3.2.cmml" xref="S4.p2.5.m5.1.1.3.3.3.2">110</cn><ci id="S4.p2.5.m5.1.1.3.3.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3.3.3">𝐾</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">w_{d}=w_{120K}-w_{110K}</annotation></semantics></math>), then compute the absolute-value norm of the difference tensor (<math id="S4.p2.6.m6.1" class="ltx_Math" alttext="\lVert w_{d}\rVert" display="inline"><semantics id="S4.p2.6.m6.1a"><mrow id="S4.p2.6.m6.1.1.1" xref="S4.p2.6.m6.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.p2.6.m6.1.1.1.2" xref="S4.p2.6.m6.1.1.2.1.cmml">∥</mo><msub id="S4.p2.6.m6.1.1.1.1" xref="S4.p2.6.m6.1.1.1.1.cmml"><mi id="S4.p2.6.m6.1.1.1.1.2" xref="S4.p2.6.m6.1.1.1.1.2.cmml">w</mi><mi id="S4.p2.6.m6.1.1.1.1.3" xref="S4.p2.6.m6.1.1.1.1.3.cmml">d</mi></msub><mo fence="true" lspace="0em" id="S4.p2.6.m6.1.1.1.3" xref="S4.p2.6.m6.1.1.2.1.cmml">∥</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><apply id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.1"><csymbol cd="latexml" id="S4.p2.6.m6.1.1.2.1.cmml" xref="S4.p2.6.m6.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.p2.6.m6.1.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.6.m6.1.1.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1.1">subscript</csymbol><ci id="S4.p2.6.m6.1.1.1.1.2.cmml" xref="S4.p2.6.m6.1.1.1.1.2">𝑤</ci><ci id="S4.p2.6.m6.1.1.1.1.3.cmml" xref="S4.p2.6.m6.1.1.1.1.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">\lVert w_{d}\rVert</annotation></semantics></math>). The norm value indicates the shift of each tensor during FL rounds. Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a histograms of norm values that belong to different tensors from the encoders and decoders of our translation engines (for more information about the engines and datasets see Section <a href="#S5" title="5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2205.01557/assets/diff-tensor-enc-dec.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="222" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The histograms of the norm values of the difference tensors computed for the <math id="S4.F1.8.m1.1" class="ltx_Math" alttext="110" display="inline"><semantics id="S4.F1.8.m1.1b"><mn id="S4.F1.8.m1.1.1" xref="S4.F1.8.m1.1.1.cmml">110</mn><annotation-xml encoding="MathML-Content" id="S4.F1.8.m1.1c"><cn type="integer" id="S4.F1.8.m1.1.1.cmml" xref="S4.F1.8.m1.1.1">110</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.8.m1.1d">110</annotation></semantics></math>K and <math id="S4.F1.9.m2.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.F1.9.m2.1b"><mn id="S4.F1.9.m2.1.1" xref="S4.F1.9.m2.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.F1.9.m2.1c"><cn type="integer" id="S4.F1.9.m2.1.1.cmml" xref="S4.F1.9.m2.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.9.m2.1d">120</annotation></semantics></math>K checkpoints. The <span id="S4.F1.19.1" class="ltx_text ltx_font_italic">x</span> and <span id="S4.F1.20.2" class="ltx_text ltx_font_italic">y</span> axes show the <span id="S4.F1.21.3" class="ltx_text ltx_font_italic">norm</span> and the <span id="S4.F1.22.4" class="ltx_text ltx_font_italic">number of tensors</span>, respectively. Information related to encoder tensors is visualized in the upper half and the bottom sub-figure consists of decoder tensors’ information. The first blue bar of the encoder sub-figure indicates that around <math id="S4.F1.10.m3.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.F1.10.m3.1b"><mn id="S4.F1.10.m3.1.1" xref="S4.F1.10.m3.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.F1.10.m3.1c"><cn type="integer" id="S4.F1.10.m3.1.1.cmml" xref="S4.F1.10.m3.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.10.m3.1d">40</annotation></semantics></math> tensors in the WMT encoder only changed slightly from the checkpoint <math id="S4.F1.11.m4.1" class="ltx_Math" alttext="110" display="inline"><semantics id="S4.F1.11.m4.1b"><mn id="S4.F1.11.m4.1.1" xref="S4.F1.11.m4.1.1.cmml">110</mn><annotation-xml encoding="MathML-Content" id="S4.F1.11.m4.1c"><cn type="integer" id="S4.F1.11.m4.1.1.cmml" xref="S4.F1.11.m4.1.1">110</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.11.m4.1d">110</annotation></semantics></math>K to <math id="S4.F1.12.m5.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.F1.12.m5.1b"><mn id="S4.F1.12.m5.1.1" xref="S4.F1.12.m5.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.F1.12.m5.1c"><cn type="integer" id="S4.F1.12.m5.1.1.cmml" xref="S4.F1.12.m5.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.12.m5.1d">120</annotation></semantics></math>K as their norm is in the range [0,5], whereas the last bar on the other end of the same sub-figure shows that around <math id="S4.F1.13.m6.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.F1.13.m6.1b"><mn id="S4.F1.13.m6.1.1" xref="S4.F1.13.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.F1.13.m6.1c"><cn type="integer" id="S4.F1.13.m6.1.1.cmml" xref="S4.F1.13.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.13.m6.1d">2</annotation></semantics></math> tensors in the
WMT model changed drastically as the norm of their difference tensors is close to <math id="S4.F1.14.m7.1" class="ltx_Math" alttext="175" display="inline"><semantics id="S4.F1.14.m7.1b"><mn id="S4.F1.14.m7.1.1" xref="S4.F1.14.m7.1.1.cmml">175</mn><annotation-xml encoding="MathML-Content" id="S4.F1.14.m7.1c"><cn type="integer" id="S4.F1.14.m7.1.1.cmml" xref="S4.F1.14.m7.1.1">175</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.14.m7.1d">175</annotation></semantics></math>.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Results from Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, together with our other observations, show that a small subset of tensors diverge substantially (mostly shown in the right half of the figures), but for the rest, there is a heavy concentration around small norms. More interestingly, we realized that this is a pattern that consistently occurs from one round to the next in our FL experiments, namely each tensor either belongs to a set of highly-fluctuating parameters or it only changes marginally and lies in the less active set. The fluctuation threshold can change but tensors almost always stay in their respective clusters between rounds. We used this finding as a basis of our design to improve bandwidth consumption, such that we decided to focus on either highly fluctuating tensors or those in the other cluster and only <span id="S4.p3.1.1" class="ltx_text ltx_font_typewriter">Pull</span> one type of tensors during communication. The strategy is simple but has led to promising results.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.5" class="ltx_p">More formally, a variation of the aforementioned idea can be simply formulated as shown in Equation <a href="#S4.E2" title="In 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.24" class="ltx_Math" alttext="\begin{split}DP_{g}^{c}=\{w_{r}^{t};\lVert w_{r}^{t}-w_{r-1}^{t}\rVert\geq\theta\}\\
\end{split}" display="block"><semantics id="S4.E2.m1.24a"><mtable displaystyle="true" id="S4.E2.m1.24.24.2" xref="S4.E2.m1.23.23.1.cmml"><mtr id="S4.E2.m1.24.24.2a" xref="S4.E2.m1.23.23.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S4.E2.m1.24.24.2b" xref="S4.E2.m1.23.23.1.cmml"><mrow id="S4.E2.m1.24.24.2.23.23.23" xref="S4.E2.m1.23.23.1.cmml"><mrow id="S4.E2.m1.24.24.2.23.23.23.24" xref="S4.E2.m1.23.23.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.24.24.2.23.23.23.24.1" xref="S4.E2.m1.23.23.1.cmml">​</mo><msubsup id="S4.E2.m1.24.24.2.23.23.23.24.2" xref="S4.E2.m1.23.23.1.cmml"><mi id="S4.E2.m1.2.2.2.2.2.2" xref="S4.E2.m1.2.2.2.2.2.2.cmml">P</mi><mi id="S4.E2.m1.3.3.3.3.3.3.1" xref="S4.E2.m1.3.3.3.3.3.3.1.cmml">g</mi><mi id="S4.E2.m1.4.4.4.4.4.4.1" xref="S4.E2.m1.4.4.4.4.4.4.1.cmml">c</mi></msubsup></mrow><mo id="S4.E2.m1.5.5.5.5.5.5" xref="S4.E2.m1.5.5.5.5.5.5.cmml">=</mo><mrow id="S4.E2.m1.24.24.2.23.23.23.23.1" xref="S4.E2.m1.23.23.1.cmml"><mo stretchy="false" id="S4.E2.m1.6.6.6.6.6.6" xref="S4.E2.m1.23.23.1.cmml">{</mo><mrow id="S4.E2.m1.24.24.2.23.23.23.23.1.1" xref="S4.E2.m1.23.23.1.cmml"><mrow id="S4.E2.m1.24.24.2.23.23.23.23.1.1.2.2" xref="S4.E2.m1.23.23.1.cmml"><msubsup id="S4.E2.m1.24.24.2.23.23.23.23.1.1.1.1.1" xref="S4.E2.m1.23.23.1.cmml"><mi id="S4.E2.m1.7.7.7.7.7.7" xref="S4.E2.m1.7.7.7.7.7.7.cmml">w</mi><mi id="S4.E2.m1.8.8.8.8.8.8.1" xref="S4.E2.m1.8.8.8.8.8.8.1.cmml">r</mi><mi id="S4.E2.m1.9.9.9.9.9.9.1" xref="S4.E2.m1.9.9.9.9.9.9.1.cmml">t</mi></msubsup><mo id="S4.E2.m1.10.10.10.10.10.10" xref="S4.E2.m1.23.23.1.cmml">;</mo><mrow id="S4.E2.m1.24.24.2.23.23.23.23.1.1.2.2.2" xref="S4.E2.m1.23.23.1.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S4.E2.m1.11.11.11.11.11.11" xref="S4.E2.m1.23.23.1.cmml">∥</mo><mrow id="S4.E2.m1.24.24.2.23.23.23.23.1.1.2.2.2.1.1" xref="S4.E2.m1.23.23.1.cmml"><msubsup id="S4.E2.m1.24.24.2.23.23.23.23.1.1.2.2.2.1.1.1" xref="S4.E2.m1.23.23.1.cmml"><mi id="S4.E2.m1.12.12.12.12.12.12" xref="S4.E2.m1.12.12.12.12.12.12.cmml">w</mi><mi id="S4.E2.m1.13.13.13.13.13.13.1" xref="S4.E2.m1.13.13.13.13.13.13.1.cmml">r</mi><mi id="S4.E2.m1.14.14.14.14.14.14.1" xref="S4.E2.m1.14.14.14.14.14.14.1.cmml">t</mi></msubsup><mo id="S4.E2.m1.15.15.15.15.15.15" xref="S4.E2.m1.15.15.15.15.15.15.cmml">−</mo><msubsup id="S4.E2.m1.24.24.2.23.23.23.23.1.1.2.2.2.1.1.2" xref="S4.E2.m1.23.23.1.cmml"><mi id="S4.E2.m1.16.16.16.16.16.16" xref="S4.E2.m1.16.16.16.16.16.16.cmml">w</mi><mrow id="S4.E2.m1.17.17.17.17.17.17.1" xref="S4.E2.m1.17.17.17.17.17.17.1.cmml"><mi id="S4.E2.m1.17.17.17.17.17.17.1.2" xref="S4.E2.m1.17.17.17.17.17.17.1.2.cmml">r</mi><mo id="S4.E2.m1.17.17.17.17.17.17.1.1" xref="S4.E2.m1.17.17.17.17.17.17.1.1.cmml">−</mo><mn id="S4.E2.m1.17.17.17.17.17.17.1.3" xref="S4.E2.m1.17.17.17.17.17.17.1.3.cmml">1</mn></mrow><mi id="S4.E2.m1.18.18.18.18.18.18.1" xref="S4.E2.m1.18.18.18.18.18.18.1.cmml">t</mi></msubsup></mrow><mo fence="true" lspace="0em" rspace="0.1389em" id="S4.E2.m1.19.19.19.19.19.19" xref="S4.E2.m1.23.23.1.cmml">∥</mo></mrow></mrow><mo lspace="0.1389em" id="S4.E2.m1.20.20.20.20.20.20" xref="S4.E2.m1.20.20.20.20.20.20.cmml">≥</mo><mi id="S4.E2.m1.21.21.21.21.21.21" xref="S4.E2.m1.21.21.21.21.21.21.cmml">θ</mi></mrow><mo stretchy="false" id="S4.E2.m1.22.22.22.22.22.22" xref="S4.E2.m1.23.23.1.cmml">}</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S4.E2.m1.24b"><apply id="S4.E2.m1.23.23.1.cmml" xref="S4.E2.m1.24.24.2"><eq id="S4.E2.m1.5.5.5.5.5.5.cmml" xref="S4.E2.m1.5.5.5.5.5.5"></eq><apply id="S4.E2.m1.23.23.1.3.cmml" xref="S4.E2.m1.24.24.2"><times id="S4.E2.m1.23.23.1.3.1.cmml" xref="S4.E2.m1.24.24.2"></times><ci id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1">𝐷</ci><apply id="S4.E2.m1.23.23.1.3.3.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.3.3.1.cmml" xref="S4.E2.m1.24.24.2">superscript</csymbol><apply id="S4.E2.m1.23.23.1.3.3.2.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.3.3.2.1.cmml" xref="S4.E2.m1.24.24.2">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.2.2">𝑃</ci><ci id="S4.E2.m1.3.3.3.3.3.3.1.cmml" xref="S4.E2.m1.3.3.3.3.3.3.1">𝑔</ci></apply><ci id="S4.E2.m1.4.4.4.4.4.4.1.cmml" xref="S4.E2.m1.4.4.4.4.4.4.1">𝑐</ci></apply></apply><set id="S4.E2.m1.23.23.1.1.2.cmml" xref="S4.E2.m1.24.24.2"><apply id="S4.E2.m1.23.23.1.1.1.1.cmml" xref="S4.E2.m1.24.24.2"><geq id="S4.E2.m1.20.20.20.20.20.20.cmml" xref="S4.E2.m1.20.20.20.20.20.20"></geq><list id="S4.E2.m1.23.23.1.1.1.1.2.3.cmml" xref="S4.E2.m1.24.24.2"><apply id="S4.E2.m1.23.23.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.24.24.2">superscript</csymbol><apply id="S4.E2.m1.23.23.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.24.24.2">subscript</csymbol><ci id="S4.E2.m1.7.7.7.7.7.7.cmml" xref="S4.E2.m1.7.7.7.7.7.7">𝑤</ci><ci id="S4.E2.m1.8.8.8.8.8.8.1.cmml" xref="S4.E2.m1.8.8.8.8.8.8.1">𝑟</ci></apply><ci id="S4.E2.m1.9.9.9.9.9.9.1.cmml" xref="S4.E2.m1.9.9.9.9.9.9.1">𝑡</ci></apply><apply id="S4.E2.m1.23.23.1.1.1.1.2.2.2.2.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="latexml" id="S4.E2.m1.23.23.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E2.m1.24.24.2">delimited-∥∥</csymbol><apply id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.cmml" xref="S4.E2.m1.24.24.2"><minus id="S4.E2.m1.15.15.15.15.15.15.cmml" xref="S4.E2.m1.15.15.15.15.15.15"></minus><apply id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.2.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.2.1.cmml" xref="S4.E2.m1.24.24.2">superscript</csymbol><apply id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.2.2.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.2.2.1.cmml" xref="S4.E2.m1.24.24.2">subscript</csymbol><ci id="S4.E2.m1.12.12.12.12.12.12.cmml" xref="S4.E2.m1.12.12.12.12.12.12">𝑤</ci><ci id="S4.E2.m1.13.13.13.13.13.13.1.cmml" xref="S4.E2.m1.13.13.13.13.13.13.1">𝑟</ci></apply><ci id="S4.E2.m1.14.14.14.14.14.14.1.cmml" xref="S4.E2.m1.14.14.14.14.14.14.1">𝑡</ci></apply><apply id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.3.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.3.1.cmml" xref="S4.E2.m1.24.24.2">superscript</csymbol><apply id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.3.2.cmml" xref="S4.E2.m1.24.24.2"><csymbol cd="ambiguous" id="S4.E2.m1.23.23.1.1.1.1.2.2.2.1.1.3.2.1.cmml" xref="S4.E2.m1.24.24.2">subscript</csymbol><ci id="S4.E2.m1.16.16.16.16.16.16.cmml" xref="S4.E2.m1.16.16.16.16.16.16">𝑤</ci><apply id="S4.E2.m1.17.17.17.17.17.17.1.cmml" xref="S4.E2.m1.17.17.17.17.17.17.1"><minus id="S4.E2.m1.17.17.17.17.17.17.1.1.cmml" xref="S4.E2.m1.17.17.17.17.17.17.1.1"></minus><ci id="S4.E2.m1.17.17.17.17.17.17.1.2.cmml" xref="S4.E2.m1.17.17.17.17.17.17.1.2">𝑟</ci><cn type="integer" id="S4.E2.m1.17.17.17.17.17.17.1.3.cmml" xref="S4.E2.m1.17.17.17.17.17.17.1.3">1</cn></apply></apply><ci id="S4.E2.m1.18.18.18.18.18.18.1.cmml" xref="S4.E2.m1.18.18.18.18.18.18.1">𝑡</ci></apply></apply></apply></list><ci id="S4.E2.m1.21.21.21.21.21.21.cmml" xref="S4.E2.m1.21.21.21.21.21.21">𝜃</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.24c">\begin{split}DP_{g}^{c}=\{w_{r}^{t};\lVert w_{r}^{t}-w_{r-1}^{t}\rVert\geq\theta\}\\
\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p4.4" class="ltx_p"><math id="S4.p4.1.m1.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">​</mo><msubsup id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml"><mi id="S4.p4.1.m1.1.1.3.2.2" xref="S4.p4.1.m1.1.1.3.2.2.cmml">P</mi><mi id="S4.p4.1.m1.1.1.3.2.3" xref="S4.p4.1.m1.1.1.3.2.3.cmml">g</mi><mi id="S4.p4.1.m1.1.1.3.3" xref="S4.p4.1.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><times id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></times><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">𝐷</ci><apply id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.3.1.cmml" xref="S4.p4.1.m1.1.1.3">superscript</csymbol><apply id="S4.p4.1.m1.1.1.3.2.cmml" xref="S4.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.3.2.1.cmml" xref="S4.p4.1.m1.1.1.3">subscript</csymbol><ci id="S4.p4.1.m1.1.1.3.2.2.cmml" xref="S4.p4.1.m1.1.1.3.2.2">𝑃</ci><ci id="S4.p4.1.m1.1.1.3.2.3.cmml" xref="S4.p4.1.m1.1.1.3.2.3">𝑔</ci></apply><ci id="S4.p4.1.m1.1.1.3.3.cmml" xref="S4.p4.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">DP_{g}^{c}</annotation></semantics></math> in the <span id="S4.p4.4.1" class="ltx_text ltx_font_italic">r</span>-<span id="S4.p4.4.2" class="ltx_text ltx_font_italic">th</span> round consists of all tensors (<math id="S4.p4.2.m2.1" class="ltx_Math" alttext="w^{t}_{r}" display="inline"><semantics id="S4.p4.2.m2.1a"><msubsup id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mi id="S4.p4.2.m2.1.1.2.2" xref="S4.p4.2.m2.1.1.2.2.cmml">w</mi><mi id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml">r</mi><mi id="S4.p4.2.m2.1.1.2.3" xref="S4.p4.2.m2.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1">subscript</csymbol><apply id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.2.1.cmml" xref="S4.p4.2.m2.1.1">superscript</csymbol><ci id="S4.p4.2.m2.1.1.2.2.cmml" xref="S4.p4.2.m2.1.1.2.2">𝑤</ci><ci id="S4.p4.2.m2.1.1.2.3.cmml" xref="S4.p4.2.m2.1.1.2.3">𝑡</ci></apply><ci id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">w^{t}_{r}</annotation></semantics></math>) that deviate from their previous values in the previous round by <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\theta</annotation></semantics></math>. <span id="S4.p4.4.3" class="ltx_text ltx_font_italic">DP</span> stands for <span id="S4.p4.4.4" class="ltx_text ltx_font_italic">Dynamic Pulling</span> and <span id="S4.p4.4.5" class="ltx_text ltx_font_italic">g</span> indicates that the difference norm of candidate tensors should be <span id="S4.p4.4.6" class="ltx_text ltx_font_italic">greater than or equal</span> to the threshold. <span id="S4.p4.4.7" class="ltx_text ltx_font_italic">DP</span> is exclusively computed for each client which is specified with the <span id="S4.p4.4.8" class="ltx_text ltx_font_italic">c</span> superscript. Moreover, the <span id="S4.p4.4.9" class="ltx_text ltx_font_italic">DP</span> sets for decoders and encoders are calculated separately as they vary at different scales. Based on Equation <a href="#S4.E2" title="In 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we do not need to <span id="S4.p4.4.10" class="ltx_text ltx_font_typewriter">Pull</span> all tensors but each client decides what to share with <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S4.p4.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><ci id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">\mathcal{S}</annotation></semantics></math> (only highly-fluctuating tensors in this case). Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> visualizes this concept.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2205.01557/assets/fl-update.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>DP-based communication in our FL setting.</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.10" class="ltx_p">As shown in the figure, <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="C_{1}" display="inline"><semantics id="S4.p5.1.m1.1a"><msub id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">C</mi><mn id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1">subscript</csymbol><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">C_{1}</annotation></semantics></math> computes the difference between tensors from rounds <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.p5.2.m2.1a"><mi id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><ci id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">r</annotation></semantics></math> and <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="r-1" display="inline"><semantics id="S4.p5.3.m3.1a"><mrow id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml"><mi id="S4.p5.3.m3.1.1.2" xref="S4.p5.3.m3.1.1.2.cmml">r</mi><mo id="S4.p5.3.m3.1.1.1" xref="S4.p5.3.m3.1.1.1.cmml">−</mo><mn id="S4.p5.3.m3.1.1.3" xref="S4.p5.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><apply id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1"><minus id="S4.p5.3.m3.1.1.1.cmml" xref="S4.p5.3.m3.1.1.1"></minus><ci id="S4.p5.3.m3.1.1.2.cmml" xref="S4.p5.3.m3.1.1.2">𝑟</ci><cn type="integer" id="S4.p5.3.m3.1.1.3.cmml" xref="S4.p5.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">r-1</annotation></semantics></math> and decides to only share two tensors (vertices in diagrams) that have the highest norms. In this scenario, the communication cost between <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="C_{1}" display="inline"><semantics id="S4.p5.4.m4.1a"><msub id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml"><mi id="S4.p5.4.m4.1.1.2" xref="S4.p5.4.m4.1.1.2.cmml">C</mi><mn id="S4.p5.4.m4.1.1.3" xref="S4.p5.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><apply id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p5.4.m4.1.1.1.cmml" xref="S4.p5.4.m4.1.1">subscript</csymbol><ci id="S4.p5.4.m4.1.1.2.cmml" xref="S4.p5.4.m4.1.1.2">𝐶</ci><cn type="integer" id="S4.p5.4.m4.1.1.3.cmml" xref="S4.p5.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">C_{1}</annotation></semantics></math> and <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S4.p5.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><ci id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">\mathcal{S}</annotation></semantics></math> is approximately reduced by <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="78" display="inline"><semantics id="S4.p5.6.m6.1a"><mn id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml">78</mn><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><cn type="integer" id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1">78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">78</annotation></semantics></math>% for the pulling phase as only <math id="S4.p5.7.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p5.7.m7.1a"><mn id="S4.p5.7.m7.1.1" xref="S4.p5.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p5.7.m7.1b"><cn type="integer" id="S4.p5.7.m7.1.1.cmml" xref="S4.p5.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m7.1c">2</annotation></semantics></math> out of <math id="S4.p5.8.m8.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.p5.8.m8.1a"><mn id="S4.p5.8.m8.1.1" xref="S4.p5.8.m8.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.p5.8.m8.1b"><cn type="integer" id="S4.p5.8.m8.1.1.cmml" xref="S4.p5.8.m8.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.8.m8.1c">9</annotation></semantics></math> tensors (<math id="S4.p5.9.m9.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S4.p5.9.m9.1a"><mn id="S4.p5.9.m9.1.1" xref="S4.p5.9.m9.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S4.p5.9.m9.1b"><cn type="integer" id="S4.p5.9.m9.1.1.cmml" xref="S4.p5.9.m9.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.9.m9.1c">22</annotation></semantics></math>%) are transferred. The exact percentage of the bandwidth saved in this communication protocol directly depends on the client’s architecture and <math id="S4.p5.10.m10.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.p5.10.m10.1a"><mi id="S4.p5.10.m10.1.1" xref="S4.p5.10.m10.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.p5.10.m10.1b"><ci id="S4.p5.10.m10.1.1.cmml" xref="S4.p5.10.m10.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.10.m10.1c">\theta</annotation></semantics></math>, but the figure uses an imaginary scenario to explain the reduction mechanism.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.2" class="ltx_p">The intuition behind <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><msubsup id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml"><mi id="S4.p6.1.m1.1.1.3.2.2" xref="S4.p6.1.m1.1.1.3.2.2.cmml">P</mi><mi id="S4.p6.1.m1.1.1.3.2.3" xref="S4.p6.1.m1.1.1.3.2.3.cmml">g</mi><mi id="S4.p6.1.m1.1.1.3.3" xref="S4.p6.1.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><times id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></times><ci id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">𝐷</ci><apply id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.3.1.cmml" xref="S4.p6.1.m1.1.1.3">superscript</csymbol><apply id="S4.p6.1.m1.1.1.3.2.cmml" xref="S4.p6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.3.2.1.cmml" xref="S4.p6.1.m1.1.1.3">subscript</csymbol><ci id="S4.p6.1.m1.1.1.3.2.2.cmml" xref="S4.p6.1.m1.1.1.3.2.2">𝑃</ci><ci id="S4.p6.1.m1.1.1.3.2.3.cmml" xref="S4.p6.1.m1.1.1.3.2.3">𝑔</ci></apply><ci id="S4.p6.1.m1.1.1.3.3.cmml" xref="S4.p6.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">DP_{g}^{c}</annotation></semantics></math> is that only highly-fluctuating tensors should be involved in the communication process. It assumes that the main <span id="S4.p6.2.1" class="ltx_text ltx_font_italic">adaptation</span> (or <span id="S4.p6.2.2" class="ltx_text ltx_font_italic">out-domain</span>) knowledge lies in those tensors and what each client needs to learn about its external world is only communicated through such active tensors. Therefore, clients only need to exchange them with the server. This is an assumption that might either result in effective communication or conversely hurt the client. Because, if by any chance <span id="S4.p6.2.3" class="ltx_text ltx_font_italic">local</span> (or <span id="S4.p6.2.4" class="ltx_text ltx_font_italic">in-domain</span>) knowledge is stored in such tensors, <math id="S4.p6.2.m2.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S4.p6.2.m2.1a"><mrow id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml"><mi id="S4.p6.2.m2.1.1.2" xref="S4.p6.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p6.2.m2.1.1.1" xref="S4.p6.2.m2.1.1.1.cmml">​</mo><msubsup id="S4.p6.2.m2.1.1.3" xref="S4.p6.2.m2.1.1.3.cmml"><mi id="S4.p6.2.m2.1.1.3.2.2" xref="S4.p6.2.m2.1.1.3.2.2.cmml">P</mi><mi id="S4.p6.2.m2.1.1.3.2.3" xref="S4.p6.2.m2.1.1.3.2.3.cmml">g</mi><mi id="S4.p6.2.m2.1.1.3.3" xref="S4.p6.2.m2.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><apply id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1"><times id="S4.p6.2.m2.1.1.1.cmml" xref="S4.p6.2.m2.1.1.1"></times><ci id="S4.p6.2.m2.1.1.2.cmml" xref="S4.p6.2.m2.1.1.2">𝐷</ci><apply id="S4.p6.2.m2.1.1.3.cmml" xref="S4.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.3.1.cmml" xref="S4.p6.2.m2.1.1.3">superscript</csymbol><apply id="S4.p6.2.m2.1.1.3.2.cmml" xref="S4.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.3.2.1.cmml" xref="S4.p6.2.m2.1.1.3">subscript</csymbol><ci id="S4.p6.2.m2.1.1.3.2.2.cmml" xref="S4.p6.2.m2.1.1.3.2.2">𝑃</ci><ci id="S4.p6.2.m2.1.1.3.2.3.cmml" xref="S4.p6.2.m2.1.1.3.2.3">𝑔</ci></apply><ci id="S4.p6.2.m2.1.1.3.3.cmml" xref="S4.p6.2.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">DP_{g}^{c}</annotation></semantics></math> manipulates the most important parameters and overwrites them with external domains’ information. In other words, there is a possibility that the reason for observing high fluctuations in active tensors is not because they carry the community knowledge but because they are responsible to learn local data, so they have to vary frequently to adjust and learn local data.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.2" class="ltx_p">If this second assumption is correct, modifying the active set can easily delay the convergence of local models, and thus deteriorate their quality. Due to this concern, we propose another alternative, <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><msubsup id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml"><mi id="S4.p7.1.m1.1.1.3.2.2" xref="S4.p7.1.m1.1.1.3.2.2.cmml">P</mi><mi id="S4.p7.1.m1.1.1.3.2.3" xref="S4.p7.1.m1.1.1.3.2.3.cmml">l</mi><mi id="S4.p7.1.m1.1.1.3.3" xref="S4.p7.1.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><times id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1"></times><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">𝐷</ci><apply id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.3.1.cmml" xref="S4.p7.1.m1.1.1.3">superscript</csymbol><apply id="S4.p7.1.m1.1.1.3.2.cmml" xref="S4.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.3.2.1.cmml" xref="S4.p7.1.m1.1.1.3">subscript</csymbol><ci id="S4.p7.1.m1.1.1.3.2.2.cmml" xref="S4.p7.1.m1.1.1.3.2.2">𝑃</ci><ci id="S4.p7.1.m1.1.1.3.2.3.cmml" xref="S4.p7.1.m1.1.1.3.2.3">𝑙</ci></apply><ci id="S4.p7.1.m1.1.1.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">DP_{l}^{c}</annotation></semantics></math>, which relies on less active tensors for communications. The pulling condition for <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S4.p7.2.m2.1a"><mrow id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mi id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.1" xref="S4.p7.2.m2.1.1.1.cmml">​</mo><msubsup id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml"><mi id="S4.p7.2.m2.1.1.3.2.2" xref="S4.p7.2.m2.1.1.3.2.2.cmml">P</mi><mi id="S4.p7.2.m2.1.1.3.2.3" xref="S4.p7.2.m2.1.1.3.2.3.cmml">l</mi><mi id="S4.p7.2.m2.1.1.3.3" xref="S4.p7.2.m2.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><times id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1.1"></times><ci id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">𝐷</ci><apply id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.3.1.cmml" xref="S4.p7.2.m2.1.1.3">superscript</csymbol><apply id="S4.p7.2.m2.1.1.3.2.cmml" xref="S4.p7.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.3.2.1.cmml" xref="S4.p7.2.m2.1.1.3">subscript</csymbol><ci id="S4.p7.2.m2.1.1.3.2.2.cmml" xref="S4.p7.2.m2.1.1.3.2.2">𝑃</ci><ci id="S4.p7.2.m2.1.1.3.2.3.cmml" xref="S4.p7.2.m2.1.1.3.2.3">𝑙</ci></apply><ci id="S4.p7.2.m2.1.1.3.3.cmml" xref="S4.p7.2.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">DP_{l}^{c}</annotation></semantics></math> is as in Equation <a href="#S4.E3" title="In 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\lVert w_{r}^{t}-w_{r-1}^{t}\rVert&lt;\theta" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.2.1.cmml">∥</mo><mrow id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.cmml"><msubsup id="S4.E3.m1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.1.1.1.2.2.2" xref="S4.E3.m1.1.1.1.1.1.2.2.2.cmml">w</mi><mi id="S4.E3.m1.1.1.1.1.1.2.2.3" xref="S4.E3.m1.1.1.1.1.1.2.2.3.cmml">r</mi><mi id="S4.E3.m1.1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.3.2.2" xref="S4.E3.m1.1.1.1.1.1.3.2.2.cmml">w</mi><mrow id="S4.E3.m1.1.1.1.1.1.3.2.3" xref="S4.E3.m1.1.1.1.1.1.3.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.3.2.3.2" xref="S4.E3.m1.1.1.1.1.1.3.2.3.2.cmml">r</mi><mo id="S4.E3.m1.1.1.1.1.1.3.2.3.1" xref="S4.E3.m1.1.1.1.1.1.3.2.3.1.cmml">−</mo><mn id="S4.E3.m1.1.1.1.1.1.3.2.3.3" xref="S4.E3.m1.1.1.1.1.1.3.2.3.3.cmml">1</mn></mrow><mi id="S4.E3.m1.1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.3.3.cmml">t</mi></msubsup></mrow><mo fence="true" lspace="0em" rspace="0.1389em" id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.2.1.cmml">∥</mo></mrow><mo lspace="0.1389em" id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">&lt;</mo><mi id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><lt id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"></lt><apply id="S4.E3.m1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.E3.m1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"><minus id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1"></minus><apply id="S4.E3.m1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2.2.2">𝑤</ci><ci id="S4.E3.m1.1.1.1.1.1.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.2.2.3">𝑟</ci></apply><ci id="S4.E3.m1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S4.E3.m1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.3.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.3.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.3.2.2">𝑤</ci><apply id="S4.E3.m1.1.1.1.1.1.3.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3.2.3"><minus id="S4.E3.m1.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.3.2.3.1"></minus><ci id="S4.E3.m1.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.3.2.3.2">𝑟</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3.2.3.3">1</cn></apply></apply><ci id="S4.E3.m1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><ci id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\lVert w_{r}^{t}-w_{r-1}^{t}\rVert&lt;\theta</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.p7.4" class="ltx_p">which means, unlike the previous case, highly-active tensors are protected from external updates and only modified using local data. The less active tensors are assumed to be the representatives of external domains, so they are shared with the server and co-trained with other tensors. Both types of tensors contribute to the local training process, but this time the less active group is responsible for bringing the community knowledge to the client. In the next section, we compare <math id="S4.p7.3.m1.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S4.p7.3.m1.1a"><mrow id="S4.p7.3.m1.1.1" xref="S4.p7.3.m1.1.1.cmml"><mi id="S4.p7.3.m1.1.1.2" xref="S4.p7.3.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m1.1.1.1" xref="S4.p7.3.m1.1.1.1.cmml">​</mo><msubsup id="S4.p7.3.m1.1.1.3" xref="S4.p7.3.m1.1.1.3.cmml"><mi id="S4.p7.3.m1.1.1.3.2.2" xref="S4.p7.3.m1.1.1.3.2.2.cmml">P</mi><mi id="S4.p7.3.m1.1.1.3.2.3" xref="S4.p7.3.m1.1.1.3.2.3.cmml">l</mi><mi id="S4.p7.3.m1.1.1.3.3" xref="S4.p7.3.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.3.m1.1b"><apply id="S4.p7.3.m1.1.1.cmml" xref="S4.p7.3.m1.1.1"><times id="S4.p7.3.m1.1.1.1.cmml" xref="S4.p7.3.m1.1.1.1"></times><ci id="S4.p7.3.m1.1.1.2.cmml" xref="S4.p7.3.m1.1.1.2">𝐷</ci><apply id="S4.p7.3.m1.1.1.3.cmml" xref="S4.p7.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.3.m1.1.1.3.1.cmml" xref="S4.p7.3.m1.1.1.3">superscript</csymbol><apply id="S4.p7.3.m1.1.1.3.2.cmml" xref="S4.p7.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.3.m1.1.1.3.2.1.cmml" xref="S4.p7.3.m1.1.1.3">subscript</csymbol><ci id="S4.p7.3.m1.1.1.3.2.2.cmml" xref="S4.p7.3.m1.1.1.3.2.2">𝑃</ci><ci id="S4.p7.3.m1.1.1.3.2.3.cmml" xref="S4.p7.3.m1.1.1.3.2.3">𝑙</ci></apply><ci id="S4.p7.3.m1.1.1.3.3.cmml" xref="S4.p7.3.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m1.1c">DP_{l}^{c}</annotation></semantics></math> and <math id="S4.p7.4.m2.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S4.p7.4.m2.1a"><mrow id="S4.p7.4.m2.1.1" xref="S4.p7.4.m2.1.1.cmml"><mi id="S4.p7.4.m2.1.1.2" xref="S4.p7.4.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m2.1.1.1" xref="S4.p7.4.m2.1.1.1.cmml">​</mo><msubsup id="S4.p7.4.m2.1.1.3" xref="S4.p7.4.m2.1.1.3.cmml"><mi id="S4.p7.4.m2.1.1.3.2.2" xref="S4.p7.4.m2.1.1.3.2.2.cmml">P</mi><mi id="S4.p7.4.m2.1.1.3.2.3" xref="S4.p7.4.m2.1.1.3.2.3.cmml">g</mi><mi id="S4.p7.4.m2.1.1.3.3" xref="S4.p7.4.m2.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.4.m2.1b"><apply id="S4.p7.4.m2.1.1.cmml" xref="S4.p7.4.m2.1.1"><times id="S4.p7.4.m2.1.1.1.cmml" xref="S4.p7.4.m2.1.1.1"></times><ci id="S4.p7.4.m2.1.1.2.cmml" xref="S4.p7.4.m2.1.1.2">𝐷</ci><apply id="S4.p7.4.m2.1.1.3.cmml" xref="S4.p7.4.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p7.4.m2.1.1.3.1.cmml" xref="S4.p7.4.m2.1.1.3">superscript</csymbol><apply id="S4.p7.4.m2.1.1.3.2.cmml" xref="S4.p7.4.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p7.4.m2.1.1.3.2.1.cmml" xref="S4.p7.4.m2.1.1.3">subscript</csymbol><ci id="S4.p7.4.m2.1.1.3.2.2.cmml" xref="S4.p7.4.m2.1.1.3.2.2">𝑃</ci><ci id="S4.p7.4.m2.1.1.3.2.3.cmml" xref="S4.p7.4.m2.1.1.3.2.3">𝑔</ci></apply><ci id="S4.p7.4.m2.1.1.3.3.cmml" xref="S4.p7.4.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m2.1c">DP_{g}^{c}</annotation></semantics></math> and show which strategy is more impactful.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Study</h2>

<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Corpus</span></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Source (De)</span></th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S5.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Target (En)</span></th>
<th id="S5.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Sentences</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td"></td>
<th id="S5.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Words</th>
<th id="S5.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Tokens</th>
<th id="S5.T1.1.2.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">Words</th>
<th id="S5.T1.1.2.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">Tokens</th>
<td id="S5.T1.1.2.1.6" class="ltx_td"></td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.3.2.1.1" class="ltx_text ltx_font_italic">WMT</span></td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">119,920,225</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">1,529,872</td>
<td id="S5.T1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">126,731,132</td>
<td id="S5.T1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t">691,150</td>
<td id="S5.T1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t">4,468,841</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.4.3.1.1" class="ltx_text ltx_font_italic">OS</span></td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_left">33,502,036</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_left">339,627</td>
<td id="S5.T1.1.4.3.4" class="ltx_td ltx_align_left">37,373,751</td>
<td id="S5.T1.1.4.3.5" class="ltx_td ltx_align_left">174,670</td>
<td id="S5.T1.1.4.3.6" class="ltx_td ltx_align_left">4,500,000</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.5.4.1.1" class="ltx_text ltx_font_italic">TED</span></td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_left">2,710,904</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_left">99,221</td>
<td id="S5.T1.1.5.4.4" class="ltx_td ltx_align_left">2,861,006</td>
<td id="S5.T1.1.5.4.5" class="ltx_td ltx_align_left">45,364</td>
<td id="S5.T1.1.5.4.6" class="ltx_td ltx_align_left">143,837</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.6.5.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.6.5.1.1" class="ltx_text ltx_font_italic">PHP</span></td>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_left">322,546</td>
<td id="S5.T1.1.6.5.3" class="ltx_td ltx_align_left">13,001</td>
<td id="S5.T1.1.6.5.4" class="ltx_td ltx_align_left">318,788</td>
<td id="S5.T1.1.6.5.5" class="ltx_td ltx_align_left">8,521</td>
<td id="S5.T1.1.6.5.6" class="ltx_td ltx_align_left">39,708</td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<td id="S5.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T1.1.7.6.1.1" class="ltx_text ltx_font_italic">UB</span></td>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_bb">96,355</td>
<td id="S5.T1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_bb">13,515</td>
<td id="S5.T1.1.7.6.4" class="ltx_td ltx_align_left ltx_border_bb">90,839</td>
<td id="S5.T1.1.7.6.5" class="ltx_td ltx_align_left ltx_border_bb">9,234</td>
<td id="S5.T1.1.7.6.6" class="ltx_td ltx_align_left ltx_border_bb">13,246</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Statistics of the training datasets. The second and third columns show the number of all words and unique tokens for the source and target languages, respectively, and the last column is the number of parallel sentences. <span id="S5.T1.3.1" class="ltx_text ltx_font_italic">OS</span> is a large collection, so we randomly selected a subset of it for our experiments. As the statistics show, we have different sets with different sizes from different domains which helps us have a fair and realistic simulation.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Hyper-Parameters and Datasets</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">Since our main baseline is the model proposed in <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, we follow their setting in the interest of fair comparisons. For our translation engines, we use Transformers with their <span id="S5.SS1.p1.2.1" class="ltx_text ltx_font_italic">base</span> configuration <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite>. Encoders and decoders have six layers (each), attention modules have eight heads, word embeddings and internal projection layers are <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn type="integer" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">512</annotation></semantics></math>-dimensional vectors, and the inner layer in the position-wise feed-forward module has <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="2024" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mn id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">2024</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><cn type="integer" id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">2024</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">2024</annotation></semantics></math> dimensions. All other hyper-parameters/variables such as the training algorithm and scheduling are the same as the <span id="S5.SS1.p1.2.2" class="ltx_text ltx_font_italic">base</span> configuration unless they are clearly indicated in the paper. We used four NVIDIA V100 GPUs for all experiments.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, we work on the the German<math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mo stretchy="false" id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\rightarrow</annotation></semantics></math>English direction. To train/test the models, we use five datasets of <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">WMT</span>,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="http://statmt.org/wmt14/translation-task.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://statmt.org/wmt14/translation-task.html</a></span></span></span> <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">OpenSubtitle</span> or <span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">OS</span> <cite class="ltx_cite ltx_citemacro_citep">(Lison and Tiedemann, <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>, <span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">PHP</span>, <span id="S5.SS1.p2.1.5" class="ltx_text ltx_font_italic">Ubuntu</span> or <span id="S5.SS1.p2.1.6" class="ltx_text ltx_font_italic">UB</span>, and <span id="S5.SS1.p2.1.7" class="ltx_text ltx_font_italic">TED</span> <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann, <a href="#bib.bib37" title="" class="ltx_ref">2012</a>)</cite> where all corpora are <span id="S5.SS1.p2.1.8" class="ltx_text ltx_font_italic">normalized</span> and <span id="S5.SS1.p2.1.9" class="ltx_text ltx_font_italic">tokenized</span> with the scripts provided by Moses.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/moses-smt/mosesdecoder" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/moses-smt/mosesdecoder</a></span></span></span> Table <a href="#S5.T1" title="Table 1 ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides the statistics of the training datasets.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">For the test and development sets of the <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">WMT</span> model, we use <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">newstest-14</span> and <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">newstest-13</span>, respectively. For others, we randomly select 4,000 sentences: 2,000 for the test and 2,000 for the development sets.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The same sets used in <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite></span></span></span> We also pre-processed datasets to segment words into sub-words by BPE <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite>. This helps create a shared vocabulary for source and target languages of all models and avoid out-of-vocabulary entries. Our BPE setting extracts <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mn id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><cn type="integer" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">30</annotation></semantics></math>K unique tokens for each of the source and target sides.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.7" class="ltx_p">One critical hyper-parameter in our model is <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mi id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><ci id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">\theta</annotation></semantics></math>. Considering the selection criterion in Equation <a href="#S4.E2" title="In 4 Low-Cost Domain Adaptation in FL ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, a small value of <math id="S5.SS1.p4.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mi id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><ci id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">\theta</annotation></semantics></math> allows the majority of tensors to be transferred and hence leads to a minimal reduction in bandwidth. A very large value is also not plausible as it filters lots of tensors and prevents the client from receiving external knowledge. One solution is to run an exhaustive search to find the best value, which clearly is an expensive process and sometimes impossible in the case of FL. Vacillating between different options to set up <math id="S5.SS1.p4.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S5.SS1.p4.3.m3.1a"><mi id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><ci id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">\theta</annotation></semantics></math> requires the engagement of both client and server and could in fact be more costly than simply pulling all tensors. To cope with this and also make our results easily reproducible, we simply consider the median of the differences for <math id="S5.SS1.p4.4.m4.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S5.SS1.p4.4.m4.1a"><mi id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b"><ci id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">\theta</annotation></semantics></math>, meaning we only transfer <span id="S5.SS1.p4.7.1" class="ltx_text ltx_font_italic">half</span> (<math id="S5.SS1.p4.5.m5.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS1.p4.5.m5.1a"><mrow id="S5.SS1.p4.5.m5.1.1" xref="S5.SS1.p4.5.m5.1.1.cmml"><mn id="S5.SS1.p4.5.m5.1.1.2" xref="S5.SS1.p4.5.m5.1.1.2.cmml">50</mn><mo id="S5.SS1.p4.5.m5.1.1.1" xref="S5.SS1.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.5.m5.1b"><apply id="S5.SS1.p4.5.m5.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.p4.5.m5.1.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p4.5.m5.1.1.2.cmml" xref="S5.SS1.p4.5.m5.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.5.m5.1c">50\%</annotation></semantics></math>) of the tensors and ignore the rest. The selection criterion determines which half. In <math id="S5.SS1.p4.6.m6.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S5.SS1.p4.6.m6.1a"><mrow id="S5.SS1.p4.6.m6.1.1" xref="S5.SS1.p4.6.m6.1.1.cmml"><mi id="S5.SS1.p4.6.m6.1.1.2" xref="S5.SS1.p4.6.m6.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.6.m6.1.1.1" xref="S5.SS1.p4.6.m6.1.1.1.cmml">​</mo><msubsup id="S5.SS1.p4.6.m6.1.1.3" xref="S5.SS1.p4.6.m6.1.1.3.cmml"><mi id="S5.SS1.p4.6.m6.1.1.3.2.2" xref="S5.SS1.p4.6.m6.1.1.3.2.2.cmml">P</mi><mi id="S5.SS1.p4.6.m6.1.1.3.2.3" xref="S5.SS1.p4.6.m6.1.1.3.2.3.cmml">g</mi><mi id="S5.SS1.p4.6.m6.1.1.3.3" xref="S5.SS1.p4.6.m6.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.6.m6.1b"><apply id="S5.SS1.p4.6.m6.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1"><times id="S5.SS1.p4.6.m6.1.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1.1"></times><ci id="S5.SS1.p4.6.m6.1.1.2.cmml" xref="S5.SS1.p4.6.m6.1.1.2">𝐷</ci><apply id="S5.SS1.p4.6.m6.1.1.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.6.m6.1.1.3.1.cmml" xref="S5.SS1.p4.6.m6.1.1.3">superscript</csymbol><apply id="S5.SS1.p4.6.m6.1.1.3.2.cmml" xref="S5.SS1.p4.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.6.m6.1.1.3.2.1.cmml" xref="S5.SS1.p4.6.m6.1.1.3">subscript</csymbol><ci id="S5.SS1.p4.6.m6.1.1.3.2.2.cmml" xref="S5.SS1.p4.6.m6.1.1.3.2.2">𝑃</ci><ci id="S5.SS1.p4.6.m6.1.1.3.2.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3.2.3">𝑔</ci></apply><ci id="S5.SS1.p4.6.m6.1.1.3.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.6.m6.1c">DP_{g}^{c}</annotation></semantics></math>, we consider the active half and in <math id="S5.SS1.p4.7.m7.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS1.p4.7.m7.1a"><mrow id="S5.SS1.p4.7.m7.1.1" xref="S5.SS1.p4.7.m7.1.1.cmml"><mi id="S5.SS1.p4.7.m7.1.1.2" xref="S5.SS1.p4.7.m7.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.7.m7.1.1.1" xref="S5.SS1.p4.7.m7.1.1.1.cmml">​</mo><msubsup id="S5.SS1.p4.7.m7.1.1.3" xref="S5.SS1.p4.7.m7.1.1.3.cmml"><mi id="S5.SS1.p4.7.m7.1.1.3.2.2" xref="S5.SS1.p4.7.m7.1.1.3.2.2.cmml">P</mi><mi id="S5.SS1.p4.7.m7.1.1.3.2.3" xref="S5.SS1.p4.7.m7.1.1.3.2.3.cmml">l</mi><mi id="S5.SS1.p4.7.m7.1.1.3.3" xref="S5.SS1.p4.7.m7.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.7.m7.1b"><apply id="S5.SS1.p4.7.m7.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1"><times id="S5.SS1.p4.7.m7.1.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1.1"></times><ci id="S5.SS1.p4.7.m7.1.1.2.cmml" xref="S5.SS1.p4.7.m7.1.1.2">𝐷</ci><apply id="S5.SS1.p4.7.m7.1.1.3.cmml" xref="S5.SS1.p4.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.7.m7.1.1.3.1.cmml" xref="S5.SS1.p4.7.m7.1.1.3">superscript</csymbol><apply id="S5.SS1.p4.7.m7.1.1.3.2.cmml" xref="S5.SS1.p4.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.7.m7.1.1.3.2.1.cmml" xref="S5.SS1.p4.7.m7.1.1.3">subscript</csymbol><ci id="S5.SS1.p4.7.m7.1.1.3.2.2.cmml" xref="S5.SS1.p4.7.m7.1.1.3.2.2">𝑃</ci><ci id="S5.SS1.p4.7.m7.1.1.3.2.3.cmml" xref="S5.SS1.p4.7.m7.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS1.p4.7.m7.1.1.3.3.cmml" xref="S5.SS1.p4.7.m7.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.7.m7.1c">DP_{l}^{c}</annotation></semantics></math> the less active half is exchanged. Our results show that this simple strategy leads to effective communication without compromising much on quality.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Centralized Model Training</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Our baseline results are summarized in Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Centralized Model Training ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The evaluation metric used in all experiments is BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a href="#bib.bib26" title="" class="ltx_ref">2002</a>)</cite> computed by SacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Post, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/mjpost/sacrebleu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mjpost/sacrebleu</a></span></span></span> As expected, models work accurately on in-domain data but perform poorly on other domains, specially if the domain is significantly different from training samples, e.g. the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">PHP</span> model’s BLEU score is <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">zero</span> when translating <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">WMT</span> test sentences.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.5.1.1" class="ltx_tr">
<th id="S5.T2.5.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.5.1.1.2.1" class="ltx_text ltx_font_italic">WMT</span></th>
<th id="S5.T2.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.5.1.1.3.1" class="ltx_text ltx_font_italic">OS</span></th>
<th id="S5.T2.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.5.1.1.4.1" class="ltx_text ltx_font_italic">TED</span></th>
<th id="S5.T2.5.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.5.1.1.5.1" class="ltx_text ltx_font_italic">PHP</span></th>
<th id="S5.T2.5.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.5.1.1.6.1" class="ltx_text ltx_font_italic">UB</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.5.2.1" class="ltx_tr">
<th id="S5.T2.5.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.5.2.1.1.1" class="ltx_text ltx_font_italic">WMT</span></th>
<td id="S5.T2.5.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.2.1.2.1" class="ltx_text ltx_font_bold">33.66</span></td>
<td id="S5.T2.5.2.1.3" class="ltx_td ltx_align_center ltx_border_t">18.57</td>
<td id="S5.T2.5.2.1.4" class="ltx_td ltx_align_center ltx_border_t">29.22</td>
<td id="S5.T2.5.2.1.5" class="ltx_td ltx_align_center ltx_border_t">8.04</td>
<td id="S5.T2.5.2.1.6" class="ltx_td ltx_align_center ltx_border_t">12.41</td>
</tr>
<tr id="S5.T2.5.3.2" class="ltx_tr">
<th id="S5.T2.5.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.5.3.2.1.1" class="ltx_text ltx_font_italic">OS</span></th>
<td id="S5.T2.5.3.2.2" class="ltx_td ltx_align_center">13.66</td>
<td id="S5.T2.5.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.5.3.2.3.1" class="ltx_text ltx_font_bold">23.58</span></td>
<td id="S5.T2.5.3.2.4" class="ltx_td ltx_align_center">24.22</td>
<td id="S5.T2.5.3.2.5" class="ltx_td ltx_align_center">7.84</td>
<td id="S5.T2.5.3.2.6" class="ltx_td ltx_align_center">13.83</td>
</tr>
<tr id="S5.T2.5.4.3" class="ltx_tr">
<th id="S5.T2.5.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.5.4.3.1.1" class="ltx_text ltx_font_italic">TED</span></th>
<td id="S5.T2.5.4.3.2" class="ltx_td ltx_align_center">12.09</td>
<td id="S5.T2.5.4.3.3" class="ltx_td ltx_align_center">13.59</td>
<td id="S5.T2.5.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.4.3.4.1" class="ltx_text ltx_font_bold">29.32</span></td>
<td id="S5.T2.5.4.3.5" class="ltx_td ltx_align_center">6.67</td>
<td id="S5.T2.5.4.3.6" class="ltx_td ltx_align_center">10.15</td>
</tr>
<tr id="S5.T2.5.5.4" class="ltx_tr">
<th id="S5.T2.5.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.5.5.4.1.1" class="ltx_text ltx_font_italic">PHP</span></th>
<td id="S5.T2.5.5.4.2" class="ltx_td ltx_align_center">0.00</td>
<td id="S5.T2.5.5.4.3" class="ltx_td ltx_align_center">0.26</td>
<td id="S5.T2.5.5.4.4" class="ltx_td ltx_align_center">0.26</td>
<td id="S5.T2.5.5.4.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.5.4.5.1" class="ltx_text ltx_font_bold">34.48</span></td>
<td id="S5.T2.5.5.4.6" class="ltx_td ltx_align_center">0.00</td>
</tr>
<tr id="S5.T2.5.6.5" class="ltx_tr">
<th id="S5.T2.5.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T2.5.6.5.1.1" class="ltx_text ltx_font_italic">UB</span></th>
<td id="S5.T2.5.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">0.28</td>
<td id="S5.T2.5.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">0.78</td>
<td id="S5.T2.5.6.5.4" class="ltx_td ltx_align_center ltx_border_bb">0.75</td>
<td id="S5.T2.5.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">2.30</td>
<td id="S5.T2.5.6.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.5.6.5.6.1" class="ltx_text ltx_font_bold">30.15</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> Baseline results for the De<math id="S5.T2.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T2.3.3.m1.1b"><mo stretchy="false" id="S5.T2.3.3.m1.1.1" xref="S5.T2.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.m1.1c"><ci id="S5.T2.3.3.m1.1.1.cmml" xref="S5.T2.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.m1.1d">\rightarrow</annotation></semantics></math>En direction. Models are trained for <math id="S5.T2.4.4.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S5.T2.4.4.m2.1b"><mn id="S5.T2.4.4.m2.1.1" xref="S5.T2.4.4.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.m2.1c"><cn type="integer" id="S5.T2.4.4.m2.1.1.cmml" xref="S5.T2.4.4.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.m2.1d">100</annotation></semantics></math>K steps. The first column indicates which training set is used to train the engine and other columns show models’ performance on different test sets, e.g. [<span id="S5.T2.10.1" class="ltx_text ltx_font_italic">OS</span>][<span id="S5.T2.11.2" class="ltx_text ltx_font_italic">TED</span>] = 24.22 indicates the BLEU score of a model trained on the <span id="S5.T2.12.3" class="ltx_text ltx_font_italic">OS</span> training set and tested on the <span id="S5.T2.13.4" class="ltx_text ltx_font_italic">TED</span> test set. The best BLEU score for each test set is bold-faced.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.3" class="ltx_p">In order to remedy the poor quality for out-domain data, we train a central model and adapt it to all other domains with two techniques of <span id="S5.SS2.p2.3.1" class="ltx_text ltx_font_italic">data combination</span> and <span id="S5.SS2.p2.3.2" class="ltx_text ltx_font_italic">chained fine-tuning</span>. In <span id="S5.SS2.p2.3.3" class="ltx_text ltx_font_italic">data combination</span>, we simply concatenate all corpora to create a much larger training set. We initialize the central model with <span id="S5.SS2.p2.3.4" class="ltx_text ltx_font_italic">WMT</span> parameters and retrain it for extra <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mn id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><cn type="integer" id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">50</annotation></semantics></math>K steps with the new dataset. In <span id="S5.SS2.p2.3.5" class="ltx_text ltx_font_italic">chained fine-tuning</span>, we do not combine datasets but instead fine-tune the central model sequentially using each domain’s training set for additional <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn type="integer" id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">50</annotation></semantics></math>K steps (<math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mn id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><cn type="integer" id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">10</annotation></semantics></math>K for each), i.e. we start from the <span id="S5.SS2.p2.3.6" class="ltx_text ltx_font_italic">WMT</span> model, then sequentially fine-tune it over <span id="S5.SS2.p2.3.7" class="ltx_text ltx_font_italic">UB</span>, <span id="S5.SS2.p2.3.8" class="ltx_text ltx_font_italic">OS</span>, and other datasets one after another. This sort of fine-tuning could suffer from catastrophic forgetting, so we ran different experiments to figure out the best order of fine-tuning.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">chained fine-tuning</span> strategy could provide relevant baselines for FL experiments. Imagine a scenario where the central model is shipped to a client environment and it is updated there with multiple local datasets. In <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">data combination</span>, we assume that all data is accessible at training time (a fully-observable environment with full access to all domains’ data) whereas <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_italic">chained fine-tuning</span> pictures a more realistic scenario by forcing to update the central model gradually on the client side.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Centralized Model Training ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes results for these two fine-tuning methods. Exposing the central model to other domains’ data yields much better quality. <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">Data combination</span> clearly outperforms and it shows the impact of having direct access to data; the privilege that we do not have in settings such as <span id="S5.SS2.p4.1.2" class="ltx_text ltx_font_italic">chained training</span> and FL.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Technique</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.1.1.2.1.1.1" class="ltx_text ltx_font_italic">WMT</span></span>
</span>
</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.1.1.3.1.1.1" class="ltx_text ltx_font_italic">OS</span></span>
</span>
</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.1.1.4.1.1.1" class="ltx_text ltx_font_italic">TED</span></span>
</span>
</th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.1.1.5.1.1.1" class="ltx_text ltx_font_italic">PHP</span></span>
</span>
</th>
<th id="S5.T3.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.1.1.6.1.1.1" class="ltx_text ltx_font_italic">UB</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.1.2.1.1.1" class="ltx_text ltx_font_italic">chained</span></th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.2.1.1" class="ltx_p" style="width:18.5pt;">18.26</span>
</span>
</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold">23.51</span></span>
</span>
</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.4.1.1" class="ltx_p" style="width:18.5pt;">28.19</span>
</span>
</td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.5.1.1" class="ltx_p" style="width:18.5pt;">16.14</span>
</span>
</td>
<td id="S5.T3.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.6.1.1" class="ltx_p" style="width:18.5pt;">23.05</span>
</span>
</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S5.T3.1.3.2.1.1" class="ltx_text ltx_font_italic">combination</span></th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.3.2.2.1.1.1" class="ltx_text ltx_font_bold">33.50</span></span>
</span>
</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.3.1.1" class="ltx_p" style="width:18.5pt;">21.82</span>
</span>
</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.3.2.4.1.1.1" class="ltx_text ltx_font_bold">31.51</span></span>
</span>
</td>
<td id="S5.T3.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.3.2.5.1.1.1" class="ltx_text ltx_font_bold">37.56</span></span>
</span>
</td>
<td id="S5.T3.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T3.1.3.2.6.1.1.1" class="ltx_text ltx_font_bold">35.61</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span> Domain adaptation results in centralized settings.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Federated Learning Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Models reported in the previous section (specially in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Centralized Model Training ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) are high-quality engines that are trained in a centralized fashion and provide acceptable performance for all domains. Fine-tuning addressed the problem of poor quality for out-domain data, but as discussed previously, centralized fine-tuning and having access to out-domain data might not be always possible. Therefor, in this section we try to train comparable alternatives in an FL setting.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.3" class="ltx_p">Our setting has one server and five clients (one for each dataset). In the interest of fair comparison between the FL and centralized approaches, we initialize all the clients with <span id="S5.SS3.p2.3.1" class="ltx_text ltx_font_italic">WMT</span> parameters. Each client updates its model with local data and shares it with the server in each round. We <span id="S5.SS3.p2.3.2" class="ltx_text ltx_font_typewriter">Pull</span> client updates after <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mn id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><cn type="integer" id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">10</annotation></semantics></math>K steps of fine-tuning for aggregation and repeat this process for <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mn id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><cn type="integer" id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">5</annotation></semantics></math> rounds. In total, each model is fine-tuned for <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mn id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><cn type="integer" id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">50</annotation></semantics></math>K steps which is identical to the setting we used for centralized training. Results for this experiment are summarized in Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Federated Learning Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.6.6.7.1" class="ltx_tr">
<th id="S5.T4.6.6.7.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T4.6.6.7.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T4.6.6.7.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.7.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.6.6.7.1.2.1.1.1" class="ltx_text ltx_font_italic">WMT</span></span>
</span>
</th>
<th id="S5.T4.6.6.7.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T4.6.6.7.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.7.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.6.6.7.1.3.1.1.1" class="ltx_text ltx_font_italic">OS</span></span>
</span>
</th>
<th id="S5.T4.6.6.7.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T4.6.6.7.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.7.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.6.6.7.1.4.1.1.1" class="ltx_text ltx_font_italic">TED</span></span>
</span>
</th>
<th id="S5.T4.6.6.7.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T4.6.6.7.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.7.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.6.6.7.1.5.1.1.1" class="ltx_text ltx_font_italic">PHP</span></span>
</span>
</th>
<th id="S5.T4.6.6.7.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T4.6.6.7.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.7.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.6.6.7.1.6.1.1.1" class="ltx_text ltx_font_italic">UB</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\mathcal{S}</annotation></semantics></math></th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">33.97</span></span>
</span>
</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.3.1.1" class="ltx_p" style="width:18.5pt;">19.17</span>
</span>
</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.4.1.1" class="ltx_p" style="width:18.5pt;">30.8</span>
</span>
</td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.5.1.1" class="ltx_p" style="width:18.5pt;">37.32</span>
</span>
</td>
<td id="S5.T4.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.6.1.1" class="ltx_p" style="width:18.5pt;">47.9</span>
</span>
</td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<th id="S5.T4.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T4.2.2.2.1.1" class="ltx_text ltx_font_italic">WMT<sup id="S5.T4.2.2.2.1.1.1" class="ltx_sup">c</sup></span></th>
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.2.2.2.2.1.1" class="ltx_p" style="width:18.5pt;">32.07</span>
</span>
</td>
<td id="S5.T4.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.2.2.2.3.1.1" class="ltx_p" style="width:18.5pt;">18.28</span>
</span>
</td>
<td id="S5.T4.2.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.2.2.2.4.1.1" class="ltx_p" style="width:18.5pt;">29.55</span>
</span>
</td>
<td id="S5.T4.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.2.2.2.5.1.1" class="ltx_p" style="width:18.5pt;">9.55</span>
</span>
</td>
<td id="S5.T4.2.2.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.2.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.2.2.2.6.1.1" class="ltx_p" style="width:18.5pt;">13.75</span>
</span>
</td>
</tr>
<tr id="S5.T4.3.3.3" class="ltx_tr">
<th id="S5.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T4.3.3.3.1.1" class="ltx_text ltx_font_italic">OS<sup id="S5.T4.3.3.3.1.1.1" class="ltx_sup">c</sup></span></th>
<td id="S5.T4.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.3.2.1.1" class="ltx_p" style="width:18.5pt;">19.05</span>
</span>
</td>
<td id="S5.T4.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.3.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.3.3.3.3.1.1.1" class="ltx_text ltx_font_bold">23.39</span></span>
</span>
</td>
<td id="S5.T4.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.3.4.1.1" class="ltx_p" style="width:18.5pt;">27.85</span>
</span>
</td>
<td id="S5.T4.3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.3.5.1.1" class="ltx_p" style="width:18.5pt;">13.57</span>
</span>
</td>
<td id="S5.T4.3.3.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.3.6.1.1" class="ltx_p" style="width:18.5pt;">18.58</span>
</span>
</td>
</tr>
<tr id="S5.T4.4.4.4" class="ltx_tr">
<th id="S5.T4.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T4.4.4.4.1.1" class="ltx_text ltx_font_italic">TED<sup id="S5.T4.4.4.4.1.1.1" class="ltx_sup">c</sup></span></th>
<td id="S5.T4.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.4.4.4.2.1.1" class="ltx_p" style="width:18.5pt;">17.37</span>
</span>
</td>
<td id="S5.T4.4.4.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.4.4.4.3.1.1" class="ltx_p" style="width:18.5pt;">16.05</span>
</span>
</td>
<td id="S5.T4.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.4.4.4.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.4.4.4.4.1.1.1" class="ltx_text ltx_font_bold">34.30</span></span>
</span>
</td>
<td id="S5.T4.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.4.4.4.5.1.1" class="ltx_p" style="width:18.5pt;">11.83</span>
</span>
</td>
<td id="S5.T4.4.4.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.4.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.4.4.4.6.1.1" class="ltx_p" style="width:18.5pt;">17.05</span>
</span>
</td>
</tr>
<tr id="S5.T4.5.5.5" class="ltx_tr">
<th id="S5.T4.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T4.5.5.5.1.1" class="ltx_text ltx_font_italic">PHP<sup id="S5.T4.5.5.5.1.1.1" class="ltx_sup">c</sup></span></th>
<td id="S5.T4.5.5.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.5.5.5.2.1.1" class="ltx_p" style="width:18.5pt;">4.07</span>
</span>
</td>
<td id="S5.T4.5.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.5.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.5.5.5.3.1.1" class="ltx_p" style="width:18.5pt;">4.33</span>
</span>
</td>
<td id="S5.T4.5.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.5.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.5.5.5.4.1.1" class="ltx_p" style="width:18.5pt;">7.48</span>
</span>
</td>
<td id="S5.T4.5.5.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.5.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.5.5.5.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.5.5.5.5.1.1.1" class="ltx_text ltx_font_bold">45.07</span></span>
</span>
</td>
<td id="S5.T4.5.5.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.5.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.5.5.5.6.1.1" class="ltx_p" style="width:18.5pt;">10.90</span>
</span>
</td>
</tr>
<tr id="S5.T4.6.6.6" class="ltx_tr">
<th id="S5.T4.6.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S5.T4.6.6.6.1.1" class="ltx_text ltx_font_italic">UB<sup id="S5.T4.6.6.6.1.1.1" class="ltx_sup">c</sup></span></th>
<td id="S5.T4.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T4.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.6.2.1.1" class="ltx_p" style="width:18.5pt;">0.77</span>
</span>
</td>
<td id="S5.T4.6.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T4.6.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.6.3.1.1" class="ltx_p" style="width:18.5pt;">4.27</span>
</span>
</td>
<td id="S5.T4.6.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T4.6.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.6.4.1.1" class="ltx_p" style="width:18.5pt;">5.66</span>
</span>
</td>
<td id="S5.T4.6.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T4.6.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.6.5.1.1" class="ltx_p" style="width:18.5pt;">14.98</span>
</span>
</td>
<td id="S5.T4.6.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T4.6.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.6.6.6.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T4.6.6.6.6.1.1.1" class="ltx_text ltx_font_bold">49.51</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span> FL results with <math id="S5.T4.12.12.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.T4.12.12.m1.1b"><mn id="S5.T4.12.12.m1.1.1" xref="S5.T4.12.12.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.T4.12.12.m1.1c"><cn type="integer" id="S5.T4.12.12.m1.1.1.cmml" xref="S5.T4.12.12.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.12.12.m1.1d">1</annotation></semantics></math> server (<math id="S5.T4.13.13.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S5.T4.13.13.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S5.T4.13.13.m2.1.1" xref="S5.T4.13.13.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S5.T4.13.13.m2.1c"><ci id="S5.T4.13.13.m2.1.1.cmml" xref="S5.T4.13.13.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.13.13.m2.1d">\mathcal{S}</annotation></semantics></math>) and <math id="S5.T4.14.14.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S5.T4.14.14.m3.1b"><mn id="S5.T4.14.14.m3.1.1" xref="S5.T4.14.14.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.T4.14.14.m3.1c"><cn type="integer" id="S5.T4.14.14.m3.1.1.cmml" xref="S5.T4.14.14.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.14.14.m3.1d">5</annotation></semantics></math> clients (indicated with the <math id="S5.T4.15.15.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S5.T4.15.15.m4.1b"><mi id="S5.T4.15.15.m4.1.1" xref="S5.T4.15.15.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.T4.15.15.m4.1c"><ci id="S5.T4.15.15.m4.1.1.cmml" xref="S5.T4.15.15.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.15.15.m4.1d">c</annotation></semantics></math> superscript), e.g. <span id="S5.T4.16.16.1" class="ltx_text ltx_font_italic">OS<sup id="S5.T4.16.16.1.1" class="ltx_sup">c</sup></span> is a client initialized with WMT parameters and updated with its own data (<span id="S5.T4.18.2" class="ltx_text ltx_font_italic">OS</span> training samples).</figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The first row in Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Federated Learning Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> belongs to the server. FL affects model training quite positively and provides significantly better BLEU scores, specially for those low-performing models such as <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_italic">PHP</span>. The average BLEU score of the server over different domains is <math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="33.83" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mn id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">33.83</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><cn type="float" id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">33.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">33.83</annotation></semantics></math>, which is <span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_bold">1.83</span> points higher than that of the best model reported in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Centralized Model Training ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This means, even though FL does not access clients’ data, it is more impactful in fusing information and training mixed-domain engines. This outcome for a complex task such as NMT was unexpected.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.5" class="ltx_p">After the final FL round, the server parameters are pushed back to clients so they can also benefit from the result of aggregation. At this point, each client <span id="S5.SS3.p4.5.2" class="ltx_text ltx_font_italic">can</span> decide to run another phase of fine-tuning with local data over the server parameters. This is a trade-off between being domain specific and remaining generic. Results for this process are listed from the second to last rows in Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Federated Learning Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, e.g. <span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_italic">PHP<sup id="S5.SS3.p4.1.1.1" class="ltx_sup">c</sup></span> after the last <span id="S5.SS3.p4.5.3" class="ltx_text ltx_font_typewriter">Push</span> step has access to all server parameters so its BLEU score on in-domain data is <math id="S5.SS3.p4.2.m1.1" class="ltx_Math" alttext="37.32" display="inline"><semantics id="S5.SS3.p4.2.m1.1a"><mn id="S5.SS3.p4.2.m1.1.1" xref="S5.SS3.p4.2.m1.1.1.cmml">37.32</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m1.1b"><cn type="float" id="S5.SS3.p4.2.m1.1.1.cmml" xref="S5.SS3.p4.2.m1.1.1">37.32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m1.1c">37.32</annotation></semantics></math>, similar to that of the server. It is also able to translate other domains with the average BLEU score of <math id="S5.SS3.p4.3.m2.1" class="ltx_Math" alttext="32.96" display="inline"><semantics id="S5.SS3.p4.3.m2.1a"><mn id="S5.SS3.p4.3.m2.1.1" xref="S5.SS3.p4.3.m2.1.1.cmml">32.96</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m2.1b"><cn type="float" id="S5.SS3.p4.3.m2.1.1.cmml" xref="S5.SS3.p4.3.m2.1.1">32.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m2.1c">32.96</annotation></semantics></math>. However, as the fifth row shows, if it decides to fine-tune the last parameter set it received from the server with its own local data, its BLEU score increases from <math id="S5.SS3.p4.4.m3.1" class="ltx_Math" alttext="37.32" display="inline"><semantics id="S5.SS3.p4.4.m3.1a"><mn id="S5.SS3.p4.4.m3.1.1" xref="S5.SS3.p4.4.m3.1.1.cmml">37.32</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m3.1b"><cn type="float" id="S5.SS3.p4.4.m3.1.1.cmml" xref="S5.SS3.p4.4.m3.1.1">37.32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m3.1c">37.32</annotation></semantics></math> to <math id="S5.SS3.p4.5.m4.1" class="ltx_Math" alttext="45.07" display="inline"><semantics id="S5.SS3.p4.5.m4.1a"><mn id="S5.SS3.p4.5.m4.1.1" xref="S5.SS3.p4.5.m4.1.1.cmml">45.07</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m4.1b"><cn type="float" id="S5.SS3.p4.5.m4.1.1.cmml" xref="S5.SS3.p4.5.m4.1.1">45.07</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m4.1c">45.07</annotation></semantics></math> on the in-domain test set, but at the same time it loses its generalization over other domains.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">We also ran an ablation study to evaluate how the number of FL rounds impact model quality. Two important factors in FL settings are the number of clients and training rounds. We can pass over the first one as we have a cross-silo setting with a limited number of clients, but Table <a href="#S5.T5" title="Table 5 ‣ 5.3 Federated Learning Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Figure <a href="#S5.F3" title="Figure 3 ‣ 5.3 Federated Learning Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provide additional information on the second hyper-parameter.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.3.3.4.1" class="ltx_tr">
<th id="S5.T5.3.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T5.3.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.3.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.4.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.4.1.2.1.1.1" class="ltx_text ltx_font_italic">WMT</span></span>
</span>
</th>
<th id="S5.T5.3.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.3.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.4.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.4.1.3.1.1.1" class="ltx_text ltx_font_italic">OS</span></span>
</span>
</th>
<th id="S5.T5.3.3.4.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.3.3.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.4.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.4.1.4.1.1.1" class="ltx_text ltx_font_italic">TED</span></span>
</span>
</th>
<th id="S5.T5.3.3.4.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.3.3.4.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.4.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.4.1.5.1.1.1" class="ltx_text ltx_font_italic">PHP</span></span>
</span>
</th>
<th id="S5.T5.3.3.4.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.3.3.4.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.4.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.4.1.6.1.1.1" class="ltx_text ltx_font_italic">UB</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}_{5}" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><msub id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.1.1.1.1.m1.1.1.2" xref="S5.T5.1.1.1.1.m1.1.1.2.cmml">𝒮</mi><mn id="S5.T5.1.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.1.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.1.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T5.1.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S5.T5.1.1.1.1.m1.1.1.3.cmml" xref="S5.T5.1.1.1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\mathcal{S}_{5}</annotation></semantics></math></th>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">33.97</span></span>
</span>
</td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T5.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.1.3.1.1" class="ltx_p" style="width:18.5pt;">19.17</span>
</span>
</td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T5.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.1.4.1.1" class="ltx_p" style="width:18.5pt;">30.8</span>
</span>
</td>
<td id="S5.T5.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T5.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.1.5.1.1" class="ltx_p" style="width:18.5pt;">37.32</span>
</span>
</td>
<td id="S5.T5.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T5.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.1.6.1.1" class="ltx_p" style="width:18.5pt;">47.9</span>
</span>
</td>
</tr>
<tr id="S5.T5.2.2.2" class="ltx_tr">
<th id="S5.T5.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S5.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="\mathcal{S}_{10}" display="inline"><semantics id="S5.T5.2.2.2.1.m1.1a"><msub id="S5.T5.2.2.2.1.m1.1.1" xref="S5.T5.2.2.2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.2.2.2.1.m1.1.1.2" xref="S5.T5.2.2.2.1.m1.1.1.2.cmml">𝒮</mi><mn id="S5.T5.2.2.2.1.m1.1.1.3" xref="S5.T5.2.2.2.1.m1.1.1.3.cmml">10</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.m1.1b"><apply id="S5.T5.2.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.2.2.2.1.m1.1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S5.T5.2.2.2.1.m1.1.1.2.cmml" xref="S5.T5.2.2.2.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S5.T5.2.2.2.1.m1.1.1.3.cmml" xref="S5.T5.2.2.2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.m1.1c">\mathcal{S}_{10}</annotation></semantics></math></th>
<td id="S5.T5.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.2.1.1" class="ltx_p" style="width:18.5pt;">31.90</span>
</span>
</td>
<td id="S5.T5.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.3.1.1" class="ltx_p" style="width:18.5pt;">19.63</span>
</span>
</td>
<td id="S5.T5.2.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.4.1.1" class="ltx_p" style="width:18.5pt;">31.04</span>
</span>
</td>
<td id="S5.T5.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.5.1.1" class="ltx_p" style="width:18.5pt;">38.33</span>
</span>
</td>
<td id="S5.T5.2.2.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.2.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.6.1.1" class="ltx_p" style="width:18.5pt;">48.63</span>
</span>
</td>
</tr>
<tr id="S5.T5.3.3.3" class="ltx_tr">
<th id="S5.T5.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><math id="S5.T5.3.3.3.1.m1.1" class="ltx_Math" alttext="\mathcal{S}_{50}" display="inline"><semantics id="S5.T5.3.3.3.1.m1.1a"><msub id="S5.T5.3.3.3.1.m1.1.1" xref="S5.T5.3.3.3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.3.3.3.1.m1.1.1.2" xref="S5.T5.3.3.3.1.m1.1.1.2.cmml">𝒮</mi><mn id="S5.T5.3.3.3.1.m1.1.1.3" xref="S5.T5.3.3.3.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.1.m1.1b"><apply id="S5.T5.3.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.3.3.3.1.m1.1.1.1.cmml" xref="S5.T5.3.3.3.1.m1.1.1">subscript</csymbol><ci id="S5.T5.3.3.3.1.m1.1.1.2.cmml" xref="S5.T5.3.3.3.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S5.T5.3.3.3.1.m1.1.1.3.cmml" xref="S5.T5.3.3.3.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.1.m1.1c">\mathcal{S}_{50}</annotation></semantics></math></th>
<td id="S5.T5.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T5.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.2.1.1" class="ltx_p" style="width:18.5pt;">31.05</span>
</span>
</td>
<td id="S5.T5.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T5.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.3.3.1.1.1" class="ltx_text ltx_font_bold">20.83</span></span>
</span>
</td>
<td id="S5.T5.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T5.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.3.4.1.1.1" class="ltx_text ltx_font_bold">32.27</span></span>
</span>
</td>
<td id="S5.T5.3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T5.3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.3.5.1.1.1" class="ltx_text ltx_font_bold">43.99</span></span>
</span>
</td>
<td id="S5.T5.3.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T5.3.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T5.3.3.3.6.1.1.1" class="ltx_text ltx_font_bold">51.07</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span> FL results for <span id="S5.T5.16.1" class="ltx_text ltx_font_italic">rounds</span> <math id="S5.T5.9.9.m1.3" class="ltx_Math" alttext="\in\{5,10,50\}" display="inline"><semantics id="S5.T5.9.9.m1.3b"><mrow id="S5.T5.9.9.m1.3.4" xref="S5.T5.9.9.m1.3.4.cmml"><mi id="S5.T5.9.9.m1.3.4.2" xref="S5.T5.9.9.m1.3.4.2.cmml"></mi><mo id="S5.T5.9.9.m1.3.4.1" xref="S5.T5.9.9.m1.3.4.1.cmml">∈</mo><mrow id="S5.T5.9.9.m1.3.4.3.2" xref="S5.T5.9.9.m1.3.4.3.1.cmml"><mo stretchy="false" id="S5.T5.9.9.m1.3.4.3.2.1" xref="S5.T5.9.9.m1.3.4.3.1.cmml">{</mo><mn id="S5.T5.9.9.m1.1.1" xref="S5.T5.9.9.m1.1.1.cmml">5</mn><mo id="S5.T5.9.9.m1.3.4.3.2.2" xref="S5.T5.9.9.m1.3.4.3.1.cmml">,</mo><mn id="S5.T5.9.9.m1.2.2" xref="S5.T5.9.9.m1.2.2.cmml">10</mn><mo id="S5.T5.9.9.m1.3.4.3.2.3" xref="S5.T5.9.9.m1.3.4.3.1.cmml">,</mo><mn id="S5.T5.9.9.m1.3.3" xref="S5.T5.9.9.m1.3.3.cmml">50</mn><mo stretchy="false" id="S5.T5.9.9.m1.3.4.3.2.4" xref="S5.T5.9.9.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.m1.3c"><apply id="S5.T5.9.9.m1.3.4.cmml" xref="S5.T5.9.9.m1.3.4"><in id="S5.T5.9.9.m1.3.4.1.cmml" xref="S5.T5.9.9.m1.3.4.1"></in><csymbol cd="latexml" id="S5.T5.9.9.m1.3.4.2.cmml" xref="S5.T5.9.9.m1.3.4.2">absent</csymbol><set id="S5.T5.9.9.m1.3.4.3.1.cmml" xref="S5.T5.9.9.m1.3.4.3.2"><cn type="integer" id="S5.T5.9.9.m1.1.1.cmml" xref="S5.T5.9.9.m1.1.1">5</cn><cn type="integer" id="S5.T5.9.9.m1.2.2.cmml" xref="S5.T5.9.9.m1.2.2">10</cn><cn type="integer" id="S5.T5.9.9.m1.3.3.cmml" xref="S5.T5.9.9.m1.3.3">50</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.m1.3d">\in\{5,10,50\}</annotation></semantics></math>. <span id="S5.T5.17.2" class="ltx_text ltx_font_italic">rounds</span> = <math id="S5.T5.10.10.m2.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.T5.10.10.m2.1b"><mn id="S5.T5.10.10.m2.1.1" xref="S5.T5.10.10.m2.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.T5.10.10.m2.1c"><cn type="integer" id="S5.T5.10.10.m2.1.1.cmml" xref="S5.T5.10.10.m2.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.10.m2.1d">50</annotation></semantics></math> means aggregation occurs <math id="S5.T5.11.11.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.T5.11.11.m3.1b"><mn id="S5.T5.11.11.m3.1.1" xref="S5.T5.11.11.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.T5.11.11.m3.1c"><cn type="integer" id="S5.T5.11.11.m3.1.1.cmml" xref="S5.T5.11.11.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.11.m3.1d">50</annotation></semantics></math> times between checkpoints <math id="S5.T5.12.12.m4.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S5.T5.12.12.m4.1b"><mn id="S5.T5.12.12.m4.1.1" xref="S5.T5.12.12.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.T5.12.12.m4.1c"><cn type="integer" id="S5.T5.12.12.m4.1.1.cmml" xref="S5.T5.12.12.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.12.m4.1d">100</annotation></semantics></math>K (where FL training starts) and <math id="S5.T5.13.13.m5.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S5.T5.13.13.m5.1b"><mn id="S5.T5.13.13.m5.1.1" xref="S5.T5.13.13.m5.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S5.T5.13.13.m5.1c"><cn type="integer" id="S5.T5.13.13.m5.1.1.cmml" xref="S5.T5.13.13.m5.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.13.m5.1d">150</annotation></semantics></math>K (where FL training ends).</figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2205.01557/assets/fl-rounds.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The learning curve of the <span id="S5.F3.5.1" class="ltx_text ltx_font_italic">UB</span> model for <span id="S5.F3.6.2" class="ltx_text ltx_font_italic">rounds</span> <math id="S5.F3.2.m1.3" class="ltx_Math" alttext="\in\{5,10,50\}" display="inline"><semantics id="S5.F3.2.m1.3b"><mrow id="S5.F3.2.m1.3.4" xref="S5.F3.2.m1.3.4.cmml"><mi id="S5.F3.2.m1.3.4.2" xref="S5.F3.2.m1.3.4.2.cmml"></mi><mo id="S5.F3.2.m1.3.4.1" xref="S5.F3.2.m1.3.4.1.cmml">∈</mo><mrow id="S5.F3.2.m1.3.4.3.2" xref="S5.F3.2.m1.3.4.3.1.cmml"><mo stretchy="false" id="S5.F3.2.m1.3.4.3.2.1" xref="S5.F3.2.m1.3.4.3.1.cmml">{</mo><mn id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml">5</mn><mo id="S5.F3.2.m1.3.4.3.2.2" xref="S5.F3.2.m1.3.4.3.1.cmml">,</mo><mn id="S5.F3.2.m1.2.2" xref="S5.F3.2.m1.2.2.cmml">10</mn><mo id="S5.F3.2.m1.3.4.3.2.3" xref="S5.F3.2.m1.3.4.3.1.cmml">,</mo><mn id="S5.F3.2.m1.3.3" xref="S5.F3.2.m1.3.3.cmml">50</mn><mo stretchy="false" id="S5.F3.2.m1.3.4.3.2.4" xref="S5.F3.2.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.3c"><apply id="S5.F3.2.m1.3.4.cmml" xref="S5.F3.2.m1.3.4"><in id="S5.F3.2.m1.3.4.1.cmml" xref="S5.F3.2.m1.3.4.1"></in><csymbol cd="latexml" id="S5.F3.2.m1.3.4.2.cmml" xref="S5.F3.2.m1.3.4.2">absent</csymbol><set id="S5.F3.2.m1.3.4.3.1.cmml" xref="S5.F3.2.m1.3.4.3.2"><cn type="integer" id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1">5</cn><cn type="integer" id="S5.F3.2.m1.2.2.cmml" xref="S5.F3.2.m1.2.2">10</cn><cn type="integer" id="S5.F3.2.m1.3.3.cmml" xref="S5.F3.2.m1.3.3">50</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.3d">\in\{5,10,50\}</annotation></semantics></math>.</figcaption>
</figure>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p">Results from the figure/table above show that the number of rounds and model performance increase proportionally in low-quality clients such as <span id="S5.SS3.p6.1.1" class="ltx_text ltx_font_italic">UB</span> or <span id="S5.SS3.p6.1.2" class="ltx_text ltx_font_italic">PHP</span>. This was expected since with a higher number of rounds clients are updated more frequently with rich information from the central server. However, it comes at a price as it increases communication load. It also delays local model training, because in each round the client has to suspend training to read server values and updates its internal model. For other high-quality clients such as <span id="S5.SS3.p6.1.3" class="ltx_text ltx_font_italic">WMT</span>, higher rounds lead to some degradation since they receive external updates from less-accurate peers and have to compensate for their low quality. The choice of the number of rounds is a trade-off between quality and bandwidth.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Dynamic Pulling Results</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We proposed a novel technique for better pulling and mentioned that it is able to reduce the communication load yet maintain model quality. Table <a href="#S5.T6" title="Table 6 ‣ 5.4 Dynamic Pulling Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports related results to support our claims.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.2.2.3.1" class="ltx_tr">
<th id="S5.T6.2.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T6.2.2.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T6.2.2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.3.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.2.2.3.1.2.1.1.1" class="ltx_text ltx_font_italic">WMT</span></span>
</span>
</th>
<th id="S5.T6.2.2.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T6.2.2.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.3.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.2.2.3.1.3.1.1.1" class="ltx_text ltx_font_italic">OS</span></span>
</span>
</th>
<th id="S5.T6.2.2.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T6.2.2.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.3.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.2.2.3.1.4.1.1.1" class="ltx_text ltx_font_italic">TED</span></span>
</span>
</th>
<th id="S5.T6.2.2.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T6.2.2.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.3.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.2.2.3.1.5.1.1.1" class="ltx_text ltx_font_italic">PHP</span></span>
</span>
</th>
<th id="S5.T6.2.2.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T6.2.2.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.3.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.2.2.3.1.6.1.1.1" class="ltx_text ltx_font_italic">UB</span></span>
</span>
</th>
</tr>
<tr id="S5.T6.2.2.4.2" class="ltx_tr">
<th id="S5.T6.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T6.2.2.4.2.1.1" class="ltx_text ltx_font_italic">default</span></th>
<th id="S5.T6.2.2.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S5.T6.2.2.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.4.2.2.1.1" class="ltx_p" style="width:18.5pt;">33.97</span>
</span>
</th>
<th id="S5.T6.2.2.4.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S5.T6.2.2.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.4.2.3.1.1" class="ltx_p" style="width:18.5pt;">19.17</span>
</span>
</th>
<th id="S5.T6.2.2.4.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S5.T6.2.2.4.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.4.2.4.1.1" class="ltx_p" style="width:18.5pt;">30.8</span>
</span>
</th>
<th id="S5.T6.2.2.4.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S5.T6.2.2.4.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.4.2.5.1.1" class="ltx_p" style="width:18.5pt;">37.32</span>
</span>
</th>
<th id="S5.T6.2.2.4.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S5.T6.2.2.4.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.4.2.6.1.1" class="ltx_p" style="width:18.5pt;">47.90</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><math id="S5.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.T6.1.1.1.1.m1.1a"><mrow id="S5.T6.1.1.1.1.m1.1.1" xref="S5.T6.1.1.1.1.m1.1.1.cmml"><mi id="S5.T6.1.1.1.1.m1.1.1.2" xref="S5.T6.1.1.1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.T6.1.1.1.1.m1.1.1.1" xref="S5.T6.1.1.1.1.m1.1.1.1.cmml">​</mo><msubsup id="S5.T6.1.1.1.1.m1.1.1.3" xref="S5.T6.1.1.1.1.m1.1.1.3.cmml"><mi id="S5.T6.1.1.1.1.m1.1.1.3.2.2" xref="S5.T6.1.1.1.1.m1.1.1.3.2.2.cmml">P</mi><mi id="S5.T6.1.1.1.1.m1.1.1.3.2.3" xref="S5.T6.1.1.1.1.m1.1.1.3.2.3.cmml">l</mi><mi id="S5.T6.1.1.1.1.m1.1.1.3.3" xref="S5.T6.1.1.1.1.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><apply id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1"><times id="S5.T6.1.1.1.1.m1.1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1.1"></times><ci id="S5.T6.1.1.1.1.m1.1.1.2.cmml" xref="S5.T6.1.1.1.1.m1.1.1.2">𝐷</ci><apply id="S5.T6.1.1.1.1.m1.1.1.3.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T6.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3">superscript</csymbol><apply id="S5.T6.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T6.1.1.1.1.m1.1.1.3.2.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S5.T6.1.1.1.1.m1.1.1.3.2.2.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3.2.2">𝑃</ci><ci id="S5.T6.1.1.1.1.m1.1.1.3.2.3.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3.2.3">𝑙</ci></apply><ci id="S5.T6.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">DP_{l}^{c}</annotation></semantics></math></th>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.2.1.1" class="ltx_p" style="width:18.5pt;">29.28</span>
</span>
</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">19.17</span></span>
</span>
</td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">30.88</span></span>
</span>
</td>
<td id="S5.T6.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">36.33</span></span>
</span>
</td>
<td id="S5.T6.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">45.74</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.2.2.2" class="ltx_tr">
<th id="S5.T6.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S5.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S5.T6.2.2.2.1.m1.1a"><mrow id="S5.T6.2.2.2.1.m1.1.1" xref="S5.T6.2.2.2.1.m1.1.1.cmml"><mi id="S5.T6.2.2.2.1.m1.1.1.2" xref="S5.T6.2.2.2.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.T6.2.2.2.1.m1.1.1.1" xref="S5.T6.2.2.2.1.m1.1.1.1.cmml">​</mo><msubsup id="S5.T6.2.2.2.1.m1.1.1.3" xref="S5.T6.2.2.2.1.m1.1.1.3.cmml"><mi id="S5.T6.2.2.2.1.m1.1.1.3.2.2" xref="S5.T6.2.2.2.1.m1.1.1.3.2.2.cmml">P</mi><mi id="S5.T6.2.2.2.1.m1.1.1.3.2.3" xref="S5.T6.2.2.2.1.m1.1.1.3.2.3.cmml">g</mi><mi id="S5.T6.2.2.2.1.m1.1.1.3.3" xref="S5.T6.2.2.2.1.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.1.m1.1b"><apply id="S5.T6.2.2.2.1.m1.1.1.cmml" xref="S5.T6.2.2.2.1.m1.1.1"><times id="S5.T6.2.2.2.1.m1.1.1.1.cmml" xref="S5.T6.2.2.2.1.m1.1.1.1"></times><ci id="S5.T6.2.2.2.1.m1.1.1.2.cmml" xref="S5.T6.2.2.2.1.m1.1.1.2">𝐷</ci><apply id="S5.T6.2.2.2.1.m1.1.1.3.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T6.2.2.2.1.m1.1.1.3.1.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3">superscript</csymbol><apply id="S5.T6.2.2.2.1.m1.1.1.3.2.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T6.2.2.2.1.m1.1.1.3.2.1.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3">subscript</csymbol><ci id="S5.T6.2.2.2.1.m1.1.1.3.2.2.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3.2.2">𝑃</ci><ci id="S5.T6.2.2.2.1.m1.1.1.3.2.3.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3.2.3">𝑔</ci></apply><ci id="S5.T6.2.2.2.1.m1.1.1.3.3.cmml" xref="S5.T6.2.2.2.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.1.m1.1c">DP_{g}^{c}</annotation></semantics></math></th>
<td id="S5.T6.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T6.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.2.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T6.2.2.2.2.1.1.1" class="ltx_text ltx_font_bold">30.74</span></span>
</span>
</td>
<td id="S5.T6.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T6.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.2.3.1.1" class="ltx_p" style="width:18.5pt;">18.28</span>
</span>
</td>
<td id="S5.T6.2.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T6.2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.2.4.1.1" class="ltx_p" style="width:18.5pt;">27.61</span>
</span>
</td>
<td id="S5.T6.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T6.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.2.5.1.1" class="ltx_p" style="width:18.5pt;">13.69</span>
</span>
</td>
<td id="S5.T6.2.2.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T6.2.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.2.6.1.1" class="ltx_p" style="width:18.5pt;">32.16</span>
</span>
</td>
</tr>
<tr id="S5.T6.2.2.5.1" class="ltx_tr">
<th id="S5.T6.2.2.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S5.T6.2.2.5.1.1.1" class="ltx_text ltx_font_italic">random</span></th>
<td id="S5.T6.2.2.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T6.2.2.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.5.1.2.1.1" class="ltx_p" style="width:18.5pt;">24.58</span>
</span>
</td>
<td id="S5.T6.2.2.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T6.2.2.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.5.1.3.1.1" class="ltx_p" style="width:18.5pt;">19.16</span>
</span>
</td>
<td id="S5.T6.2.2.5.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T6.2.2.5.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.5.1.4.1.1" class="ltx_p" style="width:18.5pt;">30.57</span>
</span>
</td>
<td id="S5.T6.2.2.5.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T6.2.2.5.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.5.1.5.1.1" class="ltx_p" style="width:18.5pt;">35.33</span>
</span>
</td>
<td id="S5.T6.2.2.5.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T6.2.2.5.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.5.1.6.1.1" class="ltx_p" style="width:18.5pt;">42.50</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span> The impact of different communication techniques on model quality. The first row is copied from Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Federated Learning Results ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for easier comparison. The bold-faced numbers are the best results obtained by DP-based techniques.</figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.7" class="ltx_p">The average BLEU scores for the <span id="S5.SS4.p2.7.1" class="ltx_text ltx_font_italic">default</span>, <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mi id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p2.1.m1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.3.cmml"><mi id="S5.SS4.p2.1.m1.1.1.3.2.2" xref="S5.SS4.p2.1.m1.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p2.1.m1.1.1.3.2.3" xref="S5.SS4.p2.1.m1.1.1.3.2.3.cmml">l</mi><mi id="S5.SS4.p2.1.m1.1.1.3.3" xref="S5.SS4.p2.1.m1.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><times id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1"></times><ci id="S5.SS4.p2.1.m1.1.1.2.cmml" xref="S5.SS4.p2.1.m1.1.1.2">𝐷</ci><apply id="S5.SS4.p2.1.m1.1.1.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.1.m1.1.1.3.1.cmml" xref="S5.SS4.p2.1.m1.1.1.3">superscript</csymbol><apply id="S5.SS4.p2.1.m1.1.1.3.2.cmml" xref="S5.SS4.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.1.m1.1.1.3.2.1.cmml" xref="S5.SS4.p2.1.m1.1.1.3">subscript</csymbol><ci id="S5.SS4.p2.1.m1.1.1.3.2.2.cmml" xref="S5.SS4.p2.1.m1.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p2.1.m1.1.1.3.2.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS4.p2.1.m1.1.1.3.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">DP_{l}^{c}</annotation></semantics></math>, and <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mrow id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml"><mi id="S5.SS4.p2.2.m2.1.1.2" xref="S5.SS4.p2.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p2.2.m2.1.1.1" xref="S5.SS4.p2.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p2.2.m2.1.1.3" xref="S5.SS4.p2.2.m2.1.1.3.cmml"><mi id="S5.SS4.p2.2.m2.1.1.3.2.2" xref="S5.SS4.p2.2.m2.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p2.2.m2.1.1.3.2.3" xref="S5.SS4.p2.2.m2.1.1.3.2.3.cmml">g</mi><mi id="S5.SS4.p2.2.m2.1.1.3.3" xref="S5.SS4.p2.2.m2.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><apply id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1"><times id="S5.SS4.p2.2.m2.1.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1.1"></times><ci id="S5.SS4.p2.2.m2.1.1.2.cmml" xref="S5.SS4.p2.2.m2.1.1.2">𝐷</ci><apply id="S5.SS4.p2.2.m2.1.1.3.cmml" xref="S5.SS4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.2.m2.1.1.3.1.cmml" xref="S5.SS4.p2.2.m2.1.1.3">superscript</csymbol><apply id="S5.SS4.p2.2.m2.1.1.3.2.cmml" xref="S5.SS4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.2.m2.1.1.3.2.1.cmml" xref="S5.SS4.p2.2.m2.1.1.3">subscript</csymbol><ci id="S5.SS4.p2.2.m2.1.1.3.2.2.cmml" xref="S5.SS4.p2.2.m2.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p2.2.m2.1.1.3.2.3.cmml" xref="S5.SS4.p2.2.m2.1.1.3.2.3">𝑔</ci></apply><ci id="S5.SS4.p2.2.m2.1.1.3.3.cmml" xref="S5.SS4.p2.2.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">DP_{g}^{c}</annotation></semantics></math> methods on different domains are <math id="S5.SS4.p2.3.m3.1" class="ltx_Math" alttext="33.83" display="inline"><semantics id="S5.SS4.p2.3.m3.1a"><mn id="S5.SS4.p2.3.m3.1.1" xref="S5.SS4.p2.3.m3.1.1.cmml">33.83</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.3.m3.1b"><cn type="float" id="S5.SS4.p2.3.m3.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1">33.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.3.m3.1c">33.83</annotation></semantics></math>, <math id="S5.SS4.p2.4.m4.1" class="ltx_Math" alttext="32.28" display="inline"><semantics id="S5.SS4.p2.4.m4.1a"><mn id="S5.SS4.p2.4.m4.1.1" xref="S5.SS4.p2.4.m4.1.1.cmml">32.28</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.4.m4.1b"><cn type="float" id="S5.SS4.p2.4.m4.1.1.cmml" xref="S5.SS4.p2.4.m4.1.1">32.28</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.4.m4.1c">32.28</annotation></semantics></math>, and <math id="S5.SS4.p2.5.m5.1" class="ltx_Math" alttext="24.49" display="inline"><semantics id="S5.SS4.p2.5.m5.1a"><mn id="S5.SS4.p2.5.m5.1.1" xref="S5.SS4.p2.5.m5.1.1.cmml">24.49</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.5.m5.1b"><cn type="float" id="S5.SS4.p2.5.m5.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1">24.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.5.m5.1c">24.49</annotation></semantics></math>, respectively. This means the assumption that less active tensors are responsible for domain adaption could be true and highly-fluctuating tensors should only be kept for learning in-domain knowledge. We also provide results from our <span id="S5.SS4.p2.7.2" class="ltx_text ltx_font_italic">random</span> configuration, in which we exchange the same number of parameters as in <math id="S5.SS4.p2.6.m6.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS4.p2.6.m6.1a"><mrow id="S5.SS4.p2.6.m6.1.1" xref="S5.SS4.p2.6.m6.1.1.cmml"><mi id="S5.SS4.p2.6.m6.1.1.2" xref="S5.SS4.p2.6.m6.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p2.6.m6.1.1.1" xref="S5.SS4.p2.6.m6.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p2.6.m6.1.1.3" xref="S5.SS4.p2.6.m6.1.1.3.cmml"><mi id="S5.SS4.p2.6.m6.1.1.3.2.2" xref="S5.SS4.p2.6.m6.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p2.6.m6.1.1.3.2.3" xref="S5.SS4.p2.6.m6.1.1.3.2.3.cmml">l</mi><mi id="S5.SS4.p2.6.m6.1.1.3.3" xref="S5.SS4.p2.6.m6.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.6.m6.1b"><apply id="S5.SS4.p2.6.m6.1.1.cmml" xref="S5.SS4.p2.6.m6.1.1"><times id="S5.SS4.p2.6.m6.1.1.1.cmml" xref="S5.SS4.p2.6.m6.1.1.1"></times><ci id="S5.SS4.p2.6.m6.1.1.2.cmml" xref="S5.SS4.p2.6.m6.1.1.2">𝐷</ci><apply id="S5.SS4.p2.6.m6.1.1.3.cmml" xref="S5.SS4.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.6.m6.1.1.3.1.cmml" xref="S5.SS4.p2.6.m6.1.1.3">superscript</csymbol><apply id="S5.SS4.p2.6.m6.1.1.3.2.cmml" xref="S5.SS4.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.6.m6.1.1.3.2.1.cmml" xref="S5.SS4.p2.6.m6.1.1.3">subscript</csymbol><ci id="S5.SS4.p2.6.m6.1.1.3.2.2.cmml" xref="S5.SS4.p2.6.m6.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p2.6.m6.1.1.3.2.3.cmml" xref="S5.SS4.p2.6.m6.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS4.p2.6.m6.1.1.3.3.cmml" xref="S5.SS4.p2.6.m6.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.6.m6.1c">DP_{l}^{c}</annotation></semantics></math> and <math id="S5.SS4.p2.7.m7.1" class="ltx_Math" alttext="DP_{g}^{c}" display="inline"><semantics id="S5.SS4.p2.7.m7.1a"><mrow id="S5.SS4.p2.7.m7.1.1" xref="S5.SS4.p2.7.m7.1.1.cmml"><mi id="S5.SS4.p2.7.m7.1.1.2" xref="S5.SS4.p2.7.m7.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p2.7.m7.1.1.1" xref="S5.SS4.p2.7.m7.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p2.7.m7.1.1.3" xref="S5.SS4.p2.7.m7.1.1.3.cmml"><mi id="S5.SS4.p2.7.m7.1.1.3.2.2" xref="S5.SS4.p2.7.m7.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p2.7.m7.1.1.3.2.3" xref="S5.SS4.p2.7.m7.1.1.3.2.3.cmml">g</mi><mi id="S5.SS4.p2.7.m7.1.1.3.3" xref="S5.SS4.p2.7.m7.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.7.m7.1b"><apply id="S5.SS4.p2.7.m7.1.1.cmml" xref="S5.SS4.p2.7.m7.1.1"><times id="S5.SS4.p2.7.m7.1.1.1.cmml" xref="S5.SS4.p2.7.m7.1.1.1"></times><ci id="S5.SS4.p2.7.m7.1.1.2.cmml" xref="S5.SS4.p2.7.m7.1.1.2">𝐷</ci><apply id="S5.SS4.p2.7.m7.1.1.3.cmml" xref="S5.SS4.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.7.m7.1.1.3.1.cmml" xref="S5.SS4.p2.7.m7.1.1.3">superscript</csymbol><apply id="S5.SS4.p2.7.m7.1.1.3.2.cmml" xref="S5.SS4.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.7.m7.1.1.3.2.1.cmml" xref="S5.SS4.p2.7.m7.1.1.3">subscript</csymbol><ci id="S5.SS4.p2.7.m7.1.1.3.2.2.cmml" xref="S5.SS4.p2.7.m7.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p2.7.m7.1.1.3.2.3.cmml" xref="S5.SS4.p2.7.m7.1.1.3.2.3">𝑔</ci></apply><ci id="S5.SS4.p2.7.m7.1.1.3.3.cmml" xref="S5.SS4.p2.7.m7.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.7.m7.1c">DP_{g}^{c}</annotation></semantics></math> but those parameters are selected randomly. The comparison between <span id="S5.SS4.p2.7.3" class="ltx_text ltx_font_italic">random</span> and other alternatives shows that the selection criterion directly affects model quality.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.7" class="ltx_p">Although there is a gap by <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="1.55" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mn id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">1.55</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><cn type="float" id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">1.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">1.55</annotation></semantics></math> points between <span id="S5.SS4.p3.7.1" class="ltx_text ltx_font_italic">default</span> and <math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mrow id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml"><mi id="S5.SS4.p3.2.m2.1.1.2" xref="S5.SS4.p3.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p3.2.m2.1.1.1" xref="S5.SS4.p3.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p3.2.m2.1.1.3" xref="S5.SS4.p3.2.m2.1.1.3.cmml"><mi id="S5.SS4.p3.2.m2.1.1.3.2.2" xref="S5.SS4.p3.2.m2.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p3.2.m2.1.1.3.2.3" xref="S5.SS4.p3.2.m2.1.1.3.2.3.cmml">l</mi><mi id="S5.SS4.p3.2.m2.1.1.3.3" xref="S5.SS4.p3.2.m2.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><apply id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1"><times id="S5.SS4.p3.2.m2.1.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1.1"></times><ci id="S5.SS4.p3.2.m2.1.1.2.cmml" xref="S5.SS4.p3.2.m2.1.1.2">𝐷</ci><apply id="S5.SS4.p3.2.m2.1.1.3.cmml" xref="S5.SS4.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.2.m2.1.1.3.1.cmml" xref="S5.SS4.p3.2.m2.1.1.3">superscript</csymbol><apply id="S5.SS4.p3.2.m2.1.1.3.2.cmml" xref="S5.SS4.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.2.m2.1.1.3.2.1.cmml" xref="S5.SS4.p3.2.m2.1.1.3">subscript</csymbol><ci id="S5.SS4.p3.2.m2.1.1.3.2.2.cmml" xref="S5.SS4.p3.2.m2.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p3.2.m2.1.1.3.2.3.cmml" xref="S5.SS4.p3.2.m2.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS4.p3.2.m2.1.1.3.3.cmml" xref="S5.SS4.p3.2.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">DP_{l}^{c}</annotation></semantics></math> (which is meaningful in NMT), <math id="S5.SS4.p3.3.m3.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS4.p3.3.m3.1a"><mrow id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml"><mi id="S5.SS4.p3.3.m3.1.1.2" xref="S5.SS4.p3.3.m3.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p3.3.m3.1.1.1" xref="S5.SS4.p3.3.m3.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p3.3.m3.1.1.3" xref="S5.SS4.p3.3.m3.1.1.3.cmml"><mi id="S5.SS4.p3.3.m3.1.1.3.2.2" xref="S5.SS4.p3.3.m3.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p3.3.m3.1.1.3.2.3" xref="S5.SS4.p3.3.m3.1.1.3.2.3.cmml">l</mi><mi id="S5.SS4.p3.3.m3.1.1.3.3" xref="S5.SS4.p3.3.m3.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.1b"><apply id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1"><times id="S5.SS4.p3.3.m3.1.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1.1"></times><ci id="S5.SS4.p3.3.m3.1.1.2.cmml" xref="S5.SS4.p3.3.m3.1.1.2">𝐷</ci><apply id="S5.SS4.p3.3.m3.1.1.3.cmml" xref="S5.SS4.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.3.m3.1.1.3.1.cmml" xref="S5.SS4.p3.3.m3.1.1.3">superscript</csymbol><apply id="S5.SS4.p3.3.m3.1.1.3.2.cmml" xref="S5.SS4.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.3.m3.1.1.3.2.1.cmml" xref="S5.SS4.p3.3.m3.1.1.3">subscript</csymbol><ci id="S5.SS4.p3.3.m3.1.1.3.2.2.cmml" xref="S5.SS4.p3.3.m3.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p3.3.m3.1.1.3.2.3.cmml" xref="S5.SS4.p3.3.m3.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS4.p3.3.m3.1.1.3.3.cmml" xref="S5.SS4.p3.3.m3.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.1c">DP_{l}^{c}</annotation></semantics></math> could still be a strong candidate when training large models in the context of FL, because the number of parameters exchanged in each pulling step is <span id="S5.SS4.p3.7.2" class="ltx_text ltx_font_italic">45,724,160</span> for <span id="S5.SS4.p3.7.3" class="ltx_text ltx_font_italic">default</span> whereas this number is only <span id="S5.SS4.p3.7.4" class="ltx_text ltx_font_italic">22,863,104</span> (<math id="S5.SS4.p3.4.m4.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS4.p3.4.m4.1a"><mn id="S5.SS4.p3.4.m4.1.1" xref="S5.SS4.p3.4.m4.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.4.m4.1b"><cn type="integer" id="S5.SS4.p3.4.m4.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.4.m4.1c">50</annotation></semantics></math>% less) for <math id="S5.SS4.p3.5.m5.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS4.p3.5.m5.1a"><mrow id="S5.SS4.p3.5.m5.1.1" xref="S5.SS4.p3.5.m5.1.1.cmml"><mi id="S5.SS4.p3.5.m5.1.1.2" xref="S5.SS4.p3.5.m5.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p3.5.m5.1.1.1" xref="S5.SS4.p3.5.m5.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p3.5.m5.1.1.3" xref="S5.SS4.p3.5.m5.1.1.3.cmml"><mi id="S5.SS4.p3.5.m5.1.1.3.2.2" xref="S5.SS4.p3.5.m5.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p3.5.m5.1.1.3.2.3" xref="S5.SS4.p3.5.m5.1.1.3.2.3.cmml">l</mi><mi id="S5.SS4.p3.5.m5.1.1.3.3" xref="S5.SS4.p3.5.m5.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.5.m5.1b"><apply id="S5.SS4.p3.5.m5.1.1.cmml" xref="S5.SS4.p3.5.m5.1.1"><times id="S5.SS4.p3.5.m5.1.1.1.cmml" xref="S5.SS4.p3.5.m5.1.1.1"></times><ci id="S5.SS4.p3.5.m5.1.1.2.cmml" xref="S5.SS4.p3.5.m5.1.1.2">𝐷</ci><apply id="S5.SS4.p3.5.m5.1.1.3.cmml" xref="S5.SS4.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.5.m5.1.1.3.1.cmml" xref="S5.SS4.p3.5.m5.1.1.3">superscript</csymbol><apply id="S5.SS4.p3.5.m5.1.1.3.2.cmml" xref="S5.SS4.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.5.m5.1.1.3.2.1.cmml" xref="S5.SS4.p3.5.m5.1.1.3">subscript</csymbol><ci id="S5.SS4.p3.5.m5.1.1.3.2.2.cmml" xref="S5.SS4.p3.5.m5.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p3.5.m5.1.1.3.2.3.cmml" xref="S5.SS4.p3.5.m5.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS4.p3.5.m5.1.1.3.3.cmml" xref="S5.SS4.p3.5.m5.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.5.m5.1c">DP_{l}^{c}</annotation></semantics></math>. It should also be noted that pulling occurs not once but for multiple rounds and saving <math id="S5.SS4.p3.6.m6.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS4.p3.6.m6.1a"><mn id="S5.SS4.p3.6.m6.1.1" xref="S5.SS4.p3.6.m6.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.6.m6.1b"><cn type="integer" id="S5.SS4.p3.6.m6.1.1.cmml" xref="S5.SS4.p3.6.m6.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.6.m6.1c">50</annotation></semantics></math>% each time is a significant gain. Moreover, <math id="S5.SS4.p3.7.m7.1" class="ltx_Math" alttext="DP_{l}^{c}" display="inline"><semantics id="S5.SS4.p3.7.m7.1a"><mrow id="S5.SS4.p3.7.m7.1.1" xref="S5.SS4.p3.7.m7.1.1.cmml"><mi id="S5.SS4.p3.7.m7.1.1.2" xref="S5.SS4.p3.7.m7.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p3.7.m7.1.1.1" xref="S5.SS4.p3.7.m7.1.1.1.cmml">​</mo><msubsup id="S5.SS4.p3.7.m7.1.1.3" xref="S5.SS4.p3.7.m7.1.1.3.cmml"><mi id="S5.SS4.p3.7.m7.1.1.3.2.2" xref="S5.SS4.p3.7.m7.1.1.3.2.2.cmml">P</mi><mi id="S5.SS4.p3.7.m7.1.1.3.2.3" xref="S5.SS4.p3.7.m7.1.1.3.2.3.cmml">l</mi><mi id="S5.SS4.p3.7.m7.1.1.3.3" xref="S5.SS4.p3.7.m7.1.1.3.3.cmml">c</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.7.m7.1b"><apply id="S5.SS4.p3.7.m7.1.1.cmml" xref="S5.SS4.p3.7.m7.1.1"><times id="S5.SS4.p3.7.m7.1.1.1.cmml" xref="S5.SS4.p3.7.m7.1.1.1"></times><ci id="S5.SS4.p3.7.m7.1.1.2.cmml" xref="S5.SS4.p3.7.m7.1.1.2">𝐷</ci><apply id="S5.SS4.p3.7.m7.1.1.3.cmml" xref="S5.SS4.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.7.m7.1.1.3.1.cmml" xref="S5.SS4.p3.7.m7.1.1.3">superscript</csymbol><apply id="S5.SS4.p3.7.m7.1.1.3.2.cmml" xref="S5.SS4.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p3.7.m7.1.1.3.2.1.cmml" xref="S5.SS4.p3.7.m7.1.1.3">subscript</csymbol><ci id="S5.SS4.p3.7.m7.1.1.3.2.2.cmml" xref="S5.SS4.p3.7.m7.1.1.3.2.2">𝑃</ci><ci id="S5.SS4.p3.7.m7.1.1.3.2.3.cmml" xref="S5.SS4.p3.7.m7.1.1.3.2.3">𝑙</ci></apply><ci id="S5.SS4.p3.7.m7.1.1.3.3.cmml" xref="S5.SS4.p3.7.m7.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.7.m7.1c">DP_{l}^{c}</annotation></semantics></math> still performs on par with <span id="S5.SS4.p3.7.5" class="ltx_text ltx_font_italic">data combination</span> which is a strong but centralized and not private baseline.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Comparison to Controllers</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.3" class="ltx_p">As previously discussed in Section <a href="#S3.SS1" title="3.1 Domain Adaptation ‣ 3 Federated Learning for NLP ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, Controllers <cite class="ltx_cite ltx_citemacro_citep">(Roosta et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> only exchange 4 layers (2 from the encoder and 2 from the decoder in a 12-layer Transformer) with the server in the <span id="S5.SS5.p1.3.1" class="ltx_text ltx_font_typewriter">Push</span> and <span id="S5.SS5.p1.3.2" class="ltx_text ltx_font_typewriter">Pull</span> phases, which means the communication bandwidth they reduce is around <math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="66\%" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mrow id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml"><mn id="S5.SS5.p1.1.m1.1.1.2" xref="S5.SS5.p1.1.m1.1.1.2.cmml">66</mn><mo id="S5.SS5.p1.1.m1.1.1.1" xref="S5.SS5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><apply id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS5.p1.1.m1.1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS5.p1.1.m1.1.1.2.cmml" xref="S5.SS5.p1.1.m1.1.1.2">66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">66\%</annotation></semantics></math> (<math id="S5.SS5.p1.2.m2.1" class="ltx_Math" alttext="\frac{4}{12}\approx 33\%" display="inline"><semantics id="S5.SS5.p1.2.m2.1a"><mrow id="S5.SS5.p1.2.m2.1.1" xref="S5.SS5.p1.2.m2.1.1.cmml"><mfrac id="S5.SS5.p1.2.m2.1.1.2" xref="S5.SS5.p1.2.m2.1.1.2.cmml"><mn id="S5.SS5.p1.2.m2.1.1.2.2" xref="S5.SS5.p1.2.m2.1.1.2.2.cmml">4</mn><mn id="S5.SS5.p1.2.m2.1.1.2.3" xref="S5.SS5.p1.2.m2.1.1.2.3.cmml">12</mn></mfrac><mo id="S5.SS5.p1.2.m2.1.1.1" xref="S5.SS5.p1.2.m2.1.1.1.cmml">≈</mo><mrow id="S5.SS5.p1.2.m2.1.1.3" xref="S5.SS5.p1.2.m2.1.1.3.cmml"><mn id="S5.SS5.p1.2.m2.1.1.3.2" xref="S5.SS5.p1.2.m2.1.1.3.2.cmml">33</mn><mo id="S5.SS5.p1.2.m2.1.1.3.1" xref="S5.SS5.p1.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.2.m2.1b"><apply id="S5.SS5.p1.2.m2.1.1.cmml" xref="S5.SS5.p1.2.m2.1.1"><approx id="S5.SS5.p1.2.m2.1.1.1.cmml" xref="S5.SS5.p1.2.m2.1.1.1"></approx><apply id="S5.SS5.p1.2.m2.1.1.2.cmml" xref="S5.SS5.p1.2.m2.1.1.2"><divide id="S5.SS5.p1.2.m2.1.1.2.1.cmml" xref="S5.SS5.p1.2.m2.1.1.2"></divide><cn type="integer" id="S5.SS5.p1.2.m2.1.1.2.2.cmml" xref="S5.SS5.p1.2.m2.1.1.2.2">4</cn><cn type="integer" id="S5.SS5.p1.2.m2.1.1.2.3.cmml" xref="S5.SS5.p1.2.m2.1.1.2.3">12</cn></apply><apply id="S5.SS5.p1.2.m2.1.1.3.cmml" xref="S5.SS5.p1.2.m2.1.1.3"><csymbol cd="latexml" id="S5.SS5.p1.2.m2.1.1.3.1.cmml" xref="S5.SS5.p1.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS5.p1.2.m2.1.1.3.2.cmml" xref="S5.SS5.p1.2.m2.1.1.3.2">33</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.2.m2.1c">\frac{4}{12}\approx 33\%</annotation></semantics></math> of the layers are only exchanged between the server and clients). In our case, <span id="S5.SS5.p1.3.3" class="ltx_text ltx_font_italic">DP</span> only affects <span id="S5.SS5.p1.3.4" class="ltx_text ltx_font_typewriter">Pull</span> which leads to <math id="S5.SS5.p1.3.m3.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S5.SS5.p1.3.m3.1a"><mn id="S5.SS5.p1.3.m3.1.1" xref="S5.SS5.p1.3.m3.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.3.m3.1b"><cn type="integer" id="S5.SS5.p1.3.m3.1.1.cmml" xref="S5.SS5.p1.3.m3.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.3.m3.1c">25</annotation></semantics></math>% bandwidth reduction in client-to-server exchanges.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span id="footnote6.1" class="ltx_text ltx_font_typewriter">Pull</span> is only <math id="footnote6.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="footnote6.m1.1b"><mrow id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml"><mn id="footnote6.m1.1.1.2" xref="footnote6.m1.1.1.2.cmml">50</mn><mo id="footnote6.m1.1.1.1" xref="footnote6.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><apply id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1"><csymbol cd="latexml" id="footnote6.m1.1.1.1.cmml" xref="footnote6.m1.1.1.1">percent</csymbol><cn type="integer" id="footnote6.m1.1.1.2.cmml" xref="footnote6.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">50\%</annotation></semantics></math> of the communication where we excluding half the parameters using the threshold, so <math id="footnote6.m2.1" class="ltx_Math" alttext="50\%\times 50\%=25\%" display="inline"><semantics id="footnote6.m2.1b"><mrow id="footnote6.m2.1.1" xref="footnote6.m2.1.1.cmml"><mrow id="footnote6.m2.1.1.2" xref="footnote6.m2.1.1.2.cmml"><mrow id="footnote6.m2.1.1.2.2" xref="footnote6.m2.1.1.2.2.cmml"><mn id="footnote6.m2.1.1.2.2.2" xref="footnote6.m2.1.1.2.2.2.cmml">50</mn><mo rspace="0.055em" id="footnote6.m2.1.1.2.2.1" xref="footnote6.m2.1.1.2.2.1.cmml">%</mo></mrow><mo rspace="0.222em" id="footnote6.m2.1.1.2.1" xref="footnote6.m2.1.1.2.1.cmml">×</mo><mrow id="footnote6.m2.1.1.2.3" xref="footnote6.m2.1.1.2.3.cmml"><mn id="footnote6.m2.1.1.2.3.2" xref="footnote6.m2.1.1.2.3.2.cmml">50</mn><mo id="footnote6.m2.1.1.2.3.1" xref="footnote6.m2.1.1.2.3.1.cmml">%</mo></mrow></mrow><mo id="footnote6.m2.1.1.1" xref="footnote6.m2.1.1.1.cmml">=</mo><mrow id="footnote6.m2.1.1.3" xref="footnote6.m2.1.1.3.cmml"><mn id="footnote6.m2.1.1.3.2" xref="footnote6.m2.1.1.3.2.cmml">25</mn><mo id="footnote6.m2.1.1.3.1" xref="footnote6.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m2.1c"><apply id="footnote6.m2.1.1.cmml" xref="footnote6.m2.1.1"><eq id="footnote6.m2.1.1.1.cmml" xref="footnote6.m2.1.1.1"></eq><apply id="footnote6.m2.1.1.2.cmml" xref="footnote6.m2.1.1.2"><times id="footnote6.m2.1.1.2.1.cmml" xref="footnote6.m2.1.1.2.1"></times><apply id="footnote6.m2.1.1.2.2.cmml" xref="footnote6.m2.1.1.2.2"><csymbol cd="latexml" id="footnote6.m2.1.1.2.2.1.cmml" xref="footnote6.m2.1.1.2.2.1">percent</csymbol><cn type="integer" id="footnote6.m2.1.1.2.2.2.cmml" xref="footnote6.m2.1.1.2.2.2">50</cn></apply><apply id="footnote6.m2.1.1.2.3.cmml" xref="footnote6.m2.1.1.2.3"><csymbol cd="latexml" id="footnote6.m2.1.1.2.3.1.cmml" xref="footnote6.m2.1.1.2.3.1">percent</csymbol><cn type="integer" id="footnote6.m2.1.1.2.3.2.cmml" xref="footnote6.m2.1.1.2.3.2">50</cn></apply></apply><apply id="footnote6.m2.1.1.3.cmml" xref="footnote6.m2.1.1.3"><csymbol cd="latexml" id="footnote6.m2.1.1.3.1.cmml" xref="footnote6.m2.1.1.3.1">percent</csymbol><cn type="integer" id="footnote6.m2.1.1.3.2.cmml" xref="footnote6.m2.1.1.3.2">25</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m2.1d">50\%\times 50\%=25\%</annotation></semantics></math>.</span></span></span></p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.2" class="ltx_p">We challenged our model to see if we can also save <math id="S5.SS5.p2.1.m1.1" class="ltx_Math" alttext="66\%" display="inline"><semantics id="S5.SS5.p2.1.m1.1a"><mrow id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml"><mn id="S5.SS5.p2.1.m1.1.1.2" xref="S5.SS5.p2.1.m1.1.1.2.cmml">66</mn><mo id="S5.SS5.p2.1.m1.1.1.1" xref="S5.SS5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><apply id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS5.p2.1.m1.1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS5.p2.1.m1.1.1.2.cmml" xref="S5.SS5.p2.1.m1.1.1.2">66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">66\%</annotation></semantics></math> in bandwidth by sending/receiving the same number of parameters as in Controllers. More precisely, we applied our threshold-based strategy in <span id="S5.SS5.p2.2.1" class="ltx_text ltx_font_italic">both</span> <span id="S5.SS5.p2.2.2" class="ltx_text ltx_font_typewriter">Push</span> and <span id="S5.SS5.p2.2.3" class="ltx_text ltx_font_typewriter">Pull</span> and modified the value of the threshold such that it only accepts <math id="S5.SS5.p2.2.m2.1" class="ltx_Math" alttext="33\%" display="inline"><semantics id="S5.SS5.p2.2.m2.1a"><mrow id="S5.SS5.p2.2.m2.1.1" xref="S5.SS5.p2.2.m2.1.1.cmml"><mn id="S5.SS5.p2.2.m2.1.1.2" xref="S5.SS5.p2.2.m2.1.1.2.cmml">33</mn><mo id="S5.SS5.p2.2.m2.1.1.1" xref="S5.SS5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.2.m2.1b"><apply id="S5.SS5.p2.2.m2.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS5.p2.2.m2.1.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS5.p2.2.m2.1.1.2.cmml" xref="S5.SS5.p2.2.m2.1.1.2">33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.2.m2.1c">33\%</annotation></semantics></math> of the parameters to exchange with the server. Table <a href="#S5.T7" title="Table 7 ‣ 5.5 Comparison to Controllers ‣ 5 Experimental Study ‣ Training Mixed-Domain Translation Models via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes results of this experiment.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<table id="S5.T7.2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.2.2.3.1" class="ltx_tr">
<th id="S5.T7.2.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T7.2.2.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T7.2.2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.3.1.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.3.1.2.1.1.1" class="ltx_text ltx_font_italic">WMT</span></span>
</span>
</th>
<th id="S5.T7.2.2.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T7.2.2.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.3.1.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.3.1.3.1.1.1" class="ltx_text ltx_font_italic">OS</span></span>
</span>
</th>
<th id="S5.T7.2.2.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T7.2.2.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.3.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.3.1.4.1.1.1" class="ltx_text ltx_font_italic">TED</span></span>
</span>
</th>
<th id="S5.T7.2.2.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T7.2.2.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.3.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.3.1.5.1.1.1" class="ltx_text ltx_font_italic">PHP</span></span>
</span>
</th>
<th id="S5.T7.2.2.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T7.2.2.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.3.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.3.1.6.1.1.1" class="ltx_text ltx_font_italic">UB</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">DP<math id="S5.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="{}^{c}_{l}" display="inline"><semantics id="S5.T7.1.1.1.1.m1.1a"><mmultiscripts id="S5.T7.1.1.1.1.m1.1.1" xref="S5.T7.1.1.1.1.m1.1.1.cmml"><mi id="S5.T7.1.1.1.1.m1.1.1.2.2" xref="S5.T7.1.1.1.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S5.T7.1.1.1.1.m1.1.1a" xref="S5.T7.1.1.1.1.m1.1.1.cmml"></mprescripts><mi id="S5.T7.1.1.1.1.m1.1.1.3" xref="S5.T7.1.1.1.1.m1.1.1.3.cmml">l</mi><mrow id="S5.T7.1.1.1.1.m1.1.1b" xref="S5.T7.1.1.1.1.m1.1.1.cmml"></mrow><mrow id="S5.T7.1.1.1.1.m1.1.1c" xref="S5.T7.1.1.1.1.m1.1.1.cmml"></mrow><mi id="S5.T7.1.1.1.1.m1.1.1.2.3" xref="S5.T7.1.1.1.1.m1.1.1.2.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.m1.1b"><apply id="S5.T7.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.1.1.1.1.m1.1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1">subscript</csymbol><apply id="S5.T7.1.1.1.1.m1.1.1.2.cmml" xref="S5.T7.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.1.1.1.1.m1.1.1.2.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S5.T7.1.1.1.1.m1.1.1.2.2.cmml" xref="S5.T7.1.1.1.1.m1.1.1.2.2">absent</csymbol><ci id="S5.T7.1.1.1.1.m1.1.1.2.3.cmml" xref="S5.T7.1.1.1.1.m1.1.1.2.3">𝑐</ci></apply><ci id="S5.T7.1.1.1.1.m1.1.1.3.cmml" xref="S5.T7.1.1.1.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.m1.1c">{}^{c}_{l}</annotation></semantics></math>
</th>
<td id="S5.T7.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T7.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.1.2.1.1" class="ltx_p" style="width:18.5pt;">29.52</span>
</span>
</td>
<td id="S5.T7.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T7.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.1.3.1.1" class="ltx_p" style="width:18.5pt;">19.25</span>
</span>
</td>
<td id="S5.T7.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T7.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.1.4.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">31.53</span></span>
</span>
</td>
<td id="S5.T7.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T7.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.1.5.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">36.19</span></span>
</span>
</td>
<td id="S5.T7.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T7.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.1.6.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">45.40</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.2.2.2" class="ltx_tr">
<th id="S5.T7.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">DP<math id="S5.T7.2.2.2.1.m1.1" class="ltx_Math" alttext="{}^{c}_{g}" display="inline"><semantics id="S5.T7.2.2.2.1.m1.1a"><mmultiscripts id="S5.T7.2.2.2.1.m1.1.1" xref="S5.T7.2.2.2.1.m1.1.1.cmml"><mi id="S5.T7.2.2.2.1.m1.1.1.2.2" xref="S5.T7.2.2.2.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S5.T7.2.2.2.1.m1.1.1a" xref="S5.T7.2.2.2.1.m1.1.1.cmml"></mprescripts><mi id="S5.T7.2.2.2.1.m1.1.1.3" xref="S5.T7.2.2.2.1.m1.1.1.3.cmml">g</mi><mrow id="S5.T7.2.2.2.1.m1.1.1b" xref="S5.T7.2.2.2.1.m1.1.1.cmml"></mrow><mrow id="S5.T7.2.2.2.1.m1.1.1c" xref="S5.T7.2.2.2.1.m1.1.1.cmml"></mrow><mi id="S5.T7.2.2.2.1.m1.1.1.2.3" xref="S5.T7.2.2.2.1.m1.1.1.2.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.1.m1.1b"><apply id="S5.T7.2.2.2.1.m1.1.1.cmml" xref="S5.T7.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.2.2.2.1.m1.1.1.1.cmml" xref="S5.T7.2.2.2.1.m1.1.1">subscript</csymbol><apply id="S5.T7.2.2.2.1.m1.1.1.2.cmml" xref="S5.T7.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.2.2.2.1.m1.1.1.2.1.cmml" xref="S5.T7.2.2.2.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S5.T7.2.2.2.1.m1.1.1.2.2.cmml" xref="S5.T7.2.2.2.1.m1.1.1.2.2">absent</csymbol><ci id="S5.T7.2.2.2.1.m1.1.1.2.3.cmml" xref="S5.T7.2.2.2.1.m1.1.1.2.3">𝑐</ci></apply><ci id="S5.T7.2.2.2.1.m1.1.1.3.cmml" xref="S5.T7.2.2.2.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.1.m1.1c">{}^{c}_{g}</annotation></semantics></math>
</th>
<td id="S5.T7.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.2.2.1.1" class="ltx_p" style="width:18.5pt;">29.09</span>
</span>
</td>
<td id="S5.T7.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.2.3.1.1" class="ltx_p" style="width:18.5pt;">19.07</span>
</span>
</td>
<td id="S5.T7.2.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.2.4.1.1" class="ltx_p" style="width:18.5pt;">30.93</span>
</span>
</td>
<td id="S5.T7.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.2.5.1.1" class="ltx_p" style="width:18.5pt;">34.47</span>
</span>
</td>
<td id="S5.T7.2.2.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.2.6.1.1" class="ltx_p" style="width:18.5pt;">41.57</span>
</span>
</td>
</tr>
<tr id="S5.T7.2.2.4.1" class="ltx_tr">
<th id="S5.T7.2.2.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6E6D (0-3)</th>
<td id="S5.T7.2.2.4.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.4.1.2.1.1" class="ltx_p" style="width:18.5pt;">31.13</span>
</span>
</td>
<td id="S5.T7.2.2.4.1.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.4.1.3.1.1" class="ltx_p" style="width:18.5pt;">19.19</span>
</span>
</td>
<td id="S5.T7.2.2.4.1.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.4.1.4.1.1" class="ltx_p" style="width:18.5pt;">30.95</span>
</span>
</td>
<td id="S5.T7.2.2.4.1.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.4.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.4.1.5.1.1" class="ltx_p" style="width:18.5pt;">33.79</span>
</span>
</td>
<td id="S5.T7.2.2.4.1.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T7.2.2.4.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.4.1.6.1.1" class="ltx_p" style="width:18.5pt;">32.85</span>
</span>
</td>
</tr>
<tr id="S5.T7.2.2.5.2" class="ltx_tr">
<th id="S5.T7.2.2.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">8E8D (0-6)</th>
<td id="S5.T7.2.2.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T7.2.2.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.5.2.2.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.5.2.2.1.1.1" class="ltx_text ltx_font_bold">31.79</span></span>
</span>
</td>
<td id="S5.T7.2.2.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T7.2.2.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.5.2.3.1.1" class="ltx_p" style="width:18.5pt;"><span id="S5.T7.2.2.5.2.3.1.1.1" class="ltx_text ltx_font_bold">20.02</span></span>
</span>
</td>
<td id="S5.T7.2.2.5.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T7.2.2.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.5.2.4.1.1" class="ltx_p" style="width:18.5pt;">30.6</span>
</span>
</td>
<td id="S5.T7.2.2.5.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T7.2.2.5.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.5.2.5.1.1" class="ltx_p" style="width:18.5pt;">32.43</span>
</span>
</td>
<td id="S5.T7.2.2.5.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T7.2.2.5.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.5.2.6.1.1" class="ltx_p" style="width:18.5pt;">33.41</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span> Dynamic parameter selection versus Controllers.</figcaption>
</figure>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">In our comparison, we selected the two best performing Controller models reported in <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>. The <span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_italic">6E6D (0-3)</span> configuration is a Transformer with a 6-layer encoder and 6-layer decoder whose first and fourth layers are selected to act as Controllers. In the <span id="S5.SS5.p3.1.2" class="ltx_text ltx_font_italic">8E8D (0-6)</span> configuration, instead of using the original encoder/decode layers as Controllers four additional layers (two for the encoder and two for the decoder) are defined which are placed after layers 0 and 6. This means, in an 8-layer encoder/decoder the first and seventh layers are Controllers and the rest are ordinary layers.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p">Results show that our model could be a reliable alternative for communication-efficient FL, even though we aggressively limited the number of parameters exchanged in this new setting. Moreover, in our model we do not need to define additional layers. Unlike Controllers, we also do not have to deal with finding the correct position to place Controller layers. According to <cite class="ltx_cite ltx_citemacro_citet">Roosta et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, the final model performance is directly impacted by misplacing Controllers and our solution solves that problem.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we reported a set of benchmarking result for NMT in the context of FL. We also proposed an effective technique to reduce the communication bandwidth. Our solution tries to determine a subset of parameters that are responsible for learning out-domain knowledge and only exchanges them with the server.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In future work, we are interested in <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">i</span>) adding more languages to train multilingual engines, <span id="S6.p2.1.2" class="ltx_text ltx_font_italic">ii</span>) improving communication protocols even further, <span id="S6.p2.1.3" class="ltx_text ltx_font_italic">iii</span>) comparing other FL algorithms in the presence of <span id="S6.p2.1.4" class="ltx_text ltx_font_italic">DP</span>, and <span id="S6.p2.1.5" class="ltx_text ltx_font_italic">iv</span>) investigating NMT in cross-device settings.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to thank our anonymous reviewers for their valuable comments as well as Liling Tan, Grant Strimel, and Sriram Venkatapathy (from Amazon) for providing feedback on the first version of this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2014)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.0473</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna et al. (2019)</span>
<span class="ltx_bibblock">
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock">Simple, scalable adaptation for neural machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08478</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bui et al. (2019)</span>
<span class="ltx_bibblock">
Duc Bui, Kshitiz Malik, Jack Goetz, Honglei Liu, Seungwhan Moon, Anuj Kumar,
and Kang G Shin. 2019.

</span>
<span class="ltx_bibblock">Federated user representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.12535</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays. 2019.

</span>
<span class="ltx_bibblock">Federated learning of out-of-vocabulary words.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.10635</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu and Wang (2018)</span>
<span class="ltx_bibblock">
Chenhui Chu and Rui Wang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/C18-1111" title="" class="ltx_ref ltx_href">A survey of domain
adaptation for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th International Conference on
Computational Linguistics</em>, pages 1304–1319, Santa Fe, New Mexico, USA.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2020)</span>
<span class="ltx_bibblock">
Suyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2020.

</span>
<span class="ltx_bibblock">Fedner: Privacy-preserving medical named entity recognition with
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pages arXiv–2003.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehring et al. (2017)</span>
<span class="ltx_bibblock">
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin.
2017.

</span>
<span class="ltx_bibblock">Convolutional sequence to sequence learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
1243–1252. PMLR.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al. (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.
2020.

</span>
<span class="ltx_bibblock">Inverting gradients–how easy is it to break privacy in federated
learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.14053</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer et al. (2017)</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client level
perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, K. Rao, Rajiv Mathews, F. Beaufays, S. Augenstein, Hubert Eichner,
Chloé Kiddon, and D. Ramage. 2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1811.03604.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardy et al. (2019)</span>
<span class="ltx_bibblock">
Corentin Hardy, Erwan Le Merrer, and Bruno Sericola. 2019.

</span>
<span class="ltx_bibblock">Md-gan: Multi-discriminator generative adversarial networks for
distributed datasets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2019 IEEE international parallel and distributed processing
symposium (IPDPS)</em>, pages 866–877. IEEE.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartmann et al. (2019)</span>
<span class="ltx_bibblock">
Florian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim D Smith, and Ilana
Segall. 2019.

</span>
<span class="ltx_bibblock">Federated learning for ranking browser history suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.11807</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
2790–2799. PMLR.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2020)</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Wenqi Jiang, Anwar Walid, and Xue Li. 2020.

</span>
<span class="ltx_bibblock">Dynamic sampling and selective masking for communication-efficient
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.09603</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2019)</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, and Zi Huang. 2019.

</span>
<span class="ltx_bibblock">Learning private neural language modeling with attentive aggregation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</em>, pages 1–8. IEEE.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2020)</span>
<span class="ltx_bibblock">
Heejae Kim, Taewoo Kim, and Chan-Hyun Youn. 2020.

</span>
<span class="ltx_bibblock">On federated learning of deep networks from non-{iid} data:
Parameter divergence and the effects of hyperparametric methods.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy et al. (2019)</span>
<span class="ltx_bibblock">
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph
Dureau. 2019.

</span>
<span class="ltx_bibblock">Federated learning for keyword spotting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6341–6345. IEEE.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2021.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.02079</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and
Bingsheng He. 2019.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.09693</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MSP.2020.2975749" title="" class="ltx_ref ltx_href">Federated learning:
Challenges, methods, and future directions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37(3):50–60.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2018)</span>
<span class="ltx_bibblock">
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. 2018.

</span>
<span class="ltx_bibblock">Deep gradient compression: Reducing the communication bandwidth for
distributed training.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)</span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock">OpenSubtitles2016: Extracting large parallel corpora from movie
and TV subtitles.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC’16)</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Miller (2020)</span>
<span class="ltx_bibblock">
Dianbo Liu and Tim Miller. 2020.

</span>
<span class="ltx_bibblock">Federated pretraining and fine tuning of bert using clinical notes
from multiple silos.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.08562</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, pages 1273–1282.
PMLR.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association
for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passban et al. (2020)</span>
<span class="ltx_bibblock">
Peyman Passban, Puneeth SM Saladi, and Qun Liu. 2020.

</span>
<span class="ltx_bibblock">Revisiting robust neural machine translation: A transformer case
study.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.15710</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et al. (2018)</span>
<span class="ltx_bibblock">
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer. 2018.

</span>
<span class="ltx_bibblock">Deep contextualized word representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. of NAACL</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2020)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan
Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock">Adapterhub: A framework for adapting transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.07779</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/W18-6319" title="" class="ltx_ref ltx_href">A call for clarity
in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186–191, Belgium, Brussels. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2021)</span>
<span class="ltx_bibblock">
Hanchi Ren, J. Deng, and Xianghua Xie. 2021.

</span>
<span class="ltx_bibblock">Grnn: Generative regression neural network – a data leakage attack
for federated learning.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roosta et al. (2021)</span>
<span class="ltx_bibblock">
Tanya Roosta, Peyman Passban, and Ankit R. Chadha. 2021.

</span>
<span class="ltx_bibblock">Communication-efficient federated learning for neural machine
translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2112.06135.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rücklé et al. (2020)</span>
<span class="ltx_bibblock">
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas
Pfeiffer, Nils Reimers, and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock">Adapterdrop: On the efficiency of adapters in transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11918</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1162" title="" class="ltx_ref ltx_href">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stremmel and Singh (2020)</span>
<span class="ltx_bibblock">
Joel Stremmel and Arjun Singh. 2020.

</span>
<span class="ltx_bibblock">Pretraining federated text models for next word prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pages arXiv–2005.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever et al. (2014)</span>
<span class="ltx_bibblock">
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.

</span>
<span class="ltx_bibblock">Sequence to sequence learning with neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.3215</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European
Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.03762</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang,
and Christopher G Brinton. 2021.

</span>
<span class="ltx_bibblock">Device sampling for heterogeneous federated learning: Theory,
algorithms, and implementation.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00787</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020)</span>
<span class="ltx_bibblock">
Xing Wu, Zhaowang Liang, and Jianjia Wang. 2020.

</span>
<span class="ltx_bibblock">Fedmed: A federated learning framework for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 20(14):4048.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
10(2):1–19.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. 2020.

</span>
<span class="ltx_bibblock">Transfer learning in deep reinforcement learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.07888</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.01556" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.01557" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.01557">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.01557" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.01558" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 14:13:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
