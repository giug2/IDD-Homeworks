<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems</title>
<!--Generated on Thu Aug 29 12:28:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="music recommender systems,  recommender systems,  pretrained audio representations,  hybrid recommender systems" lang="en" name="keywords"/>
<base href="/html/2409.08987v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S1" title="In Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2" title="In Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.SS1" title="In 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pretrained Audio Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.SS2" title="In 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Recommendation models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.SS3" title="In 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.SS4" title="In 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Training Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3" title="In Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.SS1" title="In 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Comparison to MIR results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.SS2" title="In 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.SS2.SSS1" title="In 3.2. Discussion ‣ 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>RQ1: Are pretrained audio representations a viable option for MRS?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.SS2.SSS2" title="In 3.2. Discussion ‣ 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>RQ2: How do different backend models compare in the context of MRS?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.SS2.SSS3" title="In 3.2. Discussion ‣ 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>RQ3: How does pretrained backend model performance in MRS correspond to performance in MIR tasks?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.SS3" title="In 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S4" title="In Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yan-Martin Tamm
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yanmart.tamm@gmail.com">yanmart.tamm@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6174-7736" title="ORCID identifier">0000-0002-6174-7736</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Tartu</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">Narva mnt 18</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Tartu</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">Estonia</span><span class="ltx_text ltx_affiliation_postcode" id="id5.5.id5">51009</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anna Aljanaki
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aljanaki@gmail.com">aljanaki@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-7119-8312" title="ORCID identifier">0000-0002-7119-8312</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">University of Tartu</span><span class="ltx_text ltx_affiliation_streetaddress" id="id7.2.id2">Narva mnt 18</span><span class="ltx_text ltx_affiliation_city" id="id8.3.id3">Tartu</span><span class="ltx_text ltx_affiliation_country" id="id9.4.id4">Estonia</span><span class="ltx_text ltx_affiliation_postcode" id="id10.5.id5">51009</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id11.id1">Over the years, Music Information Retrieval (MIR) has proposed various models pretrained on large amounts of music data. Transfer learning showcases the proven effectiveness of pretrained backend models with a broad spectrum of downstream tasks, including auto-tagging and genre classification. However, MIR papers generally do not explore the efficiency of pretrained models for Music Recommender Systems (MRS). In addition, the Recommender Systems community tends to favour traditional end-to-end neural network learning over these models. Our research addresses this gap and evaluates the applicability of six pretrained backend models (MusicFM, <span class="ltx_text" id="id11.id1.1">Music2Vec</span>, MERT, EncodecMAE, Jukebox, and MusiCNN) in the context of MRS. We assess their performance using three recommendation models: K-nearest neighbours (KNN), shallow neural network, and <span class="ltx_text" id="id11.id1.2">BERT4Rec</span>. Our findings suggest that pretrained audio representations exhibit significant performance variability between traditional MIR tasks and MRS, indicating that valuable aspects of musical information captured by backend models may differ depending on the task. This study establishes a foundation for further exploration of pretrained audio representations to enhance music recommendation systems.</p>
</div>
<div class="ltx_keywords">music recommender systems, recommender systems, pretrained audio representations, hybrid recommender systems
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>18th ACM Conference on Recommender Systems; October 14–18, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>18th ACM Conference on Recommender Systems (RecSys ’24), October 14–18, 2024, Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3688172</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0505-2/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Music retrieval</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Neural networks</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Music Recommender Systems (MRS) are naturally fit for a hybrid recommendation setting because both collaborative interactions and audio data are available, allowing us to gain deeper insight into user preferences. This not only improves performance but also has the potential to address the cold start problem for new items.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In content-aware MRS, Convolutional Neural Networks (CNN) are commonly trained on Mel-Spectrograms extracted from music segments, as popularized by <cite class="ltx_cite ltx_citemacro_citep">(van den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib23" title="">2013</a>)</cite>. This approach proved highly effective in processing audio and incorporating content information into recommender systems.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Since then, the Music Information Retrieval (MIR) community has introduced numerous backend models that are pretrained on extensive amounts of music data. Some of these models, such as musiCNN  <cite class="ltx_cite ltx_citemacro_citep">(Pons and Serra, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib19" title="">2019</a>)</cite>, are trained in a supervised manner, usually on auto-tagging, while others, like Jukebox  <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib7" title="">2020</a>)</cite>, are self-supervised. Regardless of the approach, the crucial factor is that these models can effectively be utilized for downstream tasks through transfer learning, yielding results comparable to state-of-the-art models specifically designed for those tasks. This opens up the possibility of using large quantities of unlabeled music data to address problems with limited labeled examples, which could be particularly beneficial in MRS research, where access to large datasets containing both music data and user play history is limited due to copyright.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, there has been a lack of exploration of pretrained audio representations within the context of MRS. MIR papers typically do not research the effectiveness of pretrained backend models for MRS. At the same time, the Recommender Systems (RS) community tends to lean towards traditional end-to-end neural network learning over these models. One notable exception is <cite class="ltx_cite ltx_citemacro_citep">(Park and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib17" title="">2022</a>)</cite>, where the authors utilized three pretrained encoders: CLMR  <cite class="ltx_cite ltx_citemacro_citep">(Spijkervet and Burgoyne, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib20" title="">2021</a>)</cite>, MEE <cite class="ltx_cite ltx_citemacro_citep">(Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib10" title="">2022</a>)</cite>, and Jukebox  <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib7" title="">2020</a>)</cite>. However, the primary focus of that paper was to study the role of negative preferences in user music tastes, and the use of different pretrained models emphasised the stability of the proposed method rather than being integral to the research.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our paper studies the performance of pretrained embeddings in the context of MRS using six recent pretrained backend models. We outline the following research questions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">RQ1: Are pretrained audio representations a viable option for MRS?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">RQ2: How do different backend models compare in the context of MRS?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">RQ3: How does pretrained backend model performance in MRS correspond to performance in MIR tasks?</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The rest of the paper is organized as follows: first, we briefly describe pretrained backend models that will be used to generate audio representations. Then, we describe the dataset and the training details<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code is available at <a class="ltx_ref ltx_href" href="https://github.com/Darel13712/pretrained-audio-representations" title="">github.com/Darel13712/pretrained-audio-representations</a></span></span></span>. We conclude with results and discussion.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Pretrained Audio Representations</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The list of the models we used can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.T1" title="Table 1 ‣ 2.1. Pretrained Audio Representations ‣ 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_tag">1</span></a>. Further, we describe each of them.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.2.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S2.T1.3.2" style="font-size:90%;">Embedding sizes for pretrained audio representations we used</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.4.1.1.2">Embedding Size</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.4.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.2.1.1">MFCC</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.2.1.2">104</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.3.2.1">MusiCNN <cite class="ltx_cite ltx_citemacro_citep">(Pons and Serra, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib19" title="">2019</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.3.2.2">200</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.4.3.1">MusicFM <cite class="ltx_cite ltx_citemacro_citep">(Won et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib24" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.3.2">750</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.5.4.1">EncodecMAE <cite class="ltx_cite ltx_citemacro_citep">(Pepino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib18" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.5.4.2">768</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.6.5.1">Music2Vec <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib14" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.6.5.2">768</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.7.6.1">MERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib13" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.7.6.2">1024</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.4.8.7.1">Jukebox <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib7" title="">2020</a>; Castellon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib4" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.8.7.2">4800</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Mel Frequency Cepstral Coefficients (MFCCs) represent a short-term power spectrum of a sound based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. MFCC is a low-level acoustic feature designed to capture the timbral characteristics of an audio signal. It is widely utilized in various domains <cite class="ltx_cite ltx_citemacro_citep">(Abdul and Al-Talabani, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib2" title="">2022</a>)</cite>, including RS. Strictly speaking, MFCC is not a pretrained audio representation since there is no backend model with learnable parameters behind it. However, we employ this precalculated representation as a baseline and a reference point, utilizing the mean and flattened covariance matrix of MFCCs provided with the dataset we have utilized.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">MusiCNN <cite class="ltx_cite ltx_citemacro_citep">(Pons and Serra, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib19" title="">2019</a>)</cite> is a CNN trained in a supervised way to predict crowd-sourced labels (50 classes: tags from last.fm). It takes log mel spectrograms of audio files as an input and applies a series of convolutional and dense layers to them. It was trained on 200k audio files from the Million Song Dataset  <cite class="ltx_cite ltx_citemacro_citep">(Bertin-Mahieux et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib3" title="">2011</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Jukebox <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib7" title="">2020</a>)</cite> is a music generation model that consists of three separate Vector Quantized Variational Autoencoders (VQ-VAE) with different temporal resolutions. The encoder part of VQ-VAE compresses raw audio input into a sequence of embeddings using <span class="ltx_text" id="S2.SS1.p4.1.1">1-D convolutions</span>. This sequence is then turned into a sequence of discrete tokens using codebooks. The decoder reconstructs raw audio from latent representations. Sequences of compressed tokens are then processed by an autoregressive Sparse Transformer to learn the prior to generate further samples. This approach of training a Language Model over tokenized music representation showed to be a robust foundation for downstream MIR tasks <cite class="ltx_cite ltx_citemacro_citep">(Castellon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib4" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Music2vec <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib14" title="">2022</a>)</cite> uses a multi-layer 1-D CNN feature extractor to compress 16kHz audio input into 50Hz representations that are fed into 12-layer Transformer Blocks. The student model takes partially masked input and tries to predict the average of the top-K layers of the teacher model.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">Encodec <cite class="ltx_cite ltx_citemacro_citep">(D’efossez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib6" title="">2022</a>)</cite> is a neural audio codec that compresses raw audio from 24kHz to 75Hz. It is done by applying a series of convolutional and LSTM blocks to get a sequence of 128-dimensional vectors, which are processed with a residual vector quantization block (RVQ) that maps the input vector to the index of one of the 1024 closest codebook words, then calculates the residual and maps it to a second codebook and so on for a total of 32 codebooks. EncodecMAE  <cite class="ltx_cite ltx_citemacro_citep">(Pepino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib18" title="">2023</a>)</cite> further adopts these representations and compresses them into a single embedding. That is done by applying a Masked Auto Encoder (MAE) on raw Encodec outputs before the RVQ block to predict discrete targets from the RVQ codebooks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">MERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib13" title="">2023</a>)</cite> is trained in a masked language modelling paradigm, incorporating teacher models to generate pseudo labels: an acoustic teacher based on Residual Vector Quantisation — Variational AutoEncoder and a musical teacher based on the Constant-Q Transform. Notably, the model can scale from 95M to 330M parameters.</p>
</div>
<div class="ltx_para" id="S2.SS1.p8">
<p class="ltx_p" id="S2.SS1.p8.1">Musicfm <cite class="ltx_cite ltx_citemacro_citep">(Won et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib24" title="">2023</a>)</cite> is an improvement over MERT design where a BERT-style encoder <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib8" title="">2021</a>)</cite> is replaced with a Conformer <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib25" title="">2020</a>)</cite> and <span class="ltx_text" id="S2.SS1.p8.1.1">k-means</span> clustering tokenization is replaced with random projection and random codebook approach from BEST-RQ <cite class="ltx_cite ltx_citemacro_citep">(Chiu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib5" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p9">
<p class="ltx_p" id="S2.SS1.p9.1">All models described above produce representations for small chunks of audio with lengths ranging from a couple of milliseconds to a couple of seconds. To get a single track-level representation, we average these embeddings over time.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Recommendation models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">After obtaining embeddings for each audio track using pretrained backend models, we must decide how to use them to produce recommendations. It is important to note that the goal of our study is not finding the best possible architecture of a neural network for this task but rather estimating the usefulness of such embeddings using diverse approaches. To this end, we use three methods of increasing complexity:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">K-Nearest Neighbours (KNN)</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Shallow neural network</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">BERT4Rec <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib21" title="">2019</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To implement KNN, we create a representation of a user by averaging the embeddings of the items in their profile and then generate recommendations that are close to that average point. While this is a simple approach, it is crucial for our experiment as it allows us to assess the potential amount of valuable information available for a recommendation task in content-based embeddings.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">For the next approach, we incorporate the listening history described in section <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.SS3" title="2.3. Dataset ‣ 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_tag">2.3</span></a>. Specifically, we process user and item IDs with an embedding layer and a fully connected layer preserving dimensions with a ReLU activation. The score for the user-item pair is the cosine between the resulting vectors. The item embedding layer is initialized with pretrained embeddings from the corresponding backend model. Moreover, we freeze the weights for the item embedding layer to preserve useful content information stored in them. The user embedding layer is initialized with a mean of the user’s tracks, but the weights are unfrozen and can be changed. The model is trained with Max Margin Hinge Loss as in <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib12" title="">2018</a>)</cite> and negative sampling strategy from <cite class="ltx_cite ltx_citemacro_citep">(Magron and F’evotte, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib15" title="">2021</a>)</cite>. More specifically, for each user-item pair from the dataset, we consider an item and a user as positive examples and sample an additional 20 negative users who did not interact with this item for training. Our preliminary studies tested the configuration for frozen item weights, user initialization, and negative sampling strategy and showed the best results. We refer to this model as Shallow Net.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">BERT4Rec <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib21" title="">2019</a>)</cite> is a popular and effective model for sequential recommendations that leverages the Bidirectional Encoder Representations from Transformers (BERT). <span class="ltx_text" id="S2.SS2.p4.1.1">BERT4Rec</span> learning task is to mask some elements in a sequence and to predict them from the context. We chose it to estimate the potential of a more complex and drastically different architecture in our setting. We incorporate pretrained embeddings as a frozen projection of the embedding module, just as we did with the Shallow Net. Due to memory constraints, we limited the maximum length of a sequence to 300, thus limiting the amount of data used for prediction. This value approximately equals the mean length of a user history in our dataset, so it fully covers more than half of the profiles. However, the results may still be further improved by increasing this parameter to cover the complete profiles of all users. To generate recommendations we ranked all items according to scores predicted at the last timestamp of a user profile.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Our study primarily focuses on comparing pretrained audio representations. As a part of this, we keep the item embeddings frozen, as explained earlier. However, we also present the results with regular item initializations for Shallow Net and <span class="ltx_text" id="S2.SS2.p5.1.1">BERT4Rec</span> to provide a reference point. This helps us test whether frozen item embeddings enhance the recommendation model or impose an unnecessary constraint that hinders performance. We refer to this as random initialization. In this scenario, item embeddings are randomly initialized, unfrozen, and fully learned using the recommendation model. Another option is to initialize the model with pretrained embeddings and unfreeze the weights, but we have omitted this as the results are similar to the frozen variant.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Dataset</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We use the Music4All-Onion <cite class="ltx_cite ltx_citemacro_citep">(Moscati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib16" title="">2022</a>)</cite> dataset because it contains both music samples, user listening history and precomputed MFCC features. We select the last month of listening history (everything after 2020-02-20) as test and validation data and the previous year as training data. Test and validation contain only the items that users have not listened to before 2020-02-20, so all the items in the test and validation set are new in a user’s listening history. We split last month into validation and test set by users in half. Further, we ensure that validation and test data do not contain new users and items that do not appear in train data. Initially, we also planned to test the performance on cold items. However, the particular properties of this dataset are not well suited for this since there are not enough cold items in the last month for proper evaluation. Resulting split sizes can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S2.T2" title="Table 2 ‣ 2.3. Dataset ‣ 2. Methods ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.2.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S2.T2.3.2" style="font-size:90%;">Train-test split for Music4All-Onion dataset. We used last month for validation and testing, splitting users equally into both groups at random. The previous 12 months are used as train data. Cold users and items are removed.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.4.1.1">
<td class="ltx_td" id="S2.T2.4.1.1.1"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.1.1.2">Train</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.1.1.3">Validation</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.1.1.4">Test</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.2.2">
<td class="ltx_td ltx_align_center" id="S2.T2.4.2.2.1">Num Users</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.2.2.2">17,053</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.2.2.3">6,092</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.2.2.4">6,092</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.3.3">
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.3.1">Num Items</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.3.2">56,193</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.3.3">36,942</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.3.3.4">37,797</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.4.4">
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.1">Num Interactions</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.2">5,122,221</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.3">132,425</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4">138,299</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Training Details</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">We use Adam optimizer with lr=0.001, early stopping, and we reduce the learning rate on the plateau. For Shallow Net, we train for 100 epochs with cosine-based Hinge Loss. For each positive user-item pair, we sampled 20 negative users at random for this item. For <span class="ltx_text" id="S2.SS4.p1.1.1">BERT4Rec</span>, we train for 200 epochs with Cross Entropy loss. We calculate HitRate@50, Recall@50 and NDCG@50 to evaluate the results. We also computed MRR and Precision, but we omit them from this paper because they are highly correlated with the other metrics and do not give a broader perspective. We remove listened tracks from recommendations with each model to predict only new items.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.T3" title="Table 3 ‣ 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_tag">3</span></a> shows the performance of audio representations using different recommendation models.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.3.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S3.T3.4.2" style="font-size:90%;">Comparison of performance of different pretrained embeddings using KNN, Shallow Net, and <span class="ltx_text" id="S3.T3.4.2.1">BERT4Rec</span> models to generate recommendations. Random value in the embeddings column corresponds to random initialization of item embeddings in a model. With random initialization, item embeddings are unfrozen and can be learned like usual.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T3.5.1.1.1">Embeddings</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.5.1.1.2">HitRate@50</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.5.1.1.3">Recall@50</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.5.1.1.4">NDCG@50</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S3.T3.5.2.2.1">KNN</th>
</tr>
<tr class="ltx_tr" id="S3.T3.5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.3.3.1">MusicFM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.3.3.2">0.009</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.3.3.3">0.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.3.3.4">0.000</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.4.4.1">MFCC</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.4.4.2">0.028</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.4.4.3">0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.4.4.4">0.001</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.5.5.1">Music2Vec</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.2">0.033</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.3">0.002</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.4">0.001</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.6.6.1">MERT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.6.6.2">0.049</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.6.6.3">0.003</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.6.6.4">0.002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.7.7.1">EncodecMAE</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.7.7.2">0.054</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.7.7.3">0.003</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.7.7.4">0.002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.8.8.1">Jukebox</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.8.8.2">0.057</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.8.8.3">0.003</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.8.8.4">0.002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.9.9.1">MusiCNN</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.9.9.2"><span class="ltx_text ltx_font_bold" id="S3.T3.5.9.9.2.1">0.089</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.9.9.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.9.9.3.1">0.005</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.9.9.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.9.9.4.1">0.004</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S3.T3.5.10.10.1">Shallow Net</th>
</tr>
<tr class="ltx_tr" id="S3.T3.5.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.11.11.1">Random</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.11.11.2">0.021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.11.11.3">0.001</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.11.11.4">0.001</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.12.12.1">MusicFM</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.12.12.2">0.108</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.12.12.3">0.007</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.12.12.4">0.005</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.13.13.1">MFCC</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.13.13.2">0.226</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.13.13.3">0.018</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.13.13.4">0.013</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.14.14.1">Music2Vec</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.14.14.2">0.291</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.14.14.3">0.029</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.14.14.4">0.021</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.15.15.1">MERT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.15.15.2">0.291</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.15.15.3">0.030</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.15.15.4">0.021</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.16.16.1">EncodecMAE</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.16.16.2">0.296</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.16.16.3">0.031</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.16.16.4">0.021</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.17.17.1">Jukebox</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.17.17.2">0.272</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.17.17.3">0.029</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.17.17.4">0.020</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.18.18.1">MusiCNN</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.18.18.2"><span class="ltx_text ltx_font_bold" id="S3.T3.5.18.18.2.1">0.329</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.18.18.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.18.18.3.1">0.037</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.18.18.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.18.18.4.1">0.025</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S3.T3.5.19.19.1">BERT4Rec</th>
</tr>
<tr class="ltx_tr" id="S3.T3.5.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.20.20.1">Random</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.20.20.2">0.348</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.20.20.3">0.049</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.20.20.4">0.038</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.21.21.1">MusicFM</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.21.21.2">0.261</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.21.21.3">0.021</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.21.21.4">0.016</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.22.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.22.22.1">MFCC</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.22.22.2">0.231</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.22.22.3">0.019</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.22.22.4">0.014</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.23.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.23.23.1">Music2Vec</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.23.23.2">0.281</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.23.23.3">0.025</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.23.23.4">0.020</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.24.24.1">MERT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.24.24.2">0.360</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.24.24.3">0.051</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.24.24.4">0.038</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.25.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.25.25.1">EncodecMAE</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.25.25.2">0.349</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.25.25.3">0.050</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.25.25.4">0.038</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.26.26.1">Jukebox</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.26.26.2">0.219</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.26.26.3">0.015</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.26.26.4">0.012</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.27.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.5.27.27.1">MusiCNN</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.27.27.2"><span class="ltx_text ltx_font_bold" id="S3.T3.5.27.27.2.1">0.385</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.27.27.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.27.27.3.1">0.058</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.27.27.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.27.27.4.1">0.044</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The first observation is that, on average, more complicated recommendation models tend to give better performance. Across different pretrained representations, Shallow Net performs better than KNN, and <span class="ltx_text" id="S3.p2.1.1">BERT4Rec</span> performs better than Shallow Net, which is expected but highlights the importance of choosing a model architecture with appropriate complexity.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Secondly, combining content and collaborative information tends to improve results over pure collaborative variant. This is true for all backend models used with Shallow Net. The poor performance of Shallow Net with random initialization can be seen as a disadvantage. However, in our experiment, it is rather an advantage since all the boost in performance with content embeddings comes from the information stored in them. The fact that all backend models enriched with collaborative information show 10 times better performance than their respective raw KNN variants, shows the synergy between content embeddings and collaborative data.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">However, with <span class="ltx_text" id="S3.p4.1.1">BERT4Rec</span>, results vary for different embeddings. The model by itself shows good performance. MusiCNN shows a statistically significant (p ¡ 0.05) improvement over the base <span class="ltx_text" id="S3.p4.1.2">BERT4Rec</span>. MERT and EncodecMAE perform comparably to collaborative <span class="ltx_text" id="S3.p4.1.3">BERT4Rec</span> with random initialization, but the difference is not statistically significant. The performance of MusicFM, <span class="ltx_text" id="S3.p4.1.4">Music2Vec</span> and Jukebox is worse than that of the pure collaborative variant. This indicates that it is harder for <span class="ltx_text" id="S3.p4.1.5">BERT4Rec</span> to extract useful information from them, and a more elaborate procedure of inferring content knowledge should be employed.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">If we compare the performance of different backend models across all the approaches, we can see that MusicFM tends to end up in lower positions, even lower than MFCC for KNN and Shallow Net. Music2Vec tends to be slightly better. Jukebox is the second best option with KNN and Shallow Net but significantly drops with <span class="ltx_text" id="S3.p5.1.1">BERT4Rec</span>, probably because it has a dimension size 4800, which is much bigger than other models. MERT and EncodecMAE show similar performance overall, but MERT works better with <span class="ltx_text" id="S3.p5.1.2">BERT4Rec</span>. MusiCNN constantly shows the best performance across all tests.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Comparison to MIR results</h3>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.2.1.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S3.T4.3.2" style="font-size:90%;">Comparison of backend models applied to different MIR tasks and MRS. Results for MIR are taken from the respective papers of each model; the last column is from this paper.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.4.1.1">
<td class="ltx_td ltx_align_center" id="S3.T4.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.1.1.2">Tags</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.1.1.3">Genres</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.1.1.4">Key</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.1.1.5">Recs</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.2.2">
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.2.1">MusicFM</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T4.4.2.2.2.1">0.924</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.2.3">—</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T4.4.2.2.4.1">0.674</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.2.5">0.261</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.3.3">
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.3.1">Music2Vec</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.3.2">0.895</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.3.3">0.766</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.3.4">0.508</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.3.5">0.281</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.1">MERT</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.2">0.913</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.3">0.793</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.4">0.656</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.5">0.360</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.5.5">
<td class="ltx_td ltx_align_center" id="S3.T4.4.5.5.1">EncodecMAE</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.5.5.2">—</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.5.5.3"><span class="ltx_text ltx_font_bold" id="S3.T4.4.5.5.3.1">0.862</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.5.5.4">—</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.5.5.5">0.349</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.6.6">
<td class="ltx_td ltx_align_center" id="S3.T4.4.6.6.1">Jukebox</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.6.6.2">0.915</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.6.6.3">0.797</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.6.6.4">0.667</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.6.6.5">0.219</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.7.7">
<td class="ltx_td ltx_align_center" id="S3.T4.4.7.7.1">MusiCNN</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.7.7.2">0.906</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.7.7.3">0.790</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.7.7.4">0.128</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.7.7.5"><span class="ltx_text ltx_font_bold" id="S3.T4.4.7.7.5.1">0.385</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We used self-reported data from corresponding papers to compare our results against the performance of backend models on MIR tasks. Specifically, in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#S3.T4" title="Table 4 ‣ 3.1. Comparison to MIR results ‣ 3. Results ‣ Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems"><span class="ltx_text ltx_ref_tag">4</span></a> Tags column is the AUC metric on MagnaTagATune <cite class="ltx_cite ltx_citemacro_citep">(Law et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib11" title="">2009</a>)</cite>, Genres is the genre classification accuracy on GTZAN <cite class="ltx_cite ltx_citemacro_citep">(Tzanetakis and Cook, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib22" title="">2002</a>)</cite>, Key is the key detection accuracy on Giantsteps <cite class="ltx_cite ltx_citemacro_citep">(Knees et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib9" title="">2015</a>)</cite>, and Recs is the recommendation HitRate@50 with <span class="ltx_text" id="S3.SS1.p1.1.1">BERT4Rec</span> reported in this paper. We tried to choose the tasks and datasets reported by all the models we used. However, EncodecMAE results are unavailable for tag prediction and key estimation, and MusicFM results are unavailable for genre prediction.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We can see a drastic difference when we compare the performance of backend models in MIR tasks and MRS. MusicFM and Jukebox hold the best results in auto-tagging and key prediction but are the worst for recommendations. However, MERT, the third-ranking model for other MIR tasks, is the second-best model for recommendations, which suggests that it contains valuable information for both tasks. The same goes for EncodecMAE, which shows the best performance in genre prediction and the third best in recommendations. <span class="ltx_text" id="S3.SS1.p2.1.1">Music2Vec</span> tends to show worse results across all tasks, both MIR and MRS. A surprising difference can be found with MusiCNN because it is comparable to MusicFM and Jukebox but has slightly lower results for tags and genres, the lowest performance for key detection, and the best performance for recommendations across all our experiments. The low performance of MusicFM in our evaluations and high performance in the MIR context might suggest technical problems with published weights for the model we used since MusicFM is a modification over MERT, which shows good performance across all tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Discussion</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>RQ1: Are pretrained audio representations a viable option for MRS?</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Our experiments show that improving a pure collaborative model with content information is possible without model finetuning or end-to-end re-learning, thus advocating for broader usage of pretrained backend models in MRS.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>RQ2: How do different backend models compare in the context of MRS?</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">MusiCNN shows consistently good results in all our tests, which suggests that the supervised auto-tagging task it was trained on contains a lot of useful information for MRS. Two other good options for MRS are MERT and EncodecMAE. <span class="ltx_text" id="S3.SS2.SSS2.p1.1.1">Music2Vec</span> shows slightly worse performance, which aligns with its performance in other MIR tasks. MusicFM shows inferior performance in our tests but outstanding performance in MIR, which may suggest some technical problems with the published model weights that we used. Jukebox’s performance is better than most models with KNN but tends to fall in position relative to other models with the increasing model complexity in our experiments. Combined with good results in MIR, it may suggest a need for more elaborate embedding processing than we used, possibly because the embedding size is much larger.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>RQ3: How does pretrained backend model performance in MRS correspond to performance in MIR tasks?</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">The performance can vary between different downstream tasks, the most notable difference being that MusiCNN is showing outstanding results in MRS.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Limitations</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">One of the limitations of our work is the usage of only one dataset, which may undermine the generalizability of our results. The second limitation is the number and scope of the recommendation models studied. Our results show that model architecture plays a significant role in the amount of useful information extracted from content embeddings, which calls for a broader scope of models. Moreover, we studied only one way to incorporate embeddings into a recommender system by using them as frozen item embeddings with a learned transformation over them. However, it is also possible to try other approaches, like predicting collaborative embeddings using content information <cite class="ltx_cite ltx_citemacro_citep">(van den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib23" title="">2013</a>)</cite> or using content embedding as a regularization on collaborative one <cite class="ltx_cite ltx_citemacro_citep">(Magron and F’evotte, <a class="ltx_ref" href="https://arxiv.org/html/2409.08987v1#bib.bib15" title="">2021</a>)</cite>. Our work can be further improved by comparing end-to-end CNN models widespread in MRS with general pretrained MIR models. Finally, a notable direction for further work is measuring the performance of our approach in the cold-start scenario.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We compared different frozen backend models for an MRS task using three ways to incorporate them into the recommendation process. We showed it is a viable approach to improve pure collaborative model performance. We found that EncodecMAE, MERT and MusiCNN performed well in the context of MRS. Comparing the performance of these models in MRS and MIR tasks, we demonstrate that best-performing MIR models do not always translate to best-performing MRS models. Notably, the supervised tag prediction task of MusiCNN suggests the usefulness of tags like genres, instruments, and emotions in improving recommendations. We hope this paper proves helpful in inspiring the adoption of pretrained audio representations in MRS.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdul and Al-Talabani (2022)</span>
<span class="ltx_bibblock">
Zrar Kh. Abdul and Abdulbasit K. Al-Talabani. 2022.

</span>
<span class="ltx_bibblock">Mel Frequency Cepstral Coefficient and its Applications: A Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Access</em> 10 (2022), 122136–122158.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ACCESS.2022.3223444" title="">https://doi.org/10.1109/ACCESS.2022.3223444</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertin-Mahieux et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. 2011.

</span>
<span class="ltx_bibblock">The Million Song Dataset. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castellon et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rodrigo Castellon, Chris Donahue, and Percy Liang. 2021.

</span>
<span class="ltx_bibblock">Codified audio language modeling learns useful representations for music information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">ArXiv</em> abs/2107.05677 (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiu et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. 2022.

</span>
<span class="ltx_bibblock">Self-supervised Learning with Random-projection Quantizer for Speech Recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D’efossez et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Alexandre D’efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022.

</span>
<span class="ltx_bibblock">High Fidelity Neural Audio Compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">ArXiv</em> abs/2210.13438 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. 2020.

</span>
<span class="ltx_bibblock">Jukebox: A Generative Model for Music.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">ArXiv</em> abs/2005.00341 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel rahman Mohamed. 2021.

</span>
<span class="ltx_bibblock">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 29 (2021), 3451–3460.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knees et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Peter Knees, Ángel Faraldo, Perfecto Herrera, Richard Vogl, Sebastian Böck, Florian Hörschläger, and Mickael Le Goff. 2015.

</span>
<span class="ltx_bibblock">Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">International Society for Music Information Retrieval Conference</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koo et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Junghyun Koo, Seungryeol Paik, and Kyogu Lee. 2022.

</span>
<span class="ltx_bibblock">End-To-End Music Remastering System Using Self-Supervised And Adversarial Training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (2022), 4608–4612.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Law et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Edith Law, Kris West, Michael I. Mandel, Mert Bay, and J. S. Downie. 2009.

</span>
<span class="ltx_bibblock">10 th International Society for Music Information Retrieval Conference ( ISMIR 2009 ) EVALUATION OF ALGORITHMS USING GAMES : THE CASE OF MUSIC TAGGING.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jongpil Lee, Kyungyun Lee, Jiyoung Park, Jangyeon Park, and Juhan Nam. 2018.

</span>
<span class="ltx_bibblock">Deep Content-User Embedding Model for Music Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">ArXiv</em> abs/1807.06786 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yizhi Li, Ruibin Yuan, Ge Zhang, Yi Ma, Xingran Chen, Hanzhi Yin, Chen-Li Lin, Anton Ragni, Emmanouil Benetos, N. Gyenge, Roger B. Dannenberg, Ruibo Liu, Wenhu Chen, Gus G. Xia, Yemin Shi, Wen-Fen Huang, Yi-Ting Guo, and Jie Fu. 2023.

</span>
<span class="ltx_bibblock">MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">ArXiv</em> abs/2306.00107 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yizhi Li, Ruibin Yuan, Ge Zhang, Yi Ma, Chenghua Lin, Xingran Chen, Anton Ragni, Hanzhi Yin, Zhijie Hu, Haoyu He, Emmanouil Benetos, Norbert Gyenge, Ruibo Liu, and Jie Fu. 2022.

</span>
<span class="ltx_bibblock">MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ArXiv</em> abs/2212.02508 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magron and F’evotte (2021)</span>
<span class="ltx_bibblock">
Paul Magron and C’edric F’evotte. 2021.

</span>
<span class="ltx_bibblock">Neural content-aware collaborative filtering for cold-start music recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Data Mining and Knowledge Discovery</em> 36 (2021), 1971 – 2005.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moscati et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Marta Moscati, Emilia Parada-Cabaleiro, Yashar Deldjoo, Eva Zangerle, and Markus Schedl. 2022.

</span>
<span class="ltx_bibblock">Music4All-Onion – A Large-Scale Multi-faceted Content-Centric Music Recommendation Dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park and Lee (2022)</span>
<span class="ltx_bibblock">
Minju Park and Kyogu Lee. 2022.

</span>
<span class="ltx_bibblock">Exploiting Negative Preference in Content-based Music Recommendation with Contrastive Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 16th ACM Conference on Recommender Systems</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pepino et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Leonardo Pepino, Pablo Ernesto Riera, and Luciana Ferrer. 2023.

</span>
<span class="ltx_bibblock">EnCodecMAE: Leveraging neural codecs for universal audio representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">ArXiv</em> abs/2309.07391 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pons and Serra (2019)</span>
<span class="ltx_bibblock">
Jordi Pons and Xavier Serra. 2019.

</span>
<span class="ltx_bibblock">musicnn: Pre-trained convolutional neural networks for music audio tagging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ArXiv</em> abs/1909.06654 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spijkervet and Burgoyne (2021)</span>
<span class="ltx_bibblock">
Janne Spijkervet and John Ashley Burgoyne. 2021.

</span>
<span class="ltx_bibblock">Contrastive Learning of Musical Representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ArXiv</em> abs/2103.09410 (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tzanetakis and Cook (2002)</span>
<span class="ltx_bibblock">
George Tzanetakis and Perry R. Cook. 2002.

</span>
<span class="ltx_bibblock">Musical genre classification of audio signals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Trans. Speech Audio Process.</em> 10 (2002), 293–302.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Aäron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013.

</span>
<span class="ltx_bibblock">Deep content-based music recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Neural Information Processing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Won et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Minz Won, Yun-Ning Hung, and Duc Le. 2023.

</span>
<span class="ltx_bibblock">A Foundation Model for Music Informatics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">ArXiv</em> abs/2311.03318 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, and Yonghui Wu. 2020.

</span>
<span class="ltx_bibblock">Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">ArXiv</em> abs/2010.10504 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 29 12:28:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
