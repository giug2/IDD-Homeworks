<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.07116] Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques</title><meta property="og:description" content="To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency oâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.07116">

<!--Generated on Thu Feb 29 06:56:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pepijn de Reus
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">Master Artificial Intelligence</span>
<br class="ltx_break"><span id="id5.2.id2" class="ltx_text ltx_font_italic">University of Amsterdam
<br class="ltx_break"></span>Amsterdam, The Netherlands 
<br class="ltx_break">p.dereus@uva.nl
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ana Oprescu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_font_italic">Complex Cyber Infrastructure</span>
<br class="ltx_break"><span id="id7.2.id2" class="ltx_text ltx_font_italic">University of Amsterdam
<br class="ltx_break"></span>Amsterdam, The Netherlands 
<br class="ltx_break">a.m.oprescu@uva.nl
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Koen van Elsen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id8.1.id1" class="ltx_text ltx_font_italic">Institute for Informatics</span>
<br class="ltx_break"><span id="id9.2.id2" class="ltx_text ltx_font_italic">University of Amsterdam
<br class="ltx_break"></span>Amsterdam, The Netherlands 
<br class="ltx_break">k.m.j.vanelsen@uva.nl
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on <span id="id2.2.1" class="ltx_text ltx_font_italic">both</span> the energy consumption and accuracy of the machine learning models, focusing on <math id="id1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="id1.1.m1.1a"><mi id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">k</annotation></semantics></math>-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on <math id="id2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="id2.2.m2.1a"><mi id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">k</annotation></semantics></math>-anonymised data consume less energy than models trained on the original data, with a similar performance regarding accuracy. Models trained on synthetic data have a similar energy consumption and a similar to lower accuracy compared to models trained on the original data.
<br class="ltx_break"></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<math id="id3.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="id3.m1.1a"><mi id="id3.m1.1.1" xref="id3.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="id3.m1.1b"><ci id="id3.m1.1.1.cmml" xref="id3.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.m1.1c">k</annotation></semantics></math>-anonymity, synthetic data, machine learning, energy consumption of machine learning, energy consumption of artificial intelligence, privacy-enhancing machine learning

</div>
<div id="p1" class="ltx_para">
<svg id="p1.pic1" class="ltx_picture" height="86.71" overflow="visible" version="1.1" width="604.52"><g transform="translate(0,86.71) matrix(1 0 0 -1 0 0) translate(-122.74,0) translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 127.35 52.63)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="595.3" height="77.49" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="p1.pic1.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:430.2pt;">
<span id="p1.pic1.1.1.1.1.1" class="ltx_p"><span id="p1.pic1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Â© 2023 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers or
lists, or reuse of any copyrighted component of this work in other works.
DOI: 10.1109/ICT4S58814.2023.00015. Available at:
<br class="ltx_break"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/document/10292174</span></span></span>
</span></foreignObject></g></svg>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">To address climate change the European Commission has set a goal of reducing net carbon emission to zero in 2050Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The amount of publications on Artificial Intelligence (AI) has increased more than fivefold over the last decadeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Early studies already warned of the dangers of using digitalisation without rebound considerations regarding the environmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and identified the need for a more fine-grained analysis of digital processes with respect to their ecological footprintÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Nowadays, several international initiatives, such as the EU Green Deal, aim to reduce carbon emissions. With this goal in mind, the EU aims to make data centres and ICT infrastructures climate-neutral by 2030 and aims to make use of artificial intelligence and other digital technologies to reduce the impact of climate changeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This has encouraged more research with a focus on energy consumption within machine learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Apart from the energy aspect there is growing concern about privacy amongst citizens of all agesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To ensure the privacy of those whose data are collected the General Data Protection Regulation (GDPR) has been adopted in Europe in 2016. The GDPR regulates that all citizens in Europe have control over their personal dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. There is one exception to the GDPR; it does not apply to anonymised data. In the GDPR, anonymous data is defined as: â€information which does not relate to an identified or identifiable natural person or to personal data rendered anonymous in such a manner that the data subject is not or no longer identifiable.â€Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Therefore it is interesting to look into methods to anonymise data such that data can be shared without GDPR constraints, especially in light of the upcoming Data ActÂ <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ec.europa.eu/commission/presscorner/detail/en/ip_22_1113</span></span></span></span>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">One common method for enhancing privacy of data is <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">k</annotation></semantics></math>-anonymity, using either generalisation and suppressionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or micro-aggregationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Apart from <math id="S1.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p3.2.m2.1a"><mi id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">k</annotation></semantics></math>-anonymity, synthetic data is becoming increasingly popular as a privacy-enhancing techniqueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Synthetic data is unique as it mimics the original data in correlations and properties, yet it does not contain the original dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">Combining these societal concerns and trends raises the question of what privacy-enhancing algorithms add to the energy consumption of regular machine learning tasks. And how do these algorithms relate to the accuracy of machine learning models? In this paper, we analyse both aspects for both <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">k</annotation></semantics></math>-anonymity and synthetic data. Previous research evaluated the impact of the <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p4.2.m2.1a"><mi id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><ci id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">k</annotation></semantics></math>-valueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and of synthetic dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on the accuracy of the resulting machine learning models. While there is work analysing both the energy cost and accuracy impact of the k-valueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, to the best of our knowledge, no work has been conducted on comparing synthetic data with <math id="S1.p4.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p4.3.m3.1a"><mi id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><ci id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">k</annotation></semantics></math>-anonymised data looking at both accuracy and energy consumption. Understanding this impact could help users make an informed decision between energy consumption, accuracy, and anonymity.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.4.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.5.2" class="ltx_text ltx_font_italic">Research questions</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">In the context of considering both an increased use of data and its climate impact, one exploratory goal of this research is to understand which method would be preferred when using data protection: <math id="S1.SS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.SS1.p1.1.m1.1a"><mi id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.1b"><ci id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.1c">k</annotation></semantics></math>-anonymity or synthetic data?</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">In this paper we examine two directly related questions:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">RQ1:</span></span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">Which method yields the least loss of accuracy?</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">RQ2:</span></span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">Which method is the least energy consuming?</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S1.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Outline</h3>

<div id="S1.SSx1.p1" class="ltx_para">
<p id="S1.SSx1.p1.1" class="ltx_p">We briefly review background knowledge in SectionÂ <a href="#S2" title="II Background â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and we explain our research method in SectionÂ <a href="#S3" title="III Research method â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. In SectionÂ <a href="#S4" title="IV Experimental set-up â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> we describe the experimental set-up. The results are shown in SectionÂ <a href="#S5" title="V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> and discussed in SectionÂ <a href="#S6" title="VI Discussion â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>. In SectionÂ <a href="#S7" title="VII Related work â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> we relate similar efforts to our research and we conclude in SectionÂ <a href="#S8" title="VIII Conclusion â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We briefly present several concepts that are key for our research.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><math id="S2.SS1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.1.m1.1b"><mi id="S2.SS1.1.m1.1.1" xref="S2.SS1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.1.m1.1c"><ci id="S2.SS1.1.m1.1.1.cmml" xref="S2.SS1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.1.m1.1d">k</annotation></semantics></math><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">-anonymity</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.4" class="ltx_p">A well known property to define anonymised data is <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">k</annotation></semantics></math>-anonymity, which was first introduced by SweeneyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In this paper Sweeney proved that anonymisation by removal of identifying attributes such as names was insufficient to ensure privacy. By linking two publicly available, anonymised, data sets Sweeney was able to identify 87% of the data subjects using a combination of ZIP code and date of birthÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Following this Sweeney introduced <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">k</annotation></semantics></math>-anonymity to protect data subjectâ€™s privacy. Shortly put, a data set with a <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">k</annotation></semantics></math>-anonymity of 5 ensures that for each data subject in the set, at least 5 other data subjects share the exact same properties. These 6 data subjects would thus be indistinguishable from each other. To come to this <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">k</annotation></semantics></math>-anonymity, all attributes should be classified as one of four data types:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Insensitive</span> attributes are deemed unimportant for privacy and will remain unaltered.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Sensitive</span> attributes are important for the subject (think of political view).</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Identifying</span> attributes such as names are directly linked to a person and will be removed from the data.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Quasi-identifying</span> attributes refer to the data that could compromise privacy when linked with other attributes or data sets.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">Once the attributes of a data set are labeled using the data types above and our value for <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">k</annotation></semantics></math> is picked, we can use generalisation and suppression to alter our data such that it suffices to this <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">k</annotation></semantics></math>-anonymityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. It could be that data that is labeled as quasi-identifying is in fact a unique tracker because one data subject has a unique value for this attribute. A known example is that a university has only one dean. If a university were to publish the salaries, the function of a dean is a unique tracker. Even though functions in general are not per se identifying, e.g. there could be hundreds of teachers. For generalisation and suppression we need hierarchies to group our data, as can be seen for ZIP codes in Figure <a href="#S2.F1" title="Figure 1 â€£ II-A ğ‘˜-anonymity â€£ II Background â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2305.07116/assets/Images/Gen+Supp.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="252" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Generalisation and suppression for ZIP codes. In the orange boxes we see the original values, green boxes represent the anonymised boxes using generalisation and suppression. Next to the anonymised boxes we have the corresponding <math id="S2.F1.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.F1.2.m1.1b"><mi id="S2.F1.2.m1.1.1" xref="S2.F1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.F1.2.m1.1c"><ci id="S2.F1.2.m1.1.1.cmml" xref="S2.F1.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.m1.1d">k</annotation></semantics></math>-value.</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.2" class="ltx_p">The first level of our hierarchy would be to suppress the last digit, after which we could suppress all but the first digit. The algorithm will try to leave as much data as is, but will increase the hierarchy if the attribute prevents the data set from obtaining a certain <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">k</annotation></semantics></math>-anonymity. These hierarchies are increased until the desired <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">k</annotation></semantics></math>-anonymity is reached or until the entire attribute is suppressed.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Synthetic data</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">An increasingly popular methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to share data without comprising the subjectsâ€™ privacy is synthetic data. Synthetic data is a form of AI that learns the properties, covariance and distributions of a data set. Using this information, a new, synthetic, data set can be constructed with very similar statistics to the original data set such that the performance of machine learning on the synthetic data set is similar to that of the original data setÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. By making sure that each data subject within the synthetic data is not present in the actual data we can publish the data set without compromising the data subjectâ€™s privacyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Measuring energy consumption</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Processors of Intel from 2012 and later have Running Average Power Limiters (RAPL) energy sensors in their CPUsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The RAPL sensors measure the energy consumption of the CPU and DRAM of the Intel chip. An advantage of RAPL is that the user can select which parts of the chip should be measuredÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, e.g. solely the CPU or the DRAM energy consumption.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Research method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In order to answer the research questions a setup was designed to measure both the energy consumption and the impact on the accuracy of the selected machine learning techniques. The input of this setup is the data, the values of k in the <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">k</annotation></semantics></math>-anonymisation process and lastly the hyperparameters for the synthetic data and the machine learning techniques. This will output the total energy consumption of the setup and the accuracy score for each ML technique based on the input data.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Firstly the data is preprocessed, after which either the synthetic data generator or the anonymisation algorithm is used to obtain the new data. This data is then fed to the machine learning techniques which will yield an accuracy for each technique. This will give insights to answer the research question regarding loss of accuracy (<span id="S3.p2.1.1" class="ltx_text ltx_font_bold">RQ1</span>). Over this entire process we measure the energy consumption by reading the RAPL counters, which helps to answer our research question for the energy consumption (<span id="S3.p2.1.2" class="ltx_text ltx_font_bold">RQ2</span>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Data</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To answer our research questions we require data sets that contain personal information, preferably with unique trackers. In general, such data sets are hard to obtain because of their privacy sensitive character. A good alternative is to use data sets from the UC Irvine Machine Learning repository, which contains many public data sets for machine learning researchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Preferably the data sets differ in size and in the type of data they contain so that we can diversify our findings. If a data set contains missing information we choose to delete these rows as is common in literatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. After deleting the rows with missing information we say our data is cleaned.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">Following this data cleaning we anonymise the data using <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">k</annotation></semantics></math>-anonymity using generalisation and suppression. Micro-aggregation is left out as recent work suggests that anonymisation by generalisation and suppression has less loss of accuracyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. As explained in the background we first wish to, manually, label our attributes as either insensitive, sensitive, identifying or quasi-identifying. Insensitive information will remain unaltered and identifying information will be suppressed, though we do not expect to find identifying attributes in public data sets. To anonymise the data we will use the <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">k</annotation></semantics></math>-values of 3, 10 and 27 as earlier work shows that these values could lead to improved accuracy scores when compared to the baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The suppression limit is set to 20% as is the default setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The anonymised output data will then be used to train the machine learning techniques, after which the energy consumption and accuracy of the techniques will be compared to the baseline.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The generator for the synthetic data will also use the cleaned data to analyse the data features and structures. By computing the probability functions and the correlations between the attributes it will output a synthetic data set. As for the anonymised data the machine learning techniques will then be trained on this synthetic data, where after the energy consumption and accuracy of the techniques will be evaluated to the baseline.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Machine Learning</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To evaluate the impact of using <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">k</annotation></semantics></math>-anonymity and synthetic data on machine learning, three common techniques are used:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">k-nearest neighbours</span> groups data points based on shared features, assuming that data from the same class lies close to each other <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Logistic Regression</span> is a common technique for classification problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. It computes the probability of each class and then picks the class with the highest probability.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Neural networks</span> are used for a wide variety of problems, e.g. regression and classification. Neural networks are computationally more expensive and handle noise better than most other ML techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To evaluate these machine learning models, we compute the accuracy. The accuracy consists of the correctly classified units divided by the total amount of data units as implemented in sklearn and keras sequential. Since the setup of all models will be similar for the different data sets, the possibility for variations to come from anything other than the data are excluded. In this way we can measure the information loss of anonymised and synthetic data by analysing the accuracy of the machine learning models.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Following the information above, we summarise the set-up in Figure <a href="#S3.F2" title="Figure 2 â€£ III-B Machine Learning â€£ III Research method â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.07116/assets/Images/GeneralOverview.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>General overview of the experiment. In blue we see the cleaned input data, in yellow we see the anonymisation method where the purple bar specifies the used method. The green box represents the machine learning, where likewise the techniques are specified in the purple bars.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Measuring energy consumption</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">There are two options to measure the energy consumption of our code; hardware-based or software-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The hardware-based method, also known as power plug method, is an external device between the socket and the used machine. This device is very accurate, but can measure only the entire energy input to the device. The software-based solution uses RAPL. RAPL is a built-in tool for Intel processors and measures the energy consumption for both the CPU and DRAM. Measurements that compare RAPL with the power plug method show a correlation of 0.99 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, so the influence of RAPL on the total energy consumption may be neglected. To reduce the chance of outliers we perform 10 measurements and take the weighted average. Additionally, we perform 10 idle measurements as well. In these measurements we compute the energy consumption of one second sleep in the code. This idle measurement will be averaged again and deducted from the other measurements to obtain a measurement solely based upon the used code.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental set-up</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As RAPL is not available for macOS, even when using virtual machines, we run the code on a server instead. We run the experiments on an Ubuntu 22.04 LTS, with an 8x Intel CPU E5-1620 3.5GHz and 16 GB (4 *sk 4GB) DDR4 2400 MT/s. We used ARX 3.9.0, DataSynthesizer v0.1.11, pyRAPL 0.2.3.1, Python 3.10.4 and Java 11.0.15. The machine learning models were implemented using the sklearn (Logistic Regression and k-nearest neighbours) and Tensorflow (neural network) Python libraries. Our code is publicly available on GitHubÂ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/PepijndeReus/Privacy-Enhancing-ML</span></span></span></span>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">We use two known data sets from the UC Irvine Machine Learning RepositoryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, the Adult data set because it is seen as a benchmark set for k-nearest neighboursÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and the Student Performance set. The Student Performance set has a different structure, as can be seen in TableÂ <a href="#S4.T1" title="TABLE I â€£ IV Experimental set-up â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. We use a set with a different structure as this could help gaining insight in how far these findings extend to different kinds of data sets. However to keep as many parameters equal we modify the Student Performance set such that it will also become a binary prediction task, as seen in earlier literatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Furthermore, we use the Portuguese class as this contains more instances (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="n=649" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">n</mi><mo id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">649</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><eq id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></eq><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ğ‘›</ci><cn type="integer" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">649</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n=649</annotation></semantics></math>) compared to the mathematics class (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="n=395" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">n</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">395</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><eq id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></eq><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ğ‘›</ci><cn type="integer" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">395</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">n=395</annotation></semantics></math>).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Overview of the structure of the selected data sets.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" colspan="2"># entries</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"># attributes</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Prediction</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left">Adult</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">48 842</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">15</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">Binary</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_bb">Student Performance</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_right ltx_border_bb">649</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_bb">33</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">Value (0-20)</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">k-anonymity</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To create the data sets that have <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">k</annotation></semantics></math>-anonymity we run our data trough the ARX libraryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, as this is often used in research to generate anonymised data and is open-sourceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. ARX requires predefined hierarchies to use generalisation and suppression. We use a similar hierarchy as in earlier workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, the hyperparameters are available on GitHub.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Synthetic data</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For the creation of synthetic data we use the DataSynthesizer by Ping et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, as literature suggests that this method has the lowest information loss compared to other modulesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. To minimise randomness the parameter for differential privacy is switched off and the degree of the Bayesian network is set to 2 as in the DataSynthesizer examplesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The number of instances that will be created is set to the length of the data set as shown in TableÂ <a href="#S4.T1" title="TABLE I â€£ IV Experimental set-up â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. As with <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">k</annotation></semantics></math>-anonymity we provide the hyperparameters on GitHub.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present the results in the following order: A) The privacy-enhancing processes; B) Energy consumption of the models; C) Accuracy of the models.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Privacy-enhancing processes</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In TableÂ <a href="#S5.T2" title="TABLE II â€£ V-A Privacy-enhancing processes â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> we present the run time and energy cost to obtain the anonymised data. The table shows the deviation for each method compared to the benchmark in percentages. The deviation is computed as an average over 10 individual samples for each method. TableÂ <a href="#S5.T3" title="TABLE III â€£ V-A Privacy-enhancing processes â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> shows the percentage of data that is suppressed after obtaining anonymised data via generalisation and suppression.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The run time and energy consumption for the algorithms used to obtain the anonymised data. The <math id="S5.T2.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T2.2.m1.1b"><mi id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><ci id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">k</annotation></semantics></math>-values specify the value given as input to the ARX algorithm, the synthetic data reports the measurements by the DataSynthesizer.</figcaption>
<table id="S5.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.5.4.1" class="ltx_tr">
<td id="S5.T2.5.4.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S5.T2.5.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S5.T2.5.4.1.2.1" class="ltx_text ltx_font_bold">Adult data set</span></th>
<th id="S5.T2.5.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S5.T2.5.4.1.3.1" class="ltx_text ltx_font_bold">Student performance set</span></th>
</tr>
<tr id="S5.T2.3.1" class="ltx_tr">
<td id="S5.T2.3.1.1" class="ltx_td ltx_align_left">
<math id="S5.T2.3.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T2.3.1.1.m1.1a"><mi id="S5.T2.3.1.1.m1.1.1" xref="S5.T2.3.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.1.m1.1b"><ci id="S5.T2.3.1.1.m1.1.1.cmml" xref="S5.T2.3.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.1.m1.1c">k</annotation></semantics></math>=3</td>
<td id="S5.T2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">32.89s</td>
<td id="S5.T2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">1013.52J</td>
<td id="S5.T2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">33.49s</td>
<td id="S5.T2.3.1.5" class="ltx_td ltx_align_left ltx_border_t">1069.26J</td>
</tr>
<tr id="S5.T2.4.2" class="ltx_tr">
<td id="S5.T2.4.2.1" class="ltx_td ltx_align_left">
<math id="S5.T2.4.2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T2.4.2.1.m1.1a"><mi id="S5.T2.4.2.1.m1.1.1" xref="S5.T2.4.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.1.m1.1b"><ci id="S5.T2.4.2.1.m1.1.1.cmml" xref="S5.T2.4.2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.1.m1.1c">k</annotation></semantics></math>=10</td>
<td id="S5.T2.4.2.2" class="ltx_td ltx_align_center">31.27s</td>
<td id="S5.T2.4.2.3" class="ltx_td ltx_align_center">1011.56J</td>
<td id="S5.T2.4.2.4" class="ltx_td ltx_align_center">33.84s</td>
<td id="S5.T2.4.2.5" class="ltx_td ltx_align_left">1069.09J</td>
</tr>
<tr id="S5.T2.5.3" class="ltx_tr">
<td id="S5.T2.5.3.1" class="ltx_td ltx_align_left">
<math id="S5.T2.5.3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T2.5.3.1.m1.1a"><mi id="S5.T2.5.3.1.m1.1.1" xref="S5.T2.5.3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T2.5.3.1.m1.1b"><ci id="S5.T2.5.3.1.m1.1.1.cmml" xref="S5.T2.5.3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.3.1.m1.1c">k</annotation></semantics></math>=27</td>
<td id="S5.T2.5.3.2" class="ltx_td ltx_align_center">31.31s</td>
<td id="S5.T2.5.3.3" class="ltx_td ltx_align_center">1012.60J</td>
<td id="S5.T2.5.3.4" class="ltx_td ltx_align_center">34.06s</td>
<td id="S5.T2.5.3.5" class="ltx_td ltx_align_left">1074.30J</td>
</tr>
<tr id="S5.T2.5.5.2" class="ltx_tr">
<td id="S5.T2.5.5.2.1" class="ltx_td ltx_align_left ltx_border_bb">Synthetic data</td>
<td id="S5.T2.5.5.2.2" class="ltx_td ltx_align_center ltx_border_bb">67.48s</td>
<td id="S5.T2.5.5.2.3" class="ltx_td ltx_align_center ltx_border_bb">3861.79J</td>
<td id="S5.T2.5.5.2.4" class="ltx_td ltx_align_center ltx_border_bb">72.37s</td>
<td id="S5.T2.5.5.2.5" class="ltx_td ltx_align_left ltx_border_bb">4227.54J</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>The percentage of suppressed data for the Adult set and Student Performance with respect to the obtained <math id="S5.T3.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T3.2.m1.1b"><mi id="S5.T3.2.m1.1.1" xref="S5.T3.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T3.2.m1.1c"><ci id="S5.T3.2.m1.1.1.cmml" xref="S5.T3.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.m1.1d">k</annotation></semantics></math>-anonymity. The percentage is defined as suppressed cells divided by the total amount of cells.</figcaption>
<table id="S5.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.5.4.1" class="ltx_tr">
<th id="S5.T3.5.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="3"><span id="S5.T3.5.4.1.1.1" class="ltx_text ltx_font_bold">Suppressed data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.5.5.1" class="ltx_tr">
<th id="S5.T3.5.5.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T3.5.5.1.2" class="ltx_td ltx_align_center ltx_border_t">Adult set</td>
<td id="S5.T3.5.5.1.3" class="ltx_td ltx_align_center ltx_border_t">Student Performance set</td>
</tr>
<tr id="S5.T3.3.1" class="ltx_tr">
<th id="S5.T3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T3.3.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T3.3.1.1.m1.1a"><mi id="S5.T3.3.1.1.m1.1.1" xref="S5.T3.3.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.m1.1b"><ci id="S5.T3.3.1.1.m1.1.1.cmml" xref="S5.T3.3.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.m1.1c">k</annotation></semantics></math>=3</th>
<td id="S5.T3.3.1.2" class="ltx_td ltx_align_center">19%</td>
<td id="S5.T3.3.1.3" class="ltx_td ltx_align_center">74%</td>
</tr>
<tr id="S5.T3.4.2" class="ltx_tr">
<th id="S5.T3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T3.4.2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T3.4.2.1.m1.1a"><mi id="S5.T3.4.2.1.m1.1.1" xref="S5.T3.4.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T3.4.2.1.m1.1b"><ci id="S5.T3.4.2.1.m1.1.1.cmml" xref="S5.T3.4.2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.2.1.m1.1c">k</annotation></semantics></math>=10</th>
<td id="S5.T3.4.2.2" class="ltx_td ltx_align_center">28%</td>
<td id="S5.T3.4.2.3" class="ltx_td ltx_align_center">80%</td>
</tr>
<tr id="S5.T3.5.3" class="ltx_tr">
<th id="S5.T3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<math id="S5.T3.5.3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T3.5.3.1.m1.1a"><mi id="S5.T3.5.3.1.m1.1.1" xref="S5.T3.5.3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T3.5.3.1.m1.1b"><ci id="S5.T3.5.3.1.m1.1.1.cmml" xref="S5.T3.5.3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.3.1.m1.1c">k</annotation></semantics></math>=27</th>
<td id="S5.T3.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">32%</td>
<td id="S5.T3.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">84%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Energy consumption of training the models</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In TableÂ <a href="#S5.T4" title="TABLE IV â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> we present the results of the energy consumption for the Adult data set on the left and the Student Performance set on the right. The benchmark is presented on the first row, after which the results are shown for the models trained on either anonymised or synthetic data. We present the deviation in percentages to the benchmark for these methods. The machine learning models are abbreviated to increase readability. In FiguresÂ <a href="#S5.F3" title="Figure 3 â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> andÂ <a href="#S5.F4" title="Figure 4 â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we present a scatterplot of the data points for <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">k</annotation></semantics></math>-anonymity and synthetic data respectively. Complementary we provide the results of the Mann Whitney U test in TableÂ <a href="#S5.T5" title="TABLE V â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Lastly, the idle energy consumption of the machine is measured. For each second of inactivity 7.512 Joules is consumed by the machine.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>The average run time and energy consumption for the Adult and Student Performance data sets. On the rows we define the input data and on the columns we define each method, devised into run time and energy consumption. The benchmark resembles the unaltered data as obtained online, the percentages below show the deviation for each method to this benchmark.</figcaption>
<table id="S5.T4.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.6.7.1" class="ltx_tr">
<th id="S5.T4.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="7"><span id="S5.T4.6.7.1.1.1" class="ltx_text ltx_font_bold">Adult data set</span></th>
<th id="S5.T4.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="7"><span id="S5.T4.6.7.1.2.1" class="ltx_text ltx_font_bold">Student Performance set</span></th>
</tr>
<tr id="S5.T4.6.8.2" class="ltx_tr">
<th id="S5.T4.6.8.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S5.T4.6.8.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">knn</th>
<th id="S5.T4.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">LogReg</th>
<th id="S5.T4.6.8.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">NN</th>
<th id="S5.T4.6.8.2.5" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t"></th>
<th id="S5.T4.6.8.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">knn</th>
<th id="S5.T4.6.8.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">LogReg</th>
<th id="S5.T4.6.8.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">NN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.6.9.1" class="ltx_tr">
<th id="S5.T4.6.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Benchmark</th>
<td id="S5.T4.6.9.1.2" class="ltx_td ltx_align_center ltx_border_t">8.19s</td>
<td id="S5.T4.6.9.1.3" class="ltx_td ltx_align_center ltx_border_t">329.33J</td>
<td id="S5.T4.6.9.1.4" class="ltx_td ltx_align_center ltx_border_t">1.41s</td>
<td id="S5.T4.6.9.1.5" class="ltx_td ltx_align_center ltx_border_t">73.70J</td>
<td id="S5.T4.6.9.1.6" class="ltx_td ltx_align_center ltx_border_t">9.85s</td>
<td id="S5.T4.6.9.1.7" class="ltx_td ltx_align_center ltx_border_t">157.59J</td>
<th id="S5.T4.6.9.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_row">Benchmark</th>
<td id="S5.T4.6.9.1.9" class="ltx_td ltx_align_center ltx_border_t">0.04s</td>
<td id="S5.T4.6.9.1.10" class="ltx_td ltx_align_center ltx_border_t">2.16J</td>
<td id="S5.T4.6.9.1.11" class="ltx_td ltx_align_center ltx_border_t">0.06s</td>
<td id="S5.T4.6.9.1.12" class="ltx_td ltx_align_center ltx_border_t">2.95J</td>
<td id="S5.T4.6.9.1.13" class="ltx_td ltx_align_center ltx_border_t">3.40s</td>
<td id="S5.T4.6.9.1.14" class="ltx_td ltx_align_center ltx_border_t">61.20J</td>
</tr>
<tr id="S5.T4.2.2" class="ltx_tr">
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mi id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">k</annotation></semantics></math>=3</th>
<td id="S5.T4.2.2.3" class="ltx_td ltx_align_center">-67%</td>
<td id="S5.T4.2.2.4" class="ltx_td ltx_align_center">-67%</td>
<td id="S5.T4.2.2.5" class="ltx_td ltx_align_center">-45%</td>
<td id="S5.T4.2.2.6" class="ltx_td ltx_align_center">-46%</td>
<td id="S5.T4.2.2.7" class="ltx_td ltx_align_center">-44%</td>
<td id="S5.T4.2.2.8" class="ltx_td ltx_align_center">-42%</td>
<th id="S5.T4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><mi id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">k</annotation></semantics></math>=3</th>
<td id="S5.T4.2.2.9" class="ltx_td ltx_align_center">-25%</td>
<td id="S5.T4.2.2.10" class="ltx_td ltx_align_center">-37%</td>
<td id="S5.T4.2.2.11" class="ltx_td ltx_align_center">-66%</td>
<td id="S5.T4.2.2.12" class="ltx_td ltx_align_center">-77%</td>
<td id="S5.T4.2.2.13" class="ltx_td ltx_align_center">-26%</td>
<td id="S5.T4.2.2.14" class="ltx_td ltx_align_center">-24%</td>
</tr>
<tr id="S5.T4.4.4" class="ltx_tr">
<th id="S5.T4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T4.3.3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T4.3.3.1.m1.1a"><mi id="S5.T4.3.3.1.m1.1.1" xref="S5.T4.3.3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.1.m1.1b"><ci id="S5.T4.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.1.m1.1c">k</annotation></semantics></math>=10</th>
<td id="S5.T4.4.4.3" class="ltx_td ltx_align_center">-69%</td>
<td id="S5.T4.4.4.4" class="ltx_td ltx_align_center">-68%</td>
<td id="S5.T4.4.4.5" class="ltx_td ltx_align_center">-61%</td>
<td id="S5.T4.4.4.6" class="ltx_td ltx_align_center">-62%</td>
<td id="S5.T4.4.4.7" class="ltx_td ltx_align_center">-50%</td>
<td id="S5.T4.4.4.8" class="ltx_td ltx_align_center">-47%</td>
<th id="S5.T4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T4.4.4.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T4.4.4.2.m1.1a"><mi id="S5.T4.4.4.2.m1.1.1" xref="S5.T4.4.4.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.2.m1.1b"><ci id="S5.T4.4.4.2.m1.1.1.cmml" xref="S5.T4.4.4.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.2.m1.1c">k</annotation></semantics></math>=10</th>
<td id="S5.T4.4.4.9" class="ltx_td ltx_align_center">-75%</td>
<td id="S5.T4.4.4.10" class="ltx_td ltx_align_center">-79%</td>
<td id="S5.T4.4.4.11" class="ltx_td ltx_align_center">-66%</td>
<td id="S5.T4.4.4.12" class="ltx_td ltx_align_center">-78%</td>
<td id="S5.T4.4.4.13" class="ltx_td ltx_align_center">-28%</td>
<td id="S5.T4.4.4.14" class="ltx_td ltx_align_center">-27%</td>
</tr>
<tr id="S5.T4.6.6" class="ltx_tr">
<th id="S5.T4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T4.5.5.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T4.5.5.1.m1.1a"><mi id="S5.T4.5.5.1.m1.1.1" xref="S5.T4.5.5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.1.m1.1b"><ci id="S5.T4.5.5.1.m1.1.1.cmml" xref="S5.T4.5.5.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.1.m1.1c">k</annotation></semantics></math>=27</th>
<td id="S5.T4.6.6.3" class="ltx_td ltx_align_center">-73%</td>
<td id="S5.T4.6.6.4" class="ltx_td ltx_align_center">-73%</td>
<td id="S5.T4.6.6.5" class="ltx_td ltx_align_center">-78%</td>
<td id="S5.T4.6.6.6" class="ltx_td ltx_align_center">-78%</td>
<td id="S5.T4.6.6.7" class="ltx_td ltx_align_center">-54%</td>
<td id="S5.T4.6.6.8" class="ltx_td ltx_align_center">-51%</td>
<th id="S5.T4.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T4.6.6.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T4.6.6.2.m1.1a"><mi id="S5.T4.6.6.2.m1.1.1" xref="S5.T4.6.6.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.2.m1.1b"><ci id="S5.T4.6.6.2.m1.1.1.cmml" xref="S5.T4.6.6.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.2.m1.1c">k</annotation></semantics></math>=27</th>
<td id="S5.T4.6.6.9" class="ltx_td ltx_align_center">-75%</td>
<td id="S5.T4.6.6.10" class="ltx_td ltx_align_center">-80%</td>
<td id="S5.T4.6.6.11" class="ltx_td ltx_align_center">-66%</td>
<td id="S5.T4.6.6.12" class="ltx_td ltx_align_center">-79%</td>
<td id="S5.T4.6.6.13" class="ltx_td ltx_align_center">-29%</td>
<td id="S5.T4.6.6.14" class="ltx_td ltx_align_center">-26%</td>
</tr>
<tr id="S5.T4.6.10.2" class="ltx_tr">
<th id="S5.T4.6.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Synthetic data</th>
<td id="S5.T4.6.10.2.2" class="ltx_td ltx_align_center ltx_border_bb">-5%</td>
<td id="S5.T4.6.10.2.3" class="ltx_td ltx_align_center ltx_border_bb">-4%</td>
<td id="S5.T4.6.10.2.4" class="ltx_td ltx_align_center ltx_border_bb">-9%</td>
<td id="S5.T4.6.10.2.5" class="ltx_td ltx_align_center ltx_border_bb">-10%</td>
<td id="S5.T4.6.10.2.6" class="ltx_td ltx_align_center ltx_border_bb">-1%</td>
<td id="S5.T4.6.10.2.7" class="ltx_td ltx_align_center ltx_border_bb">0%</td>
<th id="S5.T4.6.10.2.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Synthetic data</th>
<td id="S5.T4.6.10.2.9" class="ltx_td ltx_align_center ltx_border_bb">0%</td>
<td id="S5.T4.6.10.2.10" class="ltx_td ltx_align_center ltx_border_bb">+2%</td>
<td id="S5.T4.6.10.2.11" class="ltx_td ltx_align_center ltx_border_bb">-33%</td>
<td id="S5.T4.6.10.2.12" class="ltx_td ltx_align_center ltx_border_bb">+3%</td>
<td id="S5.T4.6.10.2.13" class="ltx_td ltx_align_center ltx_border_bb">-1%</td>
<td id="S5.T4.6.10.2.14" class="ltx_td ltx_align_center ltx_border_bb">-2%</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2305.07116/assets/Images/kanon.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Scatter plot of the energy consumption in Joules and duration in seconds for the models trained on <math id="S5.F3.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.F3.2.m1.1b"><mi id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.1c"><ci id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.1d">k</annotation></semantics></math>-anonymity. The left plot shows the results of the models trained on the Adult set, the right plot shows the results of models trained on the Student Performance set.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2305.07116/assets/Images/SynthData.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Scatter plot of the energy consumption in Joules and duration in seconds for the models trained on synthetic data. The left plot shows the results of the models trained on the Adult set, the right plot shows the results of models trained on the Student Performance set.</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>The results of the Mann Whitney U test applied to the energy consumption data. The presented p-value shows the probability of the method on the row being greater than the alternative in the column.</figcaption>
<table id="S5.T5.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.12.13.1" class="ltx_tr">
<th id="S5.T5.12.13.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="5"><span id="S5.T5.12.13.1.1.1" class="ltx_text ltx_font_bold">Adult data set</span></th>
<th id="S5.T5.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="4"><span id="S5.T5.12.13.1.2.1" class="ltx_text ltx_font_bold">Student Performance set</span></th>
<td id="S5.T5.12.13.1.3" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T5.6.6" class="ltx_tr">
<th id="S5.T5.6.6.7" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T5.1.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.1.1.1.m1.1a"><mi id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">k</annotation></semantics></math>=3</td>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T5.2.2.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.2.2.2.m1.1a"><mi id="S5.T5.2.2.2.m1.1.1" xref="S5.T5.2.2.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.m1.1b"><ci id="S5.T5.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.m1.1c">k</annotation></semantics></math>=10</td>
<td id="S5.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T5.3.3.3.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.3.3.3.m1.1a"><mi id="S5.T5.3.3.3.m1.1.1" xref="S5.T5.3.3.3.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.m1.1b"><ci id="S5.T5.3.3.3.m1.1.1.cmml" xref="S5.T5.3.3.3.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.m1.1c">k</annotation></semantics></math>=27</td>
<td id="S5.T5.6.6.8" class="ltx_td ltx_align_center ltx_border_t">Synthetic data</td>
<th id="S5.T5.6.6.9" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S5.T5.4.4.4" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T5.4.4.4.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.4.4.4.m1.1a"><mi id="S5.T5.4.4.4.m1.1.1" xref="S5.T5.4.4.4.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.m1.1b"><ci id="S5.T5.4.4.4.m1.1.1.cmml" xref="S5.T5.4.4.4.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.m1.1c">k</annotation></semantics></math>=3</td>
<td id="S5.T5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T5.5.5.5.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.5.5.5.m1.1a"><mi id="S5.T5.5.5.5.m1.1.1" xref="S5.T5.5.5.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.m1.1b"><ci id="S5.T5.5.5.5.m1.1.1.cmml" xref="S5.T5.5.5.5.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.m1.1c">k</annotation></semantics></math>=10</td>
<td id="S5.T5.6.6.6" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T5.6.6.6.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.6.6.6.m1.1a"><mi id="S5.T5.6.6.6.m1.1.1" xref="S5.T5.6.6.6.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.m1.1b"><ci id="S5.T5.6.6.6.m1.1.1.cmml" xref="S5.T5.6.6.6.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.m1.1c">k</annotation></semantics></math>=27</td>
<td id="S5.T5.6.6.10" class="ltx_td ltx_align_center ltx_border_t">Synthetic data</td>
</tr>
<tr id="S5.T5.8.8" class="ltx_tr">
<th id="S5.T5.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T5.7.7.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.7.7.1.m1.1a"><mi id="S5.T5.7.7.1.m1.1.1" xref="S5.T5.7.7.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.1.m1.1b"><ci id="S5.T5.7.7.1.m1.1.1.cmml" xref="S5.T5.7.7.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.1.m1.1c">k</annotation></semantics></math>=3</th>
<td id="S5.T5.8.8.3" class="ltx_td ltx_align_center">x</td>
<td id="S5.T5.8.8.4" class="ltx_td ltx_align_center">p &lt;0.01</td>
<td id="S5.T5.8.8.5" class="ltx_td ltx_align_center">p &lt;0.01</td>
<td id="S5.T5.8.8.6" class="ltx_td ltx_align_center">p &gt;0.99</td>
<th id="S5.T5.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T5.8.8.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.8.8.2.m1.1a"><mi id="S5.T5.8.8.2.m1.1.1" xref="S5.T5.8.8.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.2.m1.1b"><ci id="S5.T5.8.8.2.m1.1.1.cmml" xref="S5.T5.8.8.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.2.m1.1c">k</annotation></semantics></math>=3</th>
<td id="S5.T5.8.8.7" class="ltx_td ltx_align_center">x</td>
<td id="S5.T5.8.8.8" class="ltx_td ltx_align_center">p &lt;0.01</td>
<td id="S5.T5.8.8.9" class="ltx_td ltx_align_center">p &lt;0.01</td>
<td id="S5.T5.8.8.10" class="ltx_td ltx_align_center">p &gt;0.99</td>
</tr>
<tr id="S5.T5.10.10" class="ltx_tr">
<th id="S5.T5.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T5.9.9.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.9.9.1.m1.1a"><mi id="S5.T5.9.9.1.m1.1.1" xref="S5.T5.9.9.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.1.m1.1b"><ci id="S5.T5.9.9.1.m1.1.1.cmml" xref="S5.T5.9.9.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.1.m1.1c">k</annotation></semantics></math>=10</th>
<td id="S5.T5.10.10.3" class="ltx_td ltx_align_center">p &gt;0.99</td>
<td id="S5.T5.10.10.4" class="ltx_td ltx_align_center">x</td>
<td id="S5.T5.10.10.5" class="ltx_td ltx_align_center">p &lt;0.01</td>
<td id="S5.T5.10.10.6" class="ltx_td ltx_align_center">p &gt;0.99</td>
<th id="S5.T5.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T5.10.10.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.10.10.2.m1.1a"><mi id="S5.T5.10.10.2.m1.1.1" xref="S5.T5.10.10.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.10.10.2.m1.1b"><ci id="S5.T5.10.10.2.m1.1.1.cmml" xref="S5.T5.10.10.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.10.2.m1.1c">k</annotation></semantics></math>=10</th>
<td id="S5.T5.10.10.7" class="ltx_td ltx_align_center">p &gt;0.99</td>
<td id="S5.T5.10.10.8" class="ltx_td ltx_align_center">x</td>
<td id="S5.T5.10.10.9" class="ltx_td ltx_align_center">p &lt;0.01</td>
<td id="S5.T5.10.10.10" class="ltx_td ltx_align_center">p &gt;0.99</td>
</tr>
<tr id="S5.T5.12.12" class="ltx_tr">
<th id="S5.T5.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T5.11.11.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.11.11.1.m1.1a"><mi id="S5.T5.11.11.1.m1.1.1" xref="S5.T5.11.11.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.11.11.1.m1.1b"><ci id="S5.T5.11.11.1.m1.1.1.cmml" xref="S5.T5.11.11.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.11.1.m1.1c">k</annotation></semantics></math>=27</th>
<td id="S5.T5.12.12.3" class="ltx_td ltx_align_center">p &gt;0.99</td>
<td id="S5.T5.12.12.4" class="ltx_td ltx_align_center">p &gt;0.99</td>
<td id="S5.T5.12.12.5" class="ltx_td ltx_align_center">x</td>
<td id="S5.T5.12.12.6" class="ltx_td ltx_align_center">p &gt;0.99</td>
<th id="S5.T5.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T5.12.12.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T5.12.12.2.m1.1a"><mi id="S5.T5.12.12.2.m1.1.1" xref="S5.T5.12.12.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T5.12.12.2.m1.1b"><ci id="S5.T5.12.12.2.m1.1.1.cmml" xref="S5.T5.12.12.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.12.2.m1.1c">k</annotation></semantics></math>=27</th>
<td id="S5.T5.12.12.7" class="ltx_td ltx_align_center">p &gt;0.99</td>
<td id="S5.T5.12.12.8" class="ltx_td ltx_align_center">p &gt;0.99</td>
<td id="S5.T5.12.12.9" class="ltx_td ltx_align_center">x</td>
<td id="S5.T5.12.12.10" class="ltx_td ltx_align_center">p &gt;0.99</td>
</tr>
<tr id="S5.T5.12.14.2" class="ltx_tr">
<th id="S5.T5.12.14.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Synthetic</th>
<td id="S5.T5.12.14.2.2" class="ltx_td ltx_align_center ltx_border_bb">p &lt;0.01</td>
<td id="S5.T5.12.14.2.3" class="ltx_td ltx_align_center ltx_border_bb">p &lt;0.01</td>
<td id="S5.T5.12.14.2.4" class="ltx_td ltx_align_center ltx_border_bb">p &lt;0.01</td>
<td id="S5.T5.12.14.2.5" class="ltx_td ltx_align_center ltx_border_bb">x</td>
<th id="S5.T5.12.14.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Synthetic</th>
<td id="S5.T5.12.14.2.7" class="ltx_td ltx_align_center ltx_border_bb">p &lt;0.01</td>
<td id="S5.T5.12.14.2.8" class="ltx_td ltx_align_center ltx_border_bb">p &lt;0.01</td>
<td id="S5.T5.12.14.2.9" class="ltx_td ltx_align_center ltx_border_bb">p &lt;0.01</td>
<td id="S5.T5.12.14.2.10" class="ltx_td ltx_align_center ltx_border_bb">x</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Accuracy of the models</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">TableÂ <a href="#S5.T6" title="TABLE VI â€£ V-C Accuracy of the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the accuracy score of the machine learning techniques for each data set and the privacy-enhancing technique. The benchmark reflects the accuracy scores for the unmodified or original data. To reflect the accuracies as accurate as possible we present the accuracy scores instead of the deviation to the benchmark.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>The weighted accuracy over 10 measurements for each data set, machine learning technique and privacy-enhancing technique.</figcaption>
<table id="S5.T6.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.3.4.1" class="ltx_tr">
<th id="S5.T6.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T6.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S5.T6.3.4.1.2.1" class="ltx_text ltx_font_bold">Adult data set</span></th>
<th id="S5.T6.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S5.T6.3.4.1.3.1" class="ltx_text ltx_font_bold">Student Performance set</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.3.5.1" class="ltx_tr">
<th id="S5.T6.3.5.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T6.3.5.1.2" class="ltx_td ltx_align_center ltx_border_t">knn</td>
<td id="S5.T6.3.5.1.3" class="ltx_td ltx_align_center ltx_border_t">LogReg</td>
<td id="S5.T6.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t">NN</td>
<td id="S5.T6.3.5.1.5" class="ltx_td ltx_align_center ltx_border_t">knn</td>
<td id="S5.T6.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t">LogReg</td>
<td id="S5.T6.3.5.1.7" class="ltx_td ltx_align_center ltx_border_t">NN</td>
</tr>
<tr id="S5.T6.3.6.2" class="ltx_tr">
<th id="S5.T6.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Benchmark</th>
<td id="S5.T6.3.6.2.2" class="ltx_td ltx_align_center">0.820</td>
<td id="S5.T6.3.6.2.3" class="ltx_td ltx_align_center">0.846</td>
<td id="S5.T6.3.6.2.4" class="ltx_td ltx_align_center">0.846</td>
<td id="S5.T6.3.6.2.5" class="ltx_td ltx_align_center">0.700</td>
<td id="S5.T6.3.6.2.6" class="ltx_td ltx_align_center">0.719</td>
<td id="S5.T6.3.6.2.7" class="ltx_td ltx_align_center">0.706</td>
</tr>
<tr id="S5.T6.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T6.1.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T6.1.1.1.m1.1a"><mi id="S5.T6.1.1.1.m1.1.1" xref="S5.T6.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.m1.1c">k</annotation></semantics></math>=3</th>
<td id="S5.T6.1.1.2" class="ltx_td ltx_align_center">0.828</td>
<td id="S5.T6.1.1.3" class="ltx_td ltx_align_center">0.848</td>
<td id="S5.T6.1.1.4" class="ltx_td ltx_align_center">0.847</td>
<td id="S5.T6.1.1.5" class="ltx_td ltx_align_center">0.872</td>
<td id="S5.T6.1.1.6" class="ltx_td ltx_align_center">0.851</td>
<td id="S5.T6.1.1.7" class="ltx_td ltx_align_center">0.861</td>
</tr>
<tr id="S5.T6.2.2" class="ltx_tr">
<th id="S5.T6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T6.2.2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T6.2.2.1.m1.1a"><mi id="S5.T6.2.2.1.m1.1.1" xref="S5.T6.2.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.1.m1.1b"><ci id="S5.T6.2.2.1.m1.1.1.cmml" xref="S5.T6.2.2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.1.m1.1c">k</annotation></semantics></math>=10</th>
<td id="S5.T6.2.2.2" class="ltx_td ltx_align_center">0.832</td>
<td id="S5.T6.2.2.3" class="ltx_td ltx_align_center">0.842</td>
<td id="S5.T6.2.2.4" class="ltx_td ltx_align_center">0.842</td>
<td id="S5.T6.2.2.5" class="ltx_td ltx_align_center">0.826</td>
<td id="S5.T6.2.2.6" class="ltx_td ltx_align_center">0.826</td>
<td id="S5.T6.2.2.7" class="ltx_td ltx_align_center">0.826</td>
</tr>
<tr id="S5.T6.3.3" class="ltx_tr">
<th id="S5.T6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S5.T6.3.3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T6.3.3.1.m1.1a"><mi id="S5.T6.3.3.1.m1.1.1" xref="S5.T6.3.3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.1.m1.1b"><ci id="S5.T6.3.3.1.m1.1.1.cmml" xref="S5.T6.3.3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.1.m1.1c">k</annotation></semantics></math>=27</th>
<td id="S5.T6.3.3.2" class="ltx_td ltx_align_center">0.828</td>
<td id="S5.T6.3.3.3" class="ltx_td ltx_align_center">0.837</td>
<td id="S5.T6.3.3.4" class="ltx_td ltx_align_center">0.837</td>
<td id="S5.T6.3.3.5" class="ltx_td ltx_align_center">0.831</td>
<td id="S5.T6.3.3.6" class="ltx_td ltx_align_center">0.853</td>
<td id="S5.T6.3.3.7" class="ltx_td ltx_align_center">0.853</td>
</tr>
<tr id="S5.T6.3.7.3" class="ltx_tr">
<th id="S5.T6.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Synthetic data</th>
<td id="S5.T6.3.7.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.802</td>
<td id="S5.T6.3.7.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.828</td>
<td id="S5.T6.3.7.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.828</td>
<td id="S5.T6.3.7.3.5" class="ltx_td ltx_align_center ltx_border_bb">0.728</td>
<td id="S5.T6.3.7.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.756</td>
<td id="S5.T6.3.7.3.7" class="ltx_td ltx_align_center ltx_border_bb">0.755</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">First we will validate the presented benchmarks by comparing the results to related work. Then we discuss the results of the anonymisation process itself by looking at the energy consumption and suppression rate. Next we analyse (<span id="S6.p1.1.1" class="ltx_text ltx_font_bold">RQ1</span>): the accuracy of the machine learning models trained on anonymised or synthetic data, after which we discuss (<span id="S6.p1.1.2" class="ltx_text ltx_font_bold">RQ2</span>): the energy consumption of our machine learning models. Finally, we compare the results of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.p1.1.m1.1a"><mi id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">k</annotation></semantics></math>-anonymity with the results of synthetic data to formulate some guidelines on when to use which method.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Validation of the benchmark</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In this section we compare the accuracy and energy consumption of our presented benchmark in TableÂ <a href="#S5.T6" title="TABLE VI â€£ V-C Accuracy of the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> to the accuracy presented in other work. Since the goal of this paper is to examine the effect of anonymised data or synthetic data compared to original data, small differences in regard to other literature are deemed acceptable.</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS1.SSS1.4.1.1" class="ltx_text">VI-A</span>1 </span><span id="S6.SS1.SSS1.5.2" class="ltx_text ltx_font_bold">Adult data set</span>
</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">Starting with the accuracies for the Adult set, the k-nearest neighbours and logistic regression models outperform related workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> with 0.6% and 1.4% respectively. The neural network has an accuracy that is 1.0% lower compared to related workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Since a neural network can have many hyperparameters that influence the accuracy we deem this difference acceptable.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS1.SSS2.4.1.1" class="ltx_text">VI-A</span>2 </span><span id="S6.SS1.SSS2.5.2" class="ltx_text ltx_font_bold">Student Performance set</span>
</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">Literature suggests that for k-nearest neighbours and logistic regression accuracies of 71.0% and 76.0% may be expected. TableÂ <a href="#S5.T6" title="TABLE VI â€£ V-C Accuracy of the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows that our models deviate 1.0% and 4.1% respectively. As mentioned earlier, the focus of this paper lies on the difference between the benchmark and anonymised or synthetic data and thus we deem this difference acceptable. For the neural network, no work was found that used all columns for this task. The original paper reports an accuracy of 83.4% but that included a larger network and hyperparameters that were optimised using grid searchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
<br class="ltx_break"></p>
</div>
<div id="S6.SS1.SSS2.p2" class="ltx_para">
<p id="S6.SS1.SSS2.p2.2" class="ltx_p">Finally, for the energy consumption. In general logistic regression is computationally efficientÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which should result in a lower energy consumption. For both sets TableÂ <a href="#S5.T4" title="TABLE IV â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows that logistic regression consumes the least amount of energy compared to the other machine learning methods. Contrary to this finding, k-nearest neighbours becomes computationally more complex as the size of the data growsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. TableÂ <a href="#S5.T4" title="TABLE IV â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows that indeed for the Adult set the k-nearest neighbours algorithm used <math id="S6.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.SSS2.p2.1.m1.1a"><mo id="S6.SS1.SSS2.p2.1.m1.1.1" xref="S6.SS1.SSS2.p2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.1.m1.1c">\sim</annotation></semantics></math>152 times more energy while the Adult set is just <math id="S6.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.SSS2.p2.2.m2.1a"><mo id="S6.SS1.SSS2.p2.2.m2.1.1" xref="S6.SS1.SSS2.p2.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.2.m2.1b"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.2.m2.1c">\sim</annotation></semantics></math>17 times larger than the Student Performance set, confirming this statement. The energy consumption of neural network depends on many parametersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, which makes it hard to compare the baseline to literature.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Creating k-anonymous and synthetic data</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In TableÂ <a href="#S5.T2" title="TABLE II â€£ V-A Privacy-enhancing processes â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> we observe that creating synthetic data requires a higher run time than anonymising to obtain <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mi id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><ci id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">k</annotation></semantics></math>-anonymity. In accordance with these findings we observe that the energy consumption is nearly 4 times higher for synthetic data. This is expected as the DataSynthesizer requires more steps to come to the new data than the ARX algorithm. We also find that the DataSynthesizer required more time for the Student Performance set than for the Adult data set, even though the Adult set contains more data as a whole. This might be explained by the DataSynthesizer having to analyse all features to create their respective probability distribution. Since the Student Performance set contains more attributes, as seen in TableÂ <a href="#S4.T1" title="TABLE I â€£ IV Experimental set-up â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we hypothesise that this leads to more required run time for the Student Performance set.
<br class="ltx_break"></p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">An important note is that the privacy-enhancement process, either via generalisation and suppression or via synthetic data, is a marginal cost. The process requires only one run and then the data is anonymised and may be published. Running algorithms on this data might be done repeatedly by different users, resulting in an initial energy cost which is shared amongst users.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic">Accuracy of the models</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">For the accuracy of the models we divide our discussion in two parts; one component for the Adult set and another component for the Student Performance set. We study TableÂ <a href="#S5.T6" title="TABLE VI â€£ V-C Accuracy of the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> in this section.</p>
</div>
<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS3.SSS1.4.1.1" class="ltx_text">VI-C</span>1 </span><span id="S6.SS3.SSS1.5.2" class="ltx_text ltx_font_bold">Adult data set</span>
</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.3" class="ltx_p">First of all, the amount of suppressed data. We see that for the larger Adult set the percentage of suppressed data lies between 19%-32%. For the Student Performance set the amount of suppressed data lies between 74% - 83%. As expected the smaller set is more sensitive to suppression than the larger set. Though we did not define suppression as information loss, it is an interesting finding.
Secondly, when we look at the accuracies and compare the synthetic data to the benchmark we see a minor deviation of about -1.8% accuracy for all machine learning methods. For the anonymised data using ARX these findings are different. The k-nearest neighbours algorithm actually obtains a higher accuracy for anonymised data than for the benchmark, with a maximum increase of 1.2% for a <math id="S6.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS1.p1.1.m1.1a"><mi id="S6.SS3.SSS1.p1.1.m1.1.1" xref="S6.SS3.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.1.m1.1b"><ci id="S6.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.1.m1.1c">k</annotation></semantics></math> of 10. For other <math id="S6.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS1.p1.2.m2.1a"><mi id="S6.SS3.SSS1.p1.2.m2.1.1" xref="S6.SS3.SSS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.2.m2.1b"><ci id="S6.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS1.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.2.m2.1c">k</annotation></semantics></math>-values the increase is 0.8% on the k-nearest neighbours algorithm. For both logistic regression and the neural network the accuracy tends to be similar (<math id="S6.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS1.p1.3.m3.1a"><mi id="S6.SS3.SSS1.p1.3.m3.1.1" xref="S6.SS3.SSS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.3.m3.1b"><ci id="S6.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S6.SS3.SSS1.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.3.m3.1c">k</annotation></semantics></math>=3) or a little lower with a maximum decrease of 0.9%. We find a close resemblance between the accuracy of logistic regression and that of the neural network. Other literature finds a similar pattern for logistic regression and binary neural networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
<br class="ltx_break"></p>
</div>
</section>
<section id="S6.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS3.SSS2.4.1.1" class="ltx_text">VI-C</span>2 </span><span id="S6.SS3.SSS2.5.2" class="ltx_text ltx_font_bold">Student Performance set</span>
</h4>

<div id="S6.SS3.SSS2.p1" class="ltx_para">
<p id="S6.SS3.SSS2.p1.3" class="ltx_p">In the case of the models trained on the Student Performance set, the models trained on synthetic data have higher accuracy compared to models trained on the benchmark. The increase in accuracy ranges between 2.8% for k-nearest neighbours and 4.9% for the neural network. Other work suggests that this could be explained by clustering of synthetic dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. However, this increase is rather low compared to the accuracy of the models trained on anonymised data using <math id="S6.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS2.p1.1.m1.1a"><mi id="S6.SS3.SSS2.p1.1.m1.1.1" xref="S6.SS3.SSS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p1.1.m1.1b"><ci id="S6.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p1.1.m1.1c">k</annotation></semantics></math>-anonymity. With increases up to 17.2% the models trained with <math id="S6.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS2.p1.2.m2.1a"><mi id="S6.SS3.SSS2.p1.2.m2.1.1" xref="S6.SS3.SSS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p1.2.m2.1b"><ci id="S6.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS2.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p1.2.m2.1c">k</annotation></semantics></math>-anonymity outperform the benchmark and synthetic data, regardless of the machine learning method. We tend to see the accuracy decrease as the value of <math id="S6.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS2.p1.3.m3.1a"><mi id="S6.SS3.SSS2.p1.3.m3.1.1" xref="S6.SS3.SSS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p1.3.m3.1b"><ci id="S6.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS3.SSS2.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p1.3.m3.1c">k</annotation></semantics></math> increases.
<br class="ltx_break"></p>
</div>
<div id="S6.SS3.SSS2.p2" class="ltx_para">
<p id="S6.SS3.SSS2.p2.1" class="ltx_p">The k-anonymised data sets improved the accuracy compared to the benchmark, probably due to a preselection of the data but this should be examined in future work. Apart from extending this work to more data sets to see if these findings endure, an interesting idea for further research would be to actively use feature selection in the anonymisation process. One could think of suppressing the columns with the least importance to the prediction task and then continue suppressing more important features if the set <math id="S6.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS2.p2.1.m1.1a"><mi id="S6.SS3.SSS2.p2.1.m1.1.1" xref="S6.SS3.SSS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p2.1.m1.1b"><ci id="S6.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p2.1.m1.1c">k</annotation></semantics></math>-value requires so. This could lead to increased privacy with the least possible information loss.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS4.4.1.1" class="ltx_text">VI-D</span> </span><span id="S6.SS4.5.2" class="ltx_text ltx_font_italic">Energy consumption of training the ML models</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">To discuss the energy consumption of the models trained on data with <math id="S6.SS4.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.p1.1.m1.1a"><mi id="S6.SS4.p1.1.m1.1.1" xref="S6.SS4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.m1.1b"><ci id="S6.SS4.p1.1.m1.1.1.cmml" xref="S6.SS4.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.m1.1c">k</annotation></semantics></math>-anonymity or on synthetic data, we again split our discussion into two parts; one for each data set. After analysis of both data sets we will try to generalise our findings. An important remark is that the presented values should be seen as indications of energy consumption. Their individual value should be related to other values to obtain a respective score.</p>
</div>
<section id="S6.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS4.SSS1.4.1.1" class="ltx_text">VI-D</span>1 </span><span id="S6.SS4.SSS1.5.2" class="ltx_text ltx_font_bold">Adult data set</span>
</h4>

<div id="S6.SS4.SSS1.p1" class="ltx_para">
<p id="S6.SS4.SSS1.p1.9" class="ltx_p">For the energy consumption of models trained on the Adult data set, the left-side of TableÂ <a href="#S5.T4" title="TABLE IV â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows us that the synthetic data lies in close range compared to the benchmark. This indicates that the DataSynthesizer has similar characteristics to the original data. For the models trained on anonymised data using <math id="S6.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.1.m1.1a"><mi id="S6.SS4.SSS1.p1.1.m1.1.1" xref="S6.SS4.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.1.m1.1b"><ci id="S6.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS4.SSS1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.1.m1.1c">k</annotation></semantics></math>-anonymity we see a clear deviation from the benchmark. Depending on the chosen <math id="S6.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.2.m2.1a"><mi id="S6.SS4.SSS1.p1.2.m2.1.1" xref="S6.SS4.SSS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.2.m2.1b"><ci id="S6.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S6.SS4.SSS1.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.2.m2.1c">k</annotation></semantics></math>-value, the run time and energy consumption decrease. Similar to other work, we find that the run time and energy consumption decrease as we increase our <math id="S6.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.3.m3.1a"><mi id="S6.SS4.SSS1.p1.3.m3.1.1" xref="S6.SS4.SSS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.3.m3.1b"><ci id="S6.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S6.SS4.SSS1.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.3.m3.1c">k</annotation></semantics></math>-valueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Additionally, FigureÂ <a href="#S5.F3" title="Figure 3 â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the measurements for models trained using <math id="S6.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.4.m4.1a"><mi id="S6.SS4.SSS1.p1.4.m4.1.1" xref="S6.SS4.SSS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.4.m4.1b"><ci id="S6.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S6.SS4.SSS1.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.4.m4.1c">k</annotation></semantics></math>-anonymity are closely clustered around their mean. This indicates that there is little variation amongst the data. We see a similar trend in FigureÂ <a href="#S5.F4" title="Figure 4 â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for the models trained on synthetic data. Affirmative with these findings are the results of the Mann Whitney U test, presented in TableÂ <a href="#S5.T5" title="TABLE V â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. The outcomes in this table show that the probability of the higher <math id="S6.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.5.m5.1a"><mi id="S6.SS4.SSS1.p1.5.m5.1.1" xref="S6.SS4.SSS1.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.5.m5.1b"><ci id="S6.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S6.SS4.SSS1.p1.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.5.m5.1c">k</annotation></semantics></math>-values resulting in a higher energy cost are less than 0.01. Hence we can reject this hypothesis and confirm that the higher <math id="S6.SS4.SSS1.p1.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.6.m6.1a"><mi id="S6.SS4.SSS1.p1.6.m6.1.1" xref="S6.SS4.SSS1.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.6.m6.1b"><ci id="S6.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S6.SS4.SSS1.p1.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.6.m6.1c">k</annotation></semantics></math>-values lead to a lower energy consumption. Finally we see that the probability for synthetic data consuming more energy compared to any <math id="S6.SS4.SSS1.p1.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.7.m7.1a"><mi id="S6.SS4.SSS1.p1.7.m7.1.1" xref="S6.SS4.SSS1.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.7.m7.1b"><ci id="S6.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S6.SS4.SSS1.p1.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.7.m7.1c">k</annotation></semantics></math>-value is <math id="S6.SS4.SSS1.p1.8.m8.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S6.SS4.SSS1.p1.8.m8.1a"><mo id="S6.SS4.SSS1.p1.8.m8.1.1" xref="S6.SS4.SSS1.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.8.m8.1b"><gt id="S6.SS4.SSS1.p1.8.m8.1.1.cmml" xref="S6.SS4.SSS1.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.8.m8.1c">&gt;</annotation></semantics></math>Â 0.99. So we may conclude that models trained on synthetic data consume significantly more energy than the same models trained on anonymised data using <math id="S6.SS4.SSS1.p1.9.m9.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS1.p1.9.m9.1a"><mi id="S6.SS4.SSS1.p1.9.m9.1.1" xref="S6.SS4.SSS1.p1.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS1.p1.9.m9.1b"><ci id="S6.SS4.SSS1.p1.9.m9.1.1.cmml" xref="S6.SS4.SSS1.p1.9.m9.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS1.p1.9.m9.1c">k</annotation></semantics></math>-anonymity.
<br class="ltx_break"></p>
</div>
</section>
<section id="S6.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS4.SSS2.4.1.1" class="ltx_text">VI-D</span>2 </span><span id="S6.SS4.SSS2.5.2" class="ltx_text ltx_font_bold">Student Performance set</span>
</h4>

<div id="S6.SS4.SSS2.p1" class="ltx_para">
<p id="S6.SS4.SSS2.p1.3" class="ltx_p">When we look at the right-side of TableÂ <a href="#S5.T4" title="TABLE IV â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and compare it to our findings on the Adult set, we find that the smaller Student Performance set also has a lower run time and related lower energy consumption. This finding holds for the benchmark as well as for the anonymised and synthetic data. Similar to the Adult data, we observe that the synthetic data shows close resemblance to the benchmark. This similarity extends to the anonymised data, where we see both the run time and energy consumption decrease as the chosen <math id="S6.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS2.p1.1.m1.1a"><mi id="S6.SS4.SSS2.p1.1.m1.1.1" xref="S6.SS4.SSS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.1.m1.1b"><ci id="S6.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS4.SSS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.1.m1.1c">k</annotation></semantics></math>-value increases. In addition, FiguresÂ <a href="#S5.F3" title="Figure 3 â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> andÂ <a href="#S5.F4" title="Figure 4 â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> clearly show that the data points are clustered with little variation around their mean. As with the Adult data set, we see in TableÂ <a href="#S5.T5" title="TABLE V â€£ V-B Energy consumption of training the models â€£ V Results â€£ Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> that the probability of higher <math id="S6.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS2.p1.2.m2.1a"><mi id="S6.SS4.SSS2.p1.2.m2.1.1" xref="S6.SS4.SSS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.2.m2.1b"><ci id="S6.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS4.SSS2.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.2.m2.1c">k</annotation></semantics></math>-values resulting in a higher energy cost are less than 0.01. Again the probability of models trained on synthetic data resulting in a higher energy consumption are <math id="S6.SS4.SSS2.p1.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S6.SS4.SSS2.p1.3.m3.1a"><mo id="S6.SS4.SSS2.p1.3.m3.1.1" xref="S6.SS4.SSS2.p1.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.3.m3.1b"><gt id="S6.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS4.SSS2.p1.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.3.m3.1c">&gt;</annotation></semantics></math>Â 0.99.</p>
</div>
<div id="S6.SS4.SSS2.p2" class="ltx_para">
<p id="S6.SS4.SSS2.p2.1" class="ltx_p">For the neural network we see an interesting finding where the run time decreases faster than the energy consumption. Conversely, the k-nearest neighbours and logistic regression do show a trend for the run time and energy consumption. We therefore find that the run time and energy consumption might be correlated, but there is no definite relation between those. This could also be due to the energy consumption resembling the initial energy cost of the neural network.
<br class="ltx_break"></p>
</div>
<div id="S6.SS4.SSS2.p3" class="ltx_para">
<p id="S6.SS4.SSS2.p3.2" class="ltx_p">Combining these findings to obtain a general view, we find that there is a relation between the chosen <math id="S6.SS4.SSS2.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS2.p3.1.m1.1a"><mi id="S6.SS4.SSS2.p3.1.m1.1.1" xref="S6.SS4.SSS2.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p3.1.m1.1b"><ci id="S6.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S6.SS4.SSS2.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p3.1.m1.1c">k</annotation></semantics></math>-value and the run time and energy consumption of machine learning techniques. More specifically, as the chosen <math id="S6.SS4.SSS2.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS4.SSS2.p3.2.m2.1a"><mi id="S6.SS4.SSS2.p3.2.m2.1.1" xref="S6.SS4.SSS2.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p3.2.m2.1b"><ci id="S6.SS4.SSS2.p3.2.m2.1.1.cmml" xref="S6.SS4.SSS2.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p3.2.m2.1c">k</annotation></semantics></math> increases we find a significant decrease in run time and energy consumption. This is likely due to an increased amount of suppressed attributes but that does not account for all decline. In addition to this we find that machine learning techniques trained on synthetic data show close resemblance to the benchmark with respect to their run time and energy consumption.</p>
</div>
</section>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS5.5.2.1" class="ltx_text">VI-E</span> </span><span id="S6.SS5.1.1" class="ltx_text ltx_font_italic">Comparing <math id="S6.SS5.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS5.1.1.m1.1b"><mi id="S6.SS5.1.1.m1.1.1" xref="S6.SS5.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.1.1.m1.1c"><ci id="S6.SS5.1.1.m1.1.1.cmml" xref="S6.SS5.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.1.1.m1.1d">k</annotation></semantics></math>-anonymity to synthetic data</span>
</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">When comparing the Adult set to the Student Performance set we see some surprising results. Where the ML techniques trained on synthetic data show a lower accuracy for the Adult data set, they have a higher accuracy for the Student Performance set. Our findings coincide with earlier workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> which states that the accuracy of synthetic data could be influenced by how DataSynthesizer clusters the data set, yet no evidence is provided. Apart from that, we observe that models trained on anonymised data using <math id="S6.SS5.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS5.p1.1.m1.1a"><mi id="S6.SS5.p1.1.m1.1.1" xref="S6.SS5.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.1.m1.1b"><ci id="S6.SS5.p1.1.m1.1.1.cmml" xref="S6.SS5.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.1.m1.1c">k</annotation></semantics></math>-anonymity outperform the models trained on synthetic data for both data sets in terms of accuracy. This deviation is rather small (+0.018) for the Adult data set, but is more distinct for the Student Performance set (up to +0.172).</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">Another finding, regardless of the data and privacy-enhancing techniques, is that the machine learning methods logistic regression and the neural network show a close resemblance for both sets. This could lie in the fact that the neural network was made a binary classifier, which will be discussed in section D of the related work.</p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS6.4.1.1" class="ltx_text">VI-F</span> </span><span id="S6.SS6.5.2" class="ltx_text ltx_font_italic">Limitations of this study</span>
</h3>

<div id="S6.SS6.p1" class="ltx_para">
<p id="S6.SS6.p1.1" class="ltx_p">The use of two data sets in this work can be seen as a limitation; using more data sets of different properties such as size and data types could lead to insight into whether <math id="S6.SS6.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS6.p1.1.m1.1a"><mi id="S6.SS6.p1.1.m1.1.1" xref="S6.SS6.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS6.p1.1.m1.1b"><ci id="S6.SS6.p1.1.m1.1.1.cmml" xref="S6.SS6.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS6.p1.1.m1.1c">k</annotation></semantics></math>-anonymity generally outperforms synthetic data or just for some specific data sets. However, as this research is still in an early phase of exploration, we adopt an incremental approach.
<br class="ltx_break"></p>
</div>
<div id="S6.SS6.p2" class="ltx_para">
<p id="S6.SS6.p2.1" class="ltx_p">Moreover, when cleaning and preprocessing the data unique values were left out. This removal of outliers can be seen as bias, though this only occurred for one data subject in the entire Adult data set. Besides, this practise is compliant with literatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">To the best of our knowledge, no work is conducted on comparing synthetic data with anonymised data with respect to machine learning looking at both accuracy and energy consumption. Below we will discuss some related work that served as inspiration for this research.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.4.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.5.2" class="ltx_text ltx_font_italic">Energy consumption versus accuracy</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">A recent study discusses the trade-off between energy consumption and machine learning performance for neural networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Other recent work investigates the influence of the chosen <math id="S7.SS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS1.p1.1.m1.1a"><mi id="S7.SS1.p1.1.m1.1.1" xref="S7.SS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p1.1.m1.1b"><ci id="S7.SS1.p1.1.m1.1.1.cmml" xref="S7.SS1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p1.1.m1.1c">k</annotation></semantics></math> value on energy consumption and accuracyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. This paper tries to broaden the latter research by including synthetic data as another privacy-enhancing technique.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.4.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.5.2" class="ltx_text ltx_font_italic">Machine learning performance on anonymised data</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.6" class="ltx_p">Though a lot of research is conducted on both the proof and the influence of <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mi id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><ci id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">k</annotation></semantics></math>-anonymity to privacy,
little research has been done regarding the influence of <math id="S7.SS2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS2.p1.2.m2.1a"><mi id="S7.SS2.p1.2.m2.1.1" xref="S7.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.2.m2.1b"><ci id="S7.SS2.p1.2.m2.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.2.m2.1c">k</annotation></semantics></math>-anonymity on the performance of machine learning. Recent research shows that by increasing the <math id="S7.SS2.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS2.p1.3.m3.1a"><mi id="S7.SS2.p1.3.m3.1.1" xref="S7.SS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.3.m3.1b"><ci id="S7.SS2.p1.3.m3.1.1.cmml" xref="S7.SS2.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.3.m3.1c">k</annotation></semantics></math> the information loss increases as wellÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Other studies strengthen this claim by showing that for k-nearest neighbours <math id="S7.SS2.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS2.p1.4.m4.1a"><mi id="S7.SS2.p1.4.m4.1.1" xref="S7.SS2.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.4.m4.1b"><ci id="S7.SS2.p1.4.m4.1.1.cmml" xref="S7.SS2.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.4.m4.1c">k</annotation></semantics></math>-anonymity can outperform the baseline depending on the chosen value of <math id="S7.SS2.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS2.p1.5.m5.1a"><mi id="S7.SS2.p1.5.m5.1.1" xref="S7.SS2.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.5.m5.1b"><ci id="S7.SS2.p1.5.m5.1.1.cmml" xref="S7.SS2.p1.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.5.m5.1c">k</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Hence a data set that holds <math id="S7.SS2.p1.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS2.p1.6.m6.1a"><mi id="S7.SS2.p1.6.m6.1.1" xref="S7.SS2.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.6.m6.1b"><ci id="S7.SS2.p1.6.m6.1.1.cmml" xref="S7.SS2.p1.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.6.m6.1c">k</annotation></semantics></math>-anonymity is not always outperformed by its original parent data set.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS3.4.1.1" class="ltx_text">VII-C</span> </span><span id="S7.SS3.5.2" class="ltx_text ltx_font_italic">Machine learning performance on synthetic data</span>
</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">As is the case with <math id="S7.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS3.p1.1.m1.1a"><mi id="S7.SS3.p1.1.m1.1.1" xref="S7.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.1.m1.1b"><ci id="S7.SS3.p1.1.m1.1.1.cmml" xref="S7.SS3.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.1.m1.1c">k</annotation></semantics></math>-anonymity, research shows that the use of synthetic data can lead to information lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This does not mean that models trained on synthetic data will always yield a lower accuracy than models trained on the original data. For some data sets models trained on the synthetic data actually outperformed the models trained on original dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Both works emphasise the need for more research in this field as the impact remains unclear.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">The findings discussed in section C of the discussion show a similar trend. Compliant to other literatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, the models trained on synthetic data show a minor decrease in accuracy compared to the benchmark models trained using the original data.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">The goal of this research was to examine the differences between machine learning trained on anonymised data using <math id="S8.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p1.1.m1.1a"><mi id="S8.p1.1.m1.1.1" xref="S8.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p1.1.m1.1b"><ci id="S8.p1.1.m1.1.1.cmml" xref="S8.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.1.m1.1c">k</annotation></semantics></math>-anonymity or on synthetic data, with a focus on information loss and energy consumption. To aim the research we defined two research questions:</p>
<ol id="S8.I1" class="ltx_enumerate">
<li id="S8.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S8.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">RQ1:</span></span> 
<div id="S8.I1.ix1.p1" class="ltx_para">
<p id="S8.I1.ix1.p1.1" class="ltx_p">Which method has the least information loss?</p>
</div>
</li>
<li id="S8.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S8.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">RQ2:</span></span> 
<div id="S8.I1.ix2.p1" class="ltx_para">
<p id="S8.I1.ix2.p1.1" class="ltx_p">Which method is the least energy consuming?</p>
</div>
</li>
</ol>
<p id="S8.p1.2" class="ltx_p">In the research method and experimental set-up we provide the approach to answer these questions.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.3" class="ltx_p">We find that <math id="S8.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p2.1.m1.1a"><mi id="S8.p2.1.m1.1.1" xref="S8.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p2.1.m1.1b"><ci id="S8.p2.1.m1.1.1.cmml" xref="S8.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.1.m1.1c">k</annotation></semantics></math>-anonymisation via generalisation and suppression uses approximately a quarter of the energy compared to creating synthetic data. The run time of anonymisation is about half of creating synthetic data. However we also find that, especially for the smaller Student Performance set, quite some data is suppressed when using <math id="S8.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p2.2.m2.1a"><mi id="S8.p2.2.m2.1.1" xref="S8.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p2.2.m2.1b"><ci id="S8.p2.2.m2.1.1.cmml" xref="S8.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.2.m2.1c">k</annotation></semantics></math>-anonymity. For synthetic data no data is suppressed. We find that for k-nearest neighbours, logistic regression and a simple neural network the loss of accuracy is the least when the models are trained on anonymised data using <math id="S8.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p2.3.m3.1a"><mi id="S8.p2.3.m3.1.1" xref="S8.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p2.3.m3.1b"><ci id="S8.p2.3.m3.1.1.cmml" xref="S8.p2.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.3.m3.1c">k</annotation></semantics></math>-anonymity. There is an indication that models trained on anonymised data can outperform models trained on the original data.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Relating back to <span id="S8.p3.1.1" class="ltx_text ltx_font_bold">RQ1</span>, we may conclude that models trained on anonymous data via <math id="S8.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p3.1.m1.1a"><mi id="S8.p3.1.m1.1.1" xref="S8.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p3.1.m1.1b"><ci id="S8.p3.1.m1.1.1.cmml" xref="S8.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p3.1.m1.1c">k</annotation></semantics></math>-anonymity have lower loss of accuracy than models trained on synthetic data. However we should be aware that generalisation and suppression leads to quite some suppression of the data set.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">Then for the energy consumption, <span id="S8.p4.1.1" class="ltx_text ltx_font_bold">RQ2</span>, the models trained on anonymous data have a lower run time and overall energy consumption than the models trained on the original or synthetic data. This is seen in both data sets across all the machine learning models. We think that most of the decrease in energy consumption can be related to the suppression of data. All in all we may conclude that regarding energy consumption we prefer <math id="S8.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p4.1.m1.1a"><mi id="S8.p4.1.m1.1.1" xref="S8.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p4.1.m1.1b"><ci id="S8.p4.1.m1.1.1.cmml" xref="S8.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p4.1.m1.1c">k</annotation></semantics></math>-anonymity over synthetic data.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">Apart from the answers on the research questions above, we also find that anonymising data can lead to both a higher accuracy as well as a lower energy consumption. This finding is particularly interesting because these two important properties do not have to be a trade-off and exclude each other.</p>
</div>
<div id="S8.p6" class="ltx_para">
<p id="S8.p6.1" class="ltx_p">Ultimately, when going back to our research question, using <math id="S8.p6.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S8.p6.1.m1.1a"><mi id="S8.p6.1.m1.1.1" xref="S8.p6.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.p6.1.m1.1b"><ci id="S8.p6.1.m1.1.1.cmml" xref="S8.p6.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p6.1.m1.1c">k</annotation></semantics></math>-anonymity over synthetic data could be preferable as it has a higher accuracy over both data sets and consumes less energy. However, this is also dependent on the size of the data set and on whether suppressing the data set is a decision factor.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is partially funded through the AMdEX Fieldlab project supported by Kansen Voor West EFRO (KVW00309) and the province of Noord-Holland.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">In addition to that, this work was financially supported by the Dutch NWO Research project â€œData Logistics for Logistics Dataâ€ (DL4LD, www.dl4ld.net), supported by the Dutch Top consortia for Knowledge and Innovation â€œInstitute for Advanced Logisticsâ€ (TKI Dinalog, www.dinalog.nl) of the Ministry of Economy and Environment in The Netherlands and the Dutch Commit-toData initiative (https://commit2data.nl/).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
E.Â Commission and D.-G. for Communication, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The European Green Deal :
delivering step by step</em>.Â Â Â Publications
Office, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
â€œPublications on artificial intelligence per year,â€ accessed: 2023-02-07.
[Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">\url{https://app.dimensions.ai/discover/publication}</span>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
V.Â C. CoroamÄƒ and F.Â Mattern, â€œDigital reboundâ€“why digitalization will
not redeem us our environmental sins,â€ in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings 6th
international conference on ICT for sustainability. Lappeenranta.
http://ceur-ws. org</em>, vol. 2382, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T.Â Kopp and S.Â Lange, â€œThe climate effect of digitalization in production and
consumption in oecd countries,â€ in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">CEUR Workshop Proc</em>, vol. 2382,
2019, pp. 1â€“11.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
E.Â Union, â€œSupporting the green transition, shaping europeâ€™s digital
future.â€Â Â Â Publicatios Office of the
European Union, 2 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
E.Â GarcÃ­aÂ MartÃ­n, â€œEnergy efficiency in machine learning: A position
paper,â€ in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">30th Annual Workshop of the Swedish Artificial Intelligence
Society SAIS, Karlskrona</em>, vol. 137.Â Â Â LinkÃ¶ping University Electronic Press, 2017, pp. 68â€“72.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
E.Â GarcÃ­a-MartÃ­n, C.Â F. Rodrigues, G.Â Riley, and H.Â Grahn,
â€œEstimation of energy consumption in machine learning,â€ <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Journal of
Parallel and Distributed Computing</em>, vol. 134, pp. 75â€“88, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D.Â Li, X.Â Chen, M.Â Becchi, and Z.Â Zong, â€œEvaluating the energy efficiency of
deep convolutional neural networks on cpus and gpus,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2016 IEEE
international conferences on big data and cloud computing (BDCloud), social
computing and networking (SocialCom), sustainable computing and
communications (SustainCom)(BDCloud-SocialCom-SustainCom)</em>.Â Â Â IEEE, 2016, pp. 477â€“484.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R.Â Verdecchia, L.Â Cruz, J.Â Sallou, M.Â Lin, J.Â Wickenden, and E.Â Hotellier,
â€œData-centric green ai an exploratory empirical study,â€ in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2022
International Conference on ICT for Sustainability (ICT4S)</em>.Â Â Â IEEE, 2022, pp. 35â€“45.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
I.Â Elueze and A.Â Quan-Haase, â€œPrivacy attitudes and concerns in the digital
lives of older adults: Westinâ€™s privacy attitude typology revisited,â€
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">American Behavioral Scientist</em>, vol.Â 62, no.Â 10, pp. 1372â€“1391, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
2018 reform of eu data protection rules. European Commission. [Online].
Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ec.europa.eu/info/law/law-topic/data-protection/eu-data-protection-rules\_en</span>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Regulation (eu) 2016/679 of the european parliament and of the council.
European Commission. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679</span>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L.Â Sweeney, â€œk-anonymity: A model for protecting privacy,â€
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Journal of Uncertainty, Fuzziness and Knowledge-Based
Systems</em>, vol.Â 10, no.Â 05, pp. 557â€“570, 2002.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D.Â Defays and M.Â Anwar, â€œMasking microdata using micro-aggregation,â€
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Journal of Official Statistics</em>, vol.Â 14, no.Â 4, p. 449, 1998.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â Machanavajjhala, D.Â Kifer, J.Â Abowd, J.Â Gehrke, and L.Â Vilhuber, â€œPrivacy:
Theory meets practice on the map,â€ in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2008 IEEE 24th international
conference on data engineering</em>.Â Â Â IEEE,
2008, pp. 277â€“286.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D.Â B. Rubin, â€œStatistical disclosure limitation,â€ <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Journal of official
Statistics</em>, vol.Â 9, no.Â 2, pp. 461â€“468, 1993.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F.Â Kouwenberg, â€œThe effect of k-anonymity on machine learning models,â€
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Available upon request</em>, 5 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.Â Madan and P.Â Goswami, â€œA technique for securing big data using
k-anonymization with a hybrid optimization algorithm,â€ <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International
Journal of Operations Research and Information Systems</em>, vol.Â 12, no.Â 4, pp.
1â€“21, 8 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
B.Â S. Bhati, J.Â Ivanchev, I.Â Bojic, A.Â Datta, and D.Â Eckhoff, â€œUtility-driven
k-anonymization of public transport user data,â€ <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>,
vol.Â 9, pp. 23â€‰608â€“23â€‰623, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H.Â Ping, J.Â Stoyanovich, and B.Â Howe, â€œDatasynthesizer: Privacy-preserving
synthetic datasets,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International
Conference on Scientific and Statistical Database Management</em>, 2017, pp.
1â€“5.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M.Â Hittmeir, A.Â Ekelhart, and R.Â Mayer, â€œOn the utility of synthetic data: An
empirical evaluation on machine learning tasks,â€ in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
14th International Conference on Availability, Reliability and Security</em>,
2019, pp. 1â€“6.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.Â Oprescu, S.Â Misdorp, and K.Â van Elsen, â€œEnergy cost and accuracy impact of
k-anonymity.â€ in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICT4S, June 2022, Plovdiv, Bulgaria</em>.Â Â Â International Conference on ICT for Sustainability,
2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.Â Hernandez, G.Â Epelde, A.Â Alberdi, R.Â Cilla, and D.Â Rankin, â€œSynthetic data
generation for tabular health records: A systematic review,â€
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M.Â Abadi, A.Â Chu, I.Â Goodfellow, H.Â B. McMahan, I.Â Mironov, K.Â Talwar, and
L.Â Zhang, â€œDeep learning with differential privacy,â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2016 ACM SIGSAC conference on computer and communications security</em>,
2016, pp. 308â€“318.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S.Â Desrochers, C.Â Paradis, and V.Â M. Weaver, â€œA validation of dram rapl power
measurements,â€ in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second International Symposium on
Memory Systems</em>, 2016, pp. 455â€“470.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M.Â HÃ¤hnel, B.Â DÃ¶bel, M.Â VÃ¶lp, and H.Â HÃ¤rtig, â€œMeasuring energy
consumption for short code paths using rapl,â€ <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ACM SIGMETRICS
Performance Evaluation Review</em>, vol.Â 40, no.Â 3, pp. 13â€“17, 2012.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K.Â N. Khan, M.Â Hirki, T.Â Niemi, J.Â K. Nurminen, and Z.Â Ou, â€œRapl in action:
Experiences in using rapl for power measurements,â€ <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on
Modeling and Performance Evaluation of Computing Systems (TOMPECS)</em>, vol.Â 3,
no.Â 2, pp. 1â€“26, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J.Â A. Garcia, â€œExploration of energy consumption using the intel running
average power limit interface,â€ in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Space Computing
Conference (SCC)</em>.Â Â Â IEEE, 7 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
K.Â Bache and M.Â Lichman, â€œUCI machine learning repository,â€ 2013. [Online].
Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://archive.ics.uci.edu/ml</span>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C.Â A. Escobar, M.Â E. McGovern, and R.Â Morales-Menendez, â€œQuality 4.0: a review
of big data challenges in manufacturing,â€ <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Journal of Intelligent
Manufacturing</em>, vol.Â 32, no.Â 8, pp. 2319â€“2334, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S.Â Misdorp, â€œThe energy cost of k-anonymity and its impact on the accuracy of
machine learning models,â€ Bachelorâ€™s thesis, University of Amsterdam,
February 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
F.Â Prasser, F.Â Kohlmayer, R.Â LautenschlÃ¤ger, and K.Â A. Kuhn, â€œArx-a
comprehensive tool for anonymizing biomedical data,â€ in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">AMIA Annual
Symposium Proceedings</em>, vol. 2014.Â Â Â American Medical Informatics Association, 2014, p. 984.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S.Â Ray, â€œA quick review of machine learning algorithms,â€ in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2019
International conference on machine learning, big data, cloud and parallel
computing (COMITCon)</em>.Â Â Â IEEE, 2019, pp.
35â€“39.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J.Â Hao and T.Â K. Ho, â€œMachine learning made easy: a review of scikit-learn
package in python programming language,â€ <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Journal of Educational and
Behavioral Statistics</em>, vol.Â 44, no.Â 3, pp. 348â€“361, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A.Â Friedman, R.Â Wolff, and A.Â Schuster, â€œProviding k-anonymity in data
mining,â€ <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">The VLDB Journal</em>, vol.Â 17, no.Â 4, pp. 789â€“804, 2008.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
P.Â Cortez and A.Â M.Â G. Silva, â€œUsing data mining to predict secondary school
student performance,â€ 2008.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
N.Â Chakrabarty and S.Â Biswas, â€œA statistical approach to adult census income
level prediction,â€ in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2018 International Conference on Advances in
Computing, Communication Control and Networking (ICACCCN)</em>.Â Â Â IEEE, 2018, pp. 207â€“212.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A.Â E. Brownlee, J.Â Adair, S.Â O. Haraldsson, and J.Â Jabbo, â€œExploring the
accuracyâ€“energy trade-off in machine learning,â€ in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/ACM
International Workshop on Genetic Improvement (GI)</em>.Â Â Â IEEE, 2021, pp. 11â€“18.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A.Â Kazemnejad, Z.Â Batvandi, and J.Â Faradmal, â€œComparison of artificial neural
network and binary logistic regression for determination of impaired glucose
tolerance/diabetes,â€ <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">EMHJ-Eastern Mediterranean Health Journal, 16
(6), 615-620, 2010</em>, 2010.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M.Â Schumacher, R.Â RoÃŸner, and W.Â Vach, â€œNeural networks and logistic
regression: Part i,â€ <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Computational Statistics &amp; Data Analysis</em>,
vol.Â 21, no.Â 6, pp. 661â€“682, 1996.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
D.Â SlijepÄeviÄ‡, M.Â Henzl, L.Â D. Klausner, T.Â Dam, P.Â Kieseberg, and
M.Â Zeppelzauer, â€œk-anonymity in practice: How generalisation and suppression
affect machine learning classifiers,â€ <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Security</em>, vol.
111, p. 102488, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.07115" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.07116" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.07116">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.07116" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.07117" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 06:56:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
