<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.01107] Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning</title><meta property="og:description" content="Text-to-speech (TTS) systems are being built using end-to-end deep learning approaches. However, these systems require huge amounts of training data. We present our approach to built production quality TTS and perform …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.01107">

<!--Generated on Tue Feb 27 15:28:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raviraj Joshi 
<br class="ltx_break">Flipkart, Bengaluru 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">raviraj.j@flipkart.com</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Nikesh Garera 
<br class="ltx_break">Flipkart, Bengaluru 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">nikesh.garera@flipkart.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Text-to-speech (TTS) systems are being built using end-to-end deep learning approaches. However, these systems require huge amounts of training data. We present our approach to built production quality TTS and perform speaker adaptation in extremely low resource settings. We propose a transfer learning approach using high-resource language data and synthetically generated data. We transfer the learnings from the out-domain high-resource English language. Further, we make use of out-of-the-box single-speaker TTS in the target language to generate in-domain synthetic data. We employ a three-step approach to train a high-quality single-speaker TTS system in a low-resource Indian language Hindi. We use a Tacotron2 like setup with a spectrogram prediction network and a waveglow vocoder. The Tacotron2 acoustic model is trained on English data, followed by synthetic Hindi data from the existing TTS system. Finally, the decoder of this model is fine-tuned on only 3 hours of target Hindi speaker data to enable rapid speaker adaptation. We show the importance of this dual pre-training and decoder-only fine-tuning using subjective MOS evaluation. Using transfer learning from high-resource language and synthetic corpus we present a low-cost solution to train a custom TTS model.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech synthesis systems are widely used in applications like voice assistants and customer service voice bots <cite class="ltx_cite ltx_citemacro_cite">Joshi and Kannan (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Joshi and Singh (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>. They are used commonly along with automatic speech recognition (ASR) <cite class="ltx_cite ltx_citemacro_cite">Joshi and Kumar (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> systems to provide an end-to-end voice interface. Recently, text-to-speech (TTS) systems have been trained using end-to-end deep learning approaches <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>. The TTS models are based on an independent acoustic model converting text to spectrogram and a vocoder converting spectrogram to speech. More recently, these two models have been integrated into the model directly converting text to target speech <cite class="ltx_cite ltx_citemacro_cite">Weiss et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. However, the single end-to-end model requires large amounts of transcribed data. The dual model approach can be trained on comparatively less data as training a vocoder only requires audio data without its text transcripts. In general, training an end-to-end TTS requires a large amount of high-quality studio recordings to build a production-quality model.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2312.01107/assets/tts_model_flow.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="407" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overall flow of the TTS system</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The popular text to spectrogram models include Tacotron2 <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>, Transformer-TTS <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, FastSpeech2 <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite>, FastPitch <cite class="ltx_cite ltx_citemacro_cite">Łańcucki (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, and Glow-TTS <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. In terms of voice quality the Tacotron2 model is still competitive with other models and less prone to over-fitting in low resource settings <cite class="ltx_cite ltx_citemacro_cite">Favaro et al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>); Abdelali et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); García et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>); Finkelstein et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. There are multiple options for the vocoder as well like Clarinet <cite class="ltx_cite ltx_citemacro_cite">Ping et al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>, Waveglow <cite class="ltx_cite ltx_citemacro_cite">Prenger et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, MelGAN <cite class="ltx_cite ltx_citemacro_cite">Kumar et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, HiFiGAN <cite class="ltx_cite ltx_citemacro_cite">Kong et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, StyleMelGAN <cite class="ltx_cite ltx_citemacro_cite">Mustafa et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>, and ParallelWaveGAN <cite class="ltx_cite ltx_citemacro_cite">Yamamoto et al. (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>. We choose Waveglow since it is competitive with other vocoders and is easy to train <cite class="ltx_cite ltx_citemacro_cite">Abdelali et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); García et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>); Shih et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>.
There is other single model end-to-end architectures like VITS <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, Wave-Tacotron <cite class="ltx_cite ltx_citemacro_cite">Weiss et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> and JETS <cite class="ltx_cite ltx_citemacro_cite">Lim et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> for spectrogram-free TTS approaches. Although such models are more desirable since they remove the vocoder spectrogram features mismatch during training and inference but do not work well in low-resource settings.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we explore the Tacotron2-based acoustic model and Waveglow-based vocoder to build a production-quality TTS system in low-resource, low-budget settings. The high-level flow is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In general, these models require 10 to 20 hours of quality data to train high-quality TTS systems. We aim to reduce the data requirements using simple strategies. Previous works in literature have proposed approaches to adapt to a new speaker with a few hours to a few minutes of data <cite class="ltx_cite ltx_citemacro_cite">Prakash and Murthy (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>. However, these approaches have only been tested on some 10-30 utterances and might not be suitable for high-quality applications. Recently, a TTS system Vall-E <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> has shown extraordinary zero-shot capabilities. However, this system uses a complex architecture and requires 60K hours of pre-training data making it infeasible in low-resource scenarios.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to build low-resource TTS, we explore transfer learning from English data and synthetic audio corpus from the existing TTS model. We show the effectiveness of our approach in the context of the low-resource Indian language Hindi. While transfer learning from English is a common approach, we propose the usage of existing out-of-the-box TTS to further augment the data. Using an out-of-the-box TTS system has multiple advantages. It allows us to get a large amount of (real-text, synthetic-audio) pairs in the domain of our choice. It is a low-cost solution as compared to obtaining studio recordings for an equivalent amount of data. With high-quality out-of-the-box single-speaker text-to-speech systems available in the majority of languages, we leverage it to build a TTS in the speaker of our choice. We use an in-house single-speaker Hindi TTS system to generate synthetic corpus, however, the approach is applicable to any out-of-the-box TTS system. While we could have directly used the original training data of the initial TTS system, we utilized the model as a black box so that we can generate data in the domain of our choice.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We propose a three-step approach to build a low-cost TTS system. This approach is depicted in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We initially pre-train the Tacotron2 acoustic model with public English LJSpeech data.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We then ignore the initial character embedding layer based on English (Roman script) and re-train the entire model using a synthetic Hindi corpus (Devanagari script). This synthetic data pre-training step is important as it adapts the Tacotron2 encoder to the target domain text in the Devanagari script.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Finally, we adapt the model to the target speaker using 3 hours of target speaker Hindi data. In the second step, although the audios are synthetic and from a different speaker, a large amount of real target domain text ensures high-quality pretraining of the text encoder. Therefore, during the final step, we freeze the encoder and only fine-tune the decoder of the Tacotron2 encoder-decoder model.</p>
</div>
</li>
</ul>
<p id="S1.p5.2" class="ltx_p">We show that using this three-step strategy allows us to rapidly build a TTS system in the speaker of our choice. Although we can further reduce the data requirements overall stability of the model is impacted thus hindering its deployment in high-quality applications. We perform a subjective evaluation of our approach on an unseen domain with a larger test set and show its effectiveness.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2312.01107/assets/tts_flow.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="201" height="68" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The training strategy for low resource TTS using less amount of Target-Hindi Data</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we describe the previous attempts to train a low-resource TTS system.
A generic Indic TTS system using multiple languages and voices was built in <cite class="ltx_cite ltx_citemacro_cite">Prakash and Murthy (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>. They used the tacotron2 + waveglow setup, along with a common linguistic representation multi-language character map (MLCM) to map all languages to a common script. They also utilize speaker embeddings based on x-vectors to train a multi-speaker model. Their observations indicate that such a system does not scale to the unseen speakers. So they propose a new speaker adaptation using only 7 minutes of data. However, the adapted system was tested only on 10 utterances which might not scale well to a larger set in high-quality settings. In this work, we focus on the original script of the language as mapping it to a common script could result in information loss for some languages. Previously, a multi-lingual TTS system using MLCM character representation was introduced in <cite class="ltx_cite ltx_citemacro_cite">Prakash et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Cross-lingual transfer learning and data augmentation approach for low resource TTS were proposed in <cite class="ltx_cite ltx_citemacro_cite">Byambadorj et al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>. The spectrogram prediction network was trained using cross-lingual transfer learning (TL) from high resource language, data augmentation by varying parameters like pitch and speed, and a combination of two approaches. In the TL approach models were sequentially trained on high resource language (English) followed by a low-resource language (Mongolian). The input script was first converted into IPA phonetic format to enable the transfer of knowledge. We followed a similar approach in our first two steps but without using the common IPA phones instead of relying on the original script. This work also indicates a minimum of 3 hours of data is needed to cover all the phones. However, this approach is based on a multi-speaker TTS system as opposed to our single-speaker model. Similarly, data augmentation using a voice conversion module was explored in <cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Ribeiro et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Another approach for data augmentation using the parent TTS system was proposed in <cite class="ltx_cite ltx_citemacro_cite">Hwang et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. An auto-regressive TTS was first trained and used to generate large-scale synthetic corpora. This synthetic corpus along with real corpus is then used to train a non-auto-regressive TTS system. A similar approach utilizing synthetic corpus from existing TTS is explored in <cite class="ltx_cite ltx_citemacro_cite">Finkelstein et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>); Song et al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>. Our work is similar to these approaches where the common aspect is to generate synthetic audio from another TTS system. However, these works train different TTS architectures for the same speaker and propose complicated training approaches. We instead focus on any-speaker out-of-the-box TTS system and propose a simple fine-tuning strategy.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Multi-speaker models leveraging external speaker embedding are commonly used to address speaker adaptation in low-resource settings. Transfer learning from external speaker verification models to Tacotron2 TTS was initially explored in <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>. Further, zero-shot un-seen speaker adaptation using a similar speaker embedding approach was explored in <cite class="ltx_cite ltx_citemacro_cite">Cooper et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. Other approaches have been proposed over to time to combine speaker embedding and style embedding in Tacotron2 setup <cite class="ltx_cite ltx_citemacro_cite">Chung and Mak (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>. In this work, we are only concerned about single-speaker models in this work and directly use the input script to preserve the original representation.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Architecture</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use a Tacotron2-based spectrogram prediction network followed by a Waveglow-based speech synthesis model. The two pass models are preferred in low resource settings as compared to fully end-to-end models. We observe that the vanilla Tacotron2 model is less prone to over-fitting in low-resource scenarios as compared to the vanilla Transformer based Tacotron model. Moreover, these models are competitive with more recent models in terms of audio quality and hence used to evaluate our transfer learning approaches <cite class="ltx_cite ltx_citemacro_cite">Tan et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>. The approaches presented in this work are data-oriented and can be easily extended to any other model architecture.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tacotron2</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The Tacotron2 <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> model uses an auto-regressive spectrogram prediction network followed by a wavenet vocoder <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">van den Oord et al. </a></cite>. We describe the details of the spectrogram prediction network from Tacotron2 used in this work. The text-to-Mel spectrogram prediction is done using a sequence-to-sequence network. The input characters are converted into 512-dimensional embeddings and passed to the encoder-decoder network. The encoder consists of 3 stacked convolution layers with 512 filters and a filter size of 5 x 1. Each convolution layer is followed by batch normalization and relu activation. The convolution block is followed by a single Bi-LSTM layer with 512 units. The decoder is an auto-regressive network conditioned on encoder hidden representation. It uses location-sensitive attention <cite class="ltx_cite ltx_citemacro_cite">Chorowski et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> to compute the context vector during each time step. The decoder uses pre-net, containing 2 feed-forward layers (256 units) followed by relu units. The output of the pre-net is concatenated with the context vector and passed through two uni-LSTM layers with 1024 units. The output of lstm is again concatenated with the context vector and passed through a dense layer to predict the spectrogram frame. At this step, another parallel projection predicts the stop token. The predicted frame is passed through a post-net comprising of 5 conv layers (512 filters, 5 x 1 filter size) each followed by batch norm and tanh non-linearity. The post-net predicts the residual to be added to spectrogram prediction to enhance the output. The mean squared error (MSE) is used as a loss function. The loss is computed on both output of LSTM projection and post-net projection.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The ground truth mel spectrogram is computed with STFT using 50 ms frame length and 12 ms hop. The STFT magnitude is transformed into a Mel scale using an 80-channel Mel filter bank. This is followed by log compression to get ground truth log-Mel spectrogram. Other hyperparameters are the same as those described in the original work.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Waveglow</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Waveglow <cite class="ltx_cite ltx_citemacro_cite">Prenger et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> is a flow-based network that converts Mel-spectrogram to speech. It is a generative model that generates audio by sampling from a distribution. The samples are taken from zero mean, spherical Gaussian distribution, and transformed into audio distribution by passing it through a series of invertible transformations. It essentially models the audio distribution conditioned on a Mel-spectrogram. The model is trained by minimizing the log-likelihood of the data.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Details</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We use three different datasets in this work. These datasets are single-speaker labeled audio-text pairs. One dataset is synthetically generated while the other two are real data. These are described below.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">LJSpeech-English (24 hrs)</span>: It is a public domain single-speaker audio dataset consisting of 13,100 audio clips <cite class="ltx_cite ltx_citemacro_cite">Ito and Johnson (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>. The text of the audio is taken from 7 non-fiction English books. The total length of the dataset is approximately 24 hours.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Syntethic-Hindi (15 hrs)</span>: A synthetic data set is created using an in-house TTS system. The output of the system was a single speaker and a female voice. Around 16k short utterances in Devanagari script majorly from the grocery voice assistant domain were converted to speech. The size of this dataset is around 15 hrs.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Real-Hindi (3 hrs)</span>: This is the target low-resource speaker data. A subset of utterances (2.5k) from the voice assistant domain were recorded in the voice of an external female speaker. These were high-quality studio recordings and the size of the dataset was around 3 hrs. We also evaluate the full 15 hrs of studio recordings of the above voice assistant utterances for comparative analysis.</p>
</div>
</li>
</ul>
<p id="S4.p1.2" class="ltx_p">All the audio data is re-sampled at 16kHz and encoded in 16-bit PCM wav format for training and inference.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.5.6.1" class="ltx_tr">
<th id="S4.T1.5.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T1.5.6.1.1.1" class="ltx_text ltx_font_bold">Training Strategy</span></th>
<th id="S4.T1.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.5.6.1.2.1" class="ltx_text ltx_font_bold">MOS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Ground Truth Audios (Real)</th>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">4.65 <math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.62</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">LJSpeech + Real (3 hrs)</th>
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">4.27 <math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mo id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><csymbol cd="latexml" id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\pm</annotation></semantics></math> 0.95</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<th id="S4.T1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">LJSpeech + Synthetic + Real (3 hrs)</th>
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">4.54 <math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mo id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\pm</annotation></semantics></math> 0.58</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">LJSpeech + Synthetic + Real (frozen encoder, 3 hrs)</th>
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.4.4.1.1" class="ltx_text ltx_font_bold">4.59</span> <math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mo id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\pm</annotation></semantics></math> 0.68</td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<th id="S4.T1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">LJSpeech + Synthetic + Real (frozen encoder, 15 hrs)</th>
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">4.65 <math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mo id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\pm</annotation></semantics></math> 0.58</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>MOS scores for different training strategies</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluate different variations of the pre-training strategy and try to come up with the best training strategy. We use English data and synthetic data for pre-training. First, the model is trained on English data followed by Hindi synthetic data. Finally, the model is trained on the target real Hindi corpus. The third corpus is the smallest in size and depicts the low-resource speaker. The final fine-tuning is done in two different ways. One approach is to perform full-finetuning and the second approach is to perform partial fine-tuning by freezing the encoder. The frozen encoder approach yields the best results and the corresponding flow is shown in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The frozen text encoder is desirable since it is pre-trained on a large amount of target domain text as opposed to a small amount of data in the third step.
Overall we consider the following pre-training strategies:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Direct target speaker (Hindi) training</span> - With just 3 hours of data and the results were mostly noisy and the training did not converge to a decent model. So the results of this model training are not discussed in the next sections.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LJSpeech English pre-training + Target-Hindi finetuning</span> - In this setup, since the input symbols of English and Hindi are completely different we discard the embedding layer weights and retain all other weights of the pre-trained English model. Post this we perform target Hindi data fine-tuning. Also, full fine-tuning is performed using target data since the embedding layer is part of the encoder and needs to be re-trained.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">LJSpeech English pre-training + Synthetic Hindi pre-training + Target-Hindi full finetuning</span> - In this strategy, the model is initially trained on English corpus, followed by full training on synthetic Hindi corpus. Finally, target Hindi speaker data is used to again fully fine-tune the models.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">LJSpeech English pre-training + Synthetic Hindi pre-training + Target-Hindi decoder only finetuning</span> - This strategy is similar to the last strategy. The only difference is in the final fine-tuning only decoder weights are updated and the encoder is completely frozen. With this, we try to retain learnings from a much larger Hindi corpus.</p>
</div>
</li>
</ul>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2312.01107/assets/tts_spectrogram.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="226" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Mel-spectrogram and attention alignment plot for a sample sentence using config (a) LJSpeech + Real (3 hrs) and (b) LJSpeech + Synthetic + Real (frozen encoder, 3 hrs). The difference in the resolution can be clearly seen at the end of the two spectrograms. </figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results and Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We evaluate different pre-training strategies explored in this work using subjective mean opinion score (MOS) evaluation. A test set was created using 200 consumer experience (CX) voice bot interaction utterances. The domain of the test data (CX) is different from the domain of training utterances (voice assistant). This allows us to perform more rigorous testing of the model to unseen domains. These utterances were evaluated on (1-5) MOS scale by 10 specialized listeners with each audio evaluated by at least 3 listeners. The evaluators were explicitly trained for the evaluation activity and were part of the internal operations team thus ensuring high quality of evaluation. The audio were rated based on intelligibility and naturalness of the audios.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The results of the evaluation are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Dataset Details ‣ Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The results indicate that LJSpeech English pre-training is helpful to create usable TTS models. Without this cross-lingual transfer learning, the model fails to produce intelligible output. Further training the model on synthetic Hindi data improves the output even further. Synthetic data pre-training is evaluated in two configurations. The full model is fine-tuned in the first config and the encoder is frozen in the second config. In both configurations, we perform LJSpeech pre-training. We observe that the frozen encoder approach yields superior performance in low-resource settings. All these experiments used 3 hrs of real Hindi data, 15 hours of synthetic Hindi data, and 24 hours of real English corpus. We also evaluate the three-step approach using full 15 hrs real data and see minimal improvements in performance. This indicates that 3 hrs of data is sufficient to build a production-ready TTS system with transfer learning from cross-lingual data and synthetic corpus. The plot of spectrogram and attention alignment weights for a sample sentence with and without using synthetic Hindi data is shown in Figure <a href="#S5.F3" title="Figure 3 ‣ 5 Experimental Setup ‣ Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We present our transfer learning strategy to build low resource TTS system. We explore transfer learning from cross-lingual data and same-language synthetic data. The synthetic data is created using existing out of the box TTS system. The three-step approach involves pre-training with English data followed by synthetic Hindi data and low-resource real Hindi data. We evaluate these pre-training approaches using a strong out-of-domain test set using subjective MOS evaluation. In the final step, we observe that the decoder only fine-tuning works better than full tuning. The high-level text representations (encoder output) of text trained on the large real text and synthetic audio pairs are better than just using the low-resource data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelali et al. (2022)</span>
<span class="ltx_bibblock">
Ahmed Abdelali, Nadir Durrani, Cenk Demiroglu, Fahim Dalvi, Hamdy Mubarak, and
Kareem Darwish. 2022.

</span>
<span class="ltx_bibblock">Natiq: An end-to-end text-to-speech system for arabic.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.07373</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byambadorj et al. (2021)</span>
<span class="ltx_bibblock">
Zolzaya Byambadorj, Ryota Nishimura, Altangerel Ayush, Kengo Ohta, and Norihide
Kitaoka. 2021.

</span>
<span class="ltx_bibblock">Text-to-speech system for low-resource language using cross-lingual
transfer learning and data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">EURASIP Journal on Audio, Speech, and Music Processing</em>,
2021(1):1–20.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2023)</span>
<span class="ltx_bibblock">
Zexin Cai, Yaogen Yang, and Ming Li. 2023.

</span>
<span class="ltx_bibblock">Cross-lingual multi-speaker speech synthesis with limited bilingual
training data.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, 77:101427.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chorowski et al. (2015)</span>
<span class="ltx_bibblock">
Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua
Bengio. 2015.

</span>
<span class="ltx_bibblock">Attention-based models for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Mak (2021)</span>
<span class="ltx_bibblock">
Raymond Chung and Brian Mak. 2021.

</span>
<span class="ltx_bibblock">On-the-fly data augmentation for text-to-speech style transfer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU)</em>, pages 634–641. IEEE.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cooper et al. (2020)</span>
<span class="ltx_bibblock">
Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen,
and Junichi Yamagishi. 2020.

</span>
<span class="ltx_bibblock">Zero-shot multi-speaker text-to-speech with state-of-the-art neural
speaker embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6184–6188. IEEE.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Favaro et al. (2021)</span>
<span class="ltx_bibblock">
Anna Favaro, Licia Sbattella, Roberto Tedesco, and Vincenzo Scotti. 2021.

</span>
<span class="ltx_bibblock">Itacotron 2: transfering english speech synthesis architectures and
speech features to italian.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of The Fourth International Conference on
Natural Language and Speech Processing (ICNLSP 2021)</em>, pages 83–88.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finkelstein et al. (2022)</span>
<span class="ltx_bibblock">
Lev Finkelstein, Heiga Zen, Norman Casagrande, Chun-an Chan, Ye Jia, Tom
Kenter, Alexey Petelin, Jonathan Shen, Vincent Wan, Yu Zhang, et al. 2022.

</span>
<span class="ltx_bibblock">Training text-to-speech systems from synthetic data: A practical
approach for accent transfer tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.13183</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">García et al. (2022)</span>
<span class="ltx_bibblock">
Víctor García, Inma Hernáez, and Eva Navas. 2022.

</span>
<span class="ltx_bibblock">Evaluation of tacotron based synthesizers for spanish and basque.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 12(3):1686.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al. (2021)</span>
<span class="ltx_bibblock">
Min-Jae Hwang, Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. 2021.

</span>
<span class="ltx_bibblock">Tts-by-tts: Tts-driven data augmentation for fast and high-quality
speech synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6598–6602. IEEE.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ito and Johnson (2017)</span>
<span class="ltx_bibblock">
Keith Ito and Linda Johnson. 2017.

</span>
<span class="ltx_bibblock">The lj speech dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://keithito.com/LJ-Speech-Dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keithito.com/LJ-Speech-Dataset/</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2018)</span>
<span class="ltx_bibblock">
Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen,
Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, et al. 2018.

</span>
<span class="ltx_bibblock">Transfer learning from speaker verification to multispeaker
text-to-speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 31.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi and Kannan (2021)</span>
<span class="ltx_bibblock">
Raviraj Joshi and Venkateshan Kannan. 2021.

</span>
<span class="ltx_bibblock">Attention based end to end speech recognition for voice search in
hindi and english.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Annual Meeting of the Forum for
Information Retrieval Evaluation</em>, pages 107–113.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi and Kumar (2022)</span>
<span class="ltx_bibblock">
Raviraj Joshi and Subodh Kumar. 2022.

</span>
<span class="ltx_bibblock">On comparison of encoders for attention based end to end speech
recognition in standalone and rescoring mode.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Signal Processing and
Communications (SPCOM)</em>, pages 1–4. IEEE.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi and Singh (2022)</span>
<span class="ltx_bibblock">
Raviraj Joshi and Anupam Singh. 2022.

</span>
<span class="ltx_bibblock">A simple baseline for domain adaptation in end to end asr systems
using synthetic data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Workshop on e-Commerce and NLP
(ECNLP 5)</em>, pages 244–249.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2020)</span>
<span class="ltx_bibblock">
Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. 2020.

</span>
<span class="ltx_bibblock">Glow-tts: A generative flow for text-to-speech via monotonic
alignment search.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:8067–8077.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2021)</span>
<span class="ltx_bibblock">
Jaehyeon Kim, Jungil Kong, and Juhee Son. 2021.

</span>
<span class="ltx_bibblock">Conditional variational autoencoder with adversarial learning for
end-to-end text-to-speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
5530–5540. PMLR.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al. (2020)</span>
<span class="ltx_bibblock">
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020.

</span>
<span class="ltx_bibblock">Hifi-gan: Generative adversarial networks for efficient and high
fidelity speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:17022–17033.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2019)</span>
<span class="ltx_bibblock">
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen
Teoh, Jose Sotelo, Alexandre de Brébisson, Yoshua Bengio, and Aaron C
Courville. 2019.

</span>
<span class="ltx_bibblock">Melgan: Generative adversarial networks for conditional waveform
synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Łańcucki (2021)</span>
<span class="ltx_bibblock">
Adrian Łańcucki. 2021.

</span>
<span class="ltx_bibblock">Fastpitch: Parallel text-to-speech with pitch prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6588–6592. IEEE.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019.

</span>
<span class="ltx_bibblock">Neural speech synthesis with transformer network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 33, pages 6706–6713.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al. (2022)</span>
<span class="ltx_bibblock">
Dan Lim, Sunghee Jung, and Eesung Kim. 2022.

</span>
<span class="ltx_bibblock">Jets: Jointly training fastspeech2 and hifi-gan for end to end text
to speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.16852</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mustafa et al. (2021)</span>
<span class="ltx_bibblock">
Ahmed Mustafa, Nicola Pia, and Guillaume Fuchs. 2021.

</span>
<span class="ltx_bibblock">Stylemelgan: An efficient high-fidelity adversarial vocoder with
temporal adaptive normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6034–6038. IEEE.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping et al. (2018)</span>
<span class="ltx_bibblock">
Wei Ping, Kainan Peng, and Jitong Chen. 2018.

</span>
<span class="ltx_bibblock">Clarinet: Parallel wave generation in end-to-end text-to-speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.07281</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prakash and Murthy (2020)</span>
<span class="ltx_bibblock">
Anusha Prakash and Hema A Murthy. 2020.

</span>
<span class="ltx_bibblock">Generic indic text-to-speech synthesisers with rapid adaptation in an
end-to-end framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 2962–2966.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prakash et al. (2019)</span>
<span class="ltx_bibblock">
Anusha Prakash, A Leela Thomas, S Umesh, and Hema A Murthy. 2019.

</span>
<span class="ltx_bibblock">Building multilingual end-to-end speech synthesisers for indian
languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. of 10th ISCA Speech Synthesis Workshop (SSW’10)</em>,
pages 194–199.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prenger et al. (2019)</span>
<span class="ltx_bibblock">
Ryan Prenger, Rafael Valle, and Bryan Catanzaro. 2019.

</span>
<span class="ltx_bibblock">Waveglow: A flow-based generative network for speech synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 3617–3621. IEEE.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2020)</span>
<span class="ltx_bibblock">
Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu.
2020.

</span>
<span class="ltx_bibblock">Fastspeech 2: Fast and high-quality end-to-end text to speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.04558</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al. (2022)</span>
<span class="ltx_bibblock">
Manuel Sam Ribeiro, Julian Roth, Giulia Comini, Goeric Huybrechts, Adam
Gabryś, and Jaime Lorenzo-Trueba. 2022.

</span>
<span class="ltx_bibblock">Cross-speaker style transfer for text-to-speech using data
augmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6797–6801. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2018)</span>
<span class="ltx_bibblock">
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly,
Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al.
2018.

</span>
<span class="ltx_bibblock">Natural tts synthesis by conditioning wavenet on mel spectrogram
predictions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pages 4779–4783. IEEE.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih et al. (2021)</span>
<span class="ltx_bibblock">
Kevin J Shih, Rafael Valle, Rohan Badlani, Adrian Lancucki, Wei Ping, and Bryan
Catanzaro. 2021.

</span>
<span class="ltx_bibblock">Rad-tts: Parallel flow-based tts with robust alignment learning and
diverse synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ICML Workshop on Invertible Neural Networks, Normalizing
Flows, and Explicit Likelihood Models</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2022)</span>
<span class="ltx_bibblock">
Eunwoo Song, Ryuichi Yamamoto, Ohsung Kwon, Chan-Ho Song, Min-Jae Hwang,
Suhyeon Oh, Hyun-Wook Yoon, Jin-Seob Kim, and Jae-Min Kim. 2022.

</span>
<span class="ltx_bibblock">Tts-by-tts 2: Data-selective augmentation for neural speech synthesis
using ranking support vector machine with variational autoencoder.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.14984</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2021)</span>
<span class="ltx_bibblock">
Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. 2021.

</span>
<span class="ltx_bibblock">A survey on neural speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.15561</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.

</span>
<span class="ltx_bibblock">Wavenet: A generative model for raw audio.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">9th ISCA Speech Synthesis Workshop</em>, pages 125–125.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo
Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023.

</span>
<span class="ltx_bibblock">Neural codec language models are zero-shot text to speech
synthesizers.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.02111</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiss et al. (2021)</span>
<span class="ltx_bibblock">
Ron J Weiss, RJ Skerry-Ryan, Eric Battenberg, Soroosh Mariooryad, and
Diederik P Kingma. 2021.

</span>
<span class="ltx_bibblock">Wave-tacotron: Spectrogram-free end-to-end text-to-speech synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 5679–5683. IEEE.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamamoto et al. (2020)</span>
<span class="ltx_bibblock">
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. 2020.

</span>
<span class="ltx_bibblock">Parallel wavegan: A fast waveform generation model based on
generative adversarial networks with multi-resolution spectrogram.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6199–6203. IEEE.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.01106" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.01107" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.01107">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.01107" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.01108" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 15:28:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
