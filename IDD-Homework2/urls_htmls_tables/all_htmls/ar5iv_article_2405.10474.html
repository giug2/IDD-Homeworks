<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text ltx_font_bold" id="id1.1.id1">
     Xinzhe Li
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     School of IT, Deakin University, Australia
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_typewriter" id="id2.2.id1">
      {lixinzhe, m.liu}@deakin.edu.au
     </span>
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text ltx_font_bold" id="id3.1.id1">
     Ming Liu
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     School of IT, Deakin University, Australia
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_typewriter" id="id4.2.id1">
      {lixinzhe, m.liu}@deakin.edu.au
     </span>
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id5.id1">
   Over the last decade, a wide range of training and deployment strategies for Large Language Models (LLMs) have emerged. Among these, the prompting paradigms of Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in Artificial Intelligence (AI). This paper aims to emphasize
   <em class="ltx_emph ltx_font_italic" id="id5.id1.1">
    the significance of utilizing free-form
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        <span class="ltx_text ltx_font_upright" id="footnote1.1.1.1">
         1
        </span>
       </span>
       <span class="ltx_text ltx_font_upright" id="footnote1.5">
        “Free-form” describes a stream of meaningful symbols, created by humans or through auto-regressive methods.
       </span>
      </span>
     </span>
    </span>
    modalities (forms of
input and output) and verbal free-form contexts as user-directed channels (methods for transforming modalities) for downstream deployment
   </em>
   .
Specifically, we analyze the structure of modalities within both two types of LLMs and six task-specific channels during deployment. From the perspective of users, our analysis introduces and applies the analytical metrics of task customizability, transparency, and complexity to gauge their usability, highlighting the superior nature of AR-LLMs’ prompting paradigms.
Moreover, we examine the stimulation of diverse cognitive behaviors in LLMs through the adoption of free-form text and verbal contexts, mirroring human linguistic expressions of such behaviors.
We then detail four common cognitive behaviors to underscore how AR-LLMs’ prompting successfully imitate human-like behaviors using this free-form modality and channel.
Lastly, the potential for improving LLM deployment, both as autonomous agents and within multi-agent systems, is identified via cognitive behavior concepts and principles.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    ChatGPT has emerged as the most popular AI application, with a vast user base. The success of GPT models can be attributed to the scaling of transformer-based neural networks and the extensive pre-training data, as explored in previous studies
    <cite class="ltx_cite ltx_citemacro_citep">
     (Radford
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2019
     </a>
     ; Brown
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2020
     </a>
     )
    </cite>
    .
The scope of this paper is directed towards Large Language Models (LLMs) that are sufficiently large to acquire world knowledge, commonsense, and the linguistic capabilities required to attain high performance on benchmarks such as GLUE
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wang
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2019
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Although LLMs are commonly perceived as general-purpose language intelligence models, the practice often diverges from employing a singular, all-encompassing model for every task. Instead, the deployment frequently entails developing a suite of specialized models tailored to specific tasks. This specialization is facilitated through the introduction of task-specific channels, modifying the model’s structure or its pre-trained parameters to better suit the nuances of individual tasks. This highlights a departure from the ideal of a universal, one-size-fits-all model, while the broad capabilities of LLMs suggest they could serve as jack-of-all-trades in language processing.
This trend towards creating task-specific models may stem from the tradition of evaluating linguistic intelligence through a variety of distinct tasks and benchmarks
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wang
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2019
     </a>
     )
    </cite>
    , with researchers striving to excel in these tasks independently to set new benchmarks. In this paper, we delve into the mechanisms behind prevalent deployment paradigms including AR-LLMs’ prompting, which underpins ChatGPT’s operation, and highlight several critical observations:
1) Models tailored with optimized task-specific channels often suffer from issues related to task customizability, transparency, and user-level complexity during deployment, affecting their overall usability;
2) Anticipated to mimic human-like intelligence, they often exhibit slow thinking through shortcuts
    <cite class="ltx_cite ltx_citemacro_citep">
     (Kahneman,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2011
     </a>
     )
    </cite>
    ;
3) They frequently fall short in showcasing advanced cognitive behaviors, which we contend are vital for convincing users of the models’ intelligence. Conversely, AR-LLMs’ prompting paradigms introduce a more natural, human-like channel (verbal free-form context) for representing a wide array of real-life tasks and employ form-form output modalities to showcase cognitive behaviors in complex scenarios.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Specifically, in this paper, we commence by examining the foundational principles of language modeling, revisiting the notable split in language modeling approaches that emerged in the late 2010s: auto-encoding LMs (AE-LMs) exemplified by BERT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Jin
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2020
     </a>
     )
    </cite>
    and auto-regressive LMs (AR-LMs) exemplified by the GPT series
    <cite class="ltx_cite ltx_citemacro_citep">
     (Radford
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2018
     </a>
     ; Brown
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2020
     </a>
     )
    </cite>
    . Rather than delve into an extensive array of deployment paradigms, we introduce and discuss the concepts of modalities and channels to investigate how LLMs are deployed (§
    <a class="ltx_ref" href="#S2" title="2 Deploying Large Language Models ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    ).
Upon evaluating different deployment paradigms for LLMs, it becomes clear that aside from the AR-LLMs’ prompting approach, other paradigms struggle to demonstrate advanced human-like cognitive behaviors. This shortfall is attributed to the constraints within modalities and channels, coupled with a tendency towards superficial learning, i.e., slow thinking (§
    <a class="ltx_ref" href="#S3" title="3 Evaluation of Modalities and Channels ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    and §
    <a class="ltx_ref" href="#S4.SS1" title="4.1 Thinking, Fast And Slow ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
     <span class="ltx_text ltx_ref_tag">
      4.1
     </span>
    </a>
    ).
In contrast, via specified context in the free-from text, the AR-LLMs’ prompting strategy imitate human-like cognitive behaviors, such as reasoning, planning, and feedback learning, which are elucidated in Table
    <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    (§
    <a class="ltx_ref" href="#S4" title="4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    ).
Finally, we explore how understanding cognitive behaviors can help overcome the tuning and deployment obstacles encountered by LLMs functioning as autonomous entities and within multi-agent frameworks (§
    <a class="ltx_ref" href="#S5" title="5 Bridging LLM Deployment Gaps with Insights from Cognitive Behaviors ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    ).
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Deploying Large Language Models
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    This section elucidates the dual objectives underlying language models, which both aim to model the joint probability distribution of text sequences through self-supervised learning techniques and generate text that is relevant to the given context.
After this introduction, we present a novel framework that facilitate the characterization of various deployment paradigms through two types of data modalities, which support language comprehension, coupled with six unique channels for processing these modalities.
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    The Fundamental Dichotomy in Language Modeling
   </h3>
   <section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Objective of Language Modeling
    </h4>
    <div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">
      The goal of language modeling is to estimate the joint probability distribution of sequences of text
      <cite class="ltx_cite ltx_citemacro_citep">
       (Bengio
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib2" title="">
        2003
       </a>
       )
      </cite>
      . This involves developing two distinct yet relaxed formulations for constructing LLMs that leverage self-supervised learning from vast quantities of unlabeled text data. The self-supervised approach enables the training of LLMs on extensive text corpora, a practice that has been thoroughly investigated in various studies
      <cite class="ltx_cite ltx_citemacro_citep">
       (Liu
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib18" title="">
        2019
       </a>
       ; Wei
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib39" title="">
        2022a
       </a>
       )
      </cite>
      . This paper focuses on how the intrinsic design of language models impacts their usability and potential to express cognitive behaviors.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Auto-Regressive (Left-to-Right) Language Modeling
    </h4>
    <div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.2">
      Typically, language modeling is approached by predicting the subsequent token in a sequence based on the preceding tokens. This prediction is quantified as the product of conditional probabilities for each subsequent token, considering its previous tokens, in accordance with the chain rule
      <cite class="ltx_cite ltx_citemacro_citep">
       (Bengio
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib2" title="">
        2003
       </a>
       )
      </cite>
      .
     </p>
     <table class="ltx_equation ltx_eqn_table" id="S2.E1">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\begin{split}P\left(w_{1},\ldots,w_{N}\right)=\prod_{t=1}^{N}P\left(w_{t}\mid w_{0},\ldots,w_{t-1}\right)\end{split}" class="ltx_Math" display="block" id="S2.E1.m1.33">
          <semantics id="S2.E1.m1.33a">
           <mtable displaystyle="true" id="S2.E1.m1.33.33.6" xref="S2.E1.m1.30.30.3.cmml">
            <mtr id="S2.E1.m1.33.33.6a" xref="S2.E1.m1.30.30.3.cmml">
             <mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.33.33.6b" xref="S2.E1.m1.30.30.3.cmml">
              <mrow id="S2.E1.m1.33.33.6.30.30.30" xref="S2.E1.m1.30.30.3.cmml">
               <mrow id="S2.E1.m1.32.32.5.29.29.29.29" xref="S2.E1.m1.30.30.3.cmml">
                <mi id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">
                 P
                </mi>
                <mo id="S2.E1.m1.32.32.5.29.29.29.29.3" lspace="0em" rspace="0em" xref="S2.E1.m1.30.30.3.cmml">
                 ​
                </mo>
                <mrow id="S2.E1.m1.32.32.5.29.29.29.29.2.2" xref="S2.E1.m1.30.30.3.cmml">
                 <mo id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.30.30.3.cmml">
                  (
                 </mo>
                 <msub id="S2.E1.m1.31.31.4.28.28.28.28.1.1.1" xref="S2.E1.m1.30.30.3.cmml">
                  <mi id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml">
                   w
                  </mi>
                  <mn id="S2.E1.m1.4.4.4.4.4.4.1" xref="S2.E1.m1.4.4.4.4.4.4.1.cmml">
                   1
                  </mn>
                 </msub>
                 <mo id="S2.E1.m1.5.5.5.5.5.5" xref="S2.E1.m1.30.30.3.cmml">
                  ,
                 </mo>
                 <mi id="S2.E1.m1.6.6.6.6.6.6" mathvariant="normal" xref="S2.E1.m1.6.6.6.6.6.6.cmml">
                  …
                 </mi>
                 <mo id="S2.E1.m1.7.7.7.7.7.7" xref="S2.E1.m1.30.30.3.cmml">
                  ,
                 </mo>
                 <msub id="S2.E1.m1.32.32.5.29.29.29.29.2.2.2" xref="S2.E1.m1.30.30.3.cmml">
                  <mi id="S2.E1.m1.8.8.8.8.8.8" xref="S2.E1.m1.8.8.8.8.8.8.cmml">
                   w
                  </mi>
                  <mi id="S2.E1.m1.9.9.9.9.9.9.1" xref="S2.E1.m1.9.9.9.9.9.9.1.cmml">
                   N
                  </mi>
                 </msub>
                 <mo id="S2.E1.m1.10.10.10.10.10.10" xref="S2.E1.m1.30.30.3.cmml">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mo id="S2.E1.m1.11.11.11.11.11.11" rspace="0.111em" xref="S2.E1.m1.11.11.11.11.11.11.cmml">
                =
               </mo>
               <mrow id="S2.E1.m1.33.33.6.30.30.30.30" xref="S2.E1.m1.30.30.3.cmml">
                <munderover id="S2.E1.m1.33.33.6.30.30.30.30.2" xref="S2.E1.m1.30.30.3.cmml">
                 <mo id="S2.E1.m1.12.12.12.12.12.12" movablelimits="false" xref="S2.E1.m1.12.12.12.12.12.12.cmml">
                  ∏
                 </mo>
                 <mrow id="S2.E1.m1.13.13.13.13.13.13.1" xref="S2.E1.m1.13.13.13.13.13.13.1.cmml">
                  <mi id="S2.E1.m1.13.13.13.13.13.13.1.2" xref="S2.E1.m1.13.13.13.13.13.13.1.2.cmml">
                   t
                  </mi>
                  <mo id="S2.E1.m1.13.13.13.13.13.13.1.1" xref="S2.E1.m1.13.13.13.13.13.13.1.1.cmml">
                   =
                  </mo>
                  <mn id="S2.E1.m1.13.13.13.13.13.13.1.3" xref="S2.E1.m1.13.13.13.13.13.13.1.3.cmml">
                   1
                  </mn>
                 </mrow>
                 <mi id="S2.E1.m1.14.14.14.14.14.14.1" xref="S2.E1.m1.14.14.14.14.14.14.1.cmml">
                  N
                 </mi>
                </munderover>
                <mrow id="S2.E1.m1.33.33.6.30.30.30.30.1" xref="S2.E1.m1.30.30.3.cmml">
                 <mi id="S2.E1.m1.15.15.15.15.15.15" xref="S2.E1.m1.15.15.15.15.15.15.cmml">
                  P
                 </mi>
                 <mo id="S2.E1.m1.33.33.6.30.30.30.30.1.2" lspace="0em" rspace="0em" xref="S2.E1.m1.30.30.3.cmml">
                  ​
                 </mo>
                 <mrow id="S2.E1.m1.33.33.6.30.30.30.30.1.1.1" xref="S2.E1.m1.30.30.3.cmml">
                  <mo id="S2.E1.m1.16.16.16.16.16.16" xref="S2.E1.m1.30.30.3.cmml">
                   (
                  </mo>
                  <mrow id="S2.E1.m1.33.33.6.30.30.30.30.1.1.1.1" xref="S2.E1.m1.30.30.3.cmml">
                   <msub id="S2.E1.m1.33.33.6.30.30.30.30.1.1.1.1.3" xref="S2.E1.m1.30.30.3.cmml">
                    <mi id="S2.E1.m1.17.17.17.17.17.17" xref="S2.E1.m1.17.17.17.17.17.17.cmml">
                     w
                    </mi>
                    <mi id="S2.E1.m1.18.18.18.18.18.18.1" xref="S2.E1.m1.18.18.18.18.18.18.1.cmml">
                     t
                    </mi>
                   </msub>
                   <mo id="S2.E1.m1.19.19.19.19.19.19" xref="S2.E1.m1.19.19.19.19.19.19.cmml">
                    ∣
                   </mo>
                   <mrow id="S2.E1.m1.33.33.6.30.30.30.30.1.1.1.1.2.2" xref="S2.E1.m1.30.30.3.cmml">
                    <msub id="S2.E1.m1.33.33.6.30.30.30.30.1.1.1.1.1.1.1" xref="S2.E1.m1.30.30.3.cmml">
                     <mi id="S2.E1.m1.20.20.20.20.20.20" xref="S2.E1.m1.20.20.20.20.20.20.cmml">
                      w
                     </mi>
                     <mn id="S2.E1.m1.21.21.21.21.21.21.1" xref="S2.E1.m1.21.21.21.21.21.21.1.cmml">
                      0
                     </mn>
                    </msub>
                    <mo id="S2.E1.m1.22.22.22.22.22.22" xref="S2.E1.m1.30.30.3.cmml">
                     ,
                    </mo>
                    <mi id="S2.E1.m1.23.23.23.23.23.23" mathvariant="normal" xref="S2.E1.m1.23.23.23.23.23.23.cmml">
                     …
                    </mi>
                    <mo id="S2.E1.m1.24.24.24.24.24.24" xref="S2.E1.m1.30.30.3.cmml">
                     ,
                    </mo>
                    <msub id="S2.E1.m1.33.33.6.30.30.30.30.1.1.1.1.2.2.2" xref="S2.E1.m1.30.30.3.cmml">
                     <mi id="S2.E1.m1.25.25.25.25.25.25" xref="S2.E1.m1.25.25.25.25.25.25.cmml">
                      w
                     </mi>
                     <mrow id="S2.E1.m1.26.26.26.26.26.26.1" xref="S2.E1.m1.26.26.26.26.26.26.1.cmml">
                      <mi id="S2.E1.m1.26.26.26.26.26.26.1.2" xref="S2.E1.m1.26.26.26.26.26.26.1.2.cmml">
                       t
                      </mi>
                      <mo id="S2.E1.m1.26.26.26.26.26.26.1.1" xref="S2.E1.m1.26.26.26.26.26.26.1.1.cmml">
                       −
                      </mo>
                      <mn id="S2.E1.m1.26.26.26.26.26.26.1.3" xref="S2.E1.m1.26.26.26.26.26.26.1.3.cmml">
                       1
                      </mn>
                     </mrow>
                    </msub>
                   </mrow>
                  </mrow>
                  <mo id="S2.E1.m1.27.27.27.27.27.27" xref="S2.E1.m1.30.30.3.cmml">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </mrow>
              </mrow>
             </mtd>
            </mtr>
           </mtable>
           <annotation-xml encoding="MathML-Content" id="S2.E1.m1.33b">
            <apply id="S2.E1.m1.30.30.3.cmml" xref="S2.E1.m1.33.33.6">
             <eq id="S2.E1.m1.11.11.11.11.11.11.cmml" xref="S2.E1.m1.11.11.11.11.11.11">
             </eq>
             <apply id="S2.E1.m1.29.29.2.2.cmml" xref="S2.E1.m1.33.33.6">
              <times id="S2.E1.m1.29.29.2.2.3.cmml" xref="S2.E1.m1.33.33.6">
              </times>
              <ci id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">
               𝑃
              </ci>
              <vector id="S2.E1.m1.29.29.2.2.2.3.cmml" xref="S2.E1.m1.33.33.6">
               <apply id="S2.E1.m1.28.28.1.1.1.1.1.cmml" xref="S2.E1.m1.33.33.6">
                <csymbol cd="ambiguous" id="S2.E1.m1.28.28.1.1.1.1.1.1.cmml" xref="S2.E1.m1.33.33.6">
                 subscript
                </csymbol>
                <ci id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3">
                 𝑤
                </ci>
                <cn id="S2.E1.m1.4.4.4.4.4.4.1.cmml" type="integer" xref="S2.E1.m1.4.4.4.4.4.4.1">
                 1
                </cn>
               </apply>
               <ci id="S2.E1.m1.6.6.6.6.6.6.cmml" xref="S2.E1.m1.6.6.6.6.6.6">
                …
               </ci>
               <apply id="S2.E1.m1.29.29.2.2.2.2.2.cmml" xref="S2.E1.m1.33.33.6">
                <csymbol cd="ambiguous" id="S2.E1.m1.29.29.2.2.2.2.2.1.cmml" xref="S2.E1.m1.33.33.6">
                 subscript
                </csymbol>
                <ci id="S2.E1.m1.8.8.8.8.8.8.cmml" xref="S2.E1.m1.8.8.8.8.8.8">
                 𝑤
                </ci>
                <ci id="S2.E1.m1.9.9.9.9.9.9.1.cmml" xref="S2.E1.m1.9.9.9.9.9.9.1">
                 𝑁
                </ci>
               </apply>
              </vector>
             </apply>
             <apply id="S2.E1.m1.30.30.3.3.cmml" xref="S2.E1.m1.33.33.6">
              <apply id="S2.E1.m1.30.30.3.3.2.cmml" xref="S2.E1.m1.33.33.6">
               <csymbol cd="ambiguous" id="S2.E1.m1.30.30.3.3.2.1.cmml" xref="S2.E1.m1.33.33.6">
                superscript
               </csymbol>
               <apply id="S2.E1.m1.30.30.3.3.2.2.cmml" xref="S2.E1.m1.33.33.6">
                <csymbol cd="ambiguous" id="S2.E1.m1.30.30.3.3.2.2.1.cmml" xref="S2.E1.m1.33.33.6">
                 subscript
                </csymbol>
                <csymbol cd="latexml" id="S2.E1.m1.12.12.12.12.12.12.cmml" xref="S2.E1.m1.12.12.12.12.12.12">
                 product
                </csymbol>
                <apply id="S2.E1.m1.13.13.13.13.13.13.1.cmml" xref="S2.E1.m1.13.13.13.13.13.13.1">
                 <eq id="S2.E1.m1.13.13.13.13.13.13.1.1.cmml" xref="S2.E1.m1.13.13.13.13.13.13.1.1">
                 </eq>
                 <ci id="S2.E1.m1.13.13.13.13.13.13.1.2.cmml" xref="S2.E1.m1.13.13.13.13.13.13.1.2">
                  𝑡
                 </ci>
                 <cn id="S2.E1.m1.13.13.13.13.13.13.1.3.cmml" type="integer" xref="S2.E1.m1.13.13.13.13.13.13.1.3">
                  1
                 </cn>
                </apply>
               </apply>
               <ci id="S2.E1.m1.14.14.14.14.14.14.1.cmml" xref="S2.E1.m1.14.14.14.14.14.14.1">
                𝑁
               </ci>
              </apply>
              <apply id="S2.E1.m1.30.30.3.3.1.cmml" xref="S2.E1.m1.33.33.6">
               <times id="S2.E1.m1.30.30.3.3.1.2.cmml" xref="S2.E1.m1.33.33.6">
               </times>
               <ci id="S2.E1.m1.15.15.15.15.15.15.cmml" xref="S2.E1.m1.15.15.15.15.15.15">
                𝑃
               </ci>
               <apply id="S2.E1.m1.30.30.3.3.1.1.1.1.cmml" xref="S2.E1.m1.33.33.6">
                <csymbol cd="latexml" id="S2.E1.m1.19.19.19.19.19.19.cmml" xref="S2.E1.m1.19.19.19.19.19.19">
                 conditional
                </csymbol>
                <apply id="S2.E1.m1.30.30.3.3.1.1.1.1.4.cmml" xref="S2.E1.m1.33.33.6">
                 <csymbol cd="ambiguous" id="S2.E1.m1.30.30.3.3.1.1.1.1.4.1.cmml" xref="S2.E1.m1.33.33.6">
                  subscript
                 </csymbol>
                 <ci id="S2.E1.m1.17.17.17.17.17.17.cmml" xref="S2.E1.m1.17.17.17.17.17.17">
                  𝑤
                 </ci>
                 <ci id="S2.E1.m1.18.18.18.18.18.18.1.cmml" xref="S2.E1.m1.18.18.18.18.18.18.1">
                  𝑡
                 </ci>
                </apply>
                <list id="S2.E1.m1.30.30.3.3.1.1.1.1.2.3.cmml" xref="S2.E1.m1.33.33.6">
                 <apply id="S2.E1.m1.30.30.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.33.33.6">
                  <csymbol cd="ambiguous" id="S2.E1.m1.30.30.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.33.33.6">
                   subscript
                  </csymbol>
                  <ci id="S2.E1.m1.20.20.20.20.20.20.cmml" xref="S2.E1.m1.20.20.20.20.20.20">
                   𝑤
                  </ci>
                  <cn id="S2.E1.m1.21.21.21.21.21.21.1.cmml" type="integer" xref="S2.E1.m1.21.21.21.21.21.21.1">
                   0
                  </cn>
                 </apply>
                 <ci id="S2.E1.m1.23.23.23.23.23.23.cmml" xref="S2.E1.m1.23.23.23.23.23.23">
                  …
                 </ci>
                 <apply id="S2.E1.m1.30.30.3.3.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.33.33.6">
                  <csymbol cd="ambiguous" id="S2.E1.m1.30.30.3.3.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.33.33.6">
                   subscript
                  </csymbol>
                  <ci id="S2.E1.m1.25.25.25.25.25.25.cmml" xref="S2.E1.m1.25.25.25.25.25.25">
                   𝑤
                  </ci>
                  <apply id="S2.E1.m1.26.26.26.26.26.26.1.cmml" xref="S2.E1.m1.26.26.26.26.26.26.1">
                   <minus id="S2.E1.m1.26.26.26.26.26.26.1.1.cmml" xref="S2.E1.m1.26.26.26.26.26.26.1.1">
                   </minus>
                   <ci id="S2.E1.m1.26.26.26.26.26.26.1.2.cmml" xref="S2.E1.m1.26.26.26.26.26.26.1.2">
                    𝑡
                   </ci>
                   <cn id="S2.E1.m1.26.26.26.26.26.26.1.3.cmml" type="integer" xref="S2.E1.m1.26.26.26.26.26.26.1.3">
                    1
                   </cn>
                  </apply>
                 </apply>
                </list>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S2.E1.m1.33c">
            \begin{split}P\left(w_{1},\ldots,w_{N}\right)=\prod_{t=1}^{N}P\left(w_{t}\mid w_{0},\ldots,w_{t-1}\right)\end{split}
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_right">
          (1)
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">
      Here,
      <math alttext="w_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.1.m1.1">
       <semantics id="S2.SS1.SSS0.Px2.p1.1.m1.1a">
        <msub id="S2.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">
          w
         </mi>
         <mn id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">
          0
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.1.m1.1b">
         <apply id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2">
           𝑤
          </ci>
          <cn id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3">
           0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.1.m1.1c">
         w_{0}
        </annotation>
       </semantics>
      </math>
      serves as a marker for the beginning of text.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Auto-Encoding (Denoising) Language Modeling
    </h4>
    <div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">
      In the context of auto-encoding language modeling, noise is intentionally introduced to an input sequence
      <math alttext="w_{1},w_{2},...w_{N}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.1.m1.3">
       <semantics id="S2.SS1.SSS0.Px3.p1.1.m1.3a">
        <mrow id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">
         <msub id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.cmml">
          <mi id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml">
           w
          </mi>
          <mn id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml">
           1
          </mn>
         </msub>
         <mo id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.4" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">
          ,
         </mo>
         <msub id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.cmml">
          <mi id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2.cmml">
           w
          </mi>
          <mn id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3.cmml">
           2
          </mn>
         </msub>
         <mo id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.5" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">
          ,
         </mo>
         <mrow id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.cmml">
          <mi id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2" mathvariant="normal" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2.cmml">
           …
          </mi>
          <mo id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1.cmml">
           ​
          </mo>
          <msub id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.cmml">
           <mi id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.2" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.2.cmml">
            w
           </mi>
           <mi id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.3" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.3.cmml">
            N
           </mi>
          </msub>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px3.p1.1.m1.3b">
         <list id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3">
          <apply id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1">
            subscript
           </csymbol>
           <ci id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2">
            𝑤
           </ci>
           <cn id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2">
            subscript
           </csymbol>
           <ci id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2">
            𝑤
           </ci>
           <cn id="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3">
            2
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3">
           <times id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1">
           </times>
           <ci id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2">
            …
           </ci>
           <apply id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3">
             subscript
            </csymbol>
            <ci id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.2">
             𝑤
            </ci>
            <ci id="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.3">
             𝑁
            </ci>
           </apply>
          </apply>
         </list>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px3.p1.1.m1.3c">
         w_{1},w_{2},...w_{N}
        </annotation>
       </semantics>
      </math>
      . The primary aim is to optimize
     </p>
     <table class="ltx_equation ltx_eqn_table" id="S2.E2">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\max\prod_{t=1}^{N}P\left(w_{t}\mid\hat{w}_{1},\ldots,\hat{w}_{N}\right)" class="ltx_Math" display="block" id="S2.E2.m1.2">
          <semantics id="S2.E2.m1.2a">
           <mrow id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">
            <mi id="S2.E2.m1.2.2.3" xref="S2.E2.m1.2.2.3.cmml">
             max
            </mi>
            <mo id="S2.E2.m1.2.2.2" lspace="0em" rspace="0em" xref="S2.E2.m1.2.2.2.cmml">
             ​
            </mo>
            <mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.cmml">
             <munderover id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.2.cmml">
              <mo id="S2.E2.m1.2.2.1.2.2.2" movablelimits="false" xref="S2.E2.m1.2.2.1.2.2.2.cmml">
               ∏
              </mo>
              <mrow id="S2.E2.m1.2.2.1.2.2.3" xref="S2.E2.m1.2.2.1.2.2.3.cmml">
               <mi id="S2.E2.m1.2.2.1.2.2.3.2" xref="S2.E2.m1.2.2.1.2.2.3.2.cmml">
                t
               </mi>
               <mo id="S2.E2.m1.2.2.1.2.2.3.1" xref="S2.E2.m1.2.2.1.2.2.3.1.cmml">
                =
               </mo>
               <mn id="S2.E2.m1.2.2.1.2.2.3.3" xref="S2.E2.m1.2.2.1.2.2.3.3.cmml">
                1
               </mn>
              </mrow>
              <mi id="S2.E2.m1.2.2.1.2.3" xref="S2.E2.m1.2.2.1.2.3.cmml">
               N
              </mi>
             </munderover>
             <mrow id="S2.E2.m1.2.2.1.1" xref="S2.E2.m1.2.2.1.1.cmml">
              <mi id="S2.E2.m1.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.3.cmml">
               P
              </mi>
              <mo id="S2.E2.m1.2.2.1.1.2" lspace="0em" rspace="0em" xref="S2.E2.m1.2.2.1.1.2.cmml">
               ​
              </mo>
              <mrow id="S2.E2.m1.2.2.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.cmml">
               <mo id="S2.E2.m1.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.cmml">
                (
               </mo>
               <mrow id="S2.E2.m1.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.cmml">
                <msub id="S2.E2.m1.2.2.1.1.1.1.1.4" xref="S2.E2.m1.2.2.1.1.1.1.1.4.cmml">
                 <mi id="S2.E2.m1.2.2.1.1.1.1.1.4.2" xref="S2.E2.m1.2.2.1.1.1.1.1.4.2.cmml">
                  w
                 </mi>
                 <mi id="S2.E2.m1.2.2.1.1.1.1.1.4.3" xref="S2.E2.m1.2.2.1.1.1.1.1.4.3.cmml">
                  t
                 </mi>
                </msub>
                <mo id="S2.E2.m1.2.2.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.3.cmml">
                 ∣
                </mo>
                <mrow id="S2.E2.m1.2.2.1.1.1.1.1.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.2.3.cmml">
                 <msub id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">
                  <mover accent="true" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">
                   <mi id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml">
                    w
                   </mi>
                   <mo id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml">
                    ^
                   </mo>
                  </mover>
                  <mn id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">
                   1
                  </mn>
                 </msub>
                 <mo id="S2.E2.m1.2.2.1.1.1.1.1.2.2.3" xref="S2.E2.m1.2.2.1.1.1.1.1.2.3.cmml">
                  ,
                 </mo>
                 <mi id="S2.E2.m1.1.1" mathvariant="normal" xref="S2.E2.m1.1.1.cmml">
                  …
                 </mi>
                 <mo id="S2.E2.m1.2.2.1.1.1.1.1.2.2.4" xref="S2.E2.m1.2.2.1.1.1.1.1.2.3.cmml">
                  ,
                 </mo>
                 <msub id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.cmml">
                  <mover accent="true" id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.cmml">
                   <mi id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.2.cmml">
                    w
                   </mi>
                   <mo id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.1" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.1.cmml">
                    ^
                   </mo>
                  </mover>
                  <mi id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.3.cmml">
                   N
                  </mi>
                 </msub>
                </mrow>
               </mrow>
               <mo id="S2.E2.m1.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b">
            <apply id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">
             <times id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2">
             </times>
             <max id="S2.E2.m1.2.2.3.cmml" xref="S2.E2.m1.2.2.3">
             </max>
             <apply id="S2.E2.m1.2.2.1.cmml" xref="S2.E2.m1.2.2.1">
              <apply id="S2.E2.m1.2.2.1.2.cmml" xref="S2.E2.m1.2.2.1.2">
               <csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.2.1.cmml" xref="S2.E2.m1.2.2.1.2">
                superscript
               </csymbol>
               <apply id="S2.E2.m1.2.2.1.2.2.cmml" xref="S2.E2.m1.2.2.1.2">
                <csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.2.2.1.cmml" xref="S2.E2.m1.2.2.1.2">
                 subscript
                </csymbol>
                <csymbol cd="latexml" id="S2.E2.m1.2.2.1.2.2.2.cmml" xref="S2.E2.m1.2.2.1.2.2.2">
                 product
                </csymbol>
                <apply id="S2.E2.m1.2.2.1.2.2.3.cmml" xref="S2.E2.m1.2.2.1.2.2.3">
                 <eq id="S2.E2.m1.2.2.1.2.2.3.1.cmml" xref="S2.E2.m1.2.2.1.2.2.3.1">
                 </eq>
                 <ci id="S2.E2.m1.2.2.1.2.2.3.2.cmml" xref="S2.E2.m1.2.2.1.2.2.3.2">
                  𝑡
                 </ci>
                 <cn id="S2.E2.m1.2.2.1.2.2.3.3.cmml" type="integer" xref="S2.E2.m1.2.2.1.2.2.3.3">
                  1
                 </cn>
                </apply>
               </apply>
               <ci id="S2.E2.m1.2.2.1.2.3.cmml" xref="S2.E2.m1.2.2.1.2.3">
                𝑁
               </ci>
              </apply>
              <apply id="S2.E2.m1.2.2.1.1.cmml" xref="S2.E2.m1.2.2.1.1">
               <times id="S2.E2.m1.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.2">
               </times>
               <ci id="S2.E2.m1.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3">
                𝑃
               </ci>
               <apply id="S2.E2.m1.2.2.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1">
                <csymbol cd="latexml" id="S2.E2.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.3">
                 conditional
                </csymbol>
                <apply id="S2.E2.m1.2.2.1.1.1.1.1.4.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.4">
                 <csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.4.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.4">
                  subscript
                 </csymbol>
                 <ci id="S2.E2.m1.2.2.1.1.1.1.1.4.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.4.2">
                  𝑤
                 </ci>
                 <ci id="S2.E2.m1.2.2.1.1.1.1.1.4.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.4.3">
                  𝑡
                 </ci>
                </apply>
                <list id="S2.E2.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2">
                 <apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1">
                  <csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1">
                   subscript
                  </csymbol>
                  <apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2">
                   <ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.1">
                    ^
                   </ci>
                   <ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2">
                    𝑤
                   </ci>
                  </apply>
                  <cn id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3">
                   1
                  </cn>
                 </apply>
                 <ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">
                  …
                 </ci>
                 <apply id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2">
                  <csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2">
                   subscript
                  </csymbol>
                  <apply id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2">
                   <ci id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.1">
                    ^
                   </ci>
                   <ci id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.2.2">
                    𝑤
                   </ci>
                  </apply>
                  <ci id="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.2.2.2.3">
                   𝑁
                  </ci>
                 </apply>
                </list>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S2.E2.m1.2c">
            \max\prod_{t=1}^{N}P\left(w_{t}\mid\hat{w}_{1},\ldots,\hat{w}_{N}\right)
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_right">
          (2)
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.2">
      where
      <math alttext="\hat{w}_{1},\hat{w}_{2},...\hat{w}_{N}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.2.m1.3">
       <semantics id="S2.SS1.SSS0.Px3.p1.2.m1.3a">
        <mrow id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.4.cmml">
         <msub id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.cmml">
          <mover accent="true" id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.cmml">
           <mi id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.2.cmml">
            w
           </mi>
           <mo id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.1" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.1.cmml">
            ^
           </mo>
          </mover>
          <mn id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.3" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.3.cmml">
           1
          </mn>
         </msub>
         <mo id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.4" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.4.cmml">
          ,
         </mo>
         <msub id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.cmml">
          <mover accent="true" id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.cmml">
           <mi id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.2.cmml">
            w
           </mi>
           <mo id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.1" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.1.cmml">
            ^
           </mo>
          </mover>
          <mn id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.3" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.3.cmml">
           2
          </mn>
         </msub>
         <mo id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.5" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.4.cmml">
          ,
         </mo>
         <mrow id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.cmml">
          <mi id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.2" mathvariant="normal" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.2.cmml">
           …
          </mi>
          <mo id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.1" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.1.cmml">
           ​
          </mo>
          <msub id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.cmml">
           <mover accent="true" id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.cmml">
            <mi id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.2" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.2.cmml">
             w
            </mi>
            <mo id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.1" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.1.cmml">
             ^
            </mo>
           </mover>
           <mi id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.3" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.3.cmml">
            N
           </mi>
          </msub>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px3.p1.2.m1.3b">
         <list id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.4.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3">
          <apply id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1">
            subscript
           </csymbol>
           <apply id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2">
            <ci id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.1">
             ^
            </ci>
            <ci id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.2.2">
             𝑤
            </ci>
           </apply>
           <cn id="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px3.p1.2.m1.1.1.1.1.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2">
            subscript
           </csymbol>
           <apply id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2">
            <ci id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.1">
             ^
            </ci>
            <ci id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.2.2">
             𝑤
            </ci>
           </apply>
           <cn id="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px3.p1.2.m1.2.2.2.2.3">
            2
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3">
           <times id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.1">
           </times>
           <ci id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.2">
            …
           </ci>
           <apply id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3">
             subscript
            </csymbol>
            <apply id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2">
             <ci id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.1.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.1">
              ^
             </ci>
             <ci id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.2.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.2.2">
              𝑤
             </ci>
            </apply>
            <ci id="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px3.p1.2.m1.3.3.3.3.3.3">
             𝑁
            </ci>
           </apply>
          </apply>
         </list>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px3.p1.2.m1.3c">
         \hat{w}_{1},\hat{w}_{2},...\hat{w}_{N}
        </annotation>
       </semantics>
      </math>
      represents the altered, noise-added version of the input sequence.
The approach of masking specific tokens in the text at random, known as token-level masked language modeling
      <cite class="ltx_cite ltx_citemacro_citep">
       (Devlin
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib4" title="">
        2019
       </a>
       )
      </cite>
      , is a widely adopted strategy. This involves substituting original tokens with a special token, such as “[MASK]”, and training the model to predict these original tokens based on the context of the surrounding, unmasked tokens. The discrepancy between the original and reconstructed sequences is quantified through a reconstruction loss:
     </p>
     <table class="ltx_equation ltx_eqn_table" id="S2.E3">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="L_{reconstruction}=-\sum_{t=1}^{N}\log P\left(w_{t}\mid\hat{w}_{1},\ldots,\hat{w}_{N}\right)" class="ltx_Math" display="block" id="S2.E3.m1.2">
          <semantics id="S2.E3.m1.2a">
           <mrow id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">
            <msub id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml">
             <mi id="S2.E3.m1.2.2.3.2" xref="S2.E3.m1.2.2.3.2.cmml">
              L
             </mi>
             <mrow id="S2.E3.m1.2.2.3.3" xref="S2.E3.m1.2.2.3.3.cmml">
              <mi id="S2.E3.m1.2.2.3.3.2" xref="S2.E3.m1.2.2.3.3.2.cmml">
               r
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.3" xref="S2.E3.m1.2.2.3.3.3.cmml">
               e
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1a" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.4" xref="S2.E3.m1.2.2.3.3.4.cmml">
               c
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1b" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.5" xref="S2.E3.m1.2.2.3.3.5.cmml">
               o
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1c" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.6" xref="S2.E3.m1.2.2.3.3.6.cmml">
               n
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1d" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.7" xref="S2.E3.m1.2.2.3.3.7.cmml">
               s
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1e" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.8" xref="S2.E3.m1.2.2.3.3.8.cmml">
               t
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1f" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.9" xref="S2.E3.m1.2.2.3.3.9.cmml">
               r
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1g" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.10" xref="S2.E3.m1.2.2.3.3.10.cmml">
               u
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1h" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.11" xref="S2.E3.m1.2.2.3.3.11.cmml">
               c
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1i" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.12" xref="S2.E3.m1.2.2.3.3.12.cmml">
               t
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1j" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.13" xref="S2.E3.m1.2.2.3.3.13.cmml">
               i
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1k" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.14" xref="S2.E3.m1.2.2.3.3.14.cmml">
               o
              </mi>
              <mo id="S2.E3.m1.2.2.3.3.1l" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.3.3.1.cmml">
               ​
              </mo>
              <mi id="S2.E3.m1.2.2.3.3.15" xref="S2.E3.m1.2.2.3.3.15.cmml">
               n
              </mi>
             </mrow>
            </msub>
            <mo id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml">
             =
            </mo>
            <mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.cmml">
             <mo id="S2.E3.m1.2.2.1a" xref="S2.E3.m1.2.2.1.cmml">
              −
             </mo>
             <mrow id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml">
              <munderover id="S2.E3.m1.2.2.1.1.2" xref="S2.E3.m1.2.2.1.1.2.cmml">
               <mo id="S2.E3.m1.2.2.1.1.2.2.2" movablelimits="false" xref="S2.E3.m1.2.2.1.1.2.2.2.cmml">
                ∑
               </mo>
               <mrow id="S2.E3.m1.2.2.1.1.2.2.3" xref="S2.E3.m1.2.2.1.1.2.2.3.cmml">
                <mi id="S2.E3.m1.2.2.1.1.2.2.3.2" xref="S2.E3.m1.2.2.1.1.2.2.3.2.cmml">
                 t
                </mi>
                <mo id="S2.E3.m1.2.2.1.1.2.2.3.1" xref="S2.E3.m1.2.2.1.1.2.2.3.1.cmml">
                 =
                </mo>
                <mn id="S2.E3.m1.2.2.1.1.2.2.3.3" xref="S2.E3.m1.2.2.1.1.2.2.3.3.cmml">
                 1
                </mn>
               </mrow>
               <mi id="S2.E3.m1.2.2.1.1.2.3" xref="S2.E3.m1.2.2.1.1.2.3.cmml">
                N
               </mi>
              </munderover>
              <mrow id="S2.E3.m1.2.2.1.1.1" xref="S2.E3.m1.2.2.1.1.1.cmml">
               <mrow id="S2.E3.m1.2.2.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.3.cmml">
                <mi id="S2.E3.m1.2.2.1.1.1.3.1" xref="S2.E3.m1.2.2.1.1.1.3.1.cmml">
                 log
                </mi>
                <mo id="S2.E3.m1.2.2.1.1.1.3a" lspace="0.167em" xref="S2.E3.m1.2.2.1.1.1.3.cmml">
                 ⁡
                </mo>
                <mi id="S2.E3.m1.2.2.1.1.1.3.2" xref="S2.E3.m1.2.2.1.1.1.3.2.cmml">
                 P
                </mi>
               </mrow>
               <mo id="S2.E3.m1.2.2.1.1.1.2" lspace="0em" rspace="0em" xref="S2.E3.m1.2.2.1.1.1.2.cmml">
                ​
               </mo>
               <mrow id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml">
                <mo id="S2.E3.m1.2.2.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml">
                 (
                </mo>
                <mrow id="S2.E3.m1.2.2.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml">
                 <msub id="S2.E3.m1.2.2.1.1.1.1.1.1.4" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4.cmml">
                  <mi id="S2.E3.m1.2.2.1.1.1.1.1.1.4.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4.2.cmml">
                   w
                  </mi>
                  <mi id="S2.E3.m1.2.2.1.1.1.1.1.1.4.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4.3.cmml">
                   t
                  </mi>
                 </msub>
                 <mo id="S2.E3.m1.2.2.1.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3.cmml">
                  ∣
                 </mo>
                 <mrow id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.3.cmml">
                  <msub id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">
                   <mover accent="true" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">
                    <mi id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">
                     w
                    </mi>
                    <mo id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml">
                     ^
                    </mo>
                   </mover>
                   <mn id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">
                    1
                   </mn>
                  </msub>
                  <mo id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.3.cmml">
                   ,
                  </mo>
                  <mi id="S2.E3.m1.1.1" mathvariant="normal" xref="S2.E3.m1.1.1.cmml">
                   …
                  </mi>
                  <mo id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.4" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.3.cmml">
                   ,
                  </mo>
                  <msub id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.cmml">
                   <mover accent="true" id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml">
                    <mi id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.2.cmml">
                     w
                    </mi>
                    <mo id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.1.cmml">
                     ^
                    </mo>
                   </mover>
                   <mi id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml">
                    N
                   </mi>
                  </msub>
                 </mrow>
                </mrow>
                <mo id="S2.E3.m1.2.2.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml">
                 )
                </mo>
               </mrow>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b">
            <apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">
             <eq id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2">
             </eq>
             <apply id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3">
              <csymbol cd="ambiguous" id="S2.E3.m1.2.2.3.1.cmml" xref="S2.E3.m1.2.2.3">
               subscript
              </csymbol>
              <ci id="S2.E3.m1.2.2.3.2.cmml" xref="S2.E3.m1.2.2.3.2">
               𝐿
              </ci>
              <apply id="S2.E3.m1.2.2.3.3.cmml" xref="S2.E3.m1.2.2.3.3">
               <times id="S2.E3.m1.2.2.3.3.1.cmml" xref="S2.E3.m1.2.2.3.3.1">
               </times>
               <ci id="S2.E3.m1.2.2.3.3.2.cmml" xref="S2.E3.m1.2.2.3.3.2">
                𝑟
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.3.cmml" xref="S2.E3.m1.2.2.3.3.3">
                𝑒
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.4.cmml" xref="S2.E3.m1.2.2.3.3.4">
                𝑐
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.5.cmml" xref="S2.E3.m1.2.2.3.3.5">
                𝑜
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.6.cmml" xref="S2.E3.m1.2.2.3.3.6">
                𝑛
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.7.cmml" xref="S2.E3.m1.2.2.3.3.7">
                𝑠
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.8.cmml" xref="S2.E3.m1.2.2.3.3.8">
                𝑡
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.9.cmml" xref="S2.E3.m1.2.2.3.3.9">
                𝑟
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.10.cmml" xref="S2.E3.m1.2.2.3.3.10">
                𝑢
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.11.cmml" xref="S2.E3.m1.2.2.3.3.11">
                𝑐
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.12.cmml" xref="S2.E3.m1.2.2.3.3.12">
                𝑡
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.13.cmml" xref="S2.E3.m1.2.2.3.3.13">
                𝑖
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.14.cmml" xref="S2.E3.m1.2.2.3.3.14">
                𝑜
               </ci>
               <ci id="S2.E3.m1.2.2.3.3.15.cmml" xref="S2.E3.m1.2.2.3.3.15">
                𝑛
               </ci>
              </apply>
             </apply>
             <apply id="S2.E3.m1.2.2.1.cmml" xref="S2.E3.m1.2.2.1">
              <minus id="S2.E3.m1.2.2.1.2.cmml" xref="S2.E3.m1.2.2.1">
              </minus>
              <apply id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1.1">
               <apply id="S2.E3.m1.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.2">
                <csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2">
                 superscript
                </csymbol>
                <apply id="S2.E3.m1.2.2.1.1.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2">
                 <csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2">
                  subscript
                 </csymbol>
                 <sum id="S2.E3.m1.2.2.1.1.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2">
                 </sum>
                 <apply id="S2.E3.m1.2.2.1.1.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.3">
                  <eq id="S2.E3.m1.2.2.1.1.2.2.3.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.3.1">
                  </eq>
                  <ci id="S2.E3.m1.2.2.1.1.2.2.3.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.3.2">
                   𝑡
                  </ci>
                  <cn id="S2.E3.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S2.E3.m1.2.2.1.1.2.2.3.3">
                   1
                  </cn>
                 </apply>
                </apply>
                <ci id="S2.E3.m1.2.2.1.1.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.3">
                 𝑁
                </ci>
               </apply>
               <apply id="S2.E3.m1.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1">
                <times id="S2.E3.m1.2.2.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.2">
                </times>
                <apply id="S2.E3.m1.2.2.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.3">
                 <log id="S2.E3.m1.2.2.1.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.3.1">
                 </log>
                 <ci id="S2.E3.m1.2.2.1.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.3.2">
                  𝑃
                 </ci>
                </apply>
                <apply id="S2.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1">
                 <csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3">
                  conditional
                 </csymbol>
                 <apply id="S2.E3.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4">
                  <csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.4.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4">
                   subscript
                  </csymbol>
                  <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.4.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4.2">
                   𝑤
                  </ci>
                  <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.4.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.4.3">
                   𝑡
                  </ci>
                 </apply>
                 <list id="S2.E3.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2">
                  <apply id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1">
                   <csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1">
                    subscript
                   </csymbol>
                   <apply id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2">
                    <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.1">
                     ^
                    </ci>
                    <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">
                     𝑤
                    </ci>
                   </apply>
                   <cn id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3">
                    1
                   </cn>
                  </apply>
                  <ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">
                   …
                  </ci>
                  <apply id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2">
                   <csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2">
                    subscript
                   </csymbol>
                   <apply id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2">
                    <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.1">
                     ^
                    </ci>
                    <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.2.2">
                     𝑤
                    </ci>
                   </apply>
                   <ci id="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.2.2.3">
                    𝑁
                   </ci>
                  </apply>
                 </list>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S2.E3.m1.2c">
            L_{reconstruction}=-\sum_{t=1}^{N}\log P\left(w_{t}\mid\hat{w}_{1},\ldots,\hat{w}_{N}\right)
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_right">
          (3)
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.3">
      This denoising methodology also includes other variants such as span-level masked language modeling
      <cite class="ltx_cite ltx_citemacro_citep">
       (Joshi
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib11" title="">
        2020
       </a>
       )
      </cite>
      , text infilling
      <cite class="ltx_cite ltx_citemacro_citep">
       (Lewis
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib14" title="">
        2020
       </a>
       )
      </cite>
      , among others.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Exploring the Modalities within Large Language Models
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     This section delves into the concept of “modalities” within LLMs, a term often implicitly associated with research on multimodal systems to describe diverse, human-like channels of communication, such as text, speech, gestures, and visual inputs
     <cite class="ltx_cite ltx_citemacro_citep">
      (Bartneck
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib1" title="">
       2020
      </a>
      )
     </cite>
     . Here, “modalities” specifically refer to the various forms of input and output data utilized in LLM deployment.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.11">
     In the operation of both AR-LLMs and AE-LLMs, we identify three primary modalities: a unique textual modality for both the input and output in AR-LLMs (unrestricted text), a distinct textual modality for AE-LLMs (masked text or contextualized n-grams), and a shared modality of intermediate dense representations applicable to both models:
1)
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p2.11.1">
      Intermediate Dense Representations
     </span>
     : Fundamentally, LLMs convert each word (or subword) in a sequence into dense vector embeddings. These embeddings are generated through a series of mathematical operations, such as the self-attention mechanism, at every layer of the neural network, and are represented as
     <math alttext="\left\{h_{i}^{l}\right\}" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1">
      <semantics id="S2.SS2.p2.1.m1.1a">
       <mrow id="S2.SS2.p2.1.m1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.2.cmml">
        <mo id="S2.SS2.p2.1.m1.1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">
         {
        </mo>
        <msubsup id="S2.SS2.p2.1.m1.1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1.1.cmml">
         <mi id="S2.SS2.p2.1.m1.1.1.1.1.2.2" xref="S2.SS2.p2.1.m1.1.1.1.1.2.2.cmml">
          h
         </mi>
         <mi id="S2.SS2.p2.1.m1.1.1.1.1.2.3" xref="S2.SS2.p2.1.m1.1.1.1.1.2.3.cmml">
          i
         </mi>
         <mi id="S2.SS2.p2.1.m1.1.1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.1.1.3.cmml">
          l
         </mi>
        </msubsup>
        <mo id="S2.SS2.p2.1.m1.1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.2.cmml">
         }
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b">
        <set id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.1">
         <apply id="S2.SS2.p2.1.m1.1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1">
          <csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1">
           superscript
          </csymbol>
          <apply id="S2.SS2.p2.1.m1.1.1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1">
           <csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.1.2.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1">
            subscript
           </csymbol>
           <ci id="S2.SS2.p2.1.m1.1.1.1.1.2.2.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1.2.2">
            ℎ
           </ci>
           <ci id="S2.SS2.p2.1.m1.1.1.1.1.2.3.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1.2.3">
            𝑖
           </ci>
          </apply>
          <ci id="S2.SS2.p2.1.m1.1.1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1.3">
           𝑙
          </ci>
         </apply>
        </set>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">
        \left\{h_{i}^{l}\right\}
       </annotation>
      </semantics>
     </math>
     for every position
     <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1">
      <semantics id="S2.SS2.p2.2.m2.1a">
       <mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">
        i
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b">
        <ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">
         𝑖
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">
        i
       </annotation>
      </semantics>
     </math>
     within the sequence and for every layer
     <math alttext="l" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m3.1">
      <semantics id="S2.SS2.p2.3.m3.1a">
       <mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">
        l
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b">
        <ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">
         𝑙
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">
        l
       </annotation>
      </semantics>
     </math>
     in the model. Here,
     <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p2.4.m4.1">
      <semantics id="S2.SS2.p2.4.m4.1a">
       <mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">
        i
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b">
        <ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">
         𝑖
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">
        i
       </annotation>
      </semantics>
     </math>
     ranges from
     <math alttext="1" class="ltx_Math" display="inline" id="S2.SS2.p2.5.m5.1">
      <semantics id="S2.SS2.p2.5.m5.1a">
       <mn id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">
        1
       </mn>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b">
        <cn id="S2.SS2.p2.5.m5.1.1.cmml" type="integer" xref="S2.SS2.p2.5.m5.1.1">
         1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">
        1
       </annotation>
      </semantics>
     </math>
     to
     <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p2.6.m6.1">
      <semantics id="S2.SS2.p2.6.m6.1a">
       <mi id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b">
        <ci id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">
         𝑁
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">
        N
       </annotation>
      </semantics>
     </math>
     , with
     <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p2.7.m7.1">
      <semantics id="S2.SS2.p2.7.m7.1a">
       <mi id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b">
        <ci id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">
         𝑁
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">
        N
       </annotation>
      </semantics>
     </math>
     indicating the total number of elements in the sequence, and
     <math alttext="l" class="ltx_Math" display="inline" id="S2.SS2.p2.8.m8.1">
      <semantics id="S2.SS2.p2.8.m8.1a">
       <mi id="S2.SS2.p2.8.m8.1.1" xref="S2.SS2.p2.8.m8.1.1.cmml">
        l
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m8.1b">
        <ci id="S2.SS2.p2.8.m8.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1">
         𝑙
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.8.m8.1c">
        l
       </annotation>
      </semantics>
     </math>
     spans from
     <math alttext="1" class="ltx_Math" display="inline" id="S2.SS2.p2.9.m9.1">
      <semantics id="S2.SS2.p2.9.m9.1a">
       <mn id="S2.SS2.p2.9.m9.1.1" xref="S2.SS2.p2.9.m9.1.1.cmml">
        1
       </mn>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m9.1b">
        <cn id="S2.SS2.p2.9.m9.1.1.cmml" type="integer" xref="S2.SS2.p2.9.m9.1.1">
         1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.9.m9.1c">
        1
       </annotation>
      </semantics>
     </math>
     to
     <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.p2.10.m10.1">
      <semantics id="S2.SS2.p2.10.m10.1a">
       <mi id="S2.SS2.p2.10.m10.1.1" xref="S2.SS2.p2.10.m10.1.1.cmml">
        L
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m10.1b">
        <ci id="S2.SS2.p2.10.m10.1.1.cmml" xref="S2.SS2.p2.10.m10.1.1">
         𝐿
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.10.m10.1c">
        L
       </annotation>
      </semantics>
     </math>
     , where
     <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.p2.11.m11.1">
      <semantics id="S2.SS2.p2.11.m11.1a">
       <mi id="S2.SS2.p2.11.m11.1.1" xref="S2.SS2.p2.11.m11.1.1.cmml">
        L
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.11.m11.1b">
        <ci id="S2.SS2.p2.11.m11.1.1.cmml" xref="S2.SS2.p2.11.m11.1.1">
         𝐿
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.11.m11.1c">
        L
       </annotation>
      </semantics>
     </math>
     represents the complete count of layers within the model.
2)
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p2.11.2">
      Textual Modalities
     </span>
     : AE-LLMs feature an input modality of masked text, with the output modality being contextualized n-grams designed to reconstruct the masked sections. Conversely, due to their auto-regressive design, AR-LLMs are capable of encoding any text as context and generating free-form text outputs, thereby employing unrestricted text for both input and output. These modalities are inherently linked to their respective language modeling strategies.
    </p>
   </div>
   <figure class="ltx_table" id="S2.T1">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.6">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S2.T1.6.7.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.6.7.1.1">
        <span class="ltx_text" id="S2.T1.6.7.1.1.1" style="font-size:90%;">
         Channels
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.2">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.7.1.2.1">
         <span class="ltx_p" id="S2.T1.6.7.1.2.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.6.7.1.2.1.1.1" style="font-size:90%;">
           Relevant Paradigms
          </span>
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.7.1.3.1">
         <span class="ltx_p" id="S2.T1.6.7.1.3.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.6.7.1.3.1.1.1" style="font-size:90%;">
           Customizability
          </span>
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.7.1.4.1">
         <span class="ltx_p" id="S2.T1.6.7.1.4.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.6.7.1.4.1.1.1" style="font-size:90%;">
           Transparency
          </span>
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.6.7.1.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.7.1.5.1">
         <span class="ltx_p" id="S2.T1.6.7.1.5.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.6.7.1.5.1.1.1" style="font-size:90%;">
           Complexity
          </span>
         </span>
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S2.T1.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.2">
        <span class="ltx_text" id="S2.T1.1.1.2.1" style="font-size:90%;">
         Adapter
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.1">
         <span class="ltx_p" id="S2.T1.1.1.3.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.1.1.3.1.1.1" style="font-size:90%;">
           Adapter tuning
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1">
         <span class="ltx_p" id="S2.T1.1.1.4.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.1.1.4.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.1">
         <span class="ltx_p" id="S2.T1.1.1.5.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.1.1.5.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.1">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1">
         <span class="ltx_p" id="S2.T1.1.1.1.1.1" style="width:56.9pt;">
          <math alttext="T" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.m1.1">
           <semantics id="S2.T1.1.1.1.1.1.m1.1a">
            <mi id="S2.T1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S2.T1.1.1.1.1.1.m1.1.1.cmml">
             T
            </mi>
            <annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.m1.1b">
             <ci id="S2.T1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1">
              𝑇
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.m1.1c">
             T
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T1.2.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.2.2.2">
        <span class="ltx_text" id="S2.T1.2.2.2.1" style="font-size:90%;">
         Output layers
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.2.2.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.3.1">
         <span class="ltx_p" id="S2.T1.2.2.3.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.2.2.3.1.1.1" style="font-size:90%;">
           LLM fine-tuning; Adapter tuning
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.2.2.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.4.1">
         <span class="ltx_p" id="S2.T1.2.2.4.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.2.2.4.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.2.2.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.5.1">
         <span class="ltx_p" id="S2.T1.2.2.5.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.2.2.5.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.2.2.1">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.1.1">
         <span class="ltx_p" id="S2.T1.2.2.1.1.1" style="width:56.9pt;">
          <math alttext="T" class="ltx_Math" display="inline" id="S2.T1.2.2.1.1.1.m1.1">
           <semantics id="S2.T1.2.2.1.1.1.m1.1a">
            <mi id="S2.T1.2.2.1.1.1.m1.1.1" mathsize="90%" xref="S2.T1.2.2.1.1.1.m1.1.1.cmml">
             T
            </mi>
            <annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.1.1.m1.1b">
             <ci id="S2.T1.2.2.1.1.1.m1.1.1.cmml" xref="S2.T1.2.2.1.1.1.m1.1.1">
              𝑇
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S2.T1.2.2.1.1.1.m1.1c">
             T
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T1.3.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.3.3.2">
        <span class="ltx_text" id="S2.T1.3.3.2.1" style="font-size:90%;">
         LLMs
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.3.3.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.3.1">
         <span class="ltx_p" id="S2.T1.3.3.3.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.3.3.3.1.1.1" style="font-size:90%;">
           LLM fine-tuning; PET
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.3.3.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.4.1">
         <span class="ltx_p" id="S2.T1.3.3.4.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.3.3.4.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.3.3.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.5.1">
         <span class="ltx_p" id="S2.T1.3.3.5.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.3.3.5.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.3.3.1">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.1.1">
         <span class="ltx_p" id="S2.T1.3.3.1.1.1" style="width:56.9pt;">
          <math alttext="T" class="ltx_Math" display="inline" id="S2.T1.3.3.1.1.1.m1.1">
           <semantics id="S2.T1.3.3.1.1.1.m1.1a">
            <mi id="S2.T1.3.3.1.1.1.m1.1.1" mathsize="90%" xref="S2.T1.3.3.1.1.1.m1.1.1.cmml">
             T
            </mi>
            <annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.1.1.m1.1b">
             <ci id="S2.T1.3.3.1.1.1.m1.1.1.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1">
              𝑇
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S2.T1.3.3.1.1.1.m1.1c">
             T
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T1.4.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.4.4.2">
        <span class="ltx_text" id="S2.T1.4.4.2.1" style="font-size:90%;">
         Activation prefixes
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.4.4.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.1">
         <span class="ltx_p" id="S2.T1.4.4.3.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.4.4.3.1.1.1" style="font-size:90%;">
           Prefix tuning
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.4.4.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.4.1">
         <span class="ltx_p" id="S2.T1.4.4.4.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.4.4.4.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.4.4.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.5.1">
         <span class="ltx_p" id="S2.T1.4.4.5.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.4.4.5.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.4.4.1">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.1.1">
         <span class="ltx_p" id="S2.T1.4.4.1.1.1" style="width:56.9pt;">
          <math alttext="T" class="ltx_Math" display="inline" id="S2.T1.4.4.1.1.1.m1.1">
           <semantics id="S2.T1.4.4.1.1.1.m1.1a">
            <mi id="S2.T1.4.4.1.1.1.m1.1.1" mathsize="90%" xref="S2.T1.4.4.1.1.1.m1.1.1.cmml">
             T
            </mi>
            <annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.1.1.m1.1b">
             <ci id="S2.T1.4.4.1.1.1.m1.1.1.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1">
              𝑇
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S2.T1.4.4.1.1.1.m1.1c">
             T
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T1.5.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.5.5.2">
        <span class="ltx_text" id="S2.T1.5.5.2.1" style="font-size:90%;">
         Verbal free-form context
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.5.5.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.5.5.3.1">
         <span class="ltx_p" id="S2.T1.5.5.3.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.5.5.3.1.1.1" style="font-size:90%;">
           AR-LLMs’ prompting
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.5.5.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.5.5.4.1">
         <span class="ltx_p" id="S2.T1.5.5.4.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.5.5.4.1.1.1" style="font-size:90%;">
           ✓
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.5.5.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.5.5.5.1">
         <span class="ltx_p" id="S2.T1.5.5.5.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.5.5.5.1.1.1" style="font-size:90%;">
           ✓
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.5.5.1">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.5.5.1.1">
         <span class="ltx_p" id="S2.T1.5.5.1.1.1" style="width:56.9pt;">
          <math alttext="0" class="ltx_Math" display="inline" id="S2.T1.5.5.1.1.1.m1.1">
           <semantics id="S2.T1.5.5.1.1.1.m1.1a">
            <mn id="S2.T1.5.5.1.1.1.m1.1.1" mathsize="90%" xref="S2.T1.5.5.1.1.1.m1.1.1.cmml">
             0
            </mn>
            <annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.1.1.m1.1b">
             <cn id="S2.T1.5.5.1.1.1.m1.1.1.cmml" type="integer" xref="S2.T1.5.5.1.1.1.m1.1.1">
              0
             </cn>
            </annotation-xml>
           </semantics>
          </math>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T1.6.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S2.T1.6.6.2">
        <span class="ltx_text" id="S2.T1.6.6.2.1" style="font-size:90%;">
         Contextual text patterns
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T1.6.6.3">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.6.3.1">
         <span class="ltx_p" id="S2.T1.6.6.3.1.1" style="width:85.4pt;">
          <span class="ltx_text" id="S2.T1.6.6.3.1.1.1" style="font-size:90%;">
           PET;
          </span>
          <br class="ltx_break"/>
          <span class="ltx_text" id="S2.T1.6.6.3.1.1.2" style="font-size:90%;">
           Auto-prompt
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T1.6.6.4">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.6.4.1">
         <span class="ltx_p" id="S2.T1.6.6.4.1.1" style="width:56.9pt;">
          <span class="ltx_text" id="S2.T1.6.6.4.1.1.1" style="font-size:90%;">
           ✗
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T1.6.6.5">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.6.5.1">
         <span class="ltx_p" id="S2.T1.6.6.5.1.1" style="width:71.1pt;">
          <span class="ltx_text" id="S2.T1.6.6.5.1.1.1" style="font-size:90%;">
           ✓(PET);
          </span>
          <br class="ltx_break"/>
          <span class="ltx_text" id="S2.T1.6.6.5.1.1.2" style="font-size:90%;">
           ✗(Auto-prompt)
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T1.6.6.1">
        <span class="ltx_inline-block ltx_align_top" id="S2.T1.6.6.1.1">
         <span class="ltx_p" id="S2.T1.6.6.1.1.1" style="width:56.9pt;">
          <math alttext="N\times T" class="ltx_Math" display="inline" id="S2.T1.6.6.1.1.1.m1.1">
           <semantics id="S2.T1.6.6.1.1.1.m1.1a">
            <mrow id="S2.T1.6.6.1.1.1.m1.1.1" xref="S2.T1.6.6.1.1.1.m1.1.1.cmml">
             <mi id="S2.T1.6.6.1.1.1.m1.1.1.2" mathsize="90%" xref="S2.T1.6.6.1.1.1.m1.1.1.2.cmml">
              N
             </mi>
             <mo id="S2.T1.6.6.1.1.1.m1.1.1.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S2.T1.6.6.1.1.1.m1.1.1.1.cmml">
              ×
             </mo>
             <mi id="S2.T1.6.6.1.1.1.m1.1.1.3" mathsize="90%" xref="S2.T1.6.6.1.1.1.m1.1.1.3.cmml">
              T
             </mi>
            </mrow>
            <annotation-xml encoding="MathML-Content" id="S2.T1.6.6.1.1.1.m1.1b">
             <apply id="S2.T1.6.6.1.1.1.m1.1.1.cmml" xref="S2.T1.6.6.1.1.1.m1.1.1">
              <times id="S2.T1.6.6.1.1.1.m1.1.1.1.cmml" xref="S2.T1.6.6.1.1.1.m1.1.1.1">
              </times>
              <ci id="S2.T1.6.6.1.1.1.m1.1.1.2.cmml" xref="S2.T1.6.6.1.1.1.m1.1.1.2">
               𝑁
              </ci>
              <ci id="S2.T1.6.6.1.1.1.m1.1.1.3.cmml" xref="S2.T1.6.6.1.1.1.m1.1.1.3">
               𝑇
              </ci>
             </apply>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S2.T1.6.6.1.1.1.m1.1c">
             N\times T
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Evaluation of deployment channels for language models: A comparative analysis of task customizability, transparency and complexity from the users’ perspective. PET: Pattern exploitation training;
     <math alttext="T" class="ltx_Math" display="inline" id="S2.T1.9.m1.1">
      <semantics id="S2.T1.9.m1.1b">
       <mi id="S2.T1.9.m1.1.1" xref="S2.T1.9.m1.1.1.cmml">
        T
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.T1.9.m1.1c">
        <ci id="S2.T1.9.m1.1.1.cmml" xref="S2.T1.9.m1.1.1">
         𝑇
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.T1.9.m1.1d">
        T
       </annotation>
      </semantics>
     </math>
     : the total number of task;
     <math alttext="N" class="ltx_Math" display="inline" id="S2.T1.10.m2.1">
      <semantics id="S2.T1.10.m2.1b">
       <mi id="S2.T1.10.m2.1.1" xref="S2.T1.10.m2.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.T1.10.m2.1c">
        <ci id="S2.T1.10.m2.1.1.cmml" xref="S2.T1.10.m2.1.1">
         𝑁
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.T1.10.m2.1d">
        N
       </annotation>
      </semantics>
     </math>
     : the number of patterns per task.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Task-specific Channels for Deployment
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     To tailor the core capabilities of LLMs for specific downstream tasks, both input and intermediate modalities can be altered directly (for instance, by appending prefixes or incorporating verbal context) or indirectly through the use of parametric modules such as neural networks, including adapters and output layers as described subsequently. It’s worth noting that direct modifications, such as prefixes, can also be achieved using parametric modules. These parametric modules undergo optimization via task-specific supervised learning. In this context, we describe the means for modality transformation aimed at specific tasks as task-specific channels.
For clarify, modalities are the types of data or the form in which data is processed, while channels are the pathways or methods through which these data modalities are adapted or transformed for specific tasks. Task-specific channels encompass:
1)
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.1">
      Adapter
     </span>
     : Adapters are compact neural networks that can be embedded between an LLM’s layers. A well-known approach, adapter tuning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Houlsby
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2019
      </a>
      )
     </cite>
     , involves optimizing the adapter’s parameters while leaving the original LLM parameters intact. These adapters are designed to adjust the intermediate layer representations to better align with task-specific needs.
2)
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.2">
      LLMs Themselves
     </span>
     : An alternative strategy involves modifying the LLM directly to produce task-specific representations by fine-tuning the model’s weights across all or selected layers. This method of fine-tuning is prevalent for AE-LLMs
     <cite class="ltx_cite ltx_citemacro_citep">
      (Jin
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2020
      </a>
      )
     </cite>
     and has also been applied to AR-LLMs in early use of GPT-like models
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2018
      </a>
      )
     </cite>
     .
3)
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.3">
      Output Layers
     </span>
     : Once task-specific representations are produced by either adapters or the LLM directly, the function of the output layers is to translate these representations into a designated output space. These layers typically consist of one or several linear layers. For example, linear functions are frequently used for tasks involving classification, while tasks that involve extractive question answering often necessitate the use of two linear functions to determine the beginning and concluding positions of the answer within a text passage.
4)
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.4">
      Activation Prefixes
     </span>
     : Within the scope of deploying LLMs via task-specific supervised learning, where training neural networks is common, prefix tuning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li and Liang,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2021
      </a>
      )
     </cite>
     presents an innovative method that employs prefixes to directly modify intermediate representations. These prefixes are essentially embeddings that are added at various layers, with dimensions identical to those of token embeddings, functioning as virtual tokens. Introducing these prefixes at earlier stages in the model allows for the infusion of task-specific information into more advanced layers, thereby improving the model’s alignment with the desired task objectives.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p2">
    <p class="ltx_p" id="S2.SS3.p2.1">
     Beyond the four channels previously outlined, verbal channels offer a unique approach for articulating the task context in which LLMs can identify and execute the intended tasks. These channels include:
5)
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">
      Verbal Free-form Context
     </span>
     :
In this approach, a context is articulated using free-form text, such as task instructions and few-shot demonstrations, which can activate complex cognitive functions. By merely incorporating task instructions within the context, AR-LLMs are enabled to undertake a multitude of tasks through zero-shot prompts. Another widely adopted method is few-shot prompting
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2019
      </a>
      ; Brown
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2020
      </a>
      )
     </cite>
     , which involves learning from a limited number of examples for in-context learning without the need for gradient updates, showcasing a human-like efficiency in acquiring new tasks. This method is particularly effective in eliciting cognitive behaviors akin to those observed with few-shot demonstrations, with further details discussed in Section
     <a class="ltx_ref" href="#S4" title="4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     .
It’s important to recognize that, in contrast to channels that are easily differentiated by input-side modalities (such as task-specific examples), this channel (e.g., task instructions) can intertwine with model inputs, e.g., task-specific examples. This allows for the seamless integration of the models’ world knowledge into tasks, for instance, “summarize deep learning technology”.
6)
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.2">
      Contextual Text Patterns
     </span>
     :
Given their training on a denoising language model objective, AE-LLMs excel in completing texts by filling in missing words, a trait that can be leveraged for downstream tasks. Task-specific patterns, in this regard, serve as a mechanism to alter given task-specific examples. Typically, this involves appending the examples with a cloze-style phrase or sentence (text with missing words) tailored to the task, allowing the model to predict the intended task outcomes based on the placeholders filled within the text. Pattern Exploitation Training (PET)
     <cite class="ltx_cite ltx_citemacro_citep">
      (Schick and Schütze,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2021
      </a>
      )
     </cite>
     involves the creative design of task-specific patterns and the fine-tuning of LLMs to these patterns. Conversely, auto-prompt methods
     <cite class="ltx_cite ltx_citemacro_citep">
      (Shin
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2020
      </a>
      )
     </cite>
     seek to optimize task-specific patterns to better fit the models, enhancing their ability to interpret and respond to the given tasks effectively.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Evaluation of Modalities and Channels
  </h2>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Evaluating Usability of Deployment Channels
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     This section introduces a framework for assessing the usability of language model deployment channels, focusing on their customizability, transparency, and complexity, as summarized in Table
     <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2.2 Exploring the Modalities within Large Language Models ‣ 2 Deploying Large Language Models ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
    </p>
   </div>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Customizability of User-level Tasks: Extent of User Control over Channels
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">
      Essentially, any task can be articulated in human languages, such as English, using free-form context. This adaptability is a testament to the evolution of human language over thousands of years, which has been refined to describe a vast array of everyday and complex scientific problems. Typically, in a zero-shot learning context, the channel consists solely of task instructions within the prompts, capable of encompassing a wide range of tasks. For instance, Wang et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Wang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib35" title="">
        2022
       </a>
       )
      </cite>
      have converted standard NLP datasets designed for optimized channels into instruction-based formats for 76 different tasks.
Moreover, free-form task instructions allow for nuanced control mechanisms, including explicit directives (such as specifying output formats or initiating reasoning processes) and subtle cues (such as inducing cognitive behaviors through few-shot examples). These aspects will be further explored in Section
      <a class="ltx_ref" href="#S4" title="4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      and summarized in Table
      <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      .
In contrast, since other channels are set during the optimization process for specific tasks, they lack the flexibility for user-directed modifications. Channels that require adjustments, such as fine-tuning the LLM, adapter tuning, or prefix tuning, rely on supervised learning methods for configuration. Although prompting in AE-LLMs could, in theory, facilitate task adjustments at inference time without prior task-specific fine-tuning—akin to AR-LLMs’ prompting approach—it often requires task-specific optimization to achieve effective channel performance. For example, techniques like Pattern Exploitation Training (PET)
      <cite class="ltx_cite ltx_citemacro_citep">
       (Schick and Schütze,
       <a class="ltx_ref" href="#bib.bib28" title="">
        2021
       </a>
       )
      </cite>
      utilize mathematical optimization to adapt models to specific patterns, whereas Auto-prompt
      <cite class="ltx_cite ltx_citemacro_citep">
       (Shin
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib29" title="">
        2020
       </a>
       )
      </cite>
      optimizes text patterns for language models. The question of whether this need for optimization arises from the inherent complexities of auto-encoding language models invites further research.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     User-level Transparency: Can Channel Formulation Be Easily Understood by Users?
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">
      The focus here is on the understandability of the channels themselves to lay users, rather than their functional effectiveness, as this greatly influences the user experience. For example, the objective of an output layer is clear — transforming LLM representations into a specific output format. However, the process involving dense representations through matrix multiplication is not intuitively understandable to the non-specialist. Moreover, text patterns refined through AE-LLMs’ Auto-prompting often lack the straightforwardness found in manually created prompts.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     User-level Complexity: Assessing the Number of Conceptual Components
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.7">
      This analysis evaluates the conceptual load required to deploy
      <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a">
        <mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">
         T
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b">
         <ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1">
          𝑇
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">
         T
        </annotation>
       </semantics>
      </math>
      tasks using various channels, moving away from the parameter size metric, which is more pertinent to researchers and developers. Assuming each task is accommodable across all channels, we quantify the complexity as follows:
For fine-tuned LLMs, prefixes, adapters and output layers, each task-specific adjustment equates to a complexity of
      <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a">
        <mi id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">
         T
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b">
         <ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">
          𝑇
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">
         T
        </annotation>
       </semantics>
      </math>
      , with
      <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.3.m3.1a">
        <mi id="S3.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml">
         T
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m3.1b">
         <ci id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1">
          𝑇
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.1c">
         T
        </annotation>
       </semantics>
      </math>
      denoting the total number of tasks. Additionally,
      <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.4.m4.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.4.m4.1a">
        <mi id="S3.SS1.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml">
         N
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.4.m4.1b">
         <ci id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1">
          𝑁
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.4.m4.1c">
         N
        </annotation>
       </semantics>
      </math>
      text patterns are devised per task, resulting in a complexity of
      <math alttext="N\times T" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.5.m5.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.5.m5.1a">
        <mrow id="S3.SS1.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml">
         <mi id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2.cmml">
          N
         </mi>
         <mo id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.1.cmml">
          ×
         </mo>
         <mi id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3.cmml">
          T
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.5.m5.1b">
         <apply id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1">
          <times id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.1">
          </times>
          <ci id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2">
           𝑁
          </ci>
          <ci id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3">
           𝑇
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.5.m5.1c">
         N\times T
        </annotation>
       </semantics>
      </math>
      , where
      <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.6.m6.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.6.m6.1a">
        <mi id="S3.SS1.SSS0.Px3.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px3.p1.6.m6.1.1.cmml">
         N
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.6.m6.1b">
         <ci id="S3.SS1.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.1.1">
          𝑁
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.6.m6.1c">
         N
        </annotation>
       </semantics>
      </math>
      represents the number of patterns per task. The complexity for verbal free-form context is considered negligible, as these are formulated spontaneously by users at the time of use.
From this framework, we can deduce the complexity inherent to each deployment paradigm. For instance, LLM fine-tuning, which necessitates one LLM and one output layer per task, carries a complexity of
      <math alttext="2\times T" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.7.m7.1">
       <semantics id="S3.SS1.SSS0.Px3.p1.7.m7.1a">
        <mrow id="S3.SS1.SSS0.Px3.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.cmml">
         <mn id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.2" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.2.cmml">
          2
         </mn>
         <mo id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.1.cmml">
          ×
         </mo>
         <mi id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.3" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.3.cmml">
          T
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.7.m7.1b">
         <apply id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1">
          <times id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.1">
          </times>
          <cn id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.2">
           2
          </cn>
          <ci id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.3">
           𝑇
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.7.m7.1c">
         2\times T
        </annotation>
       </semantics>
      </math>
      .
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Evaluating Expressiveness of Modalities
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     During LLM fine-tuning and adapter tuning, the task-specific output layers strictly limit the range of possible outputs, hindering the potential for detailed expressiveness and, by extension, advanced cognitive behaviors. The output space is tightly defined, with actions or labels being pre-determined and given specific meanings through task-specific supervised learning. Nonetheless, certain probing techniques allow us to uncover the thought processes behind their predictions, a topic we will explore further in Section
     <a class="ltx_ref" href="#S4.SS1" title="4.1 Thinking, Fast And Slow ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
      <span class="ltx_text ltx_ref_tag">
       4.1
      </span>
     </a>
     .
When it comes to AE-LLMs prompted with text patterns, these models are limited to generating only specific tokens or words, constrained by the patterns set in advance. These constraints, such as token positions and quantities dictated by the input patterns, along with the need for grammatical and coherent text completion, restrict the models’ ability to articulate complex ideas, plans, and actions.
On the other hand, AR-LLMs’ prompting capitalizes on their auto-regressive nature to produce unbounded, free-form text, influenced solely by the given input context. This capability is further demonstrated in Section
     <a class="ltx_ref" href="#S4" title="4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     and summarized in Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , showcasing the open-ended expressiveness unique to the AR-LLM prompting paradigm.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    This section elucidates the capability of AR-LLM prompting paradigms to exhibit cognitive behaviors expressed by the free-form modalities by mainpulating the free-form channels. It’s important to clarify that not every AR-LLM demonstrates cognitive behaviors—smaller models like GPT-2
    <cite class="ltx_cite ltx_citemacro_citep">
     (Radford
     <span class="ltx_text ltx_font_italic">
      et al.
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2019
     </a>
     )
    </cite>
    may not.
Specifically, we analyze four cognitive behaviors: thinking, reasoning, planning, and feedback learning, leaving the examination of their interrelationships for future research.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Thinking, Fast And Slow
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     At the core of cognitive behavior lies thinking. The Kahneman’s framework
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kahneman,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2011
      </a>
      )
     </cite>
     divides thinking into two distinct systems: the fast system operates through intuitive shortcuts for quick navigation of daily situations without extensive analysis. Conversely, the slow system, or System 2, involves conscious, detailed and methodical examination of information, necessitating logical deliberation to arrive at decisions and address challenges.
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Fast Thinking via Task-specific Channels
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">
      Using channels trained through task-specific supervised learning can achieve performances that rival or exceed human performance.
Nonetheless, they often struggle with generalizing to data from natural domain shifts, adversarial perturbations and debiased data, as summarized by Li et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Li
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib17" title="">
        2023
       </a>
       )
      </cite>
      . This limitation is consistently attributed to shortcut learning, such as classifying sentences containing the word “No” as “contradiction” in text entailment tasks
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wallace
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib33" title="">
        2019
       </a>
       ; Du
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib5" title="">
        2021
       </a>
       )
      </cite>
      .
The intriguing question arises whether task-specific channels can also develop System 2 — the fast system.
While the limited expressiveness of task-specific outputs does not offer straightforward evidence, Li et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Li and Liu (
       <a class="ltx_ref" href="#bib.bib16" title="">
        2023
       </a>
       )
      </cite>
      employ a technical probe
      <cite class="ltx_cite ltx_citemacro_citep">
       (Sundararajan
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib32" title="">
        2017
       </a>
       )
      </cite>
      to reveal that indulgence in shortcut learning during task-specific training impedes the development of the slow system.
While the mentioned research primarily examines the LLM fine-tuning paradigm, it’s our contention that shortcut learning and the fast thinking are likely prevalent across all the parametric channels, including prefixes and adapters, trained on supervised datasets to some degree. This is attributed to the inherent characteristics of gradient descent optimization, as demonstrated by empirical findings in Li et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Li and Liu (
       <a class="ltx_ref" href="#bib.bib16" title="">
        2023
       </a>
       )
      </cite>
      .
Another empirical evidence shows that methods like prefix and adapter tuning, although more resilient, still notably falter under distribution shifts and adversarial attacks
      <cite class="ltx_cite ltx_citemacro_citep">
       (Han
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib6" title="">
        2021
       </a>
       ; Yang and Liu,
       <a class="ltx_ref" href="#bib.bib41" title="">
        2022
       </a>
       )
      </cite>
      .
The mitigated impact observed in prefix and adapter tuning is attributed to the fact that the underlying LLMs are not directly engaged as task-specific channels, as explored by
      <cite class="ltx_cite ltx_citemacro_citep">
       (Han
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib6" title="">
        2021
       </a>
       )
      </cite>
      . While we draw parallels between reliance on shortcuts and fast thinking within human cognition, some research within the NLP field argues that such dependency on shortcuts (dataset biases) detracts from the models’ relevance to human-level cognition
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zhong
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib46" title="">
        2023
       </a>
       )
      </cite>
      . This perspective arises from the view that the shortcuts might not reflect genuine human cognitive activities within the field of NLP.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Minimal Fast Thinking Evident with AR-LLMs Prompting
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">
      Research findings
      <cite class="ltx_cite ltx_citemacro_citep">
       (Si
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib31" title="">
        2023
       </a>
       ; Zhang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib44" title="">
        2022
       </a>
       )
      </cite>
      consistently indicate the difficulty of inducing fast thinking in AR-LLMs through prompting techniques. These models typically remain unfazed by various distributional shifts, such as domain shift and adversarial perturbations. Min et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Min
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib21" title="">
        2022
       </a>
       )
      </cite>
      demonstrate that, even with few-shot demonstrations for in-context learning, the models tend to leverage the structure of these demonstrations to organize the generation rather than relying on simplistic input-to-label mappings for predictions.
Additionally, Raman et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Raman
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib26" title="">
        2023
       </a>
       )
      </cite>
      show that PET prompting improve the AE-LLMs’ ability to withstand adversarial attacks. Nonetheless, this enhanced robustness is somewhat restricted. The constrained effectiveness could be attributed to the dependency on task-specific channels inherent during the deployment of the PET prompting.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Slow Thinking in Prompting Paradigms
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">
      The remainder of this section will illustrate the capacity of AR-LLMs’ prompting to replicate the human slow thinking process through the exhibition of effortful mental activities, as encapsulated in Table
      <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      .
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Reasoning
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Reasoning is a thinking process to conclusions or decisions with the sequential and interconnected nature, i.e., chain-of-thoughts (CoTs)
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wei
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2022b
      </a>
      )
     </cite>
     . This is the most common definition in the NLP/LLM are to investigate the LLMs’ reasoning ability.
With a reasoning path in free-form modality, models can better solve complicated tasks requiring multi-step reasoning compared to the conclusion without CoTs. As an illustration, Wei et al.,
     <cite class="ltx_cite ltx_citemacro_citet">
      Wei
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      (
      <a class="ltx_ref" href="#bib.bib40" title="">
       2022b
      </a>
      )
     </cite>
     substantially boosts model efficacy in solving mathematical reasoning bechmarks.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     Reasoning is defined as the process of arriving at conclusions or decisions through a sequential and interconnected series of thoughts, often referred to as a chain-of-thoughts (CoTs)
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wei
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2022b
      </a>
      )
     </cite>
     . This definition is widely accepted in the field of Natural Language Processing (NLP) for exploring the reasoning capabilities of LLMs. By employing a reasoning path via the modality of free-form text, models are more adept at tackling complex tasks that necessitate multi-step reasoning, as opposed to reaching conclusions without the aid of CoTs.
Technically, the auto-regressive nature employs the thoughts or intermediate steps generated as the prior for generating subsequent thoughts and, ultimately, the final predictions.
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Context for Eliciting Reasoning
    </h4>
    <div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">
      Two primary contexts are employed to facilitate the creation of intermediate reasoning steps: incorporating a Chain of Thought (CoT) triggers in task instructions (
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.1">
       zero-shot CoTs
      </span>
      ), such as “Let’s think step-by-step”
      <cite class="ltx_cite ltx_citemacro_citep">
       (Kojima
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib13" title="">
        2022
       </a>
       )
      </cite>
      , within prompts, or integrating manually crafted reasoning steps in a few-shot learning context (
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.2">
       few-shot CoTs
      </span>
      )
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wei
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib40" title="">
        2022b
       </a>
       )
      </cite>
      . To circumvent the manual compilation of few-shot demonstrations with reasoning sequences, Zhang et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Zhang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib45" title="">
        2023
       </a>
       )
      </cite>
      developed a method to automatically generate few-shot demonstrations by choosing several queries and utilizing zero-shot CoTs to craft reasoning sequences for each query (
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.3">
       Auto CoTs
      </span>
      ).
Given that simple greedy decoding (producing a single chain) is prone to error accumulation in intermediate steps, Wang et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Wang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib37" title="">
        2023b
       </a>
       )
      </cite>
      propose generating multiple chains and consolidating them through majority voting, thereby enhancing model accuracy in both scenarios (
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.4">
       CoTs-SC
      </span>
      ).
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Planning
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Planning involves the forethought and organization of actions or steps to achieve a predetermined objective. This process fundamentally requires a comprehension or representation of the environment and involves breaking down tasks into smaller, manageable subgoals.
It represents a key cognitive behavior modeled within the fields of AI. Typical planning methods break down tasks into subgoals through explicit symbolic representation
     <cite class="ltx_cite ltx_citemacro_citep">
      (Russell and Norvig,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2010
      </a>
      )
     </cite>
     . For instance, partial-order planning ensures the logical sequencing of actions by modeling actions, preconditions, effects, and the relations among actions in such a way that actions are logically sequenced to meet the goal’s preconditions.
Differing from traditional approaches that rely on explicitly modeled knowledge and reasoning mechanisms, LLMs leverage their inherent knowledge and inferential capabilities to mimic planning. They do this by producing text sequences that suggest a logical progression of steps or actions directed towards an objective
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hao
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      ; Wang
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023a
      </a>
      ; Huang
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2022
      </a>
      )
     </cite>
     . This skill stems from the models’ proficiency in forecasting the subsequent most likely word sequence based on a context indicative of planning or reasoning processes.
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Context to Elicit Plans
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">
      Similar to the activation of reasoning processes, the process of planning can be prompted through the inclusion of specific planning cues in zero-shot scenarios, such as the prompt “let’s carry out the plan”
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib36" title="">
        2023a
       </a>
       )
      </cite>
      , or through the demonstration of planning steps in few-shot examples
      <cite class="ltx_cite ltx_citemacro_citep">
       (Huang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib9" title="">
        2022
       </a>
       )
      </cite>
      . Experimental findings indicate that instructions tailored to tasks significantly enhance the performance of LLMs on various tasks. For instance, directives like “pay attention to calculation”
      <cite class="ltx_cite ltx_citemacro_citep">
       (Hao
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib7" title="">
        2023
       </a>
       )
      </cite>
      or “identify key variables and their corresponding figures to formulate a plan”
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib36" title="">
        2023a
       </a>
       )
      </cite>
      have been shown to improve outcomes in tasks requiring numerical reasoning.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Applying Planning for Sequential Decision-making
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">
      This ability is essential for addressing problems requiring a series of decisions, especially when deploying LLMs in open-world scenarios like robotics. In such environments, tasks typically need physical actions (grounded), involve translating broad objectives into actionable steps (high-level), and present a vast range of possible actions (open-ended). Research has demonstrated the effectiveness of LLMs in deconstructing complex goals into actionable sequences within such dynamic environments, as seen in projects like ALFWorld
      <cite class="ltx_cite ltx_citemacro_citep">
       (Yao
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023b
       </a>
       )
      </cite>
      , VirtualHome
      <cite class="ltx_cite ltx_citemacro_citep">
       (Huang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib9" title="">
        2022
       </a>
       )
      </cite>
      , and Minecraft
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib38" title="">
        2023c
       </a>
       )
      </cite>
      .
An example from ALFWorld illustrates this: achieving the objective of “examining paper under desklamp” necessitates LLMs to devise practical plans (e.g., initially approaching the coffee table, then acquiring the paper and utilizing the desklamp) and subsequently generate textual instructions for execution in real-world settings.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Feedback Learning
   </h3>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     As Kahneman et al.,
     <cite class="ltx_cite ltx_citemacro_citet">
      Kahneman (
      <a class="ltx_ref" href="#bib.bib12" title="">
       2011
      </a>
      )
     </cite>
     elucidates, although System 1 may rush to judgments that are biased or erroneous, System 2 has the capacity to identify and rectify these mistakes through introspection on the rapid decisions made by System 1. Similarly, LLMs have shown the ability to mimic this aspect of human cognition.
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Feedback Generation and Contextual Basis
    </h4>
    <div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">
      LLMs are adept at generating feedback by reflecting on their previously given responses or observations from interactions with the external environment. There are two primary scenarios for feedback generation:
1) Feedback based on previous actions and external feedback: In the work by Yao et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Yao
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023b
       </a>
       )
      </cite>
      , LLMs engaging with a Wikipedia API to search for entities that do not exist, such as “Search[goddess frigg]”, may encounter a 404 error, delivered in JSON format. In response, LLMs can articulate feedback about the error related to their action, such as stating, “Could not find goddess frigg.”.
2) Feedback based solely on prior responses:
This approach is relevant in various situations where external environmental feedback is absent
      <cite class="ltx_cite ltx_citemacro_citep">
       (Shinn
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib30" title="">
        2023
       </a>
       ; Hao
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib7" title="">
        2023
       </a>
       )
      </cite>
      . In such cases, LLMs can give feedback on previous answers by applying certain evaluation metrics, such as determining the relevance of a sub-question to a broader question requiring intricate, multi-step reasoning
      <cite class="ltx_cite ltx_citemacro_citep">
       (Hao
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib7" title="">
        2023
       </a>
       )
      </cite>
      . This process involves using a prompt that asks the LLM to judge the utility of a sub-question in addressing the main question (Prompt: “Given a question, assess if the subquestion aids in solving the original question. Answer ’Yes’ or ’No’. Question: {goal}; Subquestion: {action}. Is the subquestion useful?”).
Furthermore, feedback may be presented as numerical scores, such as the confidence scores (normalized logits) for ’Yes’ or ’No’ answers, instead of in verbal form. The decision to use numerical rather than verbal feedback is contingent on the specific requirements of the feedback mechanism, as explored in subsequent discussions.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Mechanism for Feedback Learning
    </h4>
    <div class="ltx_para" id="S4.SS4.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">
      After generating feedback, Shinn et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Shinn
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib30" title="">
        2023
       </a>
       )
      </cite>
      directly include verbal feedback LLMs have produced to enhance the accuracy of their responses or decisions.
Meanwhile, Hao et al.,
      <cite class="ltx_cite ltx_citemacro_citet">
       Hao
       <span class="ltx_text ltx_font_italic">
        et al.
       </span>
       (
       <a class="ltx_ref" href="#bib.bib7" title="">
        2023
       </a>
       )
      </cite>
      detail a methodology where numerical feedback serves as a reward system for guiding LLMs for action selection. Following this, the LLMs function as world models to predict the subsequent state of state-action pairs during the planning phase, utilizing Monte-Carlo Tree Search (MCTS) to achieve this aim. Instead of allowing LLMs to directly process the feedback, an implicit feedback learning strategy is employed where a feedback loop is deliberately established to influence the sequence of actions undertaken by the LLMs via the update of state values during the propagation phase of MCTS.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T2">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T2.2.1.1">
        <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.1.1">
          <span class="ltx_p" id="S4.T2.2.1.1.1.1.1" style="width:37.0pt;">
           <span class="ltx_text" id="S4.T2.2.1.1.1.1.1.1" style="font-size:80%;">
            Behaviors
           </span>
          </span>
         </span>
        </th>
        <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.2.1">
          <span class="ltx_p" id="S4.T2.2.1.1.2.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.1.1.2.1.1.1" style="font-size:80%;">
            Context
           </span>
          </span>
         </span>
        </th>
        <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.3">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.3.1">
          <span class="ltx_p" id="S4.T2.2.1.1.3.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.1.1.3.1.1.1" style="font-size:80%;">
            Relevant Works
           </span>
          </span>
         </span>
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T2.2.2.1">
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.1.1" rowspan="2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.1.1">
          <span class="ltx_p" id="S4.T2.2.2.1.1.1.1" style="width:37.0pt;">
           <span class="ltx_text" id="S4.T2.2.2.1.1.1.1.1" style="font-size:80%;">
            Reasoning
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.1.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.2.1">
          <span class="ltx_p" id="S4.T2.2.2.1.2.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.2.1.2.1.1.1" style="font-size:80%;">
            CoT triggers, e.g., “Let’s think step by step.”
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.1.3">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.3.1">
          <span class="ltx_p" id="S4.T2.2.2.1.3.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.1" style="font-size:80%;">
            Zero-shot CoTs
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.2.1" style="font-size:80%;">
             (
            </span>
            Kojima
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.3.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib13" title="">
             2022
            </a>
            <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.4.3" style="font-size:80%;">
             )
            </span>
           </cite>
           <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.5" style="font-size:80%;">
            ,
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.6" style="font-size:80%;">
            Auto-CoTs
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.7.1" style="font-size:80%;">
             (
            </span>
            Zhang
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.8.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib45" title="">
             2023
            </a>
            <span class="ltx_text" id="S4.T2.2.2.1.3.1.1.9.3" style="font-size:80%;">
             )
            </span>
           </cite>
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.2.3.2">
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.3.2.1">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.2.1.1">
          <span class="ltx_p" id="S4.T2.2.3.2.1.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.3.2.1.1.1.1" style="font-size:80%;">
            Few-shot demos with CoTs
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.3.2.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.2.2.1">
          <span class="ltx_p" id="S4.T2.2.3.2.2.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.1" style="font-size:80%;">
            Few-shot CoTs
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.2.1" style="font-size:80%;">
             (
            </span>
            Wei
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.3.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib40" title="">
             2022b
            </a>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.4.3" style="font-size:80%;">
             )
            </span>
           </cite>
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.5" style="font-size:80%;">
            ,
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.6" style="font-size:80%;">
            CoTs-SC
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.7.1" style="font-size:80%;">
             (
            </span>
            Wang
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.8.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib37" title="">
             2023b
            </a>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.9.3" style="font-size:80%;">
             )
            </span>
           </cite>
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.10" style="font-size:80%;">
            ,
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.11" style="font-size:80%;">
            Auto-prompt
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.12.1" style="font-size:80%;">
             (
            </span>
            Zhang
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.13.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib45" title="">
             2023
            </a>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.14.3" style="font-size:80%;">
             )
            </span>
           </cite>
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.15" style="font-size:80%;">
            ,
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.16" style="font-size:80%;">
            ToT
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.17.1" style="font-size:80%;">
             (
            </span>
            Yao
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.18.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib42" title="">
             2023a
            </a>
            <span class="ltx_text" id="S4.T2.2.3.2.2.1.1.19.3" style="font-size:80%;">
             )
            </span>
           </cite>
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.2.4.3">
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.3.1" rowspan="2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.3.1.1">
          <span class="ltx_p" id="S4.T2.2.4.3.1.1.1" style="width:37.0pt;">
           <span class="ltx_text" id="S4.T2.2.4.3.1.1.1.1" style="font-size:80%;">
            Planning
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.3.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.3.2.1">
          <span class="ltx_p" id="S4.T2.2.4.3.2.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.4.3.2.1.1.1" style="font-size:80%;">
            Zero-shot instruction
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.3.3">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.3.3.1">
          <span class="ltx_p" id="S4.T2.2.4.3.3.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.4.3.3.1.1.1" style="font-size:80%;">
            Wang et al.,
           </span>
           <cite class="ltx_cite ltx_citemacro_citet">
            Wang
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.4.3.3.1.1.2.1.1.1" style="font-size:80%;">
             (
            </span>
            <a class="ltx_ref" href="#bib.bib36" title="">
             2023a
            </a>
            <span class="ltx_text" id="S4.T2.2.4.3.3.1.1.3.2.2.1" style="font-size:80%;">
             )
            </span>
           </cite>
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.2.5.4">
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.5.4.1">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.4.1.1">
          <span class="ltx_p" id="S4.T2.2.5.4.1.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.5.4.1.1.1.1" style="font-size:80%;">
            Few-shot demos with planning steps
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.5.4.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.4.2.1">
          <span class="ltx_p" id="S4.T2.2.5.4.2.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.5.4.2.1.1.1" style="font-size:80%;">
            Huang et al.,
           </span>
           <cite class="ltx_cite ltx_citemacro_citet">
            Huang
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.5.4.2.1.1.2.1.1.1" style="font-size:80%;">
             (
            </span>
            <a class="ltx_ref" href="#bib.bib9" title="">
             2022
            </a>
            <span class="ltx_text" id="S4.T2.2.5.4.2.1.1.3.2.2.1" style="font-size:80%;">
             )
            </span>
           </cite>
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.2.6.5">
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.5.1">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.5.1.1">
          <span class="ltx_p" id="S4.T2.2.6.5.1.1.1" style="width:37.0pt;">
           <span class="ltx_text" id="S4.T2.2.6.5.1.1.1.1" style="font-size:80%;">
            Feedback
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.6.5.1.1.1.2" style="font-size:80%;">
            Learning
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.5.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.5.2.1">
          <span class="ltx_p" id="S4.T2.2.6.5.2.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.6.5.2.1.1.1" style="font-size:80%;">
            Observations from external environments
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.5.3">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.5.3.1">
          <span class="ltx_p" id="S4.T2.2.6.5.3.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.6.5.3.1.1.1" style="font-size:80%;">
            Reflexion
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.6.5.3.1.1.2.1" style="font-size:80%;">
             (
            </span>
            Shinn
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.6.5.3.1.1.3.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib30" title="">
             2023
            </a>
            <span class="ltx_text" id="S4.T2.2.6.5.3.1.1.4.3" style="font-size:80%;">
             )
            </span>
           </cite>
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.2.7.6">
        <td class="ltx_td ltx_align_top ltx_border_bb" id="S4.T2.2.7.6.1">
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.2.7.6.2">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.7.6.2.1">
          <span class="ltx_p" id="S4.T2.2.7.6.2.1.1" style="width:71.1pt;">
           <span class="ltx_text" id="S4.T2.2.7.6.2.1.1.1" style="font-size:80%;">
            Previous answers/decisions
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.2.7.6.3">
         <span class="ltx_inline-block ltx_align_top" id="S4.T2.2.7.6.3.1">
          <span class="ltx_p" id="S4.T2.2.7.6.3.1.1" style="width:99.6pt;">
           <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.1" style="font-size:80%;">
            Self-refine
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.2.1" style="font-size:80%;">
             (
            </span>
            Madaan
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.3.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib20" title="">
             2023
            </a>
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.4.3" style="font-size:80%;">
             )
            </span>
           </cite>
           <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.5" style="font-size:80%;">
            ,
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.6" style="font-size:80%;">
            Reflexion
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.7.1" style="font-size:80%;">
             (
            </span>
            Shinn
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.8.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib30" title="">
             2023
            </a>
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.9.3" style="font-size:80%;">
             )
            </span>
           </cite>
           <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.10" style="font-size:80%;">
            ,
           </span>
           <br class="ltx_break"/>
           <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.11" style="font-size:80%;">
            RAP
           </span>
           <cite class="ltx_cite ltx_citemacro_citep">
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.12.1" style="font-size:80%;">
             (
            </span>
            Hao
            <span class="ltx_text ltx_font_italic">
             et al.
            </span>
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.13.2.1.1" style="font-size:80%;">
             ,
            </span>
            <a class="ltx_ref" href="#bib.bib7" title="">
             2023
            </a>
            <span class="ltx_text" id="S4.T2.2.7.6.3.1.1.14.3" style="font-size:80%;">
             )
            </span>
           </cite>
          </span>
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering" style="font-size:80%;">
      <span class="ltx_tag ltx_tag_table">
       <span class="ltx_text" id="S4.T2.5.1.1" style="font-size:113%;">
        Table 2
       </span>
       :
      </span>
      <span class="ltx_text" id="S4.T2.6.2" style="font-size:113%;">
       Cognitive behaviors enabled by free-form context. For the “Feedback Learning” sections, we illustrate the contexts utilized to produce feedback. It’s worth noting that the methods for feedback adaptation might not always employ free-form context; for instance, they may involve advanced search techniques as outlined in our study. The final column presents examples of tasks for demonstration purposes, though the list is not comprehensive.
      </span>
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Bridging LLM Deployment Gaps with Insights from Cognitive Behaviors
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    This section investigates how insights into cognitive behaviors can aid in addressing the tuning and deployment challenges faced by LLMs operating as autonomous agents and within multi-agent systems.
   </p>
  </div>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Autonomous Cognitive Behaviors
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">
     Instead of relying on explicit contextual cues to trigger advanced cognitive functions, an intelligent system is expected to independently engage in reasoning, planning, and decision-making as it interacts with the external world—for instance, by seeking input from humans or utilizing available tools. To foster such autonomous behaviors, various algorithms aim to tune LLMs for independently exhibiting behaviors that align with human cognitive processes. For instance, Liu et al.,
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      (
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     have developed techniques for instruction tuning that facilitates autonomous reasoning.
Yet, the challenge remains in creating instructional data that encapsulates higher-order cognitive functions. A pivotal question emerges:
     <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.1.1">
      How can various cognitive behaviors be encapsulated within free-form text (instruction data)?
     </em>
     Addressing this question is crucial for ensuring that the data used for tuning mirrors human cognitive processes, thereby making the resulting model actions more human-like. Unraveling this issue might necessitate insights from both cognitive psychology and linguistics.
Another approach to tuning involves the use of reliable reward models, such as reinforcement learning from human feedback (RLHF)
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ouyang
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2022
      </a>
      )
     </cite>
     and behavior cloning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nakano
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2021
      </a>
      )
     </cite>
     .
Many studies
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ouyang
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2022
      </a>
      ; Nakano
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2021
      </a>
      )
     </cite>
     develop reward models based on comparisons of model-generated responses, with human evaluators ranking these responses.
An unresolved inquiry remains:
     <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.1.2">
      How can reward models be devised to truly reflect human cognitive preferences?
     </em>
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Navigating Free-form Contexts in Multi-turn Interactions Within Multi-agent Systems
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">
     In exploring planning and feedback learning strategies, it becomes evident that multi-agent systems are designed to facilitate interactions between LLMs and the external world, as well as among LLMs themselves. Examples include LLMs acting as both evaluators and actors in feedback learning environments
     <cite class="ltx_cite ltx_citemacro_citep">
      (Shinn
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      )
     </cite>
     or taking on roles as evaluators, actors, and environmental simulators in planning scenarios
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hao
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     .
Such setups necessitate more than a one-time interaction between LLMs and users, requiring instead sustained, multi-turn dialogues.
Challenges emerge within these complex interactions:
     <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">
      How do agents process and integrate information from other agents and their own previous dialogues? How can environmental data be stringified into a format understandable by LLMs?
What strategies can simplify the management of extended sequences of interaction trajectories?
     </em>
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Advancing Towards A Unified Inference Framework for Multi-agent Systems
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">
     Despite the recent development of various LLM-based multi-agent systems, there remains an absence of a unified framework across these models.
Exploring such framework serves as a key drive for this research on a cognitive framework.
Organizing LLM-based agents by their cognitive behaviors offers a pathway to this unification. For instance, as indicated in Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , the ReAct framework
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yao
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023b
      </a>
      )
     </cite>
     employs a reasoning agent for decision-making, whereas the RAP framework
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hao
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     utilizes a reasoning agent for decision-making alongside a feedback learning agent for evaluation.
Delving deeper into the relationships between cognitive behaviors might benefit from insights in cognitive psychology. Taking the concept of Self-Regulated Learning (SRL) as defined by
     <cite class="ltx_cite ltx_citemacro_citet">
      Zimmerman (
      <a class="ltx_ref" href="#bib.bib47" title="">
       2000
      </a>
      )
     </cite>
     , planning and feedback learning are intertwined, enhancing the learning process.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS0.SSS0.Px3.p2">
    <p class="ltx_p" id="S5.SS0.SSS0.Px3.p2.1">
     “Self-regulated learning refers to self-generated thoughts, feelings, and actions that are planned and cyclically adapted to the attainment of personal goals”
    </p>
   </div>
   <div class="ltx_para" id="S5.SS0.SSS0.Px3.p3">
    <p class="ltx_p" id="S5.SS0.SSS0.Px3.p3.1">
     This rationale lays the groundwork for future efforts to combine planning frameworks (like RAP) with feedback learning frameworks (such as Reflexion).
Furthermore, the structure of certain inference models showcases their interconnections and the presence of common detailed components.
For instance, both feedback learning frameworks, exemplified by Reflexion
     <cite class="ltx_cite ltx_citemacro_citet">
      Shinn
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      )
     </cite>
     , and planning frameworks, such as RAP
     <cite class="ltx_cite ltx_citemacro_citet">
      Hao
      <span class="ltx_text ltx_font_italic">
       et al.
      </span>
      (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     , incorporate a common detailed module: a feedback learning agent. This agent plays a pivotal role in decision-making by facilitating the selection of appropriate actions from an assortment of tools and environments.
It is evident that the interconnection within the intricate inference framework can be examined, especially at the level of cognitive behavior.
Our comprehensive analysis aims to pave the way for the systematic creation of multi-agent LLM-based frameworks, or inversely, to stimulate research in cognitive psychology.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In summary, our analysis seeks to inspire further research in AI, within the domain of language intelligence and beyond, to move away from heavily optimized task-specific channels. Instead, we advocate for the adoption of natural and free-form modalities throughout the pretraining phase via self-supervised learning, followed by straightforward inference-time deployment that eschews the necessity for mathematically optimizing task-specific channels.
We developed an analytical framework to examine the deployment of LLMs to reach the conclusion.
Besides, the auto-regressive nature of free-form modalities, leveraged during pretraining, enhances the capacity for exhibiting a range of human-like cognitive behaviors by utilizing the free-form channel. It is important to clarify that we do not advocate that LLMs possess conscious thought. Rather, our findings illustrate how LLMs, such as ChatGPT, can imitate the outcomes of human cognitive activities via the free-form modality given suitable verbal context.
Lastly, we highlight the opportunity to address challenges in LLM deployment through the integration of cognitive behavior concepts.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bartneck
     <span class="ltx_text ltx_font_italic" id="bib.bib1.2.2.1">
      et al.
     </span>
     [2020]
    </span>
    <span class="ltx_bibblock">
     Christoph Bartneck, Tony Belpaeme, Friederike Eyssel, Takayuki Kanda, Merel Keijsers, and Selma Šabanović.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib1.3.1">
      Human-robot interaction: An introduction
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     Cambridge University Press, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bengio
     <span class="ltx_text ltx_font_italic" id="bib.bib2.2.2.1">
      et al.
     </span>
     [2003]
    </span>
    <span class="ltx_bibblock">
     Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin.
    </span>
    <span class="ltx_bibblock">
     A neural probabilistic language model.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib2.3.1">
      JMLR
     </span>
     , 3:1137–1155, 2003.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown
     <span class="ltx_text ltx_font_italic" id="bib.bib3.2.2.1">
      et al.
     </span>
     [2020]
    </span>
    <span class="ltx_bibblock">
     Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib3.3.1">
      Advances in NIPS
     </span>
     , volume 33, pages 1877–1901, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Devlin
     <span class="ltx_text ltx_font_italic" id="bib.bib4.2.2.1">
      et al.
     </span>
     [2019]
    </span>
    <span class="ltx_bibblock">
     Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    </span>
    <span class="ltx_bibblock">
     BERT: Pre-training of deep bidirectional transformers for language understanding.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib4.3.1">
      NAACL-HLT
     </span>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du
     <span class="ltx_text ltx_font_italic" id="bib.bib5.2.2.1">
      et al.
     </span>
     [2021]
    </span>
    <span class="ltx_bibblock">
     Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu.
    </span>
    <span class="ltx_bibblock">
     Towards interpreting and mitigating shortcut learning behavior of NLU models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib5.3.1">
      NAACL-HLT
     </span>
     , pages 915–929, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Han
     <span class="ltx_text ltx_font_italic" id="bib.bib6.2.2.1">
      et al.
     </span>
     [2021]
    </span>
    <span class="ltx_bibblock">
     Wenjuan Han, Bo Pang, and Ying Nian Wu.
    </span>
    <span class="ltx_bibblock">
     Robust transfer learning with pretrained language models through adapters.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib6.3.1">
      ACL-IJCNLP
     </span>
     , pages 854–861, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hao
     <span class="ltx_text ltx_font_italic" id="bib.bib7.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu.
    </span>
    <span class="ltx_bibblock">
     Reasoning with language model is planning with world model.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib7.3.1">
      EMNLP
     </span>
     , pages 8154–8173, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Houlsby
     <span class="ltx_text ltx_font_italic" id="bib.bib8.2.2.1">
      et al.
     </span>
     [2019]
    </span>
    <span class="ltx_bibblock">
     Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
    </span>
    <span class="ltx_bibblock">
     Parameter-efficient transfer learning for nlp.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib8.3.1">
      ICML
     </span>
     , pages 2790–2799, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang
     <span class="ltx_text ltx_font_italic" id="bib.bib9.2.2.1">
      et al.
     </span>
     [2022]
    </span>
    <span class="ltx_bibblock">
     Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    </span>
    <span class="ltx_bibblock">
     Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib9.3.1">
      ICML
     </span>
     , pages 9118–9147, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jin
     <span class="ltx_text ltx_font_italic" id="bib.bib10.2.2.1">
      et al.
     </span>
     [2020]
    </span>
    <span class="ltx_bibblock">
     Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    </span>
    <span class="ltx_bibblock">
     Is bert really robust? a strong baseline for natural language attack on text classification and entailment.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib10.3.1">
      AAAI
     </span>
     , volume 34, pages 8018–8025, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Joshi
     <span class="ltx_text ltx_font_italic" id="bib.bib11.2.2.1">
      et al.
     </span>
     [2020]
    </span>
    <span class="ltx_bibblock">
     Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.
    </span>
    <span class="ltx_bibblock">
     SpanBERT: Improving pre-training by representing and predicting spans.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib11.3.1">
      TACL
     </span>
     , 8:64–77, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kahneman [2011]
    </span>
    <span class="ltx_bibblock">
     D. Kahneman.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">
      Thinking, Fast and Slow
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     Farrar, Straus and Giroux, 2011.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima
     <span class="ltx_text ltx_font_italic" id="bib.bib13.2.2.1">
      et al.
     </span>
     [2022]
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
    </span>
    <span class="ltx_bibblock">
     Large language models are zero-shot reasoners.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib13.3.1">
      Advances in NIPS
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lewis
     <span class="ltx_text ltx_font_italic" id="bib.bib14.2.2.1">
      et al.
     </span>
     [2020]
    </span>
    <span class="ltx_bibblock">
     Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
    </span>
    <span class="ltx_bibblock">
     BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib14.3.1">
      ACL
     </span>
     , pages 7871–7880, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li and Liang [2021]
    </span>
    <span class="ltx_bibblock">
     Xiang Lisa Li and Percy Liang.
    </span>
    <span class="ltx_bibblock">
     Prefix-tuning: Optimizing continuous prompts for generation.
    </span>
    <span class="ltx_bibblock">
     In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,
     <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">
      ACL-IJCNLP
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li and Liu [2023]
    </span>
    <span class="ltx_bibblock">
     Xinzhe Li and Ming Liu.
    </span>
    <span class="ltx_bibblock">
     Make text unlearnable: Exploiting effective patterns to protect personal data.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">
      TrustNLP
     </span>
     , pages 249–259, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li
     <span class="ltx_text ltx_font_italic" id="bib.bib17.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine.
    </span>
    <span class="ltx_bibblock">
     A survey on out-of-distribution evaluation of neural nlp models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib17.3.1">
      IJCAI-23
     </span>
     , 7 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu
     <span class="ltx_text ltx_font_italic" id="bib.bib18.2.2.1">
      et al.
     </span>
     [2019]
    </span>
    <span class="ltx_bibblock">
     Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
    </span>
    <span class="ltx_bibblock">
     Roberta: A robustly optimized BERT pretraining approach.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib18.3.1">
      CoRR
     </span>
     , abs/1907.11692, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu
     <span class="ltx_text ltx_font_italic" id="bib.bib19.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang.
    </span>
    <span class="ltx_bibblock">
     Logicot: Logical chain-of-thought instruction tuning.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib19.3.1">
      EMNLP
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Madaan
     <span class="ltx_text ltx_font_italic" id="bib.bib20.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.
    </span>
    <span class="ltx_bibblock">
     Self-refine: Iterative refinement with self-feedback.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib20.3.1">
      Advances in NIPS
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Min
     <span class="ltx_text ltx_font_italic" id="bib.bib21.2.2.1">
      et al.
     </span>
     [2022]
    </span>
    <span class="ltx_bibblock">
     Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
    </span>
    <span class="ltx_bibblock">
     Rethinking the role of demonstrations: What makes in-context learning work?
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib21.3.1">
      EMNLP
     </span>
     , pages 11048–11064, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano
     <span class="ltx_text ltx_font_italic" id="bib.bib22.2.2.1">
      et al.
     </span>
     [2021]
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
    </span>
    <span class="ltx_bibblock">
     Webgpt: Browser-assisted question-answering with human feedback.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib22.3.1">
      arXiv preprint arXiv:2112.09332
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ouyang
     <span class="ltx_text ltx_font_italic" id="bib.bib23.2.2.1">
      et al.
     </span>
     [2022]
    </span>
    <span class="ltx_bibblock">
     Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
    </span>
    <span class="ltx_bibblock">
     Training language models to follow instructions with human feedback.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib23.3.1">
      Advances in NIPS
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford
     <span class="ltx_text ltx_font_italic" id="bib.bib24.2.2.1">
      et al.
     </span>
     [2018]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    </span>
    <span class="ltx_bibblock">
     Improving language understanding by generative pre-training.
    </span>
    <span class="ltx_bibblock">
     2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford
     <span class="ltx_text ltx_font_italic" id="bib.bib25.2.2.1">
      et al.
     </span>
     [2019]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
    </span>
    <span class="ltx_bibblock">
     Language models are unsupervised multitask learners.
    </span>
    <span class="ltx_bibblock">
     2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Raman
     <span class="ltx_text ltx_font_italic" id="bib.bib26.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Mrigank Raman, Pratyush Maini, J Kolter, Zachary Lipton, and Danish Pruthi.
    </span>
    <span class="ltx_bibblock">
     Model-tuning via prompts makes NLP models adversarially robust.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib26.3.1">
      EMNLP
     </span>
     , pages 9266–9286, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Russell and Norvig [2010]
    </span>
    <span class="ltx_bibblock">
     Stuart J Russell and Peter Norvig.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">
      Artificial intelligence a modern approach
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     London, 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick and Schütze [2021]
    </span>
    <span class="ltx_bibblock">
     Timo Schick and Hinrich Schütze.
    </span>
    <span class="ltx_bibblock">
     Exploiting cloze-questions for few-shot text classification and natural language inference.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">
      EACL
     </span>
     , pages 255–269, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shin
     <span class="ltx_text ltx_font_italic" id="bib.bib29.2.2.1">
      et al.
     </span>
     [2020]
    </span>
    <span class="ltx_bibblock">
     Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh.
    </span>
    <span class="ltx_bibblock">
     AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib29.3.1">
      EMNLP
     </span>
     , pages 4222–4235, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn
     <span class="ltx_text ltx_font_italic" id="bib.bib30.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.
    </span>
    <span class="ltx_bibblock">
     Reflexion: language agents with verbal reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib30.3.1">
      Advances in NIPS
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Si
     <span class="ltx_text ltx_font_italic" id="bib.bib31.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang.
    </span>
    <span class="ltx_bibblock">
     Prompting GPT-3 to be reliable.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib31.3.1">
      ICLR
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sundararajan
     <span class="ltx_text ltx_font_italic" id="bib.bib32.2.2.1">
      et al.
     </span>
     [2017]
    </span>
    <span class="ltx_bibblock">
     Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
    </span>
    <span class="ltx_bibblock">
     Axiomatic attribution for deep networks.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib32.3.1">
      ICML
     </span>
     , pages 3319–3328, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wallace
     <span class="ltx_text ltx_font_italic" id="bib.bib33.2.2.1">
      et al.
     </span>
     [2019]
    </span>
    <span class="ltx_bibblock">
     Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
    </span>
    <span class="ltx_bibblock">
     Universal adversarial triggers for attacking and analyzing nlp.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib33.3.1">
      EMNLP-IJCNLP
     </span>
     , pages 2153–2162, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang
     <span class="ltx_text ltx_font_italic" id="bib.bib34.2.2.1">
      et al.
     </span>
     [2019]
    </span>
    <span class="ltx_bibblock">
     Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
    </span>
    <span class="ltx_bibblock">
     GLUE: A multi-task benchmark and analysis platform for natural language understanding.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib34.3.1">
      ICLR
     </span>
     . OpenReview.net, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang
     <span class="ltx_text ltx_font_italic" id="bib.bib35.2.2.1">
      et al.
     </span>
     [2022]
    </span>
    <span class="ltx_bibblock">
     Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.
    </span>
    <span class="ltx_bibblock">
     Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib35.3.1">
      EMNLP
     </span>
     , pages 5085–5109, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang
     <span class="ltx_text ltx_font_italic" id="bib.bib36.2.2.1">
      et al.
     </span>
     [2023a]
    </span>
    <span class="ltx_bibblock">
     Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
    </span>
    <span class="ltx_bibblock">
     Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib36.3.1">
      ACL
     </span>
     , pages 2609–2634, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang
     <span class="ltx_text ltx_font_italic" id="bib.bib37.2.2.1">
      et al.
     </span>
     [2023b]
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Self-consistency improves chain of thought reasoning in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib37.3.1">
      ICLR
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang
     <span class="ltx_text ltx_font_italic" id="bib.bib38.2.2.1">
      et al.
     </span>
     [2023c]
    </span>
    <span class="ltx_bibblock">
     Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang.
    </span>
    <span class="ltx_bibblock">
     Describe, explain, plan and select: Interactive planning with LLMs enables open-world multi-task agents.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib38.3.1">
      Advances in NIPS
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei
     <span class="ltx_text ltx_font_italic" id="bib.bib39.2.2.1">
      et al.
     </span>
     [2022a]
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
    </span>
    <span class="ltx_bibblock">
     Emergent abilities of large language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib39.3.1">
      TMLR
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei
     <span class="ltx_text ltx_font_italic" id="bib.bib40.2.2.1">
      et al.
     </span>
     [2022b]
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Chain of thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib40.3.1">
      Advances in NIPS
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang and Liu [2022]
    </span>
    <span class="ltx_bibblock">
     Zonghan Yang and Yang Liu.
    </span>
    <span class="ltx_bibblock">
     On robust prefix-tuning for text classification.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">
      ICLR
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao
     <span class="ltx_text ltx_font_italic" id="bib.bib42.2.2.1">
      et al.
     </span>
     [2023a]
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan.
    </span>
    <span class="ltx_bibblock">
     Tree of thoughts: Deliberate problem solving with large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib42.3.1">
      Advances in NIPS
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao
     <span class="ltx_text ltx_font_italic" id="bib.bib43.2.2.1">
      et al.
     </span>
     [2023b]
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib43.3.1">
      ICLR
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang
     <span class="ltx_text ltx_font_italic" id="bib.bib44.2.2.1">
      et al.
     </span>
     [2022]
    </span>
    <span class="ltx_bibblock">
     Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, and Diyi Yang.
    </span>
    <span class="ltx_bibblock">
     Robustness of demonstration-based learning under limited data scenario.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib44.3.1">
      EMNLP
     </span>
     , pages 1769–1782, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang
     <span class="ltx_text ltx_font_italic" id="bib.bib45.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.
    </span>
    <span class="ltx_bibblock">
     Automatic chain of thought prompting in large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib45.3.1">
      ICLR
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhong
     <span class="ltx_text ltx_font_italic" id="bib.bib46.2.2.1">
      et al.
     </span>
     [2023]
    </span>
    <span class="ltx_bibblock">
     Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
    </span>
    <span class="ltx_bibblock">
     Agieval: A human-centric benchmark for evaluating foundation models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib46.3.1">
      arXiv preprint arXiv:2304.06364
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zimmerman [2000]
    </span>
    <span class="ltx_bibblock">
     Barry J Zimmerman.
    </span>
    <span class="ltx_bibblock">
     Attaining self-regulation: A social cognitive perspective.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">
      Handbook of self-regulation
     </span>
     , pages 13–39. 2000.
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
</article>
