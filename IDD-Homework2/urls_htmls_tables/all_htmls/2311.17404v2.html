<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.17404] VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models</title><meta property="og:description" content="The ability to perceive how objects change over time is a crucial ingredient in human intelligence.
However, current benchmarks cannot faithfully reflect the temporal understanding abilities of video-language models (V…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.17404">

<!--Generated on Tue Feb 27 16:17:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shicheng Li<sup id="id1.1.id1" class="ltx_sup">§</sup>,
Lei Li<sup id="id2.2.id2" class="ltx_sup">§</sup>,
Shuhuai Ren<sup id="id3.3.id3" class="ltx_sup">§</sup>,
Yuanxin Liu<sup id="id4.4.id4" class="ltx_sup">§</sup>,
Yi Liu<sup id="id5.5.id5" class="ltx_sup">§</sup>,
Rundong Gao<sup id="id6.6.id6" class="ltx_sup">§</sup>,
Xu Sun<sup id="id7.7.id7" class="ltx_sup">§</sup>,
Lu Hou<sup id="id8.8.id8" class="ltx_sup">‡</sup> 
<br class="ltx_break"><sup id="id9.9.id9" class="ltx_sup">§</sup> National Key Laboratory for Multimedia Information Processing,
<br class="ltx_break">School of Computer Science, Peking University
<br class="ltx_break"><sup id="id10.10.id10" class="ltx_sup">‡</sup> Huawei Noah’s Ark Lab 
<br class="ltx_break"><span id="id11.11.id11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{lisc99, xusun}@pku.edu.cn  nlp.lilei@gmail.com  houlu3@huawei.com</span> 
<br class="ltx_break"><span id="id12.12.id12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{liuyuanxin, shuhuai_ren, gaord20}@stu.pku.edu.cn  yliu.pku@outlook.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">The ability to perceive how objects change over time is a crucial ingredient in human intelligence.
However, current benchmarks cannot faithfully reflect the temporal understanding abilities of video-language models (VidLMs) due to the existence of static visual shortcuts.
To remedy this issue, we present <span id="id13.id1.1" class="ltx_text ltx_font_bold">VITATECS</span>, a diagnostic <span id="id13.id1.2" class="ltx_text ltx_font_bold">VI</span>deo-<span id="id13.id1.3" class="ltx_text ltx_font_bold">T</span>ext d<span id="id13.id1.4" class="ltx_text ltx_font_bold">A</span>taset for the evaluation of <span id="id13.id1.5" class="ltx_text ltx_font_bold">TE</span>mporal <span id="id13.id1.6" class="ltx_text ltx_font_bold">C</span>oncept under<span id="id13.id1.7" class="ltx_text ltx_font_bold">S</span>tanding.
Specifically, we first introduce a fine-grained taxonomy of temporal concepts in natural language in order to diagnose the capability of VidLMs to comprehend different temporal aspects.
Furthermore, to disentangle the correlation between static and temporal information, we generate counterfactual video descriptions that differ from the original one only in the specified temporal aspect. We employ a semi-automatic data collection framework using large language models and human-in-the-loop annotation to obtain high-quality counterfactual descriptions efficiently. Evaluation of representative video-language understanding models confirms their deficiency in temporal understanding, revealing the need for greater emphasis on the temporal elements in video-language research.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our dataset is available at <a target="_blank" href="https://github.com/lscpku/VITATECS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lscpku/VITATECS</a>.</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.17404/assets/main/figures/gap3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="509" height="646" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Illustration of the gap between current training and evaluation procedures and real-world applications. In current video-language datasets (<span id="S1.F1.5.2.1" class="ltx_text" style="color:#0000FF;">blue</span> box), temporal information is highly correlated with static scenes. Models trained and evaluated on them cannot acquire the ability to understand temporal concepts, leading to failure in challenging real-world applications (<span id="S1.F1.5.2.2" class="ltx_text" style="color:#FF0000;">red</span> box).</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Many important concepts in human languages contain a temporal dimension <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, such as human actions, changes in status, and event order, which are beyond the expressive power of individual static images.
Such temporal concepts bring great challenges to video-language learning and are crucial for the generalization capability of intelligent systems in real-life scenarios.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although these temporal concepts are present in existing text-to-video retrieval <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>, <a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> or video question answering <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite> benchmarks, most of these datasets fail to faithfully assess the temporal understanding ability of Video-Language Models (VidLMs) due to the strong correlation between static objects/scenes and temporal information.
For example, in the blue box in <a href="#S1.F1" title="In 1 Introduction ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>,
each video can be aligned to its description by merely identifying the static objects such as the fire, the microphone, and the PC case.
As a consequence, the models may learn to simply rely on static clues to make predictions, leading to failure in real-world applications that require a genuine understanding of temporal concepts, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, to distinguish between the action of “connecting something to system” and “disconnecting something from system” as demonstrated by the red box in <a href="#S1.F1" title="In 1 Introduction ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
Previous works <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> have pointed out similar issues and provided several solutions. However, they do not properly define and categorize different aspects of temporal information.
The lack of a clear definition adds to the difficulty of assessing the precise abilities of VidLMs.
Additionally, they often construct evaluation datasets by following certain templates or using synthetic scenes, making them unsuitable for more diverse and realistic scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In light of the drawbacks of current video-language testbeds, we propose a new dataset for VidLMs, <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">VITATECS</span>, to fill the gap for temporal concept understanding evaluation by decoupling temporal information and static information.
Inspired by Winoground <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>, to measure the ability of VidLMs to understand and align the temporal concepts, we ask the models to distinguish between the correct caption of a video and a modified version of the caption which contains similar static information and only differs in temporal information.
To allow for a more comprehensive and fine-grained evaluation of temporal understanding ability, we summarize several aspects of temporal concepts that are commonly present in video descriptions, including <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">Direction</em>, <em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">Intensity</em>, <em id="S1.p3.1.4" class="ltx_emph ltx_font_italic">Sequence</em>, <em id="S1.p3.1.5" class="ltx_emph ltx_font_italic">Localization</em>, <em id="S1.p3.1.6" class="ltx_emph ltx_font_italic">Compositionality</em> and <em id="S1.p3.1.7" class="ltx_emph ltx_font_italic">Type</em>, which according to our study cover most of the temporal information in video-language datasets.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Since collecting high-quality video-text pairs is time-consuming and expensive, we follow previous works in dataset construction <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, and augment existing open-domain video-language datasets by harnessing the world knowledge encoded in pre-trained large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>.
Specifically, given an annotated video-text pair in the dataset, we ask the LLM to generate a counterfactual description that only differs from the original description in one given temporal aspect using in-context learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.
To prevent potential mismatch when dealing with complex instructions, we design a human-in-the-loop procedure to filter out low-quality generations by iteratively generating counterfactual descriptions, human labeling, and fine-tuning a filter model.
In each iteration, the generated samples are used to update the filter model and the in-context learning exemplar set to boost generation and filtering quality.
This annotation framework allows us to construct a 13k+ dataset from 231 human-written counterfactuals while maintaining high quality and diversity.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Based on our dataset, we conduct a comprehensive evaluation of state-of-the-art video-language understanding models.
Our findings can be summarized as follows.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Existing models barely surpass random guesses in many aspects, confirming their general lack of temporal understanding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Temporally-adapted image-text models outperform video-text pre-training, but primarily due to better utilization of static clues.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Failure of text encoders to learn temporal concepts during pre-training is partly responsible for low performance on temporal understanding.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Different video-text datasets tend to invoke different temporal understanding abilities.</p>
</div>
</li>
</ul>
<p id="S1.p5.2" class="ltx_p">In summary, our work with VITATECS sheds light on limitations in current VidLMs’ temporal understanding, providing insights for future development.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:105.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.1pt,10.2pt) scale(0.83742235688064,0.83742235688064) ;">
<table id="S1.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.2.1.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S1.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">video-language</th>
<th id="S1.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">open-domain</th>
<th id="S1.T1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">temporal</th>
<th id="S1.T1.2.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">fine-grained</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.2.1.2.1" class="ltx_tr">
<td id="S1.T1.2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Temporal Dataset <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>
</td>
<td id="S1.T1.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S1.T1.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">✗</td>
</tr>
<tr id="S1.T1.2.1.3.2" class="ltx_tr">
<td id="S1.T1.2.1.3.2.1" class="ltx_td ltx_align_left">CATER <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>
</td>
<td id="S1.T1.2.1.3.2.2" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.2.1.3.2.3" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.2.1.3.2.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center">✗</td>
</tr>
<tr id="S1.T1.2.1.4.3" class="ltx_tr">
<td id="S1.T1.2.1.4.3.1" class="ltx_td ltx_align_left">CLEVRER <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite>
</td>
<td id="S1.T1.2.1.4.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.4.3.3" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.2.1.4.3.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.2.1.5.4" class="ltx_tr">
<td id="S1.T1.2.1.5.4.1" class="ltx_td ltx_align_left">SSv2-label <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td id="S1.T1.2.1.5.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.5.4.3" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.2.1.5.4.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center">✗</td>
</tr>
<tr id="S1.T1.2.1.6.5" class="ltx_tr">
<td id="S1.T1.2.1.6.5.1" class="ltx_td ltx_align_left">Contrast set <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>
</td>
<td id="S1.T1.2.1.6.5.2" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.6.5.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.2.1.6.5.4" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.2.1.6.5.5" class="ltx_td ltx_nopad_r ltx_align_center">✗</td>
</tr>
<tr id="S1.T1.2.1.7.6" class="ltx_tr">
<td id="S1.T1.2.1.7.6.1" class="ltx_td ltx_align_left ltx_border_bb">VITATECS (Ours)</td>
<td id="S1.T1.2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S1.T1.2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S1.T1.2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S1.T1.2.1.7.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">✓</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison with other diagnostic video datasets from four aspects: whether they are video-language datasets, whether they are open-domain, whether they target temporal understanding ability, and whether they contain a fine-grained evaluation of model abilities. </span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Video-Language Understanding.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">With the great success of end-to-end deep learning models in natural language processing and image-text understanding, the research community has shown a growing interest in the more challenging task of video-language understanding, with promising results achieved on a wide range of tasks including video captioning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">91</span></a>]</cite>, video question answering <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite> and video-text retrieval <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>.
Some research <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> follows the prevalent paradigm in NLP and multi-modal understanding by directly conducting pre-training on video-text pairs.
Another line of work <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite> adapts powerful image-text pre-trained models like CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> to transfer their knowledge to the video-language domain.
Recent studies <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> have also explored the possibility of integrating LLMs with vision encoders to perform video-language understanding tasks.
Despite these valuable efforts, we argue that the apparent prosperity of video-language understanding models still rests upon the power of image-language models and that more attention should be paid to their temporal understanding abilities.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets on Temporal Understanding.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Although the temporal dimension is the primary difference between videos and images, it has not received proper acknowledgment from current model design and dataset construction processes in the video-language community.
Previous works <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite> have pointed out the lack of emphasis on temporal understanding abilities.
Evidence of this negligence includes the insensitivity of models to the frame order of input videos <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">88</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>, several order-agnostic architecture designs with state-of-the-art retrieval performance <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, visualization of intermediate layer features or saliency maps <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, and even the success of using single frames training to achieve promising results <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
Although a few datasets have been proposed to address this issue, they do not properly define and categorize different aspects of temporal information in video-language understanding.
In addition, their videos are constructed from either computer-rendered synthetic videos <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite> or videos focusing on single human actions <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>, which are not representative of real-world videos.
We remedy these problems by identifying aspects of temporality and introducing a new dataset for measuring the temporal understanding abilities in VidLMs. See <a href="#S1.T1" title="In 1 Introduction ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> for a comparison between VITATECS and existing video datasets.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VITATECS: Diagnosing Temporal Concept Understanding</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we propose VITATECS, a new dataset for measuring how well VidLMs capture temporal information across modalities. It consists of <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">(video, caption, counterfactual)</span> triples, where the counterfactual description retains the same static information as the original caption while modifying its temporal information in one of the six fine-grained aspects that we define in <a href="#S3.SS1" title="3.1 Fine-Grained Temporal Understanding ‣ 3 VITATECS: Diagnosing Temporal Concept Understanding ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.
We elaborate on the details of our temporal dataset in <a href="#S3.SS2" title="3.2 Dataset Format ‣ 3 VITATECS: Diagnosing Temporal Concept Understanding ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> and the human-in-the-loop annotation framework we devise to facilitate its construction process in <a href="#S3.SS3" title="3.3 Human-in-the-Loop Annotation Framework ‣ 3 VITATECS: Diagnosing Temporal Concept Understanding ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Fine-Grained Temporal Understanding</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.17404/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.5.2" class="ltx_text" style="font-size:90%;">Examples from the six aspects of our dataset. Each sample contains a video, a ground-truth caption, and a counterfactual description with modifications in the given temporal aspect. Differences between the sentence pairs are highlighted in <span id="S3.F2.5.2.1" class="ltx_text" style="color:#0000FF;">blue</span> and <span id="S3.F2.5.2.2" class="ltx_text" style="color:#FF0000;">red</span>.</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Measuring the temporal understanding ability of VidLMs is a challenging task.
On one hand, it is not clear how to define and characterize the temporal information in a video.
Previous works <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite> draw a rough equivalence between temporal information and the actions in the video.
In reality, temporal information can emerge in a variety of forms, such as human actions, changes in object status, dynamics of substances, the order of events, <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">etc.</em>, and is widely manifested in daily activities.
On the other hand, it is infeasible to completely disentangle the temporal information from the static information.
The background scenes, objects, and people’s postures are all highly correlated with the temporal information in open-domain videos.
If not properly controlled, such static bias would allow models to rely on static clues as shortcuts for making predictions while seemingly learning to capture the temporal information.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To achieve high coverage of temporal information in video-language datasets and allow for fine-grained diagnosis of temporal understanding abilities, we identify six aspects of temporal concepts commonly reflected in natural language: <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">Direction</em>, <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Intensity</em>, <em id="S3.SS1.p2.1.3" class="ltx_emph ltx_font_italic">Sequence</em>, <em id="S3.SS1.p2.1.4" class="ltx_emph ltx_font_italic">Localization</em>, <em id="S3.SS1.p2.1.5" class="ltx_emph ltx_font_italic">Compositionality</em> and <em id="S3.SS1.p2.1.6" class="ltx_emph ltx_font_italic">Type</em>.
These aspects of temporal information are disentangled from static information to different degrees and address different facets of the temporal information in video-language datasets, allowing us to pinpoint the temporal understanding abilities of VidLMs.
Since our final target is to construct text pairs with aspect-specific modifications, for clarity, we define these aspects in terms of the temporal questions they address and the corresponding modification patterns as follows.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">“Direction”</span> measures the model’s ability to answer the following question: “In which direction does the status of objects change?” Examples of this aspect include sentence pairs describing opposite spatial movements or one action reversing the effect of the other.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">“Intensity” </span> measures the model’s ability to answer the following question: “How fast or how intense does the change occur?” Examples of this aspect include counterfactual sentences which change the words that modify the verbs or change the verb to a similar action with subtle differences in the manner it is conducted.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">“Sequence”</span> measures the model’s ability to answer the following question: “How many events are depicted in the video and in what order?” Examples of this aspect usually involve changing the temporal order or number of occurrences of the events.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">“Localization”</span> measures the model’s ability to answer the following question: “On which part of the frame does the change occur?” Examples of this aspect include sentence pairs with the same action conducted either in different absolute spatial locations or in different locations in relation to other objects in the video.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">“Compositionality”</span> measures the model’s ability to answer the following question: “Who performed which action and to whom?” Examples of this aspect often include actions with interchanged subjects or objects.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">“Type”</span> measures the model’s ability to answer the following question: “What is the action depicted in the video?” This aspect contains general alterations to the actions with a less stringent constraint on the static information contained.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">To validate the coverage of our temporal concept categorization, we randomly sample 200 video-text pairs from MSR-VTT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite> and VATEX <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> and inspect the types of temporal information they contain. We find that for 98% of the samples, their temporal information falls in one of our categories, which demonstrates that our taxonomy is able to achieve high coverage while taking into account the disentanglement from static information.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Format</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.9" class="ltx_p">Following Winoground <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>, we measure the ability of VidLMs to match the videos to their correct descriptions among some well-designed choices as a proxy for their temporal understanding abilities.
Specifically, for each aspect <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">a</annotation></semantics></math>, we collect <span id="S3.SS2.p1.9.1" class="ltx_text ltx_font_typewriter">(video, caption, counterfactual)</span> triples <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\{(V_{i},C_{i},\tilde{C}_{i})\}_{i=1}^{N_{a}}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msubsup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mrow id="S3.SS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.4.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.4" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.4.cmml">(</mo><msub id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.5" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.2.cmml">C</mi><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.6" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.cmml"><mover accent="true" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.2.cmml">C</mi><mo id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.7" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.4.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p1.2.m2.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p1.2.m2.1.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p1.2.m2.1.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.1.3.3.cmml">1</mn></mrow><msub id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">N</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">a</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><set id="S3.SS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1"><vector id="S3.SS2.p1.2.m2.1.1.1.1.1.1.4.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3"><apply id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.2">𝐶</ci><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><apply id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3">subscript</csymbol><apply id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2"><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.1">~</ci><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.2.2">𝐶</ci></apply><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></vector></set><apply id="S3.SS2.p1.2.m2.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.3"><eq id="S3.SS2.p1.2.m2.1.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.3.1"></eq><ci id="S3.SS2.p1.2.m2.1.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">𝑁</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\{(V_{i},C_{i},\tilde{C}_{i})\}_{i=1}^{N_{a}}</annotation></semantics></math> where <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="N_{a}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">N</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑁</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">N_{a}</annotation></semantics></math> denotes the number of samples for aspect <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">a</annotation></semantics></math>, <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">V</mi><mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑉</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">V_{i}</annotation></semantics></math> denotes the video, <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">C</mi><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝐶</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">C_{i}</annotation></semantics></math> denotes the true caption of the video, and <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="\tilde{C}_{i}" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><msub id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mover accent="true" id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2.2" xref="S3.SS2.p1.7.m7.1.1.2.2.cmml">C</mi><mo id="S3.SS2.p1.7.m7.1.1.2.1" xref="S3.SS2.p1.7.m7.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2"><ci id="S3.SS2.p1.7.m7.1.1.2.1.cmml" xref="S3.SS2.p1.7.m7.1.1.2.1">~</ci><ci id="S3.SS2.p1.7.m7.1.1.2.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2.2">𝐶</ci></apply><ci id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">\tilde{C}_{i}</annotation></semantics></math> is the counterfactual description that differs from <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><msub id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">C</mi><mi id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">𝐶</ci><ci id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">C_{i}</annotation></semantics></math> only in the temporal aspect <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mi id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><ci id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">a</annotation></semantics></math>. <a href="#S3.F2" title="In 3.1 Fine-Grained Temporal Understanding ‣ 3 VITATECS: Diagnosing Temporal Concept Understanding ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows examples of our dataset.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2311.17404/assets/main/figures/flowchart.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.7.2" class="ltx_text" style="font-size:90%;">Illustration of our human-in-the-loop annotation framework. Texts in <span id="S3.F3.7.2.1" class="ltx_text" style="color:#FF8000;">orange</span> indicate the labels predicted by the filter model. Texts in <span id="S3.F3.7.2.2" class="ltx_text" style="color:#FF0000;">red</span>, <span id="S3.F3.7.2.3" class="ltx_text" style="color:#00FFFF;">blue</span> and <span id="S3.F3.7.2.4" class="ltx_text" style="color:#800080;">purple</span> indicate candidates that are eliminated by the NLI model, the filter model and human annotators, respectively. </span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Human-in-the-Loop Annotation Framework</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Due to the heavy expenses of collecting high-quality <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">(video, caption, counterfactual)</span> triples, we present a human-in-the-loop annotation framework for semi-automatic counterfactual generation based on existing <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">(video, caption)</span> datasets.
At the core of our framework is a loop consisting of three stages: <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_bold">generation</span>, <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_bold">filtering</span>, and <span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_bold">revision</span>.
In stage 1, we use in-context learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> to generate candidate counterfactuals based on ground-truth video-text pairs with LLMs.
In stage 2, the candidates are filtered using a combination of rules, off-the-shelf language understanding models, and fine-tuned language understanding models.
In stage 3, we ask human annotators to verify the quality of the candidates and use the high-quality ones to refine the generation process and the filter model.
The three stages are conducted on a small subset and are repeated until the filter model achieves satisfactory precision on a held-out evaluation set.
Below, we first lay out the criteria for our counterfactual descriptions and then elaborate on the details of each stage.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Criteria.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">During our effort to construct the dataset, we found that LLMs encounter some difficulty in following our instructions when generating counterfactual descriptions, possibly due to the reflective nature of our temporal concepts.
To enable consistent and high-quality counterfactual generation, we first identify five major criteria for measuring the quality of generated counterfactuals as follows.</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S3.I2.ix1.p1" class="ltx_para">
<p id="S3.I2.ix1.p1.1" class="ltx_p">The counterfactual should neither entail nor be entailed by the caption.</p>
</div>
</li>
<li id="S3.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S3.I2.ix2.p1" class="ltx_para">
<p id="S3.I2.ix2.p1.1" class="ltx_p">The counterfactual should contain roughly the same amount of information as the caption.</p>
</div>
</li>
<li id="S3.I2.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S3.I2.ix3.p1" class="ltx_para">
<p id="S3.I2.ix3.p1.1" class="ltx_p">The counterfactual should be grammatically correct and semantically plausible.</p>
</div>
</li>
<li id="S3.I2.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="S3.I2.ix4.p1" class="ltx_para">
<p id="S3.I2.ix4.p1.1" class="ltx_p">The counterfactual should retain the static information in the caption and only change the given aspect of temporal information.</p>
</div>
</li>
<li id="S3.I2.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(e)</span> 
<div id="S3.I2.ix5.p1" class="ltx_para">
<p id="S3.I2.ix5.p1.1" class="ltx_p">The pattern of counterfactual description should be diverse across the entire dataset.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p2.1" class="ltx_p">Among these desirable properties, criteria (a)-(d) are instance-level criteria we aim to address in both the generation and filtering stages. In contrast, criterion (e) is a dataset-level criterion dealt with in a finalization step after the filter model has converged.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Exemplar Sets.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.3" class="ltx_p">Throughout our annotation process, we maintain three sets of exemplars: positive set <math id="S3.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{X}^{+}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><msup id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2">𝒳</ci><plus id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\mathcal{X}^{+}</annotation></semantics></math> contains sentence pairs that differ only in a given aspect; negative set <math id="S3.SS3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{X}^{-}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><msup id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2">𝒳</ci><minus id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">\mathcal{X}^{-}</annotation></semantics></math> contains sentence pairs that violate one of the aforementioned criteria (a)-(d); N/A set <math id="S3.SS3.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{X}^{NA}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><msup id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml">𝒳</mi><mrow id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.3.cmml">A</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2">𝒳</ci><apply id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3"><times id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.2">𝑁</ci><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">\mathcal{X}^{NA}</annotation></semantics></math> contains captions that do not describe a certain aspect of the temporal concept.
These exemplars serve two purposes: on the one hand, they compose the demonstrations of valid and invalid data samples for in-context learning, which supply the generative language models with clearer and better-informed instructions; on the other hand, they provide supervision signals for the fine-tuning of the filter model.
These three sets are initialized with manually annotated examples and expanded semi-automatically to boost the generation and filter model performance as more data samples are generated.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">In-Context Learning Generation.</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.2" class="ltx_p">In this stage, we draw upon the generative strength of ChatGPT (<span id="S3.SS3.SSS0.Px3.p1.2.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo-0613</span>) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> to generate counterfactual descriptions given the original caption and the desired aspect of variation.
The use of in-context learning allows us to capture the different aspects of temporal concepts through carefully-designed instructions and demonstrations.
Specifically, we first randomly sample a small subset (500 for each aspect) of <span id="S3.SS3.SSS0.Px3.p1.2.2" class="ltx_text ltx_font_typewriter">(video, caption)</span> pairs from the test sets of two popular video-text retrieval datasets, MSR-VTT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite> and VATEX <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>.
Then, for each <span id="S3.SS3.SSS0.Px3.p1.2.3" class="ltx_text ltx_font_typewriter">(video, caption)</span> pair, we invoke the instruction following and pattern replication abilities of ChatGPT by constructing a prompt consisting of an aspect-specific instruction, demonstrations sampled from the exemplar sets, and the query for which we aim to generate the counterfactual description.
The demonstrations are sampled from both <math id="S3.SS3.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{X}^{+}" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><msup id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2">𝒳</ci><plus id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">\mathcal{X}^{+}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{X}^{NA}" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.2.m2.1a"><msup id="S3.SS3.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml">𝒳</mi><mrow id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.2" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.1" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.3" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.3.cmml">A</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2">𝒳</ci><apply id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3"><times id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.1"></times><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.2">𝑁</ci><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.2.m2.1c">\mathcal{X}^{NA}</annotation></semantics></math> so that the LLM not only learns to generate counterfactual descriptions for valid captions but also learns to recognize which captions do not concern the temporal aspect of interest.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Automatic Filtering.</h4>

<div id="S3.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px4.p1.2" class="ltx_p">In view of the uneven quality of generated examples, we propose to filter the candidates and automatize this procedure using natural language understanding models.
First, we leverage an off-the-shelf natural language inference (NLI) model, Sentence-BERT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite>, to filter out examples that do not meet criterion (a), <em id="S3.SS3.SSS0.Px4.p1.2.1" class="ltx_emph ltx_font_italic">i.e.</em>, the cases where one description entails the other.
Then, to filter out candidates that do not meet criterion (b)-(d), we use a neural network that takes a pair of sentences as input and performs a 7-way classification task, where category 0 corresponds to disqualified generations and categories 1-6 correspond to the six aspects we define.
Considering the similarity in task formulation, we initialize the filter model with the same NLI model above.
The fine-tuning data consists of samples from both <math id="S3.SS3.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{X}^{+}" display="inline"><semantics id="S3.SS3.SSS0.Px4.p1.1.m1.1a"><msup id="S3.SS3.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2">𝒳</ci><plus id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.1.m1.1c">\mathcal{X}^{+}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{X}^{-}" display="inline"><semantics id="S3.SS3.SSS0.Px4.p1.2.m2.1a"><msup id="S3.SS3.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2">𝒳</ci><minus id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.2.m2.1c">\mathcal{X}^{-}</annotation></semantics></math>.
We adopt a rigorous decision mechanism that classifies the given sentence pair into one of the six aspects only if the model makes consistent predictions for the pair and its reversed version with high confidence, as we care more about the precision of the filter model than its recall.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Human Revision.</h4>

<div id="S3.SS3.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px5.p1.1" class="ltx_p">To guarantee the quality of filtered examples and guide both the in-context learning procedure and the filter model in the right direction, we introduce human supervision to revise the filtering results.
We manually check the samples that are predicted to fall in one of the six aspects and correct the wrong predictions.
Note that, on the one hand, due to the relatively small size of the sampled subset and the rigorous confidence-based filtering procedure, the number of examples for human revision is reduced significantly; on the other hand, human annotators only need to rectify the predicted labels instead of writing the entire counterfactual description.
Therefore, this revision stage does not require excessive human effort and only incurs acceptable annotation costs.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Iterative Procedure.</h4>

<div id="S3.SS3.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px6.p1.2" class="ltx_p">We repeat the generation, filtering, and revision procedure to iteratively enlarge the exemplar sets and refine the filter model.
In each iteration, the previously revised examples are incorporated into <math id="S3.SS3.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{X}^{+}" display="inline"><semantics id="S3.SS3.SSS0.Px6.p1.1.m1.1a"><msup id="S3.SS3.SSS0.Px6.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px6.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px6.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px6.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px6.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px6.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1.2">𝒳</ci><plus id="S3.SS3.SSS0.Px6.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px6.p1.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px6.p1.1.m1.1c">\mathcal{X}^{+}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px6.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{X}^{-}" display="inline"><semantics id="S3.SS3.SSS0.Px6.p1.2.m2.1a"><msup id="S3.SS3.SSS0.Px6.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px6.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1.2.cmml">𝒳</mi><mo id="S3.SS3.SSS0.Px6.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px6.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px6.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px6.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px6.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1.2">𝒳</ci><minus id="S3.SS3.SSS0.Px6.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px6.p1.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px6.p1.2.m2.1c">\mathcal{X}^{-}</annotation></semantics></math> according to their labels.
This simultaneously augments the demonstration set of in-context learning for better generation quality and provides more training data for fine-tuning the filter model.
After each iteration, the fine-tuned filter model is evaluated on an independently annotated test set.
We terminate the iteration once no significant improvement of the filter model is observed.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Finalization.</h4>

<div id="S3.SS3.SSS0.Px7.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px7.p1.1" class="ltx_p">After the filter model has converged, we perform generation and filtering on a larger scale (20,000 for each aspect) without human revision.
As a finalization step, we address the issue of diversity by favoring generations that involve a less common change of verb throughout the dataset when merging the filtered samples.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:128.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.3pt,-19.4pt) scale(1.43057535843142,1.43057535843142) ;">
<table id="S3.T2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S3.T2.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dir.</span></th>
<td id="S3.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Int.</span></td>
<td id="S3.T2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Seq.</span></td>
<td id="S3.T2.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Loc.</span></td>
<td id="S3.T2.2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Comp.</span></td>
<td id="S3.T2.2.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S3.T2.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Type</span></td>
</tr>
<tr id="S3.T2.2.1.2.2" class="ltx_tr">
<th id="S3.T2.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T2.2.1.2.2.1.1" class="ltx_text" style="font-size:90%;"># samples</span></th>
<th id="S3.T2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T2.2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">3,800</span></th>
<td id="S3.T2.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.2.2.3.1" class="ltx_text" style="font-size:90%;">779</span></td>
<td id="S3.T2.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.2.2.4.1" class="ltx_text" style="font-size:90%;">151</span></td>
<td id="S3.T2.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.2.2.5.1" class="ltx_text" style="font-size:90%;">1,053</span></td>
<td id="S3.T2.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.2.2.6.1" class="ltx_text" style="font-size:90%;">1,450</span></td>
<td id="S3.T2.2.1.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S3.T2.2.1.2.2.7.1" class="ltx_text" style="font-size:90%;">6,605</span></td>
</tr>
<tr id="S3.T2.2.1.3.3" class="ltx_tr">
<th id="S3.T2.2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T2.2.1.3.3.1.1" class="ltx_text" style="font-size:90%;"># videos</span></th>
<th id="S3.T2.2.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T2.2.1.3.3.2.1" class="ltx_text" style="font-size:90%;">2,646</span></th>
<td id="S3.T2.2.1.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T2.2.1.3.3.3.1" class="ltx_text" style="font-size:90%;">692</span></td>
<td id="S3.T2.2.1.3.3.4" class="ltx_td ltx_align_center"><span id="S3.T2.2.1.3.3.4.1" class="ltx_text" style="font-size:90%;">150</span></td>
<td id="S3.T2.2.1.3.3.5" class="ltx_td ltx_align_center"><span id="S3.T2.2.1.3.3.5.1" class="ltx_text" style="font-size:90%;">915</span></td>
<td id="S3.T2.2.1.3.3.6" class="ltx_td ltx_align_center"><span id="S3.T2.2.1.3.3.6.1" class="ltx_text" style="font-size:90%;">1,110</span></td>
<td id="S3.T2.2.1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.2.1.3.3.7.1" class="ltx_text" style="font-size:90%;">4,287</span></td>
</tr>
<tr id="S3.T2.2.1.4.4" class="ltx_tr">
<th id="S3.T2.2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T2.2.1.4.4.1.1" class="ltx_text" style="font-size:90%;">Avg. len (caption)</span></th>
<th id="S3.T2.2.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T2.2.1.4.4.2.1" class="ltx_text" style="font-size:90%;">13.6</span></th>
<td id="S3.T2.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.4.4.3.1" class="ltx_text" style="font-size:90%;">13.6</span></td>
<td id="S3.T2.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.4.4.4.1" class="ltx_text" style="font-size:90%;">14.9</span></td>
<td id="S3.T2.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.4.4.5.1" class="ltx_text" style="font-size:90%;">14.6</span></td>
<td id="S3.T2.2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.2.1.4.4.6.1" class="ltx_text" style="font-size:90%;">13.9</span></td>
<td id="S3.T2.2.1.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S3.T2.2.1.4.4.7.1" class="ltx_text" style="font-size:90%;">11.7</span></td>
</tr>
<tr id="S3.T2.2.1.5.5" class="ltx_tr">
<th id="S3.T2.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S3.T2.2.1.5.5.1.1" class="ltx_text" style="font-size:90%;">Avg. len (counterfactual)</span></th>
<th id="S3.T2.2.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T2.2.1.5.5.2.1" class="ltx_text" style="font-size:90%;">13.8</span></th>
<td id="S3.T2.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.5.3.1" class="ltx_text" style="font-size:90%;">13.9</span></td>
<td id="S3.T2.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.5.4.1" class="ltx_text" style="font-size:90%;">14.9</span></td>
<td id="S3.T2.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.5.5.1" class="ltx_text" style="font-size:90%;">14.5</span></td>
<td id="S3.T2.2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.5.6.1" class="ltx_text" style="font-size:90%;">13.9</span></td>
<td id="S3.T2.2.1.5.5.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.5.7.1" class="ltx_text" style="font-size:90%;">11.6</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.4.2" class="ltx_text" style="font-size:90%;">Statistics of our dataset including the number of samples, the number of videos, and the average length of the original captions and the counterfactual descriptions. </span></figcaption>
</figure>
</section>
<section id="S3.SS3.SSS0.Px8" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotation Efficiency of the Framework.</h4>

<div id="S3.SS3.SSS0.Px8.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px8.p1.1" class="ltx_p">Our framework can be easily scaled to generate larger datasets since no more human efforts are required once the filter model has converged.
In our case, it only takes 231 human-written descriptions and around 1500 labeling annotations to obtain the final benchmark with 13k+ samples, showing the efficiency of our annotation framework. The statistics of our dataset are shown in <a href="#S3.T2" title="In Finalization. ‣ 3.3 Human-in-the-Loop Annotation Framework ‣ 3 VITATECS: Diagnosing Temporal Concept Understanding ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation of Video-Language Models</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we evaluate prevailing VidLMs to examine their temporal understanding ability.
We first introduce the evaluation settings and then discuss the findings drawn from our evaluation to facilitate future studies.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluated Models.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">In our experiments, we focus on models designed for the video-text retrieval task, which can calculate the similarity score between a video and a text query.
We test three pre-trained VidLMs (VIOLET <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, ALPRO <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> and Singularity <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>) and three temporally-adapted image-language models (CLIP4Clip <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, X-Pool <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and X-CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>).
We also include two recent video large language models, VideoLLaMA <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> and VideoChat <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, as well as pure image-text foundation models such as BLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, which has shown strong performance on zero-shot video-text retrieval.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Metric.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">A model’s prediction is considered correct if the similarity score of the correct caption is higher than that of the generated counterfactual.
We measure the accuracy of the models on each of the six aspects of temporal concepts, and explore a recall-based metric in <a href="#S4.SS3" title="4.3 Discussions ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Human Baseline.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">We randomly choose 100 samples for each aspect from our dataset and ask five volunteers to help establish a human performance baseline. The annotators are shown a video and two text descriptions at a time and are required to choose the text that best describes the video.
We report the average accuracy of the five annotators as the human baseline.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:422.3pt;height:132.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-103.6pt,32.6pt) scale(0.670874416236253,0.670874416236253) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="S4.T3.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Direction</span></th>
<th id="S4.T3.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Intensity</span></th>
<th id="S4.T3.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Sequence</span></th>
<th id="S4.T3.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Localization</span></th>
<th id="S4.T3.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Compositionality</span></th>
<th id="S4.T3.2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="S4.T3.2.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.2.1.1.1.9.1" class="ltx_text" style="font-size:90%;">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.2.1" class="ltx_tr">
<th id="S4.T3.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">BLIP-large </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a><span id="S4.T3.2.1.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Zero-shot</span></td>
<td id="S4.T3.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.3.1" class="ltx_text" style="font-size:90%;">58.6</span></td>
<td id="S4.T3.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">67.7</span></td>
<td id="S4.T3.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.5.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T3.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.6.1" class="ltx_text" style="font-size:90%;">66.2</span></td>
<td id="S4.T3.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.7.1" class="ltx_text" style="font-size:90%;">61.8</span></td>
<td id="S4.T3.2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.8.1" class="ltx_text" style="font-size:90%;">78.6</span></td>
<td id="S4.T3.2.1.2.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.9.1" class="ltx_text" style="font-size:90%;">64.1</span></td>
</tr>
<tr id="S4.T3.2.1.3.2" class="ltx_tr">
<th id="S4.T3.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Singularity </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a><span id="S4.T3.2.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T3.2.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.3.1" class="ltx_text" style="font-size:90%;">54.7</span></td>
<td id="S4.T3.2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.4.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
<td id="S4.T3.2.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.5.1" class="ltx_text" style="font-size:90%;">52.3</span></td>
<td id="S4.T3.2.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.6.1" class="ltx_text" style="font-size:90%;">63.0</span></td>
<td id="S4.T3.2.1.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.5</span></td>
<td id="S4.T3.2.1.3.2.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.3.2.8.1" class="ltx_text" style="font-size:90%;">77.4</span></td>
<td id="S4.T3.2.1.3.2.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.3.2.9.1" class="ltx_text" style="font-size:90%;">62.4</span></td>
</tr>
<tr id="S4.T3.2.1.4.3" class="ltx_tr">
<th id="S4.T3.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.4.3.1.1" class="ltx_text" style="font-size:90%;">ALPRO </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a><span id="S4.T3.2.1.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T3.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.3.1" class="ltx_text" style="font-size:90%;">55.4</span></td>
<td id="S4.T3.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.4.1" class="ltx_text" style="font-size:90%;">56.0</span></td>
<td id="S4.T3.2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.5.1" class="ltx_text" style="font-size:90%;">45.7</span></td>
<td id="S4.T3.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.6.1" class="ltx_text" style="font-size:90%;">59.2</span></td>
<td id="S4.T3.2.1.4.3.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.7.1" class="ltx_text" style="font-size:90%;">58.6</span></td>
<td id="S4.T3.2.1.4.3.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.8.1" class="ltx_text" style="font-size:90%;">74.5</span></td>
<td id="S4.T3.2.1.4.3.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.4.3.9.1" class="ltx_text" style="font-size:90%;">58.2</span></td>
</tr>
<tr id="S4.T3.2.1.5.4" class="ltx_tr">
<th id="S4.T3.2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.5.4.1.1" class="ltx_text" style="font-size:90%;">VIOLET </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a><span id="S4.T3.2.1.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T3.2.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.3.1" class="ltx_text" style="font-size:90%;">60.2</span></td>
<td id="S4.T3.2.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.4.1" class="ltx_text" style="font-size:90%;">62.8</span></td>
<td id="S4.T3.2.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">61.6</span></td>
<td id="S4.T3.2.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.6.1" class="ltx_text" style="font-size:90%;">60.6</span></td>
<td id="S4.T3.2.1.5.4.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.7.1" class="ltx_text" style="font-size:90%;">64.8</span></td>
<td id="S4.T3.2.1.5.4.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.8.1" class="ltx_text" style="font-size:90%;">78.2</span></td>
<td id="S4.T3.2.1.5.4.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.5.4.9.1" class="ltx_text" style="font-size:90%;">64.7</span></td>
</tr>
<tr id="S4.T3.2.1.6.5" class="ltx_tr">
<th id="S4.T3.2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.6.5.1.1" class="ltx_text" style="font-size:90%;">CLIP4clip </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.6.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a><span id="S4.T3.2.1.6.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T3.2.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.3.1" class="ltx_text" style="font-size:90%;">62.6</span></td>
<td id="S4.T3.2.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.4.1" class="ltx_text" style="font-size:90%;">65.3</span></td>
<td id="S4.T3.2.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.5.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T3.2.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.5</span></td>
<td id="S4.T3.2.1.6.5.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.7.1" class="ltx_text" style="font-size:90%;">63.5</span></td>
<td id="S4.T3.2.1.6.5.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.8.1" class="ltx_text" style="font-size:90%;">82.4</span></td>
<td id="S4.T3.2.1.6.5.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.6.5.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.3</span></td>
</tr>
<tr id="S4.T3.2.1.7.6" class="ltx_tr">
<th id="S4.T3.2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.7.6.1.1" class="ltx_text" style="font-size:90%;">X-Pool </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.7.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="S4.T3.2.1.7.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T3.2.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.3.1" class="ltx_text" style="font-size:90%;">59.9</span></td>
<td id="S4.T3.2.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.4.1" class="ltx_text" style="font-size:90%;">63.0</span></td>
<td id="S4.T3.2.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.5.1" class="ltx_text" style="font-size:90%;">55.6</span></td>
<td id="S4.T3.2.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.5</span></td>
<td id="S4.T3.2.1.7.6.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.7.1" class="ltx_text" style="font-size:90%;">64.3</span></td>
<td id="S4.T3.2.1.7.6.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.8.1" class="ltx_text" style="font-size:90%;">81.3</span></td>
<td id="S4.T3.2.1.7.6.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.7.6.9.1" class="ltx_text" style="font-size:90%;">65.1</span></td>
</tr>
<tr id="S4.T3.2.1.8.7" class="ltx_tr">
<th id="S4.T3.2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.8.7.1.1" class="ltx_text" style="font-size:90%;">X-CLIP </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a><span id="S4.T3.2.1.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.8.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T3.2.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.6</span></td>
<td id="S4.T3.2.1.8.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.4.1" class="ltx_text" style="font-size:90%;">60.8</span></td>
<td id="S4.T3.2.1.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.5.1" class="ltx_text" style="font-size:90%;">55.6</span></td>
<td id="S4.T3.2.1.8.7.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.6.1" class="ltx_text" style="font-size:90%;">64.5</span></td>
<td id="S4.T3.2.1.8.7.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.7.1" class="ltx_text" style="font-size:90%;">63.7</span></td>
<td id="S4.T3.2.1.8.7.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.8.7.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">83.2</span></td>
<td id="S4.T3.2.1.8.7.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.8.7.9.1" class="ltx_text" style="font-size:90%;">65.2</span></td>
</tr>
<tr id="S4.T3.2.1.9.8" class="ltx_tr">
<th id="S4.T3.2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.9.8.1.1" class="ltx_text" style="font-size:90%;">Video-LLaMA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.9.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a><span id="S4.T3.2.1.9.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.9.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.2.1" class="ltx_text" style="font-size:90%;">Zero-shot</span></td>
<td id="S4.T3.2.1.9.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.3.1" class="ltx_text" style="font-size:90%;">51.6</span></td>
<td id="S4.T3.2.1.9.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.4.1" class="ltx_text" style="font-size:90%;">52.2</span></td>
<td id="S4.T3.2.1.9.8.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.5.1" class="ltx_text" style="font-size:90%;">56.3</span></td>
<td id="S4.T3.2.1.9.8.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.6.1" class="ltx_text" style="font-size:90%;">51.0</span></td>
<td id="S4.T3.2.1.9.8.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.7.1" class="ltx_text" style="font-size:90%;">49.4</span></td>
<td id="S4.T3.2.1.9.8.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.9.8.8.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T3.2.1.9.8.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.9.8.9.1" class="ltx_text" style="font-size:90%;">52.0</span></td>
</tr>
<tr id="S4.T3.2.1.10.9" class="ltx_tr">
<th id="S4.T3.2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.1.10.9.1.1" class="ltx_text" style="font-size:90%;">VideoChat </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.2.1.10.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a><span id="S4.T3.2.1.10.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.2.1.10.9.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.2.1" class="ltx_text" style="font-size:90%;">Zero-shot</span></td>
<td id="S4.T3.2.1.10.9.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.3.1" class="ltx_text" style="font-size:90%;">52.3</span></td>
<td id="S4.T3.2.1.10.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.4.1" class="ltx_text" style="font-size:90%;">50.3</span></td>
<td id="S4.T3.2.1.10.9.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.5.1" class="ltx_text" style="font-size:90%;">46.4</span></td>
<td id="S4.T3.2.1.10.9.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.6.1" class="ltx_text" style="font-size:90%;">50.4</span></td>
<td id="S4.T3.2.1.10.9.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.7.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T3.2.1.10.9.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.10.9.8.1" class="ltx_text" style="font-size:90%;">51.0</span></td>
<td id="S4.T3.2.1.10.9.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.2.1.10.9.9.1" class="ltx_text" style="font-size:90%;">50.4</span></td>
</tr>
<tr id="S4.T3.2.1.11.10" class="ltx_tr">
<th id="S4.T3.2.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.1.1" class="ltx_text" style="font-size:90%;color:#808080;">Human</span></th>
<td id="S4.T3.2.1.11.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T3.2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.3.1" class="ltx_text" style="font-size:90%;color:#808080;">94.6</span></td>
<td id="S4.T3.2.1.11.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.4.1" class="ltx_text" style="font-size:90%;color:#808080;">93.2</span></td>
<td id="S4.T3.2.1.11.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.5.1" class="ltx_text" style="font-size:90%;color:#808080;">94.0</span></td>
<td id="S4.T3.2.1.11.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.6.1" class="ltx_text" style="font-size:90%;color:#808080;">93.8</span></td>
<td id="S4.T3.2.1.11.10.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.7.1" class="ltx_text" style="font-size:90%;color:#808080;">97.8</span></td>
<td id="S4.T3.2.1.11.10.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.8.1" class="ltx_text" style="font-size:90%;color:#808080;">92.2</span></td>
<td id="S4.T3.2.1.11.10.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.11.10.9.1" class="ltx_text" style="font-size:90%;color:#808080;">94.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy (%) of human annotators and state-of-the-art VidLMs on VITATECS. Human performance is marked in <span id="S4.T3.7.1" class="ltx_text" style="color:#808080;">gray</span> to indicate it is evaluated only on a randomly sampled subset.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Results</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Overall Performance.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">As shown in <a href="#S4.T3" title="In Human Baseline. ‣ 4.1 Experimental Setup ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, although humans can easily match the videos to their correct descriptions with high consistency (<math id="S4.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\kappa=0.86" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">κ</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">0.86</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1"><eq id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝜅</ci><cn type="float" id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3">0.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">\kappa=0.86</annotation></semantics></math>) and nearly no mistakes, the overall performance of all the evaluated models is still far from expectations.
No model achieves an accuracy of over 70% on the temporal aspects other than the relatively easy “Type” aspect, which has the strongest correlation with the static information.
Particularly, on the more temporally demanding aspects (“Direction”, “Intensity”, and “Sequence”), the models perform barely over the random baseline (50%).
Considering that part of our videos directly comes from MSR-VTT, the poor performance of models fine-tuned on MSR-VTT reaffirms our statement that existing video-language datasets are incapable of assessing the temporal understanding ability of models.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:118.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.3pt,3.9pt) scale(0.938220642298882,0.938220642298882) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Temporal</span></th>
<th id="S4.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Dir.</span></th>
<th id="S4.T4.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Int.</span></th>
<th id="S4.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Seq.</span></th>
<th id="S4.T4.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Loc.</span></th>
<th id="S4.T4.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Comp.</span></th>
<th id="S4.T4.2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="S4.T4.2.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.1.1.1.9.1" class="ltx_text" style="font-size:90%;">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.2.1" class="ltx_tr">
<th id="S4.T4.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T4.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">CLIP4Clip <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite></span></th>
<td id="S4.T4.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S4.T4.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62.6</span></td>
<td id="S4.T4.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.4.1" class="ltx_text" style="font-size:90%;">65.3</span></td>
<td id="S4.T4.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.5.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T4.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.5</span></td>
<td id="S4.T4.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.5</span></td>
<td id="S4.T4.2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">82.4</span></td>
<td id="S4.T4.2.1.2.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.9.1" class="ltx_text" style="font-size:90%;">65.3</span></td>
</tr>
<tr id="S4.T4.2.1.3.2" class="ltx_tr">
<td id="S4.T4.2.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S4.T4.2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">61.6</span></td>
<td id="S4.T4.2.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">67.3</span></td>
<td id="S4.T4.2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.3</span></td>
<td id="S4.T4.2.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.5.1" class="ltx_text" style="font-size:90%;">66.1</span></td>
<td id="S4.T4.2.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.6.1" class="ltx_text" style="font-size:90%;">62.8</span></td>
<td id="S4.T4.2.1.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.3.2.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">82.4</span></td>
<td id="S4.T4.2.1.3.2.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.3.2.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.8</span></td>
</tr>
<tr id="S4.T4.2.1.4.3" class="ltx_tr">
<th id="S4.T4.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T4.2.1.4.3.1.1" class="ltx_text" style="font-size:90%;">X-CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite></span></th>
<td id="S4.T4.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S4.T4.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.6</span></td>
<td id="S4.T4.2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.4.1" class="ltx_text" style="font-size:90%;">60.8</span></td>
<td id="S4.T4.2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.5.1" class="ltx_text" style="font-size:90%;">55.6</span></td>
<td id="S4.T4.2.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.6.1" class="ltx_text" style="font-size:90%;">64.5</span></td>
<td id="S4.T4.2.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.7.1" class="ltx_text" style="font-size:90%;">63.7</span></td>
<td id="S4.T4.2.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">83.2</span></td>
<td id="S4.T4.2.1.4.3.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.3.9.1" class="ltx_text" style="font-size:90%;">65.2</span></td>
</tr>
<tr id="S4.T4.2.1.5.4" class="ltx_tr">
<td id="S4.T4.2.1.5.4.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.1.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S4.T4.2.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.2.1" class="ltx_text" style="font-size:90%;">62.1</span></td>
<td id="S4.T4.2.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.8</span></td>
<td id="S4.T4.2.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">59.6</span></td>
<td id="S4.T4.2.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.6</span></td>
<td id="S4.T4.2.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.2</span></td>
<td id="S4.T4.2.1.5.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.7.1" class="ltx_text" style="font-size:90%;">82.6</span></td>
<td id="S4.T4.2.1.5.4.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.5.4.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.3</span></td>
</tr>
<tr id="S4.T4.2.1.6.5" class="ltx_tr">
<th id="S4.T4.2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T4.2.1.6.5.1.1" class="ltx_text" style="font-size:90%;">X-Pool <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite></span></th>
<td id="S4.T4.2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S4.T4.2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.4</span></td>
<td id="S4.T4.2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.5</span></td>
<td id="S4.T4.2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">58.3</span></td>
<td id="S4.T4.2.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.6.1" class="ltx_text" style="font-size:90%;">65.0</span></td>
<td id="S4.T4.2.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.7.1" class="ltx_text" style="font-size:90%;">62.1</span></td>
<td id="S4.T4.2.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.8.1" class="ltx_text" style="font-size:90%;">79.9</span></td>
<td id="S4.T4.2.1.6.5.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T4.2.1.6.5.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.2</span></td>
</tr>
<tr id="S4.T4.2.1.7.6" class="ltx_tr">
<td id="S4.T4.2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.1.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S4.T4.2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.2.1" class="ltx_text" style="font-size:90%;">59.9</span></td>
<td id="S4.T4.2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.3.1" class="ltx_text" style="font-size:90%;">63.0</span></td>
<td id="S4.T4.2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.4.1" class="ltx_text" style="font-size:90%;">55.6</span></td>
<td id="S4.T4.2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.5</span></td>
<td id="S4.T4.2.1.7.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.3</span></td>
<td id="S4.T4.2.1.7.6.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">81.3</span></td>
<td id="S4.T4.2.1.7.6.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.7.6.8.1" class="ltx_text" style="font-size:90%;">65.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy (%) of CLIP-based models with and without temporal aggregation modules.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effects of Vision Encoders.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">Among the models we evaluate, the temporally-adapted image-text models based on CLIP generally outperform the models with video-text pre-training.
To further investigate how much the temporal aggregation modules contribute to the temporal understanding abilities of the CLIP-based models, we disable the temporal aggregation module in these models and replace it with a simple mean pooling layer.
The results are shown in <a href="#S4.T4" title="In Overall Performance. ‣ 4.2 Evaluation Results ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
Contrary to what is expected, disabling the temporal aggregation module only results in a slight drop in performance for X-Pool. It even improves the temporal understanding ability of CLIP4Clip and X-CLIP.
This suggests that these temporal aggregation modules are potentially under-trained due to the weak requirement of temporal modeling in video-language datasets like MSR-VTT.
Consequently, the superiority of the CLIP-based models mainly stems from the effective utilization of the static information in the video instead of a true understanding of the temporal concepts.
For a similar reason, image-text models are able to achieve comparable performance on our dataset without further video-text training.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:83.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.7pt,3.0pt) scale(0.932420196949376,0.932420196949376) ;">
<table id="S4.T5.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T5.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Text Encoder</span></th>
<th id="S4.T5.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dir.</span></th>
<th id="S4.T5.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Int.</span></th>
<th id="S4.T5.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Seq.</span></th>
<th id="S4.T5.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Loc.</span></th>
<th id="S4.T5.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Comp.</span></th>
<th id="S4.T5.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="S4.T5.2.1.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.2.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.1.2.1" class="ltx_tr">
<th id="S4.T5.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T5.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">CLIP-text </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.2.1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a><span id="S4.T5.2.1.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T5.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">0.963</span></td>
<td id="S4.T5.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.3.1" class="ltx_text" style="font-size:90%;">0.964</span></td>
<td id="S4.T5.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.4.1" class="ltx_text" style="font-size:90%;">0.975</span></td>
<td id="S4.T5.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.5.1" class="ltx_text" style="font-size:90%;">0.965</span></td>
<td id="S4.T5.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.6.1" class="ltx_text" style="font-size:90%;">0.970</span></td>
<td id="S4.T5.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.7.1" class="ltx_text" style="font-size:90%;">0.912</span></td>
<td id="S4.T5.2.1.2.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T5.2.1.2.1.8.1" class="ltx_text" style="font-size:90%;">0.958</span></td>
</tr>
<tr id="S4.T5.2.1.3.2" class="ltx_tr">
<th id="S4.T5.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T5.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Sentence-BERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.2.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a><span id="S4.T5.2.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T5.2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">0.890</span></td>
<td id="S4.T5.2.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.3.2.3.1" class="ltx_text" style="font-size:90%;">0.940</span></td>
<td id="S4.T5.2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.3.2.4.1" class="ltx_text" style="font-size:90%;">0.970</span></td>
<td id="S4.T5.2.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.3.2.5.1" class="ltx_text" style="font-size:90%;">0.916</span></td>
<td id="S4.T5.2.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.3.2.6.1" class="ltx_text" style="font-size:90%;">0.939</span></td>
<td id="S4.T5.2.1.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.3.2.7.1" class="ltx_text" style="font-size:90%;">0.704</span></td>
<td id="S4.T5.2.1.3.2.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.1.3.2.8.1" class="ltx_text" style="font-size:90%;">0.893</span></td>
</tr>
<tr id="S4.T5.2.1.4.3" class="ltx_tr">
<th id="S4.T5.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T5.2.1.4.3.1.1" class="ltx_text" style="font-size:90%;">CLIP4Clip </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.2.1.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a><span id="S4.T5.2.1.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T5.2.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.4.3.2.1" class="ltx_text" style="font-size:90%;">0.941</span></td>
<td id="S4.T5.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.4.3.3.1" class="ltx_text" style="font-size:90%;">0.939</span></td>
<td id="S4.T5.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.4.3.4.1" class="ltx_text" style="font-size:90%;">0.969</span></td>
<td id="S4.T5.2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.4.3.5.1" class="ltx_text" style="font-size:90%;">0.932</span></td>
<td id="S4.T5.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.4.3.6.1" class="ltx_text" style="font-size:90%;">0.947</span></td>
<td id="S4.T5.2.1.4.3.7" class="ltx_td ltx_align_center"><span id="S4.T5.2.1.4.3.7.1" class="ltx_text" style="font-size:90%;">0.828</span></td>
<td id="S4.T5.2.1.4.3.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.1.4.3.8.1" class="ltx_text" style="font-size:90%;">0.926</span></td>
</tr>
<tr id="S4.T5.2.1.5.4" class="ltx_tr">
<th id="S4.T5.2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S4.T5.2.1.5.4.1.1" class="ltx_text" style="font-size:90%;">CLIP4Clip-temporal </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.2.1.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a><span id="S4.T5.2.1.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T5.2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.2.1" class="ltx_text" style="font-size:90%;">0.946</span></td>
<td id="S4.T5.2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.3.1" class="ltx_text" style="font-size:90%;">0.946</span></td>
<td id="S4.T5.2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.4.1" class="ltx_text" style="font-size:90%;">0.971</span></td>
<td id="S4.T5.2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.5.1" class="ltx_text" style="font-size:90%;">0.939</span></td>
<td id="S4.T5.2.1.5.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.6.1" class="ltx_text" style="font-size:90%;">0.953</span></td>
<td id="S4.T5.2.1.5.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.7.1" class="ltx_text" style="font-size:90%;">0.847</span></td>
<td id="S4.T5.2.1.5.4.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T5.2.1.5.4.8.1" class="ltx_text" style="font-size:90%;">0.934</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Average cosine similarity between the representations of original captions and counterfactual descriptions produced by different text encoders. </figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Similarity of Text Representations.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">We calculate the average cosine similarity between the representations of the original captions and the counterfactual descriptions with different text encoders.
As shown in <a href="#S4.T5" title="In Effects of Vision Encoders. ‣ 4.2 Evaluation Results ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, both the CLIP text encoder and Sentence-BERT produce highly similar sentence representations for samples in the “Sequence” aspect, indicating that the struggle of the evaluated models can partly be explained by the inability of text encoders to recognize the temporal distinction between the captions and the counterfactual descriptions.
We also notice that the CLIP text encoder generally produces higher similarity scores even after it is fine-tuned on video-text data.
This suggests that the ability to identify temporal concepts in natural language may be lost during the image-text pre-training stage and cannot be recovered by fine-tuning on existing video-language datasets.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:422.3pt;height:86.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-183.5pt,37.7pt) scale(0.535098787152227,0.535098787152227) ;">
<table id="S4.T6.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.2.1.1.1" class="ltx_tr">
<th id="S4.T6.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T6.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<td id="S4.T6.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dataset</span></td>
<td id="S4.T6.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Direction</span></td>
<td id="S4.T6.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Intensity</span></td>
<td id="S4.T6.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Sequence</span></td>
<td id="S4.T6.2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Localization</span></td>
<td id="S4.T6.2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Compositionality</span></td>
<td id="S4.T6.2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Type</span></td>
<td id="S4.T6.2.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S4.T6.2.1.1.1.9.1" class="ltx_text" style="font-size:90%;">Avg.</span></td>
</tr>
<tr id="S4.T6.2.1.2.2" class="ltx_tr">
<th id="S4.T6.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T6.2.1.2.2.1.1" class="ltx_text" style="font-size:90%;">VIOLET <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite></span></th>
<td id="S4.T6.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T6.2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">DiDeMo </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T6.2.1.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.3.1" class="ltx_text" style="font-size:90%;">50.9</span></td>
<td id="S4.T6.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.4.1" class="ltx_text" style="font-size:90%;">59.7</span></td>
<td id="S4.T6.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.5.1" class="ltx_text" style="font-size:90%;">55.6</span></td>
<td id="S4.T6.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">61.6</span></td>
<td id="S4.T6.2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.7.1" class="ltx_text" style="font-size:90%;">64.5</span></td>
<td id="S4.T6.2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.8.1" class="ltx_text" style="font-size:90%;">77.7</span></td>
<td id="S4.T6.2.1.2.2.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T6.2.1.2.2.9.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
</tr>
<tr id="S4.T6.2.1.3.3" class="ltx_tr">
<td id="S4.T6.2.1.3.3.1" class="ltx_td ltx_align_center">
<span id="S4.T6.2.1.3.3.1.1" class="ltx_text" style="font-size:90%;">LSMDC </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.3.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a><span id="S4.T6.2.1.3.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.3.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.2</span></td>
<td id="S4.T6.2.1.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62.8</span></td>
<td id="S4.T6.2.1.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.3.3.4.1" class="ltx_text" style="font-size:90%;">61.6</span></td>
<td id="S4.T6.2.1.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.3.3.5.1" class="ltx_text" style="font-size:90%;">60.6</span></td>
<td id="S4.T6.2.1.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.3.3.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.8</span></td>
<td id="S4.T6.2.1.3.3.7" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.3.3.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">78.2</span></td>
<td id="S4.T6.2.1.3.3.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.2.1.3.3.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.7</span></td>
</tr>
<tr id="S4.T6.2.1.4.4" class="ltx_tr">
<td id="S4.T6.2.1.4.4.1" class="ltx_td ltx_align_center">
<span id="S4.T6.2.1.4.4.1.1" class="ltx_text" style="font-size:90%;">YouCook2 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a><span id="S4.T6.2.1.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.4.4.2.1" class="ltx_text" style="font-size:90%;">58.2</span></td>
<td id="S4.T6.2.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.4.4.3.1" class="ltx_text" style="font-size:90%;">60.2</span></td>
<td id="S4.T6.2.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62.9</span></td>
<td id="S4.T6.2.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.4.4.5.1" class="ltx_text" style="font-size:90%;">61.1</span></td>
<td id="S4.T6.2.1.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.4.4.6.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
<td id="S4.T6.2.1.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.4.4.7.1" class="ltx_text" style="font-size:90%;">76.8</span></td>
<td id="S4.T6.2.1.4.4.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.2.1.4.4.8.1" class="ltx_text" style="font-size:90%;">63.5</span></td>
</tr>
<tr id="S4.T6.2.1.5.5" class="ltx_tr">
<th id="S4.T6.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T6.2.1.5.5.1.1" class="ltx_text" style="font-size:90%;">Singularity <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite></span></th>
<td id="S4.T6.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T6.2.1.5.5.2.1" class="ltx_text" style="font-size:90%;">ActivityNet </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.5.5.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a><span id="S4.T6.2.1.5.5.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.3.1" class="ltx_text" style="font-size:90%;">54.0</span></td>
<td id="S4.T6.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.4.1" class="ltx_text" style="font-size:90%;">64.8</span></td>
<td id="S4.T6.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.5.1" class="ltx_text" style="font-size:90%;">50.3</span></td>
<td id="S4.T6.2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.6.1" class="ltx_text" style="font-size:90%;">64.7</span></td>
<td id="S4.T6.2.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.7.1" class="ltx_text" style="font-size:90%;">61.8</span></td>
<td id="S4.T6.2.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.8.1" class="ltx_text" style="font-size:90%;">76.0</span></td>
<td id="S4.T6.2.1.5.5.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T6.2.1.5.5.9.1" class="ltx_text" style="font-size:90%;">61.9</span></td>
</tr>
<tr id="S4.T6.2.1.6.6" class="ltx_tr">
<td id="S4.T6.2.1.6.6.1" class="ltx_td ltx_align_center">
<span id="S4.T6.2.1.6.6.1.1" class="ltx_text" style="font-size:90%;">DiDeMo </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T6.2.1.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.6.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">57.1</span></td>
<td id="S4.T6.2.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.3</span></td>
<td id="S4.T6.2.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">53.6</span></td>
<td id="S4.T6.2.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.6.6.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">67.2</span></td>
<td id="S4.T6.2.1.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.6.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.3</span></td>
<td id="S4.T6.2.1.6.6.7" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.6.6.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">76.9</span></td>
<td id="S4.T6.2.1.6.6.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.2.1.6.6.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.1</span></td>
</tr>
<tr id="S4.T6.2.1.7.7" class="ltx_tr">
<td id="S4.T6.2.1.7.7.1" class="ltx_td ltx_align_center">
<span id="S4.T6.2.1.7.7.1.1" class="ltx_text" style="font-size:90%;">SSv2-label </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a><span id="S4.T6.2.1.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.7.7.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">57.1</span></td>
<td id="S4.T6.2.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.7.7.3.1" class="ltx_text" style="font-size:90%;">65.1</span></td>
<td id="S4.T6.2.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.7.7.4.1" class="ltx_text" style="font-size:90%;">49.7</span></td>
<td id="S4.T6.2.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.7.7.5.1" class="ltx_text" style="font-size:90%;">63.5</span></td>
<td id="S4.T6.2.1.7.7.6" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.7.7.6.1" class="ltx_text" style="font-size:90%;">59.4</span></td>
<td id="S4.T6.2.1.7.7.7" class="ltx_td ltx_align_center"><span id="S4.T6.2.1.7.7.7.1" class="ltx_text" style="font-size:90%;">75.2</span></td>
<td id="S4.T6.2.1.7.7.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.2.1.7.7.8.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
</tr>
<tr id="S4.T6.2.1.8.8" class="ltx_tr">
<th id="S4.T6.2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T6.2.1.8.8.1.1" class="ltx_text" style="font-size:90%;">X-Pool <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite></span></th>
<td id="S4.T6.2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T6.2.1.8.8.2.1" class="ltx_text" style="font-size:90%;">LSMDC </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.8.8.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a><span id="S4.T6.2.1.8.8.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.3.1" class="ltx_text" style="font-size:90%;">60.1</span></td>
<td id="S4.T6.2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.2</span></td>
<td id="S4.T6.2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.5.1" class="ltx_text" style="font-size:90%;">50.3</span></td>
<td id="S4.T6.2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.6.1" class="ltx_text" style="font-size:90%;">66.6</span></td>
<td id="S4.T6.2.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.7.1" class="ltx_text" style="font-size:90%;">59.4</span></td>
<td id="S4.T6.2.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.8.1" class="ltx_text" style="font-size:90%;">77.1</span></td>
<td id="S4.T6.2.1.8.8.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T6.2.1.8.8.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.8</span></td>
</tr>
<tr id="S4.T6.2.1.9.9" class="ltx_tr">
<td id="S4.T6.2.1.9.9.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T6.2.1.9.9.1.1" class="ltx_text" style="font-size:90%;">MSVD </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.2.1.9.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a><span id="S4.T6.2.1.9.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.4</span></td>
<td id="S4.T6.2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.3.1" class="ltx_text" style="font-size:90%;">57.9</span></td>
<td id="S4.T6.2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">51.0</span></td>
<td id="S4.T6.2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">68.3</span></td>
<td id="S4.T6.2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62.1</span></td>
<td id="S4.T6.2.1.9.9.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">78.8</span></td>
<td id="S4.T6.2.1.9.9.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T6.2.1.9.9.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.8</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Accuracy (%) of VIOLET, Singularity, and X-Pool fine-tuned on different video-text datasets. </figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effects of Fine-Tuning Data.</h4>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.1" class="ltx_p">We conduct a comparison between the performance of VidLMs fine-tuned on different downstream datasets.
The results are shown in <a href="#S4.T6" title="In Similarity of Text Representations. ‣ 4.2 Evaluation Results ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.
We find that models fine-tuned on different text-to-video retrieval datasets exhibit different temporal understanding abilities.
For example, DiDeMo tends to elicit higher accuracy on “Localization” and “Compositionality”, while LSMDC contributes to better understanding of “Intensity”.
Also, since SSv2 only depicts single human actions, it brings benefits on the “Direction” aspect but not on “Sequence” understanding, which can be improved by fine-tuning on datasets with longer video duration and dense captions such as YouCook2.
This finding advocates the use of diverse videos and captions in the training process.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Discussions</h3>

<figure id="S4.T7" class="ltx_table">
<div id="S4.T7.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:88.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.3pt,19.0pt) scale(0.699026306776525,0.699026306776525) ;">
<table id="S4.T7.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.1.1.1" class="ltx_tr">
<th id="S4.T7.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T7.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T7.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="S4.T7.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Description</span></th>
<th id="S4.T7.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Direction</span></th>
<th id="S4.T7.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Intensity</span></th>
<th id="S4.T7.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Sequence</span></th>
<th id="S4.T7.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Localization</span></th>
<th id="S4.T7.2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Compositionality</span></th>
<th id="S4.T7.2.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.9.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="S4.T7.2.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.1.1.1.10.1" class="ltx_text" style="font-size:90%;">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.2.1.2.1" class="ltx_tr">
<th id="S4.T7.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T7.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">ALPRO</span></th>
<td id="S4.T7.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T7.2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Zero-shot</span></td>
<td id="S4.T7.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Caption</span></td>
<td id="S4.T7.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.4.1" class="ltx_text" style="font-size:90%;">28.5</span></td>
<td id="S4.T7.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.5.1" class="ltx_text" style="font-size:90%;">48.1</span></td>
<td id="S4.T7.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.6.1" class="ltx_text" style="font-size:90%;">70.3</span></td>
<td id="S4.T7.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.7.1" class="ltx_text" style="font-size:90%;">43.9</span></td>
<td id="S4.T7.2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.8.1" class="ltx_text" style="font-size:90%;">34.6</span></td>
<td id="S4.T7.2.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.9.1" class="ltx_text" style="font-size:90%;">22.9</span></td>
<td id="S4.T7.2.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.2.1.10.1" class="ltx_text" style="font-size:90%;">41.4</span></td>
</tr>
<tr id="S4.T7.2.1.3.2" class="ltx_tr">
<td id="S4.T7.2.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Counterfactual</span></td>
<td id="S4.T7.2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">28.2</span></td>
<td id="S4.T7.2.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.3.1" class="ltx_text" style="font-size:90%;">42.9</span></td>
<td id="S4.T7.2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.4.1" class="ltx_text" style="font-size:90%;">70.0</span></td>
<td id="S4.T7.2.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.5.1" class="ltx_text" style="font-size:90%;">38.4</span></td>
<td id="S4.T7.2.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.6.1" class="ltx_text" style="font-size:90%;">32.6</span></td>
<td id="S4.T7.2.1.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.7.1" class="ltx_text" style="font-size:90%;">12.7</span></td>
<td id="S4.T7.2.1.3.2.8" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.3.2.8.1" class="ltx_text" style="font-size:90%;">37.5</span></td>
</tr>
<tr id="S4.T7.2.1.4.3" class="ltx_tr">
<td id="S4.T7.2.1.4.3.1" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.1.1" class="ltx_text" style="font-size:90%;">All</span></td>
<td id="S4.T7.2.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.2.1" class="ltx_text" style="font-size:90%;">28.3</span></td>
<td id="S4.T7.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.3.1" class="ltx_text" style="font-size:90%;">45.5</span></td>
<td id="S4.T7.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.4.1" class="ltx_text" style="font-size:90%;">70.2</span></td>
<td id="S4.T7.2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.5.1" class="ltx_text" style="font-size:90%;">41.2</span></td>
<td id="S4.T7.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.6.1" class="ltx_text" style="font-size:90%;">33.6</span></td>
<td id="S4.T7.2.1.4.3.7" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.7.1" class="ltx_text" style="font-size:90%;">17.8</span></td>
<td id="S4.T7.2.1.4.3.8" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.4.3.8.1" class="ltx_text" style="font-size:90%;">39.4</span></td>
</tr>
<tr id="S4.T7.2.1.5.4" class="ltx_tr">
<th id="S4.T7.2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T7.2.1.5.4.1.1" class="ltx_text" style="font-size:90%;">CLIP4Clip</span></th>
<td id="S4.T7.2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T7.2.1.5.4.2.1" class="ltx_text" style="font-size:90%;">MSR-VTT</span></td>
<td id="S4.T7.2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.3.1" class="ltx_text" style="font-size:90%;">Caption</span></td>
<td id="S4.T7.2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.4.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
<td id="S4.T7.2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.5.1" class="ltx_text" style="font-size:90%;">73.7</span></td>
<td id="S4.T7.2.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.6.1" class="ltx_text" style="font-size:90%;">90.7</span></td>
<td id="S4.T7.2.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.7.1" class="ltx_text" style="font-size:90%;">69.8</span></td>
<td id="S4.T7.2.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.8.1" class="ltx_text" style="font-size:90%;">65.2</span></td>
<td id="S4.T7.2.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.9.1" class="ltx_text" style="font-size:90%;">48.5</span></td>
<td id="S4.T7.2.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.1.5.4.10.1" class="ltx_text" style="font-size:90%;">66.9</span></td>
</tr>
<tr id="S4.T7.2.1.6.5" class="ltx_tr">
<td id="S4.T7.2.1.6.5.1" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Counterfactual</span></td>
<td id="S4.T7.2.1.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.2.1" class="ltx_text" style="font-size:90%;">47.7</span></td>
<td id="S4.T7.2.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.3.1" class="ltx_text" style="font-size:90%;">66.0</span></td>
<td id="S4.T7.2.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.4.1" class="ltx_text" style="font-size:90%;">88.7</span></td>
<td id="S4.T7.2.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.5.1" class="ltx_text" style="font-size:90%;">60.0</span></td>
<td id="S4.T7.2.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.6.1" class="ltx_text" style="font-size:90%;">60.4</span></td>
<td id="S4.T7.2.1.6.5.7" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.7.1" class="ltx_text" style="font-size:90%;">22.4</span></td>
<td id="S4.T7.2.1.6.5.8" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.6.5.8.1" class="ltx_text" style="font-size:90%;">57.5</span></td>
</tr>
<tr id="S4.T7.2.1.7.6" class="ltx_tr">
<td id="S4.T7.2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.1.1" class="ltx_text" style="font-size:90%;">All</span></td>
<td id="S4.T7.2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.2.1" class="ltx_text" style="font-size:90%;">50.7</span></td>
<td id="S4.T7.2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.3.1" class="ltx_text" style="font-size:90%;">69.9</span></td>
<td id="S4.T7.2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.4.1" class="ltx_text" style="font-size:90%;">89.7</span></td>
<td id="S4.T7.2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.5.1" class="ltx_text" style="font-size:90%;">64.9</span></td>
<td id="S4.T7.2.1.7.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.6.1" class="ltx_text" style="font-size:90%;">62.8</span></td>
<td id="S4.T7.2.1.7.6.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.7.1" class="ltx_text" style="font-size:90%;">35.4</span></td>
<td id="S4.T7.2.1.7.6.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.1.7.6.8.1" class="ltx_text" style="font-size:90%;">62.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Recall@10 of ALPRO and CLIP4Clip on video-to-text retrieval on VITATECS. </figcaption>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Recall on VITATECS. </h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.2" class="ltx_p">Previous work <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> on the challenges of Winoground points out that accuracies based on cosine similarity comparison might be too harsh for the models, and it is possible that they under-perform on Winoground because the image-text pairs are out-of-distribution for them.
This is also a concern for our dataset, so we follow them by calculating the Recall at <math id="S4.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="k&gt;1" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">k</mi><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><gt id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1"></gt><ci id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">k&gt;1</annotation></semantics></math> on the task of video-to-text retrieval on the entire VITATECS dataset for each aspect.
Since a video may have multiple caption-counterfactual pairs in our dataset, we choose <math id="S4.SS3.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1"><eq id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1"></eq><ci id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.2.m2.1c">k=10</annotation></semantics></math> and show the recalls for captions, counterfactuals, and both descriptions in <a href="#S4.T7" title="In 4.3 Discussions ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>.
We observe that for both ALPRO and CLIP4Clip, the recalls of captions and counterfactuals are very close.
This indicates that the models are able to connect the texts with their corresponding videos through the shared static information, but cannot distinguish between the different temporal information in the caption and the counterfactual.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<div id="S4.T8.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:127.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.4pt,-18.6pt) scale(1.41346397135959,1.41346397135959) ;">
<table id="S4.T8.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.2.1.1.1" class="ltx_tr">
<th id="S4.T8.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">POS of replaced words</th>
<th id="S4.T8.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">All</th>
<th id="S4.T8.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Noun</th>
<th id="S4.T8.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Verb</th>
<th id="S4.T8.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Adj.</th>
</tr>
<tr id="S4.T8.2.1.2.2" class="ltx_tr">
<th id="S4.T8.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"># replaced words</th>
<th id="S4.T8.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">1</th>
<th id="S4.T8.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">2</th>
<th id="S4.T8.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">3</th>
<th id="S4.T8.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">1</th>
<th id="S4.T8.2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">1</th>
<th id="S4.T8.2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.2.1.3.1" class="ltx_tr">
<th id="S4.T8.2.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Random</th>
<td id="S4.T8.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">74.5</td>
<td id="S4.T8.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">82.7</td>
<td id="S4.T8.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">90.3</td>
<td id="S4.T8.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">82.1</td>
<td id="S4.T8.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">67.0</td>
<td id="S4.T8.2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">71.3</td>
</tr>
<tr id="S4.T8.2.1.4.2" class="ltx_tr">
<th id="S4.T8.2.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Synonym</th>
<td id="S4.T8.2.1.4.2.2" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T8.2.1.4.2.3" class="ltx_td ltx_align_center">77.2</td>
<td id="S4.T8.2.1.4.2.4" class="ltx_td ltx_align_center">83.3</td>
<td id="S4.T8.2.1.4.2.5" class="ltx_td ltx_align_center">72.1</td>
<td id="S4.T8.2.1.4.2.6" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T8.2.1.4.2.7" class="ltx_td ltx_align_center">67.5</td>
</tr>
<tr id="S4.T8.2.1.5.3" class="ltx_tr">
<th id="S4.T8.2.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">VITATECS (subset)</th>
<td id="S4.T8.2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="6">64.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.3.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S4.T8.4.2" class="ltx_text" style="font-size:90%;">Accuracy (%) of X-CLIP on VITATECS and other counterfactual construction strategies. </span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ablation Study of Counterfactual Design.</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">To verify the design of our counterfactual descriptions, we randomly sample 100 instances from each aspect of VITATECS and apply different modification strategies to the original captions.
Specifically, we randomly choose 1-3 words in the caption and replace them with its synonym or a random word of the same part of speech. We also experiment with different types of words (nouns, verbs, or adjectives) as the target for replacement.
The results are shown in <a href="#S4.T8" title="In Recall on VITATECS. ‣ 4.3 Discussions ‣ 4 Evaluation of Video-Language Models ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a>.
On the one hand, we can conclude that discriminating between the original caption and these altered ones is much easier when we randomly replace the words in the caption, even when only one word is changed.
This margin is greater when we modify the nouns than when we modify the verbs in the captions, which aligns with our observation that current models rely heavily on static clues to make predictions.
This demonstrates that the temporal understanding addressed by our VITATECS is more difficult to solve than simple object or action replacement.
Also, the accuracy of the model rises quickly as we increase the number of replaced words, while our VITATECS maintains its difficulty despite showing greater lingual diversity.
On the other hand, replacing words with their synonyms without contextual information may change their semantics significantly, as evidenced by the relatively high accuracy of models on these counterfactuals compared to VITATECS.
This cautions us against the use of purely lexical methods for counterfactual construction.
Finally, neither of these replacement methods is able to attach fine-grained labels to the resulting sentence, demonstrating the superiority of our counterfactual design.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work aims to address the deficiency of temporal understanding evaluation abilities in existing video-language datasets.
We present a fine-grained characterization of temporal concepts in video descriptions, and introduce a novel dataset that measures the temporal understanding capabilities of VidLMs by their ability to distinguish between the actual description of a video and its temporally modified alternative.
To facilitate dataset construction, we design a human-in-the-loop annotation framework by leveraging LLMs for counterfactual description generation.
Evaluation of state-of-the-art models demonstrates their failure to fully grasp temporal concepts.
We hope our work can provide valuable insight into the future development of video-language understanding research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Bagad et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Piyush Bagad, Makarand Tapaswi, and Cees G. M. Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Test of time: Instilling video-language models with a sense of time.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, abs/2301.02074, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Bain et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Frozen in time: A joint video and image encoder for end-to-end retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, pages 1708–1718, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Bain et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">A clip-hitchhiker’s guide to long video retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib3.10.2" class="ltx_text" style="font-size:90%;">, abs/2205.08508, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Bao et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Hangbo Bao, Li Dong, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Beit: Bert pre-training of image transformers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, abs/2106.08254, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Bertasius et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Is space-time attention all you need for video understanding?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Brown et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, abs/2005.14165, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Buch et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
S. Buch, Cristobal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Revisiting the “video” in video-language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, pages 2907–2917, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Changpinyo et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Soravit Changpinyo, Piyush Kumar Sharma, Nan Ding, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, pages 3557–3567, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.4.4.1" class="ltx_text" style="font-size:90%;">Chen and Dolan [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">
David L. Chen and William B. Dolan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">Collecting highly parallel data for paraphrase evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual Meeting of the Association for Computational Linguistics</em><span id="bib.bib9.10.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, abs/1504.00325, 2015.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Cheng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas Bertasius.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Vindlu: A recipe for effective video-and-language pretraining.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.05051</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Chiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Choi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Jinwoo Choi, Chen Gao, Joseph C.E. Messou, and Jia-Bin Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Why can’t i dance in the mall? learning to mitigate scene bias in action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural Information Processing Systems</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Dai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, abs/2305.06500, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Devlin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib15.10.2" class="ltx_text" style="font-size:90%;">, abs/1810.04805, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Diwan et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Anuj Diwan, Layne Berry, Eunsol Choi, David F. Harwath, and Kyle Mahowald.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Why is winoground hard? investigating failures in visuolinguistic compositionality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Dong et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">A survey for in-context learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, abs/2301.00234, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Dosovitskiy et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib18.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.4.4.1" class="ltx_text" style="font-size:90%;">Evans [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">
Vyvyan Evans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">22 how we conceptualise time: language, meaning and temporal cognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The cognitive linguistics reader</em><span id="bib.bib19.9.2" class="ltx_text" style="font-size:90%;">, page 733, 2004.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Fellbaum [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Christiane D. Fellbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">Wordnet : an electronic lexical database.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Language</em><span id="bib.bib20.9.2" class="ltx_text" style="font-size:90%;">, 76:706, 2000.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Fu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Violet: End-to-end video-language transformers with masked visual-token modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2111.1268</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Fu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">An empirical study of end-to-end video-language transformers with masked visual modeling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, abs/2209.01540, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Gao et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Zijian Gao, Jingyun Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Clip2tv: An empirical study on transformer-based methods for video-text retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, abs/2111.05610, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Gers et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Felix Alexander Gers, Nicol N. Schraudolph, and Jürgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Learning precise timing with lstm recurrent networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">J. Mach. Learn. Res.</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 3:115–143, 2003.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Ghodrati et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Amir Ghodrati, Efstratios Gavves, and Cees G. M. Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Video time: Properties, encoders and evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">British Machine Vision Conference</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.4.4.1" class="ltx_text" style="font-size:90%;">Girdhar and Ramanan [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">
Rohit Girdhar and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">Cater: A diagnostic dataset for compositional actions and temporal reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib26.9.2" class="ltx_text" style="font-size:90%;">, abs/1910.04744, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Girdhar et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Imagebind: One embedding space to bind them all.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, abs/2305.05665, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Gorti et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Satya Krishna Gorti, Noël Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">X-pool: Cross-modal language-video attention for text-video retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, pages 4996–5005, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Goyal et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fründ, Peter N. Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">The “something something” video database for learning and evaluating visual common sense.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Computer Vision (ICCV)</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, pages 5843–5851, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Hendricks et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan C. Russell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Localizing moments in video with temporal language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.4.4.1" class="ltx_text" style="font-size:90%;">Hochreiter and Schmidhuber [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">
Sepp Hochreiter and Jürgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural Computation</em><span id="bib.bib31.9.2" class="ltx_text" style="font-size:90%;">, 9:1735–1780, 1997.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Holtzman et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">The curious case of neural text degeneration.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, abs/1904.09751, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
De-An Huang, Vignesh Ramanathan, Dhruv Kumar Mahajan, Lorenzo Torresani, Manohar Paluri, Li Fei-Fei, and Juan Carlos Niebles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">What makes a video a video: Analyzing temporal information in video understanding models and datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, pages 7366–7375, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu, and Rong Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Self-supervised video representation learning by context and motion decoupling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, pages 13881–13890, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Kay et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Will Kay, João Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Apostol Natsev, Mustafa Suleyman, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">The kinetics human action video dataset.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib35.10.2" class="ltx_text" style="font-size:90%;">, abs/1705.06950, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.4.4.1" class="ltx_text" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</em><span id="bib.bib36.9.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.4.4.1" class="ltx_text" style="font-size:90%;">Klein [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">
Wolfgang Klein.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Time in language</em><span id="bib.bib37.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">routledge, 2013.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Krishna et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib38.10.2" class="ltx_text" style="font-size:90%;">, 123:32–73, 2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Krishna et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Dense-captioning events in videos.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Computer Vision (ICCV)</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">, pages 706–715, 2017.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Lei et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Less is more: Clipbert for video-and-language learning via sparse sampling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Lei et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Jie Lei, Tamara L. Berg, and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Revealing single frame bias for video-and-language learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, abs/2206.03428, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven C. H. Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Align and prompt: Video-and-language pre-training with entity prompts.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, pages 4943–4953, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, 2022a.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, abs/2301.12597, 2023a.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Videochat: Chat-centric video understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, abs/2305.06355, 2023b.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Hero: Hierarchical encoder for video+language omni-representation pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Lavender: Unifying video-language understanding as masked language modeling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, abs/2206.07160, 2022b.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Wanli: Worker and ai collaboration for natural language inference dataset creation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, abs/2201.05955, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib49.10.2" class="ltx_text" style="font-size:90%;">, abs/2304.08485, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Video swin transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib50.10.2" class="ltx_text" style="font-size:90%;">, pages 3192–3201, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Luo et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and Ming Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Univilm: A unified video and language pre-training model for multimodal understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, abs/2002.06353, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Luo et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Clip4clip: An empirical study of clip for end to end video clip retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 508:293–304, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Ma et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Chao Zhang, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">X-clip: End-to-end multi-grained contrastive learning for video-text retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib53.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 30th ACM International Conference on Multimedia</em><span id="bib.bib53.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Meng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Generating training data with language models: Towards zero-shot language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib54.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib54.10.2" class="ltx_text" style="font-size:90%;">, abs/2202.04538, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.4.4.1" class="ltx_text" style="font-size:90%;">Miller [1995]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text" style="font-size:90%;">
George A. Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">Wordnet: A lexical database for english.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Commun. ACM</em><span id="bib.bib55.9.2" class="ltx_text" style="font-size:90%;">, 38:39–41, 1995.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">Introducing ChatGPT, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib57.9.2" class="ltx_text" style="font-size:90%;">, abs/2303.08774, 2023.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Ordonez et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Im2text: Describing images using 1 million captioned photographs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib58.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib58.11.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Park et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Jae Sung Park, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, and Anna Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Exposing the limits of video-text models through contrast sets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib59.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">North American Chapter of the Association for Computational Linguistics</em><span id="bib.bib59.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib60.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib60.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Ramesh et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">Zero-shot text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib61.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib61.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.4.4.1" class="ltx_text" style="font-size:90%;">Reimers and Gurevych [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text" style="font-size:90%;">
Nils Reimers and Iryna Gurevych.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">Sentence-bert: Sentence embeddings using siamese bert-networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib62.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib62.10.3" class="ltx_text" style="font-size:90%;">. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Rohrbach et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">A dataset for movie description.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib63.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib63.10.2" class="ltx_text" style="font-size:90%;">, pages 3202–3212, 2015.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.4.4.1" class="ltx_text" style="font-size:90%;">Schick and Schütze [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.6.1" class="ltx_text" style="font-size:90%;">
Timo Schick and Hinrich Schütze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">Generating datasets with pretrained language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib64.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib64.9.2" class="ltx_text" style="font-size:90%;">, abs/2104.07540, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Sevilla-Lara et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj Goswami, Matt Feiszli, and Lorenzo Torresani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Only time can tell: Discovering temporal data for temporal modeling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib65.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</em><span id="bib.bib65.10.2" class="ltx_text" style="font-size:90%;">, pages 535–544, 2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Sharma et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib66.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual Meeting of the Association for Computational Linguistics</em><span id="bib.bib66.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.4.4.1" class="ltx_text" style="font-size:90%;">StabilityAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.6.1" class="ltx_text" style="font-size:90%;">
StabilityAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">Stablelm: Stability ai language models, 2023.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Sun et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
Quan Sun, Yuxin Fang, Ledell Yu Wu, Xinlong Wang, and Yue Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Eva-clip: Improved training techniques for clip at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib68.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib68.10.2" class="ltx_text" style="font-size:90%;">, abs/2303.15389, 2023.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Thrush et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">Winoground: Probing vision and language models for visio-linguistic compositionality.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib69.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib69.10.2" class="ltx_text" style="font-size:90%;">, pages 5228–5238, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Touvron et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib70.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib70.10.2" class="ltx_text" style="font-size:90%;">, abs/2302.13971, 2023a.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Touvron et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">Llama 2: Open foundation and fine-tuned chat models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib71.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib71.10.2" class="ltx_text" style="font-size:90%;">, abs/2307.09288, 2023b.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text" style="font-size:90%;">Vaswani et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib72.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib72.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Alex Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">All in one: Exploring unified video-language pre-training.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib73.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib73.10.2" class="ltx_text" style="font-size:90%;">, abs/2203.07303, 2022a.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy Jinhua Ma, and Xing Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">Removing the background by adding the background: Towards background robust self-supervised video representation learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib74.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib74.10.2" class="ltx_text" style="font-size:90%;">, pages 11799–11808, 2020.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text" style="font-size:90%;">
Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.8.1" class="ltx_text" style="font-size:90%;">Omnivl: One foundation model for image-language and video-language tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib75.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib75.10.2" class="ltx_text" style="font-size:90%;">, abs/2209.07526, 2022b.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xiansheng Hua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Disentangled representation learning for text-video retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib76.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib76.10.2" class="ltx_text" style="font-size:90%;">, abs/2203.07111, 2022c.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
Xin Eric Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan fang Wang, and William Yang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib77.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em><span id="bib.bib77.10.2" class="ltx_text" style="font-size:90%;">, pages 4580–4590, 2019.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Hoai [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.6.1" class="ltx_text" style="font-size:90%;">
Y. Wang and Minh Hoai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">Pulling actions out of context: Explicit separation for effective combination.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib78.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib78.9.2" class="ltx_text" style="font-size:90%;">, pages 7044–7053, 2018.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">
Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">Internvideo: General video foundation models via generative and discriminative learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib79.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib79.10.2" class="ltx_text" style="font-size:90%;">, abs/2212.03191, 2022d.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.4.4.1" class="ltx_text" style="font-size:90%;">Wei and Zou [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.6.1" class="ltx_text" style="font-size:90%;">
Jason W. Wei and Kai Zou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text" style="font-size:90%;">EDA: easy data augmentation techniques for boosting performance on text classification tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib80.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EMNLP/IJCNLP (1)</em><span id="bib.bib80.10.3" class="ltx_text" style="font-size:90%;">, pages 6381–6387. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text" style="font-size:90%;">Xiao et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text" style="font-size:90%;">
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text" style="font-size:90%;">Next-qa: Next phase of question-answering to explaining temporal actions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib81.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib81.10.2" class="ltx_text" style="font-size:90%;">, pages 9772–9781, 2021.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.5.5.1" class="ltx_text" style="font-size:90%;">Xu et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text" style="font-size:90%;">
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.8.1" class="ltx_text" style="font-size:90%;">Msr-vtt: A large video description dataset for bridging video and language.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib82.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib82.10.2" class="ltx_text" style="font-size:90%;">, pages 5288–5296, 2016.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.5.5.1" class="ltx_text" style="font-size:90%;">Xue et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text" style="font-size:90%;">
Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Rui Song, Houqiang Li, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text" style="font-size:90%;">Clip-vip: Adapting pre-trained image-text model to video-language representation alignment.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib83.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib83.10.2" class="ltx_text" style="font-size:90%;">, abs/2209.06430, 2022.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.5.5.1" class="ltx_text" style="font-size:90%;">Ye et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text" style="font-size:90%;">
Jiacheng Ye, Jiahui Gao, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.8.1" class="ltx_text" style="font-size:90%;">Progen: Progressive zero-shot dataset generation via in-context feedback.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib84.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib84.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.5.5.1" class="ltx_text" style="font-size:90%;">Yi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text" style="font-size:90%;">
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.8.1" class="ltx_text" style="font-size:90%;">Clevrer: Collision events for video representation and reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib85.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib85.10.2" class="ltx_text" style="font-size:90%;">, abs/1910.01442, 2019.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text" style="font-size:90%;">
Youngjae Yu, Jongseok Kim, and Gunhee Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.8.1" class="ltx_text" style="font-size:90%;">A joint sequence fusion model for video question answering and retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib86.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span id="bib.bib86.11.3" class="ltx_text" style="font-size:90%;">, pages 471–487, 2018.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, D. Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text" style="font-size:90%;">Activitynet-qa: A dataset for understanding complex web videos via question answering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib87.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib87.10.2" class="ltx_text" style="font-size:90%;">, abs/1906.02467, 2019.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text" style="font-size:90%;">Yun et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text" style="font-size:90%;">
Sukmin Yun, Jaehyung Kim, Dongyoon Han, Hwanjun Song, Jung-Woo Ha, and Jinwoo Shin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text" style="font-size:90%;">Time is matter: Temporal self-supervision for video transformers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib88.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib88.10.2" class="ltx_text" style="font-size:90%;">, abs/2207.09067, 2022.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.5.5.1" class="ltx_text" style="font-size:90%;">Zellers et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.7.1" class="ltx_text" style="font-size:90%;">
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.8.1" class="ltx_text" style="font-size:90%;">Merlot: Multimodal neural script knowledge models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib89.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib89.10.2" class="ltx_text" style="font-size:90%;">, abs/2106.02636, 2021.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text" style="font-size:90%;">
Hang Zhang, Xin Li, and Lidong Bing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.8.1" class="ltx_text" style="font-size:90%;">Video-llama: An instruction-tuned audio-visual language model for video understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib90.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib90.10.2" class="ltx_text" style="font-size:90%;">, abs/2306.02858, 2023.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text" style="font-size:90%;">
Zhihan Zhang, Zhiyi Yin, Shuhuai Ren, Xinhang Li, and Shicheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.8.1" class="ltx_text" style="font-size:90%;">Dca: Diversified co-attention towards informative live video commenting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib91.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Natural Language Processing and Chinese Computing</em><span id="bib.bib91.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text" style="font-size:90%;">
Luowei Zhou, Chenliang Xu, and Jason J. Corso.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text" style="font-size:90%;">Towards automatic learning of procedures from web instructional videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib92.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI Conference on Artificial Intelligence</em><span id="bib.bib92.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.8.1" class="ltx_text" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib93.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib93.10.2" class="ltx_text" style="font-size:90%;">, abs/2304.10592, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_noindent">
<span id="p1.1" class="ltx_ERROR undefined">\thetitle</span>
<br class="ltx_break ltx_centering">
<p id="p1.2" class="ltx_p ltx_align_center"><span id="p1.2.1" class="ltx_text" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"></span></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>VITATECS</h2>

<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Source Datasets. </h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="A1.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:144%;">The videos and captions in our dataset come from two existing video-text datasets: MSR-VTT </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A1.SS0.SSS0.Px1.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a><span id="A1.SS0.SSS0.Px1.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A1.SS0.SSS0.Px1.p1.1.4" class="ltx_text" style="font-size:144%;"> and VATEX </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A1.SS0.SSS0.Px1.p1.1.5.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a><span id="A1.SS0.SSS0.Px1.p1.1.6.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A1.SS0.SSS0.Px1.p1.1.7" class="ltx_text" style="font-size:144%;">.
MSR-VTT is a video description dataset with 10K web video clips, each annotated with 20 natural sentences. Following common practice, we adopt the split introduced by </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> <span id="A1.SS0.SSS0.Px1.p1.1.8.1.1.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">86</span></a><span id="A1.SS0.SSS0.Px1.p1.1.9.2.2.1" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A1.SS0.SSS0.Px1.p1.1.10" class="ltx_text" style="font-size:144%;"> and use 1,000 video-text pairs as the test set.
VATEX is a large-scale multilingual video description dataset with comprehensive video content and a rich lexicon. Its public test set contains 6,000 videos, each annotated with 10 English descriptions.
Both datasets are open-domain video-text datasets with diverse video content and descriptions.</span></p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Coverage. </h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="A1.SS0.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:144%;">We validate our categorization of temporal concepts by conducting a preliminary study on the temporal information in open-domain video-language datasets.
Specifically, we randomly sample 100 video-text pairs from MSR-VTT and 100 from VATEX, and inspect whether they fall in one of the temporal aspects we define, contain other types of temporal information, or have no temporal information.
As shown in </span><a href="#A1.T9" title="In Coverage. ‣ Appendix A VITATECS ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">9</span></a><span id="A1.SS0.SSS0.Px2.p1.1.2" class="ltx_text" style="font-size:144%;">, the result indicates the high coverage of our temporal aspect categorization, which allows us to assess the ability of VidLMs in a comprehensive and fine-grained manner.</span></p>
</div>
<figure id="A1.T9" class="ltx_table">
<div id="A1.T9.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:84pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.4pt,-15.0pt) scale(1.55530638737647,1.55530638737647) ;">
<table id="A1.T9.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T9.2.1.1.1" class="ltx_tr">
<th id="A1.T9.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A1.T9.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dir.</span></th>
<th id="A1.T9.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Int.</span></th>
<th id="A1.T9.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Seq.</span></th>
<th id="A1.T9.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Loc.</span></th>
<th id="A1.T9.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Comp.</span></th>
<th id="A1.T9.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="A1.T9.2.1.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T9.2.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T9.2.1.2.1" class="ltx_tr">
<th id="A1.T9.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T9.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;"># samples</span></th>
<td id="A1.T9.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">20</span></td>
<td id="A1.T9.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.3.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="A1.T9.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.4.1" class="ltx_text" style="font-size:90%;">39</span></td>
<td id="A1.T9.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.5.1" class="ltx_text" style="font-size:90%;">32</span></td>
<td id="A1.T9.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.6.1" class="ltx_text" style="font-size:90%;">40</span></td>
<td id="A1.T9.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.7.1" class="ltx_text" style="font-size:90%;">56</span></td>
<td id="A1.T9.2.1.2.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="A1.T9.2.1.2.1.8.1" class="ltx_text" style="font-size:90%;">4</span></td>
</tr>
<tr id="A1.T9.2.1.3.2" class="ltx_tr">
<th id="A1.T9.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="A1.T9.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Percentage</span></th>
<td id="A1.T9.2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">10%</span></td>
<td id="A1.T9.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.3.1" class="ltx_text" style="font-size:90%;">5%</span></td>
<td id="A1.T9.2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.4.1" class="ltx_text" style="font-size:90%;">19.5%</span></td>
<td id="A1.T9.2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.5.1" class="ltx_text" style="font-size:90%;">16%</span></td>
<td id="A1.T9.2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.6.1" class="ltx_text" style="font-size:90%;">20%</span></td>
<td id="A1.T9.2.1.3.2.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.7.1" class="ltx_text" style="font-size:90%;">28%</span></td>
<td id="A1.T9.2.1.3.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="A1.T9.2.1.3.2.8.1" class="ltx_text" style="font-size:90%;">2%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Preliminary study on the coverage of our temporal aspect categorization. The total number of samples is 200. Some samples may fall into more than one category, and some samples may contain no temporal information. </figcaption>
</figure>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Diversity. </h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p"><span id="A1.SS0.SSS0.Px3.p1.1.1" class="ltx_text" style="font-size:144%;">One of the principles of our dataset is to be as diverse as possible and include open-domain video-text pairs with comprehensive content to simulate real-world scenarios.
We verify the diversity of our dataset from two perspectives: videos and descriptions.
Since our dataset is built upon existing open-domain video-text datasets, the diversity of videos is naturally guaranteed. </span><a href="#A1.F4" title="In Discussion ‣ Appendix A VITATECS ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a><span id="A1.SS0.SSS0.Px3.p1.1.2" class="ltx_text" style="font-size:144%;"> shows some videos from our dataset and other datasets aimed at temporal understanding evaluation.
Other datasets often use computer-rendered scenes of simple geometric objects like CLEVRER </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A1.SS0.SSS0.Px3.p1.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a><span id="A1.SS0.SSS0.Px3.p1.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A1.SS0.SSS0.Px3.p1.1.5" class="ltx_text" style="font-size:144%;">, or action-centric videos of short duration like Something-Something </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A1.SS0.SSS0.Px3.p1.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a><span id="A1.SS0.SSS0.Px3.p1.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A1.SS0.SSS0.Px3.p1.1.8" class="ltx_text" style="font-size:144%;">, while our dataset contains videos of various topics and styles.
Besides diversity in video content, VITATECS also involves diversified modification patterns of text descriptions owing to the flexibility of LLM generation.
</span><a href="#A1.F5" title="In Discussion ‣ Appendix A VITATECS ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a><span id="A1.SS0.SSS0.Px3.p1.1.9" class="ltx_text" style="font-size:144%;"> illustrates the occurrence of verb pairs in our dataset, where the verbs in the inner circle correspond to the original caption, and the verbs in the outer circle correspond to the counterfactual description.
Our dataset exhibits high diversity, allowing for a more comprehensive and faithful evaluation of VidLMs.</span></p>
</div>
</section>
<section id="A1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Quality Check. </h4>

<div id="A1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px4.p1.1" class="ltx_p"><span id="A1.SS0.SSS0.Px4.p1.1.1" class="ltx_text" style="font-size:144%;">We randomly sample 100 instances for each aspect from the final dataset and manually check if they satisfy our criteria. As shown in </span><a href="#A1.T10" title="In Discussion ‣ Appendix A VITATECS ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a><span id="A1.SS0.SSS0.Px4.p1.1.2" class="ltx_text" style="font-size:144%;">, the six aspects achieve an average pass rate of 94.8%, demonstrating the quality of our generated counterfactuals.</span></p>
</div>
</section>
<section id="A1.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Discussion</h4>

<div id="A1.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px5.p1.1" class="ltx_p"><span id="A1.SS0.SSS0.Px5.p1.1.1" class="ltx_text" style="font-size:144%;">We notice that for some of the instances in VITATECS (mostly in the “Intensity” aspect), it is infeasible to distinguish the caption from the counterfactual solely based on the frame contents.
Some instances may require information about the audio track (</span><em id="A1.SS0.SSS0.Px5.p1.1.2" class="ltx_emph ltx_font_italic" style="font-size:144%;">e.g</em><span id="A1.SS0.SSS0.Px5.p1.1.3" class="ltx_text" style="font-size:144%;">.</span><span id="A1.SS0.SSS0.Px5.p1.1.4" class="ltx_text"></span><span id="A1.SS0.SSS0.Px5.p1.1.5" class="ltx_text" style="font-size:144%;"> </span><span id="A1.SS0.SSS0.Px5.p1.1.6" class="ltx_text ltx_font_italic" style="font-size:144%;">“loud music”</span><span id="A1.SS0.SSS0.Px5.p1.1.7" class="ltx_text" style="font-size:144%;"> v.s. </span><span id="A1.SS0.SSS0.Px5.p1.1.8" class="ltx_text ltx_font_italic" style="font-size:144%;">“serene music”</span><span id="A1.SS0.SSS0.Px5.p1.1.9" class="ltx_text" style="font-size:144%;">) or the frame sampling rate (</span><em id="A1.SS0.SSS0.Px5.p1.1.10" class="ltx_emph ltx_font_italic" style="font-size:144%;">e.g</em><span id="A1.SS0.SSS0.Px5.p1.1.11" class="ltx_text" style="font-size:144%;">.</span><span id="A1.SS0.SSS0.Px5.p1.1.12" class="ltx_text"></span><span id="A1.SS0.SSS0.Px5.p1.1.13" class="ltx_text" style="font-size:144%;"> </span><span id="A1.SS0.SSS0.Px5.p1.1.14" class="ltx_text ltx_font_italic" style="font-size:144%;">“slow motion”</span><span id="A1.SS0.SSS0.Px5.p1.1.15" class="ltx_text" style="font-size:144%;"> v.s. </span><span id="A1.SS0.SSS0.Px5.p1.1.16" class="ltx_text ltx_font_italic" style="font-size:144%;">“normal speed”</span><span id="A1.SS0.SSS0.Px5.p1.1.17" class="ltx_text" style="font-size:144%;">).
However, most of the prevailing models still focus on the visual contents alone, rendering them incapable of distinguishing between these concepts.
Nevertheless, we do not filter out these instances for the following two reasons. First, we believe the perception of audio and time is a vital part of video understanding and such information should be taken account into for future video understanding models. Second, these instances only constitute a minor part of the entire dataset.
In fact, we find that for 51 instances, the modified part includes the word </span><span id="A1.SS0.SSS0.Px5.p1.1.18" class="ltx_text ltx_font_italic" style="font-size:144%;">“loud”</span><span id="A1.SS0.SSS0.Px5.p1.1.19" class="ltx_text" style="font-size:144%;"> or </span><span id="A1.SS0.SSS0.Px5.p1.1.20" class="ltx_text ltx_font_italic" style="font-size:144%;">“quiet”</span><span id="A1.SS0.SSS0.Px5.p1.1.21" class="ltx_text" style="font-size:144%;">, and for 218 instances it includes the word </span><span id="A1.SS0.SSS0.Px5.p1.1.22" class="ltx_text ltx_font_italic" style="font-size:144%;">“fast”</span><span id="A1.SS0.SSS0.Px5.p1.1.23" class="ltx_text" style="font-size:144%;"> or </span><span id="A1.SS0.SSS0.Px5.p1.1.24" class="ltx_text ltx_font_italic" style="font-size:144%;">“slow”</span><span id="A1.SS0.SSS0.Px5.p1.1.25" class="ltx_text" style="font-size:144%;">, most of them belonging to the “Intensity” aspect.
This indicates that the conclusions of our evaluation experiments are still valid.</span></p>
</div>
<figure id="A1.T10" class="ltx_table">
<div id="A1.T10.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:35.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.5pt,0.1pt) scale(0.993263407152147,0.993263407152147) ;">
<table id="A1.T10.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T10.2.1.1.1" class="ltx_tr">
<th id="A1.T10.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="A1.T10.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.2.1" class="ltx_text" style="font-size:144%;">Dir.</span></td>
<td id="A1.T10.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.3.1" class="ltx_text" style="font-size:144%;">Int.</span></td>
<td id="A1.T10.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.4.1" class="ltx_text" style="font-size:144%;">Seq.</span></td>
<td id="A1.T10.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.5.1" class="ltx_text" style="font-size:144%;">Loc.</span></td>
<td id="A1.T10.2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.6.1" class="ltx_text" style="font-size:144%;">Comp.</span></td>
<td id="A1.T10.2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.7.1" class="ltx_text" style="font-size:144%;">Type</span></td>
<td id="A1.T10.2.1.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="A1.T10.2.1.1.1.8.1" class="ltx_text" style="font-size:144%;">Avg.</span></td>
</tr>
<tr id="A1.T10.2.1.2.2" class="ltx_tr">
<th id="A1.T10.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.1.1" class="ltx_text" style="font-size:144%;">Pass Rate</span></th>
<td id="A1.T10.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.2.1" class="ltx_text" style="font-size:144%;">95.0%</span></td>
<td id="A1.T10.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.3.1" class="ltx_text" style="font-size:144%;">96.0%</span></td>
<td id="A1.T10.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.4.1" class="ltx_text" style="font-size:144%;">95.0%</span></td>
<td id="A1.T10.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.5.1" class="ltx_text" style="font-size:144%;">96.0%</span></td>
<td id="A1.T10.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.6.1" class="ltx_text" style="font-size:144%;">87.0%</span></td>
<td id="A1.T10.2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.7.1" class="ltx_text" style="font-size:144%;">100.0%</span></td>
<td id="A1.T10.2.1.2.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T10.2.1.2.2.8.1" class="ltx_text" style="font-size:144%;">94.8%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A1.T10.5.1.1" class="ltx_text" style="font-size:63%;">Table 10</span>: </span><span id="A1.T10.6.2" class="ltx_text" style="font-size:63%;">Pass rates of sampled final data. 100 instances are sampled for each aspect and manually checked whether they satisfy our criteria. </span></figcaption>
</figure>
<figure id="A1.F4" class="ltx_figure"><img src="/html/2311.17404/assets/x2.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="202" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="A1.F4.4.1.1" class="ltx_text" style="font-size:63%;">Figure 4</span>: </span><span id="A1.F4.5.2" class="ltx_text" style="font-size:63%;">Sample videos from our dataset and other datasets targeted at temporal understanding evaluation. Videos in our dataset exhibit higher diversity. </span></figcaption>
</figure>
<figure id="A1.F5" class="ltx_figure"><img src="/html/2311.17404/assets/supplementary/figures/sunburst.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="449" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="A1.F5.4.1.1" class="ltx_text" style="font-size:63%;">Figure 5</span>: </span><span id="A1.F5.5.2" class="ltx_text" style="font-size:63%;">The frequency of verbs in the original captions and the counterfactual descriptions. We only show the top 20 verbs in the inner circle and the top 4 verbs in the outer circle. </span></figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Model Details</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text" style="font-size:144%;">Our evaluation experiments are conducted on three pre-trained VidLMs (VIOLET </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a><span id="A2.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.4" class="ltx_text" style="font-size:144%;">, ALPRO </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.5.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a><span id="A2.p1.1.6.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.7" class="ltx_text" style="font-size:144%;">, Singularity </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.8.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a><span id="A2.p1.1.9.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.10" class="ltx_text" style="font-size:144%;">), three temporally-adapted image-text model based on CLIP </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.11.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a><span id="A2.p1.1.12.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.13" class="ltx_text" style="font-size:144%;"> (CLIP4Clip </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.14.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a><span id="A2.p1.1.15.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.16" class="ltx_text" style="font-size:144%;">, X-Pool </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.17.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="A2.p1.1.18.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.19" class="ltx_text" style="font-size:144%;">, X-CLIP </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.20.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a><span id="A2.p1.1.21.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.22" class="ltx_text" style="font-size:144%;">), and one image-text foundation model (BLIP </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.p1.1.23.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a><span id="A2.p1.1.24.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.p1.1.25" class="ltx_text" style="font-size:144%;">).
We also measure the performance of two recently open-sourced video LLMs.
This section introduces the architectures and training paradigms of the evaluated models.</span></p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Pre-trained VidLMs</h3>

<section id="A2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Model Architectures.</h4>

<div id="A2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px1.p1.1" class="ltx_p"><span id="A2.SS1.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:144%;">All the pre-trained models are composed of a vision encoder, a text encoder, and a cross-modal encoder.
VIOLET initializes the vision encoder from VideoSwin-Base </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px1.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a><span id="A2.SS1.SSS0.Px1.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px1.p1.1.4" class="ltx_text" style="font-size:144%;"> pre-trained on Kinetics-400 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px1.p1.1.5.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="A2.SS1.SSS0.Px1.p1.1.6.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px1.p1.1.7" class="ltx_text" style="font-size:144%;">, and initializes both the text encoder and the cross-modal encoder with BERT-base </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px1.p1.1.8.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a><span id="A2.SS1.SSS0.Px1.p1.1.9.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px1.p1.1.10" class="ltx_text" style="font-size:144%;">.
ALPRO uses a 12-layer pre-trained TimeSformer </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px1.p1.1.11.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a><span id="A2.SS1.SSS0.Px1.p1.1.12.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px1.p1.1.13" class="ltx_text" style="font-size:144%;"> as the vision encoder, and initializes the text encoder using the first six layers of BERT-base and the cross-modal encoder using the last six layers.
Singularity explores using only single frames for video-language pre-training.
Its vision encoder is initialized from BEiT-base </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px1.p1.1.14.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a><span id="A2.SS1.SSS0.Px1.p1.1.15.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px1.p1.1.16" class="ltx_text" style="font-size:144%;">, while its text encoder is initialized from the first nine layers of BERT-base and its cross-modal encoder from the last three layers.
It also has a temporal version, Singularity-temporal, which adds a 2-layer temporal Transformer </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px1.p1.1.17.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a><span id="A2.SS1.SSS0.Px1.p1.1.18.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px1.p1.1.19" class="ltx_text" style="font-size:144%;"> encoder following the vision encoder.</span></p>
</div>
</section>
<section id="A2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Pre-training Objectives.</h4>

<div id="A2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px2.p1.1" class="ltx_p"><span id="A2.SS1.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:144%;">All the models adopt Masked Language Modeling (MLM) and Video-Text Matching (VTM) as part of their pre-training objectives.
MLM randomly masks the input text and predicts the masked tokens based on the video and text context.
VTM is a binary classification task that predicts whether the video and the text description are correctly matched with negative samples generated from the same batch.
Besides these two objectives, VIOLET also designs the Masked Visual-token Modeling (MVM) objective using pre-trained DALL-E </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px2.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a><span id="A2.SS1.SSS0.Px2.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px2.p1.1.4" class="ltx_text" style="font-size:144%;"> to discretize visual tokens.
Both ALPRO and Singularity use Video-Text Contrastive (VTC) loss which aligns the representations from the vision and text encoders.
ALPRO further proposes Prompt Entity Modeling (PEM) which predicts the entities in a video by leveraging a pre-trained prompter to generate pseudo entity labels.</span></p>
</div>
</section>
<section id="A2.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Pre-training Data.</h4>

<div id="A2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px3.p1.1" class="ltx_p"><span id="A2.SS1.SSS0.Px3.p1.1.1" class="ltx_text" style="font-size:144%;">All three models are pre-trained using a combination of video-text data and image-text data.
VIOLET is first pre-trained on YT-Temporal </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">89</span></a><span id="A2.SS1.SSS0.Px3.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.4" class="ltx_text" style="font-size:144%;"> videos with noisy Automatic Speech Recognition (ASR) texts and then on WebVid-2.5M </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.5.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a><span id="A2.SS1.SSS0.Px3.p1.1.6.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.7" class="ltx_text" style="font-size:144%;"> video-text pairs and CC-3M </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.8.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a><span id="A2.SS1.SSS0.Px3.p1.1.9.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.10" class="ltx_text" style="font-size:144%;"> image-text pairs.
ALPRO only utilizes WebVid-2.5M and CC-3M during pre-training.
Singularity has two versions trained on different data.
Singularity-5M is trained on WebVid-2.5M and CC-3M, while Singularity-17M is also trained using image-text pairs from COCO </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.11.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a><span id="A2.SS1.SSS0.Px3.p1.1.12.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.13" class="ltx_text" style="font-size:144%;">, Visual Genome </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.14.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a><span id="A2.SS1.SSS0.Px3.p1.1.15.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.16" class="ltx_text" style="font-size:144%;">, SBU Captions </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.17.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a><span id="A2.SS1.SSS0.Px3.p1.1.18.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.19" class="ltx_text" style="font-size:144%;">, and CC-12M </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS1.SSS0.Px3.p1.1.20.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a><span id="A2.SS1.SSS0.Px3.p1.1.21.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS1.SSS0.Px3.p1.1.22" class="ltx_text" style="font-size:144%;">.
Singularity-temporal further performs a second stage pre-training using WebVid-2.5M based on single-frame pre-trained checkpoints.</span></p>
</div>
</section>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>CLIP-based models</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p"><span id="A2.SS2.p1.1.1" class="ltx_text" style="font-size:144%;">CLIP-based models initialize the vision and text encoder from the pre-trained image-text model CLIP with an extra temporal aggregation module for video modeling.
CLIP4Clip first encodes each video frame into a single vector using the CLIP vision encoder.
The frames are then aggregated using mean pooling (“meanP”), an LSTM </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS2.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a><span id="A2.SS2.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS2.p1.1.4" class="ltx_text" style="font-size:144%;"> (“seqLSTM”), or a 4-layer Transformer encoder initialized from CLIP pre-trained weights (“seqTransf”).
The video-text similarities are calculated with cosine similarity.
CLIP4Clip also experiments with a tight Transformer (“tightTransf”) where the frame and text representations are concatenated and fed into a Transformer to produce the similarity score.
X-Pool alters the aggregation of frames in CLIP4Clip by performing text-conditioned pooling over the frame representations.
It uses a simple cross-attention module to pool the frames and thus is agnostic to the frame order.
X-CLIP modifies the similarity calculation of CLIP4Clip by jointly considering video-sentence, video-word, sentence-frame, and frame-word similarity.
The models are directly trained on downstream tasks without further video-text pre-training.</span></p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Zero-shot image-text models</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p"><span id="A2.SS3.p1.1.1" class="ltx_text" style="font-size:144%;">Besides models trained on video-language data, some pre-trained image-language models have also shown strong generalization capabilities and can perform zero-shot transfer to video-language tasks.
For example, BLIP is able to outperform many VidLMs on text-to-video retrieval and video question answering datasets despite ignoring all temporal information.
We also evaluate BLIP on our dataset in a zero-shot manner and compare its performance with the VidLMs.</span></p>
</div>
<figure id="A2.F6" class="ltx_figure"><img src="/html/2311.17404/assets/supplementary/figures/human_ui.png" id="A2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="A2.F6.4.1.1" class="ltx_text" style="font-size:63%;">Figure 6</span>: </span><span id="A2.F6.5.2" class="ltx_text" style="font-size:63%;">A screenshot of the user interface for the human evaluation experiment. </span></figcaption>
</figure>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Video LLMs</h3>

<div id="A2.SS4.p1" class="ltx_para">
<p id="A2.SS4.p1.1" class="ltx_p"><span id="A2.SS4.p1.1.1" class="ltx_text" style="font-size:144%;">Recently, with the advance of LLMs and chatbots </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>, <a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a><span id="A2.SS4.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.4" class="ltx_text" style="font-size:144%;">, some works aim to endow them with the ability of visual understanding by connecting these LLMs with powerful visual encoders </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.5.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a><span id="A2.SS4.p1.1.6.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.7" class="ltx_text" style="font-size:144%;">.
Two pioneering models that introduce this idea into the video domain are Video-LLaMA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.8.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a><span id="A2.SS4.p1.1.9.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.10" class="ltx_text" style="font-size:144%;"> and VideoChat </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.11.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a><span id="A2.SS4.p1.1.12.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.13" class="ltx_text" style="font-size:144%;">. Both of the video LLMs first encode the video into visual embeddings, concatenate them with the input text embeddings, and feed the sequence into LLMs for auto-regressive generation.
Video-LLaMA first encodes each sampled frame with a frozen ViT-G/14 from EVA-CLIP </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.14.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a><span id="A2.SS4.p1.1.15.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.16" class="ltx_text" style="font-size:144%;"> and a frozen Q-Former from BLIP-2 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.17.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a><span id="A2.SS4.p1.1.18.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.19" class="ltx_text" style="font-size:144%;"> to obtain frame-level embeddings.
It then aggregates them into video-level embeddings with a video Q-Former.
Video-LLaMA also comprises an audio encoder based on ImageBind </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.20.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="A2.SS4.p1.1.21.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.22" class="ltx_text" style="font-size:144%;">.
VideoChat has a similar architecture to Video-LLaMA, except that it uses a GMHRA module </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A2.SS4.p1.1.23.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a><span id="A2.SS4.p1.1.24.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A2.SS4.p1.1.25" class="ltx_text" style="font-size:144%;"> for temporal aggregation instead of a video Q-Former.
Both models are trained on large-scale image-text pairs and video-text pairs, as well as curated multimodal instruction data.</span></p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Implementation Details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p"><span id="A3.p1.1.1" class="ltx_text" style="font-size:144%;">For BLIP and ALPRO, we use the checkpoints available on the LAVIS</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/salesforce/LAVIS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/salesforce/LAVIS</a></span></span></span><span id="A3.p1.1.2" class="ltx_text" style="font-size:144%;"> library for evaluation.
We adapt the code from the original GitHub repository for BLIP </span><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/salesforce/BLIP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/salesforce/BLIP</a></span></span></span><span id="A3.p1.1.3" class="ltx_text" style="font-size:144%;"> to evaluate its performance on video-language tasks.
For VIOLET and Singularity, we use the checkpoints and code released in their respective GitHub repositories </span><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/tsujuifu/pytorch_violet" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tsujuifu/pytorch_violet</a></span></span></span><span id="A3.p1.1.4" class="ltx_text" style="font-size:144%;"> </span><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/jayleicn/singularity" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jayleicn/singularity</a></span></span></span><span id="A3.p1.1.5" class="ltx_text" style="font-size:144%;"> for evaluation.
We report the performance of the temporal version of Singularity-17M in the main experiment of our paper.</span></p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p"><span id="A3.p2.1.1" class="ltx_text" style="font-size:144%;">For the CLIP-based models, since no fine-tuned checkpoints are released, we adopt ViT-B/32 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A3.p2.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="A3.p2.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A3.p2.1.4" class="ltx_text" style="font-size:144%;"> as the backbone of the visual encoder and fine-tune the models on MSR-VTT according to the instructions in their respective codebases.
Since the GitHub repository of X-CLIP</span><span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/xuguohai/X-CLIP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/xuguohai/X-CLIP</a></span></span></span><span id="A3.p2.1.5" class="ltx_text" style="font-size:144%;"> is adapted from that of CLIP4Clip</span><span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/ArrowLuo/CLIP4Clip" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ArrowLuo/CLIP4Clip</a></span></span></span><span id="A3.p2.1.6" class="ltx_text" style="font-size:144%;">, we use the former for fine-tuning and evaluating both models.
We experiment with the “meanP” (non-temporal) and “seqTransf” (temporal) aggregation strategies for both models and report the performance of the temporal version in the main experiment of our paper.
For X-Pool, we fine-tune the model based on the code in its GitHub repository</span><span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/layer6ai-labs/xpool" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/layer6ai-labs/xpool</a></span></span></span><span id="A3.p2.1.7" class="ltx_text" style="font-size:144%;">.
Since the original X-Pool performs text-conditioned pooling directly on the set of all frames without temporal information, we report the performance of this non-temporal version in the main experiment.
To enable the analysis of temporal aggregation modules, we follow CLIP4Clip and X-CLIP, and introduce a temporal version that adds a Transformer module before the text-conditioned pooling with the same initialization strategy.
The fine-tuning of the temporal X-Pool employs the same hyper-parameters as the non-temporal version.
The fine-tuning experiments are conducted on 8 NVIDIA TITAN RTX GPUs.</span></p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p"><span id="A3.p3.1.1" class="ltx_text" style="font-size:144%;">For the video LLMs, we perform zero-shot evaluation directly using the open-sourced models </span><span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://github.com/DAMO-NLP-SG/Video-LLaMA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/DAMO-NLP-SG/Video-LLaMA</a></span></span></span><span id="A3.p3.1.2" class="ltx_text" style="font-size:144%;"> </span><span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://github.com/OpenGVLab/Ask-Anything" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenGVLab/Ask-Anything</a></span></span></span><span id="A3.p3.1.3" class="ltx_text" style="font-size:144%;">.
We use Vicuna-7B-v0 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A3.p3.1.4.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a><span id="A3.p3.1.5.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A3.p3.1.6" class="ltx_text" style="font-size:144%;"> for Video-LLaMA and StableVicuna-13B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A3.p3.1.7.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a><span id="A3.p3.1.8.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A3.p3.1.9" class="ltx_text" style="font-size:144%;"> for VideoChat as the LLMs. We disable the audio input of Video-LLaMA for fair comparison.
The instruction we give the video LLMs is shown in </span><a href="#A3.T11" title="In Appendix C Implementation Details ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a><span id="A3.p3.1.10" class="ltx_text" style="font-size:144%;">, which is similar to the ones we use in human evaluation.
The predictions are generated with temperature </span><math id="A3.p3.1.m1.1" class="ltx_Math" alttext="T=0.2" display="inline"><semantics id="A3.p3.1.m1.1a"><mrow id="A3.p3.1.m1.1.1" xref="A3.p3.1.m1.1.1.cmml"><mi mathsize="144%" id="A3.p3.1.m1.1.1.2" xref="A3.p3.1.m1.1.1.2.cmml">T</mi><mo mathsize="144%" id="A3.p3.1.m1.1.1.1" xref="A3.p3.1.m1.1.1.1.cmml">=</mo><mn mathsize="144%" id="A3.p3.1.m1.1.1.3" xref="A3.p3.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p3.1.m1.1b"><apply id="A3.p3.1.m1.1.1.cmml" xref="A3.p3.1.m1.1.1"><eq id="A3.p3.1.m1.1.1.1.cmml" xref="A3.p3.1.m1.1.1.1"></eq><ci id="A3.p3.1.m1.1.1.2.cmml" xref="A3.p3.1.m1.1.1.2">𝑇</ci><cn type="float" id="A3.p3.1.m1.1.1.3.cmml" xref="A3.p3.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.1.m1.1c">T=0.2</annotation></semantics></math><span id="A3.p3.1.11" class="ltx_text" style="font-size:144%;">.</span></p>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.1" class="ltx_p"><span id="A3.p4.1.1" class="ltx_text" style="font-size:144%;">For human evaluation, the annotators are shown a webpage containing a video and two text descriptions, as shown in </span><a href="#A2.F6" title="In B.3 Zero-shot image-text models ‣ Appendix B Model Details ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span id="A3.p4.1.2" class="ltx_text" style="font-size:144%;">. They are required to choose the caption that best describes the video content.
The differences between the two descriptions are underlined and emphasized in boldface.</span></p>
</div>
<figure id="A3.T11" class="ltx_table"><svg id="A3.T11.pic1" class="ltx_picture ltx_centering" height="163.71" overflow="visible" version="1.1" width="600"><g transform="translate(0,163.71) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 157.81 C 0 161.07 2.64 163.71 5.91 163.71 L 594.09 163.71 C 597.36 163.71 600 161.07 600 157.81 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 157.81 C 1.97 159.98 3.73 161.75 5.91 161.75 L 594.09 161.75 C 596.27 161.75 598.03 159.98 598.03 157.81 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="136.16" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A3.T11.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A3.T11.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A3.T11.pic1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:144%;">System: You are given a short video clip and two sentences. Select the sentence that best describes the content of the video.</span></span>
<span id="A3.T11.pic1.1.1.1.1.1.2" class="ltx_p"><span id="A3.T11.pic1.1.1.1.1.1.2.1" class="ltx_text" style="font-size:144%;">User: [VideoHere] Which of the following best describes the content of the video:</span></span>
<span id="A3.T11.pic1.1.1.1.1.1.3" class="ltx_p"><span id="A3.T11.pic1.1.1.1.1.1.3.1" class="ltx_text" style="font-size:144%;">(A) [description 1]</span></span>
<span id="A3.T11.pic1.1.1.1.1.1.4" class="ltx_p"><span id="A3.T11.pic1.1.1.1.1.1.4.1" class="ltx_text" style="font-size:144%;">(B) [description 2]</span></span>
<span id="A3.T11.pic1.1.1.1.1.1.5" class="ltx_p"><span id="A3.T11.pic1.1.1.1.1.1.5.1" class="ltx_text" style="font-size:144%;">Respond with a single letter (A or B).</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A3.T11.4.1.1" class="ltx_text" style="font-size:63%;">Table 11</span>: </span><span id="A3.T11.5.2" class="ltx_text" style="font-size:63%;">Prompt for video LLM evaluation. </span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Full Evaluation Results</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p"><span id="A4.p1.1.1" class="ltx_text" style="font-size:144%;">We summarize the results of evaluation experiments in </span><a href="#A7.T13" title="In Appendix G Limitations ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">13</span></a><span id="A4.p1.1.2" class="ltx_text" style="font-size:144%;">.</span></p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Details of Dataset Construction</h2>

<section id="A5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Initialization of Exemplar Sets.</h4>

<div id="A5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="A5.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:144%;">For each of the six temporal aspects, we manually annotate 30 caption-counterfactual pairs as the initialization of the positive exemplar set.
We also write 30 negative examples and a total of 21 N/A examples for different aspects. Some examples are shown in </span><a href="#A7.T14" title="In Appendix G Limitations ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">14</span></a><span id="A5.SS0.SSS0.Px1.p1.1.2" class="ltx_text" style="font-size:144%;">.</span></p>
</div>
</section>
<section id="A5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">In-Context Learning Generation.</h4>

<div id="A5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="A5.SS0.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:144%;">For the prompt of in-context learning generation, we design the input template as shown in </span><a href="#A7.T15" title="In Appendix G Limitations ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">15</span></a><span id="A5.SS0.SSS0.Px2.p1.1.2" class="ltx_text" style="font-size:144%;">.
The instruction is determined according to the given temporal aspect as in </span><a href="#A7.T16" title="In Appendix G Limitations ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">16</span></a><span id="A5.SS0.SSS0.Px2.p1.1.3" class="ltx_text" style="font-size:144%;">.
For each input query, we choose six positive exemplars and two N/A exemplars from the specified aspect, and construct the input to ChatGPT with the template, the instruction, the exemplars, and the query.
We sample one candidate for each query using top-p sampling </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A5.SS0.SSS0.Px2.p1.1.4.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a><span id="A5.SS0.SSS0.Px2.p1.1.5.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A5.SS0.SSS0.Px2.p1.1.6" class="ltx_text" style="font-size:144%;"> with p=0.8.</span></p>
</div>
</section>
<section id="A5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Filter Model.</h4>

<div id="A5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px3.p1.3" class="ltx_p"><span id="A5.SS0.SSS0.Px3.p1.3.1" class="ltx_text" style="font-size:144%;">The NLI models are implemented using the Sentence-BERT library</span><span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://www.sbert.net/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sbert.net/</a></span></span></span><span id="A5.SS0.SSS0.Px3.p1.3.2" class="ltx_text" style="font-size:144%;"> and are initialized with the pre-trained </span><span id="A5.SS0.SSS0.Px3.p1.3.3" class="ltx_text ltx_font_typewriter" style="font-size:144%;">nli-deberta-v3-base</span><span id="A5.SS0.SSS0.Px3.p1.3.4" class="ltx_text" style="font-size:144%;"> models.
We use a simple data augmentation technique </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A5.SS0.SSS0.Px3.p1.3.5.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a><span id="A5.SS0.SSS0.Px3.p1.3.6.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A5.SS0.SSS0.Px3.p1.3.7" class="ltx_text" style="font-size:144%;"> during the fine-tuning procedure, </span><em id="A5.SS0.SSS0.Px3.p1.3.8" class="ltx_emph ltx_font_italic" style="font-size:144%;">i.e.</em><span id="A5.SS0.SSS0.Px3.p1.3.9" class="ltx_text" style="font-size:144%;">, randomly choose one word from the sentence and replace it with its synonym based on WordNet </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A5.SS0.SSS0.Px3.p1.3.10.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a><span id="A5.SS0.SSS0.Px3.p1.3.11.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A5.SS0.SSS0.Px3.p1.3.12" class="ltx_text" style="font-size:144%;">.
The filter model is fine-tuned with the Adam </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A5.SS0.SSS0.Px3.p1.3.13.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a><span id="A5.SS0.SSS0.Px3.p1.3.14.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="A5.SS0.SSS0.Px3.p1.3.15" class="ltx_text" style="font-size:144%;"> optimizer with a learning rate of 5e-5.
We adopt a batch size of 16 and fine-tune our filter model on the augmented positive and negative exemplar sets for five epochs.
After the fine-tuning procedure is finished, for each candidate sentence pair </span><math id="A5.SS0.SSS0.Px3.p1.1.m1.2" class="ltx_Math" alttext="(t_{1},t_{2})" display="inline"><semantics id="A5.SS0.SSS0.Px3.p1.1.m1.2a"><mrow id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.3.cmml"><mo maxsize="144%" minsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.3" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.3.cmml">(</mo><msub id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.cmml"><mi mathsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml">t</mi><mn mathsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo mathsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.4" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.3.cmml">,</mo><msub id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.cmml"><mi mathsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.2.cmml">t</mi><mn mathsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.3" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo maxsize="144%" minsize="144%" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.5" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS0.SSS0.Px3.p1.1.m1.2b"><interval closure="open" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.3.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2"><apply id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2">𝑡</ci><cn type="integer" id="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.1.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.2.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.2">𝑡</ci><cn type="integer" id="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.3.cmml" xref="A5.SS0.SSS0.Px3.p1.1.m1.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A5.SS0.SSS0.Px3.p1.1.m1.2c">(t_{1},t_{2})</annotation></semantics></math><span id="A5.SS0.SSS0.Px3.p1.3.16" class="ltx_text" style="font-size:144%;">, we feed both </span><math id="A5.SS0.SSS0.Px3.p1.2.m2.2" class="ltx_Math" alttext="(t_{1},t_{2})" display="inline"><semantics id="A5.SS0.SSS0.Px3.p1.2.m2.2a"><mrow id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml"><mo maxsize="144%" minsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.3" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml">(</mo><msub id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.cmml"><mi mathsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.2" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.2.cmml">t</mi><mn mathsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.3" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.3.cmml">1</mn></msub><mo mathsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.4" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml">,</mo><msub id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.cmml"><mi mathsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.cmml">t</mi><mn mathsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.3" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.3.cmml">2</mn></msub><mo maxsize="144%" minsize="144%" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.5" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS0.SSS0.Px3.p1.2.m2.2b"><interval closure="open" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2"><apply id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.2.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.2">𝑡</ci><cn type="integer" id="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.3.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.3">1</cn></apply><apply id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.1.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2">𝑡</ci><cn type="integer" id="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.3.cmml" xref="A5.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A5.SS0.SSS0.Px3.p1.2.m2.2c">(t_{1},t_{2})</annotation></semantics></math><span id="A5.SS0.SSS0.Px3.p1.3.17" class="ltx_text" style="font-size:144%;"> and </span><math id="A5.SS0.SSS0.Px3.p1.3.m3.2" class="ltx_Math" alttext="(t_{2},t_{1})" display="inline"><semantics id="A5.SS0.SSS0.Px3.p1.3.m3.2a"><mrow id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.3.cmml"><mo maxsize="144%" minsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.3" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.3.cmml">(</mo><msub id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.cmml"><mi mathsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.2" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.2.cmml">t</mi><mn mathsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.3" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.3.cmml">2</mn></msub><mo mathsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.4" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.3.cmml">,</mo><msub id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.cmml"><mi mathsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.2" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.2.cmml">t</mi><mn mathsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.3" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.3.cmml">1</mn></msub><mo maxsize="144%" minsize="144%" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.5" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS0.SSS0.Px3.p1.3.m3.2b"><interval closure="open" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.3.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2"><apply id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.2.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.2">𝑡</ci><cn type="integer" id="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.3.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.3">2</cn></apply><apply id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.1.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.2.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.2">𝑡</ci><cn type="integer" id="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.3.cmml" xref="A5.SS0.SSS0.Px3.p1.3.m3.2.2.2.2.3">1</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A5.SS0.SSS0.Px3.p1.3.m3.2c">(t_{2},t_{1})</annotation></semantics></math><span id="A5.SS0.SSS0.Px3.p1.3.18" class="ltx_text" style="font-size:144%;"> into the NLI model and multiply the corresponding classification probabilities.
We then classify the candidate into one of the six aspects if its score is higher than all the other scores by a threshold of 0.3.
Consequently, only those candidates for which the filter model produces consistent and highly confident scores will be regarded as positive exemplars, which allows us to filter out most of the wrong predictions and maintain the quality of the exemplar sets.</span></p>
</div>
</section>
<section id="A5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Iterative Procedure. </h4>

<div id="A5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px4.p1.1" class="ltx_p"><span id="A5.SS0.SSS0.Px4.p1.1.1" class="ltx_text" style="font-size:144%;">We conduct iterative generation, filtering, and human revision until the performance of the filter model converges on a held-out evaluation set with 100 samples per class.
The filter model converges after three iterations, as shown in Table </span><a href="#A5.T12" title="Table 12 ‣ Iterative Procedure. ‣ Appendix E Details of Dataset Construction ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">12</span></a><span id="A5.SS0.SSS0.Px4.p1.1.2" class="ltx_text" style="font-size:144%;">.
Table </span><a href="#A5.T12" title="Table 12 ‣ Iterative Procedure. ‣ Appendix E Details of Dataset Construction ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">12</span></a><span id="A5.SS0.SSS0.Px4.p1.1.3" class="ltx_text" style="font-size:144%;"> also shows the valid samples generated in each iteration after human revision.
Its increasing number demonstrates the iterative procedure is able to benefit the generation quality.
The number of human labeling needed for each iteration is 554, 523, 477, respectively.</span></p>
</div>
<figure id="A5.T12" class="ltx_table">
<table id="A5.T12.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.T12.2.1.1" class="ltx_tr">
<td id="A5.T12.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A5.T12.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T12.2.1.1.2.1" class="ltx_text" style="font-size:144%;">Iter. 0</span></td>
<td id="A5.T12.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T12.2.1.1.3.1" class="ltx_text" style="font-size:144%;">Iter. 1</span></td>
<td id="A5.T12.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T12.2.1.1.4.1" class="ltx_text" style="font-size:144%;">Iter. 2</span></td>
<td id="A5.T12.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T12.2.1.1.5.1" class="ltx_text" style="font-size:144%;">Iter. 3</span></td>
</tr>
<tr id="A5.T12.2.2.2" class="ltx_tr">
<td id="A5.T12.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.2.2.1.1" class="ltx_text" style="font-size:144%;">Precision</span></td>
<td id="A5.T12.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.2.2.2.1" class="ltx_text" style="font-size:144%;">0.79</span></td>
<td id="A5.T12.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.2.2.3.1" class="ltx_text" style="font-size:144%;">0.83</span></td>
<td id="A5.T12.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.2.2.4.1" class="ltx_text" style="font-size:144%;">0.84</span></td>
<td id="A5.T12.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.2.2.5.1" class="ltx_text" style="font-size:144%;">0.84</span></td>
</tr>
<tr id="A5.T12.2.3.3" class="ltx_tr">
<td id="A5.T12.2.3.3.1" class="ltx_td ltx_align_center"><span id="A5.T12.2.3.3.1.1" class="ltx_text" style="font-size:144%;">Recall</span></td>
<td id="A5.T12.2.3.3.2" class="ltx_td ltx_align_center"><span id="A5.T12.2.3.3.2.1" class="ltx_text" style="font-size:144%;">0.68</span></td>
<td id="A5.T12.2.3.3.3" class="ltx_td ltx_align_center"><span id="A5.T12.2.3.3.3.1" class="ltx_text" style="font-size:144%;">0.79</span></td>
<td id="A5.T12.2.3.3.4" class="ltx_td ltx_align_center"><span id="A5.T12.2.3.3.4.1" class="ltx_text" style="font-size:144%;">0.77</span></td>
<td id="A5.T12.2.3.3.5" class="ltx_td ltx_align_center"><span id="A5.T12.2.3.3.5.1" class="ltx_text" style="font-size:144%;">0.78</span></td>
</tr>
<tr id="A5.T12.2.4.4" class="ltx_tr">
<td id="A5.T12.2.4.4.1" class="ltx_td ltx_align_center"><span id="A5.T12.2.4.4.1.1" class="ltx_text" style="font-size:144%;">F1-score</span></td>
<td id="A5.T12.2.4.4.2" class="ltx_td ltx_align_center"><span id="A5.T12.2.4.4.2.1" class="ltx_text" style="font-size:144%;">0.71</span></td>
<td id="A5.T12.2.4.4.3" class="ltx_td ltx_align_center"><span id="A5.T12.2.4.4.3.1" class="ltx_text" style="font-size:144%;">0.8</span></td>
<td id="A5.T12.2.4.4.4" class="ltx_td ltx_align_center"><span id="A5.T12.2.4.4.4.1" class="ltx_text" style="font-size:144%;">0.78</span></td>
<td id="A5.T12.2.4.4.5" class="ltx_td ltx_align_center"><span id="A5.T12.2.4.4.5.1" class="ltx_text" style="font-size:144%;">0.8</span></td>
</tr>
<tr id="A5.T12.2.5.5" class="ltx_tr">
<td id="A5.T12.2.5.5.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.5.5.1.1" class="ltx_text" style="font-size:144%;">Valid samples</span></td>
<td id="A5.T12.2.5.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.5.5.2.1" class="ltx_text" style="font-size:144%;">-</span></td>
<td id="A5.T12.2.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.5.5.3.1" class="ltx_text" style="font-size:144%;">374</span></td>
<td id="A5.T12.2.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.5.5.4.1" class="ltx_text" style="font-size:144%;">419</span></td>
<td id="A5.T12.2.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T12.2.5.5.5.1" class="ltx_text" style="font-size:144%;">437</span></td>
</tr>
<tr id="A5.T12.2.6.6" class="ltx_tr">
<td id="A5.T12.2.6.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A5.T12.2.6.6.1.1" class="ltx_text" style="font-size:144%;">Exemplar size</span></td>
<td id="A5.T12.2.6.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A5.T12.2.6.6.2.1" class="ltx_text" style="font-size:144%;">231</span></td>
<td id="A5.T12.2.6.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A5.T12.2.6.6.3.1" class="ltx_text" style="font-size:144%;">785</span></td>
<td id="A5.T12.2.6.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A5.T12.2.6.6.4.1" class="ltx_text" style="font-size:144%;">1308</span></td>
<td id="A5.T12.2.6.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A5.T12.2.6.6.5.1" class="ltx_text" style="font-size:144%;">1785</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A5.T12.5.1.1" class="ltx_text" style="font-size:63%;">Table 12</span>: </span><span id="A5.T12.6.2" class="ltx_text" style="font-size:63%;">The first three rows are macro precision, recall, and F1-score of the fine-tuned filter model in each iteration. Iteration 0 corresponds to the filter model fine-tuned using the manually initialized exemplars alone. The fourth row is the number of valid positive samples after human revision in each iteration. The last row is the size of exemplar sets for generation and filtering in each iteration. </span></figcaption>
</figure>
</section>
<section id="A5.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Annotation Instructions. </h4>

<div id="A5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px5.p1.1" class="ltx_p"><span id="A5.SS0.SSS0.Px5.p1.1.1" class="ltx_text" style="font-size:144%;">During the human revision stage, we first give the annotators instructions including the definition of the aspect and typical positive and negative examples, as shown in Table </span><a href="#A7.T17" title="Table 17 ‣ Appendix G Limitations ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">17</span></a><span id="A5.SS0.SSS0.Px5.p1.1.2" class="ltx_text" style="font-size:144%;"> and Table </span><a href="#A7.T18" title="Table 18 ‣ Appendix G Limitations ‣ VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">18</span></a><span id="A5.SS0.SSS0.Px5.p1.1.3" class="ltx_text" style="font-size:144%;">.
Then the annotators are shown the candidate text pairs and need to decide whether the candidate falls in one of the six temporal aspects or should be classified as a negative example.</span></p>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Ethics Consideration &amp; Broader Impact</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p"><span id="A6.p1.1.1" class="ltx_text" style="font-size:144%;">We are aware that texts generated by LLMs may contain toxic and biased data.
To mitigate this issue, we leverage Perspective API </span><span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a target="_blank" href="https://developers.perspectiveapi.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developers.perspectiveapi.com/</a></span></span></span><span id="A6.p1.1.2" class="ltx_text" style="font-size:144%;"> for scoring possible negative impacts of generated sentences, including toxicity, severe toxicity, identity attack, insult, profanity, threat, and sexually explicit content.
Data samples with one of these scores exceeding 0.9 are discarded from our dataset.
It is still possible, though, that a few harmful samples may exist in the final dataset.
Future work that adopts our annotation framework should take similar safety measures to reduce the harm of generated data.</span></p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Limitations</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p"><span id="A7.p1.1.1" class="ltx_text" style="font-size:144%;">First, our categorization of temporal aspects does not completely eliminate the correlation between temporal and static information.
Since current text-to-video generation models still cannot generate sufficiently natural videos, it is unrealistic for us to control for the static scenes and generate counterfactual videos.
As a result, our dataset focuses on manipulating the text description, and our videos are still subject to the correlation between temporal and static information, especially for the “Localization”, “Compositionality” and “Type” aspects.
Nevertheless, by picking out video-text pairs with different types of temporal information and generating temporally counterfactual descriptions, our dataset can offer a fine-grained evaluation of these temporality-related concepts, which are disentangled from static information to different degrees.
This allows us to pinpoint the abilities of VidLMs at our best while covering various types of temporal information, given the limited resources and the current state of video-language research.
Future work may achieve better disentanglement by employing massive human labor or leveraging stronger video generation models.</span></p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p"><span id="A7.p2.1.1" class="ltx_text" style="font-size:144%;">Second, our dataset may contain wrong labels or unnatural counterfactual descriptions, since it is partly generated by language models. However, this should account for only a small proportion of the entire dataset according to the performance of our filter model and the result of quality check.</span></p>
</div>
<div id="A7.p3" class="ltx_para">
<p id="A7.p3.1" class="ltx_p"><span id="A7.p3.1.1" class="ltx_text" style="font-size:144%;">In addition, due to limitations in time and resources, we were not able to conduct a full-scale controlled experiment to investigate the impact of specific model design choices on temporal understanding abilities.
We hope our dataset can promote research in the temporal understanding of VidLMs and leave a more in-depth analysis of architectures for future work.</span></p>
</div>
<figure id="A7.T13" class="ltx_table">
<div id="A7.T13.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:486.9pt;height:571.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-124.7pt,146.4pt) scale(0.661211513867996,0.661211513867996) ;">
<table id="A7.T13.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A7.T13.2.1.1.1" class="ltx_tr">
<td id="A7.T13.2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.1.1" class="ltx_text" style="font-size:144%;">Model</span></td>
<td id="A7.T13.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.2.1" class="ltx_text" style="font-size:144%;">Data</span></td>
<td id="A7.T13.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.3.1" class="ltx_text" style="font-size:144%;">Direction</span></td>
<td id="A7.T13.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.4.1" class="ltx_text" style="font-size:144%;">Intensity</span></td>
<td id="A7.T13.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.5.1" class="ltx_text" style="font-size:144%;">Sequence</span></td>
<td id="A7.T13.2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.6.1" class="ltx_text" style="font-size:144%;">Localization</span></td>
<td id="A7.T13.2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.7.1" class="ltx_text" style="font-size:144%;">Compositionality</span></td>
<td id="A7.T13.2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.8.1" class="ltx_text" style="font-size:144%;">Type</span></td>
<td id="A7.T13.2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T13.2.1.1.1.9.1" class="ltx_text" style="font-size:144%;">Avg.</span></td>
</tr>
<tr id="A7.T13.2.1.2.2" class="ltx_tr">
<td id="A7.T13.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.1.1" class="ltx_text" style="font-size:144%;">BLIP-base</span></td>
<td id="A7.T13.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.3.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.4.1" class="ltx_text" style="font-size:144%;">55.7</span></td>
<td id="A7.T13.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.5.1" class="ltx_text" style="font-size:144%;">56.3</span></td>
<td id="A7.T13.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.6.1" class="ltx_text" style="font-size:144%;">62.6</span></td>
<td id="A7.T13.2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.7.1" class="ltx_text" style="font-size:144%;">59.0</span></td>
<td id="A7.T13.2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.8.1" class="ltx_text" style="font-size:144%;">77.8</span></td>
<td id="A7.T13.2.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.2.2.9.1" class="ltx_text" style="font-size:144%;">60.6</span></td>
</tr>
<tr id="A7.T13.2.1.3.3" class="ltx_tr">
<td id="A7.T13.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.1.1" class="ltx_text" style="font-size:144%;">BLIP-large</span></td>
<td id="A7.T13.2.1.3.3.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.3.3.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.3.1" class="ltx_text" style="font-size:144%;">58.6</span></td>
<td id="A7.T13.2.1.3.3.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.4.1" class="ltx_text" style="font-size:144%;">67.7</span></td>
<td id="A7.T13.2.1.3.3.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.5.1" class="ltx_text" style="font-size:144%;">51.7</span></td>
<td id="A7.T13.2.1.3.3.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.6.1" class="ltx_text" style="font-size:144%;">66.2</span></td>
<td id="A7.T13.2.1.3.3.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.7.1" class="ltx_text" style="font-size:144%;">61.8</span></td>
<td id="A7.T13.2.1.3.3.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.8.1" class="ltx_text" style="font-size:144%;">78.6</span></td>
<td id="A7.T13.2.1.3.3.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.3.3.9.1" class="ltx_text" style="font-size:144%;">64.1</span></td>
</tr>
<tr id="A7.T13.2.1.4.4" class="ltx_tr">
<td id="A7.T13.2.1.4.4.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.1.1" class="ltx_text" style="font-size:144%;">BLIP-CapFilt</span></td>
<td id="A7.T13.2.1.4.4.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.4.4.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.3.1" class="ltx_text" style="font-size:144%;">55.7</span></td>
<td id="A7.T13.2.1.4.4.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.4.1" class="ltx_text" style="font-size:144%;">62.1</span></td>
<td id="A7.T13.2.1.4.4.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.5.1" class="ltx_text" style="font-size:144%;">49.7</span></td>
<td id="A7.T13.2.1.4.4.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.6.1" class="ltx_text" style="font-size:144%;">63.9</span></td>
<td id="A7.T13.2.1.4.4.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.7.1" class="ltx_text" style="font-size:144%;">58.8</span></td>
<td id="A7.T13.2.1.4.4.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.8.1" class="ltx_text" style="font-size:144%;">76.9</span></td>
<td id="A7.T13.2.1.4.4.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.4.4.9.1" class="ltx_text" style="font-size:144%;">61.2</span></td>
</tr>
<tr id="A7.T13.2.1.5.5" class="ltx_tr">
<td id="A7.T13.2.1.5.5.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.1.1" class="ltx_text" style="font-size:144%;">BLIP-base</span></td>
<td id="A7.T13.2.1.5.5.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.2.1" class="ltx_text" style="font-size:144%;">COCO</span></td>
<td id="A7.T13.2.1.5.5.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.3.1" class="ltx_text" style="font-size:144%;">54.0</span></td>
<td id="A7.T13.2.1.5.5.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.4.1" class="ltx_text" style="font-size:144%;">59.3</span></td>
<td id="A7.T13.2.1.5.5.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.5.1" class="ltx_text" style="font-size:144%;">57.0</span></td>
<td id="A7.T13.2.1.5.5.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.6.1" class="ltx_text" style="font-size:144%;">68.1</span></td>
<td id="A7.T13.2.1.5.5.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.7.1" class="ltx_text" style="font-size:144%;">63.7</span></td>
<td id="A7.T13.2.1.5.5.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.8.1" class="ltx_text" style="font-size:144%;">81.1</span></td>
<td id="A7.T13.2.1.5.5.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.5.5.9.1" class="ltx_text" style="font-size:144%;">63.9</span></td>
</tr>
<tr id="A7.T13.2.1.6.6" class="ltx_tr">
<td id="A7.T13.2.1.6.6.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.1.1" class="ltx_text" style="font-size:144%;">BLIP-large</span></td>
<td id="A7.T13.2.1.6.6.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.2.1" class="ltx_text" style="font-size:144%;">COCO</span></td>
<td id="A7.T13.2.1.6.6.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.3.1" class="ltx_text" style="font-size:144%;">55.8</span></td>
<td id="A7.T13.2.1.6.6.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.4.1" class="ltx_text" style="font-size:144%;">54.6</span></td>
<td id="A7.T13.2.1.6.6.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.5.1" class="ltx_text" style="font-size:144%;">53.6</span></td>
<td id="A7.T13.2.1.6.6.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.6.1" class="ltx_text" style="font-size:144%;">69.5</span></td>
<td id="A7.T13.2.1.6.6.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.7.1" class="ltx_text" style="font-size:144%;">66.0</span></td>
<td id="A7.T13.2.1.6.6.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.8.1" class="ltx_text" style="font-size:144%;">82.4</span></td>
<td id="A7.T13.2.1.6.6.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.6.6.9.1" class="ltx_text" style="font-size:144%;">63.6</span></td>
</tr>
<tr id="A7.T13.2.1.7.7" class="ltx_tr">
<td id="A7.T13.2.1.7.7.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.1.1" class="ltx_text" style="font-size:144%;">BLIP-base</span></td>
<td id="A7.T13.2.1.7.7.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.2.1" class="ltx_text" style="font-size:144%;">Flickr</span></td>
<td id="A7.T13.2.1.7.7.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.3.1" class="ltx_text" style="font-size:144%;">53.8</span></td>
<td id="A7.T13.2.1.7.7.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.4.1" class="ltx_text" style="font-size:144%;">51.6</span></td>
<td id="A7.T13.2.1.7.7.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.5.1" class="ltx_text" style="font-size:144%;">58.3</span></td>
<td id="A7.T13.2.1.7.7.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.6.1" class="ltx_text" style="font-size:144%;">67.4</span></td>
<td id="A7.T13.2.1.7.7.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.7.1" class="ltx_text" style="font-size:144%;">64.3</span></td>
<td id="A7.T13.2.1.7.7.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.8.1" class="ltx_text" style="font-size:144%;">81.2</span></td>
<td id="A7.T13.2.1.7.7.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.7.7.9.1" class="ltx_text" style="font-size:144%;">62.8</span></td>
</tr>
<tr id="A7.T13.2.1.8.8" class="ltx_tr">
<td id="A7.T13.2.1.8.8.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.1.1" class="ltx_text" style="font-size:144%;">BLIP-large</span></td>
<td id="A7.T13.2.1.8.8.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.2.1" class="ltx_text" style="font-size:144%;">Flickr</span></td>
<td id="A7.T13.2.1.8.8.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.3.1" class="ltx_text" style="font-size:144%;">55.4</span></td>
<td id="A7.T13.2.1.8.8.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.4.1" class="ltx_text" style="font-size:144%;">56.6</span></td>
<td id="A7.T13.2.1.8.8.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.5.1" class="ltx_text" style="font-size:144%;">51.0</span></td>
<td id="A7.T13.2.1.8.8.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.6.1" class="ltx_text" style="font-size:144%;">68.8</span></td>
<td id="A7.T13.2.1.8.8.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.7.1" class="ltx_text" style="font-size:144%;">66.5</span></td>
<td id="A7.T13.2.1.8.8.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.8.1" class="ltx_text" style="font-size:144%;">82.6</span></td>
<td id="A7.T13.2.1.8.8.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.8.8.9.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
</tr>
<tr id="A7.T13.2.1.9.9" class="ltx_tr">
<td id="A7.T13.2.1.9.9.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.1.1" class="ltx_text" style="font-size:144%;">VIOLET</span></td>
<td id="A7.T13.2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.3.1" class="ltx_text" style="font-size:144%;">49.5</span></td>
<td id="A7.T13.2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.4.1" class="ltx_text" style="font-size:144%;">61.2</span></td>
<td id="A7.T13.2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.5.1" class="ltx_text" style="font-size:144%;">51.7</span></td>
<td id="A7.T13.2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.6.1" class="ltx_text" style="font-size:144%;">60.6</span></td>
<td id="A7.T13.2.1.9.9.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.7.1" class="ltx_text" style="font-size:144%;">60.6</span></td>
<td id="A7.T13.2.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.8.1" class="ltx_text" style="font-size:144%;">75.3</span></td>
<td id="A7.T13.2.1.9.9.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.9.9.9.1" class="ltx_text" style="font-size:144%;">59.8</span></td>
</tr>
<tr id="A7.T13.2.1.10.10" class="ltx_tr">
<td id="A7.T13.2.1.10.10.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.1.1" class="ltx_text" style="font-size:144%;">VIOLET</span></td>
<td id="A7.T13.2.1.10.10.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.2.1" class="ltx_text" style="font-size:144%;">LSMDC</span></td>
<td id="A7.T13.2.1.10.10.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.3.1" class="ltx_text" style="font-size:144%;">54.1</span></td>
<td id="A7.T13.2.1.10.10.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.4.1" class="ltx_text" style="font-size:144%;">66.8</span></td>
<td id="A7.T13.2.1.10.10.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.5.1" class="ltx_text" style="font-size:144%;">60.9</span></td>
<td id="A7.T13.2.1.10.10.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.6.1" class="ltx_text" style="font-size:144%;">60.6</span></td>
<td id="A7.T13.2.1.10.10.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.7.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
<td id="A7.T13.2.1.10.10.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.8.1" class="ltx_text" style="font-size:144%;">75.3</span></td>
<td id="A7.T13.2.1.10.10.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.10.10.9.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
</tr>
<tr id="A7.T13.2.1.11.11" class="ltx_tr">
<td id="A7.T13.2.1.11.11.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.1.1" class="ltx_text" style="font-size:144%;">VIOLET</span></td>
<td id="A7.T13.2.1.11.11.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.2.1" class="ltx_text" style="font-size:144%;">DiDeMo</span></td>
<td id="A7.T13.2.1.11.11.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.3.1" class="ltx_text" style="font-size:144%;">50.9</span></td>
<td id="A7.T13.2.1.11.11.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.4.1" class="ltx_text" style="font-size:144%;">59.7</span></td>
<td id="A7.T13.2.1.11.11.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.5.1" class="ltx_text" style="font-size:144%;">55.6</span></td>
<td id="A7.T13.2.1.11.11.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.6.1" class="ltx_text" style="font-size:144%;">61.6</span></td>
<td id="A7.T13.2.1.11.11.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.7.1" class="ltx_text" style="font-size:144%;">64.5</span></td>
<td id="A7.T13.2.1.11.11.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.8.1" class="ltx_text" style="font-size:144%;">77.7</span></td>
<td id="A7.T13.2.1.11.11.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.11.11.9.1" class="ltx_text" style="font-size:144%;">61.7</span></td>
</tr>
<tr id="A7.T13.2.1.12.12" class="ltx_tr">
<td id="A7.T13.2.1.12.12.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.1.1" class="ltx_text" style="font-size:144%;">VIOLET</span></td>
<td id="A7.T13.2.1.12.12.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.12.12.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.3.1" class="ltx_text" style="font-size:144%;">60.2</span></td>
<td id="A7.T13.2.1.12.12.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.4.1" class="ltx_text" style="font-size:144%;">62.8</span></td>
<td id="A7.T13.2.1.12.12.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.5.1" class="ltx_text" style="font-size:144%;">61.6</span></td>
<td id="A7.T13.2.1.12.12.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.6.1" class="ltx_text" style="font-size:144%;">60.6</span></td>
<td id="A7.T13.2.1.12.12.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.7.1" class="ltx_text" style="font-size:144%;">64.8</span></td>
<td id="A7.T13.2.1.12.12.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.8.1" class="ltx_text" style="font-size:144%;">78.2</span></td>
<td id="A7.T13.2.1.12.12.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.12.12.9.1" class="ltx_text" style="font-size:144%;">64.7</span></td>
</tr>
<tr id="A7.T13.2.1.13.13" class="ltx_tr">
<td id="A7.T13.2.1.13.13.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.1.1" class="ltx_text" style="font-size:144%;">VIOLET</span></td>
<td id="A7.T13.2.1.13.13.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.2.1" class="ltx_text" style="font-size:144%;">YouCook2</span></td>
<td id="A7.T13.2.1.13.13.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.3.1" class="ltx_text" style="font-size:144%;">58.2</span></td>
<td id="A7.T13.2.1.13.13.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.4.1" class="ltx_text" style="font-size:144%;">60.2</span></td>
<td id="A7.T13.2.1.13.13.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.5.1" class="ltx_text" style="font-size:144%;">62.9</span></td>
<td id="A7.T13.2.1.13.13.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.6.1" class="ltx_text" style="font-size:144%;">61.1</span></td>
<td id="A7.T13.2.1.13.13.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.7.1" class="ltx_text" style="font-size:144%;">61.7</span></td>
<td id="A7.T13.2.1.13.13.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.8.1" class="ltx_text" style="font-size:144%;">76.8</span></td>
<td id="A7.T13.2.1.13.13.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.13.13.9.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
</tr>
<tr id="A7.T13.2.1.14.14" class="ltx_tr">
<td id="A7.T13.2.1.14.14.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.1.1" class="ltx_text" style="font-size:144%;">ALPRO</span></td>
<td id="A7.T13.2.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.3.1" class="ltx_text" style="font-size:144%;">50.3</span></td>
<td id="A7.T13.2.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.4.1" class="ltx_text" style="font-size:144%;">66.0</span></td>
<td id="A7.T13.2.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.5.1" class="ltx_text" style="font-size:144%;">44.4</span></td>
<td id="A7.T13.2.1.14.14.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.6.1" class="ltx_text" style="font-size:144%;">61.3</span></td>
<td id="A7.T13.2.1.14.14.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.7.1" class="ltx_text" style="font-size:144%;">56.5</span></td>
<td id="A7.T13.2.1.14.14.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.8.1" class="ltx_text" style="font-size:144%;">72.2</span></td>
<td id="A7.T13.2.1.14.14.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.14.14.9.1" class="ltx_text" style="font-size:144%;">58.4</span></td>
</tr>
<tr id="A7.T13.2.1.15.15" class="ltx_tr">
<td id="A7.T13.2.1.15.15.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.1.1" class="ltx_text" style="font-size:144%;">ALPRO</span></td>
<td id="A7.T13.2.1.15.15.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.15.15.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.3.1" class="ltx_text" style="font-size:144%;">55.4</span></td>
<td id="A7.T13.2.1.15.15.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.4.1" class="ltx_text" style="font-size:144%;">56.0</span></td>
<td id="A7.T13.2.1.15.15.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.5.1" class="ltx_text" style="font-size:144%;">45.7</span></td>
<td id="A7.T13.2.1.15.15.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.6.1" class="ltx_text" style="font-size:144%;">59.2</span></td>
<td id="A7.T13.2.1.15.15.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.7.1" class="ltx_text" style="font-size:144%;">58.6</span></td>
<td id="A7.T13.2.1.15.15.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.8.1" class="ltx_text" style="font-size:144%;">74.5</span></td>
<td id="A7.T13.2.1.15.15.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.15.15.9.1" class="ltx_text" style="font-size:144%;">58.2</span></td>
</tr>
<tr id="A7.T13.2.1.16.16" class="ltx_tr">
<td id="A7.T13.2.1.16.16.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.1.1" class="ltx_text" style="font-size:144%;">ALPRO</span></td>
<td id="A7.T13.2.1.16.16.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.2.1" class="ltx_text" style="font-size:144%;">DiDeMo</span></td>
<td id="A7.T13.2.1.16.16.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.3.1" class="ltx_text" style="font-size:144%;">52.8</span></td>
<td id="A7.T13.2.1.16.16.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.4.1" class="ltx_text" style="font-size:144%;">57.4</span></td>
<td id="A7.T13.2.1.16.16.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.5.1" class="ltx_text" style="font-size:144%;">45.7</span></td>
<td id="A7.T13.2.1.16.16.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.6.1" class="ltx_text" style="font-size:144%;">60.6</span></td>
<td id="A7.T13.2.1.16.16.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.7.1" class="ltx_text" style="font-size:144%;">59.6</span></td>
<td id="A7.T13.2.1.16.16.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.8.1" class="ltx_text" style="font-size:144%;">74.4</span></td>
<td id="A7.T13.2.1.16.16.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.16.16.9.1" class="ltx_text" style="font-size:144%;">58.4</span></td>
</tr>
<tr id="A7.T13.2.1.17.17" class="ltx_tr">
<td id="A7.T13.2.1.17.17.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m</span></td>
<td id="A7.T13.2.1.17.17.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.17.17.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.3.1" class="ltx_text" style="font-size:144%;">53.9</span></td>
<td id="A7.T13.2.1.17.17.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.4.1" class="ltx_text" style="font-size:144%;">52.0</span></td>
<td id="A7.T13.2.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.5.1" class="ltx_text" style="font-size:144%;">47.7</span></td>
<td id="A7.T13.2.1.17.17.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.6.1" class="ltx_text" style="font-size:144%;">55.4</span></td>
<td id="A7.T13.2.1.17.17.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.7.1" class="ltx_text" style="font-size:144%;">53.5</span></td>
<td id="A7.T13.2.1.17.17.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.8.1" class="ltx_text" style="font-size:144%;">51.1</span></td>
<td id="A7.T13.2.1.17.17.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.17.17.9.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
</tr>
<tr id="A7.T13.2.1.18.18" class="ltx_tr">
<td id="A7.T13.2.1.18.18.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m</span></td>
<td id="A7.T13.2.1.18.18.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.2.1" class="ltx_text" style="font-size:144%;">ActivityNet</span></td>
<td id="A7.T13.2.1.18.18.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.3.1" class="ltx_text" style="font-size:144%;">55.2</span></td>
<td id="A7.T13.2.1.18.18.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.4.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
<td id="A7.T13.2.1.18.18.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.5.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.18.18.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.6.1" class="ltx_text" style="font-size:144%;">65.7</span></td>
<td id="A7.T13.2.1.18.18.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.7.1" class="ltx_text" style="font-size:144%;">64.4</span></td>
<td id="A7.T13.2.1.18.18.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.8.1" class="ltx_text" style="font-size:144%;">76.7</span></td>
<td id="A7.T13.2.1.18.18.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.18.18.9.1" class="ltx_text" style="font-size:144%;">63.0</span></td>
</tr>
<tr id="A7.T13.2.1.19.19" class="ltx_tr">
<td id="A7.T13.2.1.19.19.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m</span></td>
<td id="A7.T13.2.1.19.19.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.19.19.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.3.1" class="ltx_text" style="font-size:144%;">54.9</span></td>
<td id="A7.T13.2.1.19.19.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.4.1" class="ltx_text" style="font-size:144%;">57.4</span></td>
<td id="A7.T13.2.1.19.19.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.5.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.19.19.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.6.1" class="ltx_text" style="font-size:144%;">65.0</span></td>
<td id="A7.T13.2.1.19.19.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.7.1" class="ltx_text" style="font-size:144%;">65.5</span></td>
<td id="A7.T13.2.1.19.19.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.8.1" class="ltx_text" style="font-size:144%;">77.1</span></td>
<td id="A7.T13.2.1.19.19.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.19.19.9.1" class="ltx_text" style="font-size:144%;">62.0</span></td>
</tr>
<tr id="A7.T13.2.1.20.20" class="ltx_tr">
<td id="A7.T13.2.1.20.20.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m</span></td>
<td id="A7.T13.2.1.20.20.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.2.1" class="ltx_text" style="font-size:144%;">DiDeMo</span></td>
<td id="A7.T13.2.1.20.20.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.3.1" class="ltx_text" style="font-size:144%;">57.6</span></td>
<td id="A7.T13.2.1.20.20.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.4.1" class="ltx_text" style="font-size:144%;">66.6</span></td>
<td id="A7.T13.2.1.20.20.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.5.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.20.20.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.6.1" class="ltx_text" style="font-size:144%;">65.9</span></td>
<td id="A7.T13.2.1.20.20.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.7.1" class="ltx_text" style="font-size:144%;">66.9</span></td>
<td id="A7.T13.2.1.20.20.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.8.1" class="ltx_text" style="font-size:144%;">77.8</span></td>
<td id="A7.T13.2.1.20.20.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.20.20.9.1" class="ltx_text" style="font-size:144%;">64.5</span></td>
</tr>
<tr id="A7.T13.2.1.21.21" class="ltx_tr">
<td id="A7.T13.2.1.21.21.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m</span></td>
<td id="A7.T13.2.1.21.21.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.21.21.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.3.1" class="ltx_text" style="font-size:144%;">53.1</span></td>
<td id="A7.T13.2.1.21.21.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.4.1" class="ltx_text" style="font-size:144%;">57.5</span></td>
<td id="A7.T13.2.1.21.21.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.5.1" class="ltx_text" style="font-size:144%;">49.7</span></td>
<td id="A7.T13.2.1.21.21.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.6.1" class="ltx_text" style="font-size:144%;">54.7</span></td>
<td id="A7.T13.2.1.21.21.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.7.1" class="ltx_text" style="font-size:144%;">51.1</span></td>
<td id="A7.T13.2.1.21.21.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.8.1" class="ltx_text" style="font-size:144%;">50.8</span></td>
<td id="A7.T13.2.1.21.21.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.21.21.9.1" class="ltx_text" style="font-size:144%;">52.8</span></td>
</tr>
<tr id="A7.T13.2.1.22.22" class="ltx_tr">
<td id="A7.T13.2.1.22.22.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m</span></td>
<td id="A7.T13.2.1.22.22.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.2.1" class="ltx_text" style="font-size:144%;">ActivityNet</span></td>
<td id="A7.T13.2.1.22.22.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.3.1" class="ltx_text" style="font-size:144%;">56.5</span></td>
<td id="A7.T13.2.1.22.22.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.4.1" class="ltx_text" style="font-size:144%;">60.2</span></td>
<td id="A7.T13.2.1.22.22.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.5.1" class="ltx_text" style="font-size:144%;">53.6</span></td>
<td id="A7.T13.2.1.22.22.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.6.1" class="ltx_text" style="font-size:144%;">65.1</span></td>
<td id="A7.T13.2.1.22.22.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.7.1" class="ltx_text" style="font-size:144%;">63.4</span></td>
<td id="A7.T13.2.1.22.22.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.8.1" class="ltx_text" style="font-size:144%;">75.2</span></td>
<td id="A7.T13.2.1.22.22.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.22.22.9.1" class="ltx_text" style="font-size:144%;">62.3</span></td>
</tr>
<tr id="A7.T13.2.1.23.23" class="ltx_tr">
<td id="A7.T13.2.1.23.23.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m</span></td>
<td id="A7.T13.2.1.23.23.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.23.23.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.3.1" class="ltx_text" style="font-size:144%;">57.5</span></td>
<td id="A7.T13.2.1.23.23.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.4.1" class="ltx_text" style="font-size:144%;">57.8</span></td>
<td id="A7.T13.2.1.23.23.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.5.1" class="ltx_text" style="font-size:144%;">54.3</span></td>
<td id="A7.T13.2.1.23.23.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.6.1" class="ltx_text" style="font-size:144%;">61.9</span></td>
<td id="A7.T13.2.1.23.23.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.7.1" class="ltx_text" style="font-size:144%;">61.2</span></td>
<td id="A7.T13.2.1.23.23.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.8.1" class="ltx_text" style="font-size:144%;">75.4</span></td>
<td id="A7.T13.2.1.23.23.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.23.23.9.1" class="ltx_text" style="font-size:144%;">61.4</span></td>
</tr>
<tr id="A7.T13.2.1.24.24" class="ltx_tr">
<td id="A7.T13.2.1.24.24.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m</span></td>
<td id="A7.T13.2.1.24.24.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.2.1" class="ltx_text" style="font-size:144%;">DiDeMo</span></td>
<td id="A7.T13.2.1.24.24.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.3.1" class="ltx_text" style="font-size:144%;">55.1</span></td>
<td id="A7.T13.2.1.24.24.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.4.1" class="ltx_text" style="font-size:144%;">62.6</span></td>
<td id="A7.T13.2.1.24.24.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.5.1" class="ltx_text" style="font-size:144%;">50.3</span></td>
<td id="A7.T13.2.1.24.24.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.6.1" class="ltx_text" style="font-size:144%;">65.0</span></td>
<td id="A7.T13.2.1.24.24.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.7.1" class="ltx_text" style="font-size:144%;">62.3</span></td>
<td id="A7.T13.2.1.24.24.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.8.1" class="ltx_text" style="font-size:144%;">74.5</span></td>
<td id="A7.T13.2.1.24.24.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.24.24.9.1" class="ltx_text" style="font-size:144%;">61.6</span></td>
</tr>
<tr id="A7.T13.2.1.25.25" class="ltx_tr">
<td id="A7.T13.2.1.25.25.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m</span></td>
<td id="A7.T13.2.1.25.25.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.2.1" class="ltx_text" style="font-size:144%;">SSv2-label</span></td>
<td id="A7.T13.2.1.25.25.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.3.1" class="ltx_text" style="font-size:144%;">59.5</span></td>
<td id="A7.T13.2.1.25.25.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.4.1" class="ltx_text" style="font-size:144%;">68.5</span></td>
<td id="A7.T13.2.1.25.25.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.5.1" class="ltx_text" style="font-size:144%;">49.7</span></td>
<td id="A7.T13.2.1.25.25.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.6.1" class="ltx_text" style="font-size:144%;">62.3</span></td>
<td id="A7.T13.2.1.25.25.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.7.1" class="ltx_text" style="font-size:144%;">60.0</span></td>
<td id="A7.T13.2.1.25.25.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.8.1" class="ltx_text" style="font-size:144%;">74.4</span></td>
<td id="A7.T13.2.1.25.25.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.25.25.9.1" class="ltx_text" style="font-size:144%;">62.4</span></td>
</tr>
<tr id="A7.T13.2.1.26.26" class="ltx_tr">
<td id="A7.T13.2.1.26.26.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m-t</span></td>
<td id="A7.T13.2.1.26.26.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.26.26.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.3.1" class="ltx_text" style="font-size:144%;">53.5</span></td>
<td id="A7.T13.2.1.26.26.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.4.1" class="ltx_text" style="font-size:144%;">52.0</span></td>
<td id="A7.T13.2.1.26.26.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.5.1" class="ltx_text" style="font-size:144%;">48.3</span></td>
<td id="A7.T13.2.1.26.26.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.6.1" class="ltx_text" style="font-size:144%;">53.9</span></td>
<td id="A7.T13.2.1.26.26.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.7.1" class="ltx_text" style="font-size:144%;">53.6</span></td>
<td id="A7.T13.2.1.26.26.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.8.1" class="ltx_text" style="font-size:144%;">51.4</span></td>
<td id="A7.T13.2.1.26.26.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.26.26.9.1" class="ltx_text" style="font-size:144%;">52.1</span></td>
</tr>
<tr id="A7.T13.2.1.27.27" class="ltx_tr">
<td id="A7.T13.2.1.27.27.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m-t</span></td>
<td id="A7.T13.2.1.27.27.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.2.1" class="ltx_text" style="font-size:144%;">ActivityNet</span></td>
<td id="A7.T13.2.1.27.27.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.3.1" class="ltx_text" style="font-size:144%;">54.0</span></td>
<td id="A7.T13.2.1.27.27.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.4.1" class="ltx_text" style="font-size:144%;">64.8</span></td>
<td id="A7.T13.2.1.27.27.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.5.1" class="ltx_text" style="font-size:144%;">50.3</span></td>
<td id="A7.T13.2.1.27.27.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.6.1" class="ltx_text" style="font-size:144%;">64.7</span></td>
<td id="A7.T13.2.1.27.27.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.7.1" class="ltx_text" style="font-size:144%;">61.8</span></td>
<td id="A7.T13.2.1.27.27.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.8.1" class="ltx_text" style="font-size:144%;">76.0</span></td>
<td id="A7.T13.2.1.27.27.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.27.27.9.1" class="ltx_text" style="font-size:144%;">61.9</span></td>
</tr>
<tr id="A7.T13.2.1.28.28" class="ltx_tr">
<td id="A7.T13.2.1.28.28.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m-t</span></td>
<td id="A7.T13.2.1.28.28.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.28.28.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.3.1" class="ltx_text" style="font-size:144%;">54.7</span></td>
<td id="A7.T13.2.1.28.28.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.4.1" class="ltx_text" style="font-size:144%;">61.7</span></td>
<td id="A7.T13.2.1.28.28.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.5.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.28.28.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.6.1" class="ltx_text" style="font-size:144%;">63.0</span></td>
<td id="A7.T13.2.1.28.28.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.7.1" class="ltx_text" style="font-size:144%;">65.5</span></td>
<td id="A7.T13.2.1.28.28.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.8.1" class="ltx_text" style="font-size:144%;">77.4</span></td>
<td id="A7.T13.2.1.28.28.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.28.28.9.1" class="ltx_text" style="font-size:144%;">62.4</span></td>
</tr>
<tr id="A7.T13.2.1.29.29" class="ltx_tr">
<td id="A7.T13.2.1.29.29.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m-t</span></td>
<td id="A7.T13.2.1.29.29.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.2.1" class="ltx_text" style="font-size:144%;">DiDeMo</span></td>
<td id="A7.T13.2.1.29.29.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.3.1" class="ltx_text" style="font-size:144%;">57.1</span></td>
<td id="A7.T13.2.1.29.29.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.4.1" class="ltx_text" style="font-size:144%;">65.3</span></td>
<td id="A7.T13.2.1.29.29.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.5.1" class="ltx_text" style="font-size:144%;">53.6</span></td>
<td id="A7.T13.2.1.29.29.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.6.1" class="ltx_text" style="font-size:144%;">67.2</span></td>
<td id="A7.T13.2.1.29.29.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.7.1" class="ltx_text" style="font-size:144%;">64.3</span></td>
<td id="A7.T13.2.1.29.29.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.8.1" class="ltx_text" style="font-size:144%;">76.9</span></td>
<td id="A7.T13.2.1.29.29.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.29.29.9.1" class="ltx_text" style="font-size:144%;">64.1</span></td>
</tr>
<tr id="A7.T13.2.1.30.30" class="ltx_tr">
<td id="A7.T13.2.1.30.30.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.1.1" class="ltx_text" style="font-size:144%;">Singularity-17m-t</span></td>
<td id="A7.T13.2.1.30.30.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.2.1" class="ltx_text" style="font-size:144%;">SSv2-label</span></td>
<td id="A7.T13.2.1.30.30.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.3.1" class="ltx_text" style="font-size:144%;">57.1</span></td>
<td id="A7.T13.2.1.30.30.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.4.1" class="ltx_text" style="font-size:144%;">65.1</span></td>
<td id="A7.T13.2.1.30.30.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.5.1" class="ltx_text" style="font-size:144%;">49.7</span></td>
<td id="A7.T13.2.1.30.30.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.6.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
<td id="A7.T13.2.1.30.30.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.7.1" class="ltx_text" style="font-size:144%;">59.4</span></td>
<td id="A7.T13.2.1.30.30.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.8.1" class="ltx_text" style="font-size:144%;">75.2</span></td>
<td id="A7.T13.2.1.30.30.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.30.30.9.1" class="ltx_text" style="font-size:144%;">61.7</span></td>
</tr>
<tr id="A7.T13.2.1.31.31" class="ltx_tr">
<td id="A7.T13.2.1.31.31.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m-t</span></td>
<td id="A7.T13.2.1.31.31.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.31.31.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.3.1" class="ltx_text" style="font-size:144%;">52.4</span></td>
<td id="A7.T13.2.1.31.31.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.4.1" class="ltx_text" style="font-size:144%;">54.7</span></td>
<td id="A7.T13.2.1.31.31.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.5.1" class="ltx_text" style="font-size:144%;">47.0</span></td>
<td id="A7.T13.2.1.31.31.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.6.1" class="ltx_text" style="font-size:144%;">55.7</span></td>
<td id="A7.T13.2.1.31.31.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.7.1" class="ltx_text" style="font-size:144%;">52.1</span></td>
<td id="A7.T13.2.1.31.31.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.8.1" class="ltx_text" style="font-size:144%;">50.8</span></td>
<td id="A7.T13.2.1.31.31.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.31.31.9.1" class="ltx_text" style="font-size:144%;">52.1</span></td>
</tr>
<tr id="A7.T13.2.1.32.32" class="ltx_tr">
<td id="A7.T13.2.1.32.32.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m-t</span></td>
<td id="A7.T13.2.1.32.32.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.2.1" class="ltx_text" style="font-size:144%;">ActivityNet</span></td>
<td id="A7.T13.2.1.32.32.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.3.1" class="ltx_text" style="font-size:144%;">58.5</span></td>
<td id="A7.T13.2.1.32.32.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.4.1" class="ltx_text" style="font-size:144%;">64.8</span></td>
<td id="A7.T13.2.1.32.32.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.5.1" class="ltx_text" style="font-size:144%;">50.3</span></td>
<td id="A7.T13.2.1.32.32.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.6.1" class="ltx_text" style="font-size:144%;">65.8</span></td>
<td id="A7.T13.2.1.32.32.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.7.1" class="ltx_text" style="font-size:144%;">62.3</span></td>
<td id="A7.T13.2.1.32.32.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.8.1" class="ltx_text" style="font-size:144%;">75.8</span></td>
<td id="A7.T13.2.1.32.32.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.32.32.9.1" class="ltx_text" style="font-size:144%;">62.9</span></td>
</tr>
<tr id="A7.T13.2.1.33.33" class="ltx_tr">
<td id="A7.T13.2.1.33.33.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m-t</span></td>
<td id="A7.T13.2.1.33.33.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.33.33.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.3.1" class="ltx_text" style="font-size:144%;">57.5</span></td>
<td id="A7.T13.2.1.33.33.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.4.1" class="ltx_text" style="font-size:144%;">63.0</span></td>
<td id="A7.T13.2.1.33.33.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.5.1" class="ltx_text" style="font-size:144%;">51.0</span></td>
<td id="A7.T13.2.1.33.33.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.6.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
<td id="A7.T13.2.1.33.33.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.7.1" class="ltx_text" style="font-size:144%;">62.8</span></td>
<td id="A7.T13.2.1.33.33.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.8.1" class="ltx_text" style="font-size:144%;">77.0</span></td>
<td id="A7.T13.2.1.33.33.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.33.33.9.1" class="ltx_text" style="font-size:144%;">62.5</span></td>
</tr>
<tr id="A7.T13.2.1.34.34" class="ltx_tr">
<td id="A7.T13.2.1.34.34.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m-t</span></td>
<td id="A7.T13.2.1.34.34.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.2.1" class="ltx_text" style="font-size:144%;">DiDeMo</span></td>
<td id="A7.T13.2.1.34.34.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.3.1" class="ltx_text" style="font-size:144%;">60.0</span></td>
<td id="A7.T13.2.1.34.34.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.4.1" class="ltx_text" style="font-size:144%;">66.8</span></td>
<td id="A7.T13.2.1.34.34.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.5.1" class="ltx_text" style="font-size:144%;">55.6</span></td>
<td id="A7.T13.2.1.34.34.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.6.1" class="ltx_text" style="font-size:144%;">65.6</span></td>
<td id="A7.T13.2.1.34.34.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.7.1" class="ltx_text" style="font-size:144%;">62.8</span></td>
<td id="A7.T13.2.1.34.34.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.8.1" class="ltx_text" style="font-size:144%;">75.4</span></td>
<td id="A7.T13.2.1.34.34.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.34.34.9.1" class="ltx_text" style="font-size:144%;">64.4</span></td>
</tr>
<tr id="A7.T13.2.1.35.35" class="ltx_tr">
<td id="A7.T13.2.1.35.35.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.1.1" class="ltx_text" style="font-size:144%;">Singularity-5m-t</span></td>
<td id="A7.T13.2.1.35.35.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.2.1" class="ltx_text" style="font-size:144%;">SSv2-label</span></td>
<td id="A7.T13.2.1.35.35.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.3.1" class="ltx_text" style="font-size:144%;">58.4</span></td>
<td id="A7.T13.2.1.35.35.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.4.1" class="ltx_text" style="font-size:144%;">68.3</span></td>
<td id="A7.T13.2.1.35.35.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.5.1" class="ltx_text" style="font-size:144%;">46.4</span></td>
<td id="A7.T13.2.1.35.35.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.6.1" class="ltx_text" style="font-size:144%;">63.0</span></td>
<td id="A7.T13.2.1.35.35.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.7.1" class="ltx_text" style="font-size:144%;">58.9</span></td>
<td id="A7.T13.2.1.35.35.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.8.1" class="ltx_text" style="font-size:144%;">74.2</span></td>
<td id="A7.T13.2.1.35.35.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.35.35.9.1" class="ltx_text" style="font-size:144%;">61.5</span></td>
</tr>
<tr id="A7.T13.2.1.36.36" class="ltx_tr">
<td id="A7.T13.2.1.36.36.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.1.1" class="ltx_text" style="font-size:144%;">CLIP4Clip-meanP</span></td>
<td id="A7.T13.2.1.36.36.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.36.36.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.3.1" class="ltx_text" style="font-size:144%;">61.6</span></td>
<td id="A7.T13.2.1.36.36.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.4.1" class="ltx_text" style="font-size:144%;">67.3</span></td>
<td id="A7.T13.2.1.36.36.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.5.1" class="ltx_text" style="font-size:144%;">60.3</span></td>
<td id="A7.T13.2.1.36.36.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.6.1" class="ltx_text" style="font-size:144%;">66.1</span></td>
<td id="A7.T13.2.1.36.36.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.7.1" class="ltx_text" style="font-size:144%;">62.8</span></td>
<td id="A7.T13.2.1.36.36.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.8.1" class="ltx_text" style="font-size:144%;">82.4</span></td>
<td id="A7.T13.2.1.36.36.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.36.36.9.1" class="ltx_text" style="font-size:144%;">66.8</span></td>
</tr>
<tr id="A7.T13.2.1.37.37" class="ltx_tr">
<td id="A7.T13.2.1.37.37.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.1.1" class="ltx_text" style="font-size:144%;">CLIP4Clip-seqTransf</span></td>
<td id="A7.T13.2.1.37.37.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.37.37.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.3.1" class="ltx_text" style="font-size:144%;">62.6</span></td>
<td id="A7.T13.2.1.37.37.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.4.1" class="ltx_text" style="font-size:144%;">65.3</span></td>
<td id="A7.T13.2.1.37.37.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.5.1" class="ltx_text" style="font-size:144%;">51.7</span></td>
<td id="A7.T13.2.1.37.37.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.6.1" class="ltx_text" style="font-size:144%;">66.5</span></td>
<td id="A7.T13.2.1.37.37.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.7.1" class="ltx_text" style="font-size:144%;">63.5</span></td>
<td id="A7.T13.2.1.37.37.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.8.1" class="ltx_text" style="font-size:144%;">82.4</span></td>
<td id="A7.T13.2.1.37.37.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.37.37.9.1" class="ltx_text" style="font-size:144%;">65.3</span></td>
</tr>
<tr id="A7.T13.2.1.38.38" class="ltx_tr">
<td id="A7.T13.2.1.38.38.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.1.1" class="ltx_text" style="font-size:144%;">X-CLIP-meanP</span></td>
<td id="A7.T13.2.1.38.38.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.38.38.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.3.1" class="ltx_text" style="font-size:144%;">62.1</span></td>
<td id="A7.T13.2.1.38.38.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.4.1" class="ltx_text" style="font-size:144%;">63.8</span></td>
<td id="A7.T13.2.1.38.38.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.5.1" class="ltx_text" style="font-size:144%;">59.6</span></td>
<td id="A7.T13.2.1.38.38.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.6.1" class="ltx_text" style="font-size:144%;">65.6</span></td>
<td id="A7.T13.2.1.38.38.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.7.1" class="ltx_text" style="font-size:144%;">64.2</span></td>
<td id="A7.T13.2.1.38.38.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.8.1" class="ltx_text" style="font-size:144%;">82.6</span></td>
<td id="A7.T13.2.1.38.38.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.38.38.9.1" class="ltx_text" style="font-size:144%;">66.3</span></td>
</tr>
<tr id="A7.T13.2.1.39.39" class="ltx_tr">
<td id="A7.T13.2.1.39.39.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.1.1" class="ltx_text" style="font-size:144%;">X-CLIP-seqTransf</span></td>
<td id="A7.T13.2.1.39.39.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.39.39.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.3.1" class="ltx_text" style="font-size:144%;">63.6</span></td>
<td id="A7.T13.2.1.39.39.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.4.1" class="ltx_text" style="font-size:144%;">60.8</span></td>
<td id="A7.T13.2.1.39.39.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.5.1" class="ltx_text" style="font-size:144%;">55.6</span></td>
<td id="A7.T13.2.1.39.39.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.6.1" class="ltx_text" style="font-size:144%;">64.5</span></td>
<td id="A7.T13.2.1.39.39.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.7.1" class="ltx_text" style="font-size:144%;">63.7</span></td>
<td id="A7.T13.2.1.39.39.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.8.1" class="ltx_text" style="font-size:144%;">83.2</span></td>
<td id="A7.T13.2.1.39.39.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.39.39.9.1" class="ltx_text" style="font-size:144%;">65.2</span></td>
</tr>
<tr id="A7.T13.2.1.40.40" class="ltx_tr">
<td id="A7.T13.2.1.40.40.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.1.1" class="ltx_text" style="font-size:144%;">X-Pool</span></td>
<td id="A7.T13.2.1.40.40.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.2.1" class="ltx_text" style="font-size:144%;">MSVD</span></td>
<td id="A7.T13.2.1.40.40.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.3.1" class="ltx_text" style="font-size:144%;">64.4</span></td>
<td id="A7.T13.2.1.40.40.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.4.1" class="ltx_text" style="font-size:144%;">57.9</span></td>
<td id="A7.T13.2.1.40.40.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.5.1" class="ltx_text" style="font-size:144%;">51.0</span></td>
<td id="A7.T13.2.1.40.40.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.6.1" class="ltx_text" style="font-size:144%;">68.3</span></td>
<td id="A7.T13.2.1.40.40.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.7.1" class="ltx_text" style="font-size:144%;">62.1</span></td>
<td id="A7.T13.2.1.40.40.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.8.1" class="ltx_text" style="font-size:144%;">78.8</span></td>
<td id="A7.T13.2.1.40.40.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.40.40.9.1" class="ltx_text" style="font-size:144%;">63.8</span></td>
</tr>
<tr id="A7.T13.2.1.41.41" class="ltx_tr">
<td id="A7.T13.2.1.41.41.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.1.1" class="ltx_text" style="font-size:144%;">X-Pool</span></td>
<td id="A7.T13.2.1.41.41.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.41.41.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.3.1" class="ltx_text" style="font-size:144%;">59.9</span></td>
<td id="A7.T13.2.1.41.41.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.4.1" class="ltx_text" style="font-size:144%;">63.0</span></td>
<td id="A7.T13.2.1.41.41.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.5.1" class="ltx_text" style="font-size:144%;">55.6</span></td>
<td id="A7.T13.2.1.41.41.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.6.1" class="ltx_text" style="font-size:144%;">66.5</span></td>
<td id="A7.T13.2.1.41.41.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.7.1" class="ltx_text" style="font-size:144%;">64.3</span></td>
<td id="A7.T13.2.1.41.41.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.8.1" class="ltx_text" style="font-size:144%;">81.3</span></td>
<td id="A7.T13.2.1.41.41.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.41.41.9.1" class="ltx_text" style="font-size:144%;">65.1</span></td>
</tr>
<tr id="A7.T13.2.1.42.42" class="ltx_tr">
<td id="A7.T13.2.1.42.42.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.1.1" class="ltx_text" style="font-size:144%;">X-Pool</span></td>
<td id="A7.T13.2.1.42.42.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.2.1" class="ltx_text" style="font-size:144%;">LSMDC</span></td>
<td id="A7.T13.2.1.42.42.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.3.1" class="ltx_text" style="font-size:144%;">60.1</span></td>
<td id="A7.T13.2.1.42.42.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.4.1" class="ltx_text" style="font-size:144%;">69.2</span></td>
<td id="A7.T13.2.1.42.42.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.5.1" class="ltx_text" style="font-size:144%;">50.3</span></td>
<td id="A7.T13.2.1.42.42.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.6.1" class="ltx_text" style="font-size:144%;">66.6</span></td>
<td id="A7.T13.2.1.42.42.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.7.1" class="ltx_text" style="font-size:144%;">59.4</span></td>
<td id="A7.T13.2.1.42.42.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.8.1" class="ltx_text" style="font-size:144%;">77.1</span></td>
<td id="A7.T13.2.1.42.42.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.42.42.9.1" class="ltx_text" style="font-size:144%;">63.8</span></td>
</tr>
<tr id="A7.T13.2.1.43.43" class="ltx_tr">
<td id="A7.T13.2.1.43.43.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.1.1" class="ltx_text" style="font-size:144%;">X-Pool-avg</span></td>
<td id="A7.T13.2.1.43.43.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.43.43.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.3.1" class="ltx_text" style="font-size:144%;">55.1</span></td>
<td id="A7.T13.2.1.43.43.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.4.1" class="ltx_text" style="font-size:144%;">65.3</span></td>
<td id="A7.T13.2.1.43.43.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.5.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.43.43.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.6.1" class="ltx_text" style="font-size:144%;">66.1</span></td>
<td id="A7.T13.2.1.43.43.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.7.1" class="ltx_text" style="font-size:144%;">57.8</span></td>
<td id="A7.T13.2.1.43.43.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.8.1" class="ltx_text" style="font-size:144%;">76.5</span></td>
<td id="A7.T13.2.1.43.43.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.43.43.9.1" class="ltx_text" style="font-size:144%;">62.2</span></td>
</tr>
<tr id="A7.T13.2.1.44.44" class="ltx_tr">
<td id="A7.T13.2.1.44.44.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.1.1" class="ltx_text" style="font-size:144%;">X-Pool-attn</span></td>
<td id="A7.T13.2.1.44.44.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.44.44.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.3.1" class="ltx_text" style="font-size:144%;">55.1</span></td>
<td id="A7.T13.2.1.44.44.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.4.1" class="ltx_text" style="font-size:144%;">64.4</span></td>
<td id="A7.T13.2.1.44.44.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.5.1" class="ltx_text" style="font-size:144%;">51.0</span></td>
<td id="A7.T13.2.1.44.44.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.6.1" class="ltx_text" style="font-size:144%;">65.2</span></td>
<td id="A7.T13.2.1.44.44.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.7.1" class="ltx_text" style="font-size:144%;">58.1</span></td>
<td id="A7.T13.2.1.44.44.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.8.1" class="ltx_text" style="font-size:144%;">76.0</span></td>
<td id="A7.T13.2.1.44.44.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.44.44.9.1" class="ltx_text" style="font-size:144%;">61.6</span></td>
</tr>
<tr id="A7.T13.2.1.45.45" class="ltx_tr">
<td id="A7.T13.2.1.45.45.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.1.1" class="ltx_text" style="font-size:144%;">X-Pool-topk</span></td>
<td id="A7.T13.2.1.45.45.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.45.45.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.3.1" class="ltx_text" style="font-size:144%;">54.3</span></td>
<td id="A7.T13.2.1.45.45.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.4.1" class="ltx_text" style="font-size:144%;">63.2</span></td>
<td id="A7.T13.2.1.45.45.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.5.1" class="ltx_text" style="font-size:144%;">53.6</span></td>
<td id="A7.T13.2.1.45.45.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.6.1" class="ltx_text" style="font-size:144%;">66.1</span></td>
<td id="A7.T13.2.1.45.45.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.7.1" class="ltx_text" style="font-size:144%;">56.9</span></td>
<td id="A7.T13.2.1.45.45.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.8.1" class="ltx_text" style="font-size:144%;">75.3</span></td>
<td id="A7.T13.2.1.45.45.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.45.45.9.1" class="ltx_text" style="font-size:144%;">61.6</span></td>
</tr>
<tr id="A7.T13.2.1.46.46" class="ltx_tr">
<td id="A7.T13.2.1.46.46.1" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.1.1" class="ltx_text" style="font-size:144%;">X-Pool-seqTransf</span></td>
<td id="A7.T13.2.1.46.46.2" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.2.1" class="ltx_text" style="font-size:144%;">MSR-VTT</span></td>
<td id="A7.T13.2.1.46.46.3" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.3.1" class="ltx_text" style="font-size:144%;">60.4</span></td>
<td id="A7.T13.2.1.46.46.4" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.4.1" class="ltx_text" style="font-size:144%;">65.5</span></td>
<td id="A7.T13.2.1.46.46.5" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.5.1" class="ltx_text" style="font-size:144%;">58.3</span></td>
<td id="A7.T13.2.1.46.46.6" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.6.1" class="ltx_text" style="font-size:144%;">65.0</span></td>
<td id="A7.T13.2.1.46.46.7" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.7.1" class="ltx_text" style="font-size:144%;">62.1</span></td>
<td id="A7.T13.2.1.46.46.8" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.8.1" class="ltx_text" style="font-size:144%;">79.9</span></td>
<td id="A7.T13.2.1.46.46.9" class="ltx_td ltx_align_center"><span id="A7.T13.2.1.46.46.9.1" class="ltx_text" style="font-size:144%;">65.2</span></td>
</tr>
<tr id="A7.T13.2.1.47.47" class="ltx_tr">
<td id="A7.T13.2.1.47.47.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.1.1" class="ltx_text" style="font-size:144%;">Video-LLaMA</span></td>
<td id="A7.T13.2.1.47.47.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.47.47.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.3.1" class="ltx_text" style="font-size:144%;">51.6</span></td>
<td id="A7.T13.2.1.47.47.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.4.1" class="ltx_text" style="font-size:144%;">52.2</span></td>
<td id="A7.T13.2.1.47.47.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.5.1" class="ltx_text" style="font-size:144%;">56.3</span></td>
<td id="A7.T13.2.1.47.47.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.6.1" class="ltx_text" style="font-size:144%;">51.0</span></td>
<td id="A7.T13.2.1.47.47.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.7.1" class="ltx_text" style="font-size:144%;">49.4</span></td>
<td id="A7.T13.2.1.47.47.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.8.1" class="ltx_text" style="font-size:144%;">51.7</span></td>
<td id="A7.T13.2.1.47.47.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T13.2.1.47.47.9.1" class="ltx_text" style="font-size:144%;">52.0</span></td>
</tr>
<tr id="A7.T13.2.1.48.48" class="ltx_tr">
<td id="A7.T13.2.1.48.48.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.1.1" class="ltx_text" style="font-size:144%;">VideoChat</span></td>
<td id="A7.T13.2.1.48.48.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.2.1" class="ltx_text" style="font-size:144%;">Zero-shot</span></td>
<td id="A7.T13.2.1.48.48.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.3.1" class="ltx_text" style="font-size:144%;">52.3</span></td>
<td id="A7.T13.2.1.48.48.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.4.1" class="ltx_text" style="font-size:144%;">50.3</span></td>
<td id="A7.T13.2.1.48.48.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.5.1" class="ltx_text" style="font-size:144%;">46.4</span></td>
<td id="A7.T13.2.1.48.48.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.6.1" class="ltx_text" style="font-size:144%;">50.4</span></td>
<td id="A7.T13.2.1.48.48.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.7.1" class="ltx_text" style="font-size:144%;">51.7</span></td>
<td id="A7.T13.2.1.48.48.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.8.1" class="ltx_text" style="font-size:144%;">51.0</span></td>
<td id="A7.T13.2.1.48.48.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T13.2.1.48.48.9.1" class="ltx_text" style="font-size:144%;">50.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A7.T13.5.1.1" class="ltx_text" style="font-size:63%;">Table 13</span>: </span><span id="A7.T13.6.2" class="ltx_text" style="font-size:63%;">Summarization of all evaluation results. For Singularity, “-t” denotes the temporal version and “-5m/-17m” denotes the size of pre-training data. For CLIP-based models, “-meanP” (non-temporal) and “-seqTransf” (temporal) denote the type of temporal aggregation module. </span></figcaption>
</figure>
<figure id="A7.T14" class="ltx_table">
<table id="A7.T14.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A7.T14.2.1.1" class="ltx_tr">
<td id="A7.T14.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="12"><span id="A7.T14.2.1.1.1.1" class="ltx_text" style="font-size:144%;">Positive</span></td>
<td id="A7.T14.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="A7.T14.2.1.1.2.1" class="ltx_text" style="font-size:144%;">Direction</span></td>
<td id="A7.T14.2.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T14.2.1.1.3.1" class="ltx_text" style="font-size:144%;">a person is connecting something to system</span></td>
</tr>
<tr id="A7.T14.2.2.2" class="ltx_tr">
<td id="A7.T14.2.2.2.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.2.2.1.1" class="ltx_text" style="font-size:144%;">a person is disconnecting something from system</span></td>
</tr>
<tr id="A7.T14.2.3.3" class="ltx_tr">
<td id="A7.T14.2.3.3.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.3.3.1.1" class="ltx_text" style="font-size:144%;">Intensity</span></td>
<td id="A7.T14.2.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.3.3.2.1" class="ltx_text" style="font-size:144%;">person is driving his black car fast on the street</span></td>
</tr>
<tr id="A7.T14.2.4.4" class="ltx_tr">
<td id="A7.T14.2.4.4.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.4.4.1.1" class="ltx_text" style="font-size:144%;">person is driving his black car slow on the street</span></td>
</tr>
<tr id="A7.T14.2.5.5" class="ltx_tr">
<td id="A7.T14.2.5.5.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.5.5.1.1" class="ltx_text" style="font-size:144%;">Sequence</span></td>
<td id="A7.T14.2.5.5.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.5.5.2.1" class="ltx_text" style="font-size:144%;">a person slides down a sandy path and falls down</span></td>
</tr>
<tr id="A7.T14.2.6.6" class="ltx_tr">
<td id="A7.T14.2.6.6.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.6.6.1.1" class="ltx_text" style="font-size:144%;">a person falls down and slides down a sandy path</span></td>
</tr>
<tr id="A7.T14.2.7.7" class="ltx_tr">
<td id="A7.T14.2.7.7.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.7.7.1.1" class="ltx_text" style="font-size:144%;">Localization</span></td>
<td id="A7.T14.2.7.7.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.7.7.2.1" class="ltx_text" style="font-size:144%;">a man gets shot in the face</span></td>
</tr>
<tr id="A7.T14.2.8.8" class="ltx_tr">
<td id="A7.T14.2.8.8.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.8.8.1.1" class="ltx_text" style="font-size:144%;">a man gets shot in the chest</span></td>
</tr>
<tr id="A7.T14.2.9.9" class="ltx_tr">
<td id="A7.T14.2.9.9.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.9.9.1.1" class="ltx_text" style="font-size:144%;">Compositionality</span></td>
<td id="A7.T14.2.9.9.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.9.9.2.1" class="ltx_text" style="font-size:144%;">there is a man shooting other people in a corridor</span></td>
</tr>
<tr id="A7.T14.2.10.10" class="ltx_tr">
<td id="A7.T14.2.10.10.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.10.10.1.1" class="ltx_text" style="font-size:144%;">there is a man being shot in a corridor</span></td>
</tr>
<tr id="A7.T14.2.11.11" class="ltx_tr">
<td id="A7.T14.2.11.11.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.11.11.1.1" class="ltx_text" style="font-size:144%;">Type</span></td>
<td id="A7.T14.2.11.11.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.11.11.2.1" class="ltx_text" style="font-size:144%;">a man cooks burgers and bacon on a grill</span></td>
</tr>
<tr id="A7.T14.2.12.12" class="ltx_tr">
<td id="A7.T14.2.12.12.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.12.12.1.1" class="ltx_text" style="font-size:144%;">a man eats burgers and bacon on a grill</span></td>
</tr>
<tr id="A7.T14.2.13.13" class="ltx_tr">
<td id="A7.T14.2.13.13.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="6"><span id="A7.T14.2.13.13.1.1" class="ltx_text" style="font-size:144%;">N/A</span></td>
<td id="A7.T14.2.13.13.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.13.13.2.1" class="ltx_text" style="font-size:144%;">Direction</span></td>
<td id="A7.T14.2.13.13.3" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.13.13.3.1" class="ltx_text" style="font-size:144%;">a young man is running very fast on a treadmill while being supported by a harness</span></td>
</tr>
<tr id="A7.T14.2.14.14" class="ltx_tr">
<td id="A7.T14.2.14.14.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.14.14.1.1" class="ltx_text" style="font-size:144%;">Intensity</span></td>
<td id="A7.T14.2.14.14.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.14.14.2.1" class="ltx_text" style="font-size:144%;">then ham is place on the bread</span></td>
</tr>
<tr id="A7.T14.2.15.15" class="ltx_tr">
<td id="A7.T14.2.15.15.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.15.15.1.1" class="ltx_text" style="font-size:144%;">Sequence</span></td>
<td id="A7.T14.2.15.15.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.15.15.2.1" class="ltx_text" style="font-size:144%;">a man throws an ax from overhead towards a target</span></td>
</tr>
<tr id="A7.T14.2.16.16" class="ltx_tr">
<td id="A7.T14.2.16.16.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.16.16.1.1" class="ltx_text" style="font-size:144%;">Localization</span></td>
<td id="A7.T14.2.16.16.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.16.16.2.1" class="ltx_text" style="font-size:144%;">a video game car is driving recklessly</span></td>
</tr>
<tr id="A7.T14.2.17.17" class="ltx_tr">
<td id="A7.T14.2.17.17.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.17.17.1.1" class="ltx_text" style="font-size:144%;">Compositionality</span></td>
<td id="A7.T14.2.17.17.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.17.17.2.1" class="ltx_text" style="font-size:144%;">a student in class smashes his forehead on the desk two times</span></td>
</tr>
<tr id="A7.T14.2.18.18" class="ltx_tr">
<td id="A7.T14.2.18.18.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.18.18.1.1" class="ltx_text" style="font-size:144%;">Type</span></td>
<td id="A7.T14.2.18.18.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.18.18.2.1" class="ltx_text" style="font-size:144%;">a scenic view is available through a camera while a man narrates about the camera</span></td>
</tr>
<tr id="A7.T14.2.19.19" class="ltx_tr">
<td id="A7.T14.2.19.19.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="12"><span id="A7.T14.2.19.19.1.1" class="ltx_text" style="font-size:144%;">Negative</span></td>
<td id="A7.T14.2.19.19.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.19.19.2.1" class="ltx_text" style="font-size:144%;">Entailment</span></td>
<td id="A7.T14.2.19.19.3" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.19.19.3.1" class="ltx_text" style="font-size:144%;">a little girl does gymnastics</span></td>
</tr>
<tr id="A7.T14.2.20.20" class="ltx_tr">
<td id="A7.T14.2.20.20.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.20.20.1.1" class="ltx_text" style="font-size:144%;">a little girl is doing gymnastics</span></td>
</tr>
<tr id="A7.T14.2.21.21" class="ltx_tr">
<td id="A7.T14.2.21.21.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.21.21.1.1" class="ltx_text" style="font-size:144%;">Implausible</span></td>
<td id="A7.T14.2.21.21.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.21.21.2.1" class="ltx_text" style="font-size:144%;">persons are attending a class with laptops</span></td>
</tr>
<tr id="A7.T14.2.22.22" class="ltx_tr">
<td id="A7.T14.2.22.22.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.22.22.1.1" class="ltx_text" style="font-size:144%;">laptops are attending a class with persons</span></td>
</tr>
<tr id="A7.T14.2.23.23" class="ltx_tr">
<td id="A7.T14.2.23.23.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.23.23.1.1" class="ltx_text" style="font-size:144%;">Static</span></td>
<td id="A7.T14.2.23.23.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.23.23.2.1" class="ltx_text" style="font-size:144%;">a man is driving a black car</span></td>
</tr>
<tr id="A7.T14.2.24.24" class="ltx_tr">
<td id="A7.T14.2.24.24.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.24.24.1.1" class="ltx_text" style="font-size:144%;">a man is driving a white car</span></td>
</tr>
<tr id="A7.T14.2.25.25" class="ltx_tr">
<td id="A7.T14.2.25.25.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A7.T14.2.25.25.1.1" class="ltx_text" style="font-size:144%;">Negation</span></td>
<td id="A7.T14.2.25.25.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.25.25.2.1" class="ltx_text" style="font-size:144%;">cartoon people are eating at a restaurant</span></td>
</tr>
<tr id="A7.T14.2.26.26" class="ltx_tr">
<td id="A7.T14.2.26.26.1" class="ltx_td ltx_align_left"><span id="A7.T14.2.26.26.1.1" class="ltx_text" style="font-size:144%;">cartoon people are not eating at a restaurant</span></td>
</tr>
<tr id="A7.T14.2.27.27" class="ltx_tr">
<td id="A7.T14.2.27.27.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A7.T14.2.27.27.1.1" class="ltx_text" style="font-size:144%;">Excessive difference</span></td>
<td id="A7.T14.2.27.27.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T14.2.27.27.2.1" class="ltx_text" style="font-size:144%;">a soldier is speaking to a superior person in a movie</span></td>
</tr>
<tr id="A7.T14.2.28.28" class="ltx_tr">
<td id="A7.T14.2.28.28.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A7.T14.2.28.28.1.1" class="ltx_text" style="font-size:144%;">a soldier is watching a movie</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A7.T14.5.1.1" class="ltx_text" style="font-size:63%;">Table 14</span>: </span><span id="A7.T14.6.2" class="ltx_text" style="font-size:63%;">Examples from the initial positive, negative and N/A exemplar sets. </span></figcaption>
</figure>
<figure id="A7.T15" class="ltx_table"><svg id="A7.T15.pic1" class="ltx_picture ltx_centering" height="573.29" overflow="visible" version="1.1" width="600"><g transform="translate(0,573.29) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 567.38 C 0 570.65 2.64 573.29 5.91 573.29 L 594.09 573.29 C 597.36 573.29 600 570.65 600 567.38 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 567.38 C 1.97 569.56 3.73 571.32 5.91 571.32 L 594.09 571.32 C 596.27 571.32 598.03 569.56 598.03 567.38 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="545.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A7.T15.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A7.T15.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:144%;">You will be given a video description. Your task is to [instruction] in the description, while keeping the static information and other aspects of temporal information unchanged.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.2" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.2.1" class="ltx_text" style="font-size:144%;">Ensure the modified description meets the following criteria:</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.3" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.3.1" class="ltx_text" style="font-size:144%;">1. The modified description should be grammatically correct and semantically plausible.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.4" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.4.1" class="ltx_text" style="font-size:144%;">2. The modified description should NOT contain an entailment relationship with the original description.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.5" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.5.1" class="ltx_text" style="font-size:144%;">3. The modification should be minimal and should not make unnecessary changes to the information in the original description.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.6" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.6.1" class="ltx_text" style="font-size:144%;">4. The modified description should contain roughly the same amount of information as the original description. Do not simply negate the sentence.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.7" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.7.1" class="ltx_text" style="font-size:144%;">If the specified aspect of temporal information of the original description cannot be modified without violating these criteria, please respond with ”NA”.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.8" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.8.1" class="ltx_text" style="font-size:144%;">Here are some examples:</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.9" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.9.1" class="ltx_text" style="font-size:144%;">Original Description: [exemplar caption].</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.10" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.10.1" class="ltx_text" style="font-size:144%;">Modified Description: [exemplar counterfactual].</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.11" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.11.1" class="ltx_text" style="font-size:144%;">…</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.12" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.12.1" class="ltx_text" style="font-size:144%;">Now [instruction] in the following description without violating the aforementioned criteria.</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.13" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.13.1" class="ltx_text" style="font-size:144%;">Original Description: [query caption].</span></span>
<span id="A7.T15.pic1.1.1.1.1.1.14" class="ltx_p"><span id="A7.T15.pic1.1.1.1.1.1.14.1" class="ltx_text" style="font-size:144%;">Modified Description:</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A7.T15.4.1.1" class="ltx_text" style="font-size:63%;">Table 15</span>: </span><span id="A7.T15.5.2" class="ltx_text" style="font-size:63%;">Input template for in-context learning generation. </span></figcaption>
</figure>
<figure id="A7.T16" class="ltx_table">
<table id="A7.T16.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A7.T16.2.1.1" class="ltx_tr">
<th id="A7.T16.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A7.T16.2.1.1.1.1" class="ltx_text" style="font-size:144%;">Aspect</span></th>
<th id="A7.T16.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A7.T16.2.1.1.2.1" class="ltx_text" style="font-size:144%;">Instruction</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A7.T16.2.2.1" class="ltx_tr">
<th id="A7.T16.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="A7.T16.2.2.1.1.1" class="ltx_text" style="font-size:144%;">Direction</span></th>
<td id="A7.T16.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T16.2.2.1.2.1" class="ltx_text" style="font-size:144%;">Reverse the direction of changes to object status.</span></td>
</tr>
<tr id="A7.T16.2.3.2" class="ltx_tr">
<th id="A7.T16.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A7.T16.2.3.2.1.1" class="ltx_text" style="font-size:144%;">Intensity</span></th>
<td id="A7.T16.2.3.2.2" class="ltx_td ltx_align_left"><span id="A7.T16.2.3.2.2.1" class="ltx_text" style="font-size:144%;">Modify the speed or intensity of the actions/events.</span></td>
</tr>
<tr id="A7.T16.2.4.3" class="ltx_tr">
<th id="A7.T16.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A7.T16.2.4.3.1.1" class="ltx_text" style="font-size:144%;">Sequence</span></th>
<td id="A7.T16.2.4.3.2" class="ltx_td ltx_align_left"><span id="A7.T16.2.4.3.2.1" class="ltx_text" style="font-size:144%;">Change the temporal order or the number of occurrences of the actions/events.</span></td>
</tr>
<tr id="A7.T16.2.5.4" class="ltx_tr">
<th id="A7.T16.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A7.T16.2.5.4.1.1" class="ltx_text" style="font-size:144%;">Localization</span></th>
<td id="A7.T16.2.5.4.2" class="ltx_td ltx_align_left"><span id="A7.T16.2.5.4.2.1" class="ltx_text" style="font-size:144%;">Change the location where the actions/events take place on the frame.</span></td>
</tr>
<tr id="A7.T16.2.6.5" class="ltx_tr">
<th id="A7.T16.2.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A7.T16.2.6.5.1.1" class="ltx_text" style="font-size:144%;">Compositionality</span></th>
<td id="A7.T16.2.6.5.2" class="ltx_td ltx_align_left"><span id="A7.T16.2.6.5.2.1" class="ltx_text" style="font-size:144%;">Change the composition of persons/objects with actions/events.</span></td>
</tr>
<tr id="A7.T16.2.7.6" class="ltx_tr">
<th id="A7.T16.2.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="A7.T16.2.7.6.1.1" class="ltx_text" style="font-size:144%;">Type</span></th>
<td id="A7.T16.2.7.6.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="A7.T16.2.7.6.2.1" class="ltx_text" style="font-size:144%;">Modify the type of actions.</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A7.T16.5.1.1" class="ltx_text" style="font-size:63%;">Table 16</span>: </span><span id="A7.T16.6.2" class="ltx_text" style="font-size:63%;">Instructions for in-context learning generation. </span></figcaption>
</figure>
<figure id="A7.T17" class="ltx_table"><svg id="A7.T17.pic1" class="ltx_picture ltx_centering" height="1052.49" overflow="visible" version="1.1" width="600"><g transform="translate(0,1052.49) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 1046.59 C 0 1049.85 2.64 1052.49 5.91 1052.49 L 594.09 1052.49 C 597.36 1052.49 600 1049.85 600 1046.59 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 1046.59 C 1.97 1048.76 3.73 1050.52 5.91 1050.52 L 594.09 1050.52 C 596.27 1050.52 598.03 1048.76 598.03 1046.59 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="1024.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A7.T17.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A7.T17.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Aspect: Direction</span><span id="A7.T17.pic1.1.1.1.1.1.1.2" class="ltx_text" style="font-size:144%;"></span></span>
<span id="A7.T17.pic1.1.1.1.1.1.2" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.2.1" class="ltx_text" style="font-size:144%;">This aspect focuses on the model’s ability to understand the direction of a single action temporally.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.3" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.3.1" class="ltx_text" style="font-size:144%;">The two actions differ only in the direction of the action.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.4" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.4.1" class="ltx_text" style="font-size:144%;">Typical positive examples include: opposite direction of motion, opposite direction of state change, etc.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.5" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.5.1" class="ltx_text" style="font-size:144%;">Typical negative examples include: simple negation, antonyms that do not contain direction, modification of incorrect attributes.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.6" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.6.1" class="ltx_text" style="font-size:144%;">Positive examples:</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.7" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.7.1" class="ltx_text" style="font-size:144%;">- People attach dogs to sled harness as the dogs howl.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.8" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.8.1" class="ltx_text" style="font-size:144%;">- People detach dogs from sled harness as the dogs howl.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.9" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.9.1" class="ltx_text" style="font-size:144%;">- hillary clinton is walking into a building</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.10" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.10.1" class="ltx_text" style="font-size:144%;">- hillary clinton is walking out of a building</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.11" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.11.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Aspect: Intensity</span><span id="A7.T17.pic1.1.1.1.1.1.11.2" class="ltx_text" style="font-size:144%;"></span></span>
<span id="A7.T17.pic1.1.1.1.1.1.12" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.12.1" class="ltx_text" style="font-size:144%;">This aspect focuses on the model’s ability to understand the manner in which the change happened, reflected by the dynamic information in the video.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.13" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.13.1" class="ltx_text" style="font-size:144%;">The two actions differ only in the speed, intensity, etc. of the action.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.14" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.14.1" class="ltx_text" style="font-size:144%;">Typical positive examples include: modification of adverbs describing speed or intensity, modification of verbs containing intensity information, etc.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.15" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.15.1" class="ltx_text" style="font-size:144%;">Typical negative examples include: modification of adjectives that are only related to static information, modification of incorrect attributes, etc.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.16" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.16.1" class="ltx_text" style="font-size:144%;">Positive examples:</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.17" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.17.1" class="ltx_text" style="font-size:144%;">- A man smashes the radio with a baseball bat.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.18" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.18.1" class="ltx_text" style="font-size:144%;">- A man touches the radio with a baseball bat.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.19" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.19.1" class="ltx_text" style="font-size:144%;">- Several people are dancing wildly to music as a crowd watches.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.20" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.20.1" class="ltx_text" style="font-size:144%;">- Several people are dancing gracefully to music as a crowd watches.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.21" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.21.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Aspect: Sequence</span><span id="A7.T17.pic1.1.1.1.1.1.21.2" class="ltx_text" style="font-size:144%;"></span></span>
<span id="A7.T17.pic1.1.1.1.1.1.22" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.22.1" class="ltx_text" style="font-size:144%;">This aspect focuses on the model’s ability to understand and localize overall actions and events in a video.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.23" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.23.1" class="ltx_text" style="font-size:144%;">The two descriptions feature the same actions and events, and only differ in the number or order of actions and events.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.24" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.24.1" class="ltx_text" style="font-size:144%;">Typical positive examples include: swapping the order of verbs, modifying the conjunction of time, modifying the number of times, etc.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.25" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.25.1" class="ltx_text" style="font-size:144%;">Typical negative examples include: swapping verbs that happen at the same time, modification of incorrect attributes, etc.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.26" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.26.1" class="ltx_text" style="font-size:144%;">Positive examples:</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.27" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.27.1" class="ltx_text" style="font-size:144%;">- People are running in the dark and then in the desert.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.28" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.28.1" class="ltx_text" style="font-size:144%;">- People are running in the desert and then in the dark.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.29" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.29.1" class="ltx_text" style="font-size:144%;">- The man kicks the heavy bag repeatedly.</span></span>
<span id="A7.T17.pic1.1.1.1.1.1.30" class="ltx_p"><span id="A7.T17.pic1.1.1.1.1.1.30.1" class="ltx_text" style="font-size:144%;">- The man kicks the heavy bag once.</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A7.T17.4.1.1" class="ltx_text" style="font-size:63%;">Table 17</span>: </span><span id="A7.T17.5.2" class="ltx_text" style="font-size:63%;">Instructions of the “Direction”, “Intensity”, and “Sequence” aspects for annotators in the human revision stage. </span></figcaption>
</figure>
<figure id="A7.T18" class="ltx_table"><svg id="A7.T18.pic1" class="ltx_picture ltx_centering" height="1120.18" overflow="visible" version="1.1" width="600"><g transform="translate(0,1120.18) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 1114.28 C 0 1117.54 2.64 1120.18 5.91 1120.18 L 594.09 1120.18 C 597.36 1120.18 600 1117.54 600 1114.28 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 1114.28 C 1.97 1116.45 3.73 1118.22 5.91 1118.22 L 594.09 1118.22 C 596.27 1118.22 598.03 1116.45 598.03 1114.28 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="1092.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A7.T18.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A7.T18.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Aspect: Localization</span><span id="A7.T18.pic1.1.1.1.1.1.1.2" class="ltx_text" style="font-size:144%;"></span></span>
<span id="A7.T18.pic1.1.1.1.1.1.2" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.2.1" class="ltx_text" style="font-size:144%;">This aspect focuses on the model’s ability to identify actions and locate them in space.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.3" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.3.1" class="ltx_text" style="font-size:144%;">The two actions differ only in the location of the action in the frame.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.4" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.4.1" class="ltx_text" style="font-size:144%;">Typical positive examples include: changes in body parts, the result of shooting, etc.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.5" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.5.1" class="ltx_text" style="font-size:144%;">Typical negative examples include: changes in general scenes, changes in location information that are not related to actions, and modification of incorrect attributes.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.6" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.6.1" class="ltx_text" style="font-size:144%;">Positive examples:</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.7" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.7.1" class="ltx_text" style="font-size:144%;">- a man gets shot in the face</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.8" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.8.1" class="ltx_text" style="font-size:144%;">- a man gets shot in the chest</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.9" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.9.1" class="ltx_text" style="font-size:144%;">- A girl uses a pan to throw a pancake in the air and it lands on the floor.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.10" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.10.1" class="ltx_text" style="font-size:144%;">- A girl uses a pan to throw a pancake in the air and it lands in the pan.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.11" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.11.1" class="ltx_text" style="font-size:144%;">- A boy is in a neighberhood and shoots a basketball and it goes into the hoop</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.12" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.12.1" class="ltx_text" style="font-size:144%;">- A boy is in a neighberhood and shoots a basketball and it misses the hoop</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.13" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.13.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Aspect: Compositionality</span><span id="A7.T18.pic1.1.1.1.1.1.13.2" class="ltx_text" style="font-size:144%;"></span></span>
<span id="A7.T18.pic1.1.1.1.1.1.14" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.14.1" class="ltx_text" style="font-size:144%;">This aspect focuses on the model’s ability to understand the relationship between people, objects, and actions in a video.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.15" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.15.1" class="ltx_text" style="font-size:144%;">The two descriptions feature the same actions and people, and only differ in the actor or recipient of the action.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.16" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.16.1" class="ltx_text" style="font-size:144%;">Typical positive examples include: swapping the subject and object, modifying the active or passive voice, swapping the subject of multiple actions, etc.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.17" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.17.1" class="ltx_text" style="font-size:144%;">Typical negative examples include: modification that leads to semantic incoherence, modification of incorrect attributes, etc.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.18" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.18.1" class="ltx_text" style="font-size:144%;">Positive examples:</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.19" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.19.1" class="ltx_text" style="font-size:144%;">- there is a man shooting other people in a corridor</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.20" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.20.1" class="ltx_text" style="font-size:144%;">- there is a man being shot in a corridor</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.21" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.21.1" class="ltx_text" style="font-size:144%;">- A woman is rubbing a man’s leg, placing a wax strip on it.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.22" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.22.1" class="ltx_text" style="font-size:144%;">- A man is rubbing a woman’s leg, placing a wax strip on it.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.23" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.23.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Aspect: Type</span><span id="A7.T18.pic1.1.1.1.1.1.23.2" class="ltx_text" style="font-size:144%;"></span></span>
<span id="A7.T18.pic1.1.1.1.1.1.24" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.24.1" class="ltx_text" style="font-size:144%;">This aspect focuses on the model’s ability to understand the action in a broader sense.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.25" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.25.1" class="ltx_text" style="font-size:144%;">The two descriptions feature the same scene and people, and only differ in the action.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.26" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.26.1" class="ltx_text" style="font-size:144%;">Typical positive examples include: modification of verbs, etc.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.27" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.27.1" class="ltx_text" style="font-size:144%;">Typical negative examples include: modification that leads to semantic incoherence, modification of static information, modification of incorrect attributes, etc.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.28" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.28.1" class="ltx_text" style="font-size:144%;">Positive examples:</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.29" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.29.1" class="ltx_text" style="font-size:144%;">- A man is bouncing a tennis ball on a court.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.30" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.30.1" class="ltx_text" style="font-size:144%;">- A man is collecting tennis balls on a court.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.31" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.31.1" class="ltx_text" style="font-size:144%;">- A man in face paint is looking out a window.</span></span>
<span id="A7.T18.pic1.1.1.1.1.1.32" class="ltx_p"><span id="A7.T18.pic1.1.1.1.1.1.32.1" class="ltx_text" style="font-size:144%;">- A man in face paint is cleaning a window.</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="A7.T18.4.1.1" class="ltx_text" style="font-size:63%;">Table 18</span>: </span><span id="A7.T18.5.2" class="ltx_text" style="font-size:63%;">Instructions of the “Localization”, “Compositionality”, and “Type” aspects for annotators in the human revision stage. </span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.17403" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.17404" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.17404">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.17404" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.17405" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 16:17:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
