<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation</title>
<!--Generated on Mon Sep 16 12:46:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10244v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S2" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S2.SS1" title="In 2 Related Work ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Knowledge Tracing Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S2.SS2" title="In 2 Related Work ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Knowledge Tracing Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data Description</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.SS1" title="In 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>License</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.SS2" title="In 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.SS3" title="In 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthethic Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.SS4" title="In 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Data Cleaning and Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.SS5" title="In 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Statistics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S4" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>The ES-KT-24 Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S4.SS1" title="In 4 The ES-KT-24 Benchmark ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S4.SS2" title="In 4 The ES-KT-24 Benchmark ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Baseline Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S4.SS3" title="In 4 The ES-KT-24 Benchmark ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Performance Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S5" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Additional Data Usage for Educational Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S6" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S7" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S8" title="In ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Ethical Consideration</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dohee Kim<math alttext="{{}^{1}}{{}^{*}}" class="ltx_math_unparsed" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mmultiscripts id="id1.1.m1.1.1"><msup id="id1.1.m1.1.1.2"><mi id="id1.1.m1.1.1.2a"></mi><mo id="id1.1.m1.1.1.2.1">∗</mo></msup><mprescripts id="id1.1.m1.1.1a"></mprescripts><mrow id="id1.1.m1.1.1b"></mrow><mn id="id1.1.m1.1.1.3">1</mn></mmultiscripts><annotation encoding="application/x-tex" id="id1.1.m1.1b">{{}^{1}}{{}^{*}}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1c">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT</annotation></semantics></math>, Unggi Lee<sup class="ltx_sup" id="id15.15.id1">1</sup><sup class="ltx_sup" id="id16.16.id2">,</sup><math alttext="{{}^{2}}{{}^{*}}{{}^{\dagger}}" class="ltx_math_unparsed" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><mmultiscripts id="id4.4.m4.1.1"><msup id="id4.4.m4.1.1.2.2"><mi id="id4.4.m4.1.1.2.2a"></mi><mo id="id4.4.m4.1.1.2.2.1">†</mo></msup><mprescripts id="id4.4.m4.1.1a"></mprescripts><mrow id="id4.4.m4.1.1b"></mrow><mn id="id4.4.m4.1.1.3">2</mn><mrow id="id4.4.m4.1.1c"></mrow><mo id="id4.4.m4.1.1.2.3">∗</mo></mmultiscripts><annotation encoding="application/x-tex" id="id4.4.m4.1b">{{}^{2}}{{}^{*}}{{}^{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1c">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Sookbun Lee<math alttext="{{}^{1}}{{}^{*}}" class="ltx_math_unparsed" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><mmultiscripts id="id5.5.m5.1.1"><msup id="id5.5.m5.1.1.2"><mi id="id5.5.m5.1.1.2a"></mi><mo id="id5.5.m5.1.1.2.1">∗</mo></msup><mprescripts id="id5.5.m5.1.1a"></mprescripts><mrow id="id5.5.m5.1.1b"></mrow><mn id="id5.5.m5.1.1.3">1</mn></mmultiscripts><annotation encoding="application/x-tex" id="id5.5.m5.1b">{{}^{1}}{{}^{*}}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1c">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT</annotation></semantics></math>, Jiyeong Bae<sup class="ltx_sup" id="id17.17.id3">1</sup>, Taekyung Ahn<sup class="ltx_sup" id="id18.18.id4">1</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id10.10.3">Jaekwon Park<sup class="ltx_sup" id="id10.10.3.1"><span class="ltx_text ltx_font_medium" id="id10.10.3.1.1">1</span></sup>, Gunho Lee<sup class="ltx_sup" id="id10.10.3.2"><span class="ltx_text ltx_font_medium" id="id10.10.3.2.1">1</span></sup>, Hyeoncheol Kim<math alttext="{{}^{2}}{{}^{\dagger}}" class="ltx_math_unparsed" display="inline" id="id10.10.3.m3.1"><semantics id="id10.10.3.m3.1a"><mmultiscripts id="id10.10.3.m3.1.1"><msup id="id10.10.3.m3.1.1.2"><mi id="id10.10.3.m3.1.1.2a"></mi><mo id="id10.10.3.m3.1.1.2.1">†</mo></msup><mprescripts id="id10.10.3.m3.1.1a"></mprescripts><mrow id="id10.10.3.m3.1.1b"></mrow><mn id="id10.10.3.m3.1.1.3">2</mn></mmultiscripts><annotation encoding="application/x-tex" id="id10.10.3.m3.1b">{{}^{2}}{{}^{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="id10.10.3.m3.1c">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<br class="ltx_break"/>Enuma, Inc.<sup class="ltx_sup" id="id19.19.id5">1</sup>, Korea University<sup class="ltx_sup" id="id20.20.id6">2</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id21.21.id7">{dohee, unggi, blackdew, jiyoung, taekyung, jaekwon, gunho}@enuma.com</span>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id22.22.id8">harrykim@korea.ac.kr</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id23.23.id9">∗</sup> is first authors and <math alttext="\dagger" class="ltx_Math" display="inline" id="id14.14.m11.1"><semantics id="id14.14.m11.1a"><mo id="id14.14.m11.1.1" xref="id14.14.m11.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id14.14.m11.1b"><ci id="id14.14.m11.1.1.cmml" xref="id14.14.m11.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id14.14.m11.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id14.14.m11.1d">†</annotation></semantics></math> is corresponding authors. 
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id24.id1">This paper introduces ES-KT-24, a novel multimodal Knowledge Tracing (KT) dataset for intelligent tutoring systems in educational game contexts. Although KT is crucial in adaptive learning, existing datasets often lack game-based and multimodal elements. ES-KT-24 addresses these limitations by incorporating educational game-playing videos, synthetically generated question text, and detailed game logs. The dataset covers Mathematics, English, Indonesian, and Malaysian subjects, emphasizing diversity and including non-English content.</p>
<p class="ltx_p" id="id25.id2">The synthetic text component, generated using a large language model, encompasses 28 distinct knowledge concepts and 182 questions, featuring 15,032 users and 7,782,928 interactions. Our benchmark experiments demonstrate the dataset’s utility for KT research by comparing Deep learning-based KT models with Language Model-based Knowledge Tracing (LKT) approaches. Notably, LKT models showed slightly higher performance than traditional DKT models, highlighting the potential of language model-based approaches in this field.</p>
<p class="ltx_p" id="id26.id3">Furthermore, ES-KT-24 has the potential to significantly advance research in multimodal KT models and learning analytics. By integrating game-playing videos and detailed game logs, this dataset offers a unique approach to dissecting student learning patterns through advanced data analysis and machine-learning techniques. It has the potential to unearth new insights into the learning process and inspire further exploration in the field.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.14">
<p class="ltx_p" id="p1.14.15"><span class="ltx_text ltx_font_bold" id="p1.14.15.1">ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.14.14" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.14.14.14" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.14.14.14.14">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.7"><span class="ltx_text ltx_font_bold" id="p1.7.7.7.7.7.7.7">Dohee Kim<math alttext="{{}^{1}}{{}^{*}}" class="ltx_math_unparsed" display="inline" id="p1.1.1.1.1.1.1.1.m1.1"><semantics id="p1.1.1.1.1.1.1.1.m1.1a"><mmultiscripts id="p1.1.1.1.1.1.1.1.m1.1.1"><msup id="p1.1.1.1.1.1.1.1.m1.1.1.2"><mi id="p1.1.1.1.1.1.1.1.m1.1.1.2a"></mi><mo id="p1.1.1.1.1.1.1.1.m1.1.1.2.1">∗</mo></msup><mprescripts id="p1.1.1.1.1.1.1.1.m1.1.1a"></mprescripts><mrow id="p1.1.1.1.1.1.1.1.m1.1.1b"></mrow><mn id="p1.1.1.1.1.1.1.1.m1.1.1.3">1</mn></mmultiscripts><annotation encoding="application/x-tex" id="p1.1.1.1.1.1.1.1.m1.1b">{{}^{1}}{{}^{*}}</annotation><annotation encoding="application/x-llamapun" id="p1.1.1.1.1.1.1.1.m1.1c">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT</annotation></semantics></math>, Unggi Lee<sup class="ltx_sup" id="p1.7.7.7.7.7.7.7.1"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.7.7.1.1">1</span></sup><sup class="ltx_sup" id="p1.7.7.7.7.7.7.7.2"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.7.7.2.1">,</span></sup><math alttext="{{}^{2}}{{}^{*}}{{}^{\dagger}}" class="ltx_math_unparsed" display="inline" id="p1.4.4.4.4.4.4.4.m4.1"><semantics id="p1.4.4.4.4.4.4.4.m4.1a"><mmultiscripts id="p1.4.4.4.4.4.4.4.m4.1.1"><msup id="p1.4.4.4.4.4.4.4.m4.1.1.2.2"><mi id="p1.4.4.4.4.4.4.4.m4.1.1.2.2a"></mi><mo id="p1.4.4.4.4.4.4.4.m4.1.1.2.2.1">†</mo></msup><mprescripts id="p1.4.4.4.4.4.4.4.m4.1.1a"></mprescripts><mrow id="p1.4.4.4.4.4.4.4.m4.1.1b"></mrow><mn id="p1.4.4.4.4.4.4.4.m4.1.1.3">2</mn><mrow id="p1.4.4.4.4.4.4.4.m4.1.1c"></mrow><mo id="p1.4.4.4.4.4.4.4.m4.1.1.2.3">∗</mo></mmultiscripts><annotation encoding="application/x-tex" id="p1.4.4.4.4.4.4.4.m4.1b">{{}^{2}}{{}^{*}}{{}^{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="p1.4.4.4.4.4.4.4.m4.1c">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Sookbun Lee<math alttext="{{}^{1}}{{}^{*}}" class="ltx_math_unparsed" display="inline" id="p1.5.5.5.5.5.5.5.m5.1"><semantics id="p1.5.5.5.5.5.5.5.m5.1a"><mmultiscripts id="p1.5.5.5.5.5.5.5.m5.1.1"><msup id="p1.5.5.5.5.5.5.5.m5.1.1.2"><mi id="p1.5.5.5.5.5.5.5.m5.1.1.2a"></mi><mo id="p1.5.5.5.5.5.5.5.m5.1.1.2.1">∗</mo></msup><mprescripts id="p1.5.5.5.5.5.5.5.m5.1.1a"></mprescripts><mrow id="p1.5.5.5.5.5.5.5.m5.1.1b"></mrow><mn id="p1.5.5.5.5.5.5.5.m5.1.1.3">1</mn></mmultiscripts><annotation encoding="application/x-tex" id="p1.5.5.5.5.5.5.5.m5.1b">{{}^{1}}{{}^{*}}</annotation><annotation encoding="application/x-llamapun" id="p1.5.5.5.5.5.5.5.m5.1c">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT</annotation></semantics></math>, Jiyeong Bae<sup class="ltx_sup" id="p1.7.7.7.7.7.7.7.3"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.7.7.3.1">1</span></sup>, Taekyung Ahn<sup class="ltx_sup" id="p1.7.7.7.7.7.7.7.4"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.7.7.4.1">1</span></sup></span></span></span>
<span class="ltx_tr" id="p1.10.10.10.10.10">
<span class="ltx_td ltx_align_center" id="p1.10.10.10.10.10.3"><span class="ltx_text ltx_font_bold" id="p1.10.10.10.10.10.3.3">Jaekwon Park<sup class="ltx_sup" id="p1.10.10.10.10.10.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.10.10.10.10.10.3.3.1.1">1</span></sup>, Gunho Lee<sup class="ltx_sup" id="p1.10.10.10.10.10.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.10.10.10.10.10.3.3.2.1">1</span></sup>, Hyeoncheol Kim<math alttext="{{}^{2}}{{}^{\dagger}}" class="ltx_math_unparsed" display="inline" id="p1.10.10.10.10.10.3.3.m3.1"><semantics id="p1.10.10.10.10.10.3.3.m3.1a"><mmultiscripts id="p1.10.10.10.10.10.3.3.m3.1.1"><msup id="p1.10.10.10.10.10.3.3.m3.1.1.2"><mi id="p1.10.10.10.10.10.3.3.m3.1.1.2a"></mi><mo id="p1.10.10.10.10.10.3.3.m3.1.1.2.1">†</mo></msup><mprescripts id="p1.10.10.10.10.10.3.3.m3.1.1a"></mprescripts><mrow id="p1.10.10.10.10.10.3.3.m3.1.1b"></mrow><mn id="p1.10.10.10.10.10.3.3.m3.1.1.3">2</mn></mmultiscripts><annotation encoding="application/x-tex" id="p1.10.10.10.10.10.3.3.m3.1b">{{}^{2}}{{}^{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="p1.10.10.10.10.10.3.3.m3.1c">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p1.12.12.12.12.12">
<span class="ltx_td ltx_align_center" id="p1.12.12.12.12.12.2">Enuma, Inc.<sup class="ltx_sup" id="p1.12.12.12.12.12.2.1">1</sup>, Korea University<sup class="ltx_sup" id="p1.12.12.12.12.12.2.2">2</sup></span></span>
<span class="ltx_tr" id="p1.14.14.14.14.15.1">
<span class="ltx_td ltx_align_center" id="p1.14.14.14.14.15.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.14.14.14.14.15.1.1.1">{dohee, unggi, blackdew, jiyoung, taekyung, jaekwon, gunho}@enuma.com</span>,</span></span>
<span class="ltx_tr" id="p1.14.14.14.14.16.2">
<span class="ltx_td ltx_align_center" id="p1.14.14.14.14.16.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.14.14.14.14.16.2.1.1">harrykim@korea.ac.kr</span></span></span>
<span class="ltx_tr" id="p1.14.14.14.14.14">
<span class="ltx_td ltx_align_center" id="p1.14.14.14.14.14.2"><sup class="ltx_sup" id="p1.14.14.14.14.14.2.1">∗</sup> is first authors and <math alttext="\dagger" class="ltx_Math" display="inline" id="p1.14.14.14.14.14.2.m2.1"><semantics id="p1.14.14.14.14.14.2.m2.1a"><mo id="p1.14.14.14.14.14.2.m2.1.1" xref="p1.14.14.14.14.14.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.14.14.14.14.14.2.m2.1b"><ci id="p1.14.14.14.14.14.2.m2.1.1.cmml" xref="p1.14.14.14.14.14.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.14.14.14.14.14.2.m2.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.14.14.14.14.14.2.m2.1d">†</annotation></semantics></math> is corresponding authors.</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1063" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
An example of ES-KT-24. ES-KT-24 consists of a multimodal dataset, including game-playing video, synthetic question text, knowledge concept (KC) text, and game logs collected from educational game contexts.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Knowledge Tracing (KT) is a fundamental task that aims to model students’ knowledge states over time based on their interactions with learning materials <cite class="ltx_cite ltx_citemacro_cite">Piech et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib36" title="">2015</a>)</cite>. These interactions typically include viewing problems, attempting solutions, and selecting answers in online learning systems. The goal of the KT model is to use these sequences to predict students’ future performance on unseen items <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib20" title="">2024b</a>)</cite>. Over the years, various KT models have been developed, ranging from traditional approaches like Bayesian Knowledge Tracing (BKT) <cite class="ltx_cite ltx_citemacro_cite">Corbett and Anderson (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib8" title="">1994</a>)</cite> to more recent deep learning-based methods such as Deep Knowledge Tracing (DKT) <cite class="ltx_cite ltx_citemacro_cite">Piech et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib36" title="">2015</a>)</cite>. These models have shown promising results in predicting student performance and understanding learning patterns.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
The process began with the manual game-play of educational games, which were screen-recorded. These recordings were then converted to text using OpenAI GPT-4o <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib30" title="">2024</a>)</cite> for visual content and Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib38" title="">2023</a>)</cite> for audio transcription. The resulting text was utilized to create Questions corresponding to each game. Student problem-solving histories and game logs were preprocessed and explored through Exploratory Data Analysis (EDA), then transformed into sequence data suitable for KT tasks. Finally, this text and sequence data were released as a paired dataset.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, most existing KT datasets primarily consist of numerical sequences, with only a few recent datasets incorporating textual information. These datasets typically focus on text-based interactions where students submit answers (correct or incorrect) to given problems <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib20" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib19" title="">a</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib24" title="">2019</a>)</cite>. This approach, while applicable, is limited in its ability to capture the complex interactions and outcomes possible in modern digital educational materials, particularly in game-based learning environments.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Game-based content offers a richer set of interactions and outcomes beyond simple correct/incorrect answers, including metrics such as time spent on tasks or the number of attempts made <cite class="ltx_cite ltx_citemacro_cite">Hooshyar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib16" title="">2022</a>)</cite>. Game-based learning often involves multiple modalities, including text, images, animations, and audio. To advance research in this area and develop more comprehensive KT models, there is a need for datasets that capture these multimodal aspects of game-based learning.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, there are still significant limitations in the publicly available datasets <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib26" title="">2024b</a>)</cite>. One notable gap is the need for more game-based learning datasets. While educational games have become increasingly popular as learning tools <cite class="ltx_cite ltx_citemacro_cite">Noemí and Máximo (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib28" title="">2014</a>); Moreno-Ger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib27" title="">2008</a>); Petri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib35" title="">2024</a>)</cite>, KT datasets from gaming environments, mainly publicly available, remain scarce. This scarcity is even more pronounced in game-based learning. To the best of our knowledge, there are currently no publicly accessible KT datasets derived from educational gaming environments.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Another critical limitation is the absence of multimodal data in most KT datasets. There has been a growing trend in research towards multimodal models that combine language models with other modalities such as vision and audio <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib23" title="">2024a</a>); Deshmukh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib9" title="">2023</a>); Borsos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib5" title="">2023</a>); Driess et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib11" title="">2023</a>)</cite>. These multimodal approaches have shown remarkable capabilities in understanding and generating content across different modalities.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"># Students</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"># Questions</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"># KCs</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"># Interactions</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">Subjects</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">Question Texts</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">Videos</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.1.1.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">Game Logs</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S1.T1.1.2.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">ASSISTments2009</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S1.T1.1.2.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">4,217</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S1.T1.1.2.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">26,688</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S1.T1.1.2.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">123</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S1.T1.1.2.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">346,860</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.2.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.2.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.2.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.2.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S1.T1.1.3.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">ASSISTments2012</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.3.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">46,674</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.3.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">179,999</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.3.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">265</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.3.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">6,123,270</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.2.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S1.T1.1.4.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">ASSISTments2015</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.4.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">19,917</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.4.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">100</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.4.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.4.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">708,631</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.3.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.3.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S1.T1.1.5.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">ASSISTments2017</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.5.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">1,709</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.5.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">3,162</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.5.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">102</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.5.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">942,816</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.5.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.5.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.5.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.5.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S1.T1.1.6.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">Statistics2011</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.6.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">333</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.6.5.3" style="padding-left:5.0pt;padding-right:5.0pt;">1,224</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.6.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.6.5.5" style="padding-left:5.0pt;padding-right:5.0pt;">194,947</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.6.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.6.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.6.5.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.6.5.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S1.T1.1.7.6.1" style="padding-left:5.0pt;padding-right:5.0pt;">Junyi2015</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.7.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">247,606</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.7.6.3" style="padding-left:5.0pt;padding-right:5.0pt;">722</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.7.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">41</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.7.6.5" style="padding-left:5.0pt;padding-right:5.0pt;">25,925,922</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.7.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.7.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.7.6.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.7.6.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S1.T1.1.8.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">KDD2005</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.8.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">574</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.8.7.3" style="padding-left:5.0pt;padding-right:5.0pt;">210,710</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.8.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">112</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.8.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">809,694</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.8.7.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.8.7.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.8.7.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.8.7.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.9.8">
<td class="ltx_td ltx_align_left" id="S1.T1.1.9.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">KDD2006</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.9.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">1,146</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.9.8.3" style="padding-left:5.0pt;padding-right:5.0pt;">207,856</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.9.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">493</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.9.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">3,679,199</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.9.8.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.9.8.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.9.8.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.9.8.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.10.9">
<td class="ltx_td ltx_align_left" id="S1.T1.1.10.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">NeurIPS2020</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.10.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">4,918</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.10.9.3" style="padding-left:5.0pt;padding-right:5.0pt;">948</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.10.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">57</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.10.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">1,382,727</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.10.9.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.10.9.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.10.9.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.10.9.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.11.10">
<td class="ltx_td ltx_align_left" id="S1.T1.1.11.10.1" style="padding-left:5.0pt;padding-right:5.0pt;">POJ</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.11.10.2" style="padding-left:5.0pt;padding-right:5.0pt;">22,916</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.11.10.3" style="padding-left:5.0pt;padding-right:5.0pt;">2,750</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.11.10.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.11.10.5" style="padding-left:5.0pt;padding-right:5.0pt;">996,240</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.11.10.6" style="padding-left:5.0pt;padding-right:5.0pt;">PL</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.11.10.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.11.10.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.11.10.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.12.11">
<td class="ltx_td ltx_align_left" id="S1.T1.1.12.11.1" style="padding-left:5.0pt;padding-right:5.0pt;">EdNet</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.12.11.2" style="padding-left:5.0pt;padding-right:5.0pt;">1,677,583</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.12.11.3" style="padding-left:5.0pt;padding-right:5.0pt;">52,676</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.12.11.4" style="padding-left:5.0pt;padding-right:5.0pt;">962</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.12.11.5" style="padding-left:5.0pt;padding-right:5.0pt;">372,366,720</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.12.11.6" style="padding-left:5.0pt;padding-right:5.0pt;">Linguistics</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.12.11.7" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.12.11.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.12.11.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.13.12">
<td class="ltx_td ltx_align_left" id="S1.T1.1.13.12.1" style="padding-left:5.0pt;padding-right:5.0pt;">DBE-KT22</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.13.12.2" style="padding-left:5.0pt;padding-right:5.0pt;">1,361</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.13.12.3" style="padding-left:5.0pt;padding-right:5.0pt;">212</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.13.12.4" style="padding-left:5.0pt;padding-right:5.0pt;">98</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.13.12.5" style="padding-left:5.0pt;padding-right:5.0pt;">167,222</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.13.12.6" style="padding-left:5.0pt;padding-right:5.0pt;">Computer and Information Science</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.13.12.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.13.12.7.1">Yes</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.13.12.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.13.12.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.14.13">
<td class="ltx_td ltx_align_left" id="S1.T1.1.14.13.1" style="padding-left:5.0pt;padding-right:5.0pt;">XES3G5M</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.14.13.2" style="padding-left:5.0pt;padding-right:5.0pt;">18,066</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.14.13.3" style="padding-left:5.0pt;padding-right:5.0pt;">7,652</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.14.13.4" style="padding-left:5.0pt;padding-right:5.0pt;">865</td>
<td class="ltx_td ltx_align_right" id="S1.T1.1.14.13.5" style="padding-left:5.0pt;padding-right:5.0pt;">5,549,635</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.14.13.6" style="padding-left:5.0pt;padding-right:5.0pt;">Math</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.14.13.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.14.13.7.1">Yes</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.14.13.8" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.14.13.9" style="padding-left:5.0pt;padding-right:5.0pt;">No</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.15.14">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.T1.1.15.14.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.1.1">ES-KT-24 (Ours)</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S1.T1.1.15.14.2" style="padding-left:5.0pt;padding-right:5.0pt;">15,032</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S1.T1.1.15.14.3" style="padding-left:5.0pt;padding-right:5.0pt;">182 <span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.3.1">(game)</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S1.T1.1.15.14.4" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S1.T1.1.15.14.5" style="padding-left:5.0pt;padding-right:5.0pt;">7,783,466</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.15.14.6" style="padding-left:5.0pt;padding-right:5.0pt;">English, Math, <span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.6.1">Indonesian</span>, <span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.6.2">Malay</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.15.14.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.7.1">Yes</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.15.14.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.8.1">Yes</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.15.14.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.15.14.9.1">Yes</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of KT benchmark dataset, referencing the XES3G5M research <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib26" title="">2024b</a>)</cite>. In several key aspects, existing KT datasets differ significantly from ES-KT-24. ES-KT-24 stands out for its multimodal approach, incorporating text, video, and game log data with educational game context. This dataset also broadens the scope of subjects beyond the typical Math and English, including Indonesian and Malay languages, to promote equity in educational research. Note that the number of questions corresponds directly to the number of games.</figcaption>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">It is becoming increasingly apparent in education that leveraging multiple modalities is crucial for developing comprehensive models that can fully capture the complexity of learning environments <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib21" title="">2024c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib22" title="">d</a>)</cite>. Educational contexts often involve rich, multimodal interactions, including visual aids, audio explanations, and hands-on activities, which a sequence of the number cannot adequately represent <cite class="ltx_cite ltx_citemacro_cite">Blikstein (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib3" title="">2013</a>); Ochoa et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib29" title="">2017</a>); Blikstein and Worsley (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib4" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">This paper introduces ES-KT-24 <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The dataset will be made publicly available following final review.</span></span></span>, a novel KT dataset based on educational game data. This dataset is uniquely rich in multimodal features, capturing various aspects of game-based learning environments. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, we provide <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">Game Playing Videos</span>, where one video was recorded for each available game by researchers, along with the processed <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">Question Texts</span> derived from these videos. Additionally, ES-KT-24 also contains <span class="ltx_text ltx_font_italic" id="S1.p7.1.3">Game Log</span> such as the game category (<span class="ltx_text ltx_font_italic" id="S1.p7.1.4">KC Text</span>), log registration time (<span class="ltx_text ltx_font_italic" id="S1.p7.1.5">Reg Date</span>), gameplay status (<span class="ltx_text ltx_font_italic" id="S1.p7.1.6">Content Status</span>), gameplay duration (<span class="ltx_text ltx_font_italic" id="S1.p7.1.7">Duration Time</span>), gameplay mode (<span class="ltx_text ltx_font_italic" id="S1.p7.1.8">Play Mode</span>), and the curriculum code (<span class="ltx_text ltx_font_italic" id="S1.p7.1.9">Lesson Code</span>). By providing this resource, we aim to support KT research and broader studies in Learning Analytics (LA) <cite class="ltx_cite ltx_citemacro_cite">Lang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib18" title="">2022</a>)</cite>. Researchers can utilize this dataset to investigate game-related learning phenomena, analyze multimodal learning processes, and develop more sophisticated models that account for the diverse modalities present in educational settings. The ES-KT-24 dataset thus represents a significant step towards more comprehensive and realistic modeling of student learning in digital environments.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Our research contributions are below:</p>
</div>
<div class="ltx_para" id="S1.p9">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a benchmark for KT using game data, offering a new standard for evaluating KT models in game-based learning environments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">By providing gameplay videos and learning data, we enable researchers in learning analytics to explore and analyze multimodal educational interactions.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Knowledge Tracing Datasets</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="139" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
KT dataset format. <span class="ltx_text ltx_font_italic" id="S2.F3.3.1">Left</span> shows the exercising process of a student, where the student has already done three questions and will answer question number 4. <span class="ltx_text ltx_font_italic" id="S2.F3.4.2">Right</span> shows the corresponding materials of questions that contain their contents and KCs. Note that question and KC texts are used for LKT.
</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">KT primarily involves utilizing datasets that record students’ interactions over time <cite class="ltx_cite ltx_citemacro_cite">Piech et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib36" title="">2015</a>)</cite>. These datasets generally contain sequential information about student responses to various items (e.g., KCs or questions). Each interaction in the dataset is typically associated with critical attributes such as the student ID, the item ID, the time the interaction occurred, and whether the student’s response was correct <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib20" title="">2024b</a>)</cite> (See Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S2.F3" title="Figure 3 ‣ 2.1 Knowledge Tracing Datasets ‣ 2 Related Work ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">3</span></a>). By analyzing these sequences, KT models aim to infer a student’s underlying knowledge state and predict their future performance on new, unseen items based on patterns in their past interactions <cite class="ltx_cite ltx_citemacro_cite">Pandey and Srivastava (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib34" title="">2020</a>); Ghosh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib12" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Several educational datasets have been widely used for KT tasks. These include the ASSISTments datasets <cite class="ltx_cite ltx_citemacro_cite">Heffernan and Heffernan (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib15" title="">2014</a>)</cite>, which provide instructional guidance by assessing students’ knowledge states; the Junyi Academy dataset <cite class="ltx_cite ltx_citemacro_cite">Academy (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib2" title="">2015</a>)</cite>, containing mathematics question-solving activities; the Peking Online Judge (POJ) dataset <cite class="ltx_cite ltx_citemacro_cite">Pandey and Srivastava (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib34" title="">2020</a>)</cite>, offering coding practice data; EdNet <cite class="ltx_cite ltx_citemacro_cite">Choi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib6" title="">2020</a>)</cite>, comprising TOEIC test preparation activities; and the Statics2011 <cite class="ltx_cite ltx_citemacro_cite">Project (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib37" title="">2011</a>)</cite> dataset from an online engineering course. Additionally, data mining competitions have contributed high-quality datasets, such as the KDD Cup 2010 EDM Challenge datasets <cite class="ltx_cite ltx_citemacro_cite">Organizers (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib31" title="">2010</a>)</cite> and the NeurIPS2020 Education Challenge dataset <cite class="ltx_cite ltx_citemacro_cite">Organizers (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib32" title="">2020</a>)</cite> from Eedi. The KDD Cup 2010 datasets consist of Algebra2005 and 2006 datasets. We refer to Algebra2005 as KDD2005 and Algebra2006 as KDD2006.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">While most of these datasets shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">1</span></a> primarily contain ID features of questions and knowledge components along with timestamps, recent datasets like DBE-KT22 <cite class="ltx_cite ltx_citemacro_cite">Abdelrahman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib1" title="">2022</a>)</cite> and XES3G5M <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib26" title="">2024b</a>)</cite> stand out by including the actual text content of the questions. XES3G5M, in particular, provides rich auxiliary information, including textual content of questions, knowledge component relationships, question types, and answer analyses that may enhance the modeling process of students’ learning outcomes.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">However, despite the variety of datasets available, there are two significant gaps in the field of KT datasets. First, KT datasets related to educational games are scarce. While game-based learning environments have become increasingly popular, publicly available KT datasets from these contexts are rare. The GameDKT study <cite class="ltx_cite ltx_citemacro_cite">Hooshyar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib16" title="">2022</a>)</cite> highlights this gap, presenting one of the few approaches for KT in game-based learning environments, but even this dataset is not publicly available for research. Second, there is a significant lack of KT datasets incorporating multimodal data, including text, images, videos, and audio. The scarcity of multimodal data limits researchers’ ability to develop KT models that can leverage diverse types of information to understand and predict student learning processes more accurately.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Knowledge Tracing Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">KT models have evolved significantly, aiming to accurately predict students’ knowledge states and future performance. DKT <cite class="ltx_cite ltx_citemacro_cite">Piech et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib36" title="">2015</a>)</cite> significantly advanced the field by leveraging recurrent neural networks to model students’ knowledge acquisition over time. Following DKT, various models were developed to enhance performance and address specific challenges. For instance, DKVMN <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib42" title="">2017</a>)</cite> utilized a key-value memory network to model the relationships between exercises and knowledge concepts. SAKT <cite class="ltx_cite ltx_citemacro_cite">Pandey and Karypis (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib33" title="">2019</a>)</cite> employed self-attention mechanisms to capture complex dependencies in student interaction sequences. AKT <cite class="ltx_cite ltx_citemacro_cite">Ghosh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib12" title="">2020</a>)</cite> further improved upon this by incorporating a monotonic attention mechanism and context-aware representations.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Recently, a novel approach called LKT has emerged, leveraging the power of Pre-trained Language Models (PLMs) to enhance KT performance. Lee et al. <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib20" title="">2024b</a>)</cite> introduced LKT, which directly utilizes PLMs to process learning data in a textual format. This method has demonstrated superior performance to traditional KT models by effectively capturing semantic information from questions and KCs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Building upon the success of LKT, efforts are being made to apply it to specific domains. For instance, CodeLKT <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib19" title="">2024a</a>)</cite> adapts the LKT framework to programming education, demonstrating significant improvements in predicting student performance on coding tasks.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">However, research on KT for game data and multimodal KT has been limited due to a lack of datasets in these areas. In response, this study introduces ES-KT-24, a multimodal dataset specifically designed for KT in game contexts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data Description</h2>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="514" id="S3.F4.g1" src="x4.png" width="913"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Data analysis results. <span class="ltx_text ltx_font_italic" id="S3.F4.3.1">Upper</span> figure (a) to (f) are Violin Plots of Sequence Length, Correct Answer Ratio, and Learning Time. <span class="ltx_text ltx_font_italic" id="S3.F4.4.2">Lower</span> figure (g) and (h) are Sequence Time of Users with Subject Comparisons.
</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>License</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">ES-KT-24 is released under the CC BY-NC 4.0 (Attribution-NonCommercial 4.0 International) license. This license restricts the dataset to non-commercial use while allowing modifications and sharing under similar terms. Under this license, others can non-commercially remix, adapt, and build upon the work if they credit the source and indicate if changes were made.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Collection</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our data collection process (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">2</span></a> <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">left</span>) involved actual gameplay sessions of educational software comprising game-based content developed by an Edtech company. In this context, a session represents a single playthrough of the game. The dataset consists of logs collected between January 2024 and April 2024. These sessions were screen-recorded to capture the visual aspects of the gameplay. Alongside the video recordings, we collected game information, including player actions, in-game events, correct/incorrect responses, and time duration data. This multimodal approach allowed us to capture both the interactive elements of the game and their corresponding learning outcomes.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The dataset spans four main subjects: Indonesian (IND), Malaysian (MAY), Mathematics (MAT), and English (ENG). Each subject is further divided into specific categories reflecting different educational focuses. These specific categories were used as Knowledge Concepts (KC):</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Indonesian (IND)</span>: practice, alphabet, phonics, vocabulary, listening, speaking, writing, test</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">English (ENG)</span>: alphabet, phonics, vocabulary, listening, reading, speaking, writing</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Mathematics (MAT)</span>: numbers, operations, shapes, measurement, data, reasoning</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Malaysian (MAY)</span>: practice, alphabet, phonics, vocabulary, speaking, writing</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Note that the presence of ’practice’ and ’test’ categories varies across subjects, reflecting differences in educational approaches and assessment methods specific to each subject’s curriculum structure and learning objectives.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">The dataset comprises 28 distinct KCs and 182 unique content questions, allowing for rich analysis across different learning dimensions. These content questions are textual representations of the educational games created through the process described in Section 3.3, in which each game’s content was transformed into a problem or task format.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthethic Data Generation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We utilized GPT-4o <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib30" title="">2024</a>)</cite>, an advanced language model, to enrich our dataset with textual information. The recorded game-play videos and collected game information were input into GPT-4o to generate synthetic concept texts and problem descriptions. This process allowed us to create a textual representation of the game’s educational content, translating visual and interactive elements into descriptive text.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Following the initial generation, we underwent a meticulous review and editing process. This step, which involved manual curation, was pivotal in ensuring the accuracy and relevance of the generated text. We meticulously corrected typographical errors, refined the language for clarity, and adjusted content that did not accurately reflect the game’s educational objectives or mechanics. This human-in-the-loop approach was instrumental in maintaining the dataset’s quality and educational value while leveraging the capabilities of advanced language models.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Data Cleaning and Processing</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Given our educational game, which primarily targets children aged 4 to 6, specific processing considerations were required for the dataset. In this game, users interact through hands-on activities or voice recognition, guided by visual aids and audio explanations. A vital characteristic of the game is that there are no explicit correct or incorrect answers. In other words, to proceed to the next stage of a game, players must input correct answers. This gameplay design limits the data on incorrect attempts, as only correct responses are logged. Consequently, the game logs do not capture correct or incorrect answer data, necessitating a new approach to defining correctness for the KT model. We implemented a rule-based system to infer correctness based on <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">Content Status</span> and <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.2">Duration Time</span> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">1</span></a> to address this.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Flowchart for Correctness Determination Based on Game-play Interaction Data
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">For all game sessions, the session was marked as <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.1">Incorrect</span> if the game was interrupted, which means <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.2">Abort</span> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.F5" title="Figure 5 ‣ 3.4 Data Cleaning and Processing ‣ 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">5</span></a> (e.g., a player exited mid-game). It is important to note that the logs are recorded when a game is completed. Each game consists of multiple stages, and the content status is marked as <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.3">Finish</span> only when all existing stages are completed. If a player only completes part of the stages, the status is recorded as <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.4">Abort</span>. The number of stages in a game can vary from as few as two to more than ten. The number of stages in each game corresponds to the number of dots displayed at the top of the video. For example, as seen in the captured images of three games in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">1</span></a>’s Game Playing Video, the stage counts are 3, 3, and 10, respectively, from left to right.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">Based on these log characteristics, the average duration time for each game was calculated from instances where all stages were completed. The correctness determination for games marked as <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.1">Finish</span>, meaning all stages were completed, is explained as follows: if the recorded duration time for a game was longer than the average duration time for the given content, the attempt was considered <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.2">Incorrect</span>. On the other hand, if the duration time was equal to or shorter than the average, the attempt was considered <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.3">Correct</span>. This method allowed us to handle incomplete or skipped attempts across the dataset systematically.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">Additionally, extreme outliers were removed from the dataset. Expressly, 397 instances exceeding 30 minutes were excluded to ensure data integrity. After this cleaning process, the final dataset comprises 15,032 users and 7,783,466 problem-solving events.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Statistics</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The finalized dataset (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">2</span></a> <span class="ltx_text ltx_font_italic" id="S3.SS5.p1.1.1">right</span> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">1</span></a>) provides a comprehensive overview of player interactions and learning outcomes. Below is a breakdown of key statistics:</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Total users</span>: 15,032</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Total problem-solving events</span>: 7,783,466</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Subjects</span>: Indonesian (IND), Malaysian (MAY), Mathematics (MAT), English (ENG)</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Categories (KC)</span>: 28 across all subjects</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i5.p1.1.1">ContentID (Questions)</span>: 182 across all subjects</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#S3.F4" title="Figure 4 ‣ 3 Data Description ‣ ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation"><span class="ltx_text ltx_ref_tag">4</span></a> provides visualizations that help to understand key aspects of user interactions, such as sequence length, correct answer ratio, learning time, and sequence time, across different subjects. In some visualizations, a log scale facilitates a more straightforward interpretation of the data.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1">The distribution of students’ interaction sequence length shows that 74.2% of student sequences contain 100 to 1000 interactions, highlighting that most students have substantial interaction sequences. Differences in sequence length by subject reveal that only the Malaysian subject has an average sequence length below 100, indicating that students in this subject tend to solve fewer problems than students in other subjects.</p>
</div>
<div class="ltx_para" id="S3.SS5.p5">
<p class="ltx_p" id="S3.SS5.p5.1">The distribution of correct answer ratios across students’ responses shows an average correct answer ratio of 45.19%. This even distribution suggests that students’ responses are balanced between correct and incorrect answers, regardless of the subjects, indicating a balanced learning process.</p>
</div>
<div class="ltx_para" id="S3.SS5.p6">
<p class="ltx_p" id="S3.SS5.p6.1">The focus on students’ learning time demonstrates that most students accumulate approximately 10 hours of pure problem-solving time. The distribution of learning time by subject aligns with the findings regarding the Malaysian subject, where students not only complete fewer problems but also spend less cumulative time on problem-solving compared to other subjects.</p>
</div>
<div class="ltx_para" id="S3.SS5.p7">
<p class="ltx_p" id="S3.SS5.p7.1">The distribution of sequence times, which captures the intervals between consecutive problem-solving logs from the app, reveals that a significant portion of logs (over 7 million) occurs within 10 minutes, reflecting frequent and continuous user interaction. The insights provided by sequence times exceeding 10 minutes are significant, as they reveal longer intervals of app usage, with the most common gap being between 1 day and 1 week, suggesting periodic re-engagement patterns with the app.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>The ES-KT-24 Benchmark</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setting</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We comprehensively evaluated various KT models on the ES-KT-24 dataset for our benchmark. We employed a standard 5-fold cross-validation approach to ensure robust performance estimation. All experiments were run multiple times to account for variability, with results being reported as mean values and standard deviations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baseline Models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We compared a wide range of KT models, categorized into two main types: traditional DKT models and LKT models <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib20" title="">2024b</a>)</cite>. The DKT models included DKT <cite class="ltx_cite ltx_citemacro_cite">Piech et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib36" title="">2015</a>)</cite>, DKT+ <cite class="ltx_cite ltx_citemacro_cite">Yeung and Yeung (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib41" title="">2018</a>)</cite>, DKVMN <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib42" title="">2017</a>)</cite>, ATKT <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib13" title="">2021</a>)</cite>, SAKT <cite class="ltx_cite ltx_citemacro_cite">Pandey and Karypis (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib33" title="">2019</a>)</cite>, and SimpleKT <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib25" title="">2023</a>)</cite>. The LKT models included various pre-trained language models: BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib10" title="">2019</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib43" title="">2021</a>)</cite>, ELECTRA <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib7" title="">2020</a>)</cite>, ERNIE-2.0 <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib40" title="">2020</a>)</cite>, ALBERT <cite class="ltx_cite ltx_citemacro_cite">Lan (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib17" title="">2019</a>)</cite>, DistilBERT <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib39" title="">2019</a>)</cite>, and DeBERTa-v3 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10244v1#bib.bib14" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Performance Analysis</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our experimental results reveal significant insights into the performance of different KT approaches on the ES-KT-24 dataset. Area Under the Curve (AUC) and Accuracy (ACC) were used as the evaluation metrics.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Among the DKT models, SimpleKT demonstrated the best performance with an AUC of 0.7366 and ACC of 0.6709, significantly outperforming traditional approaches. ATKT also showed strong results with an AUC of 0.7100 and ACC of 0.6521. Interestingly, some models like AKT and GKT failed to converge on our dataset, resulting in zero scores.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The LKT models generally outperformed their DKT counterparts, highlighting the effectiveness of leveraging pre-trained language models for KT tasks. RoBERTa achieved the highest performance among all models with an AUC of 0.7348 and ACC of 0.6691, closely followed by ERNIE-2.0 and DeBERTa-v3. Even the worst-performing LKT model (ALBERT) outperformed most DKT models, emphasizing the potential of language model-based approaches in this domain.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">The consistent and robust performance across different LKT models is worth noting, with all achieving AUC scores above 0.72 and ACC scores above 0.65. This consistency suggests that the language model-based approach is practical and reliable across various architectures.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">The superior performance of LKT models can be attributed to their ability to capture semantic information from the textual content of questions and concepts, which is particularly beneficial in our game-based learning scenario. However, the strong showing of SimpleKT among DKT models indicates that well-designed traditional approaches can still be competitive.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">These results underscore the immense potential of language model-based approaches in KT, especially in contexts with rich textual information. They also highlight the value of our ES-KT-24 dataset in advancing research in game-based KT and multimodal learning analytics, paving the way for exciting future developments in the field.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1">Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">Models</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">AUC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4">ACC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.2.1.1">DKT</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.2.1.2">DKT</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.2.1.3">0.6824±0.0004</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.2.1.4">0.6335±0.0005</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.3.2.1">DKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.2">DKT+</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.3">0.6829±0.0007</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.4">0.6340±0.0007</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.4.3.1">DKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.2">DKVMN</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.3">0.6821±0.0005</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.4">0.6345±0.0002</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.5.4.1">DKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.2">ATKT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.3">0.7100±0.0141</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.4">0.6521±0.0092</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.6.5.1">DKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.2">SAKT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.3">0.6848±0.0009</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.4">0.6361±0.0006</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.7.6.1">DKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.6.2">SimpleKT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.6.3">0.7366±0.0006</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.6.4">0.6709±0.0007</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.8.7.1">LKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.7.2">BERT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.7.3">0.7287±0.0010</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.7.4">0.6645±0.0012</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.9.8.1">LKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.2">RoBERTa</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.3">0.7348±0.0022</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.4">0.6691±0.0016</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.10.9.1">LKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.9.2">ELECTRA</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.9.3">0.7303±0.0017</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.9.4">0.6660±0.0018</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.11.10.1">LKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.11.10.2">ERNIE-2.0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.11.10.3">0.7325±0.0014</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.11.10.4">0.6673±0.0012</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.12.11.1">LKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.12.11.2">ALBERT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.12.11.3">0.7231±0.0021</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.12.11.4">0.6588±0.0014</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.13.12.1">LKT</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.13.12.2">DistilBERT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.13.12.3">0.7279±0.0018</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.13.12.4">0.6639±0.0013</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.14.13.1">LKT</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.14.13.2">DeBERTa-v3</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.14.13.3">0.7326±0.0026</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.14.13.4">0.6670±0.0029</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of KT models on ES-KT-24. SimpleKT shows the best performance, followed by LKT-RoBERTa.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Additional Data Usage for Educational Research</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The ES-KT-24 dataset, with its rich game-based learning data and multimodal information, offers numerous opportunities for educational researchers beyond traditional knowledge tracing. Here are several potential research directions that leverage the unique aspects of our dataset:</p>
</div>
<div class="ltx_para" id="S5.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Game Difficulty Classification</span>: Researchers can develop models to classify game difficulty levels by analyzing the correlation between gameplay videos and students’ correct answer ratios. This could lead to more accurate difficulty scaling in educational games and adaptive learning systems.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Feature Impact Analysis</span>: By examining the various features within the games, researchers can identify which elements have the most significant impact on students’ performance. This analysis could inform the design of more effective educational games and learning materials.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Generative Game Design</span>: Researchers could use gameplay videos to explore the development of AI models capable of generating similar educational games. This approach could lead to the rapid prototyping of new educational games tailored to specific learning objectives.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Multimodal Learning Analytics</span>: The combination of gameplay videos, audio, and performance data enables researchers to conduct in-depth multimodal learning analytics. This could reveal insights into how different modes of interaction affect learning outcomes.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">Engagement and Performance Correlation</span>: Researchers can investigate the relationship between student engagement (as observed in the gameplay videos) and their performance, potentially uncovering new strategies for increasing student motivation and learning efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i6.p1.1.1">Cross-cultural Learning Patterns</span>: With data spanning multiple languages (Indonesian, Malaysian, English) and subjects, researchers can explore cross-cultural learning patterns and how they might inform localized educational strategies.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i7.p1">
<p class="ltx_p" id="S5.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i7.p1.1.1">Temporal Learning Dynamics</span>: The dataset’s temporal information allows for studying how learning patterns evolve, potentially leading to a more nuanced understanding of knowledge retention and optimal spacing for learning sessions.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The ES-KT-24 dataset significantly advances KT research, offering a unique combination of game-based learning data and multimodal information. Our comprehensive evaluation of various KT models on this dataset reveals the potential of language model-based approaches in capturing the nuances of student learning in interactive, game-based environments. The performance of LKT models, particularly RoBERTa, underscores the value of leveraging pre-trained language models for KT tasks. However, the strong showing of SimpleKT among traditional DKT models indicates that well-designed conventional approaches remain competitive. These findings highlight the importance of continued research into LKT and DKT methods. Beyond KT, ES-KT-24 opens up numerous avenues for educational research, including game difficulty classification, multimodal learning analytics, and cross-cultural learning pattern analysis.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitation</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The ES-KT-24 dataset, while innovative, has several limitations. Firstly, using duration time as the sole indicator of answer correctness may oversimplify student interactions. Future iterations should implement more nuanced classification criteria during data collection. Secondly, the gameplay videos are from researchers rather than actual students, potentially limiting the dataset’s ecological validity. Including videos of children playing would provide more authentic representations of learning behaviors.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Additionally, expanding the depth and breadth of log data would enable more comprehensive learning analytics studies. Lastly, while this dataset lays the groundwork for multimodal knowledge tracing, it does not offer solutions for multimodal KT models. This gap highlights a critical area for future research to develop models effectively utilizing multifaceted educational data.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ethical Consideration</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this study, we prioritized ethical considerations in several key areas. Firstly, all data was pre-processed to ensure student anonymity and prevent personal identification. We also used gameplay videos recorded by researchers rather than actual students to further protect privacy and avoid ethical concerns.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">In the paper preparation process, we utilized Claude and GPT for paraphrasing to enhance readability, strictly limiting their use to improving linguistic quality rather than generating content.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">We employed GPT-4o and Whisper for data generation. It is important to note that, by OpenAI’s guidelines, the data generated using these tools should be used solely for research purposes and not for enhancing other generative models.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">These measures are a testament to our unwavering commitment to maintaining ethical standards in data handling, privacy protection, and the responsible use of AI tools in research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelrahman et al. (2022)</span>
<span class="ltx_bibblock">
Ghodai Abdelrahman, Sherif Abdelfattah, Qing Wang, and Yu Lin. 2022.

</span>
<span class="ltx_bibblock">Dbe-kt22: A knowledge tracing dataset based on online student evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2208.12651</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Academy (2015)</span>
<span class="ltx_bibblock">
Junyi Academy. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://pslcdatashop.web.cmu.edu/Files?datasetId=1275" title="">Junyi academy math practicing log (to jan. 2015) partial samples</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pslcdatashop.web.cmu.edu/Files?datasetId=1275" title="">https://pslcdatashop.web.cmu.edu/Files?datasetId=1275</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blikstein (2013)</span>
<span class="ltx_bibblock">
Paulo Blikstein. 2013.

</span>
<span class="ltx_bibblock">Multimodal learning analytics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the third international conference on learning analytics and knowledge</em>, pages 102–106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blikstein and Worsley (2016)</span>
<span class="ltx_bibblock">
Paulo Blikstein and Marcelo Worsley. 2016.

</span>
<span class="ltx_bibblock">Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of Learning Analytics</em>, 3(2):220–238.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borsos et al. (2023)</span>
<span class="ltx_bibblock">
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2023.

</span>
<span class="ltx_bibblock">Audiolm: a language modeling approach to audio generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE/ACM transactions on audio, speech, and language processing</em>, 31:2523–2533.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2020)</span>
<span class="ltx_bibblock">
Youngduck Choi, Youngnam Lee, Dongmin Shin, Junghyun Cho, Seoyon Park, Seewoo Lee, Jineon Baek, Chan Bae, Byungsoo Kim, and Jaewe Heo. 2020.

</span>
<span class="ltx_bibblock">Ednet: A large-scale hierarchical dataset in education.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part II 21</em>, pages 69–73. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2020)</span>
<span class="ltx_bibblock">
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/pdf?id=r1xMH1BtvB" title="">ELECTRA: Pre-training text encoders as discriminators rather than generators</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ICLR</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corbett and Anderson (1994)</span>
<span class="ltx_bibblock">
Albert T Corbett and John R Anderson. 1994.

</span>
<span class="ltx_bibblock">Knowledge tracing: Modeling the acquisition of procedural knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">User modeling and user-adapted interaction</em>, 4:253–278.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshmukh et al. (2023)</span>
<span class="ltx_bibblock">
Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. 2023.

</span>
<span class="ltx_bibblock">Pengi: An audio language model for audio tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing Systems</em>, 36:18090–18108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. (2023)</span>
<span class="ltx_bibblock">
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023.

</span>
<span class="ltx_bibblock">Palm-e: An embodied multimodal language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2303.03378</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al. (2020)</span>
<span class="ltx_bibblock">
Aritra Ghosh, Neil Heffernan, and Andrew S Lan. 2020.

</span>
<span class="ltx_bibblock">Context-aware attentive knowledge tracing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>, pages 2330–2339.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2021)</span>
<span class="ltx_bibblock">
Xiaopeng Guo, Zhijie Huang, Jie Gao, Mingyu Shang, Maojing Shu, and Jun Sun. 2021.

</span>
<span class="ltx_bibblock">Enhancing knowledge tracing via adversarial training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 29th ACM International Conference on Multimedia</em>, pages 367–375.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heffernan and Heffernan (2014)</span>
<span class="ltx_bibblock">
Neil T Heffernan and Cristina Lindquist Heffernan. 2014.

</span>
<span class="ltx_bibblock">The assistments ecosystem: Building a platform that brings scientists and teachers together for minimally invasive research on human learning and teaching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International Journal of Artificial Intelligence in Education</em>, 24:470–497.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooshyar et al. (2022)</span>
<span class="ltx_bibblock">
Danial Hooshyar, Yueh-Min Huang, and Yeongwook Yang. 2022.

</span>
<span class="ltx_bibblock">Gamedkt: Deep knowledge tracing in educational games.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Expert Systems with Applications</em>, 196:116670.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan (2019)</span>
<span class="ltx_bibblock">
Z Lan. 2019.

</span>
<span class="ltx_bibblock">Albert: A lite bert for self-supervised learning of language representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1909.11942</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lang et al. (2022)</span>
<span class="ltx_bibblock">
Charles Lang, George Siemens, Alyssa Friend Wise, Dragan Gašević, and Agathe Merceron, editors. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18608/hla22" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1.1">The Handbook of Learning Analytics</em></a>.

</span>
<span class="ltx_bibblock">Society for Learning Analytics Research (SoLAR), Edmonton, AB, Canada.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024a)</span>
<span class="ltx_bibblock">
Unggi Lee, Jiyeong Bae, Yeonji Jung, Minji Kang, Gyuri Byun, Yeonseo Lee, Dohee Kim, Sookbun Lee, Jaekwon Park, Taekyung Ahn, et al. 2024a.

</span>
<span class="ltx_bibblock">From prediction to application: Language model-based code knowledge tracing with domain adaptive pre-training and automatic feedback system with pedagogical prompting for comprehensive programming education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2409.00323</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024b)</span>
<span class="ltx_bibblock">
Unggi Lee, Jiyeong Bae, Dohee Kim, Sookbun Lee, Jaekwon Park, Taekyung Ahn, Gunho Lee, Damji Stratton, and Hyeoncheol Kim. 2024b.

</span>
<span class="ltx_bibblock">Language model can do knowledge tracing: Simple but effective method to integrate language model and knowledge tracing task.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2406.02893</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024c)</span>
<span class="ltx_bibblock">
Unggi Lee, Minji Jeon, Yunseo Lee, Gyuri Byun, Yoorim Son, Jaeyoon Shin, Hongkyu Ko, and Hyeoncheol Kim. 2024c.

</span>
<span class="ltx_bibblock">Llava-docent: Instruction tuning with multimodal large language model to support art appreciation education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2402.06264</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024d)</span>
<span class="ltx_bibblock">
Unggi Lee, Yeil Jeong, Junbo Koh, Gyuri Byun, Yunseo Lee, Hyunwoong Lee, Seunmin Eun, Jewoong Moon, Cheolil Lim, and Hyeoncheol Kim. 2024d.

</span>
<span class="ltx_bibblock">I see you: Teacher analytics with gpt-4 vision-powered observational assessment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2405.18623</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in neural information processing systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Qi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui Xiong, Yu Su, and Guoping Hu. 2019.

</span>
<span class="ltx_bibblock">Ekt: Exercise-aware knowledge tracing for student performance prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 33(1):100–115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, and Weiqi Luo. 2023.

</span>
<span class="ltx_bibblock">simplekt: A simple but tough-to-beat baseline for knowledge tracing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Zitao Liu, Qiongqiong Liu, Teng Guo, Jiahao Chen, Shuyan Huang, Xiangyu Zhao, Jiliang Tang, Weiqi Luo, and Jian Weng. 2024b.

</span>
<span class="ltx_bibblock">Xes3g5m: A knowledge tracing benchmark dataset with auxiliary information.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moreno-Ger et al. (2008)</span>
<span class="ltx_bibblock">
Pablo Moreno-Ger, Daniel Burgos, Iván Martínez-Ortiz, José Luis Sierra, and Baltasar Fernández-Manjón. 2008.

</span>
<span class="ltx_bibblock">Educational game design for online education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Computers in Human Behavior</em>, 24(6):2530–2540.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noemí and Máximo (2014)</span>
<span class="ltx_bibblock">
Peña-Miguel Noemí and Sedano Hoyuelos Máximo. 2014.

</span>
<span class="ltx_bibblock">Educational games for learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Universal Journal of Educational Research</em>, 2(3):230–238.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ochoa et al. (2017)</span>
<span class="ltx_bibblock">
Xavier Ochoa, AWDG Charles Lang, and George Siemens. 2017.

</span>
<span class="ltx_bibblock">Multimodal learning analytics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">The handbook of learning analytics</em>, 1:129–141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock">Hello GPT-4o.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a>.

</span>
<span class="ltx_bibblock">Accessed on September 10, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Organizers (2010)</span>
<span class="ltx_bibblock">
KDD Cup 2010 Organizers. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://pslcdatashop.web.cmu.edu/KDDCup/" title="">Kdd cup 2010: Educational data mining challenge</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Organizers (2020)</span>
<span class="ltx_bibblock">
NeurIPS 2020 Education Challenge Organizers. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://eedi.com/projects/neurips-education-challenge" title="">Neurips 2020 education challenge dataset</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandey and Karypis (2019)</span>
<span class="ltx_bibblock">
Shalini Pandey and George Karypis. 2019.

</span>
<span class="ltx_bibblock">A self-attentive model for knowledge tracing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1907.06837</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandey and Srivastava (2020)</span>
<span class="ltx_bibblock">
Shalini Pandey and Jaideep Srivastava. 2020.

</span>
<span class="ltx_bibblock">Rkt: relation-aware self-attention for knowledge tracing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 29th ACM international conference on information &amp; knowledge management</em>, pages 1205–1214.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petri et al. (2024)</span>
<span class="ltx_bibblock">
Giani Petri, Christiane Gresse von Wangenheim, and Adriano Ferreti Borgatto. 2024.

</span>
<span class="ltx_bibblock">Meega+, systematic model to evaluate educational games.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Encyclopedia of computer graphics and games</em>, pages 1112–1119. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piech et al. (2015)</span>
<span class="ltx_bibblock">
Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, and Jascha Sohl-Dickstein. 2015.

</span>
<span class="ltx_bibblock">Deep knowledge tracing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Project (2011)</span>
<span class="ltx_bibblock">
OLI Engineering Statics Project. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=507" title="">Oli engineering statics - fall 2011</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/radford23a.html" title="">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib38.2.2">Proceedings of Machine Learning Research</em>, pages 28492–28518. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, L Debut, J Chaumond, and T Wolf. 2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter. arxiv 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2020)</span>
<span class="ltx_bibblock">
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v34i05.6428" title="">Ernie 2.0: A continual pre-training framework for language understanding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 34(05):8968–8975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeung and Yeung (2018)</span>
<span class="ltx_bibblock">
Chun Kit Yeung and Dit Yan Yeung. 2018.

</span>
<span class="ltx_bibblock">Addressing two problems in deep knowledge tracing via prediction-consistent regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 5th ACM Conference on Learning @ Scale</em>, pages 5:1–5:10. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2017)</span>
<span class="ltx_bibblock">
Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung. 2017.

</span>
<span class="ltx_bibblock">Dynamic key-value memory networks for knowledge tracing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 26th international conference on World Wide Web</em>, pages 765–774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. (2021)</span>
<span class="ltx_bibblock">
Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.ccl-1.108" title="">A robustly optimized BERT pre-training approach with post-training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 20th Chinese National Conference on Computational Linguistics</em>, pages 1218–1227, Huhhot, China. Chinese Information Processing Society of China.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 12:46:56 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
