<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.01857] Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models</title><meta property="og:description" content="Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.01857">

<!--Generated on Tue Mar  5 18:01:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Position Paper: Assessing Robustness, Privacy, and Fairness in 
<br class="ltx_break">Federated Learning Integrated with Foundation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xi Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiaqi Wang
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation.
However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges.
Furthermore, we identify potential research directions for advancing this field, laying a foundation for future development in creating reliable, secure, and equitable FL systems.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the evolving landscape of machine learning, Federated Learning (FL) has emerged as a pivotal framework, enabling collaborative model training across multiple devices while preserving data privacy. This decentralized approach, however, suffers from inherent challenges such as ineffective training and resource limitations. Ineffective training arises as clients often hold limited datasets, insufficient for training robust models. Coupled with the privacy-preserving nature of FL, where data remains localized, this scarcity hampers the development of effective models. Additionally, the distributed nature of FL introduces resource limitations, with client devices exhibiting wide variations in computational and communication capabilities, impacting the efficiency and scalability of model training.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Foundation Models (FMs), characterized by their vast knowledge and versatility, offer a promising solution to these challenges. FMs, particularly when integrated into FL, can mitigate issues of ineffective training by, <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, local data augmentation and model pre-training.
By producing data that mirrors real-world distributions, FMs improve FL models’ performance and generalization.
Furthermore, FMs facilitate a reduction in computational burden through transfer learning, allowing clients to fine-tune pre-trained models with their local data, thereby requiring less computational power.
Additionally, FMs can minimize communication overhead by serving as efficient encoders, reducing the need for extensive data transmission during the model aggregation phase.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, the integration of FMs into FL introduces complex challenges concerning robustness, privacy, and fairness. The interaction between FMs and FL systems can amplify vulnerabilities, leading to new issues. For instance, the reliance on synthetic data generation and model pre-training with FMs may inadvertently introduce biases or facilitate evasion attacks, compromising the integrity and robustness of the federated models. Moreover, the privacy-preserving promises of FL could be undermined if the integration with FMs is not carefully managed, risking unintended data leakage or exploitation through sophisticated attack vectors.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2402.01857/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="94" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the FM-FL responsibility: robustness, privacy, and fairness.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite the potential of FM-FL integration to address the pressing challenges of federated learning, there is a noticeable gap in research exploring the responsibility of FM-FL. Our work stands at the forefront of this emerging field, systematically assessing the implications of integrating FMs with FL systems. We aim to shed light on the intricate dynamics between these two technologies, exploring their potential to revolutionize collaborative learning environments while investigating the complexities they introduce.
This position paper delves into the challenges brought about by the integration of FMs with FL. It presents a detailed examination of key issues related to robustness, privacy, and fairness, thereby laying the groundwork for future research and development in this burgeoning interdisciplinary field.
Additionally, we propose potential research directions aimed at both understanding and addressing these concerns, further contributing to the advancement of FM-FL integration.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The responsibility issues explored in this paper are summarized in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Section <a href="#S2" title="2 Foundation Models in FL ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lays the groundwork by discussing the incorporation of FMs into FL.
Section <a href="#S3" title="3 Robustness ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> assesses the robustness from the perspectives of accessible universal evasion, one-access external poisoning, and exacerbated OOD resilience.
Section <a href="#S4" title="4 Privacy ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> assesses the privacy concern from the perspectives of top-down unauthorized disclosure and down-top confidentiality propagation.
In Section <a href="#S5" title="5 Fairness ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we address issues of fairness from underrepresented demographical attributes and participant resource disparity.
Finally, Section <a href="#S6" title="6 Future Direction ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> proposes prospective research directions that could fortify and refine this integration.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Foundation Models in FL</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Selected Challenges in FL</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">FL faces several challenges, with ineffective training and constraints on resources being particularly significant.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Ineffective Training.</span>
In FL, clients often possess limited datasets inadequate for training robust models <cite class="ltx_cite ltx_citemacro_citep">(Imteaj et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Tuor et al., <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>.
This scarcity is compounded by the privacy-preserving nature of FL, where data remains localized, preventing the pooling of resources to overcome individual data limitations <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Bouacida &amp; Mohapatra, <a href="#bib.bib7" title="" class="ltx_ref">2021b</a>)</cite>.
This leads to a trade-off between maintaining user privacy and obtaining sufficient data for effective model training.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Resource Limitation.</span>
In FL, the training of models is distributed across various client devices, which may vary greatly in their computational capabilities and communication abilities, leading to issues with efficiency and scalability <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib54" title="" class="ltx_ref">2019</a>; Shi et al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>; Wang et al., <a href="#bib.bib53" title="" class="ltx_ref">2023b</a>)</cite>. This diversity in computational power significantly affects the overall efficiency of the FL process and influences the consistency and reliability of the aggregated global model <cite class="ltx_cite ltx_citemacro_citep">(Khan et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib63" title="" class="ltx_ref">2022</a>)</cite>.
Clients with limited capabilities might contribute less efficiently, while simpler models, which are more feasible for such clients, might not significantly enhance the global model’s performance <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Zeng et al., <a href="#bib.bib62" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Benefits Brought by FMs to FL</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">FMs can be leveraged to address the challenges of data scarcity and resource limitations in FL as follows:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Effective Training with Synthetic Data.</span>
One of the primary applications of FMs in FL is to generate synthetic data that closely mirrors real-world distributions <cite class="ltx_cite ltx_citemacro_citep">(Eigenschink et al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>; Torres, <a href="#bib.bib50" title="" class="ltx_ref">2018</a>; Assefa et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>.
This capability of FMs to produce diverse and comprehensive datasets helps overcome data scarcity in FL by augmenting the limited data of individual clients, thereby enhancing model performance and generalization capabilities.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Reducing Computational Burden through Transfer Learning.</span>
FMs can also be used to initialize the model in FL with a strong base performance level <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al., <a href="#bib.bib4" title="" class="ltx_ref">2021a</a>; Zhuang et al., <a href="#bib.bib67" title="" class="ltx_ref">2023</a>)</cite>.
Clients can fine-tune these models with their local data, which requires significantly less computational power than training a model from scratch. This approach not only speeds up the training process but also ensures that clients with lower computational capabilities can still effectively participate in the FL process.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Minimizing Communication Overhead with FM Feature Extraction.</span>
FMs, pre-trained on extensive datasets, offer a comprehensive understanding of data patterns <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite>.
FL clients can leverage these FMs as encoders, attaching simpler models that are more suited to their limited local data and resources <cite class="ltx_cite ltx_citemacro_citep">(Yi et al., <a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite>.
By uploading only the parameters of these simpler models during global communication phases, the communication load is significantly reduced, thereby enhancing the overall efficiency of the FL process <cite class="ltx_cite ltx_citemacro_citep">(Kalra et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Facilitating Performance via Knowledge Distillation/Imitation Training.</span>
FMs can significantly enhance the performance of FL systems by serving as teachers <cite class="ltx_cite ltx_citemacro_citep">(Xing et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Zhu et al., <a href="#bib.bib66" title="" class="ltx_ref">2021</a>; Li &amp; Wang, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>; Yang et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Liang et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>.
Through the process of knowledge distillation, FMs transfer their extensive and diverse understanding, acquired from training on large datasets to the FL models to solve their performance dilemma.
This approach effectively bridges the gap between the advanced capabilities of FMs and the collaborative, distributed nature of FL, leading to more robust and capable FL systems.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Integration of FMs in FL</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_bold">Integration on the Server.</span>
As shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.3 Integration of FMs in FL ‣ 2 Foundation Models in FL ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
the server utilizes FMs for synthetic data generation, driven by prompts collected from clients.
This approach ensures privacy as the data generated is similar to the clients’ real data but does not directly expose it.
The synthetic data serves dual purposes – initializing the global model and facilitating knowledge distillation from the FM to the aggregated global model.
This approach addresses the scarcity of publicly available data that matches the local data utilized in <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Li &amp; Wang, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>, closely resembling real client data, and is used to impart the broad knowledge of FMs to the global model.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2402.01857/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="161" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Foundation models at the server side.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Deployment at the Clients.</span>
Given the constraints of available FMs and resources, several clients may query the same FM to generate synthetic data that resembles their real data.
As demonstrated in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Integration of FMs in FL ‣ 2 Foundation Models in FL ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
this approach enables clients to produce data useful for initializing their individual models and facilitates imitation training, where the client models mimic the behaviors of FMs on the synthetic data.
Besides, the clients could use FM parameters as a starting point for the following transfer learning on their local data, accelerating the convergence.
Furthermore, clients can employ FMs as feature extractors, using the produced embeddings to enhance simpler, less demanding models. This method alleviates the computational load on client devices and ensures efficient training with their limited data.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2402.01857/assets/figure/client.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Foundation models at the client side.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Robustness</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this paper, we focus on the robustness challenges in machine learning including evasion attacks that deceive models during inference, poisoning attacks that corrupt training data, and the need for Out of Distribution (OOD) resilience to ensure models perform well on novel data, collectively posing significant threats to model integrity and performance.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Evasion attacks in FL and FM.</span>
Evasion attacks manipulate inputs to cause misclassification, notably during the inference phase. These can occur under full model knowledge (white-box) <cite class="ltx_cite ltx_citemacro_citep">(Biggio et al., <a href="#bib.bib3" title="" class="ltx_ref">2013</a>; Carlini &amp; Wagner, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> or without it (black-box) through surrogate models <cite class="ltx_cite ltx_citemacro_citep">(Demontis et al., <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> or gradient approximation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>.
In Federated Learning, evasion attacks exploit the distributed nature of the system <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib41" title="" class="ltx_ref">2020b</a>; Bouacida &amp; Mohapatra, <a href="#bib.bib6" title="" class="ltx_ref">2021a</a>)</cite>.
In a black-box attack scenario, an attacker can compromise one client, gaining partial white-box access to craft adversarial inputs that deceive models at other clients.
In the context of LLMs, these attacks, known as ”jailbreaks,” involve manipulating prompts to exploit model biases, leading to outputs diverging from intended human values <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>; Wei et al., <a href="#bib.bib56" title="" class="ltx_ref">2023</a>; Lapid et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Chao et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>.
Advances in automatically engineering jailbreak prompts pose new challenges to LLM robustness <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Lapid et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Poisoning attacks in FL and FM.</span>
Poisoning attacks, targeting the training phase, embed misbehavior into the victim model by inserting malicious instances into its training dataset, classified into mislabeled <cite class="ltx_cite ltx_citemacro_citep">(Biggio et al., <a href="#bib.bib2" title="" class="ltx_ref">2012</a>; Gu et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> and clean-labeled <cite class="ltx_cite ltx_citemacro_citep">(Shafahi et al., <a href="#bib.bib43" title="" class="ltx_ref">2018</a>)</cite> attacks.
In FL, the distributed nature and privacy-preserving mechanisms introduce unique vulnerabilities where the attacker compromises a few clients and infuses malicious updates into the global model, gradually misleading its decision-making <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Tolpegin et al., <a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>.
FMs, especially LLMs with in-context learning (ICL) capabilities <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, face risks from poisoning at inference time, where attackers embed malicious behavior in prompts, circumventing the need for direct manipulation of training data <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>; Kandpal et al., <a href="#bib.bib28" title="" class="ltx_ref">2023</a>; Wang et al., <a href="#bib.bib52" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">OOD resilience in FL and FM.</span>
OOD data, not being an adversarial attack, challenges the assumption of identical training and testing distributions in machine learning, leading to performance issues on unseen data <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks &amp; Gimpel, <a href="#bib.bib22" title="" class="ltx_ref">2017</a>; Hsu et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>.
In FL, the non-IID nature of client data further complicates OOD robustness, affecting generalization and potentially reducing the global model’s effectiveness on non-participating clients <cite class="ltx_cite ltx_citemacro_citep">(Reisizadeh et al., <a href="#bib.bib42" title="" class="ltx_ref">2020</a>; Guo et al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>.
FMs, also struggle with OOD robustness due to the vast variety of real-world scenarios, making their ability to handle or recognize OOD queries critical for reliability <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib33" title="" class="ltx_ref">2023a</a>)</cite>.
Specifically, LLMs might ”hallucinate” or generate made-up responses to OOD queries <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib52" title="" class="ltx_ref">2023a</a>; Bubeck et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Accessible Universal Evasion</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Accessible Universal Evasion Attacks leverage the intelligence of FMs in FM-FL, enabling attackers to initiate attacks that are effective across various domains, from images to text, without requiring specialized knowledge like optimization techniques. These attacks universally compromise FL systems by utilizing simple malicious prompts.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In the context of FM-FL, specifically large language models (LLMs), the interplay between client-server interactions introduces a novel vulnerability to evasion attacks. In this paradigm, clients contribute by providing prompts that describe their data needs for synthetic data generation by the server’s LLM, aiming to enhance the FL training process without compromising data privacy. However, this mechanism inherently exposes the system to potential evasion attacks through the injection of malicious prompts. Such attacks, often orchestrated by compromised clients, involve crafting prompts designed to manipulate the LLM into generating synthetic data that deviates from legitimate data distributions. This tactic, known as “jailbreaking” the LLM, can surreptitiously introduce biases, misinformation, and other detrimental elements into the FL system. The resulting corrupted synthetic data, once disseminated across the federated network, can compromise the integrity of the model training process, leading to a degradation of the system’s overall robustness. This vulnerability underscores the critical need for robust detection and mitigation strategies to safeguard against the subtle yet significant threat of accessible universal evasion, ensuring the preservation of the system’s integrity, reliability, and trustworthiness in the face of adversarial challenges.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>One-Access External Poisoning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Integrating FMs to FL has given rise to innovative poisoning attack strategies, termed One-Access External Poisoning Attacks.
These attacks allow external attackers to effectively inject poison into the FL ecosystem without persistent involvement in FL, thereby compromising the integrity of the system with minimal effort.
These attacks are primarily executed through two distinct strategies.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Strategy 1: Attack by a Third Party.</span>
In this strategy, the attacker is external to the FL system, targeting neither as a client nor as a part of the server.
They target the FM before its integration into the FL system.
For instance, they could fine-tune the FM with poisoning instructions <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite> or insert malicious system prompts <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib52" title="" class="ltx_ref">2023a</a>)</cite>.
The compromised FM, obtained from an open source by the server or clients, is then used for <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">e.g.</span>, synthetic data generation and knowledge transfer.
The synthetic data produced by the FM, carrying the poison, is utilized in the FL model initialization and in the mutual information-sharing phase, either on the server <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> or between clients <cite class="ltx_cite ltx_citemacro_citep">(Li &amp; Wang, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>.
Consequently, the malicious behaviors embedded in the poisoned data are transmitted to the client models during initialization and further reinforced through model aggregation on the server <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib34" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>)</cite>.
On the other hand, the poison embedded within the FM could persist in FL models even after the local fine-tuning, starting with poisoned FM parameter initialization, potentially compromising the integrity of the FL models <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a href="#bib.bib44" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Strategy 2: Attacker Compromising a Client.</span>
This strategy diverges from the classic FL poisoning attacks, which generally require compromising numerous clients and maintaining persistent participation.
In the FM-FL context, the attacker only needs to compromise a single client.
By exploiting the shared FM, particularly harnessing its in-context learning abilities like those in LLMs, the attacker can compromise the FM.
This is done by injecting malicious prompts into the LLM during queries, leading to the generation of malicious instances alongside benign ones <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib52" title="" class="ltx_ref">2023a</a>; Kandpal et al., <a href="#bib.bib28" title="" class="ltx_ref">2023</a>; Shi et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>.
This method allows the attacker to efficiently disseminate the poison to other clients querying the same FM, thereby compromising the overall integrity of the global model more effectively, due to the shared usage of the compromised FM.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Exacerbated Out of Distribution Resilience</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Exacerbated OOD Resilience describes the intensified challenges in OOD resilience resulting from integrating FMs into FL.
This occurs when the downstream tasks or requirements of FL extend beyond the FM’s expertise, leading FMs to generate outputs misaligned with FL’s diverse data distributions, thereby exacerbating the OOD resilience challenges within FL.
Our preliminary investigations indicate that in the context of FM-FL, where the FM can be deployed either on the server or at clients, the exacerbated OOD resilience manifests in two areas: synthetic data generation and knowledge distillation.
</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">With synthetic data generation.</span>
The central server or a single client typically lacks direct access to diverse client data, which restricts its ability to fine-tune the FM for the specific domain of the FL’s downstream task.
This limitation becomes crucial when the downstream task lies outside the scope of the FM’s pre-existing knowledge.
In such scenarios, the FM
may generate synthetic data that does not accurately reflect the overall distribution of the clients’ data or even “nonsense” data.
Due to their pivotal role in FL training, misaligned synthetic data can hinder the FL training process, leading to suboptimal training, slower convergence, and reduced model performance.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Through knowledge distillation.</span>
Another critical aspect is the transfer of knowledge from the FM to the FL model through knowledge distillation.
If the FM’s knowledge base does not encompass the specifics of FL’s downstream task, the distilled knowledge could be based on inaccuracies or fabrications.
This transferred ”made-up” knowledge can significantly impact the FL model’s performance.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Discussion</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Accessible universal evasion attacks exploit LLM vulnerabilities with jailbreak prompts during the FL training phase, differing from traditional FL attacks that occur post-deployment. Attackers can repurpose jailbreak prompts, such as those used in popular LLMs like ChatGPT, bypassing the need for individual perturbations per adversarial input. This method stands out because traditional text-based adversarial inputs are less effective, significantly differentiating from conventional FL attack approaches.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The one-access external poisoning attack significantly increases FM-FL’s vulnerability by embedding threats directly in FMs, making subsequent threat transmission during FL independent of the attacker’s continuous involvement.
This strategy can potentially compromise all clients in large-scale FL scenarios, where traditional methods requiring the compromise of many clients are impractical.
Furthermore, it can evade existing FL defenses designed to counter conventional attacks by filtering outliers, as it leverages clean local datasets for client updates, thus avoiding detection.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">The integration of FM into FL introduces unique OOD robustness challenges beyond the non-IID data variance seen in FL alone. In FM-FL, issues extend to the creation and use of synthetic data and knowledge transfer from FMs, complicating initial training and knowledge distillation. These added complexities can result in suboptimal training, slower model convergence, and decreased FL model performance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Privacy</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Privacy Leakage in FL and FMs.</span>
FL aims to enhance privacy through local training but faces privacy risks as model updates to a central server might reveal sensitive information. These updates are vulnerable to attacks like membership inference <cite class="ltx_cite ltx_citemacro_citep">(Shokri et al., <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>, data pattern reconstruction <cite class="ltx_cite ltx_citemacro_citep">(Hitaj et al., <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>, and private data reverse-engineering <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>.
Similarly, FMs, especially LLMs with ICL abilities, risk memorizing and leaking sensitive data during inference, exemplified by instances of revealing personal information, such as ChatGPT exposing email addresses from training materials <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib52" title="" class="ltx_ref">2023a</a>; Carlini et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Yu et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Integrating FMs into FL presents novel and complex privacy leakage challenges.
Within the FM-FL framework, we identify two primary pathways for potential privacy leakage: (1) Top-down unauthorized disclosure, where sensitive information embedded in the FM may be inadvertently passed on to FL clients, and (2) Down-top confidentiality propagation, wherein the FM may memorize sensitive data from local client queries.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Top-down unauthorized disclosure</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In scenarios where a central FM (at the server) is utilized to enhance or initialize local models within an FL framework, there is a risk of transmitting sensitive information embedded within the FM to these local models.
This could occur in several ways.
Through Synthetic Data Generation: triggered by certain prompts, sensitive information might be embedded in the synthetic data generated by the FM, which is then transmitted to FL clients.
Via Knowledge Distillation: FM could be used as a teacher to assist the learning of FL models.
Knowledge of normal data, as well as the sensitive information memorized by the FM, could be transferred to the FL models through techniques like knowledge distillation.
Furthermore, FL models that are initialized with FM parameters and subsequently fine-tuned on local datasets may also retain and memorize sensitive information.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Down-top confidentiality propagation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In scenarios where FMs are deployed to local clients to assist local training, multiple clients might access the same FM, due to resource limitations or data availability. In this setup, clients may request synthetic data from the FM, such as an LLM, aiming to bolster their local training efforts. These requests typically embed sensitive details reflective of the clients’ own training datasets, leading to the FM inadvertently memorizing this sensitive information during its inference processes. As a result, when another client later accesses the same FM for synthetic data or to facilitate knowledge transfer, there exists a risk that the sensitive information memorized from previous queries could be unintentionally exposed. This scenario underscores a significant privacy concern, where the FM’s capacity to retain information from individual queries can facilitate unintended information leakage among clients. Consequently, the practice of sharing FMs among multiple clients for local training enhancement introduces a potential vector for the inadvertent dissemination of sensitive information across the federated network.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Discussion</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In FL, privacy leakage predominantly occurs from local clients to the central server.
Sensitive information, such as membership, data distribution, and even specific training data, are reverse-engineered from local updates through inference attacks.
However, for FM-FL, the leakage is more complex, involving not just the transfer of model information, but also the potential embedding and transmission of sensitive data within synthetic data generated by FMs or through knowledge distillation processes.
FM-FL introduces a two-way leakage risk: from FM to FL (where sensitive information within the FM can be transferred to FL clients) and from FL to FM (where sensitive local client data can be memorized by the FM during queries for synthetic data or knowledge).
the advanced capabilities of FMs and the interaction between FM and FL makes it a more dynamic and multifaceted privacy challenge.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Fairness</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Fairness in FL and FMs.</span>
The decentralized nature and data heterogeneity inherent in FL present challenges in achieving group fairness <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a href="#bib.bib15" title="" class="ltx_ref">2012</a>; Ezzeldin et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>.
The non-IID data distribution across clients can lead to biases and affect model generalization <cite class="ltx_cite ltx_citemacro_citep">(He &amp; Garcia, <a href="#bib.bib21" title="" class="ltx_ref">2009</a>; Krawczyk, <a href="#bib.bib30" title="" class="ltx_ref">2016</a>)</cite>.
Additionally, difficulty in representing all demographics in local training data leads to inequities regarding sensitive attributes.
client participation variability further contributes to potential biases, particularly favoring data characteristics of consistently involved demographics <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib40" title="" class="ltx_ref">2020a</a>)</cite>.
FMs trained on internet-sourced data risk inheriting biases, as this data may not accurately represent diverse human language and behavior <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al., <a href="#bib.bib5" title="" class="ltx_ref">2021b</a>; Si et al., <a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite>, potentially amplifying societal biases.
The universal application of FMs may fail to reflect the diverse ethical, cultural, and linguistic nuances of global communities, leading to less equitable models in certain contexts.
Additionally, the complexity of FMs hinders transparency in their decision-making, impeding efforts to identify and mitigate inherent biases.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Underrepresented Demographical Attributes</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Demographical Attributes</span>, such as gender, race, and age, define diverse demographic groups. Underrepresented Demographical Attributes, referring to insufficient representation of demographical attributes in datasets and studies, can result in biased and stereotypical outputs from models.
This underrepresentation leads to models delivering inequitable outputs for these marginalized groups.
The concern is heightened in the era of FMs, given their broad societal impact and the inherent biases they often learn from extensive, internet-sourced datasets.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Integrating FMs into FL can significantly impact fairness, especially regarding underrepresented demographical attributes.
<span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Challenge 1: Transmission of Inherent Unfairness from FMs to FL.</span>
The use of FMs for synthetic data generation and knowledge distillation in FL, whether on servers or at client sites, risks transmitting any existing biases from FMs to FL models. This transmission occurs mainly in two ways. Firstly, biases within FMs, including those against underrepresented demographics, can be ingrained in the synthetic data they generate, affecting the initialization of local FL models. Secondly, if knowledge distillation involves data with underrepresented attributes, it can trigger and propagate FM’s unfairness to local models.
<span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_bold">Challenge 2: Unfairness Injected into FM from Compromised FL Clients.</span>
When FMs are deployed at client locations for local training assistance, new risks arise. Adversaries might introduce demographically unfair content through prompts to FMs, leading to unfair model outputs. This is particularly concerning with models capable of in-context learning, like Large Language Models, as the induced bias can quickly spread to other clients querying the same FM, amplifying unfairness across the network.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Participate Resource Disparity</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Participation Resource Disparity refers to the unequal distribution of resources among clients (participants) in federated learning, including local data volume, computational power, memory, network connectivity, and funding.
This disparity divides clients into two categories: <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">resource-abundant clients</span>, with abundant resources, and <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">resource-constrained clients</span>, with limited resources.
The participation resource disparity influences each participant’s capacity to process data, develop sophisticated models, and contribute efficiently to the collaborative system, thereby creating imbalances in terms of influence and outcomes within FL.
This disparity is further emphasized with the integration of FMs, as efficiently running these models requires significant resources, particularly in terms of memory and computing power.
Consequently, in the FM-FL environment, this resource disparity amplifies differences in local data size, the capacity for effective local training, and communication frequency.
As a result, it leads to considerable unfairness in both the training process and the distribution of benefits among participants, with resource-abundant clients potentially dominating the learning outcomes while resource-constrained clients may not benefit equally.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">In FM-FL scenarios where FMs are integrated with clients, resource-abundant clients, with the capability to build complex models and run FMs locally, gain a significant advantage. They have unrestricted access to FMs, allowing them to greatly enhance their local training sets, train their local models effectively, acquire extensive knowledge from the FM, and participate more actively in FL.
Consequently, these resource-abundant clients tend to dominate the FL training process. The substantial contributions heavily influence the aggregated global model, potentially leading to a bias that favors the data characteristics and preferences of these larger entities.
Conversely, resource-constrained clients, despite contributing to the learning process, might not be able to reap the same benefits. They often lack the resources to utilize the weights and insights derived from resource-abundant clients effectively. This asymmetry in learning and contribution means that resource-constrained clients can aid the learning of resource-abundant clients but cannot benefit equally from the reverse. Plus, clients lacking the resources to run FMs locally resort to using API-based services, sharing FMs with other resource-constrained clients. However, funding limitations and restricted FM availability present similar obstacles to direct FM integration. The constraints hinder a client’s ability to equally contribute to and benefit from the FL system, perpetuating the disparity in participation and advantages within FL.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Dissusion</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The difference in fairness issues between FL and FM-FL lies in the source and complexity of biases.
In FL, biases arise from data heterogeneity and client participation variability, leading to models potentially favoring certain client data patterns.
In FM-FL, the integration of FMs introduces additional layers of bias, stemming from the FM’s inherent biases in training and in-context learning abilities, which can further propagate to FL models through mechanisms like synthetic data generation and knowledge distillation. This integration results in more complex fairness challenges in FM-FL compared to FL alone.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Besides, incorporating FM introduces a new dimension of unfairness, resulting in aggregation bias and undermining the fairness of participation in FL. The disproportionate influence of resource-abundant clients skews the learning process, potentially marginalizing resource-constrained clients and creating a feedback loop that further entrenches the dominance of larger entities in the FL ecosystem. Addressing this disparity is crucial for maintaining the collaborative and equitable spirit of Federated Learning.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Direction</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Considering the integration of FMs into FL impacts robustness, privacy, and fairness, future research should focus on evaluating these effects and devising solutions. Interdisciplinary efforts combining machine learning, cybersecurity, and ethics are vital for developing robust, transparent, and ethical FL systems, necessitating ongoing innovation and societal impact assessments.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Robustness</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">For research investigation the robustness of FM-FL, we propose several future research directions to address the threats of FM-FL discussed in Sec. <a href="#S3" title="3 Robustness ‣ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>:</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">1. Assessing the Susceptibility of FM-FL under the novel threats.</span>
A comprehensive assessment of the susceptibility of FM-FL under the novel threats is essential.
This evaluation should address the effects of evasion attacks, particularly through ”jailbreak” prompts in LLMs, on FL model performance, convergence, and misinformation spread, including societal impacts and attacker cost-effectiveness.
It must also investigate the influence of compromised FMs, like those in backdoor attacks, on FL integrity, focusing on vulnerabilities introduced by synthetic data.
Assessing the attacker’s effort in poisoning FL systems, including FM fine-tuning and exploiting ICL, is critical.
Lastly, measuring the discrepancy between FM-generated and actual client data, alongside the FM’s performance on FL-specific tasks, is vital for identifying enhancements in data generation and knowledge distillation to boost FL robustness and reliability.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">2. Effectiveness of Current FL Defenses.</span>
A comprehensive evaluation of the current defense mechanisms in FL against emerging threats is essential. This assessment should encompass the effectiveness of robust aggregation strategies and post-training detection methods in countering these novel threats. Such evaluations will provide insights into the adequacy of existing defense approaches and the necessity for the development of robust defensive strategies tailored to the FM-FL context.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p"><span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_bold">3. Strengthening the Robustness and Security of FM-FL.</span>
Explore prompt validation techniques to identify and eliminate malicious inputs before they’re processed by FMs, such as using anomaly detection to spot harmful prompts, and preventing adversarial data manipulation.
Address the limitations of current FL defenses designed for decentralized threats, recognizing that new attacks target FMs directly and do not appear as statistical outliers.
Develop defenses against centralized attacks and those compromising numerous clients without relying on anomaly detection.
Investigate dynamic knowledge distillation methods tailored to FL’s varied tasks, focusing on enhancing knowledge transfer where FMs are confident and leveraging learner models for areas where FMs are less certain, ensuring effective learning across FL scenarios.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Privacy</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Addressing the complex privacy challenges in FM-FL requires a comprehensive approach that encompasses both impact assessment and solution development, including:
</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">1. Privacy Exposure Assessment.</span>
This involves assessing privacy leakage by evaluating the presence of sensitive data in FM-generated synthetic datasets, transfer mechanisms of sensitive data during the knowledge distillation, and assessing the persistence of privacy risks in FL models fine-tuned with local data. It also includes investigating vulnerabilities to inference attacks to understand if leaked sensitive information can be reverse-engineered.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">2. Privacy Enhancement Techniques.</span>
This involves crafting methods that not only detect and anonymize sensitive information effectively but also preserve the utility of the data for FL models. The focus should be on striking a balance between privacy preservation and maintaining the richness of data that supports the development of robust and accurate models. This subsection also explores the need for designing algorithms capable of unlearning or discarding sensitive data post-training, ensuring that FL can adaptively protect privacy without compromising on performance.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p"><span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_bold">3. Privacy-Preserving Knowledge Transfer and Effective Unlearning Algorithms.</span>
Develop knowledge distillation algorithms that are adaptive to privacy requirements, specifically obscuring sensitive data details.
Design algorithms for effective unlearning, enabling FL systems to forget or discard sensitive data during/post-training.
These unlearning algorithms must be optimized for efficiency, minimizing their impact on the computational resources and overall performance of the FL system, thus maintaining operational integrity while safeguarding privacy.</p>
</div>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.1" class="ltx_p"><span id="S6.SS2.p5.1.1" class="ltx_text ltx_font_bold">4. Ethical Guidelines and Policy Development for FM-FL.</span>
Formulate ethical guidelines specifically for FM-FL integration, addressing privacy and data protection challenges.
Develop comprehensive policy frameworks to govern data collection, use, and sharing within FM-FL systems, ensuring a balance between innovation and privacy.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Fairness</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">To foster a more equitable and bias-aware FM-FL ecosystem, we propose a series of strategic research directions aimed at addressing fairness issues in the FM-FL:</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p"><span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_bold">1. Bias Transmission Assessment from FMs to FL</span>
This involves a thorough examination of how biases present in synthetic data and during the knowledge distillation processes might be carried over into FL model outputs, with a particular focus on the impact these biases have on underrepresented groups.
Understanding how biases transfer from FMs to FL enables the creation of strategies to mitigate these biases, making FL models fairer and more representative.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p"><span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_bold">2. Fair Data Creation and Knowledge Distillation</span>
By developing methods that meticulously scrutinize and neutralize biases in data produced by FMs, we ensure that all demographic groups are equitably represented.
This effort is complemented by the innovation of knowledge distillation techniques that are specifically designed to identify and correct biases, thereby facilitating a fairer transfer of knowledge to FL models.
This dual strategy enhances FL system integrity and inclusivity by reducing bias at both the data generation and training phases.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p"><span id="S6.SS3.p4.1.1" class="ltx_text ltx_font_bold">3. Impact Analysis of Resource-Abundant Clients in FL</span>
This includes understanding how the contributions from these well-resourced clients might disproportionately shape the FL model’s development and outcomes.
For instance, whether the dominance of resource-rich participants skews model performance and fairness, particularly affecting the less-resourced, or resource-limited clients.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p id="S6.SS3.p5.1" class="ltx_p"><span id="S6.SS3.p5.1.1" class="ltx_text ltx_font_bold">4. Equitable Participation and Collaborative Learning</span>
We propose the creation of equitable participation mechanisms and collaborative learning models that bridge the gap between resource-diverse clients. This involves implementing strategies such as round-robin participation and fair access policies to FMs, ensuring all clients, irrespective of their resource capacity, contribute to and benefit from the federated learning process equally. Furthermore, we propose the exploration of collaborative learning frameworks that allow resource-constrained clients to leverage the advanced capabilities and insights of resource-abundant counterparts. Such models could facilitate the efficient transfer of knowledge to smaller, more communication-efficient models suitable for the FMFL environment, promoting a more inclusive and effective learning ecosystem. This dual strategy aims to democratize access to FMs in the FL context, enhancing the robustness, fairness, and overall performance of FMFL systems.</p>
</div>
<div id="S6.SS3.p6" class="ltx_para">
<p id="S6.SS3.p6.1" class="ltx_p"><span id="S6.SS3.p6.1.1" class="ltx_text ltx_font_bold">5. Formulation of Fairness-Driven Policies in FM-FL Integration.</span>
Policies should mitigate biases and ensure equitable participation in FM-FL integrations, fostering an inclusive digital ecosystem. They must prioritize resource distribution, fair data representation, and fairness metrics evaluation, steering FM-FL systems towards technological excellence that benefits a diverse society and promotes an accessible, fair digital future for all.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In conclusion, this paper presents an in-depth analysis of the robustness, privacy, and fairness challenges arising from the integration of FMs with FL, which remains underexplored in current research. Additionally, we shed lights on potential future directions in this field, advocating for the development of a responsible FM-FL ecosystem. This ecosystem would not only leverage the strengths of FMs to enhance FL but also prioritize the establishment of secure, reliable, and ethical practices to safeguard against any adverse implications of their deployment. Moving forward, future research must further explore the FM-FL relationship, ensuring advancements are driven by a commitment to societal welfare and justice.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assefa et al. (2020)</span>
<span class="ltx_bibblock">
Assefa, S. A., Dervovic, D., Mahfouz, M., Tillman, R. E., Reddy, P., and Veloso, M.

</span>
<span class="ltx_bibblock">Generating synthetic data in finance: opportunities, challenges and pitfalls.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the First ACM International Conference on AI in Finance</em>, pp.  1–8, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biggio et al. (2012)</span>
<span class="ltx_bibblock">
Biggio, B., Nelson, B., and Laskov, P.

</span>
<span class="ltx_bibblock">Poisoning attacks against support vector machines.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2012.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biggio et al. (2013)</span>
<span class="ltx_bibblock">
Biggio, B., Corona, I., Maiorca, D., Nelson, B., Srndic, N., Laskov, P., Giacinto, G., and Roli, F.

</span>
<span class="ltx_bibblock">Evasion attacks against machine learning at test time.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ECML PKDD</em>, 2013.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al. (2021a)</span>
<span class="ltx_bibblock">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07258</em>, 2021a.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al. (2021b)</span>
<span class="ltx_bibblock">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R. B., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M. S., Krishna, R., Kuditipudi, R., and et al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2108.07258, 2021b.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouacida &amp; Mohapatra (2021a)</span>
<span class="ltx_bibblock">
Bouacida, N. and Mohapatra, P.

</span>
<span class="ltx_bibblock">Vulnerabilities in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 2021a.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouacida &amp; Mohapatra (2021b)</span>
<span class="ltx_bibblock">
Bouacida, N. and Mohapatra, P.

</span>
<span class="ltx_bibblock">Vulnerabilities in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 9:63229–63249, 2021b.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. M., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with GPT-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.12712, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini &amp; Wagner (2017)</span>
<span class="ltx_bibblock">
Carlini, N. and Wagner, D. A.

</span>
<span class="ltx_bibblock">Towards evaluating the robustness of neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy</em>, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. (2021)</span>
<span class="ltx_bibblock">
Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T. B., Song, D., Erlingsson, Ú., Oprea, A., and Raffel, C.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">30th USENIX Security Symposium, USENIX Security</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2023)</span>
<span class="ltx_bibblock">
Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G. J., and Wong, E.

</span>
<span class="ltx_bibblock">Jailbreaking black box large language models in twenty queries.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.08419, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Chen, P., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.

</span>
<span class="ltx_bibblock">ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">AISec@CCS</em>, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demontis et al. (2019)</span>
<span class="ltx_bibblock">
Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita-Rotaru, C., and Roli, F.

</span>
<span class="ltx_bibblock">Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16, 2019</em>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2022)</span>
<span class="ltx_bibblock">
Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z.

</span>
<span class="ltx_bibblock">A survey for in-context learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.00234</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2012)</span>
<span class="ltx_bibblock">
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. S.

</span>
<span class="ltx_bibblock">Fairness through awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Innovations in Theoretical Computer Science</em>, 2012.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eigenschink et al. (2021)</span>
<span class="ltx_bibblock">
Eigenschink, P., Vamosi, S., Vamosi, R., Sun, C., Reutterer, T., and Kalcher, K.

</span>
<span class="ltx_bibblock">Deep generative models for synthetic data.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ezzeldin et al. (2023)</span>
<span class="ltx_bibblock">
Ezzeldin, Y. H., Yan, S., He, C., Ferrara, E., and Avestimehr, A. S.

</span>
<span class="ltx_bibblock">Fairfed: Enabling group fairness in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2020)</span>
<span class="ltx_bibblock">
Fang, M., Cao, X., Jia, J., and Gong, N. Z.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to byzantine-robust federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">29th USENIX Security Symposium, USENIX Security</em>, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2017)</span>
<span class="ltx_bibblock">
Gu, T., Dolan-Gavitt, B., and Garg, S.

</span>
<span class="ltx_bibblock">Badnets: Identifying vulnerabilities in the machine learning model supply chain.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1708.06733, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Guo, Y., Guo, K., Cao, X., Wu, T., and Chang, Y.

</span>
<span class="ltx_bibblock">Out-of-distribution generalization of federated learning via implicit invariant relationships.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He &amp; Garcia (2009)</span>
<span class="ltx_bibblock">
He, H. and Garcia, E. A.

</span>
<span class="ltx_bibblock">Learning from imbalanced data.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Knowl. Data Eng.</em>, 2009.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks &amp; Gimpel (2017)</span>
<span class="ltx_bibblock">
Hendrycks, D. and Gimpel, K.

</span>
<span class="ltx_bibblock">A baseline for detecting misclassified and out-of-distribution examples in neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings</em>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hitaj et al. (2017)</span>
<span class="ltx_bibblock">
Hitaj, B., Ateniese, G., and Pérez-Cruz, F.

</span>
<span class="ltx_bibblock">Deep models under the GAN: information leakage from collaborative deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS</em>, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2020)</span>
<span class="ltx_bibblock">
Hsu, Y.-C., Shen, Y., Jin, H., and Kira, Z.

</span>
<span class="ltx_bibblock">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imteaj et al. (2021)</span>
<span class="ltx_bibblock">
Imteaj, A., Thakker, U., Wang, S., Li, J., and Amini, M. H.

</span>
<span class="ltx_bibblock">A survey on federated learning for resource-constrained iot devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 9(1):1–24, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>, 14(1–2):1–210, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalra et al. (2023)</span>
<span class="ltx_bibblock">
Kalra, S., Wen, J., Cresswell, J. C., Volkovs, M., and Tizhoosh, H.

</span>
<span class="ltx_bibblock">Decentralized federated learning through proxy model sharing.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Nature communications</em>, 14(1):2899, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et al. (2023)</span>
<span class="ltx_bibblock">
Kandpal, N., Jagielski, M., Tramèr, F., and Carlini, N.

</span>
<span class="ltx_bibblock">Backdoor attacks for in-context learning with language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.14692, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2021)</span>
<span class="ltx_bibblock">
Khan, L. U., Saad, W., Han, Z., Hossain, E., and Hong, C. S.

</span>
<span class="ltx_bibblock">Federated learning for internet of things: Recent advances, taxonomy, and open challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, 23(3):1759–1799, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krawczyk (2016)</span>
<span class="ltx_bibblock">
Krawczyk, B.

</span>
<span class="ltx_bibblock">Learning from imbalanced data: open challenges and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Prog. Artif. Intell.</em>, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lapid et al. (2023)</span>
<span class="ltx_bibblock">
Lapid, R., Langberg, R., and Sipper, M.

</span>
<span class="ltx_bibblock">Open sesame! universal black box jailbreaking of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.01446, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Wang (2019)</span>
<span class="ltx_bibblock">
Li, D. and Wang, J.

</span>
<span class="ltx_bibblock">Fedmd: Heterogenous federated learning via model distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1910.03581, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1910.03581" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1910.03581</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Li, X., Fang, Y., Liu, M., Ling, Z., Tu, Z., and Su, H.

</span>
<span class="ltx_bibblock">Distilling large vision-language model with out-of-distribution generalizability.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2023a.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Li, X., Wang, S., Wu, C., Zhou, H., and Wang, J.

</span>
<span class="ltx_bibblock">Backdoor threats from compromised foundation models to federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">FL@FM-NeurIPS 23</em>, 2023b.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Li, X., Wu, C., and Wang, J.

</span>
<span class="ltx_bibblock">Unveiling backdoor risks brought by foundation models in heterogeneous federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">PAKDD</em>, 2024.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2021)</span>
<span class="ltx_bibblock">
Liang, K. J., Hao, W., Shen, D., Zhou, Y., Chen, W., Chen, C., and Carin, L.

</span>
<span class="ltx_bibblock">Mixkd: Towards efficient distillation of large-scale language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations, ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Lin, T., Kong, L., Stich, S. U., and Jaggi, M.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Liu, Y., Deng, G., Xu, Z., Li, Y., Zheng, Y., Zhang, Y., Zhao, L., Zhang, T., and Liu, Y.

</span>
<span class="ltx_bibblock">Jailbreaking chatgpt via prompt engineering: An empirical study.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13860, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Liu, Z., Chen, Y., Zhao, Y., Yu, H., Liu, Y., Bao, R., Jiang, J., Nie, Z., Xu, Q., and Yang, Q.

</span>
<span class="ltx_bibblock">Contribution-aware federated learning for smart healthcare.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 36, pp.  12396–12404, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2020a)</span>
<span class="ltx_bibblock">
Lyu, L., Xu, X., Wang, Q., and Yu, H.

</span>
<span class="ltx_bibblock">Collaborative fairness in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Federated Learning - Privacy and Incentive</em>. 2020a.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2020b)</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., and Yang, Q.

</span>
<span class="ltx_bibblock">Threats to federated learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2003.02133, 2020b.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reisizadeh et al. (2020)</span>
<span class="ltx_bibblock">
Reisizadeh, A., Farnia, F., Pedarsani, R., and Jadbabaie, A.

</span>
<span class="ltx_bibblock">Robust federated learning: The case of affine distribution shifts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shafahi et al. (2018)</span>
<span class="ltx_bibblock">
Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and Goldstein, T.

</span>
<span class="ltx_bibblock">Poison frogs! targeted clean-label poisoning attacks on neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Shen, L., Ji, S., Zhang, X., Li, J., Chen, J., Shi, J., Fang, C., Yin, J., and Wang, T.

</span>
<span class="ltx_bibblock">Backdoor pre-trained models can transfer to all.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">CCS ’21: 2021 ACM SIGSAC Conference on Computer and Communications Security, Virtual Event, Republic of Korea, November 15 - 19, 2021</em>, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023)</span>
<span class="ltx_bibblock">
Shi, J., Liu, Y., Zhou, P., and Sun, L.

</span>
<span class="ltx_bibblock">Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.12298, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2020)</span>
<span class="ltx_bibblock">
Shi, W., Zhou, S., Niu, Z., Jiang, M., and Geng, L.

</span>
<span class="ltx_bibblock">Joint device scheduling and resource allocation for latency constrained wireless federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Wireless Communications</em>, 20(1):453–467, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri et al. (2017)</span>
<span class="ltx_bibblock">
Shokri, R., Stronati, M., Song, C., and Shmatikov, V.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy, SP</em>, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. (2023)</span>
<span class="ltx_bibblock">
Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J. L., and Wang, L.

</span>
<span class="ltx_bibblock">Prompting GPT-3 to be reliable.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tolpegin et al. (2020)</span>
<span class="ltx_bibblock">
Tolpegin, V., Truex, S., Gursoy, M. E., and Liu, L.

</span>
<span class="ltx_bibblock">Data poisoning attacks against federated learning systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Computer Security - ESORICS 2020 - 25th European Symposium on Research in Computer Security, ESORICS 2020, Guildford, UK, September 14-18, 2020, Proceedings, Part I</em>, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torres (2018)</span>
<span class="ltx_bibblock">
Torres, D. G.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Generation of synthetic data with generative adversarial networks</em>.

</span>
<span class="ltx_bibblock">PhD thesis, Royal Institute of Technology Stockholm, Sweden, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuor et al. (2021)</span>
<span class="ltx_bibblock">
Tuor, T., Wang, S., Ko, B. J., Liu, C., and Leung, K. K.

</span>
<span class="ltx_bibblock">Overcoming noisy and irrelevant data in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">2020 25th International Conference on Pattern Recognition (ICPR)</em>, pp.  5020–5027. IEEE, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Wang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., Truong, S. T., Arora, S., Mazeika, M., Hendrycks, D., Lin, Z., Cheng, Y., Koyejo, S., Song, D., and Li, B.

</span>
<span class="ltx_bibblock">Decodingtrust: A comprehensive assessment of trustworthiness in GPT models.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.11698, 2023a.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Wang, J., Zeng, S., Long, Z., Wang, Y., Xiao, H., and Ma, F.

</span>
<span class="ltx_bibblock">Knowledge-enhanced semi-supervised federated learning for aggregating heterogeneous lightweight clients in iot.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)</em>, pp.  496–504. SIAM, 2023b.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Wang, S., Tuor, T., Salonidis, T., Leung, K. K., Makaya, C., He, T., and Chan, K.

</span>
<span class="ltx_bibblock">Adaptive federated learning in resource constrained edge computing systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">IEEE journal on selected areas in communications</em>, 37(6):1205–1221, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Wang, S., Nepal, S., Rudolph, C., Grobler, M., Chen, S., and Chen, T.

</span>
<span class="ltx_bibblock">Backdoor attacks against transfer learning with pre-trained deep learning models.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Serv. Comput.</em>, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Wei, A., Haghtalab, N., and Steinhardt, J.

</span>
<span class="ltx_bibblock">Jailbroken: How does LLM safety training fail?

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.02483, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al. (2022)</span>
<span class="ltx_bibblock">
Xing, H., Xiao, Z., Qu, R., Zhu, Z., and Zhao, B.

</span>
<span class="ltx_bibblock">An efficient federated distillation learning system for multitask time series classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Instrum. Meas.</em>, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Xu, J., Ma, M. D., Wang, F., Xiao, C., and Chen, M.

</span>
<span class="ltx_bibblock">Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14710, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Yang, X., Li, Q., Zhang, C., and Woodland, P. C.

</span>
<span class="ltx_bibblock">Knowledge distillation from multiple foundation models for end-to-end speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.10917, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al. (2023)</span>
<span class="ltx_bibblock">
Yi, L., Wang, G., Liu, X., Shi, Z., and Yu, H.

</span>
<span class="ltx_bibblock">Fedgh: Heterogeneous federated learning with generalized global header.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.13137</em>, 2023.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yu, W., Pang, T., Liu, Q., Du, C., Kang, B., Huang, Y., Lin, M., and Yan, S.

</span>
<span class="ltx_bibblock">Bag of tricks for training data extraction from language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2021)</span>
<span class="ltx_bibblock">
Zeng, H., Zhou, T., Guo, Y., Cai, Z., and Liu, F.

</span>
<span class="ltx_bibblock">Fedcav: contribution-aware model aggregation on distributed heterogeneous data in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 50th International Conference on Parallel Processing</em>, pp.  1–10, 2021.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Zhang, T., Gao, L., He, C., Zhang, M., Krishnamachari, B., and Avestimehr, A. S.

</span>
<span class="ltx_bibblock">Federated learning for the internet of things: Applications, challenges, and opportunities.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Magazine</em>, 5(1):24–29, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Zhou, C., Li, Q., Li, C., Yu, J., Liu, Y., Wang, G., Zhang, K., Ji, C., Yan, Q., He, L., Peng, H., Li, J., Wu, J., Liu, Z., Xie, P., Xiong, C., Pei, J., Yu, P. S., and Sun, L.

</span>
<span class="ltx_bibblock">A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2019)</span>
<span class="ltx_bibblock">
Zhu, L., Liu, Z., and Han, S.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2019.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2021)</span>
<span class="ltx_bibblock">
Zhu, Z., Hong, J., and Zhou, J.

</span>
<span class="ltx_bibblock">Data-free knowledge distillation for heterogeneous federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning, ICML</em>, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. (2023)</span>
<span class="ltx_bibblock">
Zhuang, W., Chen, C., and Lyu, L.

</span>
<span class="ltx_bibblock">When foundation model meets federated learning: Motivations, challenges, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.15546</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.01856" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.01857" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.01857">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.01857" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.01858" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:01:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
