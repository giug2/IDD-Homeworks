<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ControlAR: Controllable Image Generation with Autoregressive Models</title>
<!--Generated on Wed Oct  2 20:14:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02705v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1" title="In ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introdution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S2" title="In ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S2.SS1" title="In 2 Related Work â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Image Generation with Diffusion Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S2.SS2" title="In 2 Related Work â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Image Generation with Autoregressive Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S2.SS3" title="In 2 Related Work â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Controllable Image Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3" title="In ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>ControlAR</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS1" title="In 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminary: Image Generation with Autoregressive Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS2" title="In 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Unified Conditional Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS3" title="In 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Controllable Autoregressive Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS3.SSS0.Px1" title="In 3.3 Controllable Autoregressive Model â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Overall architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS3.SSS0.Px2" title="In 3.3 Controllable Autoregressive Model â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Control encoder.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS4" title="In 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Autoregressive arbitrary-resolution generation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4" title="In ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS1" title="In 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Setup â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Setup â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Evaluation and metrics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS1.SSS0.Px3" title="In 4.1 Experimental Setup â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Implementation details.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS2" title="In 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS2.SSS0.Px1" title="In 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">C2I controllable generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS2.SSS0.Px2" title="In 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">T2I controllable generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS2.SSS0.Px3" title="In 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Arbitrary-Resolution Generation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS3" title="In 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS3.SSS0.Px1" title="In 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Ablations on the Control Encoder.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS3.SSS0.Px2" title="In 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Ablations on the Control Fusion Strategy.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS3.SSS0.Px3" title="In 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Ablations on Sequence Model Training Strategy.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS3.SSS0.Px4" title="In 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Conditional Decoding <span class="ltx_text ltx_font_italic">v.s.</span> Conditional Prefilling.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S5" title="In ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S5.SS0.SSS0.Px1" title="In 5 Conclusion â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Acknowledgment.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1" title="In ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS1" title="In Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS1.SSS0.Px1" title="In A.1 Implementation Details â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Dataset details.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS1.SSS0.Px2" title="In A.1 Implementation Details â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Evaluation details.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS1.SSS0.Px3" title="In A.1 Implementation Details â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Training details.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS2" title="In Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS2.SSS0.Px1" title="In A.2 Discussion â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Limitation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS2.SSS0.Px2" title="In A.2 Discussion â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title">Future work.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.SS3" title="In Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>More Visualizations</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ControlAR: Controllable Image Generation with Autoregressive Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zongming Li<sup class="ltx_sup" id="id1.1.id1">1<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span></sup>,
Tianheng Cheng<sup class="ltx_sup" id="id2.2.id2">1<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span></sup>,
Shoufa Chen<sup class="ltx_sup" id="id3.3.id3">2</sup>,
Peize Sun<sup class="ltx_sup" id="id4.4.id4">2</sup>,
Haocheng Shen<sup class="ltx_sup" id="id5.5.id5">3</sup>,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.id6">Longjin Ran<sup class="ltx_sup" id="id6.6.id6.1"><span class="ltx_text ltx_font_medium" id="id6.6.id6.1.1">3</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id7.7.id7">Xiaoxin Chen<sup class="ltx_sup" id="id7.7.id7.1"><span class="ltx_text ltx_font_medium" id="id7.7.id7.1.1">3</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id8.8.id8">Wenyu Liu<sup class="ltx_sup" id="id8.8.id8.1"><span class="ltx_text ltx_font_medium" id="id8.8.id8.1.1">1</span></sup></span>
<span class="ltx_text ltx_font_bold" id="id9.9.id9">&amp; Xinggang Wang<sup class="ltx_sup" id="id9.9.id9.1"><span class="ltx_text ltx_font_medium" id="id9.9.id9.1.1">1</span><span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">2</span></span></span></span></span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id9.2"><span class="ltx_text ltx_font_medium" id="id9.9.id9.2.1">1</span></sup></span> School of EIC, Huazhong University of Science and Technology 
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id10">2</sup> Department of Computer Science, The University of Hong Kong
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.id11">3</sup> vivo AI Lab
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Autoregressive (AR) models have reformulated image generation as <span class="ltx_text ltx_font_italic" id="id12.id1.1">next-token prediction</span>, demonstrating remarkable potential and emerging as strong competitors to diffusion models.
However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models.
Although a natural approach, inspired by advancements Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens,
it still falls short in generation quality compared to ControlNet and suffers from inefficiency.
To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (<span class="ltx_text ltx_font_italic" id="id12.id1.2">e</span>.<span class="ltx_text ltx_font_italic" id="id12.id1.3">g</span>., canny edges or depth maps) into control tokens.
Then ControlAR exploits the <span class="ltx_text ltx_font_italic" id="id12.id1.4">conditional decoding</span> method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings.
Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model efficiency.
Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and the specific controls.
Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks.
Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, <span class="ltx_text ltx_font_italic" id="id12.id1.5">e</span>.<span class="ltx_text ltx_font_italic" id="id12.id1.6">g</span>., ControlNet++.
The code, models, and demo will soon be available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hustvl/ControlAR" title="">https://github.com/hustvl/ControlAR</a>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="281" id="S0.F1.g1" src="x1.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S0.F1.4.1">Arbitrary-resolution images generated by ControlAR.</span> Our ControlAR extends autoregressive models, <span class="ltx_text ltx_font_italic" id="S0.F1.5.2">e</span>.<span class="ltx_text ltx_font_italic" id="S0.F1.6.3">g</span>., LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite>, to generate high-quality images using spatial controls and expands the capability of autoregressive models to any-resolution image generation.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introdution</h2>
<span class="ltx_note ltx_role_footnote" id="footnotex4"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution. Works was done during Zongming Liâ€™s internship at vivo AI Lab.</span></span></span><span class="ltx_note ltx_role_footnote" id="footnotex5"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Corresponding author (<a class="ltx_ref ltx_url ltx_font_typewriter" href="xgwang@hust.edu.cn" title="">xgwang@hust.edu.cn</a>).</span></span></span>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advancements in image generation have led to the emergence of various models that leverage text-to-image diffusion models Â <cite class="ltx_cite ltx_citemacro_citep">(Saharia etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib43" title="">2022</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib16" title="">2022</a>; Rombach etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib40" title="">2022</a>; Podell etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib32" title="">2023</a>)</cite> to generate high-quality visual content.
Among them, several worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>; Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib22" title="">2024b</a>; Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib35" title="">2023b</a>)</cite> such as ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>)</cite>, have explored adding conditional controls to text-to-image diffusion models and allowed for image generation according to the precise spatial controls, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S1.p1.1.2">g</span>., edges, depth maps, or segmentation masks.
The control-to-image diffusion models have impressively enhanced the versatility of these models in applications ranging from creative design to augmented reality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite the success of diffusion models, most recent works reveal the potential of autoregressive models for image generation, <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">g</span>., LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite> follows the architecture of LlamaÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib51" title="">2023</a>)</cite> to achieve image generation and obtained remarkable results.
Moreover, several worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Kondratyuk etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib19" title="">2024</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib11" title="">2024</a>)</cite> have explored autoregressive models for video generation and achieved promising results, further demonstrating the great potential of autoregressive models for visual generation.
However, controlling autoregressive models as a crucial direction remains unexplored, making it challenging for autoregressive models to achieve same level of fine-grained control as diffusion models.
In contrast to controllable diffusion models, adding conditional controls to autoregressive models is not straightforward because of two major challenges: (1) <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">how to encode 2D spatial control images for autoregressive models</span> and (2) <span class="ltx_text ltx_font_italic" id="S1.p2.1.4">how to guide image generation with encoded controls</span>.
Specifically, diffusion models directly use the 2D features of control images and control the generated image through pixel-wise feature fusion.
However, autoregressive models adopt sequence modeling and next-token prediction to perform image generation sequentially.
Therefore, the techniques proposed in <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>; Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib22" title="">2024b</a>)</cite> are infeasible in autoregressive models.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S1.F2.4.1">Comparison between Conditional Prefilling <span class="ltx_text ltx_font_italic" id="S1.F2.4.1.1">v.s.</span> Conditional Decoding.</span> We encode the spatial control images into a sequence of control tokens for autoregressive models. (a) Conditional Prefilling: control condition tokens are prefilled into the autoregressive model before the first image token is generated. (b) Conditional Decoding: each image token is fused with the control condition token to predict the next image token. (c) Image Quality: we compare the performance (<span class="ltx_text ltx_font_italic" id="S1.F2.5.2">i</span>.<span class="ltx_text ltx_font_italic" id="S1.F2.6.3">e</span>., F1-Score and FID) across training epochs between conditional decoding and prefilling. Itâ€™s remarkable that conditional decoding outperforms conditional prefilling in terms of performance and training convergence speed. (d) Training cost: conditional prefilling significantly increases the training memory (+59.1%) and training latency (+96.3%) compared to conditional decoding.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we delve into the two aforementioned challenges and introduce the <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Control</span>lable <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">A</span>uto<span class="ltx_text ltx_font_bold" id="S1.p3.1.3">R</span>egressive (ControlAR) framework to enhance the control capabilities of autoregressive image generation models such as LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite> or AiMÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib21" title="">2024a</a>)</cite>.
Firstly, we propose a control encoder to obtain sequential encodings of control images and output the control tokens, which are more suitable than 2D control features for autoregressive models.
Instead of directly replicating the modules of diffusion models for control feature extraction, we use a Vision Transformer (ViT) as the encoder and further investigate the most effective ViT pre-training scheme, <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">e</span>.<span class="ltx_text ltx_font_italic" id="S1.p3.1.5">g</span>., vanillaÂ <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib7" title="">2020</a>)</cite> or self-supervisedÂ <cite class="ltx_cite ltx_citemacro_citep">(Oquab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib30" title="">2023</a>)</cite> for encoding spatial controls towards image generation.
Secondly, we naturally consider that directly prefilling control tokens, inspired by Large Language Models and prompt techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">(Pope etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib33" title="">2023</a>)</cite>, can provide a simple and effective autoregressive control approach, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (a).
However, it struggles to achieve satisfactory results, <span class="ltx_text ltx_font_italic" id="S1.p3.1.6">i</span>.<span class="ltx_text ltx_font_italic" id="S1.p3.1.7">e</span>., the LlamaGen with conditional prefilling obtains 26.45 FID with the Canny edge control on ImageNet, which is much inferior to ControlNet (10.85 FID).
Moreover, it tends to increase sequence length, inevitably raising the cost of training and inference.
To remedy the above issues, we formulate controllable autoregressive generation as <span class="ltx_text ltx_font_italic" id="S1.p3.1.8">conditional decoding</span>, in which predicting the next image token is conditioned on both the previous image token and the current control token, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (b).
Specifically, the input image token is fused with the corresponding control token and fed into the model for the next-token prediction.
The proposed ControlAR leverage the conditional decoding strategy in several intermediate layers of the AR model to maintain control information across decoding layers.
Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (c) indicates that the proposed conditional decoding clearly surpasses the well-known conditional prefilling in terms of both the image quality (FID) and control capability (F1-Score).
In addition, the proposed conditional decoding, without increasing the sequence length, brings negligible computation costs on the original autoregressive model, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (d), demonstrating superiority compared to conditional prefilling.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.2">Most importantly, ControlAR surprisingly provides an effective way to control the resolution (size and aspect ratio) of image generation, allowing autoregressive models to get rid of the constraints of generating images at a fixed resolution, <span class="ltx_text ltx_font_italic" id="S1.p4.2.1">e</span>.<span class="ltx_text ltx_font_italic" id="S1.p4.2.2">g</span>., LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite> can only generate images of <math alttext="256\times 256" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">256</mn><mo id="S1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.p4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><times id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1"></times><cn id="S1.p4.1.m1.1.1.2.cmml" type="integer" xref="S1.p4.1.m1.1.1.2">256</cn><cn id="S1.p4.1.m1.1.1.3.cmml" type="integer" xref="S1.p4.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">256 Ã— 256</annotation></semantics></math> after trained on <math alttext="256\times 256" class="ltx_Math" display="inline" id="S1.p4.2.m2.1"><semantics id="S1.p4.2.m2.1a"><mrow id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml"><mn id="S1.p4.2.m2.1.1.2" xref="S1.p4.2.m2.1.1.2.cmml">256</mn><mo id="S1.p4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.p4.2.m2.1.1.1.cmml">Ã—</mo><mn id="S1.p4.2.m2.1.1.3" xref="S1.p4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><apply id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1"><times id="S1.p4.2.m2.1.1.1.cmml" xref="S1.p4.2.m2.1.1.1"></times><cn id="S1.p4.2.m2.1.1.2.cmml" type="integer" xref="S1.p4.2.m2.1.1.2">256</cn><cn id="S1.p4.2.m2.1.1.3.cmml" type="integer" xref="S1.p4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S1.p4.2.m2.1d">256 Ã— 256</annotation></semantics></math> images.
By adjusting the input size of the control, ControlAR decodes image tokens according to the sequence of control tokens, making it easy to achieve any-resolution image generation without resolution-aware promptsÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib25" title="">2024</a>)</cite>.
In addition, we propose the multi-resolution ControlAR with multi-scale training to further enhance the image quality of different resolutions, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S0.F1" title="Figure 1 â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Quantitative and qualitative experiments demonstrate that ControlAR can obtain better performance compared to previous state-of-the-art methods based on well-established diffusion models towards diverse controllable image generation.
Especially, the experiments also showcase the zero-shot or fine-tuned ability to control any-resolution image generation and prove the effects of ControlAR.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The main contribution of this paper can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We explore controllable autoregressive image generation and present ControlAR, which enables precise control and generates high-quality images. ControlAR exploits the control encoder to transform the control images into a sequence of conditional tokens and adopt the proposed conditional decoding to predict the next image token conditioned on the control and image tokens, which proves more effective than conditional prefilling.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The proposed ControlAR easily expands autoregressive models with strong control capability. Under various control conditions, the proposed ControlAR demonstrates its highly competitive performance towards conditional consistency and image quality compared to state-of-the-art diffusion methods, <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.2">g</span>., ControlNet++.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We exploit the properties of our proposed conditional decoding to extend the ability of the autoregressive model to generate arbitrary resolution. With a simple multi-resolution training recipe, , we extend ControlAR to Multi-Resolution ControlAR (MR-ControlAR), which allows autoregressive models to generate high-quality images with different resolutions, further enhancing their control capability.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Image Generation with Diffusion Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Diffusion modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Song &amp; Ermon, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib47" title="">2019</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib15" title="">2020</a>; Song etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib46" title="">2020a</a>; Dhariwal &amp; Nichol, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib6" title="">2021</a>; Nichol etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib29" title="">2021</a>; Lu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib26" title="">2022</a>; Rombach etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib40" title="">2022</a>; Podell etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib32" title="">2023</a>)</cite> have cemented their status as a dominant paradigm in generative modeling, especially in the domain of image synthesis. They employ an iterative denoising process to create images from Gaussian noise.
Since the introduction of the diffusion modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Sohl-Dickstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib45" title="">2015</a>)</cite>, subsequent research has focused on refining training and sampling strategiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Song etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib48" title="">2020b</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib15" title="">2020</a>; Song etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib46" title="">2020a</a>)</cite>.
Simultaneously, in an effort to reduce computational complexity in the image generation process and enhance efficiency, numerous studies have sought to translate the generation process into the latent spaceÂ <cite class="ltx_cite ltx_citemacro_citep">(Rombach etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib40" title="">2022</a>; Podell etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib32" title="">2023</a>; Esser etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib9" title="">2024</a>)</cite>.
Within the realm of text-to-image generation, the prevailing framework involves utilizing U-NetÂ <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib41" title="">2015</a>)</cite> as the denoising network, while leveraging pre-trained CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib36" title="">2021</a>)</cite> or T5Â <cite class="ltx_cite ltx_citemacro_citep">(Raffel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib37" title="">2020</a>)</cite> as the text encoder to extract textual features and integrate them into the denoising process through the cross-attention mechanism.
Furthermore, DiTÂ <cite class="ltx_cite ltx_citemacro_citep">(Peebles &amp; Xie, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib31" title="">2023</a>)</cite> employs TransformerÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib54" title="">2017</a>)</cite> as the denoising network, yielding highly competitive results in image generation.
Despite the considerable progress in diffusion models, the field of image generation still trails behind the advancement of large language models based on autoregressive mechanisms.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Image Generation with Autoregressive Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In contrast to the iterative denoising process of the diffusion model, the autoregressive model operates on the principle of predicting the next image token based on the existing image tokens.
Early autoregressive image generation modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Van DenÂ Oord etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib53" title="">2016</a>; VanÂ den Oord etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib52" title="">2016</a>)</cite> focused on predicting individual pixel values.
Subsequent approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(Esser etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib8" title="">2021</a>; Ramesh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib38" title="">2021</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib56" title="">2022</a>)</cite> attempt to use an image tokenizer to convert continuous images into discrete tokens.
More recently, there has been a growing trend towards leveraging efficient language model architectures as generative networks for autoregressive image generation. LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite> and Open-MAGVIT2Â <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib27" title="">2024</a>)</cite> use the Llama architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib51" title="">2023</a>)</cite> as the generative network, demonstrating its significant potential for image generation.
AiMÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib21" title="">2024a</a>)</cite> explores an approach using the Mamba modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu &amp; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib12" title="">2023</a>)</cite> as the generative network.
Lumina-mGPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib25" title="">2024</a>)</cite> develops a family of multimodal autoregressive models capable of a wide range of visual and linguistic tasks, particularly excelling in generating flexible, photorealistic images from textual descriptions.
In addition, some recent worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib55" title="">2024</a>; Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib61" title="">2024</a>)</cite> fuse autoregressive and diffusion into one multi-modal model for simultaneous image generation and understanding.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Controllable Image Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Relying solely on textual prompts is insufficient for conveying distinctive artistic style or precise detail during T2I image generation.
Some methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Gal etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib10" title="">2022</a>; Ruiz etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib42" title="">2023</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib58" title="">2023b</a>)</cite> attempt to capture concepts from example images that are not easily described through text to guide image generation, a task known as personalization for controllable generation.
Represented by ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>)</cite> and T2I-AdapterÂ <cite class="ltx_cite ltx_citemacro_citep">(Mou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib28" title="">2024</a>)</cite>, work in this area utilizes the spatial structure of the image, such as edges, segmentation masks, depth maps, etc., to enable spatial control in the generation process.
Subsequently, UniControlÂ <cite class="ltx_cite ltx_citemacro_citep">(Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib35" title="">2023b</a>)</cite>, Uni-ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib59" title="">2024</a>)</cite>, and ControlNet++Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib22" title="">2024b</a>)</cite> further extend this realm, focusing on condition encoder design and optimization of training strategies.
Furthermore, GlueGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib34" title="">2023a</a>)</cite> pairs a multi-modal encoder with a stable diffusion model for sound-to-image generation.
Controllable generation based on autoregressive image generation models has been less explored. ControlVARÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib23" title="">2024c</a>)</cite> employs next-scale prediction to jointly model control and image,
but is still different from next-token prediction in autoregressive generation.
Our objective is to fully harness the capabilities of autoregressive models and explore a general and efficient paradigm for controllable image generation using autoregressive models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ControlAR</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminary: Image Generation with Autoregressive Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8">Autoregressive models define the generative process as <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.8.1">next-token prediction</span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathbf{x})=\prod_{i=1}^{n}p(x_{i}|x_{1},x_{2},\ldots,x_{i-1})=\prod_{i=1}^{%
n}p(x_{i}|x_{&lt;i})," class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml"><mi id="S3.E1.m1.3.3.1.1.4.2" xref="S3.E1.m1.3.3.1.1.4.2.cmml">p</mi><mo id="S3.E1.m1.3.3.1.1.4.1" xref="S3.E1.m1.3.3.1.1.4.1.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.1.1.4.3.2" xref="S3.E1.m1.3.3.1.1.4.cmml"><mo id="S3.E1.m1.3.3.1.1.4.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.4.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğ±</mi><mo id="S3.E1.m1.3.3.1.1.4.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.5" rspace="0.111em" xref="S3.E1.m1.3.3.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><munderover id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.2.2.2" movablelimits="false" xref="S3.E1.m1.3.3.1.1.1.2.2.2.cmml">âˆ</mo><mrow id="S3.E1.m1.3.3.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.2.2.3.2" xref="S3.E1.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.2.2.3.1" xref="S3.E1.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.3.3.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">p</mi><mo id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.4" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.4.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.4" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.5" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.4.cmml">,</mo><mi id="S3.E1.m1.2.2" mathvariant="normal" xref="S3.E1.m1.2.2.cmml">â€¦</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.6" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.1.cmml">âˆ’</mo><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.6" rspace="0.111em" xref="S3.E1.m1.3.3.1.1.6.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><munderover id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.2.2" movablelimits="false" xref="S3.E1.m1.3.3.1.1.2.2.2.2.cmml">âˆ</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.2.2.2.3.1" xref="S3.E1.m1.3.3.1.1.2.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.1.2.2.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">n</mi></munderover><mrow id="S3.E1.m1.3.3.1.1.2.1" xref="S3.E1.m1.3.3.1.1.2.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.3" xref="S3.E1.m1.3.3.1.1.2.1.3.cmml">p</mi><mo id="S3.E1.m1.3.3.1.1.2.1.2" xref="S3.E1.m1.3.3.1.1.2.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.cmml">|</mo><msub id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.2.cmml">x</mi><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><and id="S3.E1.m1.3.3.1.1a.cmml" xref="S3.E1.m1.3.3.1"></and><apply id="S3.E1.m1.3.3.1.1b.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.5.cmml" xref="S3.E1.m1.3.3.1.1.5"></eq><apply id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4"><times id="S3.E1.m1.3.3.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.4.1"></times><ci id="S3.E1.m1.3.3.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.4.2">ğ‘</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ±</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.2">product</csymbol><apply id="S3.E1.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3"><eq id="S3.E1.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.2">ğ‘–</ci><cn id="S3.E1.m1.3.3.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.3">ğ‘›</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"></times><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">ğ‘</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.2">ğ‘¥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.5.3">ğ‘–</ci></apply><list id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><cn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><cn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.3">2</cn></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">â€¦</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.2">ğ‘¥</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3"><minus id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><cn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply></list></apply></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1c.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.6.cmml" xref="S3.E1.m1.3.3.1.1.6"></eq><share href="https://arxiv.org/html/2410.02705v1#S3.E1.m1.3.3.1.1.1.cmml" id="S3.E1.m1.3.3.1.1d.cmml" xref="S3.E1.m1.3.3.1"></share><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2">product</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.3"><eq id="S3.E1.m1.3.3.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.3.2">ğ‘–</ci><cn id="S3.E1.m1.3.3.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">ğ‘›</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1"><times id="S3.E1.m1.3.3.1.1.2.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.2"></times><ci id="S3.E1.m1.3.3.1.1.2.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.3">ğ‘</ci><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.2">ğ‘¥</ci><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.2">ğ‘¥</ci><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3"><lt id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">p(\mathbf{x})=\prod_{i=1}^{n}p(x_{i}|x_{1},x_{2},\ldots,x_{i-1})=\prod_{i=1}^{%
n}p(x_{i}|x_{&lt;i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">italic_p ( bold_x ) = âˆ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) = âˆ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.1">and when performing image generation, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.E1" title="In 3.1 Preliminary: Image Generation with Autoregressive Models â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">1</span></a> represents the image token. The latest autoregressive image generation models such as LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite> and AiMÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib21" title="">2024a</a>)</cite> use vector quantization to convert compressed image patches into discrete image tokens. The process of image generation is formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathbf{q})=\prod_{t=1}^{h\cdot w}p(q_{t}|q_{&lt;t},c)," class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml">p</mi><mo id="S3.E2.m1.3.3.1.1.3.1" xref="S3.E2.m1.3.3.1.1.3.1.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.3.cmml"><mo id="S3.E2.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S3.E2.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">ğª</mi><mo id="S3.E2.m1.3.3.1.1.3.3.2.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.2" rspace="0.111em" xref="S3.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><munderover id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml"><mo id="S3.E2.m1.3.3.1.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.3.3.1.1.1.2.2.2.cmml">âˆ</mo><mrow id="S3.E2.m1.3.3.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E2.m1.3.3.1.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.3.3.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.1.2.3.2.cmml">h</mi><mo id="S3.E2.m1.3.3.1.1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.3.3.1.1.1.2.3.1.cmml">â‹…</mo><mi id="S3.E2.m1.3.3.1.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.1.2.3.3.cmml">w</mi></mrow></munderover><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.3.cmml">p</mi><mo id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">q</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">c</mi></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"></eq><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><times id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1"></times><ci id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2">ğ‘</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğª</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2.2.2">product</csymbol><apply id="S3.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2.2.3.2">ğ‘¡</ci><cn id="S3.E2.m1.3.3.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.2.3"><ci id="S3.E2.m1.3.3.1.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.2.3.1">â‹…</ci><ci id="S3.E2.m1.3.3.1.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2.3.2">â„</ci><ci id="S3.E2.m1.3.3.1.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.2.3.3">ğ‘¤</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"></times><ci id="S3.E2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.3">ğ‘</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3">ğ‘¡</ci></apply><list id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘¡</ci></apply></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğ‘</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">p(\mathbf{q})=\prod_{t=1}^{h\cdot w}p(q_{t}|q_{&lt;t},c),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">italic_p ( bold_q ) = âˆ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h â‹… italic_w end_POSTSUPERSCRIPT italic_p ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_q start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT , italic_c ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.4">where <math alttext="q_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m1.1"><semantics id="S3.SS1.p1.2.m1.1a"><msub id="S3.SS1.p1.2.m1.1.1" xref="S3.SS1.p1.2.m1.1.1.cmml"><mi id="S3.SS1.p1.2.m1.1.1.2" xref="S3.SS1.p1.2.m1.1.1.2.cmml">q</mi><mi id="S3.SS1.p1.2.m1.1.1.3" xref="S3.SS1.p1.2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m1.1b"><apply id="S3.SS1.p1.2.m1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m1.1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m1.1.1.2.cmml" xref="S3.SS1.p1.2.m1.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.2.m1.1.1.3.cmml" xref="S3.SS1.p1.2.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m1.1c">q_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m1.1d">italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the discretised image token, <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m2.1"><semantics id="S3.SS1.p1.3.m2.1a"><mi id="S3.SS1.p1.3.m2.1.1" xref="S3.SS1.p1.3.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m2.1b"><ci id="S3.SS1.p1.3.m2.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m2.1d">italic_c</annotation></semantics></math> is class label embedding or text embedding, and <math alttext="h\cdot w" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m3.1"><semantics id="S3.SS1.p1.4.m3.1a"><mrow id="S3.SS1.p1.4.m3.1.1" xref="S3.SS1.p1.4.m3.1.1.cmml"><mi id="S3.SS1.p1.4.m3.1.1.2" xref="S3.SS1.p1.4.m3.1.1.2.cmml">h</mi><mo id="S3.SS1.p1.4.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.4.m3.1.1.1.cmml">â‹…</mo><mi id="S3.SS1.p1.4.m3.1.1.3" xref="S3.SS1.p1.4.m3.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m3.1b"><apply id="S3.SS1.p1.4.m3.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1"><ci id="S3.SS1.p1.4.m3.1.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1.1">â‹…</ci><ci id="S3.SS1.p1.4.m3.1.1.2.cmml" xref="S3.SS1.p1.4.m3.1.1.2">â„</ci><ci id="S3.SS1.p1.4.m3.1.1.3.cmml" xref="S3.SS1.p1.4.m3.1.1.3">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m3.1c">h\cdot w</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m3.1d">italic_h â‹… italic_w</annotation></semantics></math> is the total number of image tokens. During training, these two methods use causal TransformerÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib54" title="">2017</a>)</cite> and MambaÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu &amp; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib12" title="">2023</a>)</cite> to model the sequence respectively, with the aim of minimising the prediction loss of the next image token, which can be written as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{train}=\textbf{{CE}}(\mathbf{M}([c,q_{1},q_{2},\ldots,q_{l-1}]),[%
q_{1},q_{2},\ldots,q_{l}])," class="ltx_Math" display="block" id="S3.E3.m1.4"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><msub id="S3.E3.m1.4.4.1.1.4" xref="S3.E3.m1.4.4.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.4.4.1.1.4.2" xref="S3.E3.m1.4.4.1.1.4.2.cmml">â„’</mi><mrow id="S3.E3.m1.4.4.1.1.4.3" xref="S3.E3.m1.4.4.1.1.4.3.cmml"><mi id="S3.E3.m1.4.4.1.1.4.3.2" xref="S3.E3.m1.4.4.1.1.4.3.2.cmml">t</mi><mo id="S3.E3.m1.4.4.1.1.4.3.1" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">â¢</mo><mi id="S3.E3.m1.4.4.1.1.4.3.3" xref="S3.E3.m1.4.4.1.1.4.3.3.cmml">r</mi><mo id="S3.E3.m1.4.4.1.1.4.3.1a" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">â¢</mo><mi id="S3.E3.m1.4.4.1.1.4.3.4" xref="S3.E3.m1.4.4.1.1.4.3.4.cmml">a</mi><mo id="S3.E3.m1.4.4.1.1.4.3.1b" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">â¢</mo><mi id="S3.E3.m1.4.4.1.1.4.3.5" xref="S3.E3.m1.4.4.1.1.4.3.5.cmml">i</mi><mo id="S3.E3.m1.4.4.1.1.4.3.1c" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">â¢</mo><mi id="S3.E3.m1.4.4.1.1.4.3.6" xref="S3.E3.m1.4.4.1.1.4.3.6.cmml">n</mi></mrow></msub><mo id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml"><mtext class="ltx_mathvariant_bold-italic" id="S3.E3.m1.4.4.1.1.2.4" xref="S3.E3.m1.4.4.1.1.2.4a.cmml">CE</mtext><mo id="S3.E3.m1.4.4.1.1.2.3" xref="S3.E3.m1.4.4.1.1.2.3.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.1.1.2.2.2" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml"><mo id="S3.E3.m1.4.4.1.1.2.2.2.3" stretchy="false" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml">(</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.cmml">ğŒ</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4" stretchy="false" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">[</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">c</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.5" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.6" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml">q</mi><mn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.7" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">,</mo><mi id="S3.E3.m1.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.cmml">â€¦</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.8" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2.cmml">q</mi><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">l</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml">âˆ’</mo><mn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.9" stretchy="false" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">]</mo></mrow><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.2.2.2.4" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml">,</mo><mrow id="S3.E3.m1.4.4.1.1.2.2.2.2.3" xref="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml"><mo id="S3.E3.m1.4.4.1.1.2.2.2.2.3.4" stretchy="false" xref="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml">[</mo><msub id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.2" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.2.cmml">q</mi><mn id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.3" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m1.4.4.1.1.2.2.2.2.3.5" xref="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.2" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.2.cmml">q</mi><mn id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.3" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.E3.m1.4.4.1.1.2.2.2.2.3.6" xref="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml">,</mo><mi id="S3.E3.m1.3.3" mathvariant="normal" xref="S3.E3.m1.3.3.cmml">â€¦</mi><mo id="S3.E3.m1.4.4.1.1.2.2.2.2.3.7" xref="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.cmml"><mi id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.2" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.2.cmml">q</mi><mi id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.3" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.3.cmml">l</mi></msub><mo id="S3.E3.m1.4.4.1.1.2.2.2.2.3.8" stretchy="false" xref="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml">]</mo></mrow><mo id="S3.E3.m1.4.4.1.1.2.2.2.5" stretchy="false" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><eq id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"></eq><apply id="S3.E3.m1.4.4.1.1.4.cmml" xref="S3.E3.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.4.1.cmml" xref="S3.E3.m1.4.4.1.1.4">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.4.2.cmml" xref="S3.E3.m1.4.4.1.1.4.2">â„’</ci><apply id="S3.E3.m1.4.4.1.1.4.3.cmml" xref="S3.E3.m1.4.4.1.1.4.3"><times id="S3.E3.m1.4.4.1.1.4.3.1.cmml" xref="S3.E3.m1.4.4.1.1.4.3.1"></times><ci id="S3.E3.m1.4.4.1.1.4.3.2.cmml" xref="S3.E3.m1.4.4.1.1.4.3.2">ğ‘¡</ci><ci id="S3.E3.m1.4.4.1.1.4.3.3.cmml" xref="S3.E3.m1.4.4.1.1.4.3.3">ğ‘Ÿ</ci><ci id="S3.E3.m1.4.4.1.1.4.3.4.cmml" xref="S3.E3.m1.4.4.1.1.4.3.4">ğ‘</ci><ci id="S3.E3.m1.4.4.1.1.4.3.5.cmml" xref="S3.E3.m1.4.4.1.1.4.3.5">ğ‘–</ci><ci id="S3.E3.m1.4.4.1.1.4.3.6.cmml" xref="S3.E3.m1.4.4.1.1.4.3.6">ğ‘›</ci></apply></apply><apply id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"><times id="S3.E3.m1.4.4.1.1.2.3.cmml" xref="S3.E3.m1.4.4.1.1.2.3"></times><ci id="S3.E3.m1.4.4.1.1.2.4a.cmml" xref="S3.E3.m1.4.4.1.1.2.4"><mtext class="ltx_mathvariant_bold-italic" id="S3.E3.m1.4.4.1.1.2.4.cmml" xref="S3.E3.m1.4.4.1.1.2.4">CE</mtext></ci><interval closure="open" id="S3.E3.m1.4.4.1.1.2.2.3.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2"><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3">ğŒ</ci><list id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><cn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2">ğ‘</ci><cn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3">2</cn></apply><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">â€¦</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.2">ğ‘</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3"><minus id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1"></minus><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2">ğ‘™</ci><cn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" type="integer" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></list></apply><list id="S3.E3.m1.4.4.1.1.2.2.2.2.4.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3"><apply id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.2">ğ‘</ci><cn id="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.3.cmml" type="integer" xref="S3.E3.m1.4.4.1.1.2.2.2.2.1.1.3">1</cn></apply><apply id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.2">ğ‘</ci><cn id="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">â€¦</ci><apply id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.1.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.2">ğ‘</ci><ci id="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.3.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3.3.3">ğ‘™</ci></apply></list></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\mathcal{L}_{train}=\textbf{{CE}}(\mathbf{M}([c,q_{1},q_{2},\ldots,q_{l-1}]),[%
q_{1},q_{2},\ldots,q_{l}]),</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.4d">caligraphic_L start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT = CE ( bold_M ( [ italic_c , italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_q start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT ] ) , [ italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ] ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.7">where <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic" id="S3.SS1.p1.7.1">CE</span> denotes cross-entropy loss, <math alttext="\mathbf{M}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m2.1"><semantics id="S3.SS1.p1.6.m2.1a"><mi id="S3.SS1.p1.6.m2.1.1" xref="S3.SS1.p1.6.m2.1.1.cmml">ğŒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m2.1b"><ci id="S3.SS1.p1.6.m2.1.1.cmml" xref="S3.SS1.p1.6.m2.1.1">ğŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m2.1c">\mathbf{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m2.1d">bold_M</annotation></semantics></math> denotes the sequence model, and <math alttext="l" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m3.1"><semantics id="S3.SS1.p1.7.m3.1a"><mi id="S3.SS1.p1.7.m3.1.1" xref="S3.SS1.p1.7.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m3.1b"><ci id="S3.SS1.p1.7.m3.1.1.cmml" xref="S3.SS1.p1.7.m3.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m3.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m3.1d">italic_l</annotation></semantics></math> is the sequence length.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Unified Conditional Decoding</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">Autoregressive image generation models leverage the two-phase generation process, including the prefilling and decodingÂ <cite class="ltx_cite ltx_citemacro_citep">(Pope etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib33" title="">2023</a>; Kwon etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib20" title="">2023</a>)</cite>, where prefilling processes the prompt tokens (or control tokens) and stores them in the KV CacheÂ <cite class="ltx_cite ltx_citemacro_citep">(Pope etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib33" title="">2023</a>)</cite>, and then decoding follows next-token prediction and aims to generate the output tokens (<span class="ltx_text ltx_font_italic" id="S3.SS2.p1.2.1">e</span>.<span class="ltx_text ltx_font_italic" id="S3.SS2.p1.2.2">g</span>., image tokens).
In ControlAR, we bring the condition into the decoding phase by adding the control condition token to the image token, which we refer to as conditional decoding.
Specifically, modify Eq.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.E2" title="In 3.1 Preliminary: Image Generation with Autoregressive Models â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathbf{q})=\prod_{t=1}^{h\cdot w}p(q_{t}|q_{1}+C_{2},q_{2}+C_{3},\ldots,q_{%
t-1}+C_{t},c+C_{1})," class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.3.2" xref="S3.E4.m1.3.3.1.1.3.2.cmml">p</mi><mo id="S3.E4.m1.3.3.1.1.3.1" xref="S3.E4.m1.3.3.1.1.3.1.cmml">â¢</mo><mrow id="S3.E4.m1.3.3.1.1.3.3.2" xref="S3.E4.m1.3.3.1.1.3.cmml"><mo id="S3.E4.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">ğª</mi><mo id="S3.E4.m1.3.3.1.1.3.3.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.2" rspace="0.111em" xref="S3.E4.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml"><munderover id="S3.E4.m1.3.3.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.2.cmml"><mo id="S3.E4.m1.3.3.1.1.1.2.2.2" movablelimits="false" xref="S3.E4.m1.3.3.1.1.1.2.2.2.cmml">âˆ</mo><mrow id="S3.E4.m1.3.3.1.1.1.2.2.3" xref="S3.E4.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2.2.3.2" xref="S3.E4.m1.3.3.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E4.m1.3.3.1.1.1.2.2.3.1" xref="S3.E4.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E4.m1.3.3.1.1.1.2.2.3.3" xref="S3.E4.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S3.E4.m1.3.3.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.1.2.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2.3.2" xref="S3.E4.m1.3.3.1.1.1.2.3.2.cmml">h</mi><mo id="S3.E4.m1.3.3.1.1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E4.m1.3.3.1.1.1.2.3.1.cmml">â‹…</mo><mi id="S3.E4.m1.3.3.1.1.1.2.3.3" xref="S3.E4.m1.3.3.1.1.1.2.3.3.cmml">w</mi></mrow></munderover><mrow id="S3.E4.m1.3.3.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.3.cmml">p</mi><mo id="S3.E4.m1.3.3.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.2.cmml">q</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.3.cmml">t</mi></msub><mo fence="false" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.5" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.5.cmml">|</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.5.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml">q</mi><mn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.5" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.5.cmml">,</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.2.cmml">q</mi><mn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.1.cmml">+</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.2.cmml">C</mi><mn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.3.cmml">3</mn></msub></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.6" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.5.cmml">,</mo><mi id="S3.E4.m1.2.2" mathvariant="normal" xref="S3.E4.m1.2.2.cmml">â€¦</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.7" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.5.cmml">,</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.2.cmml">q</mi><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.2.cmml">t</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.1.cmml">âˆ’</mo><mn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.1.cmml">+</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2.cmml">C</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3.cmml">t</mi></msub></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.8" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.5.cmml">,</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.2.cmml">c</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.1.cmml">+</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.2.cmml">C</mi><mn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.3.cmml">1</mn></msub></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" xref="S3.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><eq id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"></eq><apply id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"><times id="S3.E4.m1.3.3.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.1"></times><ci id="S3.E4.m1.3.3.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2">ğ‘</ci><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğª</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><apply id="S3.E4.m1.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.3.3.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.2">product</csymbol><apply id="S3.E4.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.3"><eq id="S3.E4.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S3.E4.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.2.3.2">ğ‘¡</ci><cn id="S3.E4.m1.3.3.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3"><ci id="S3.E4.m1.3.3.1.1.1.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.1">â‹…</ci><ci id="S3.E4.m1.3.3.1.1.1.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.2">â„</ci><ci id="S3.E4.m1.3.3.1.1.1.2.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.3.3">ğ‘¤</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.2"></times><ci id="S3.E4.m1.3.3.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.3">ğ‘</ci><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.5.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.5">conditional</csymbol><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.2">ğ‘</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.6.3">ğ‘¡</ci></apply><list id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.5.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4"><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1"><plus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘</ci><cn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3">1</cn></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">ğ¶</ci><cn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">2</cn></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2"><plus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.1"></plus><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.2">ğ‘</ci><cn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.2.3">2</cn></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.2">ğ¶</ci><cn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.2.2.3.3">3</cn></apply></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">â€¦</ci><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3"><plus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.1"></plus><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.2">ğ‘</ci><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3"><minus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.1"></minus><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.2">ğ‘¡</ci><cn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.2">ğ¶</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.3.3.3">ğ‘¡</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4"><plus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.1"></plus><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.2">ğ‘</ci><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.2">ğ¶</ci><cn id="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.4.4.4.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">p(\mathbf{q})=\prod_{t=1}^{h\cdot w}p(q_{t}|q_{1}+C_{2},q_{2}+C_{3},\ldots,q_{%
t-1}+C_{t},c+C_{1}),</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_p ( bold_q ) = âˆ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h â‹… italic_w end_POSTSUPERSCRIPT italic_p ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , â€¦ , italic_q start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_c + italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.1">where <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_C</annotation></semantics></math> represents the control condition token generated from the control image.
The number of control condition tokens is the same as the number of image tokens expected to be generated.
It is worth noting that we use displacement by one position when adding the control condition tokens to the sequence, which allows the model to make autoregressive predictions with control information corresponding to the next image token.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Conditional decoding avoids the network having to learn the positional correspondence between the condition signal and the image, as the positional information is fixed into the sequence during the fusion of the control condition tokens.
Additionally, the computational increase of this approach to the generation process is minimal.
Inputting conditional signals by prefilling additional tokens will result in a significant increase in computational complexity, especially when the computational complexity of the sequence model is quadratic to the length of the sequence, as in the case of the TransformerÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib54" title="">2017</a>)</cite>. The results in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (d) demonstrate this.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Controllable Autoregressive Model</h3>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Overall architecture.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">In our ControlAR framework shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.F3" title="Figure 3 â€£ Control encoder. â€£ 3.3 Controllable Autoregressive Model â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">3</span></a>, controllable generation occurs in two main steps. First, we employ a control encoder to extract features from the control images, such as hed edges, to generate a control condition sequence of length <math alttext="L" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">italic_L</annotation></semantics></math>, as depicted in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.F3" title="Figure 3 â€£ Control encoder. â€£ 3.3 Controllable Autoregressive Model â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">3</span></a> (a).
The second step involves integrating the control condition tokens into the autoregressive image generation process, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.F3" title="Figure 3 â€£ Control encoder. â€£ 3.3 Controllable Autoregressive Model â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">3</span></a> (b).
To achieve this, we expand the sequence layer (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p1.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p1.1.2">g</span>., causal Transformer layer or Mamba layer) of the autoregressive model to <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p1.1.3">conditional sequence layer</span> by directly adding the control condition tokens to the image tokens based on positional correspondence, as discussed in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S3.SS2" title="3.2 Unified Conditional Decoding â€£ 3 ControlAR â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
Specifically, we adopt a MLP to project the control tokens and then fuse them with image tokens via the simple addition.
Furthermore, to strengthen the control of the conditions over the generated images, we evenly replace the conditional sequence layer three times in the autoregressive model.
Throughout the training process of our ControlAR, we update the parameters of the sequence model, thereby enhancing the modelâ€™s capability for stronger and more controllable image generation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Control encoder.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">We propose a lightweight control encoder to transform control image to control condition tokens.
In contrast to previous approaches such as ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>)</cite> and T2I-AdapterÂ <cite class="ltx_cite ltx_citemacro_citep">(Mou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib28" title="">2024</a>)</cite>, we utilize the Vision Transformer (ViT)Â <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib7" title="">2020</a>)</cite> for feature extraction of control images. We believe that a ViT model, pre-trained on a large amount of data, is more adept at modeling sequences than a randomly initialized CNN network. For the class-to-image task on ImageNet, we use ViT-S to initialize our control encoder. Additionally, for the text-to-image task, we employ DINOv2-SÂ <cite class="ltx_cite ltx_citemacro_citep">(Oquab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib30" title="">2023</a>)</cite> as the initialization scheme for the control encoder. Further details on this are available in sectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.SS3.SSS0.Px1" title="Ablations on the Control Encoder. â€£ 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>. Notably, our ControlAR achieves efficient controllable generation with a control encoder comprising only about 22M parameters, which is significantly less than the 361M parameter count of ControlNet++.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="573" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S3.F3.6.1">The overall architecture of ControlAR</span>. The control image will be flattened into patches and encoded as a sequence of control tokens via the proposed <span class="ltx_text ltx_font_bold" id="S3.F3.7.2">control encoder</span>.
For controllable image generation, we extend several sequential layers (<span class="ltx_text ltx_font_italic" id="S3.F3.8.3">i</span>.<span class="ltx_text ltx_font_italic" id="S3.F3.9.4">e</span>., causal Transformer layer or Mamba layer) of the autoregressive model into <span class="ltx_text ltx_font_bold" id="S3.F3.10.5">conditional sequential layers</span> by incorporating the fusion of control tokens and image tokens to predict the next image token. Finally, the image tokens are decoded into a generated image through the VQGAN decoder.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Autoregressive arbitrary-resolution generation.</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Benefiting from the proposed conditional decoding, which generates the next image token conditioned on the current control token and the number of image tokens aligns with the control tokens.
Therefore, we can directly adjust the resolutions of generated images according to the length of the control tokens, allowing the autoregressive models to generate arbitrary-resolution images.
Rather than resizing the control images into a fixed resolution, <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.2">g</span>., <math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">512</mn><mo id="S3.SS4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><cn id="S3.SS4.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1.2">512</cn><cn id="S3.SS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">512 Ã— 512</annotation></semantics></math>, we can directly input the control images with original resolutions into ControlAR to obtain the generated images.
To further enhance the image quality of arbitrary-resolution image generation, we adopt a multi-resolution training recipe, which randomly samples different resolutions, and present the Multi-Resolution ControlAR (MR-ControlAR).
Without extra modules or parameters, our MR-ControlAR is capable of generating image of arbitrary resolutions without significant quality degradation, thereby further expanding the versatility of autoregressive models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">Our experiments are divided into two main parts: class-to-image (C2I) and text-to-image (T2I) controllable generation.
For the former, we follow ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>)</cite> to extract the canny edges and depth maps of the images in ImageNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib5" title="">2009</a>)</cite> for training.
In T2I experiments, we train controllable generation for segmentation masks, canny edges, hed edges, lineart edges, and depth maps. For segmentation masks, we use ADE20KÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib60" title="">2017</a>)</cite> and COCOStuffÂ <cite class="ltx_cite ltx_citemacro_citep">(Caesar etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib1" title="">2018</a>)</cite> as training data, with the text captions sourced from ControlNet++Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib22" title="">2024b</a>)</cite>, which adopts MiniGPT-4Â <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib62" title="">2023</a>)</cite> to obtain a short description of the image.
Furthermore, we use a subset of LAION-AestheticsÂ <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib44" title="">2022</a>)</cite>, MultiGen-20MÂ <cite class="ltx_cite ltx_citemacro_citep">(Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib35" title="">2023b</a>)</cite>, as the training data for canny edge, hed edge, lineart edge, and depth map controllable generation. Additional details are provided in the supplementary material.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Evaluation and metrics.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We train the proposed ControlAR for different controllable generation tasks on several datasets and evaluate them using the corresponding validation datasets.
We mainly employ two metrics: conditional consistency and FrÃ©chet Inception Distance (FID)Â <cite class="ltx_cite ltx_citemacro_citep">(Heusel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib14" title="">2017</a>)</cite>.
We evaluate the conditional consistency by calculating the similarity between the input condition images and the extracted condition images from the generated images.
When evaluating segmentation masks control, we use a segmentation method, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.1">i</span>.<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.2">e</span>., Mask2FormerÂ <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib4" title="">2022</a>)</cite>, to compute the mean Intersection-over-Union (mIoU) on generated images.
We adopt the F1-Score and Root Mean Square Error (RMSE) to evaluate the similarity of canny edges and depth maps, respectively.
Additionally, for hed edge and lineart edge, we utilize SSIM as the metric.
Alongside these quantitative metrics, we provide abundant qualitative visualizations on diverse controls.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Implementation details.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.2">In C2I controllable generation experiments, we employ LlamaGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite> and AiMÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib21" title="">2024a</a>)</cite> as the foundational autoregressive models for ControlAR. During the fine-tuning on ImageNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib5" title="">2009</a>)</cite>, we adopt the AdamW optimizerÂ <cite class="ltx_cite ltx_citemacro_citep">(Kingma, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib18" title="">2014</a>)</cite>. The learning rate is set to 1e-4 and 8e-4 for training LlamaGen and AiM respectively. We use the image size of <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">256</mn><mo id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1"></times><cn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2">256</cn><cn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px3.p1.1.m1.1d">256 Ã— 256</annotation></semantics></math>, with a batch size of 256 for canny edge and depth maps.
In T2I experiments, we mainly use LlamaGen-XL, which is built upon a T5 encoderÂ <cite class="ltx_cite ltx_citemacro_citep">(Raffel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib37" title="">2020</a>)</cite> and contains 775M parameters.
We employ the AdamW optimizer with a learning rate of 5e-5 and resize both input and control images to <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS1.SSS0.Px3.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml">512</mn><mo id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1"><times id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.1"></times><cn id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2">512</cn><cn id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px3.p1.2.m2.1d">512 Ã— 512</annotation></semantics></math> for comparison with other methods.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">C2I controllable generation.</h4>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S4.T1.12.1">C2I controllable generation.</span> Param. denotes the number of parameters of the C2I model. â€œ<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.4.m1.1"><semantics id="S4.T1.4.m1.1b"><mo id="S4.T1.4.m1.1.1" stretchy="false" xref="S4.T1.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.m1.1c"><ci id="S4.T1.4.m1.1.1.cmml" xref="S4.T1.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.m1.1e">â†‘</annotation></semantics></math>â€ or â€œ<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.5.m2.1"><semantics id="S4.T1.5.m2.1b"><mo id="S4.T1.5.m2.1.1" stretchy="false" xref="S4.T1.5.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.m2.1c"><ci id="S4.T1.5.m2.1.1.cmml" xref="S4.T1.5.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.m2.1e">â†“</annotation></semantics></math>â€ indicate lower or higher values are better. â€œ*â€ indicates that ControlVARâ€™s FID values are estimated from its histogramsÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib23" title="">2024c</a>)</cite>. The results are conducted on <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.T1.6.m3.1"><semantics id="S4.T1.6.m3.1b"><mrow id="S4.T1.6.m3.1.1" xref="S4.T1.6.m3.1.1.cmml"><mn id="S4.T1.6.m3.1.1.2" xref="S4.T1.6.m3.1.1.2.cmml">256</mn><mo id="S4.T1.6.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T1.6.m3.1.1.1.cmml">Ã—</mo><mn id="S4.T1.6.m3.1.1.3" xref="S4.T1.6.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.m3.1c"><apply id="S4.T1.6.m3.1.1.cmml" xref="S4.T1.6.m3.1.1"><times id="S4.T1.6.m3.1.1.1.cmml" xref="S4.T1.6.m3.1.1.1"></times><cn id="S4.T1.6.m3.1.1.2.cmml" type="integer" xref="S4.T1.6.m3.1.1.2">256</cn><cn id="S4.T1.6.m3.1.1.3.cmml" type="integer" xref="S4.T1.6.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.m3.1d">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.m3.1e">256 Ã— 256</annotation></semantics></math> resolution.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.10">
<tr class="ltx_tr" id="S4.T1.10.5">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.10.5.1" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T1.10.5.1.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.10.5.2" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T1.10.5.2.1">C2I Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.10.5.3" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T1.10.5.3.1">Param.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.10.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">Canny</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.10.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">Depth</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">F1-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.7.1.1.m1.1"><semantics id="S4.T1.7.1.1.m1.1a"><mo id="S4.T1.7.1.1.m1.1.1" stretchy="false" xref="S4.T1.7.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.1.1.m1.1b"><ci id="S4.T1.7.1.1.m1.1.1.cmml" xref="S4.T1.7.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.8.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.8.2.2.m1.1"><semantics id="S4.T1.8.2.2.m1.1a"><mo id="S4.T1.8.2.2.m1.1.1" stretchy="false" xref="S4.T1.8.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.2.2.m1.1b"><ci id="S4.T1.8.2.2.m1.1.1.cmml" xref="S4.T1.8.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.2.2.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.9.3.3.m1.1"><semantics id="S4.T1.9.3.3.m1.1a"><mo id="S4.T1.9.3.3.m1.1.1" stretchy="false" xref="S4.T1.9.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.3.3.m1.1b"><ci id="S4.T1.9.3.3.m1.1.1.cmml" xref="S4.T1.9.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.3.3.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.10.4.4.m1.1"><semantics id="S4.T1.10.4.4.m1.1a"><mo id="S4.T1.10.4.4.m1.1.1" stretchy="false" xref="S4.T1.10.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.4.4.m1.1b"><ci id="S4.T1.10.4.4.m1.1.1.cmml" xref="S4.T1.10.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.4.4.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.10.6.1" rowspan="4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T1.10.6.1.1">ControlVAR*</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.10.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">VAR-d16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">310M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">16.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">13.80</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.7">
<td class="ltx_td ltx_align_left" id="S4.T1.10.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">VAR-d20</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">600M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">13.00</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.7.6" style="padding-left:4.0pt;padding-right:4.0pt;">13.40</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.8">
<td class="ltx_td ltx_align_left" id="S4.T1.10.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">VAR-d24</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">1.0B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">15.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">12.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.9">
<td class="ltx_td ltx_align_left" id="S4.T1.10.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">VAR-d30</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">2.0B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">7.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">6.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.10.10.1" rowspan="3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T1.10.10.1.1">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.10.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">AiM-L</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">350M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">30.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.5" style="padding-left:4.0pt;padding-right:4.0pt;">9.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.6" style="padding-left:4.0pt;padding-right:4.0pt;">35.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.7" style="padding-left:4.0pt;padding-right:4.0pt;">7.39</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.11">
<td class="ltx_td ltx_align_left" id="S4.T1.10.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">LlamaGen-B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">111M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.11.3" style="padding-left:4.0pt;padding-right:4.0pt;">34.15</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.11.4" style="padding-left:4.0pt;padding-right:4.0pt;">10.64</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.11.5" style="padding-left:4.0pt;padding-right:4.0pt;">32.41</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.11.6" style="padding-left:4.0pt;padding-right:4.0pt;">6.67</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.10.12.1" style="padding-left:4.0pt;padding-right:4.0pt;">LlamaGen-L</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.12.2" style="padding-left:4.0pt;padding-right:4.0pt;">343M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.12.3" style="padding-left:4.0pt;padding-right:4.0pt;">34.91</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.12.4" style="padding-left:4.0pt;padding-right:4.0pt;">7.69</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.12.5" style="padding-left:4.0pt;padding-right:4.0pt;">31.11</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.12.6" style="padding-left:4.0pt;padding-right:4.0pt;">4.19</td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="S4.F4.g1" src="x4.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S4.F4.2.1">Visualization of C2I controllable generation.</span> Our ControlAR generates images with high conditional consistency and quality on both LlamaGen and AiM.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We utilize the ImageNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib5" title="">2009</a>)</cite> to conduct controllable generation experiments for C2I, and the results are shown in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T1" title="Table 1 â€£ C2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">1</span></a>.
We calculate the conditional consistency (F1-Score or RMSE) and the FID of the images generated by ControlAR and compare the FID with ControlVARÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib23" title="">2024c</a>)</cite>. It shows that our proposed ControlAR achieves lower FID based on the LlamaGen-L, which only has 16.7% of parameters of VAR-d30Â <cite class="ltx_cite ltx_citemacro_citep">(Tian etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib50" title="">2024</a>)</cite>. In addition, the experimental results show that our method achieves good results with different autoregressive models including Transformer-based LlamaGen and Mamba-based AiM.
Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.F4" title="Figure 4 â€£ C2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the visualizations of ControlAR with different autoregressive models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">T2I controllable generation.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We mainly employ LlamaGen-XL as the autoregressive model for T2I generation.
Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T2" title="Table 2 â€£ T2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> presents the quantitative comparison of controllability with state-of-the-art methods.
Among those methods in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T2" title="Table 2 â€£ T2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a>, GLIGENÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib24" title="">2023</a>)</cite> utilizes SD1.4Â <cite class="ltx_cite ltx_citemacro_citep">(Rombach etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib40" title="">2022</a>)</cite> as the generative model, while T2I-AdapterÂ <cite class="ltx_cite ltx_citemacro_citep">(Mou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib28" title="">2024</a>)</cite>, Uni-ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib59" title="">2024</a>)</cite>, UniControlÂ <cite class="ltx_cite ltx_citemacro_citep">(Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib35" title="">2023b</a>)</cite>, ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>)</cite>, and ControlNet++Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib22" title="">2024b</a>)</cite> adopt SD1.5Â <cite class="ltx_cite ltx_citemacro_citep">(Rombach etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib40" title="">2022</a>)</cite> as the generative model.
As Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T2" title="Table 2 â€£ T2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows, it is evident that our ControlAR is highly competitive compared to the existing diffusion-based methods.
The proposed ControlAR significantly outperforms ControlNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib57" title="">2023a</a>)</cite> in terms of diverse control tasks.
Compared to ControlNet++Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib22" title="">2024b</a>)</cite>, which is fine-tuned based on the well-established ControlNet, our ControlAR demonstrates comparable or even better controlling performance, for example, achieving an improvement of 4.66 SSIM on the hed edges task.
Additionally, we report the FID for the generated images in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T3" title="Table 3 â€£ T2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">3</span></a>. Our approach attains the better FID across various tasks compared with ControlNet++, indicating that it not only possesses strong controllability but also ensures the quality of image generation. We provide qualitative comparison in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.F5" title="Figure 5 â€£ T2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">5</span></a>, and more visualizations are available in the supplementary material.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S4.T2.14.1">Conditional consistency of T2I controllable generation.</span> â€œ<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.m1.1"><semantics id="S4.T2.4.m1.1b"><mo id="S4.T2.4.m1.1.1" stretchy="false" xref="S4.T2.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.m1.1c"><ci id="S4.T2.4.m1.1.1.cmml" xref="S4.T2.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.m1.1e">â†‘</annotation></semantics></math>â€ or â€œ<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.5.m2.1"><semantics id="S4.T2.5.m2.1b"><mo id="S4.T2.5.m2.1.1" stretchy="false" xref="S4.T2.5.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.m2.1c"><ci id="S4.T2.5.m2.1.1.cmml" xref="S4.T2.5.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.m2.1e">â†“</annotation></semantics></math>â€ indicate lower or higher values are better. â€œ-â€ denotes that the method does not release a model for testing. The results are conducted on <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T2.6.m3.1"><semantics id="S4.T2.6.m3.1b"><mrow id="S4.T2.6.m3.1.1" xref="S4.T2.6.m3.1.1.cmml"><mn id="S4.T2.6.m3.1.1.2" xref="S4.T2.6.m3.1.1.2.cmml">512</mn><mo id="S4.T2.6.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.6.m3.1.1.1.cmml">Ã—</mo><mn id="S4.T2.6.m3.1.1.3" xref="S4.T2.6.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.m3.1c"><apply id="S4.T2.6.m3.1.1.cmml" xref="S4.T2.6.m3.1.1"><times id="S4.T2.6.m3.1.1.1.cmml" xref="S4.T2.6.m3.1.1.1"></times><cn id="S4.T2.6.m3.1.1.2.cmml" type="integer" xref="S4.T2.6.m3.1.1.2">512</cn><cn id="S4.T2.6.m3.1.1.3.cmml" type="integer" xref="S4.T2.6.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m3.1d">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.m3.1e">512 Ã— 512</annotation></semantics></math> resolution.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.12">
<tr class="ltx_tr" id="S4.T2.12.7">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.12.7.1" rowspan="3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.12.7.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.12.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">Seg.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.12.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">Canny</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.12.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">Hed</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.12.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">Lineart</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.12.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">Depth</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">mIoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.7.1.1.m1.1"><semantics id="S4.T2.7.1.1.m1.1a"><mo id="S4.T2.7.1.1.m1.1.1" stretchy="false" xref="S4.T2.7.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.1.1.m1.1b"><ci id="S4.T2.7.1.1.m1.1.1.cmml" xref="S4.T2.7.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.8.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">mIoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.8.2.2.m1.1"><semantics id="S4.T2.8.2.2.m1.1a"><mo id="S4.T2.8.2.2.m1.1.1" stretchy="false" xref="S4.T2.8.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.2.2.m1.1b"><ci id="S4.T2.8.2.2.m1.1.1.cmml" xref="S4.T2.8.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.2.2.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">F1-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.9.3.3.m1.1"><semantics id="S4.T2.9.3.3.m1.1a"><mo id="S4.T2.9.3.3.m1.1.1" stretchy="false" xref="S4.T2.9.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.3.3.m1.1b"><ci id="S4.T2.9.3.3.m1.1.1.cmml" xref="S4.T2.9.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.3.3.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.10.4.4.m1.1"><semantics id="S4.T2.10.4.4.m1.1a"><mo id="S4.T2.10.4.4.m1.1.1" stretchy="false" xref="S4.T2.10.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.4.4.m1.1b"><ci id="S4.T2.10.4.4.m1.1.1.cmml" xref="S4.T2.10.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.4.4.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.11.5.5.m1.1"><semantics id="S4.T2.11.5.5.m1.1a"><mo id="S4.T2.11.5.5.m1.1.1" stretchy="false" xref="S4.T2.11.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.5.5.m1.1b"><ci id="S4.T2.11.5.5.m1.1.1.cmml" xref="S4.T2.11.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.5.5.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.12.6.6.m1.1"><semantics id="S4.T2.12.6.6.m1.1a"><mo id="S4.T2.12.6.6.m1.1.1" stretchy="false" xref="S4.T2.12.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.6.6.m1.1b"><ci id="S4.T2.12.6.6.m1.1.1.cmml" xref="S4.T2.12.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.6.6.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">ADE20K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">COCOStuff</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.8.6" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.12.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">GLIGEN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">23.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">26.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.9.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.12.9.7" style="padding-left:3.0pt;padding-right:3.0pt;">38.83</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.10">
<td class="ltx_td ltx_align_left" id="S4.T2.12.10.1" style="padding-left:3.0pt;padding-right:3.0pt;">T2I-Adapter</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.10.2" style="padding-left:3.0pt;padding-right:3.0pt;">12.61</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.10.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.10.4" style="padding-left:3.0pt;padding-right:3.0pt;">23.65</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.10.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.10.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.10.7" style="padding-left:3.0pt;padding-right:3.0pt;">48.40</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.11">
<td class="ltx_td ltx_align_left" id="S4.T2.12.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">Uni-ControlNet</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.11.2" style="padding-left:3.0pt;padding-right:3.0pt;">19.39</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.11.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.11.4" style="padding-left:3.0pt;padding-right:3.0pt;">27.32</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.11.5" style="padding-left:3.0pt;padding-right:3.0pt;">69.10</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.11.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.11.7" style="padding-left:3.0pt;padding-right:3.0pt;">40.65</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12">
<td class="ltx_td ltx_align_left" id="S4.T2.12.12.1" style="padding-left:3.0pt;padding-right:3.0pt;">UniControl</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.2" style="padding-left:3.0pt;padding-right:3.0pt;">25.44</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.4" style="padding-left:3.0pt;padding-right:3.0pt;">30.82</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.5" style="padding-left:3.0pt;padding-right:3.0pt;">79.69</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.7" style="padding-left:3.0pt;padding-right:3.0pt;">39.18</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.13">
<td class="ltx_td ltx_align_left" id="S4.T2.12.13.1" style="padding-left:3.0pt;padding-right:3.0pt;">ControlNet</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.13.2" style="padding-left:3.0pt;padding-right:3.0pt;">32.55</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.13.3" style="padding-left:3.0pt;padding-right:3.0pt;">27.46</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.13.4" style="padding-left:3.0pt;padding-right:3.0pt;">34.65</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.13.5" style="padding-left:3.0pt;padding-right:3.0pt;">76.21</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.13.6" style="padding-left:3.0pt;padding-right:3.0pt;">70.54</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.13.7" style="padding-left:3.0pt;padding-right:3.0pt;">35.90</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.14">
<td class="ltx_td ltx_align_left" id="S4.T2.12.14.1" style="padding-left:3.0pt;padding-right:3.0pt;">ControlNet++</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.14.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.14.2.1">43.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.14.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.12.14.3.1">34.56</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.14.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.12.14.4.1">37.04</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.14.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.12.14.5.1">80.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.14.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.14.6.1">83.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.14.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.14.7.1">28.32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.15">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.12.15.1" style="padding-left:3.0pt;padding-right:3.0pt;">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.15.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.12.15.2.1">39.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.15.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.15.3.1">37.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.15.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.15.4.1">37.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.15.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.15.5.1">85.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.15.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.12.15.6.1">79.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.15.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.12.15.7.1">29.01</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S4.T3.2.1">FID of T2I controllable generation.</span> â€œ-â€ denotes that the method does not release a model for testing. Our ControlAR achieves significant FID improvements.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.3">
<tr class="ltx_tr" id="S4.T3.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.3.1.1" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T3.3.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.3.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">Seg.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Canny</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">Hed</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">Lineart</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">Depth</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">ADE20K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">COCOStuff</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">MultiGen-20M</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">GLIGEN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">33.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">18.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">18.36</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4">
<td class="ltx_td ltx_align_left" id="S4.T3.3.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">T2I-Adapter</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">39.15</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.4.4.1">15.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">22.52</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5">
<td class="ltx_td ltx_align_left" id="S4.T3.3.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">Uni-ControlNet</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">39.70</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">17.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">17.08</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">20.27</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6">
<td class="ltx_td ltx_align_left" id="S4.T3.3.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">UniControl</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">46.34</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">19.94</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">15.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">18.66</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7">
<td class="ltx_td ltx_align_left" id="S4.T3.3.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">ControlNet</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">33.28</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">21.33</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.7.4.1">14.73</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">15.41</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">17.44</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.7" style="padding-left:3.0pt;padding-right:3.0pt;">17.76</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.8">
<td class="ltx_td ltx_align_left" id="S4.T3.3.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">ControlNet++</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.8.2.1">29.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.8.3.1">19.29</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">18.23</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.8.5.1">15.01</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.8.6.1">13.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.8.7.1">16.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.9.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.9.2.1">27.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.9.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.9.3.1">14.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">17.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.9.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.9.5.1">10.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.9.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.9.6.1">12.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.9.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.9.7.1">14.61</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="614" id="S4.F5.g1" src="x5.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S4.F5.2.1">Visualization of text-to-image controllable generation.</span> We use red boxes to mark areas where the generated results of other methods differ from the input control image.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S4.F6.g1" src="x6.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S4.F6.6.1">Comparison of ControlAR and Multi-Resolution ControlAR.</span> (a) shows the generation process of ControlAR and MR-ControlAR under the resolution of 768 <math alttext="\times" class="ltx_Math" display="inline" id="S4.F6.3.m1.1"><semantics id="S4.F6.3.m1.1b"><mo id="S4.F6.3.m1.1.1" xref="S4.F6.3.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.F6.3.m1.1c"><times id="S4.F6.3.m1.1.1.cmml" xref="S4.F6.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.3.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S4.F6.3.m1.1e">Ã—</annotation></semantics></math> 512. â€œDecoding <math alttext="\times" class="ltx_Math" display="inline" id="S4.F6.4.m2.1"><semantics id="S4.F6.4.m2.1b"><mo id="S4.F6.4.m2.1.1" xref="S4.F6.4.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.F6.4.m2.1c"><times id="S4.F6.4.m2.1.1.cmml" xref="S4.F6.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.4.m2.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S4.F6.4.m2.1e">Ã—</annotation></semantics></math> 1024â€ denotes that 1024 image tokens need to be decoded for output. (b) compares the conditional consistency of ControlAR and MR-ControlAR under different resolutions of control conditions.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Arbitrary-Resolution Generation.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">Instead of uniformly resizing the controls and images to 512 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><times id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.1.m1.1d">Ã—</annotation></semantics></math> 512,
we adopt a set of resolutions ranging from 384 to 1024 and keep the image sequence length to no more than 2304 to train our Multi-Resolution ControlAR.
Direct end-to-end controllable generation using MR-ControlAR preserves the detailed features of the control image and avoids the loss of information due to scaling. We show the difference between these two approaches in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.F6" title="Figure 6 â€£ T2I controllable generation. â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">6</span></a> (a), and perform hed edge control generation experiments on images with different resolution ratios in the validation set of MultiGen-20M. Experimental results show that after multi-resolution training, MR-ControlAR can ensure that the generation of images with different resolution ratios is not impaired. We show the visualization at different resolutions in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S0.F1" title="Figure 1 â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Ablations on the Control Encoder.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">In Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T4" title="Table 4 â€£ Ablations on the Control Encoder. â€£ 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we conduct experiments using different encoders (or pre-training schemes) towards different controls, including canny edge, depth map, and hed edge.
Firstly, we follow T2I-AdapterÂ <cite class="ltx_cite ltx_citemacro_citep">(Mou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib28" title="">2024</a>)</cite> and design a vanilla convolutional control encoder with 4 consecutive residual blocksÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib13" title="">2016</a>)</cite> and a total downsample ratio of 16.
The vanilla CNN-based control encoder contains 21.8M parameters, which has similar parameters with ViT-SÂ <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib7" title="">2020</a>)</cite>.
Further, we explore the effects of using pre-trained ViTs as our control encoder and adopt ViTs with different pre-trained schemes, <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px1.p1.1.1">i</span>.<span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px1.p1.1.2">e</span>., the ImageNet-supervisedÂ <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib7" title="">2020</a>)</cite> and self-supervisedÂ <cite class="ltx_cite ltx_citemacro_citep">(Oquab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib30" title="">2023</a>)</cite>.
For our experiments, we employ LlamaGen-Bâ€™s C2I model to conduct experiments on the ImageNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib5" title="">2009</a>)</cite> for canny edge and depth map conditions, while using LlamaGen-XLâ€™s T2I model to carry out experiments on the MultiGen-20MÂ <cite class="ltx_cite ltx_citemacro_citep">(Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib35" title="">2023b</a>)</cite> for the hed edge condition. As depicted in the table, different control encoders exhibit varying performance across different datasets, with the ViT-S demonstrating superior suitability for the ImageNet dataset, and the DINOv2-S yielding better results on the MultiGen-20M dataset.
Furthermore, we evaluate the performance of larger encoders such as DINOv2-B, and the outcomes reveal that higher parameter counts enable our method to achieve superior results.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="S4.T4.13.1">Ablations on the Control Encoder.</span> â€œ<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.3.m1.1"><semantics id="S4.T4.3.m1.1b"><mo id="S4.T4.3.m1.1.1" stretchy="false" xref="S4.T4.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.m1.1c"><ci id="S4.T4.3.m1.1.1.cmml" xref="S4.T4.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.m1.1e">â†‘</annotation></semantics></math>â€ or â€œ<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.4.m2.1"><semantics id="S4.T4.4.m2.1b"><mo id="S4.T4.4.m2.1.1" stretchy="false" xref="S4.T4.4.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.m2.1c"><ci id="S4.T4.4.m2.1.1.cmml" xref="S4.T4.4.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.m2.1e">â†“</annotation></semantics></math>â€ indicate lower or higher values are better.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.11">
<tr class="ltx_tr" id="S4.T4.11.8">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.11.8.1" rowspan="2"><span class="ltx_text" id="S4.T4.11.8.1.1">Control Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.11.8.2" rowspan="2"><span class="ltx_text" id="S4.T4.11.8.2.1">Params</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.11.8.3">Canny (C2I)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.11.8.4">Depth (C2I)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.11.8.5">Hed (T2I)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.1.1">F1-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.5.1.1.m1.1"><semantics id="S4.T4.5.1.1.m1.1a"><mo id="S4.T4.5.1.1.m1.1.1" stretchy="false" xref="S4.T4.5.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.1.1.m1.1b"><ci id="S4.T4.5.1.1.m1.1.1.cmml" xref="S4.T4.5.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.2.2">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.6.2.2.m1.1"><semantics id="S4.T4.6.2.2.m1.1a"><mo id="S4.T4.6.2.2.m1.1.1" stretchy="false" xref="S4.T4.6.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.2.2.m1.1b"><ci id="S4.T4.6.2.2.m1.1.1.cmml" xref="S4.T4.6.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.2.2.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.3.3">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.7.3.3.m1.1"><semantics id="S4.T4.7.3.3.m1.1a"><mo id="S4.T4.7.3.3.m1.1.1" stretchy="false" xref="S4.T4.7.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.3.3.m1.1b"><ci id="S4.T4.7.3.3.m1.1.1.cmml" xref="S4.T4.7.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.3.3.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.8.4.4">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.8.4.4.m1.1"><semantics id="S4.T4.8.4.4.m1.1a"><mo id="S4.T4.8.4.4.m1.1.1" stretchy="false" xref="S4.T4.8.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.4.4.m1.1b"><ci id="S4.T4.8.4.4.m1.1.1.cmml" xref="S4.T4.8.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.4.4.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.9.5.5">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.9.5.5.m1.1"><semantics id="S4.T4.9.5.5.m1.1a"><mo id="S4.T4.9.5.5.m1.1.1" stretchy="false" xref="S4.T4.9.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.5.5.m1.1b"><ci id="S4.T4.9.5.5.m1.1.1.cmml" xref="S4.T4.9.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.5.5.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.6.6">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.10.6.6.m1.1"><semantics id="S4.T4.10.6.6.m1.1a"><mo id="S4.T4.10.6.6.m1.1.1" stretchy="false" xref="S4.T4.10.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.6.6.m1.1b"><ci id="S4.T4.10.6.6.m1.1.1.cmml" xref="S4.T4.10.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.6.6.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.11.7.1">CNN (<math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S4.T4.11.7.1.m1.1"><semantics id="S4.T4.11.7.1.m1.1a"><mrow id="S4.T4.11.7.1.m1.1b"><mn id="S4.T4.11.7.1.m1.1.1">4</mn><mo id="S4.T4.11.7.1.m1.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.T4.11.7.1.m1.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.7.1.m1.1d">4 Ã—</annotation></semantics></math> Res. Blocks)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.2">21.8M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.3">33.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.4">12.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.5">33.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.6">6.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.7">81.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.11.7.8">15.33</td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.9">
<td class="ltx_td ltx_align_left" id="S4.T4.11.9.1">ViT-S</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.2">22.1M</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.3"><span class="ltx_text ltx_font_bold" id="S4.T4.11.9.3.1">34.15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.11.9.4.1">10.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.11.9.5.1">32.41</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.11.9.6.1">6.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.7">82.37</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.9.8">14.59</td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.10">
<td class="ltx_td ltx_align_left" id="S4.T4.11.10.1">DINOv2-S</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.2">22.1M</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.3">33.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.4">10.87</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.5">32.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.6">7.31</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.11.10.7.1">85.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.10.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.11.10.8.1">10.53</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.11.11.1">DINOv2-B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.2">86.6M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.11.11.3.1">34.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.4"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.4.1">9.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.5"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.5.1">31.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.6"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.6.1">6.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.7"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.7.1">86.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.8"><span class="ltx_text ltx_font_bold" id="S4.T4.11.11.8.1">8.58</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Ablations on the Control Fusion Strategy.</h4>
<figure class="ltx_table" id="S4.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T6.7" style="width:210.7pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 5: </span><span class="ltx_text ltx_font_bold" id="S4.T6.7.9.1">Ablations on the control fusion strategy.</span> â€œ<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.3.3.m1.1"><semantics id="S4.T6.3.3.m1.1b"><mo id="S4.T6.3.3.m1.1.1" stretchy="false" xref="S4.T6.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.m1.1c"><ci id="S4.T6.3.3.m1.1.1.cmml" xref="S4.T6.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.3.m1.1e">â†‘</annotation></semantics></math>â€ or â€œ<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.4.4.m2.1"><semantics id="S4.T6.4.4.m2.1b"><mo id="S4.T6.4.4.m2.1.1" stretchy="false" xref="S4.T6.4.4.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.m2.1c"><ci id="S4.T6.4.4.m2.1.1.cmml" xref="S4.T6.4.4.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.4.4.m2.1e">â†“</annotation></semantics></math>â€ indicate lower or higher values are better.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.7.7">
<tr class="ltx_tr" id="S4.T6.6.6.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.6.6.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">Fusion Strategy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.6.6.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">#Layer</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.5.5.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">F1-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.5.5.1.1.m1.1"><semantics id="S4.T6.5.5.1.1.m1.1a"><mo id="S4.T6.5.5.1.1.m1.1.1" stretchy="false" xref="S4.T6.5.5.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.1.1.m1.1b"><ci id="S4.T6.5.5.1.1.m1.1.1.cmml" xref="S4.T6.5.5.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.5.5.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.6.6.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.6.6.2.2.m1.1"><semantics id="S4.T6.6.6.2.2.m1.1a"><mo id="S4.T6.6.6.2.2.m1.1.1" stretchy="false" xref="S4.T6.6.6.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T6.6.6.2.2.m1.1b"><ci id="S4.T6.6.6.2.2.m1.1.1.cmml" xref="S4.T6.6.6.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.6.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.6.6.2.2.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.7.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.7.7.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">Cross-Attention</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.7.7.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">1-th</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.7.7.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">30.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.7.7.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">15.34</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.7.5">
<td class="ltx_td ltx_align_left" id="S4.T6.7.7.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">Addition</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">1-th</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">34.01</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">11.02</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.7.6">
<td class="ltx_td ltx_align_left" id="S4.T6.7.7.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">Addition</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">1,5,9-th</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">34.15</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">10.64</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.7.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.7.7.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">Addition</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.7.7.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<math alttext="1\!\sim\!12" class="ltx_Math" display="inline" id="S4.T6.7.7.3.1.m1.1"><semantics id="S4.T6.7.7.3.1.m1.1a"><mrow id="S4.T6.7.7.3.1.m1.1.1" xref="S4.T6.7.7.3.1.m1.1.1.cmml"><mn id="S4.T6.7.7.3.1.m1.1.1.2" xref="S4.T6.7.7.3.1.m1.1.1.2.cmml">1</mn><mo id="S4.T6.7.7.3.1.m1.1.1.1" lspace="0.108em" rspace="0.108em" xref="S4.T6.7.7.3.1.m1.1.1.1.cmml">âˆ¼</mo><mn id="S4.T6.7.7.3.1.m1.1.1.3" xref="S4.T6.7.7.3.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.7.7.3.1.m1.1b"><apply id="S4.T6.7.7.3.1.m1.1.1.cmml" xref="S4.T6.7.7.3.1.m1.1.1"><csymbol cd="latexml" id="S4.T6.7.7.3.1.m1.1.1.1.cmml" xref="S4.T6.7.7.3.1.m1.1.1.1">similar-to</csymbol><cn id="S4.T6.7.7.3.1.m1.1.1.2.cmml" type="integer" xref="S4.T6.7.7.3.1.m1.1.1.2">1</cn><cn id="S4.T6.7.7.3.1.m1.1.1.3.cmml" type="integer" xref="S4.T6.7.7.3.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.7.3.1.m1.1c">1\!\sim\!12</annotation><annotation encoding="application/x-llamapun" id="S4.T6.7.7.3.1.m1.1d">1 âˆ¼ 12</annotation></semantics></math>-th</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.7.7.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">34.21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.7.7.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">11.75</td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T6.13" style="width:178.9pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 6: </span><span class="ltx_text ltx_font_bold" id="S4.T6.13.8.1">Ablations on the training strategy.</span> â€œ<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.10.3.m1.1"><semantics id="S4.T6.10.3.m1.1b"><mo id="S4.T6.10.3.m1.1.1" stretchy="false" xref="S4.T6.10.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T6.10.3.m1.1c"><ci id="S4.T6.10.3.m1.1.1.cmml" xref="S4.T6.10.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.3.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.10.3.m1.1e">â†‘</annotation></semantics></math>â€ or â€œ<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.11.4.m2.1"><semantics id="S4.T6.11.4.m2.1b"><mo id="S4.T6.11.4.m2.1.1" stretchy="false" xref="S4.T6.11.4.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T6.11.4.m2.1c"><ci id="S4.T6.11.4.m2.1.1.cmml" xref="S4.T6.11.4.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.11.4.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.11.4.m2.1e">â†“</annotation></semantics></math>â€ indicate lower or higher values are better.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.13.6">
<tr class="ltx_tr" id="S4.T6.13.6.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.13.6.2.3">Training Strategy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.12.5.1.1">F1-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.12.5.1.1.m1.1"><semantics id="S4.T6.12.5.1.1.m1.1a"><mo id="S4.T6.12.5.1.1.m1.1.1" stretchy="false" xref="S4.T6.12.5.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T6.12.5.1.1.m1.1b"><ci id="S4.T6.12.5.1.1.m1.1.1.cmml" xref="S4.T6.12.5.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.12.5.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.12.5.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.13.6.2.2">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.13.6.2.2.m1.1"><semantics id="S4.T6.13.6.2.2.m1.1a"><mo id="S4.T6.13.6.2.2.m1.1.1" stretchy="false" xref="S4.T6.13.6.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T6.13.6.2.2.m1.1b"><ci id="S4.T6.13.6.2.2.m1.1.1.cmml" xref="S4.T6.13.6.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.13.6.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.13.6.2.2.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.13.6.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.13.6.3.1">Freeze</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.13.6.3.2">30.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.13.6.3.3">13.67</td>
</tr>
<tr class="ltx_tr" id="S4.T6.13.6.4">
<td class="ltx_td ltx_align_left" id="S4.T6.13.6.4.1">LoRA</td>
<td class="ltx_td ltx_align_center" id="S4.T6.13.6.4.2">32.90</td>
<td class="ltx_td ltx_align_center" id="S4.T6.13.6.4.3">13.20</td>
</tr>
<tr class="ltx_tr" id="S4.T6.13.6.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.13.6.5.1">Full fine-tune</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.13.6.5.2">34.15</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.13.6.5.3">10.64</td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">We explore different strategies for fusing control condition tokens with image tokens using the canny edge condition on ImageNet and LlamaGen-B as the generative model.
Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T6" title="Table 6 â€£ Ablations on the Control Fusion Strategy. â€£ 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">6</span></a> shows the results. Specifically, when using cross-attention for control fusion, we assign control condition tokens as the key and value, while image tokens serve as the query. Within LlamaGen-B, consisting of 12 layers of Transformer, we conduct experiments with addition at the first layer, addition at layer 1, 5, and 9, and addition at each layer.
The results indicate that direct addition proves more efficacious than cross-attention.
This outcome may be due to cross-attention needing to first understand the positional relationship between the image block and the control condition token, potentially leading to slower convergence.
Furthermore, augmenting the frequency of addition yields enhanced conditional coherence within the generated imagery. However, an excessive degree of addition also correlates with an increase in FID.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Ablations on Sequence Model Training Strategy.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.1">We conduct ablation experiments on the parameter update strategy of the sequence model during training. In the field of controllable generation, the most common ways of updating the parameters of a generative model include complete freezing, updating using Low-Rank Adaptation (LoRA)Â <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib17" title="">2021</a>)</cite>, and full fine-tuning. The results of the experiment are displayed in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S4.T6" title="Table 6 â€£ Ablations on the Control Fusion Strategy. â€£ 4.3 Ablation Studies â€£ 4 Experiments â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">6</span></a>. We use LlamaGen-B as the generative model for experiments on ImageNet based on canny edge. Experimental results show that full fine-tuning outperforms other schemes in terms of conditional consistency and FID of the generated images.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Conditional Decoding <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px4.1.1">v.s.</span> Conditional Prefilling.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px4.p1.1">In this part, we present a comprehensive comparison between two methods: conditional decoding and conditional prefilling.
We use LlamaGen-B as the generative model for experiments on ImageNet based on canny edge condition.
In Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (c), we depict the conditional consistency (F1-Score) and FID with respect to the number of training epochs for both approaches. Conditional decoding exhibits significant superiority over conditional prefilling in terms of both the speed of convergence and the final convergence result. Additionally, we provide a comparison of training resource consumption between the two approaches in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#S1.F2" title="Figure 2 â€£ 1 Introdution â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">2</span></a> (d). Due to the substantial increase in the length of the sequence, conditional prefilling results in heightened memory consumption during training, as well as a notable decrease in training speed.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we address autoregressive controllable image generation and present ControlAR, which allows autoregressive models to generate high-quality images according to diverse spatial controls.
The proposed ControlAR encodes the spatial controls and adopts conditional decoding to superimpose control condition tokens on the image generation process.
Moreover, ControlAR extends the capability of the autoregressive image generation model for arbitrary-resolution image generation.
Experimental results under a variety of control conditions show that ControlAR is capable of precise control without compromising image quality, and is also very competitive with the diffusion model-based state-of-the-art methods.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgment.</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">We sincerely thank Jingfeng Yao, Lianghui Zhu, and Zhuoyan Luo for kind and helpful discussions about the draft.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caesar etÂ al. (2018)</span>
<span class="ltx_bibblock">
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">Coco-stuff: Thing and stuff classes in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.Â  1209â€“1218, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Canny (1986)</span>
<span class="ltx_bibblock">
John Canny.

</span>
<span class="ltx_bibblock">A computational approach to edge detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Transactions on pattern analysis and machine intelligence</em>, (6):679â€“698, 1986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2017)</span>
<span class="ltx_bibblock">
Liang-Chieh Chen.

</span>
<span class="ltx_bibblock">Rethinking atrous convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1706.05587</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Bowen Cheng, Ishan Misra, AlexanderÂ G Schwing, Alexander Kirillov, and Rohit Girdhar.

</span>
<span class="ltx_bibblock">Masked-attention mask transformer for universal image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  1290â€“1299, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng etÂ al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiÂ Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2009 IEEE conference on computer vision and pattern recognition</em>, pp.Â  248â€“255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal &amp; Nichol (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in neural information processing systems</em>, 34:8780â€“8794, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser etÂ al. (2021)</span>
<span class="ltx_bibblock">
Patrick Esser, Robin Rombach, and Bjorn Ommer.

</span>
<span class="ltx_bibblock">Taming transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  12873â€“12883, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser etÂ al. (2024)</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, etÂ al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal etÂ al. (2022)</span>
<span class="ltx_bibblock">
Rinon Gal, Yuval Alaluf, Yuval Atzmon, OrÂ Patashnik, AmitÂ H Bermano, Gal Chechik, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">An image is worth one word: Personalizing text-to-image generation using textual inversion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2208.01618</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao.

</span>
<span class="ltx_bibblock">Vid-gpt: Introducing gpt-style autoregressive generation in video diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2406.10981</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu &amp; Dao (2023)</span>
<span class="ltx_bibblock">
Albert Gu and Tri Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2312.00752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.Â  770â€“778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel etÂ al. (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in neural information processing systems</em>, 33:6840â€“6851, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jonathan Ho, Chitwan Saharia, William Chan, DavidÂ J Fleet, Mohammad Norouzi, and Tim Salimans.

</span>
<span class="ltx_bibblock">Cascaded diffusion models for high fidelity image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Journal of Machine Learning Research</em>, 23(47):1â€“33, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma (2014)</span>
<span class="ltx_bibblock">
DiederikÂ P Kingma.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kondratyuk etÂ al. (2024)</span>
<span class="ltx_bibblock">
Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, JoshuaÂ V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, DavidÂ A. Ross, Bryan Seybold, and LuÂ Jiang.

</span>
<span class="ltx_bibblock">Videopoet: A large language model for zero-shot video generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Forty-first International Conference on Machine Learning, ICML 2024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon etÂ al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, CodyÂ Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</em>, pp.Â  611â€“626. ACM, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, and Guoqi Li.

</span>
<span class="ltx_bibblock">Scalable autoregressive image generation with mamba.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2408.12245</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen.

</span>
<span class="ltx_bibblock">Controlnet++: Improving conditional controls with efficient consistency feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2404.07987</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024c)</span>
<span class="ltx_bibblock">
Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, and Bhiksha Raj.

</span>
<span class="ltx_bibblock">Controlvar: Exploring controllable visual autoregressive modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2406.09750</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and YongÂ Jae Lee.

</span>
<span class="ltx_bibblock">Gligen: Open-set grounded text-to-image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  22511â€“22521, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Dongyang Liu, Shitian Zhao, LeÂ Zhuo, Weifeng Lin, YuÂ Qiao, Hongsheng Li, and Peng Gao.

</span>
<span class="ltx_bibblock">Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2408.02657</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.

</span>
<span class="ltx_bibblock">Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 35:5775â€“5787, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan.

</span>
<span class="ltx_bibblock">Open-magvit2: An open-source project toward democratizing auto-regressive visual generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2409.04410</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mou etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.

</span>
<span class="ltx_bibblock">T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 38, pp.Â  4296â€“4304, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.

</span>
<span class="ltx_bibblock">Glide: Towards photorealistic image generation and editing with text-guided diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2112.10741</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab etÂ al. (2023)</span>
<span class="ltx_bibblock">
Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, etÂ al.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2304.07193</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles &amp; Xie (2023)</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.Â  4195â€“4205, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Podell etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">Sdxl: Improving latent diffusion models for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2307.01952</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pope etÂ al. (2023)</span>
<span class="ltx_bibblock">
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.

</span>
<span class="ltx_bibblock">Efficiently scaling transformer inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Sixth Conference on Machine Learning and Systems</em>. mlsys.org, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, and Ran Xu.

</span>
<span class="ltx_bibblock">Gluegen: Plug and play multi-modal encoders for x-to-image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.Â  23085â€“23096, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, JuanÂ Carlos Niebles, Caiming Xiong, Silvio Savarese, etÂ al.

</span>
<span class="ltx_bibblock">Unicontrol: A unified diffusion model for controllable visual generation in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2305.11147</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International conference on machine learning</em>, pp.Â  8748â€“8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Journal of machine learning research</em>, 21(140):1â€“67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh etÂ al. (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">International conference on machine learning</em>, pp.Â  8821â€“8831. Pmlr, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranftl etÂ al. (2020)</span>
<span class="ltx_bibblock">
RenÃ© Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 44(3):1623â€“1637, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach etÂ al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  10684â€“10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger etÂ al. (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Medical image computing and computer-assisted interventionâ€“MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>, pp.Â  234â€“241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz etÂ al. (2023)</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  22500â€“22510, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia etÂ al. (2022)</span>
<span class="ltx_bibblock">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, EmilyÂ L Denton, Kamyar Ghasemipour, Raphael GontijoÂ Lopes, Burcu KaragolÂ Ayan, Tim Salimans, etÂ al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in neural information processing systems</em>, 35:36479â€“36494, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, etÂ al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems</em>, 35:25278â€“25294, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohl-Dickstein etÂ al. (2015)</span>
<span class="ltx_bibblock">
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.

</span>
<span class="ltx_bibblock">Deep unsupervised learning using nonequilibrium thermodynamics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">International conference on machine learning</em>, pp.Â  2256â€“2265. PMLR, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Jiaming Song, Chenlin Meng, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Denoising diffusion implicit models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2010.02502</em>, 2020a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song &amp; Ermon (2019)</span>
<span class="ltx_bibblock">
Yang Song and Stefano Ermon.

</span>
<span class="ltx_bibblock">Generative modeling by estimating gradients of the data distribution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Yang Song, Jascha Sohl-Dickstein, DiederikÂ P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.

</span>
<span class="ltx_bibblock">Score-based generative modeling through stochastic differential equations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2011.13456</em>, 2020b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2024)</span>
<span class="ltx_bibblock">
Peize Sun, YiÂ Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.

</span>
<span class="ltx_bibblock">Autoregressive model beats diffusion: Llama for scalable image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2406.06525</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian etÂ al. (2024)</span>
<span class="ltx_bibblock">
Keyu Tian, YiÂ Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang.

</span>
<span class="ltx_bibblock">Visual autoregressive modeling: Scalable image generation via next-scale prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2404.02905</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ den Oord etÂ al. (2016)</span>
<span class="ltx_bibblock">
Aaron VanÂ den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, etÂ al.

</span>
<span class="ltx_bibblock">Conditional image generation with pixelcnn decoders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Advances in neural information processing systems</em>, 29, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van DenÂ Oord etÂ al. (2016)</span>
<span class="ltx_bibblock">
AÃ¤ron Van DenÂ Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.

</span>
<span class="ltx_bibblock">Pixel recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">International conference on machine learning</em>, pp.Â  1747â€“1756. PMLR, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani (2017)</span>
<span class="ltx_bibblock">
AÂ Vaswani.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Advances in Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jinheng Xie, Weijia Mao, Zechen Bai, DavidÂ Junhao Zhang, Weihao Wang, KevinÂ Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and MikeÂ Zheng Shou.

</span>
<span class="ltx_bibblock">Show-o: One single transformer to unify multimodal understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2408.12528</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jiahui Yu, Yuanzhong Xu, JingÂ Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, BurcuÂ Karagol Ayan, etÂ al.

</span>
<span class="ltx_bibblock">Scaling autoregressive models for content-rich text-to-image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2206.10789</em>, 2(3):5, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.Â  3836â€“3847, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, and Changsheng Xu.

</span>
<span class="ltx_bibblock">Prospect: Prompt spectrum for attribute-aware personalization of diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ACM Transactions on Graphics (TOG)</em>, 42(6):1â€“14, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, LuÂ Yuan, and Kwan-YeeÂ K Wong.

</span>
<span class="ltx_bibblock">Uni-controlnet: All-in-one control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2017)</span>
<span class="ltx_bibblock">
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Scene parsing through ade20k dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.Â  633â€“641, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy.

</span>
<span class="ltx_bibblock">Transfusion: Predict the next token and diffuse images with one multi-modal model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2408.11039</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2304.10592</em>, 2023.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Implementation Details</h3>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Dataset details.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.1">The quantity of images from all datasets utilized in our experiment is detailed in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.T7" title="Table 7 â€£ Dataset details. â€£ A.1 Implementation Details â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">7</span></a>. We utilize the ImageNet-1KÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib5" title="">2009</a>)</cite> as the training dataset for class-to-image controllable generation, encompassing a total of 1,000 classes. The canny edge detectorÂ <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib2" title="">1986</a>)</cite> is employed to acquire the canny edge map, and the depth map is obtained using MidasÂ <cite class="ltx_cite ltx_citemacro_citep">(Ranftl etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib39" title="">2020</a>)</cite>. In the context of text-to-image controllable generation, ADE20KÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib60" title="">2017</a>)</cite> and COCOStuffÂ <cite class="ltx_cite ltx_citemacro_citep">(Caesar etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib1" title="">2018</a>)</cite> are harnessed for training the segmentation control task, while MultiGen-20M is utilized for training the edge map and depth control generation.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span><span class="ltx_text ltx_font_bold" id="A1.T7.2.1">Details of different dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T7.3">
<tr class="ltx_tr" id="A1.T7.3.1">
<td class="ltx_td ltx_border_tt" id="A1.T7.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.3.1.2">ImageNet-1K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.3.1.3">ADE20K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.3.1.4">COCOStuff</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.3.1.5">MultiGen-20M</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.3.2.1">Training Samples</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.2">1281188</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.3">20210</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.4">118287</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.2.5">2810616</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T7.3.3.1">Evaluation Samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.2">50000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.3">2000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.4">5000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.5">5000</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Evaluation details.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">To assess the conditional consistency of the generated images, we have devised various metrics tailored to each specific task. In the context of segmentation control generation, we employ a segmentation model to evaluate the mean Intersection over Union (mIoU) of the generated images. Specifically, we reference ControlNet++ to examine the results of the validation set generation on ADE20K using Mask2FormerÂ <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib4" title="">2022</a>)</cite>, and on COCOStuff using DeepLabv3Â <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib3" title="">2017</a>)</cite>.
For canny edge control generation, we utilize the canny edge detector with thresholds of (100, 200) to derive the canny edge of the results, and subsequently calculate the F1-Score in relation to the input control. In the case of hed and lineart edge, we follow the approach outlined in ControlNet to obtain control images and compute the Structural Similarity Index (SSIM). Regarding depth map control generation, we calculate the Root Mean Square Error (RMSE).</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Training details.</h4>
<figure class="ltx_table" id="A1.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span><span class="ltx_text ltx_font_bold" id="A1.T8.2.1">Training details of different tasks.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T8.3">
<tr class="ltx_tr" id="A1.T8.3.1">
<td class="ltx_td ltx_border_tt" id="A1.T8.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T8.3.1.2">Seg.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T8.3.1.3">Canny</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T8.3.1.4">Hed</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T8.3.1.5">Lineart</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T8.3.1.6">Depth</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.2">
<td class="ltx_td" id="A1.T8.3.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.2.2">ADE20K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.2.3">COCOStuff</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="A1.T8.3.2.4">MultiGen-20M</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.3.3.1">Batch size</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.2">96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.3">96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4">96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5">88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.6">88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.7">96</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T8.3.4.1">GPU hours</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.4.2">55</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.4.3">80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.4.4">340</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.4.5">160</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.4.6">110</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.4.7">370</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px3.p1.1">We use 8 Nvidia A100 80G GPUs to complete text-to-image controllable generation experiments based on LlamaGen-XLÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#bib.bib49" title="">2024</a>)</cite>. The batch size settings and GPU hours during training can be found in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.T8" title="Table 8 â€£ Training details. â€£ A.1 Implementation Details â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">8</span></a>. We use the edge extraction model to obtain the hed edge and lineart edge of the image during the training process, which takes up some memory, so the batch size is slightly smaller than the other tasks. It should be noted that since the ADE20K dataset has less training data, we first merge the ADE20K and COCOStuff datasets together to train the model, which requires roughly 50 GPU hours. Because the segmentation map labelling is inconsistent between the two datasets, we fine-tuned 2k iterations on ADE20K and 20k iterations on COCOStuff, respectively. The additional 2k iteration on ADE20K results in a mIoU improvement of 1.15.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Discussion</h3>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Limitation.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">We have shown in our experiments that updating the parameters of the generative model can achieve better results than freezing it completely. However, this approach is still not as convenient as ControlNet in terms of model portability. In addition, our method does not currently support scenarios where multiple control images are input simultaneously. Processing multiple control images simultaneously using a control encoder with a small number of parameters can be challenging.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Future work.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">We will use more data to try more kinds of conditional control generation, such as human pose and bounding box. At the same time, in order to improve the migratability of the model we will consider focusing the parameter update on the control encoder and keep the parameters of the generated model itself unchanged. In addition to this, how to use one control encoder to process different control image inputs simultaneously is also a direction worth exploring.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>More Visualizations</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">More visualization results under different conditions of control are shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F7" title="Figure 7 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">7</span></a>Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F8" title="Figure 8 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">8</span></a>Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F9" title="Figure 9 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">9</span></a>Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F10" title="Figure 10 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">10</span></a>Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F11" title="Figure 11 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">11</span></a>.
We alse show some visualization comparison of ControlAR and MR-ControlAR at different resolution in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F12" title="Figure 12 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">12</span></a> and Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.02705v1#A1.F13" title="Figure 13 â€£ A.3 More Visualizations â€£ Appendix A Appendix â€£ ControlAR: Controllable Image Generation with Autoregressive Models"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A1.F7.g1" src="x7.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="A1.F7.2.1">Segmentation mask control generation visualization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A1.F8.g1" src="x8.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="A1.F8.2.1">Canny edge control generation visualization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A1.F9.g1" src="x9.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="A1.F9.2.1">Hed edge control generation visualization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A1.F10.g1" src="x10.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold" id="A1.F10.2.1">Lineart edge control generation visualization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A1.F11.g1" src="x11.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold" id="A1.F11.2.1">Depth map control generation visualization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="582" id="A1.F12.g1" src="x12.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold" id="A1.F12.2.1">visualization comparison of MR-ControlAR and ControlAR at the resolution of <math alttext="1024\times 512" class="ltx_Math" display="inline" id="A1.F12.2.1.m1.1"><semantics id="A1.F12.2.1.m1.1b"><mrow id="A1.F12.2.1.m1.1.1" xref="A1.F12.2.1.m1.1.1.cmml"><mn id="A1.F12.2.1.m1.1.1.2" xref="A1.F12.2.1.m1.1.1.2.cmml">1024</mn><mo id="A1.F12.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.F12.2.1.m1.1.1.1.cmml">Ã—</mo><mn id="A1.F12.2.1.m1.1.1.3" xref="A1.F12.2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F12.2.1.m1.1c"><apply id="A1.F12.2.1.m1.1.1.cmml" xref="A1.F12.2.1.m1.1.1"><times id="A1.F12.2.1.m1.1.1.1.cmml" xref="A1.F12.2.1.m1.1.1.1"></times><cn id="A1.F12.2.1.m1.1.1.2.cmml" type="integer" xref="A1.F12.2.1.m1.1.1.2">1024</cn><cn id="A1.F12.2.1.m1.1.1.3.cmml" type="integer" xref="A1.F12.2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F12.2.1.m1.1d">1024\times 512</annotation><annotation encoding="application/x-llamapun" id="A1.F12.2.1.m1.1e">1024 Ã— 512</annotation></semantics></math>.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1019" id="A1.F13.g1" src="x13.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span class="ltx_text ltx_font_bold" id="A1.F13.2.1">visualization comparison of MR-ControlAR and ControlAR at the resolution of <math alttext="576\times 1024" class="ltx_Math" display="inline" id="A1.F13.2.1.m1.1"><semantics id="A1.F13.2.1.m1.1b"><mrow id="A1.F13.2.1.m1.1.1" xref="A1.F13.2.1.m1.1.1.cmml"><mn id="A1.F13.2.1.m1.1.1.2" xref="A1.F13.2.1.m1.1.1.2.cmml">576</mn><mo id="A1.F13.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.F13.2.1.m1.1.1.1.cmml">Ã—</mo><mn id="A1.F13.2.1.m1.1.1.3" xref="A1.F13.2.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.2.1.m1.1c"><apply id="A1.F13.2.1.m1.1.1.cmml" xref="A1.F13.2.1.m1.1.1"><times id="A1.F13.2.1.m1.1.1.1.cmml" xref="A1.F13.2.1.m1.1.1.1"></times><cn id="A1.F13.2.1.m1.1.1.2.cmml" type="integer" xref="A1.F13.2.1.m1.1.1.2">576</cn><cn id="A1.F13.2.1.m1.1.1.3.cmml" type="integer" xref="A1.F13.2.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.2.1.m1.1d">576\times 1024</annotation><annotation encoding="application/x-llamapun" id="A1.F13.2.1.m1.1e">576 Ã— 1024</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 20:14:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
