<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1909.13790] Incremental processing of noisy user utterances in the spoken language understanding task</title><meta property="og:description" content="The state-of-the-art neural network architectures make it possible to create spoken language understanding systems with high quality and fast processing time.
One major challenge for real-world applications is the high…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Incremental processing of noisy user utterances in the spoken language understanding task">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Incremental processing of noisy user utterances in the spoken language understanding task">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1909.13790">

<!--Generated on Sun Mar 17 09:19:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Incremental processing of noisy user utterances in the spoken language understanding task</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefan Constantin<sup id="id8.8.id1" class="ltx_sup">∗</sup>       Jan Niehues<sup id="id9.9.id2" class="ltx_sup">+</sup>       Alex Waibel<sup id="id10.10.id3" class="ltx_sup">∗</sup> 
<br class="ltx_break"><sup id="id11.11.id4" class="ltx_sup">∗</sup> Karlsruhe Institute of Technology 
<br class="ltx_break">Institute for Anthropomatics and Robotic 
<br class="ltx_break"><math id="id5.5.m5.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="id5.5.m5.1a"><mo stretchy="false" id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><ci id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">\{</annotation></semantics></math><span id="id6.6.1" class="ltx_text ltx_font_typewriter">stefan.constantin|waibel<math id="id6.6.1.m1.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="id6.6.1.m1.1a"><mo stretchy="false" id="id6.6.1.m1.1.1" xref="id6.6.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="id6.6.1.m1.1b"><ci id="id6.6.1.m1.1.1.cmml" xref="id6.6.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.1.m1.1c">\}</annotation></semantics></math>@kit.edu</span> 
<br class="ltx_break">
<br class="ltx_break"><sup id="id12.12.id5" class="ltx_sup">+</sup> Maastricht University 
<br class="ltx_break">Department of Data Science and Knowledge Engineering 
<br class="ltx_break"><span id="id13.13.id6" class="ltx_text ltx_font_typewriter">jan.niehues@maastrichtuniversity.nl</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">The state-of-the-art neural network architectures make it possible to create spoken language understanding systems with high quality and fast processing time.
One major challenge for real-world applications is the high latency of these systems caused by triggered actions with high executions times.
If an action can be separated into subactions, the reaction time of the systems can be improved through incremental processing of the user utterance and starting subactions while the utterance is still being uttered.
In this work, we present a model-agnostic method to achieve high quality in processing incrementally produced partial utterances.
Based on clean and noisy versions of the ATIS dataset, we show how to create datasets with our method to create low-latency natural language understanding components.
We get improvements of up to 47.91 absolute percentage points in the metric F<sub id="id14.id1.1" class="ltx_sub">1</sub>-score.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Dialog Systems are ubiquitous - they are used in customer hotlines, at home (Amazon Alexa, Apple Siri, Google Home, etc.), in cars, in robots <cite class="ltx_cite ltx_citemacro_cite">Asfour et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>, and in smartphones (Apple Siri, Google Assistant, etc.).
From a user experience point of view, one of the main challenges of state-of-the-art dialog systems is the slow reaction of the assistants.
Usually, these dialog systems wait for the completion of a user utterance and afterwards process the utterance.
The processed utterance can trigger a suitable action, e. g. ask for clarification, book a certain flight, or bring an object.
Actions can have a high execution time, due to which the dialog systems react slowly.
If an action can be separated into subactions, the reaction time of the dialog system can be improved through incremental processing of the user utterance and starting subactions while the utterance is still being uttered.
The action still has the same execution time but the action is completed earlier because it was started earlier and therefore the dialog system can react faster.
In the domain of airplane travel information, database queries can be finished earlier if the system can execute subqueries before the completion of the user utterance, e. g. the utterance <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">On next Wednesday flight from Kansas City to Chicago should arrive in Chicago around 7 pm</span> can be separated in the databases queries <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">flight from Kansas City to Chicago on next Wednesday</span> and <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">use result of the first query to find flights that arrive in Chicago around 7 pm</span>.
In the domain of household robots, e. g. the user goal of the user utterance <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">Bring me from the kitchen the cup that I like because it reminds me of my unforgettable vacation in the United States</span> can be fulfilled faster if the robot goes to the kitchen before the user utters what object the robot should bring.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Motivated by this approach to improve the reaction of dialog systems, our main contribution is a low-latency natural language understanding (NLU) component.
We use the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> to build this low-latency NLU component, but the ingredient to understand partial utterances and incrementally process user utterances is the model-agnostic training process presented in this work.
Secondly, partial utterances are particularly affected by noise.
This is due to the short context available in partial utterances and because automatic speech recognition (ASR) systems cannot utilize their complete language model and therefore potentially make more errors when transcribing short utterances.
We address the potential noisier inputs by including noisy inputs in the training process.
Finally, we present two evaluation schemes for low-latency NLU components.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Gambino et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> described time buying strategies to avoid long pauses, e. g. by uttering an acknowledgement or echoing the user input.
However, the triggered actions are not finished earlier with this approach, but in cases where long pauses cannot be avoided, even with incremental processing, such time buying strategies can be applied.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The automatically generated backchannel described by <cite class="ltx_cite ltx_citemacro_citet">Rüde et al. (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> gives feedback during the uttering of an utterance.
However, only acoustic features are used and it does not reduce the latency of actions that can be triggered by the utterances.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Studies have been conducted on incremental NLU.
<cite class="ltx_cite ltx_citemacro_citet">DeVault et al. (<a href="#bib.bib6" title="" class="ltx_ref">2009</a>)</cite> used a maximum entropy classificator <cite class="ltx_cite ltx_citemacro_cite">Berger et al. (<a href="#bib.bib3" title="" class="ltx_ref">1996</a>)</cite> to classify the partial utterances.
They optimized the maximum entropy classificator for partial utterances by using an individual classificator for every utterance length.
The problem of this classification approach is that it is not suitable for tasks with a lot of different parameter combinations; for such tasks, a slot filling (sequence labeling task) or word by word approach (sequence to sequence task) is more suitable.
Such a more suitable approach is described by <cite class="ltx_cite ltx_citemacro_citet">Niehues et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> for incrementally updating machine translations.
The authors used an attention-based encoder decoder <cite class="ltx_cite ltx_citemacro_cite">Bahdanau et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>, which outputs a sequence.
We described and evaluated in this work such a more suitable approach for incremental NLU.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Different approaches are available to handle noisy input, such as general-purpose regularization techniques like dropout <cite class="ltx_cite ltx_citemacro_cite">Srivastava et al. (<a href="#bib.bib19" title="" class="ltx_ref">2014</a>)</cite> and domain-specific regularization techniques e. g. data augmentation by inserting, deleting, and substituting words <cite class="ltx_cite ltx_citemacro_cite">Sperber et al. (<a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>.
Our trained models in this work uses the general-purpose techniques and some of our trained models are trained with such augmented data to have a better performance on noisy data.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Low-latency NLU component</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, we present a model-agnostic method to build an incremental processing low-latency NLU component.
The advantages of this model-agnostic method are that we can use state-of-the-art neural network architectures and reuse the method for future state-of-the-art neural network architectures.
Our used architecture is described in Section <a href="#S3.SS1" title="3.1 Architecture ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and the used data is described in Section <a href="#S3.SS2" title="3.2 Data ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
Our method to include the information necessary to incrementally process user utterances with high quality in the training dataset is described in Section <a href="#S3.SS3" title="3.3 Training process to improve incremental processing ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> and our methods to include noise to process noisy texts with high quality are described in Section <a href="#S3.SS4" title="3.4 Training process to improve robustness ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
In Sections <a href="#S3.SS5" title="3.5 Evaluation metrics ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a> and <a href="#S3.SS6" title="3.6 Evaluation schemes ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>, we present our evaluation metrics and evaluation schemes respectively.
The configuration of the used architecture is given in Section <a href="#S3.SS7" title="3.7 System Setup ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We used the Transformer architecture in our experiments to demonstrate the model-agnostic method.
The Transformer architecture, with its encoder and decoder, was used as sequence to sequence architecture.
The user utterances are the input sequences and their corresponding triggered actions are the output actions (this is described in more details in Section <a href="#S3.SS2" title="3.2 Data ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
We used the Transformer implementation used by <cite class="ltx_cite ltx_citemacro_citet">Pham et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> and added the functionality for online translation.
The original code<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/quanpn90/NMTGMinor/tree/DbMajor" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/quanpn90/NMTGMinor/tree/DbMajor</a></span></span></span> and the added code are publicly available<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/msc42/NMTGMinor/tree/DbMajor" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/msc42/NMTGMinor/tree/DbMajor</a></span></span></span>.
The partial utterances and, in the end, the full utterance were fed successively and completely into the Transformer architecture without using information of the computation of the previous partial utterances.
Our proposed method is model-agnostic because of this separate treatment and therefore an arbitrary model that can process sequences can be used to process the partial and full utterances.
The method is depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Architecture ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the utterance <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Flights to Pittsburgh</span>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><svg id="S3.F1.pic1" class="ltx_picture" height="108.82" overflow="visible" version="1.1" width="589.93"><g transform="translate(0,108.82) matrix(1 0 0 -1 0 0) translate(32.17,0) translate(0,12.91)" fill="#000000" stroke="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -27.56 81.61)" fill="#000000" stroke="#000000"><foreignObject width="55.12" height="25.12" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F1.pic1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:39.8pt;">
<span id="S3.F1.pic1.1.1.1.1.1.1.1" class="ltx_p"></span>
<span id="S3.F1.pic1.1.1.1.1.1.1.2" class="ltx_p ltx_align_left">atis_flight</span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 208.66 81.61)" fill="#000000" stroke="#000000"><foreignObject width="55.12" height="25.12" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F1.pic1.2.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:39.8pt;">
<span id="S3.F1.pic1.2.2.2.2.1.1.1" class="ltx_p"></span>
<span id="S3.F1.pic1.2.2.2.2.1.1.2" class="ltx_p ltx_align_left">atis_flight</span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 391.73 77.36)" fill="#000000" stroke="#000000"><foreignObject width="161.42" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F1.pic1.3.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:116.7pt;">
<span id="S3.F1.pic1.3.3.3.3.1.1.1" class="ltx_p"></span>
<span id="S3.F1.pic1.3.3.3.3.1.1.2" class="ltx_p ltx_align_left">atis_flight toloc pittsburgh</span>
</span></foreignObject></g><path d="M -22.87 29.95 h 45.74 v 18.83 h -45.74 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -18.26 34.57)" fill="#000000" stroke="#000000"><foreignObject width="36.51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F1.pic1.4.4.4.4.1.1" class="ltx_text">model</span></foreignObject></g><path d="M 213.35 29.95 h 45.74 v 18.83 h -45.74 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 217.96 34.57)" fill="#000000" stroke="#000000"><foreignObject width="36.51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F1.pic1.5.5.5.5.1.1" class="ltx_text">model</span></foreignObject></g><path d="M 449.57 29.95 h 45.74 v 18.83 h -45.74 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 454.18 34.57)" fill="#000000" stroke="#000000"><foreignObject width="36.51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F1.pic1.6.6.6.6.1.1" class="ltx_text">model</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -19.69 -1.38)" fill="#000000" stroke="#000000"><foreignObject width="39.37" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F1.pic1.7.7.7.7.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:28.5pt;">
<span id="S3.F1.pic1.7.7.7.7.1.1.1" class="ltx_p"></span>
<span id="S3.F1.pic1.7.7.7.7.1.1.2" class="ltx_p ltx_align_left">Flights</span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 206.69 -3.54)" fill="#000000" stroke="#000000"><foreignObject width="59.06" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F1.pic1.8.8.8.8.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:42.7pt;">
<span id="S3.F1.pic1.8.8.8.8.1.1.1" class="ltx_p"></span>
<span id="S3.F1.pic1.8.8.8.8.1.1.2" class="ltx_p ltx_align_left">Flights to</span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 403.54 -3.54)" fill="#000000" stroke="#000000"><foreignObject width="137.8" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F1.pic1.9.9.9.9.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:99.6pt;">
<span id="S3.F1.pic1.9.9.9.9.1.1.1" class="ltx_p"></span>
<span id="S3.F1.pic1.9.9.9.9.1.1.2" class="ltx_p ltx_align_left">Flights to Pittsburgh</span>
</span></foreignObject></g><path d="M 0 13.19 L 0 29.04" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 29.04)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 236.22 11.04 L 236.22 29.04" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 236.22 29.04)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 472.44 11.04 L 472.44 29.04" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 472.44 29.04)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 0 49.06 L 0 60.66" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 60.66)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 236.22 49.06 L 236.22 60.66" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 236.22 60.66)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 472.44 49.06 L 472.44 64.91" style="fill:none"></path></g><g transform="matrix(0.0 1.0 -1.0 0.0 472.44 64.91)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>model-agnostic approach</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For our experiments, we used utterances from the Airline Travel Information System (ATIS) datasets.
We used the utterances that are used by <cite class="ltx_cite ltx_citemacro_citet">Hakkani-Tur et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> and are publicly available<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/yvchen/JointSLU" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yvchen/JointSLU</a></span></span></span>.
These utterances were cleaned and every utterance is labeled with its intents and for every token, the corresponding slot is labeled with a tag (in the IOB2 format <cite class="ltx_cite ltx_citemacro_cite">Sang and Veenstra (<a href="#bib.bib16" title="" class="ltx_ref">1999</a>)</cite> that is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Data ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<table id="S3.F2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.F2.1.1.1" class="ltx_tr">
<th id="S3.F2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.F2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F2.1.1.1.1.1.1" class="ltx_p" style="width:76.8pt;">utterance <span id="S3.F2.1.1.1.1.1.1.1" class="ltx_text">(source sequence)</span></span>
</span>
</th>
<th id="S3.F2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Which</th>
<th id="S3.F2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">flights</th>
<th id="S3.F2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">go</th>
<th id="S3.F2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">from</th>
<th id="S3.F2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">New</th>
<th id="S3.F2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">York</th>
<th id="S3.F2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">to</th>
<th id="S3.F2.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Pittsburgh</th>
</tr>
<tr id="S3.F2.1.2.2" class="ltx_tr">
<th id="S3.F2.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.F2.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F2.1.2.2.1.1.1" class="ltx_p" style="width:76.8pt;">slots</span>
</span>
</th>
<th id="S3.F2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="S3.F2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="S3.F2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="S3.F2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="S3.F2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">B-fromloc</th>
<th id="S3.F2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">I-fromloc</th>
<th id="S3.F2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="S3.F2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">B-toloc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.F2.1.3.1" class="ltx_tr">
<th id="S3.F2.1.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.F2.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F2.1.3.1.1.1.1" class="ltx_p" style="width:76.8pt;">intents</span>
</span>
</th>
<td id="S3.F2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="8">atis_flight</td>
</tr>
<tr id="S3.F2.1.4.2" class="ltx_tr">
<th id="S3.F2.1.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.F2.1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F2.1.4.2.1.1.1" class="ltx_p" style="width:76.8pt;">target sequence</span>
</span>
</th>
<td id="S3.F2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="8">atis_flight fromloc new york toloc pittsburgh</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>joint intents classification and slot filling approach to end-to-end target sequence</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We converted the data from the IOB2 format to a sequence to sequence format <cite class="ltx_cite ltx_citemacro_cite">Constantin et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>.
The source sequence is a user utterance and the target sequence consists of the intents followed by the parameters.
In this work, the slot tag and the corresponding slot tokens compose an intents parameter.
An example of the conversion of the IOB2 format to the sequence to sequence format is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Data ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The sequence to sequence format has the advantages that no rules are needed for mapping the slot tokens to an API call or a database query and that this format is more robust against noisy text like <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">What is restriction ap slash fifty seven</span>, where the noise word slash is introduced (in the classical IOB2 format, the tokens <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">ap</span> and <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">fifty seven</span> would not belong to the same chunk).</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The publicly available utterances are partitioned in a training and test dataset.
The training dataset is partitioned in a training (train-2) and validation (dev-2) dataset.
Hereinafter, original training dataset refers to the utterances of the training dataset, training dataset refers to the utterances of the train-2 dataset, and validation dataset refers to the utterances of the dev-2 dataset.
We created a file that maps to every utterance in the training dataset the line number of the corresponding utterance in the original training dataset and a file that maps to every utterance in the validation dataset the line number of the corresponding utterance in the original training dataset.
We published these two files<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/msc42/ATIS-data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/msc42/ATIS-data</a></span></span></span>.
The training dataset has 4478 utterances, the validation dataset has 500 utterances, and the test dataset has 893 utterances.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The utterances were taken from the ATIS2 dataset (Linguistic Data Consortium (LDC) catalog number LDC93S5), the ATIS3 training dataset (LDC94S19) and the ATIS3 test dataset (LDC94S26).
The audio files of the spoken utterances and the uncleaned human transcribed transcripts are on the corresponding LDC CDs.
For the original training dataset and the test dataset, we published<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>see footnote <a href="#footnote4" title="footnote 4 ‣ 3.2 Data ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span></span></span> in each case a file that maps to every utterance the path of the corresponding audio file and a file that maps to every utterance the path of the corresponding transcript of the corresponding LDC CD.
One audio file is missing on the corresponding LDC CD (LDC94S19): atis3/17_2.1/atis3/sp_trn/sri/tx0/2/tx0022ss.wav (corresponding to the training dataset).
We used the tool sph2pipe<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ctools/sph2pipe_v2.5.tar.gz" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ctools/sph2pipe_v2.5.tar.gz</a></span></span></span> to convert the SPH files (with extension .wav) of the LDC CDs to WAVE files.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">The utterances have an average token length of 11.21 - 11.36 in the training dataset, 11.48 in the validation dataset, and 10.30 in the test dataset.
We tokenized the utterances with the default English word tokenizer of the Natural Language Toolkit (NLTK)<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://www.nltk.org/</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Bird et al. (<a href="#bib.bib4" title="" class="ltx_ref">2009</a>)</cite>.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">There are 19 unique intents in the ATIS data.
In the training dataset, 22 utterances are labeled with 2 intents and 1 utterance is labeled with 3 intents, in the validation dataset, there are 3 utterances with 2 intents and in the test dataset, there are 15 utterances with 2 intents, the rest of the utterances are labeled with 1 intent.
The intents are separated by the number sign in the target sequence.
The intents are unbalanced (more than 70 % of the utterances have the same intent, more than 90 % of the utterances belong to the 5 most used intents).
More information about the intents distribution is given Table <a href="#A1.T7" title="Table 7 ‣ Appendix A Supplemental Material ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
There are 83 different parameters that can parameterize the intents.
On average, a target has 3.35 (training dataset), 3.46 (validation dataset), and 3.19 (test dataset) parameters.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training process to improve incremental processing</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">We call our dataset, which contains the dataset described in Section <a href="#S3.SS2" title="3.2 Data ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, <span id="S3.SS3.p1.5.1" class="ltx_text ltx_font_italic">cleaned full transcripts</span>.
Our model-agnostic method to achieve good quality for partial utterances works in this manner:
We use the dataset with the full utterances and create partial utterances from it.
An utterance of the length <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">n</annotation></semantics></math> is split into <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">n</annotation></semantics></math> utterances, where the <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">i</annotation></semantics></math>-th utterance of these utterances has the length <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">i</annotation></semantics></math>.
The target contains all information that can be gotten from the source utterance of the length <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">i</annotation></semantics></math>.
When only a part of a chunk is in the user utterance, only this part is integrated in the target utterances, e. g. <span id="S3.SS3.p1.5.2" class="ltx_text ltx_font_italic">I want a flight from New York to San</span> has the target <span id="S3.SS3.p1.5.3" class="ltx_text ltx_font_italic">atis_flight fromloc.city_name new york toloc.city_name san</span>.
Such partial information contains information and can accelerate database queries, for example.
We created with this method the <span id="S3.SS3.p1.5.4" class="ltx_text ltx_font_italic">cleaned incremental transcripts</span> dataset.
An arbitrary model without modifications, in this work the Transformer architecture, can be trained with this dataset to have an improved incremental processing ability compared to a model trained only with full utterances.
Since every partial utterance is regarded as independent utterance, like the full utterances, our approach is model-agnostic.
The model-agnostic approach for the utterance <span id="S3.SS3.p1.5.5" class="ltx_text ltx_font_italic">Flights to Pittsburgh</span> is depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Architecture ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training process to improve robustness</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In Section <a href="#S3.SS3" title="3.3 Training process to improve incremental processing ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, the training process for improving the incremental processing is described.
However, the described process does not consider the fact that the incremental data are noisier.
We induced noise in the training by training with artificial noise, human transcribed utterances that contain the noise of spoken utterances, and utterances transcribed by an ASR system.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The dataset <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_italic">cleaned incremental transcripts with artificial noise</span> consists of the utterances from the dataset <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">cleaned incremental transcripts</span> to these artificial noise were added with the approach described by <cite class="ltx_cite ltx_citemacro_citet">Sperber et al. (<a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>.
We published the implementation<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/msc42/NLP-tools/blob/master/noise_adder.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/msc42/NLP-tools/blob/master/noise_adder.py</a></span></span></span> of this approach.
In this approach, random distributions are used to substitute, insert, and delete words.
We sampled the words for substitution and insertion based on acoustic similarity to the original input.
As vocabulary for the substitutions and insertions, we used the tokens of the utterances of the training dataset of the <span id="S3.SS4.p2.1.3" class="ltx_text ltx_font_italic">cleaned incremental transcripts</span> dataset and filled the vocabulary with the most frequent tokens not included in the used training dataset occurring in the source utterances of a subset of the OpenSubtitle corpus<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>based on <a target="_blank" href="http://www.opensubtitles.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.opensubtitles.org/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a href="#bib.bib20" title="" class="ltx_ref">2009</a>)</cite> that is publicly available<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://s3.amazonaws.com/opennmt-trainingdata/opensub_qa_en.tgz" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://s3.amazonaws.com/opennmt-trainingdata/opensub_qa_en.tgz</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Senellart (<a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>.
We chose the position of the words to be substituted and deleted based on the length.
Shorter words are often more exposed to errors in ASR systems and therefore should be substituted and deleted in the artificial noise approach more frequently.
Since substitutions are more probable in ASR systems, we reflected this in the artificial noise generating by assigning substitutions a 5-times higher probability than insertions or deletions.
For the value of the hyperparameter <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\tau</annotation></semantics></math> (the induced amount of noise), we used 0.08.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">For the dataset <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_italic">human full transcripts</span>, we used the human transcribed transcripts given by the LDC CDs.
We mapped these utterances to the corresponding targets of the datasets based on the <span id="S3.SS4.p3.1.2" class="ltx_text ltx_font_italic">cleaned full transcripts</span> dataset.
The utterances are not cleaned and have some annotations like noise and repeated words.
The dataset <span id="S3.SS4.p3.1.3" class="ltx_text ltx_font_italic">human incremental transcripts</span>, <span id="S3.SS4.p3.1.4" class="ltx_text ltx_font_italic">human incremental transcripts with artificial noise</span>, and <span id="S3.SS4.p3.1.5" class="ltx_text ltx_font_italic">human full transcripts with artificial noise</span> were generated analogous to the described approaches before.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">For the dataset <span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_italic">automatic incremental transcripts</span>, we automatically transcribed the audio files from the LDC CDs with the ASR system Janus Recognition Toolkit (JRTk) <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>.
This ASR system is used as an out-of-domain ASR system - there is no adaption for the ATIS utterances.
We used the incremental mode of the JRTk, which means that transcriptions are updated multiple times while transcribing.
It is not automatically possible to generate the partial output targets to the partial utterances, because the ASR system makes errors and it is impossible to map with 100 % accuracy automatically the wrong transcript <span id="S3.SS4.p4.1.2" class="ltx_text ltx_font_italic">to come up</span> to the correct transcript <span id="S3.SS4.p4.1.3" class="ltx_text ltx_font_italic">Tacoma</span>, for example.
We used a workaround: We measured the length of a partial transcript, searched the corresponding transcript of the <span id="S3.SS4.p4.1.4" class="ltx_text ltx_font_italic">human incremental transcripts</span> dataset that has the same length, and used the target of the found transcript.
If there were only shorter transcripts, the target of the full transcript was used.
This approach punishes insertions and deletions of the ASR system.
For the dataset <span id="S3.SS4.p4.1.5" class="ltx_text ltx_font_italic">automatic full transcripts</span>, we used the last transcript of the incremental transcripts of the ASR system for the user utterance and the full target of the corresponding utterance of the <span id="S3.SS4.p4.1.6" class="ltx_text ltx_font_italic">human full transcripts</span> dataset.
For the mentioned missing audio file, we used the human transcription of the corresponding LDC CD.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">An arbitrary model without modifications, in this work the Transformer architecture, is trained with one of the described noisy datasets to have improved robustness compared to a model trained only with clean utterances.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Evaluation metrics</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We evaluated the quality of the models, trained with the different datasets, with the metric F<sub id="S3.SS5.p1.1.1" class="ltx_sub">1</sub>-score for which we used an adapted definition for the precision and the recall in this work and the metric intents accuracy.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.9" class="ltx_p">The adapted definitions for the precision and the recall consider the order of the classes in the target sequence.
The intents and the intents parameters are the classes.
Intents parameters with the same slot tag are considered as different classes.
We call the F<sub id="S3.SS5.p2.9.1" class="ltx_sub">1</sub>-score calculated with the adapted definition of the precision and the recall <span id="S3.SS5.p2.9.2" class="ltx_text ltx_font_bold">c</span>onsidering <span id="S3.SS5.p2.9.3" class="ltx_text ltx_font_bold">o</span>rder <span id="S3.SS5.p2.9.4" class="ltx_text ltx_font_bold">m</span>ultiple <span id="S3.SS5.p2.9.5" class="ltx_text ltx_font_bold">c</span>lasses F<sub id="S3.SS5.p2.9.6" class="ltx_sub">1</sub>-score (CO-MC F<sub id="S3.SS5.p2.9.7" class="ltx_sub">1</sub>-score).
Order considering means that the predicted parameters have to be in the correct order in the target sequence.
In the target sequence</p>
<p id="S3.SS5.p2.10" class="ltx_p ltx_align_left"><span id="S3.SS5.p2.10.1" class="ltx_text ltx_font_italic">atis_flight fromloc.city_name milwaukee toloc.city_name orlando depart_date.day_name wednesday depart_time.period_of_day evening or or depart_date.day_name thursday depart_time.period_of_day morning</span></p>
<p id="S3.SS5.p2.8" class="ltx_p">the order is important.
To calculate the true positives, we adapted the Levenshtein distance <cite class="ltx_cite ltx_citemacro_cite">Levenshtein (<a href="#bib.bib10" title="" class="ltx_ref">1966</a>)</cite>.
The entities that are compared in this adapted Levenshtein distance are the classes.
The adapted Levenshtein distance is only changed by a match (incremented by one) and the maximum instead of the minimum function is used to select the best operation.
In Figure <a href="#S3.F3" title="Figure 3 ‣ 3.5 Evaluation metrics ‣ 3 Low-latency NLU component ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the recursive definition of the adapted Levenshtein distance (ALD) is depicted.
Let <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">r</annotation></semantics></math> be the reference and <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mi id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">h</annotation></semantics></math> the hypothesis and <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="|r|" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mrow id="S3.SS5.p2.3.m3.1.2.2" xref="S3.SS5.p2.3.m3.1.2.1.cmml"><mo stretchy="false" id="S3.SS5.p2.3.m3.1.2.2.1" xref="S3.SS5.p2.3.m3.1.2.1.1.cmml">|</mo><mi id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS5.p2.3.m3.1.2.2.2" xref="S3.SS5.p2.3.m3.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.2.1.cmml" xref="S3.SS5.p2.3.m3.1.2.2"><abs id="S3.SS5.p2.3.m3.1.2.1.1.cmml" xref="S3.SS5.p2.3.m3.1.2.2.1"></abs><ci id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">|r|</annotation></semantics></math> and <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="|h|" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><mrow id="S3.SS5.p2.4.m4.1.2.2" xref="S3.SS5.p2.4.m4.1.2.1.cmml"><mo stretchy="false" id="S3.SS5.p2.4.m4.1.2.2.1" xref="S3.SS5.p2.4.m4.1.2.1.1.cmml">|</mo><mi id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml">h</mi><mo stretchy="false" id="S3.SS5.p2.4.m4.1.2.2.2" xref="S3.SS5.p2.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.2.1.cmml" xref="S3.SS5.p2.4.m4.1.2.2"><abs id="S3.SS5.p2.4.m4.1.2.1.1.cmml" xref="S3.SS5.p2.4.m4.1.2.2.1"></abs><ci id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">|h|</annotation></semantics></math> the number of classes of the reference or hyptothesis respectively and <math id="S3.SS5.p2.5.m5.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.SS5.p2.5.m5.1a"><msub id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml"><mi id="S3.SS5.p2.5.m5.1.1.2" xref="S3.SS5.p2.5.m5.1.1.2.cmml">r</mi><mi id="S3.SS5.p2.5.m5.1.1.3" xref="S3.SS5.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><apply id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.5.m5.1.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.p2.5.m5.1.1.2.cmml" xref="S3.SS5.p2.5.m5.1.1.2">𝑟</ci><ci id="S3.SS5.p2.5.m5.1.1.3.cmml" xref="S3.SS5.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">r_{i}</annotation></semantics></math> and <math id="S3.SS5.p2.6.m6.1" class="ltx_Math" alttext="h_{i}" display="inline"><semantics id="S3.SS5.p2.6.m6.1a"><msub id="S3.SS5.p2.6.m6.1.1" xref="S3.SS5.p2.6.m6.1.1.cmml"><mi id="S3.SS5.p2.6.m6.1.1.2" xref="S3.SS5.p2.6.m6.1.1.2.cmml">h</mi><mi id="S3.SS5.p2.6.m6.1.1.3" xref="S3.SS5.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.6.m6.1b"><apply id="S3.SS5.p2.6.m6.1.1.cmml" xref="S3.SS5.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.6.m6.1.1.1.cmml" xref="S3.SS5.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS5.p2.6.m6.1.1.2.cmml" xref="S3.SS5.p2.6.m6.1.1.2">ℎ</ci><ci id="S3.SS5.p2.6.m6.1.1.3.cmml" xref="S3.SS5.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.6.m6.1c">h_{i}</annotation></semantics></math> the <math id="S3.SS5.p2.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS5.p2.7.m7.1a"><mi id="S3.SS5.p2.7.m7.1.1" xref="S3.SS5.p2.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.7.m7.1b"><ci id="S3.SS5.p2.7.m7.1.1.cmml" xref="S3.SS5.p2.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.7.m7.1c">i</annotation></semantics></math>-th class of the reference or hypothesis respectively.
<math id="S3.SS5.p2.8.m8.4" class="ltx_Math" alttext="L_{|h|,|r|}" display="inline"><semantics id="S3.SS5.p2.8.m8.4a"><msub id="S3.SS5.p2.8.m8.4.5" xref="S3.SS5.p2.8.m8.4.5.cmml"><mi id="S3.SS5.p2.8.m8.4.5.2" xref="S3.SS5.p2.8.m8.4.5.2.cmml">L</mi><mrow id="S3.SS5.p2.8.m8.4.4.4.4" xref="S3.SS5.p2.8.m8.4.4.4.5.cmml"><mrow id="S3.SS5.p2.8.m8.3.3.3.3.1.2" xref="S3.SS5.p2.8.m8.3.3.3.3.1.1.cmml"><mo stretchy="false" id="S3.SS5.p2.8.m8.3.3.3.3.1.2.1" xref="S3.SS5.p2.8.m8.3.3.3.3.1.1.1.cmml">|</mo><mi id="S3.SS5.p2.8.m8.1.1.1.1" xref="S3.SS5.p2.8.m8.1.1.1.1.cmml">h</mi><mo stretchy="false" id="S3.SS5.p2.8.m8.3.3.3.3.1.2.2" xref="S3.SS5.p2.8.m8.3.3.3.3.1.1.1.cmml">|</mo></mrow><mo id="S3.SS5.p2.8.m8.4.4.4.4.3" xref="S3.SS5.p2.8.m8.4.4.4.5.cmml">,</mo><mrow id="S3.SS5.p2.8.m8.4.4.4.4.2.2" xref="S3.SS5.p2.8.m8.4.4.4.4.2.1.cmml"><mo stretchy="false" id="S3.SS5.p2.8.m8.4.4.4.4.2.2.1" xref="S3.SS5.p2.8.m8.4.4.4.4.2.1.1.cmml">|</mo><mi id="S3.SS5.p2.8.m8.2.2.2.2" xref="S3.SS5.p2.8.m8.2.2.2.2.cmml">r</mi><mo stretchy="false" id="S3.SS5.p2.8.m8.4.4.4.4.2.2.2" xref="S3.SS5.p2.8.m8.4.4.4.4.2.1.1.cmml">|</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.8.m8.4b"><apply id="S3.SS5.p2.8.m8.4.5.cmml" xref="S3.SS5.p2.8.m8.4.5"><csymbol cd="ambiguous" id="S3.SS5.p2.8.m8.4.5.1.cmml" xref="S3.SS5.p2.8.m8.4.5">subscript</csymbol><ci id="S3.SS5.p2.8.m8.4.5.2.cmml" xref="S3.SS5.p2.8.m8.4.5.2">𝐿</ci><list id="S3.SS5.p2.8.m8.4.4.4.5.cmml" xref="S3.SS5.p2.8.m8.4.4.4.4"><apply id="S3.SS5.p2.8.m8.3.3.3.3.1.1.cmml" xref="S3.SS5.p2.8.m8.3.3.3.3.1.2"><abs id="S3.SS5.p2.8.m8.3.3.3.3.1.1.1.cmml" xref="S3.SS5.p2.8.m8.3.3.3.3.1.2.1"></abs><ci id="S3.SS5.p2.8.m8.1.1.1.1.cmml" xref="S3.SS5.p2.8.m8.1.1.1.1">ℎ</ci></apply><apply id="S3.SS5.p2.8.m8.4.4.4.4.2.1.cmml" xref="S3.SS5.p2.8.m8.4.4.4.4.2.2"><abs id="S3.SS5.p2.8.m8.4.4.4.4.2.1.1.cmml" xref="S3.SS5.p2.8.m8.4.4.4.4.2.2.1"></abs><ci id="S3.SS5.p2.8.m8.2.2.2.2.cmml" xref="S3.SS5.p2.8.m8.2.2.2.2">𝑟</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.8.m8.4c">L_{|h|,|r|}</annotation></semantics></math> is the resultant adapted Levenshtein distance and the number of true positives.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m1.2" class="ltx_Math" alttext="\displaystyle ALD_{0,0}=0" display="inline"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.3" xref="S3.Ex1.m1.2.3.cmml"><mrow id="S3.Ex1.m1.2.3.2" xref="S3.Ex1.m1.2.3.2.cmml"><mi id="S3.Ex1.m1.2.3.2.2" xref="S3.Ex1.m1.2.3.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.3.2.1" xref="S3.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="S3.Ex1.m1.2.3.2.3" xref="S3.Ex1.m1.2.3.2.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.3.2.1a" xref="S3.Ex1.m1.2.3.2.1.cmml">​</mo><msub id="S3.Ex1.m1.2.3.2.4" xref="S3.Ex1.m1.2.3.2.4.cmml"><mi id="S3.Ex1.m1.2.3.2.4.2" xref="S3.Ex1.m1.2.3.2.4.2.cmml">D</mi><mrow id="S3.Ex1.m1.2.2.2.4" xref="S3.Ex1.m1.2.2.2.3.cmml"><mn id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">0</mn><mo id="S3.Ex1.m1.2.2.2.4.1" xref="S3.Ex1.m1.2.2.2.3.cmml">,</mo><mn id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml">0</mn></mrow></msub></mrow><mo id="S3.Ex1.m1.2.3.1" xref="S3.Ex1.m1.2.3.1.cmml">=</mo><mn id="S3.Ex1.m1.2.3.3" xref="S3.Ex1.m1.2.3.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.3.cmml" xref="S3.Ex1.m1.2.3"><eq id="S3.Ex1.m1.2.3.1.cmml" xref="S3.Ex1.m1.2.3.1"></eq><apply id="S3.Ex1.m1.2.3.2.cmml" xref="S3.Ex1.m1.2.3.2"><times id="S3.Ex1.m1.2.3.2.1.cmml" xref="S3.Ex1.m1.2.3.2.1"></times><ci id="S3.Ex1.m1.2.3.2.2.cmml" xref="S3.Ex1.m1.2.3.2.2">𝐴</ci><ci id="S3.Ex1.m1.2.3.2.3.cmml" xref="S3.Ex1.m1.2.3.2.3">𝐿</ci><apply id="S3.Ex1.m1.2.3.2.4.cmml" xref="S3.Ex1.m1.2.3.2.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.3.2.4.1.cmml" xref="S3.Ex1.m1.2.3.2.4">subscript</csymbol><ci id="S3.Ex1.m1.2.3.2.4.2.cmml" xref="S3.Ex1.m1.2.3.2.4.2">𝐷</ci><list id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.4"><cn type="integer" id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">0</cn><cn type="integer" id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2">0</cn></list></apply></apply><cn type="integer" id="S3.Ex1.m1.2.3.3.cmml" xref="S3.Ex1.m1.2.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\displaystyle ALD_{0,0}=0</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex2.m1.5" class="ltx_Math" alttext="\displaystyle ALD_{i,0}=i,1\leq i\leq|h|" display="inline"><semantics id="S3.Ex2.m1.5a"><mrow id="S3.Ex2.m1.5.5.2" xref="S3.Ex2.m1.5.5.3.cmml"><mrow id="S3.Ex2.m1.4.4.1.1" xref="S3.Ex2.m1.4.4.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.1.1.2" xref="S3.Ex2.m1.4.4.1.1.2.cmml"><mi id="S3.Ex2.m1.4.4.1.1.2.2" xref="S3.Ex2.m1.4.4.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.2.1" xref="S3.Ex2.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.4.4.1.1.2.3" xref="S3.Ex2.m1.4.4.1.1.2.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.2.1a" xref="S3.Ex2.m1.4.4.1.1.2.1.cmml">​</mo><msub id="S3.Ex2.m1.4.4.1.1.2.4" xref="S3.Ex2.m1.4.4.1.1.2.4.cmml"><mi id="S3.Ex2.m1.4.4.1.1.2.4.2" xref="S3.Ex2.m1.4.4.1.1.2.4.2.cmml">D</mi><mrow id="S3.Ex2.m1.2.2.2.4" xref="S3.Ex2.m1.2.2.2.3.cmml"><mi id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex2.m1.2.2.2.4.1" xref="S3.Ex2.m1.2.2.2.3.cmml">,</mo><mn id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.cmml">0</mn></mrow></msub></mrow><mo id="S3.Ex2.m1.4.4.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.cmml">=</mo><mi id="S3.Ex2.m1.4.4.1.1.3" xref="S3.Ex2.m1.4.4.1.1.3.cmml">i</mi></mrow><mo id="S3.Ex2.m1.5.5.2.3" xref="S3.Ex2.m1.5.5.3a.cmml">,</mo><mrow id="S3.Ex2.m1.5.5.2.2" xref="S3.Ex2.m1.5.5.2.2.cmml"><mn id="S3.Ex2.m1.5.5.2.2.2" xref="S3.Ex2.m1.5.5.2.2.2.cmml">1</mn><mo id="S3.Ex2.m1.5.5.2.2.3" xref="S3.Ex2.m1.5.5.2.2.3.cmml">≤</mo><mi id="S3.Ex2.m1.5.5.2.2.4" xref="S3.Ex2.m1.5.5.2.2.4.cmml">i</mi><mo id="S3.Ex2.m1.5.5.2.2.5" xref="S3.Ex2.m1.5.5.2.2.5.cmml">≤</mo><mrow id="S3.Ex2.m1.5.5.2.2.6.2" xref="S3.Ex2.m1.5.5.2.2.6.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.5.5.2.2.6.2.1" xref="S3.Ex2.m1.5.5.2.2.6.1.1.cmml">|</mo><mi id="S3.Ex2.m1.3.3" xref="S3.Ex2.m1.3.3.cmml">h</mi><mo stretchy="false" id="S3.Ex2.m1.5.5.2.2.6.2.2" xref="S3.Ex2.m1.5.5.2.2.6.1.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.5b"><apply id="S3.Ex2.m1.5.5.3.cmml" xref="S3.Ex2.m1.5.5.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.3a.cmml" xref="S3.Ex2.m1.5.5.2.3">formulae-sequence</csymbol><apply id="S3.Ex2.m1.4.4.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1"><eq id="S3.Ex2.m1.4.4.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1"></eq><apply id="S3.Ex2.m1.4.4.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2"><times id="S3.Ex2.m1.4.4.1.1.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.1"></times><ci id="S3.Ex2.m1.4.4.1.1.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2">𝐴</ci><ci id="S3.Ex2.m1.4.4.1.1.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.3">𝐿</ci><apply id="S3.Ex2.m1.4.4.1.1.2.4.cmml" xref="S3.Ex2.m1.4.4.1.1.2.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.2.4.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.4">subscript</csymbol><ci id="S3.Ex2.m1.4.4.1.1.2.4.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.4.2">𝐷</ci><list id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.4"><ci id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">𝑖</ci><cn type="integer" id="S3.Ex2.m1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2">0</cn></list></apply></apply><ci id="S3.Ex2.m1.4.4.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.3">𝑖</ci></apply><apply id="S3.Ex2.m1.5.5.2.2.cmml" xref="S3.Ex2.m1.5.5.2.2"><and id="S3.Ex2.m1.5.5.2.2a.cmml" xref="S3.Ex2.m1.5.5.2.2"></and><apply id="S3.Ex2.m1.5.5.2.2b.cmml" xref="S3.Ex2.m1.5.5.2.2"><leq id="S3.Ex2.m1.5.5.2.2.3.cmml" xref="S3.Ex2.m1.5.5.2.2.3"></leq><cn type="integer" id="S3.Ex2.m1.5.5.2.2.2.cmml" xref="S3.Ex2.m1.5.5.2.2.2">1</cn><ci id="S3.Ex2.m1.5.5.2.2.4.cmml" xref="S3.Ex2.m1.5.5.2.2.4">𝑖</ci></apply><apply id="S3.Ex2.m1.5.5.2.2c.cmml" xref="S3.Ex2.m1.5.5.2.2"><leq id="S3.Ex2.m1.5.5.2.2.5.cmml" xref="S3.Ex2.m1.5.5.2.2.5"></leq><share href="#S3.Ex2.m1.5.5.2.2.4.cmml" id="S3.Ex2.m1.5.5.2.2d.cmml" xref="S3.Ex2.m1.5.5.2.2"></share><apply id="S3.Ex2.m1.5.5.2.2.6.1.cmml" xref="S3.Ex2.m1.5.5.2.2.6.2"><abs id="S3.Ex2.m1.5.5.2.2.6.1.1.cmml" xref="S3.Ex2.m1.5.5.2.2.6.2.1"></abs><ci id="S3.Ex2.m1.3.3.cmml" xref="S3.Ex2.m1.3.3">ℎ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.5c">\displaystyle ALD_{i,0}=i,1\leq i\leq|h|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex3.m1.5" class="ltx_Math" alttext="\displaystyle ALD_{0,j}=j,1\leq j\leq|r|" display="inline"><semantics id="S3.Ex3.m1.5a"><mrow id="S3.Ex3.m1.5.5.2" xref="S3.Ex3.m1.5.5.3.cmml"><mrow id="S3.Ex3.m1.4.4.1.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.2" xref="S3.Ex3.m1.4.4.1.1.2.cmml"><mi id="S3.Ex3.m1.4.4.1.1.2.2" xref="S3.Ex3.m1.4.4.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.2.1" xref="S3.Ex3.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S3.Ex3.m1.4.4.1.1.2.3" xref="S3.Ex3.m1.4.4.1.1.2.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.2.1a" xref="S3.Ex3.m1.4.4.1.1.2.1.cmml">​</mo><msub id="S3.Ex3.m1.4.4.1.1.2.4" xref="S3.Ex3.m1.4.4.1.1.2.4.cmml"><mi id="S3.Ex3.m1.4.4.1.1.2.4.2" xref="S3.Ex3.m1.4.4.1.1.2.4.2.cmml">D</mi><mrow id="S3.Ex3.m1.2.2.2.4" xref="S3.Ex3.m1.2.2.2.3.cmml"><mn id="S3.Ex3.m1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml">0</mn><mo id="S3.Ex3.m1.2.2.2.4.1" xref="S3.Ex3.m1.2.2.2.3.cmml">,</mo><mi id="S3.Ex3.m1.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.cmml">j</mi></mrow></msub></mrow><mo id="S3.Ex3.m1.4.4.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.cmml">=</mo><mi id="S3.Ex3.m1.4.4.1.1.3" xref="S3.Ex3.m1.4.4.1.1.3.cmml">j</mi></mrow><mo id="S3.Ex3.m1.5.5.2.3" xref="S3.Ex3.m1.5.5.3a.cmml">,</mo><mrow id="S3.Ex3.m1.5.5.2.2" xref="S3.Ex3.m1.5.5.2.2.cmml"><mn id="S3.Ex3.m1.5.5.2.2.2" xref="S3.Ex3.m1.5.5.2.2.2.cmml">1</mn><mo id="S3.Ex3.m1.5.5.2.2.3" xref="S3.Ex3.m1.5.5.2.2.3.cmml">≤</mo><mi id="S3.Ex3.m1.5.5.2.2.4" xref="S3.Ex3.m1.5.5.2.2.4.cmml">j</mi><mo id="S3.Ex3.m1.5.5.2.2.5" xref="S3.Ex3.m1.5.5.2.2.5.cmml">≤</mo><mrow id="S3.Ex3.m1.5.5.2.2.6.2" xref="S3.Ex3.m1.5.5.2.2.6.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.5.5.2.2.6.2.1" xref="S3.Ex3.m1.5.5.2.2.6.1.1.cmml">|</mo><mi id="S3.Ex3.m1.3.3" xref="S3.Ex3.m1.3.3.cmml">r</mi><mo stretchy="false" id="S3.Ex3.m1.5.5.2.2.6.2.2" xref="S3.Ex3.m1.5.5.2.2.6.1.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.5b"><apply id="S3.Ex3.m1.5.5.3.cmml" xref="S3.Ex3.m1.5.5.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.5.5.3a.cmml" xref="S3.Ex3.m1.5.5.2.3">formulae-sequence</csymbol><apply id="S3.Ex3.m1.4.4.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1"><eq id="S3.Ex3.m1.4.4.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1"></eq><apply id="S3.Ex3.m1.4.4.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.2"><times id="S3.Ex3.m1.4.4.1.1.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.2.1"></times><ci id="S3.Ex3.m1.4.4.1.1.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.2.2">𝐴</ci><ci id="S3.Ex3.m1.4.4.1.1.2.3.cmml" xref="S3.Ex3.m1.4.4.1.1.2.3">𝐿</ci><apply id="S3.Ex3.m1.4.4.1.1.2.4.cmml" xref="S3.Ex3.m1.4.4.1.1.2.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.2.4.1.cmml" xref="S3.Ex3.m1.4.4.1.1.2.4">subscript</csymbol><ci id="S3.Ex3.m1.4.4.1.1.2.4.2.cmml" xref="S3.Ex3.m1.4.4.1.1.2.4.2">𝐷</ci><list id="S3.Ex3.m1.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.4"><cn type="integer" id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1">0</cn><ci id="S3.Ex3.m1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2">𝑗</ci></list></apply></apply><ci id="S3.Ex3.m1.4.4.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3">𝑗</ci></apply><apply id="S3.Ex3.m1.5.5.2.2.cmml" xref="S3.Ex3.m1.5.5.2.2"><and id="S3.Ex3.m1.5.5.2.2a.cmml" xref="S3.Ex3.m1.5.5.2.2"></and><apply id="S3.Ex3.m1.5.5.2.2b.cmml" xref="S3.Ex3.m1.5.5.2.2"><leq id="S3.Ex3.m1.5.5.2.2.3.cmml" xref="S3.Ex3.m1.5.5.2.2.3"></leq><cn type="integer" id="S3.Ex3.m1.5.5.2.2.2.cmml" xref="S3.Ex3.m1.5.5.2.2.2">1</cn><ci id="S3.Ex3.m1.5.5.2.2.4.cmml" xref="S3.Ex3.m1.5.5.2.2.4">𝑗</ci></apply><apply id="S3.Ex3.m1.5.5.2.2c.cmml" xref="S3.Ex3.m1.5.5.2.2"><leq id="S3.Ex3.m1.5.5.2.2.5.cmml" xref="S3.Ex3.m1.5.5.2.2.5"></leq><share href="#S3.Ex3.m1.5.5.2.2.4.cmml" id="S3.Ex3.m1.5.5.2.2d.cmml" xref="S3.Ex3.m1.5.5.2.2"></share><apply id="S3.Ex3.m1.5.5.2.2.6.1.cmml" xref="S3.Ex3.m1.5.5.2.2.6.2"><abs id="S3.Ex3.m1.5.5.2.2.6.1.1.cmml" xref="S3.Ex3.m1.5.5.2.2.6.2.1"></abs><ci id="S3.Ex3.m1.3.3.cmml" xref="S3.Ex3.m1.3.3">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.5c">\displaystyle ALD_{0,j}=j,1\leq j\leq|r|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex4.m1.6" class="ltx_Math" alttext="\displaystyle ALD_{i,j}=max\begin{cases}ALD_{i-1,j-1}+1,~{}r_{i}=h_{j}\\
ALD_{i-1,j-1},~{}r_{i}\neq h_{j}\\
ALD_{i-1,j}\\
ALD_{i,j-1}\\
\end{cases}" display="inline"><semantics id="S3.Ex4.m1.6a"><mrow id="S3.Ex4.m1.6.7" xref="S3.Ex4.m1.6.7.cmml"><mrow id="S3.Ex4.m1.6.7.2" xref="S3.Ex4.m1.6.7.2.cmml"><mi id="S3.Ex4.m1.6.7.2.2" xref="S3.Ex4.m1.6.7.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.7.2.1" xref="S3.Ex4.m1.6.7.2.1.cmml">​</mo><mi id="S3.Ex4.m1.6.7.2.3" xref="S3.Ex4.m1.6.7.2.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.7.2.1a" xref="S3.Ex4.m1.6.7.2.1.cmml">​</mo><msub id="S3.Ex4.m1.6.7.2.4" xref="S3.Ex4.m1.6.7.2.4.cmml"><mi id="S3.Ex4.m1.6.7.2.4.2" xref="S3.Ex4.m1.6.7.2.4.2.cmml">D</mi><mrow id="S3.Ex4.m1.6.6.2.4" xref="S3.Ex4.m1.6.6.2.3.cmml"><mi id="S3.Ex4.m1.5.5.1.1" xref="S3.Ex4.m1.5.5.1.1.cmml">i</mi><mo id="S3.Ex4.m1.6.6.2.4.1" xref="S3.Ex4.m1.6.6.2.3.cmml">,</mo><mi id="S3.Ex4.m1.6.6.2.2" xref="S3.Ex4.m1.6.6.2.2.cmml">j</mi></mrow></msub></mrow><mo id="S3.Ex4.m1.6.7.1" xref="S3.Ex4.m1.6.7.1.cmml">=</mo><mrow id="S3.Ex4.m1.6.7.3" xref="S3.Ex4.m1.6.7.3.cmml"><mi id="S3.Ex4.m1.6.7.3.2" xref="S3.Ex4.m1.6.7.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.7.3.1" xref="S3.Ex4.m1.6.7.3.1.cmml">​</mo><mi id="S3.Ex4.m1.6.7.3.3" xref="S3.Ex4.m1.6.7.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.7.3.1a" xref="S3.Ex4.m1.6.7.3.1.cmml">​</mo><mi id="S3.Ex4.m1.6.7.3.4" xref="S3.Ex4.m1.6.7.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.7.3.1b" xref="S3.Ex4.m1.6.7.3.1.cmml">​</mo><mrow id="S3.Ex4.m1.4.4a" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mo id="S3.Ex4.m1.4.4a.5" xref="S3.Ex4.m1.6.7.3.5.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.Ex4.m1.4.4.4a" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mtr id="S3.Ex4.m1.4.4.4aa" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex4.m1.4.4.4ab" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mrow id="S3.Ex4.m1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex4.m1.1.1.1.1.1.1.4.2" xref="S3.Ex4.m1.1.1.1.1.1.1.4.3.cmml"><mrow id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.cmml"><mrow id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.2" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.1" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.1.cmml">​</mo><mi id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.3" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.1a" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.1.cmml">​</mo><msub id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.2" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.2.cmml">D</mi><mrow id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.3.cmml"><mrow id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.3" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">j</mi><mo id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.1" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.1.cmml">−</mo><mn id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.3.cmml">1</mn></mrow></mrow></msub></mrow><mo id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.1.cmml">+</mo><mn id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.3.cmml">1</mn></mrow><mo rspace="0.497em" id="S3.Ex4.m1.1.1.1.1.1.1.4.2.3" xref="S3.Ex4.m1.1.1.1.1.1.1.4.3.cmml">,</mo><msub id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.2" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.2.cmml">r</mi><mi id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.3" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex4.m1.1.1.1.1.1.1.5" xref="S3.Ex4.m1.1.1.1.1.1.1.5.cmml">=</mo><msub id="S3.Ex4.m1.1.1.1.1.1.1.6" xref="S3.Ex4.m1.1.1.1.1.1.1.6.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.6.2" xref="S3.Ex4.m1.1.1.1.1.1.1.6.2.cmml">h</mi><mi id="S3.Ex4.m1.1.1.1.1.1.1.6.3" xref="S3.Ex4.m1.1.1.1.1.1.1.6.3.cmml">j</mi></msub></mrow></mtd><mtd id="S3.Ex4.m1.4.4.4ac" xref="S3.Ex4.m1.6.7.3.5.1.1.cmml"></mtd></mtr><mtr id="S3.Ex4.m1.4.4.4ad" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex4.m1.4.4.4ae" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mrow id="S3.Ex4.m1.2.2.2.2.1.1" xref="S3.Ex4.m1.2.2.2.2.1.1.cmml"><mrow id="S3.Ex4.m1.2.2.2.2.1.1.4.2" xref="S3.Ex4.m1.2.2.2.2.1.1.4.3.cmml"><mrow id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.cmml"><mi id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.2" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.1" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.1.cmml">​</mo><mi id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.3" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.1a" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.1.cmml">​</mo><msub id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.cmml"><mi id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.2" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.2.cmml">D</mi><mrow id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.3.cmml"><mrow id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.2" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.2.cmml">i</mi><mo id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.1" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.3" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.3" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.3.cmml">,</mo><mrow id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.cmml"><mi id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.2" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.2.cmml">j</mi><mo id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.1" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.1.cmml">−</mo><mn id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.3" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.3.cmml">1</mn></mrow></mrow></msub></mrow><mo rspace="0.497em" id="S3.Ex4.m1.2.2.2.2.1.1.4.2.3" xref="S3.Ex4.m1.2.2.2.2.1.1.4.3.cmml">,</mo><msub id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.cmml"><mi id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.2" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.2.cmml">r</mi><mi id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.3" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex4.m1.2.2.2.2.1.1.5" xref="S3.Ex4.m1.2.2.2.2.1.1.5.cmml">≠</mo><msub id="S3.Ex4.m1.2.2.2.2.1.1.6" xref="S3.Ex4.m1.2.2.2.2.1.1.6.cmml"><mi id="S3.Ex4.m1.2.2.2.2.1.1.6.2" xref="S3.Ex4.m1.2.2.2.2.1.1.6.2.cmml">h</mi><mi id="S3.Ex4.m1.2.2.2.2.1.1.6.3" xref="S3.Ex4.m1.2.2.2.2.1.1.6.3.cmml">j</mi></msub></mrow></mtd><mtd id="S3.Ex4.m1.4.4.4af" xref="S3.Ex4.m1.6.7.3.5.1.1.cmml"></mtd></mtr><mtr id="S3.Ex4.m1.4.4.4ag" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex4.m1.4.4.4ah" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mrow id="S3.Ex4.m1.3.3.3.3.1.1" xref="S3.Ex4.m1.3.3.3.3.1.1.cmml"><mi id="S3.Ex4.m1.3.3.3.3.1.1.4" xref="S3.Ex4.m1.3.3.3.3.1.1.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.3.3.3.3.1.1.3" xref="S3.Ex4.m1.3.3.3.3.1.1.3.cmml">​</mo><mi id="S3.Ex4.m1.3.3.3.3.1.1.5" xref="S3.Ex4.m1.3.3.3.3.1.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.3.3.3.3.1.1.3a" xref="S3.Ex4.m1.3.3.3.3.1.1.3.cmml">​</mo><msub id="S3.Ex4.m1.3.3.3.3.1.1.6" xref="S3.Ex4.m1.3.3.3.3.1.1.6.cmml"><mi id="S3.Ex4.m1.3.3.3.3.1.1.6.2" xref="S3.Ex4.m1.3.3.3.3.1.1.6.2.cmml">D</mi><mrow id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.3.cmml"><mrow id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.cmml"><mi id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.2" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.2.cmml">i</mi><mo id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.1" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.1.cmml">−</mo><mn id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.3" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.3.cmml">1</mn></mrow><mo id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.2" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.3.cmml">,</mo><mi id="S3.Ex4.m1.3.3.3.3.1.1.1.1.1" xref="S3.Ex4.m1.3.3.3.3.1.1.1.1.1.cmml">j</mi></mrow></msub></mrow></mtd><mtd id="S3.Ex4.m1.4.4.4ai" xref="S3.Ex4.m1.6.7.3.5.1.1.cmml"></mtd></mtr><mtr id="S3.Ex4.m1.4.4.4aj" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex4.m1.4.4.4ak" xref="S3.Ex4.m1.6.7.3.5.1.cmml"><mrow id="S3.Ex4.m1.4.4.4.4.1.1" xref="S3.Ex4.m1.4.4.4.4.1.1.cmml"><mi id="S3.Ex4.m1.4.4.4.4.1.1.4" xref="S3.Ex4.m1.4.4.4.4.1.1.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.4.4.4.4.1.1.3" xref="S3.Ex4.m1.4.4.4.4.1.1.3.cmml">​</mo><mi id="S3.Ex4.m1.4.4.4.4.1.1.5" xref="S3.Ex4.m1.4.4.4.4.1.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.4.4.4.4.1.1.3a" xref="S3.Ex4.m1.4.4.4.4.1.1.3.cmml">​</mo><msub id="S3.Ex4.m1.4.4.4.4.1.1.6" xref="S3.Ex4.m1.4.4.4.4.1.1.6.cmml"><mi id="S3.Ex4.m1.4.4.4.4.1.1.6.2" xref="S3.Ex4.m1.4.4.4.4.1.1.6.2.cmml">D</mi><mrow id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.3.cmml"><mi id="S3.Ex4.m1.4.4.4.4.1.1.1.1.1" xref="S3.Ex4.m1.4.4.4.4.1.1.1.1.1.cmml">i</mi><mo id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.2" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.3.cmml">,</mo><mrow id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.cmml"><mi id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.2" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.2.cmml">j</mi><mo id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.1" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.1.cmml">−</mo><mn id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.3" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.3.cmml">1</mn></mrow></mrow></msub></mrow></mtd><mtd id="S3.Ex4.m1.4.4.4al" xref="S3.Ex4.m1.6.7.3.5.1.1.cmml"></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.6b"><apply id="S3.Ex4.m1.6.7.cmml" xref="S3.Ex4.m1.6.7"><eq id="S3.Ex4.m1.6.7.1.cmml" xref="S3.Ex4.m1.6.7.1"></eq><apply id="S3.Ex4.m1.6.7.2.cmml" xref="S3.Ex4.m1.6.7.2"><times id="S3.Ex4.m1.6.7.2.1.cmml" xref="S3.Ex4.m1.6.7.2.1"></times><ci id="S3.Ex4.m1.6.7.2.2.cmml" xref="S3.Ex4.m1.6.7.2.2">𝐴</ci><ci id="S3.Ex4.m1.6.7.2.3.cmml" xref="S3.Ex4.m1.6.7.2.3">𝐿</ci><apply id="S3.Ex4.m1.6.7.2.4.cmml" xref="S3.Ex4.m1.6.7.2.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.7.2.4.1.cmml" xref="S3.Ex4.m1.6.7.2.4">subscript</csymbol><ci id="S3.Ex4.m1.6.7.2.4.2.cmml" xref="S3.Ex4.m1.6.7.2.4.2">𝐷</ci><list id="S3.Ex4.m1.6.6.2.3.cmml" xref="S3.Ex4.m1.6.6.2.4"><ci id="S3.Ex4.m1.5.5.1.1.cmml" xref="S3.Ex4.m1.5.5.1.1">𝑖</ci><ci id="S3.Ex4.m1.6.6.2.2.cmml" xref="S3.Ex4.m1.6.6.2.2">𝑗</ci></list></apply></apply><apply id="S3.Ex4.m1.6.7.3.cmml" xref="S3.Ex4.m1.6.7.3"><times id="S3.Ex4.m1.6.7.3.1.cmml" xref="S3.Ex4.m1.6.7.3.1"></times><ci id="S3.Ex4.m1.6.7.3.2.cmml" xref="S3.Ex4.m1.6.7.3.2">𝑚</ci><ci id="S3.Ex4.m1.6.7.3.3.cmml" xref="S3.Ex4.m1.6.7.3.3">𝑎</ci><ci id="S3.Ex4.m1.6.7.3.4.cmml" xref="S3.Ex4.m1.6.7.3.4">𝑥</ci><apply id="S3.Ex4.m1.6.7.3.5.1.cmml" xref="S3.Ex4.m1.4.4a"><csymbol cd="latexml" id="S3.Ex4.m1.6.7.3.5.1.1.cmml" xref="S3.Ex4.m1.4.4a.5">cases</csymbol><apply id="S3.Ex4.m1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1"><eq id="S3.Ex4.m1.1.1.1.1.1.1.5.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.5"></eq><list id="S3.Ex4.m1.1.1.1.1.1.1.4.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2"><apply id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1"><plus id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.1"></plus><apply id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2"><times id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.1"></times><ci id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.2">𝐴</ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.3">𝐿</ci><apply id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.2.4.2">𝐷</ci><list id="S3.Ex4.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2"><apply id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2"><minus id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.1"></minus><ci id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.2">𝑗</ci><cn type="integer" id="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.2.2.2.2.3">1</cn></apply></list></apply></apply><cn type="integer" id="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.3.1.1.3">1</cn></apply><apply id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.2">𝑟</ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.4.2.2.3">𝑖</ci></apply></list><apply id="S3.Ex4.m1.1.1.1.1.1.1.6.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.1.6.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.6">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.1.6.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.6.2">ℎ</ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.6.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.6.3">𝑗</ci></apply></apply><ci id="S3.Ex4.m1.6.7.3.5.1.3a.cmml" xref="S3.Ex4.m1.4.4a"><mtext class="ltx_mathvariant_italic" id="S3.Ex4.m1.6.7.3.5.1.3.cmml" xref="S3.Ex4.m1.4.4a.5">otherwise</mtext></ci><apply id="S3.Ex4.m1.2.2.2.2.1.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1"><neq id="S3.Ex4.m1.2.2.2.2.1.1.5.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.5"></neq><list id="S3.Ex4.m1.2.2.2.2.1.1.4.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2"><apply id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1"><times id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.1"></times><ci id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.2">𝐴</ci><ci id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.3">𝐿</ci><apply id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.3.1.1.4.2">𝐷</ci><list id="S3.Ex4.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2"><apply id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1"><minus id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.1"></minus><ci id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2"><minus id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.1"></minus><ci id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.2">𝑗</ci><cn type="integer" id="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.2.2.2.2.3">1</cn></apply></list></apply></apply><apply id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.2">𝑟</ci><ci id="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.4.2.2.3">𝑖</ci></apply></list><apply id="S3.Ex4.m1.2.2.2.2.1.1.6.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.6"><csymbol cd="ambiguous" id="S3.Ex4.m1.2.2.2.2.1.1.6.1.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.6">subscript</csymbol><ci id="S3.Ex4.m1.2.2.2.2.1.1.6.2.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.6.2">ℎ</ci><ci id="S3.Ex4.m1.2.2.2.2.1.1.6.3.cmml" xref="S3.Ex4.m1.2.2.2.2.1.1.6.3">𝑗</ci></apply></apply><ci id="S3.Ex4.m1.6.7.3.5.1.5a.cmml" xref="S3.Ex4.m1.4.4a"><mtext class="ltx_mathvariant_italic" id="S3.Ex4.m1.6.7.3.5.1.5.cmml" xref="S3.Ex4.m1.4.4a.5">otherwise</mtext></ci><apply id="S3.Ex4.m1.3.3.3.3.1.1.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1"><times id="S3.Ex4.m1.3.3.3.3.1.1.3.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.3"></times><ci id="S3.Ex4.m1.3.3.3.3.1.1.4.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.4">𝐴</ci><ci id="S3.Ex4.m1.3.3.3.3.1.1.5.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.5">𝐿</ci><apply id="S3.Ex4.m1.3.3.3.3.1.1.6.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.6"><csymbol cd="ambiguous" id="S3.Ex4.m1.3.3.3.3.1.1.6.1.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.6">subscript</csymbol><ci id="S3.Ex4.m1.3.3.3.3.1.1.6.2.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.6.2">𝐷</ci><list id="S3.Ex4.m1.3.3.3.3.1.1.2.2.3.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2"><apply id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1"><minus id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.1.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.1"></minus><ci id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.2.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.2">𝑖</ci><cn type="integer" id="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.3.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.2.2.2.1.3">1</cn></apply><ci id="S3.Ex4.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.Ex4.m1.3.3.3.3.1.1.1.1.1">𝑗</ci></list></apply></apply><ci id="S3.Ex4.m1.6.7.3.5.1.7a.cmml" xref="S3.Ex4.m1.4.4a"><mtext class="ltx_mathvariant_italic" id="S3.Ex4.m1.6.7.3.5.1.7.cmml" xref="S3.Ex4.m1.4.4a.5">otherwise</mtext></ci><apply id="S3.Ex4.m1.4.4.4.4.1.1.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1"><times id="S3.Ex4.m1.4.4.4.4.1.1.3.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.3"></times><ci id="S3.Ex4.m1.4.4.4.4.1.1.4.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.4">𝐴</ci><ci id="S3.Ex4.m1.4.4.4.4.1.1.5.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.5">𝐿</ci><apply id="S3.Ex4.m1.4.4.4.4.1.1.6.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.6"><csymbol cd="ambiguous" id="S3.Ex4.m1.4.4.4.4.1.1.6.1.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.6">subscript</csymbol><ci id="S3.Ex4.m1.4.4.4.4.1.1.6.2.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.6.2">𝐷</ci><list id="S3.Ex4.m1.4.4.4.4.1.1.2.2.3.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2"><ci id="S3.Ex4.m1.4.4.4.4.1.1.1.1.1.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.1.1.1">𝑖</ci><apply id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1"><minus id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.1.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.1"></minus><ci id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.2.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.2">𝑗</ci><cn type="integer" id="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.3.cmml" xref="S3.Ex4.m1.4.4.4.4.1.1.2.2.2.1.3">1</cn></apply></list></apply></apply><ci id="S3.Ex4.m1.6.7.3.5.1.9a.cmml" xref="S3.Ex4.m1.4.4a"><mtext class="ltx_mathvariant_italic" id="S3.Ex4.m1.6.7.3.5.1.9.cmml" xref="S3.Ex4.m1.4.4a.5">otherwise</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.6c">\displaystyle ALD_{i,j}=max\begin{cases}ALD_{i-1,j-1}+1,~{}r_{i}=h_{j}\\
ALD_{i-1,j-1},~{}r_{i}\neq h_{j}\\
ALD_{i-1,j}\\
ALD_{i,j-1}\\
\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex5.m1.4" class="ltx_Math" alttext="\displaystyle~{}~{}~{}~{}~{}1\leq i\leq|h|,1\leq j\leq|r|" display="inline"><semantics id="S3.Ex5.m1.4a"><mrow id="S3.Ex5.m1.4.4.2" xref="S3.Ex5.m1.4.4.3.cmml"><mrow id="S3.Ex5.m1.3.3.1.1" xref="S3.Ex5.m1.3.3.1.1.cmml"><mn id="S3.Ex5.m1.3.3.1.1.2" xref="S3.Ex5.m1.3.3.1.1.2.cmml">1</mn><mo id="S3.Ex5.m1.3.3.1.1.3" xref="S3.Ex5.m1.3.3.1.1.3.cmml">≤</mo><mi id="S3.Ex5.m1.3.3.1.1.4" xref="S3.Ex5.m1.3.3.1.1.4.cmml">i</mi><mo id="S3.Ex5.m1.3.3.1.1.5" xref="S3.Ex5.m1.3.3.1.1.5.cmml">≤</mo><mrow id="S3.Ex5.m1.3.3.1.1.6.2" xref="S3.Ex5.m1.3.3.1.1.6.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.3.3.1.1.6.2.1" xref="S3.Ex5.m1.3.3.1.1.6.1.1.cmml">|</mo><mi id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml">h</mi><mo stretchy="false" id="S3.Ex5.m1.3.3.1.1.6.2.2" xref="S3.Ex5.m1.3.3.1.1.6.1.1.cmml">|</mo></mrow></mrow><mo id="S3.Ex5.m1.4.4.2.3" xref="S3.Ex5.m1.4.4.3a.cmml">,</mo><mrow id="S3.Ex5.m1.4.4.2.2" xref="S3.Ex5.m1.4.4.2.2.cmml"><mn id="S3.Ex5.m1.4.4.2.2.2" xref="S3.Ex5.m1.4.4.2.2.2.cmml">1</mn><mo id="S3.Ex5.m1.4.4.2.2.3" xref="S3.Ex5.m1.4.4.2.2.3.cmml">≤</mo><mi id="S3.Ex5.m1.4.4.2.2.4" xref="S3.Ex5.m1.4.4.2.2.4.cmml">j</mi><mo id="S3.Ex5.m1.4.4.2.2.5" xref="S3.Ex5.m1.4.4.2.2.5.cmml">≤</mo><mrow id="S3.Ex5.m1.4.4.2.2.6.2" xref="S3.Ex5.m1.4.4.2.2.6.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.4.4.2.2.6.2.1" xref="S3.Ex5.m1.4.4.2.2.6.1.1.cmml">|</mo><mi id="S3.Ex5.m1.2.2" xref="S3.Ex5.m1.2.2.cmml">r</mi><mo stretchy="false" id="S3.Ex5.m1.4.4.2.2.6.2.2" xref="S3.Ex5.m1.4.4.2.2.6.1.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.4b"><apply id="S3.Ex5.m1.4.4.3.cmml" xref="S3.Ex5.m1.4.4.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.4.4.3a.cmml" xref="S3.Ex5.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.Ex5.m1.3.3.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1"><and id="S3.Ex5.m1.3.3.1.1a.cmml" xref="S3.Ex5.m1.3.3.1.1"></and><apply id="S3.Ex5.m1.3.3.1.1b.cmml" xref="S3.Ex5.m1.3.3.1.1"><leq id="S3.Ex5.m1.3.3.1.1.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3"></leq><cn type="integer" id="S3.Ex5.m1.3.3.1.1.2.cmml" xref="S3.Ex5.m1.3.3.1.1.2">1</cn><ci id="S3.Ex5.m1.3.3.1.1.4.cmml" xref="S3.Ex5.m1.3.3.1.1.4">𝑖</ci></apply><apply id="S3.Ex5.m1.3.3.1.1c.cmml" xref="S3.Ex5.m1.3.3.1.1"><leq id="S3.Ex5.m1.3.3.1.1.5.cmml" xref="S3.Ex5.m1.3.3.1.1.5"></leq><share href="#S3.Ex5.m1.3.3.1.1.4.cmml" id="S3.Ex5.m1.3.3.1.1d.cmml" xref="S3.Ex5.m1.3.3.1.1"></share><apply id="S3.Ex5.m1.3.3.1.1.6.1.cmml" xref="S3.Ex5.m1.3.3.1.1.6.2"><abs id="S3.Ex5.m1.3.3.1.1.6.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1.6.2.1"></abs><ci id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1">ℎ</ci></apply></apply></apply><apply id="S3.Ex5.m1.4.4.2.2.cmml" xref="S3.Ex5.m1.4.4.2.2"><and id="S3.Ex5.m1.4.4.2.2a.cmml" xref="S3.Ex5.m1.4.4.2.2"></and><apply id="S3.Ex5.m1.4.4.2.2b.cmml" xref="S3.Ex5.m1.4.4.2.2"><leq id="S3.Ex5.m1.4.4.2.2.3.cmml" xref="S3.Ex5.m1.4.4.2.2.3"></leq><cn type="integer" id="S3.Ex5.m1.4.4.2.2.2.cmml" xref="S3.Ex5.m1.4.4.2.2.2">1</cn><ci id="S3.Ex5.m1.4.4.2.2.4.cmml" xref="S3.Ex5.m1.4.4.2.2.4">𝑗</ci></apply><apply id="S3.Ex5.m1.4.4.2.2c.cmml" xref="S3.Ex5.m1.4.4.2.2"><leq id="S3.Ex5.m1.4.4.2.2.5.cmml" xref="S3.Ex5.m1.4.4.2.2.5"></leq><share href="#S3.Ex5.m1.4.4.2.2.4.cmml" id="S3.Ex5.m1.4.4.2.2d.cmml" xref="S3.Ex5.m1.4.4.2.2"></share><apply id="S3.Ex5.m1.4.4.2.2.6.1.cmml" xref="S3.Ex5.m1.4.4.2.2.6.2"><abs id="S3.Ex5.m1.4.4.2.2.6.1.1.cmml" xref="S3.Ex5.m1.4.4.2.2.6.2.1"></abs><ci id="S3.Ex5.m1.2.2.cmml" xref="S3.Ex5.m1.2.2">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.4c">\displaystyle~{}~{}~{}~{}~{}1\leq i\leq|h|,1\leq j\leq|r|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>adapted Levenshtein distance</figcaption>
</figure>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">With this approach, the given example target has 7 instead of 9 true positives if the slot tokens of the intents parameters with the slot tag depart_date.day_name parameter are changed (in this case both parameters are considered as substitutions in the Levenshtein distance).
We counted all true positives for the different classes over the evaluated dataset and divided the counted true positives by the reference lengths of all targets for the recall and by the hypothesis lengths for the precision (micro-averaging).
The CO-MC F<sub id="S3.SS5.p3.1.1" class="ltx_sub">1</sub>-score is more strict than the vanilla F<sub id="S3.SS5.p3.1.2" class="ltx_sub">1</sub>-score because of the consideration of the order.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">The metric intents accuracy considers all intents as whole.
That means the intents accuracy of one target is 100 % if the intents of the reference and the hypothesis are equivalent; otherwise, the intents accuracy is 0 %.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Evaluation schemes</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">We used for the evaluation of the models the model version of the epoch with the best CO-MC F<sub id="S3.SS6.p1.1.1" class="ltx_sub">1</sub>-score on the following validation datasets with only full utterances:
For the models trained with the datasets based on the <span id="S3.SS6.p1.1.2" class="ltx_text ltx_font_italic">cleaned full transcripts</span> dataset, we used the validation dataset of the <span id="S3.SS6.p1.1.3" class="ltx_text ltx_font_italic">cleaned full transcripts</span> dataset, for models trained with the datasets based on the <span id="S3.SS6.p1.1.4" class="ltx_text ltx_font_italic">human full transcripts</span> dataset, we used the validation dataset of the <span id="S3.SS6.p1.1.5" class="ltx_text ltx_font_italic">human full transcripts</span> dataset, and for models trained with the datasets based on the <span id="S3.SS6.p1.1.6" class="ltx_text ltx_font_italic">automatic incremental transcripts</span> dataset, we used the validation dataset of the <span id="S3.SS6.p1.1.7" class="ltx_text ltx_font_italic">automatic full transcribed</span> dataset.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.3" class="ltx_p">We evaluated our models with our evaluation metrics in the following manner:
First, we evaluated the models with partial utterances that contain the first 100 %, 75 %, 50 %, and 25 % of the tokens of the full utterances.
The number of tokens is rounded off to the next integer and this number is called <math id="S3.SS6.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><mi id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><ci id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">i</annotation></semantics></math> in the following.
For evaluating with the cleaned and the human transcribed utterances, we used the first <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">i</annotation></semantics></math> tokens of the full utterances.
For evaluating with automatically transcribed utterances, we used the first utterance in the <span id="S3.SS6.p2.3.1" class="ltx_text ltx_font_italic">automatic incremental transcripts</span> dataset of the corresponding utterance that was equal than or greater than <math id="S3.SS6.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS6.p2.3.m3.1a"><mi id="S3.SS6.p2.3.m3.1.1" xref="S3.SS6.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.3.m3.1b"><ci id="S3.SS6.p2.3.m3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.3.m3.1c">i</annotation></semantics></math>, because the ASR system did not produce partial utterances for all numbers less than the token length of the full utterance.
In the following, this evaluation scheme is called <span id="S3.SS6.p2.3.2" class="ltx_text ltx_font_italic">partial utterances processing</span>.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">In addition, we evaluated our models with the metric intents accuracy in the following manner:
We predicted the intents incrementally and aborted the incremental processing once a certain confidence for the intents prediction was reached.
We used 95 %, 90 %, 85 %, and 80 % as confidence thresholds.
When the target confidence was never reached, the full utterance was used to predict the intents, even if the confidence of the full utterance was under the confidence threshold.
We used for those experiments the partial utterances successively for the cleaned and human transcribed utterances and the partial utterances successively of the automatically transcribed utterances.
In the automatically transcribed utterances, the last transcript is the full utterance.
In the following, this evaluation scheme is called <span id="S3.SS6.p3.1.1" class="ltx_text ltx_font_italic">confidence based processing</span>.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">The models trained on the cleaned transcripts cannot be evaluated appropriately on the uncleaned transcripts, because the numbers are written in Arabic numerals in the cleaned transcripts and in words in the uncleaned transcripts.
The conversion is often ambiguous.
The same applies to the other direction.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>System Setup</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">We optimized the Transformer architecture for the validation dataset of the <span id="S3.SS7.p1.1.1" class="ltx_text ltx_font_italic">cleaned full transcripts</span> dataset.
The result of this optimization is a Transformer architecture with a model and inner size of 256, 4 layers, 4 heads, Adam <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib9" title="" class="ltx_ref">2015</a>)</cite> with the noam learning rate decay scheme (used by <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> as learning rate decay scheme) as optimization algorithm, a dropout of 40 %, an attention, embedding, and residual dropout of each 20 % and a label smoothing of 15 %.
We used 64 utterances as batch size.
The vocabulary of a trained model contains all words of the training dataset with which it was trained.
We trained the models for 100 epochs.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Partial utterances processing</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In Tables <a href="#S4.T1" title="Table 1 ‣ 4.2 Confidence based processing ‣ 4 Results ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <a href="#S4.T3" title="Table 3 ‣ 4.2 Confidence based processing ‣ 4 Results ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and <a href="#S4.T5" title="Table 5 ‣ 4.2 Confidence based processing ‣ 4 Results ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the CO-MC F<sub id="S4.SS1.p1.1.1" class="ltx_sub">1</sub>-scores and the intents accuracies are depicted for the evaluation scheme <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">partial hypothesis processing</span> for the cleaned, human transcribed, and automatically transcribed utterances respectively.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In the following, all percentage differences are absolute percentage differences.
The ranges refer to the smallest and biggest improvements on the CO-MC F<sub id="S4.SS1.p2.1.1" class="ltx_sub">1</sub>-score.
If no artificial noise is explicitly mentioned, the models without artificial noise are meant.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The models that were trained only with full utterances have better results evaluated on the full utterances than models trained with the partial and full utterances (in the range from 1.3 % to 3.24 %).
However, the models trained on the partial and full utterances have better results when they are evaluated on the first 75 % and 50 % of the tokens (in the range from 0.81 % to 4.39 %).
Evaluated on the utterances of the first 25 % of the tokens, there are even bigger improvements (in the range from 14.44 % to 47.91 %).
This means that our proposed training method improves the processing of partial utterances, especially if they are partial utterances produced incrementally at the beginning of the incremental processing of an utterance.
For an incremental processing capable NLU component, the best approach is to combine the two models.
The model trained on only full utterances is used for the full utterances and the model trained on the partial and full utterances is used for the incrementally produced partial utterances.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">With the combination described above, the performance of the models trained with the automatically transcribed utterances decreased less compared to the models trained on the human transcribed utterances, evaluated on the human transcribed utterances (in the range from 0.13 % to 2.01 %) than the models trained with the human transcribed utterances decreased compared to the models trained on the automatically transcribed utterances, evaluated on the automatically transcribed utterances (in the range from 1.22 % to 4 %).
In our experiments, the result was consequently that it is better to train on noisier data.
This is especially the case on evaluating the full utterances.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">We tried to simulate the noise of the automatically transcribed utterances with artificial noise.
We used again the same combination described above.
The performance of the models trained with the human transcribed utterances with artificial noise decreased less compared to the models trained on the human transcripts, evaluated on the human transcribed utterances (in the range from <span id="S4.SS1.p5.1.1" class="ltx_text">-1.43 %</span> to 2.5 %) than the models trained with the human transcribed utterances decreased compared to the human transcribed utterances with artificial noise, on the automatically transcribed utterances (in the range from -1.06 % to 5.21 %).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Confidence based processing</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In Tables <a href="#S4.T2" title="Table 2 ‣ 4.2 Confidence based processing ‣ 4 Results ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S4.T4" title="Table 4 ‣ 4.2 Confidence based processing ‣ 4 Results ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and <a href="#S4.T6" title="Table 6 ‣ 4.2 Confidence based processing ‣ 4 Results ‣ Incremental processing of noisy user utterances in the spoken language understanding task" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the intents accuracies and the needed percentage of tokens on average are depicted for the evaluation scheme <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">confidence based processing</span> for the cleaned, human transcribed, and automatically transcribed utterances respectively.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In the following, all percentage differences are absolute percentage differences.
The ranges refer to the smallest and biggest improvements on the intents accuracy metric.
If no artificial noise is explicitly mentioned, the models without artificial noise are meant.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The following statements apply to the incrementally trained models (the models trained only on the full utterances have only good results if they can use nearly the full utterances and therefore it makes no sense to use them for early predicting of intents).
It is better to train on the automatically transcribed utterances.
The decreasing is from 1.57 % to 2.58 % if they are evaluated on the human transcribed utterances, but they have an improvement from 2.58 % to 4.25 % if they are evaluated on the automatically transcribed utterances compared to the models trained on the human transcribed utterances.
The human transcribed utterances with artificial noise decrease by -1.46 % to 2.58 % if they are evaluated on the human transcribed utterances, but they have an improvement from 0.67 % to 3.69 % if they are evaluated on the automatically transcribed utterances compared to the models trained on the human transcribed utterances.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">training dataset</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 100 %</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 75 %</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 50 %</th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">first 25 %</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">cleaned, full</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">92.90 / 97.09</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">89.95 / 96.19</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">88.98 / 90.37</td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">49.36 / 49.05</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">cleaned, incremental</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">91.60 / 96.75</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">94.20 / 94.85</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r">93.37 / 92.05</td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_left">83.15 / 79.73</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">cleaned, incremental, art. noise</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">91.97 / 96.19</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">94.65 / 94.85</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r">93.22 / 91.83</td>
<td id="S4.T1.1.4.3.5" class="ltx_td ltx_align_left">81.75 / 78.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>CO-MC F<sub id="S4.T1.3.1" class="ltx_sub">1</sub>-scores / intents accuracies of the first 100 %, 75 %, 50 %, and 25 % of the tokens of the utterances of the test dataset of the cleaned human transcribed full utterances</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">training dataset</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">95 % conf.</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">90 % conf.</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">85 % conf.</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">80 % conf.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">cleaned, full</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">96.98 / 96.36</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">80.29 / 74.14</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">72.56 / 30.62</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">70.66 / 29.93</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">cleaned, incremental</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">96.42 / 97.27</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">95.97 / 92.16</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r">92.83 / 34.74</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_left">89.47 / 30.56</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">cleaned, incremental, art. noise</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">95.86 / 99.02</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">95.41 / 92.20</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r">91.71 / 33.12</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_left">86.56 / 24.54</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Intents accuracies / percentages of the used tokens for predicting the intents using the smallest partial utterance of the test dataset of the cleaned human transcribed incremental utterances for which the system has a confidence of more or equal than 95 %, 90 %, 85 %, and 80 %, if the confidence is not reached, the full utterance is used</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">training dataset</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 100 %</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 75 %</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 50 %</th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">first 25 %</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">human, full</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">90.44 / 94.85</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">87.91 / 94.51</td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">87.75 / 89.14</td>
<td id="S4.T3.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">34.86 / 48.38</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">human, full, art. noise</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">87.94 / 95.30</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">85.77 / 94.74</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r">89.51 / 91.27</td>
<td id="S4.T3.1.3.2.5" class="ltx_td ltx_align_left">67.71 / 68.65</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">88.57 / 94.40</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">90.58 / 93.62</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r">91.51 / 90.59</td>
<td id="S4.T3.1.4.3.5" class="ltx_td ltx_align_left">82.77 / 79.06</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental, art. noise</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">88.24 / 95.41</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">90.71 / 94.18</td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r">92.94 / 91.83</td>
<td id="S4.T3.1.5.4.5" class="ltx_td ltx_align_left">84.14 / 79.17</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<td id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">automatic, full</td>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">88.43 / 93.39</td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r">86.18 / 93.62</td>
<td id="S4.T3.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r">89.24 / 90.37</td>
<td id="S4.T3.1.6.5.5" class="ltx_td ltx_align_left">56.80 / 70.66</td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<td id="S4.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">automatic, incremental</td>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">86.38 / 92.72</td>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r">89.56 / 93.73</td>
<td id="S4.T3.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r">90.05 / 89.03</td>
<td id="S4.T3.1.7.6.5" class="ltx_td ltx_align_left">82.64 / 79.17</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>CO-MC F<sub id="S4.T3.3.1" class="ltx_sub">1</sub>-scores / intents accuracies of the first 100 %, 75 %, 50 %, and 25 % of the tokens of the utterances of the test dataset of the human transcribed full utterances</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">training dataset</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">95 % conf.</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">90 % conf.</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">85 % conf.</th>
<th id="S4.T4.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">80 % conf.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">human, full</td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">94.51 / 96.51</td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">90.82 / 85.59</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">77.60 / 32.52</td>
<td id="S4.T4.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">76.37 / 30.22</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">human, full, art. noise</td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">95.41 / 96.33</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">90.71 / 81.50</td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r">77.72 / 30.58</td>
<td id="S4.T4.1.3.2.5" class="ltx_td ltx_align_left">76.04 / 28.13</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<td id="S4.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental</td>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">94.18 / 99.10</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">93.95 / 89.53</td>
<td id="S4.T4.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r">90.59 / 32.83</td>
<td id="S4.T4.1.4.3.5" class="ltx_td ltx_align_left">88.47 / 27.04</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<td id="S4.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental, art. noise</td>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">95.18 / 97.85</td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">95.41 / 92.65</td>
<td id="S4.T4.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r">91.60 / 32.78</td>
<td id="S4.T4.1.5.4.5" class="ltx_td ltx_align_left">85.89 / 24.90</td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<td id="S4.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">automatic, full</td>
<td id="S4.T4.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">88.58 / 91.51</td>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r">88.35 / 83.19</td>
<td id="S4.T4.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r">76.82 / 30.85</td>
<td id="S4.T4.1.6.5.5" class="ltx_td ltx_align_left">75.36 / 28.79</td>
</tr>
<tr id="S4.T4.1.7.6" class="ltx_tr">
<td id="S4.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">automatic, incremental</td>
<td id="S4.T4.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">92.61 / 99.63</td>
<td id="S4.T4.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r">92.16 / 93.38</td>
<td id="S4.T4.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r">88.35 / 35.41</td>
<td id="S4.T4.1.7.6.5" class="ltx_td ltx_align_left">85.89 / 30.33</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Intents accuracies / percentages of the used tokens for predicting the intents using the smallest partial utterance of the test dataset of the human transcribed incremental utterances for which the system has a confidence of more or equal than 95 %, 90 %, 85 %, and 80 %, if the confidence is not reached, the full utterance is used</figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">training dataset</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 100 %</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 75 %</th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">first 50 %</th>
<th id="S4.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">first 25 %</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">human, full</td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">83.87 / 91.49</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">80.26 / 91.04</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">83.13 / 85.78</td>
<td id="S4.T5.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">42.06 / 51.74</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">human, full, art. noise</td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">81.93 / 91.15</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">78.94 / 90.71</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r">83.76 / 88.35</td>
<td id="S4.T5.1.3.2.5" class="ltx_td ltx_align_left">74.98 / 68.31</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental</td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">80.63 / 87.91</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">82.16 / 88.24</td>
<td id="S4.T5.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r">85.85 / 85.44</td>
<td id="S4.T5.1.4.3.5" class="ltx_td ltx_align_left">80.49 / 75.93</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<td id="S4.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental, art. noise</td>
<td id="S4.T5.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">82.93 / 91.04</td>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">83.16 / 89.70</td>
<td id="S4.T5.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r">88.13 / 88.02</td>
<td id="S4.T5.1.5.4.5" class="ltx_td ltx_align_left">83.75 / 77.27</td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<td id="S4.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">automatic, full</td>
<td id="S4.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">87.14 / 93.39</td>
<td id="S4.T5.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r">82.06 / 92.61</td>
<td id="S4.T5.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r">85.15 / 90.03</td>
<td id="S4.T5.1.6.5.5" class="ltx_td ltx_align_left">70.05 / 72.45</td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<td id="S4.T5.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">automatic, incremental</td>
<td id="S4.T5.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">84.62 / 92.27</td>
<td id="S4.T5.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r">84.90 / 91.71</td>
<td id="S4.T5.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r">87.07 / 88.35</td>
<td id="S4.T5.1.7.6.5" class="ltx_td ltx_align_left">84.49 / 79.73</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>CO-MC F<sub id="S4.T5.3.1" class="ltx_sub">1</sub>-scores / intents accuracies of the first partial automatically transcribed utterances that have equal or more than the first 100 %, 75 %, 50 %, and 25 % of the tokens of the utterances of the test dataset of the automatically transcribed full utterances</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">training dataset</th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">95 % conf.</th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">90 % conf.</th>
<th id="S4.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">85 % conf.</th>
<th id="S4.T6.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">80 % conf.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<td id="S4.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">human, full</td>
<td id="S4.T6.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">91.04 / 97.63</td>
<td id="S4.T6.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">87.46 / 88.24</td>
<td id="S4.T6.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">78.84 / 41.52</td>
<td id="S4.T6.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">76.82 / 38.41</td>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<td id="S4.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">human, full, art. noise</td>
<td id="S4.T6.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">90.93 / 96.87</td>
<td id="S4.T6.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">86.79 / 85.60</td>
<td id="S4.T6.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r">77.94 / 37.99</td>
<td id="S4.T6.1.3.2.5" class="ltx_td ltx_align_left">76.48 / 34.93</td>
</tr>
<tr id="S4.T6.1.4.3" class="ltx_tr">
<td id="S4.T6.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental</td>
<td id="S4.T6.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">87.68 / 99.18</td>
<td id="S4.T6.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">87.35 / 91.23</td>
<td id="S4.T6.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r">86.56 / 42.35</td>
<td id="S4.T6.1.4.3.5" class="ltx_td ltx_align_left">83.99 / 36.96</td>
</tr>
<tr id="S4.T6.1.5.4" class="ltx_tr">
<td id="S4.T6.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">human, incremental, art. noise</td>
<td id="S4.T6.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">90.59 / 98.29</td>
<td id="S4.T6.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">90.37 / 93.58</td>
<td id="S4.T6.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r">88.47 / 40.33</td>
<td id="S4.T6.1.5.4.5" class="ltx_td ltx_align_left">82.98 / 31.53</td>
</tr>
<tr id="S4.T6.1.6.5" class="ltx_tr">
<td id="S4.T6.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">automatic, full</td>
<td id="S4.T6.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">88.24 / 93.41</td>
<td id="S4.T6.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r">87.91 / 86.65</td>
<td id="S4.T6.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r">80.40 / 38.74</td>
<td id="S4.T6.1.6.5.5" class="ltx_td ltx_align_left">78.50 / 35.70</td>
</tr>
<tr id="S4.T6.1.7.6" class="ltx_tr">
<td id="S4.T6.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">automatic, incremental</td>
<td id="S4.T6.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">91.83 / 99.16</td>
<td id="S4.T6.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r">91.60 / 94.29</td>
<td id="S4.T6.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r">89.14 / 39.67</td>
<td id="S4.T6.1.7.6.5" class="ltx_td ltx_align_left">86.67 / 34.54</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Intents accuracies / percentages of the used tokens for predicting the intents using the first partial utterance of the test dataset of the automatically transcribed incremental utterances for which the system has a confidence of more or equal than 95 %, 90 %, 85 %, and 80 %, if the confidence is not reached, the full utterance is used</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Computation time</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Since the partial utterances are fed successively in the Transformer architecture, the computation must be fast enough for the system to work off all partial utterances without latency.
On a notebook with an Intel Core i5-8250U CPU - all computations were done only on the CPU and we limited the usage to one thread (with the app taskset) so other component like the ASR system can run on the same system - it took 310 milliseconds to compute the longest utterance (46 tokens) of the cleaned utterances and 293 milliseconds to compute the utterance (38 tokens) with the longest target sequence (41 tokens - one intent with 17 parameters) of the cleaned utterances.
We processed continually both utterances for 15 minutes and selected for both utterances the run with the maximum computation time.
The model was the model trained with the cleaned full utterances.
This means that it is possible to process an utterance after every word because a normal user needs on average more than these measured times to utter a word or type a word with a keyboard.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Further Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we report that the best approach for an incremental processing capable NLU component is to mix models.
A model trained on partial and full utterances should be used for processing partial utterances and a model trained only on full utterances for processing full utterances.
In particular, the improvements are for the first incrementally produced utterances, which contain only a small number of tokens, high if the model is not only trained on full utterances.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Evaluated on the noisy human and even noisier automatically transcribed utterances, we got better results with the models trained with the human transcribed utterances with artificial noise and the models trained with the automatically transcribed utterances.
This is especially the case when evaluating the full utterances.
A reason for this could be that the partial utterances can be already considered as noisier utterances.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The short computation time of the processing of an utterance makes it possible to use the incremental processing for spoken and written utterances.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In future work, it has to be evaluated whether our results are also valid for other architectures and other datasets.
A balanced version of the ATIS datasets can also be seen as another dataset.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">We got better performance with artificial noise.
However, the results could be improved by optimizing the hyperparameter of the artificial noise generator.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">In this work, we researched the performance using incremental utterances.
There should be research on how the results of the incremental processing can be separated into subactions and how much this can accelerate the processing of actions in real-world scenarios.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">In future work not only the acceleration, but also other benefits of the incremental processing, like using semantic information for improving the backchannel, could be researched.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work has been conducted in the SecondHands project which has received funding from the European Union’s Horizon 2020 Research and Innovation programme (call:H2020- ICT-2014-1, RIA) under grant agreement No 643950.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asfour et al. (2018)</span>
<span class="ltx_bibblock">
Tamim Asfour, Lukas Kaul, Mirko Wächter, Simon Ottenhaus, Pascal Weiner,
Samuel Rader, Raphael Grimm, You Zhou, Markus Grotz, Fabian Paus, Dmitriy
Shingarey, and Hans Haubert. 2018.

</span>
<span class="ltx_bibblock">Armar-6: A collaborative humanoid robot for industrial environments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE/RAS International Conference on Humanoid Robots
(Humanoids)</em>, pages 447–454.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2015)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third International Conference on
Learning Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berger et al. (1996)</span>
<span class="ltx_bibblock">
Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996.

</span>
<span class="ltx_bibblock">A maximum entropy approach to natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Comput. Linguist.</em>, 22(1):39–71.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bird et al. (2009)</span>
<span class="ltx_bibblock">
Steven Bird, Ewan Klein, and Edward Loper. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Natural Language Processing with Python</em>, 1st edition.

</span>
<span class="ltx_bibblock">O’Reilly Media, Inc.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Constantin et al. (2019)</span>
<span class="ltx_bibblock">
Stefan Constantin, Jan Niehues, and Alex Waibel. 2019.

</span>
<span class="ltx_bibblock">Multi-task learning to improve natural language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Workshop on Spoken
Dialogue Systems (IWSDS)</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeVault et al. (2009)</span>
<span class="ltx_bibblock">
David DeVault, Kenji Sagae, and David Traum. 2009.

</span>
<span class="ltx_bibblock">Can I finish? learning when to respond to incremental
interpretation results in interactive dialogue.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the SIGDIAL 2009 Conference</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gambino et al. (2018)</span>
<span class="ltx_bibblock">
Soledad López Gambino, Sina Zarrieß, Casey Kennington, and David
Schlangen. 2018.

</span>
<span class="ltx_bibblock">Learning to buy time: A data-driven model for avoiding silence while
task-related information cannot yet be presented.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of 22nd Workshop on the Semantics and Pragmatics
of Dialogue (Semdial)</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hakkani-Tur et al. (2016)</span>
<span class="ltx_bibblock">
Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao,
Li Deng, and Ye-Yi Wang. 2016.

</span>
<span class="ltx_bibblock">Multi-domain joint semantic frame parsing using bi-directional
rnn-lstm.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Annual Meeting of the International
Speech Communication Association (Interspeech)</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock">Adam : A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third International Conference on
Learning Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein (1966)</span>
<span class="ltx_bibblock">
Vladimir I Levenshtein. 1966.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Soviet physics doklady</em>, 10(8):707–710.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2017)</span>
<span class="ltx_bibblock">
Thai-Son Nguyen, Markus Müller, Matthias Sperber, Thomas Zenkel, Sebastian
Stüker, and Alex Waibel. 2017.

</span>
<span class="ltx_bibblock">The 2017 kit iwslt speech-to-text systems for english and german.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th International Workshop on Spoken
Language Translation (IWSLT)</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2018)</span>
<span class="ltx_bibblock">
Thai Son Nguyen, Matthias Sperber, Sebastian Stüker, and Alex Waibel. 2018.

</span>
<span class="ltx_bibblock">Building real-time speech recognition without cmvn.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">20th International Conference on Speech and Computer
(SPECOM)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niehues et al. (2018)</span>
<span class="ltx_bibblock">
Jan Niehues, Ngoc-Quan Pham, Thanh-Le Ha, Matthias Sperber, and Alex Waibel.
2018.

</span>
<span class="ltx_bibblock">Low-latency neural speech translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th Annual Meeting of the International
Speech Communication Association (Interspeech)</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et al. (2019)</span>
<span class="ltx_bibblock">
Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus Müller, and Alex
Waibel. 2019.

</span>
<span class="ltx_bibblock">Very deep self-attention networks for end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th Annual Meeting of the International
Speech Communication Association (Interspeech)</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rüde et al. (2017)</span>
<span class="ltx_bibblock">
Robin Rüde, Markus Müller, Sebastian Stüker, and Alex Waibel. 2017.

</span>
<span class="ltx_bibblock">Enhancing backchannel prediction using word embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th Annual Meeting of the International
Speech Communication Association (Interspeech)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sang and Veenstra (1999)</span>
<span class="ltx_bibblock">
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999.

</span>
<span class="ltx_bibblock">Representing text chunks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Ninth Conference of the European Chapter
of the Association for Computational Linguistics (EACL)</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Senellart (2017)</span>
<span class="ltx_bibblock">
Jean Senellart. 2017.

</span>
<span class="ltx_bibblock">English chatbot model with opennmt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://forum.opennmt.net/t/english-chatbot-model-with-opennmt/184" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://forum.opennmt.net/t/english-chatbot-model-with-opennmt/184</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sperber et al. (2017)</span>
<span class="ltx_bibblock">
Matthias Sperber, Jan Niehues, and Alex Waibel. 2017.

</span>
<span class="ltx_bibblock">Toward robust neural machine translation for noisy input sequences.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th International Workshop on Spoken
Language Translation (IWSLT)</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2014)</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and
Ruslan Salakhutdinov. 2014.

</span>
<span class="ltx_bibblock">Dropout: a simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 15(1):1929–1958.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2009)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2009.

</span>
<span class="ltx_bibblock">News from OPUS - A collection of multilingual parallel corpora
with tools and interfaces.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Recent Advances in Natural Language Processing</em>, volume V,
pages 237–248. John Benjamins, Amsterdam/Philadelphia.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30</em>, pages
5998–6008. Curran Associates, Inc.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Supplemental Material</h2>

<figure id="A1.T7" class="ltx_table">
<table id="A1.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.1.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">train</td>
<td id="A1.T7.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r">valid</td>
<td id="A1.T7.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r">test</td>
<td id="A1.T7.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.1.1.4.1.1" class="ltx_p" style="width:96.7pt;">intent(s)</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.2.2" class="ltx_tr">
<td id="A1.T7.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">73.89</td>
<td id="A1.T7.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">71.4</td>
<td id="A1.T7.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">70.77</td>
<td id="A1.T7.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.2.2.4.1.1" class="ltx_p" style="width:96.7pt;">atis_flight</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.3.3" class="ltx_tr">
<td id="A1.T7.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r">8.6</td>
<td id="A1.T7.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r">7.6</td>
<td id="A1.T7.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r">5.38</td>
<td id="A1.T7.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.3.3.4.1.1" class="ltx_p" style="width:96.7pt;">atis_airfare</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.4.4" class="ltx_tr">
<td id="A1.T7.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r">5.14</td>
<td id="A1.T7.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r">5.0</td>
<td id="A1.T7.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r">4.03</td>
<td id="A1.T7.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.4.4.4.1.1" class="ltx_p" style="width:96.7pt;">atis_ground_service</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.5.5" class="ltx_tr">
<td id="A1.T7.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r">3.1</td>
<td id="A1.T7.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r">3.6</td>
<td id="A1.T7.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r">4.26</td>
<td id="A1.T7.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.5.5.4.1.1" class="ltx_p" style="width:96.7pt;">atis_airline</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.6.6" class="ltx_tr">
<td id="A1.T7.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r">2.9</td>
<td id="A1.T7.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r">3.4</td>
<td id="A1.T7.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r">3.7</td>
<td id="A1.T7.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.6.6.4.1.1" class="ltx_p" style="width:96.7pt;">atis_abbreviation</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.7.7" class="ltx_tr">
<td id="A1.T7.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r">1.56</td>
<td id="A1.T7.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r">2.2</td>
<td id="A1.T7.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r">1.01</td>
<td id="A1.T7.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.7.7.4.1.1" class="ltx_p" style="width:96.7pt;">atis_aircraft</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.8.8" class="ltx_tr">
<td id="A1.T7.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r">1.0</td>
<td id="A1.T7.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r">1.8</td>
<td id="A1.T7.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r">0.11</td>
<td id="A1.T7.1.8.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.8.8.4.1.1" class="ltx_p" style="width:96.7pt;">atis_flight_time</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.9.9" class="ltx_tr">
<td id="A1.T7.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r">0.92</td>
<td id="A1.T7.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r">2.0</td>
<td id="A1.T7.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r">0.34</td>
<td id="A1.T7.1.9.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.9.9.4.1.1" class="ltx_p" style="width:96.7pt;">atis_quantity</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.10.10" class="ltx_tr">
<td id="A1.T7.1.10.10.1" class="ltx_td ltx_align_left ltx_border_r">0.42</td>
<td id="A1.T7.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r">0.4</td>
<td id="A1.T7.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r">1.34</td>
<td id="A1.T7.1.10.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.10.10.4.1.1" class="ltx_p" style="width:96.7pt;">atis_flight#atis_airfare</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.11.11" class="ltx_tr">
<td id="A1.T7.1.11.11.1" class="ltx_td ltx_align_left ltx_border_r">0.4</td>
<td id="A1.T7.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r">0.2</td>
<td id="A1.T7.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r">0.67</td>
<td id="A1.T7.1.11.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.11.11.4.1.1" class="ltx_p" style="width:96.7pt;">atis_city</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.12.12" class="ltx_tr">
<td id="A1.T7.1.12.12.1" class="ltx_td ltx_align_left ltx_border_r">0.38</td>
<td id="A1.T7.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r">0.6</td>
<td id="A1.T7.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r">1.12</td>
<td id="A1.T7.1.12.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.12.12.4.1.1" class="ltx_p" style="width:96.7pt;">atis_distance</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.13.13" class="ltx_tr">
<td id="A1.T7.1.13.13.1" class="ltx_td ltx_align_left ltx_border_r">0.38</td>
<td id="A1.T7.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r">0.6</td>
<td id="A1.T7.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r">2.02</td>
<td id="A1.T7.1.13.13.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.13.13.4.1.1" class="ltx_p" style="width:96.7pt;">atis_airport</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.14.14" class="ltx_tr">
<td id="A1.T7.1.14.14.1" class="ltx_td ltx_align_left ltx_border_r">0.33</td>
<td id="A1.T7.1.14.14.2" class="ltx_td ltx_align_left ltx_border_r">0.6</td>
<td id="A1.T7.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r">0.78</td>
<td id="A1.T7.1.14.14.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.14.14.4.1.1" class="ltx_p" style="width:96.7pt;">atis_ground_fare</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.15.15" class="ltx_tr">
<td id="A1.T7.1.15.15.1" class="ltx_td ltx_align_left ltx_border_r">0.33</td>
<td id="A1.T7.1.15.15.2" class="ltx_td ltx_align_left ltx_border_r">0.2</td>
<td id="A1.T7.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r">2.35</td>
<td id="A1.T7.1.15.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.15.15.4.1.1" class="ltx_p" style="width:96.7pt;">atis_capacity</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.16.16" class="ltx_tr">
<td id="A1.T7.1.16.16.1" class="ltx_td ltx_align_left ltx_border_r">0.27</td>
<td id="A1.T7.1.16.16.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.16.16.3" class="ltx_td ltx_align_left ltx_border_r">8</td>
<td id="A1.T7.1.16.16.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.16.16.4.1.1" class="ltx_p" style="width:96.7pt;">atis_flight_no</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.17.17" class="ltx_tr">
<td id="A1.T7.1.17.17.1" class="ltx_td ltx_align_left ltx_border_r">0.13</td>
<td id="A1.T7.1.17.17.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.17.17.3" class="ltx_td ltx_align_left ltx_border_r">0.67</td>
<td id="A1.T7.1.17.17.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.17.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.17.17.4.1.1" class="ltx_p" style="width:96.7pt;">atis_meal</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.18.18" class="ltx_tr">
<td id="A1.T7.1.18.18.1" class="ltx_td ltx_align_left ltx_border_r">0.11</td>
<td id="A1.T7.1.18.18.2" class="ltx_td ltx_align_left ltx_border_r">0.2</td>
<td id="A1.T7.1.18.18.3" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.18.18.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.18.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.18.18.4.1.1" class="ltx_p" style="width:96.7pt;">atis_restriction</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.19.19" class="ltx_tr">
<td id="A1.T7.1.19.19.1" class="ltx_td ltx_align_left ltx_border_r">0.04</td>
<td id="A1.T7.1.19.19.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.19.19.3" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.19.19.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.19.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.19.19.4.1.1" class="ltx_p" style="width:96.7pt;">atis_airline# atis_flight_no</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.20.20" class="ltx_tr">
<td id="A1.T7.1.20.20.1" class="ltx_td ltx_align_left ltx_border_r">0.02</td>
<td id="A1.T7.1.20.20.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.20.20.3" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.20.20.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.20.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.20.20.4.1.1" class="ltx_p" style="width:96.7pt;">atis_ground_service# atis_ground_fare</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.21.21" class="ltx_tr">
<td id="A1.T7.1.21.21.1" class="ltx_td ltx_align_left ltx_border_r">0.02</td>
<td id="A1.T7.1.21.21.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.21.21.3" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.21.21.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.21.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.21.21.4.1.1" class="ltx_p" style="width:96.7pt;">atis_cheapest</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.22.22" class="ltx_tr">
<td id="A1.T7.1.22.22.1" class="ltx_td ltx_align_left ltx_border_r">0.02</td>
<td id="A1.T7.1.22.22.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.22.22.3" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.22.22.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.22.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.22.22.4.1.1" class="ltx_p" style="width:96.7pt;">atis_aircraft#atis_flight# atis_flight_no</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.23.23" class="ltx_tr">
<td id="A1.T7.1.23.23.1" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.23.23.2" class="ltx_td ltx_align_left ltx_border_r">0.2</td>
<td id="A1.T7.1.23.23.3" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.23.23.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.23.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.23.23.4.1.1" class="ltx_p" style="width:96.7pt;">atis_airfare# atis_flight_time</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.24.24" class="ltx_tr">
<td id="A1.T7.1.24.24.1" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.24.24.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.24.24.3" class="ltx_td ltx_align_left ltx_border_r">0.22</td>
<td id="A1.T7.1.24.24.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.24.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.24.24.4.1.1" class="ltx_p" style="width:96.7pt;">atis_day_name</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.25.25" class="ltx_tr">
<td id="A1.T7.1.25.25.1" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.25.25.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.25.25.3" class="ltx_td ltx_align_left ltx_border_r">0.11</td>
<td id="A1.T7.1.25.25.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.25.25.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.25.25.4.1.1" class="ltx_p" style="width:96.7pt;">atis_flight#atis_airline</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.26.26" class="ltx_tr">
<td id="A1.T7.1.26.26.1" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.26.26.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.26.26.3" class="ltx_td ltx_align_left ltx_border_r">0.11</td>
<td id="A1.T7.1.26.26.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.26.26.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.26.26.4.1.1" class="ltx_p" style="width:96.7pt;">atis_airfare#atis_flight</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.27.27" class="ltx_tr">
<td id="A1.T7.1.27.27.1" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.27.27.2" class="ltx_td ltx_align_left ltx_border_r">0</td>
<td id="A1.T7.1.27.27.3" class="ltx_td ltx_align_left ltx_border_r">0.11</td>
<td id="A1.T7.1.27.27.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T7.1.27.27.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.27.27.4.1.1" class="ltx_p" style="width:96.7pt;">atis_flight_no# atis_airline</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>intents distribution (in percent) of the ATIS utterances used in this work</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1909.13789" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1909.13790" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1909.13790">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1909.13790" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1909.13791" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 09:19:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
