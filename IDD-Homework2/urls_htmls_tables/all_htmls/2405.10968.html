<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.10968] 1 Introduction</title><meta property="og:description" content="Federated Learning (FL) typically involves a large-scale, distributed system with individual user devices/servers training models locally and then aggregating their model updates on a trusted central server.
Existing s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Introduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.10968">

<!--Generated on Wed Jun  5 16:31:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">marginparsep has been altered.
<br class="ltx_break">topmargin has been altered.
<br class="ltx_break">marginparwidth has been altered.
<br class="ltx_break">marginparpush has been altered.
<br class="ltx_break"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">The page layout violates the ICML style.</span>
Please do not change the page layout, or include packages like geometry,
savetrees, or fullpage, which change it for you.
We’re not able to reliably undo arbitrary changes to the style. Please remove
the offending package(s), or layout-changing commands and try again.</p>
</div>
<div id="p3" class="ltx_para ltx_align_center">
<p id="p3.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p4" class="ltx_para ltx_align_center">
<p id="p4.1" class="ltx_p"><span id="p4.1.1" class="ltx_text ltx_font_smallcaps">LIFL: A Lightweight, Event-driven Serverless Platform for Federated Learning</span></p>
</div>
<div id="p5" class="ltx_para ltx_align_center">
<p id="p5.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p ltx_align_center"><span id="p6.1.1" class="ltx_text ltx_font_bold">Anonymous Authors</span><sup id="p6.1.2" class="ltx_sup">1 </sup></p>
</div>
<div id="p7" class="ltx_para ltx_noindent">
<br class="ltx_break">
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL) typically involves a large-scale, distributed system with individual user devices/servers training models locally and then aggregating their model updates on a trusted central server.
Existing systems for FL often use an always-on server for model aggregation, which can be inefficient in terms of resource utilization. They may also be inelastic in their resource management. This is particularly exacerbated when aggregating model updates at scale in a highly dynamic environment with varying numbers of heterogeneous user devices/servers.</p>
<p id="id2.id2" class="ltx_p">We present LIFL, a lightweight and elastic serverless cloud platform with fine-grained resource management for efficient FL aggregation at scale. LIFL is enhanced by a streamlined, event-driven serverless design that eliminates the individual heavy-weight message broker and replaces inefficient container-based sidecars with lightweight eBPF-based proxies. We leverage shared memory processing to achieve high-performance communication for hierarchical aggregation, which is commonly adopted to speed up FL aggregation at scale. We further introduce locality-aware placement in LIFL to maximize the benefits of shared memory processing. LIFL precisely scales and carefully reuses the resources for hierarchical aggregation to achieve the highest degree of parallelism while minimizing the aggregation time and resource consumption. Our experimental results show that LIFL achieves significant improvement in resource efficiency and aggregation speed for supporting FL at scale, compared to existing serverful and serverless FL systems.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>
<sup id="footnotex1.1" class="ltx_sup">1</sup>Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country.
Correspondence to: Anonymous Author &lt;anon.email@domain.com&gt;.
 
<br class="ltx_break">Preliminary work. Under review by the
Machine Learning and Systems (MLSys) Conference. Do not distribute.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>) enables collaborative model training across a network of decentralized devices/machines while keeping individual user data secure and private. In FL, instead of sending raw data to a central server, models are trained on individual devices/machines using local data, and only the model updates are shared and aggregated to create a global model.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To support FL at scale, <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">hierarchical aggregation</span> is often adopted to increase the service capacity for model aggregation <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Jayaram et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022b</a>)</cite>. This can accommodate a large number of clients and handle a substantial volume of model updates, avoiding potential slow-down of the aggregation process. In the process, each level performs intermediate aggregation, combining the updates from lower-level aggregators or clients.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Existing FL frameworks (<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">e.g., </span>Google’s FL stack <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, Meta’s PAPAYA <cite class="ltx_cite ltx_citemacro_cite">Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>) adopt a static, always-on<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>meaning that aggregators are up all the time within a round.</span></span></span> deployment to support model aggregation.
However, in a dynamic FL environment, it’s difficult to have a one-size-fits-all service capacity for model aggregation. System heterogeneity (<span id="S1.p3.1.2" class="ltx_text ltx_font_italic">i.e., </span>different hardware capabilities) and a dynamically varying number of participating clients in each round
require frequent adjustments of the capacity so that the aggregation service
effectively uses resources on demand and avoids significant resource wastage.
</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Serverless computing promises to provide an event-driven, resource-efficient cloud computing environment, enabling services to use resources on demand <cite class="ltx_cite ltx_citemacro_cite">Shahrad et al. (<a href="#bib.bib57" title="" class="ltx_ref">2020a</a>)</cite>.
Running FL model aggregation service as serverless functions can right-size the provisioned resources and reduce resource waste compared to an always-on aggregation server implementation. In addition, stateless processing by serverless functions makes it easy to support continual updates to the aggregation hierarchy.
By increasing the capacity of aggregation through a hierarchy of serverless aggregators, model aggregation in FL can be executed in parallel, responding to increasing loads from trainer model updates.
</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">However, the excessive overhead in current serverless frameworks, caused by the loose coupling of data plane components <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, is a barrier to achieving efficient and timely aggregation, compared to a monolithic serverful design. Further, the use of individual, constantly-running components (e.g., container-based sidecars) in current serverless frameworks is inefficient and sacrifices much of the benefit of serverless computing.
This prompts us to create a more streamlined, responsive serverless framework that is tailored to achieve just-in-time FL aggregation on demand.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">We introduce LIFL, a lightweight serverless platform for FL that uses hierarchical aggregation to achieve parallelism in aggregation and exploits intra-node shared memory processing to reduce data plane overheads.
LIFL also utilizes a locality-aware placement policy to maximize the benefits of the intra-node shared memory data plane. Unlike typical serverless platforms that use a heavyweight sidecar implemented as a separate container, LIFL seeks to eliminate this wasteful overhead by taking advantage of eBPF-based event-driven processing. This ensures that resource usage is truly load-proportional.
Instead of depending on inaccurate, threshold-based autoscaling, LIFL uses hierarchy-aware autoscaling to precisely adjust the capacity of model aggregation
to match the incoming load.
We also use a policy of reusing runtimes
to sidestep the impact of startup delay on the model convergence time, while also improving resource efficiency of aggregation.
LIFL favors eager aggregation to enable timely aggregation, reducing the queuing time for model updates. By harnessing the capabilities of LIFL, FL systems can achieve efficient resource utilization and reduced aggregation time.
LIFL is available at <cite class="ltx_cite ltx_citemacro_cite">fla (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">We highlight the contributions of LIFL below:</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.2" class="ltx_p"><span id="S1.p8.2.1" class="ltx_text ltx_font_bold">(1)</span>
LIFL’s enhanced data plane achieves 3<math id="S1.p8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><times id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\times</annotation></semantics></math> (compared to serverful) and 5.8<math id="S1.p8.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><times id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\times</annotation></semantics></math> (compared to serverless) latency reduction on transferring a relatively heavyweight ResNet-152 model update within the aggregation hierarchy (intra-node).</p>
</div>
<div id="S1.p9" class="ltx_para ltx_noindent">
<p id="S1.p9.3" class="ltx_p"><span id="S1.p9.3.1" class="ltx_text ltx_font_bold">(2)</span>
LIFL’s locality-aware placement can maximize shared memory processing, achieving up to 2.1<math id="S1.p9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p9.1.m1.1a"><mo id="S1.p9.1.m1.1.1" xref="S1.p9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p9.1.m1.1b"><times id="S1.p9.1.m1.1.1.cmml" xref="S1.p9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.1.m1.1c">\times</annotation></semantics></math> additional latency reduction on aggregating a batch of updates in a round (details in §<a href="#S6" title="6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).
After applying hierarchy-planning, aggregator reuse, and eager aggregation, LIFL can further obtain 1.5<math id="S1.p9.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p9.2.m2.1a"><mo id="S1.p9.2.m2.1.1" xref="S1.p9.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p9.2.m2.1b"><times id="S1.p9.2.m2.1.1.cmml" xref="S1.p9.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.2.m2.1c">\times</annotation></semantics></math> latency reduction.
The enhanced orchestration also helps improve efficiency, saving up to 2<math id="S1.p9.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p9.3.m3.1a"><mo id="S1.p9.3.m3.1.1" xref="S1.p9.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p9.3.m3.1b"><times id="S1.p9.3.m3.1.1.cmml" xref="S1.p9.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.3.m3.1c">\times</annotation></semantics></math> CPU consumption compared to simply using the enhanced data plane.</p>
</div>
<div id="S1.p10" class="ltx_para ltx_noindent">
<p id="S1.p10.6" class="ltx_p"><span id="S1.p10.6.5" class="ltx_text ltx_font_bold">(3)</span>
Our evaluation with a real FL workload using ResNet-18 and 120 simultaneous active clients (the total number of clients used is 2,800) shows that the combination of LIFL’s enhanced data and control planes achieve <span id="S1.p10.1.1" class="ltx_text ltx_font_bold">5<math id="S1.p10.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p10.1.1.m1.1a"><mo id="S1.p10.1.1.m1.1.1" xref="S1.p10.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p10.1.1.m1.1b"><times id="S1.p10.1.1.m1.1.1.cmml" xref="S1.p10.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.1.1.m1.1c">\times</annotation></semantics></math></span> and <span id="S1.p10.2.2" class="ltx_text ltx_font_bold">1.8<math id="S1.p10.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p10.2.2.m1.1a"><mo id="S1.p10.2.2.m1.1.1" xref="S1.p10.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p10.2.2.m1.1b"><times id="S1.p10.2.2.m1.1.1.cmml" xref="S1.p10.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.2.2.m1.1c">\times</annotation></semantics></math></span> less CPU cost and reduces <span id="S1.p10.3.3" class="ltx_text ltx_font_bold">2.7<math id="S1.p10.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p10.3.3.m1.1a"><mo id="S1.p10.3.3.m1.1.1" xref="S1.p10.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p10.3.3.m1.1b"><times id="S1.p10.3.3.m1.1.1.cmml" xref="S1.p10.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.3.3.m1.1c">\times</annotation></semantics></math></span> and <span id="S1.p10.4.4" class="ltx_text ltx_font_bold">1.6<math id="S1.p10.4.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p10.4.4.m1.1a"><mo id="S1.p10.4.4.m1.1.1" xref="S1.p10.4.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p10.4.4.m1.1b"><times id="S1.p10.4.4.m1.1.1.cmml" xref="S1.p10.4.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.4.4.m1.1c">\times</annotation></semantics></math></span> on time-to-accuracy (70% accuracy level), compared to existing serverless and even serverful FL systems. We also train a relatively heavyweight ResNet-152 model. LIFL spends 1.68<math id="S1.p10.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p10.5.m1.1a"><mo id="S1.p10.5.m1.1.1" xref="S1.p10.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p10.5.m1.1b"><times id="S1.p10.5.m1.1.1.cmml" xref="S1.p10.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.5.m1.1c">\times</annotation></semantics></math> less time to reach 70% accuracy than existing serverless FL systems, while using 4.23<math id="S1.p10.6.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p10.6.m2.1a"><mo id="S1.p10.6.m2.1.1" xref="S1.p10.6.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p10.6.m2.1b"><times id="S1.p10.6.m2.1.1.cmml" xref="S1.p10.6.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.6.m2.1c">\times</annotation></semantics></math> fewer CPU cycles.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2405.10968/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synchronous FL with different aggregation timing (“Eager” and “Lazy”) <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Jayaram et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022c</a>)</cite>.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Challenges</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Basics of Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">FL aggregation:</span>
Aggregation in FL is a process of building a global model from individually trained model updates. The <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">aggregation goal</span>, <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">n</annotation></semantics></math> specifies the expected number of model updates to be received before the global model is updated to a new version. Thus, it dictates the number of selected clients for training. The aggregation process is abstracted as:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="w_{i}=f(\{(w^{k}_{i},\mathcal{A}^{k}_{i})~{}|~{}1\leq k\leq n\})." display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml">w</mi><mi id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml">{</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><msubsup id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.4" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">,</mo><msubsup id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝒜</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">k</mi></msubsup><mo rspace="0.052em" stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.5" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo rspace="0.330em" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.4" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml">|</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mn id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">1</mn><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">≤</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.4" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.4.cmml">k</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.5" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.5.cmml">≤</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.6" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.6.cmml">n</mi></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.5" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml">}</mo></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"></eq><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2">𝑤</ci><ci id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2"></times><ci id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3">𝑓</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3">conditional-set</csymbol><interval closure="open" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2"><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝒜</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑘</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2"><and id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2a.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2"></and><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2b.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2"><leq id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3"></leq><cn type="integer" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2">1</cn><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.4">𝑘</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2c.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2"><leq id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.5.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.5"></leq><share href="#S2.E1.m1.1.1.1.1.1.1.1.1.2.2.4.cmml" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2d.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2"></share><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.6.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.6">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">w_{i}=f(\{(w^{k}_{i},\mathcal{A}^{k}_{i})~{}|~{}1\leq k\leq n\}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.9" class="ltx_p">Here <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="f(\cdot)" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.2" xref="S2.SS1.p3.1.m1.1.2.cmml"><mi id="S2.SS1.p3.1.m1.1.2.2" xref="S2.SS1.p3.1.m1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.1.m1.1.2.1" xref="S2.SS1.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S2.SS1.p3.1.m1.1.2.3.2" xref="S2.SS1.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.1.m1.1.2.3.2.1" xref="S2.SS1.p3.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS1.p3.1.m1.1.2.3.2.2" xref="S2.SS1.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.2"><times id="S2.SS1.p3.1.m1.1.2.1.cmml" xref="S2.SS1.p3.1.m1.1.2.1"></times><ci id="S2.SS1.p3.1.m1.1.2.2.cmml" xref="S2.SS1.p3.1.m1.1.2.2">𝑓</ci><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">f(\cdot)</annotation></semantics></math> is an aggregation function, <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="w^{k}_{i}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msubsup id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2.2" xref="S2.SS1.p3.2.m2.1.1.2.2.cmml">w</mi><mi id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">i</mi><mi id="S2.SS1.p3.2.m2.1.1.2.3" xref="S2.SS1.p3.2.m2.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">subscript</csymbol><apply id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.2.1.cmml" xref="S2.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2.2">𝑤</ci><ci id="S2.SS1.p3.2.m2.1.1.2.3.cmml" xref="S2.SS1.p3.2.m2.1.1.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">w^{k}_{i}</annotation></semantics></math> is <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">k</annotation></semantics></math>-th local model update for global model version <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">i</annotation></semantics></math>, and <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{A}^{k}_{i}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msubsup id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.5.m5.1.1.2.2" xref="S2.SS1.p3.5.m5.1.1.2.2.cmml">𝒜</mi><mi id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">i</mi><mi id="S2.SS1.p3.5.m5.1.1.2.3" xref="S2.SS1.p3.5.m5.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">subscript</csymbol><apply id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.2.1.cmml" xref="S2.SS1.p3.5.m5.1.1">superscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2.2">𝒜</ci><ci id="S2.SS1.p3.5.m5.1.1.2.3.cmml" xref="S2.SS1.p3.5.m5.1.1.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">\mathcal{A}^{k}_{i}</annotation></semantics></math> is auxiliary information for aggregation. For the <span id="S2.SS1.p3.9.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>, <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="f(\cdot)=\sum^{n}_{k=1}w^{k}_{i}c^{k}_{i}/T_{i}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><mrow id="S2.SS1.p3.6.m6.1.2" xref="S2.SS1.p3.6.m6.1.2.cmml"><mrow id="S2.SS1.p3.6.m6.1.2.2" xref="S2.SS1.p3.6.m6.1.2.2.cmml"><mi id="S2.SS1.p3.6.m6.1.2.2.2" xref="S2.SS1.p3.6.m6.1.2.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.6.m6.1.2.2.1" xref="S2.SS1.p3.6.m6.1.2.2.1.cmml">​</mo><mrow id="S2.SS1.p3.6.m6.1.2.2.3.2" xref="S2.SS1.p3.6.m6.1.2.2.cmml"><mo stretchy="false" id="S2.SS1.p3.6.m6.1.2.2.3.2.1" xref="S2.SS1.p3.6.m6.1.2.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS1.p3.6.m6.1.2.2.3.2.2" xref="S2.SS1.p3.6.m6.1.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.SS1.p3.6.m6.1.2.1" xref="S2.SS1.p3.6.m6.1.2.1.cmml">=</mo><mrow id="S2.SS1.p3.6.m6.1.2.3" xref="S2.SS1.p3.6.m6.1.2.3.cmml"><msubsup id="S2.SS1.p3.6.m6.1.2.3.1" xref="S2.SS1.p3.6.m6.1.2.3.1.cmml"><mo id="S2.SS1.p3.6.m6.1.2.3.1.2.2" xref="S2.SS1.p3.6.m6.1.2.3.1.2.2.cmml">∑</mo><mrow id="S2.SS1.p3.6.m6.1.2.3.1.3" xref="S2.SS1.p3.6.m6.1.2.3.1.3.cmml"><mi id="S2.SS1.p3.6.m6.1.2.3.1.3.2" xref="S2.SS1.p3.6.m6.1.2.3.1.3.2.cmml">k</mi><mo id="S2.SS1.p3.6.m6.1.2.3.1.3.1" xref="S2.SS1.p3.6.m6.1.2.3.1.3.1.cmml">=</mo><mn id="S2.SS1.p3.6.m6.1.2.3.1.3.3" xref="S2.SS1.p3.6.m6.1.2.3.1.3.3.cmml">1</mn></mrow><mi id="S2.SS1.p3.6.m6.1.2.3.1.2.3" xref="S2.SS1.p3.6.m6.1.2.3.1.2.3.cmml">n</mi></msubsup><mrow id="S2.SS1.p3.6.m6.1.2.3.2" xref="S2.SS1.p3.6.m6.1.2.3.2.cmml"><mrow id="S2.SS1.p3.6.m6.1.2.3.2.2" xref="S2.SS1.p3.6.m6.1.2.3.2.2.cmml"><msubsup id="S2.SS1.p3.6.m6.1.2.3.2.2.2" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.cmml"><mi id="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.2" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.2.cmml">w</mi><mi id="S2.SS1.p3.6.m6.1.2.3.2.2.2.3" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.3.cmml">i</mi><mi id="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.3" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.3.cmml">k</mi></msubsup><mo lspace="0em" rspace="0em" id="S2.SS1.p3.6.m6.1.2.3.2.2.1" xref="S2.SS1.p3.6.m6.1.2.3.2.2.1.cmml">​</mo><msubsup id="S2.SS1.p3.6.m6.1.2.3.2.2.3" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.cmml"><mi id="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.2" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.2.cmml">c</mi><mi id="S2.SS1.p3.6.m6.1.2.3.2.2.3.3" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.3.cmml">i</mi><mi id="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.3" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.3.cmml">k</mi></msubsup></mrow><mo id="S2.SS1.p3.6.m6.1.2.3.2.1" xref="S2.SS1.p3.6.m6.1.2.3.2.1.cmml">/</mo><msub id="S2.SS1.p3.6.m6.1.2.3.2.3" xref="S2.SS1.p3.6.m6.1.2.3.2.3.cmml"><mi id="S2.SS1.p3.6.m6.1.2.3.2.3.2" xref="S2.SS1.p3.6.m6.1.2.3.2.3.2.cmml">T</mi><mi id="S2.SS1.p3.6.m6.1.2.3.2.3.3" xref="S2.SS1.p3.6.m6.1.2.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.2.cmml" xref="S2.SS1.p3.6.m6.1.2"><eq id="S2.SS1.p3.6.m6.1.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.1"></eq><apply id="S2.SS1.p3.6.m6.1.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.2"><times id="S2.SS1.p3.6.m6.1.2.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.2.1"></times><ci id="S2.SS1.p3.6.m6.1.2.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.2.2">𝑓</ci><ci id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">⋅</ci></apply><apply id="S2.SS1.p3.6.m6.1.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3"><apply id="S2.SS1.p3.6.m6.1.2.3.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.1.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1">subscript</csymbol><apply id="S2.SS1.p3.6.m6.1.2.3.1.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.1.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1">superscript</csymbol><sum id="S2.SS1.p3.6.m6.1.2.3.1.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1.2.2"></sum><ci id="S2.SS1.p3.6.m6.1.2.3.1.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1.2.3">𝑛</ci></apply><apply id="S2.SS1.p3.6.m6.1.2.3.1.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1.3"><eq id="S2.SS1.p3.6.m6.1.2.3.1.3.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1.3.1"></eq><ci id="S2.SS1.p3.6.m6.1.2.3.1.3.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1.3.2">𝑘</ci><cn type="integer" id="S2.SS1.p3.6.m6.1.2.3.1.3.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.1.3.3">1</cn></apply></apply><apply id="S2.SS1.p3.6.m6.1.2.3.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2"><divide id="S2.SS1.p3.6.m6.1.2.3.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.1"></divide><apply id="S2.SS1.p3.6.m6.1.2.3.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2"><times id="S2.SS1.p3.6.m6.1.2.3.2.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.1"></times><apply id="S2.SS1.p3.6.m6.1.2.3.2.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.2.2.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2">subscript</csymbol><apply id="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.2">𝑤</ci><ci id="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.6.m6.1.2.3.2.2.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.2.3">𝑖</ci></apply><apply id="S2.SS1.p3.6.m6.1.2.3.2.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.2.2.3.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3">subscript</csymbol><apply id="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.2">𝑐</ci><ci id="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.6.m6.1.2.3.2.2.3.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.2.3.3">𝑖</ci></apply></apply><apply id="S2.SS1.p3.6.m6.1.2.3.2.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.2.3.2.3.1.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.3">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.2.3.2.3.2.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.3.2">𝑇</ci><ci id="S2.SS1.p3.6.m6.1.2.3.2.3.3.cmml" xref="S2.SS1.p3.6.m6.1.2.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">f(\cdot)=\sum^{n}_{k=1}w^{k}_{i}c^{k}_{i}/T_{i}</annotation></semantics></math>. <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="T_{i}=\sum^{n}_{k=1}c^{k}_{i}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><mrow id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><msub id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2.2" xref="S2.SS1.p3.7.m7.1.1.2.2.cmml">T</mi><mi id="S2.SS1.p3.7.m7.1.1.2.3" xref="S2.SS1.p3.7.m7.1.1.2.3.cmml">i</mi></msub><mo rspace="0.111em" id="S2.SS1.p3.7.m7.1.1.1" xref="S2.SS1.p3.7.m7.1.1.1.cmml">=</mo><mrow id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml"><msubsup id="S2.SS1.p3.7.m7.1.1.3.1" xref="S2.SS1.p3.7.m7.1.1.3.1.cmml"><mo id="S2.SS1.p3.7.m7.1.1.3.1.2.2" xref="S2.SS1.p3.7.m7.1.1.3.1.2.2.cmml">∑</mo><mrow id="S2.SS1.p3.7.m7.1.1.3.1.3" xref="S2.SS1.p3.7.m7.1.1.3.1.3.cmml"><mi id="S2.SS1.p3.7.m7.1.1.3.1.3.2" xref="S2.SS1.p3.7.m7.1.1.3.1.3.2.cmml">k</mi><mo id="S2.SS1.p3.7.m7.1.1.3.1.3.1" xref="S2.SS1.p3.7.m7.1.1.3.1.3.1.cmml">=</mo><mn id="S2.SS1.p3.7.m7.1.1.3.1.3.3" xref="S2.SS1.p3.7.m7.1.1.3.1.3.3.cmml">1</mn></mrow><mi id="S2.SS1.p3.7.m7.1.1.3.1.2.3" xref="S2.SS1.p3.7.m7.1.1.3.1.2.3.cmml">n</mi></msubsup><msubsup id="S2.SS1.p3.7.m7.1.1.3.2" xref="S2.SS1.p3.7.m7.1.1.3.2.cmml"><mi id="S2.SS1.p3.7.m7.1.1.3.2.2.2" xref="S2.SS1.p3.7.m7.1.1.3.2.2.2.cmml">c</mi><mi id="S2.SS1.p3.7.m7.1.1.3.2.3" xref="S2.SS1.p3.7.m7.1.1.3.2.3.cmml">i</mi><mi id="S2.SS1.p3.7.m7.1.1.3.2.2.3" xref="S2.SS1.p3.7.m7.1.1.3.2.2.3.cmml">k</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><eq id="S2.SS1.p3.7.m7.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1.1"></eq><apply id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.2.1.cmml" xref="S2.SS1.p3.7.m7.1.1.2">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.2.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2.2">𝑇</ci><ci id="S2.SS1.p3.7.m7.1.1.2.3.cmml" xref="S2.SS1.p3.7.m7.1.1.2.3">𝑖</ci></apply><apply id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3"><apply id="S2.SS1.p3.7.m7.1.1.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.3.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1">subscript</csymbol><apply id="S2.SS1.p3.7.m7.1.1.3.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.3.1.2.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1">superscript</csymbol><sum id="S2.SS1.p3.7.m7.1.1.3.1.2.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1.2.2"></sum><ci id="S2.SS1.p3.7.m7.1.1.3.1.2.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1.2.3">𝑛</ci></apply><apply id="S2.SS1.p3.7.m7.1.1.3.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1.3"><eq id="S2.SS1.p3.7.m7.1.1.3.1.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1.3.1"></eq><ci id="S2.SS1.p3.7.m7.1.1.3.1.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1.3.2">𝑘</ci><cn type="integer" id="S2.SS1.p3.7.m7.1.1.3.1.3.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1.3.3">1</cn></apply></apply><apply id="S2.SS1.p3.7.m7.1.1.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.3.2.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2">subscript</csymbol><apply id="S2.SS1.p3.7.m7.1.1.3.2.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.3.2.2.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2">superscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.3.2.2.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2.2.2">𝑐</ci><ci id="S2.SS1.p3.7.m7.1.1.3.2.2.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.7.m7.1.1.3.2.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">T_{i}=\sum^{n}_{k=1}c^{k}_{i}</annotation></semantics></math> and <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{A}^{k}_{i}" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><msubsup id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.8.m8.1.1.2.2" xref="S2.SS1.p3.8.m8.1.1.2.2.cmml">𝒜</mi><mi id="S2.SS1.p3.8.m8.1.1.3" xref="S2.SS1.p3.8.m8.1.1.3.cmml">i</mi><mi id="S2.SS1.p3.8.m8.1.1.2.3" xref="S2.SS1.p3.8.m8.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><apply id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1">subscript</csymbol><apply id="S2.SS1.p3.8.m8.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.2.1.cmml" xref="S2.SS1.p3.8.m8.1.1">superscript</csymbol><ci id="S2.SS1.p3.8.m8.1.1.2.2.cmml" xref="S2.SS1.p3.8.m8.1.1.2.2">𝒜</ci><ci id="S2.SS1.p3.8.m8.1.1.2.3.cmml" xref="S2.SS1.p3.8.m8.1.1.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.8.m8.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">\mathcal{A}^{k}_{i}</annotation></semantics></math> is <math id="S2.SS1.p3.9.m9.1" class="ltx_Math" alttext="c^{k}_{i}" display="inline"><semantics id="S2.SS1.p3.9.m9.1a"><msubsup id="S2.SS1.p3.9.m9.1.1" xref="S2.SS1.p3.9.m9.1.1.cmml"><mi id="S2.SS1.p3.9.m9.1.1.2.2" xref="S2.SS1.p3.9.m9.1.1.2.2.cmml">c</mi><mi id="S2.SS1.p3.9.m9.1.1.3" xref="S2.SS1.p3.9.m9.1.1.3.cmml">i</mi><mi id="S2.SS1.p3.9.m9.1.1.2.3" xref="S2.SS1.p3.9.m9.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m9.1b"><apply id="S2.SS1.p3.9.m9.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.1.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1">subscript</csymbol><apply id="S2.SS1.p3.9.m9.1.1.2.cmml" xref="S2.SS1.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.1.1.2.1.cmml" xref="S2.SS1.p3.9.m9.1.1">superscript</csymbol><ci id="S2.SS1.p3.9.m9.1.1.2.2.cmml" xref="S2.SS1.p3.9.m9.1.1.2.2">𝑐</ci><ci id="S2.SS1.p3.9.m9.1.1.2.3.cmml" xref="S2.SS1.p3.9.m9.1.1.2.3">𝑘</ci></apply><ci id="S2.SS1.p3.9.m9.1.1.3.cmml" xref="S2.SS1.p3.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m9.1c">c^{k}_{i}</annotation></semantics></math> (the number of data samples).</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Eager aggregation and Lazy aggregation:</span>
Based on the
timing to trigger the aggregation, we can classify the model aggregation to be “<span id="S2.SS1.p4.1.2" class="ltx_text ltx_font_italic">eager</span>” or “<span id="S2.SS1.p4.1.3" class="ltx_text ltx_font_italic">lazy</span>” <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022c</a>)</cite>:
<span id="S2.SS1.p4.1.4" class="ltx_text ltx_font_italic">Eager</span> aggregation allows aggregation to happen whenever an update is received, leading to more flexible and dynamic timing of the aggregation process. <span id="S2.SS1.p4.1.5" class="ltx_text ltx_font_italic">Lazy</span> aggregation operates on a delayed schedule, where model updates that arrive early are queued without being aggregated immediately.

Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the two different aggregation methods
for synchronous FL. For instance, the <span id="S2.SS1.p4.1.6" class="ltx_text ltx_font_italic">eager</span> method is feasible for FedAvg with cumulative averaging.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Anatomy of Systems for Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.4" class="ltx_p">Designing a system to support FL at a <span id="S2.SS2.p1.4.1" class="ltx_text ltx_font_bold ltx_font_italic">large scale</span> is essential, as a larger
number of participants means a more diverse and representative dataset.
It improves the model’s ability to capture complex patterns and unseen relationships in the data.
These benefits help the model generalize in real-world deployments, <span id="S2.SS2.p1.4.2" class="ltx_text ltx_font_italic">e.g., </span>Google’s FL stack has been used to serve <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mo id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\sim</annotation></semantics></math><math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="10M" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mrow id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mn id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.1" xref="S2.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><times id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">10</cn><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">10M</annotation></semantics></math> devices daily and <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mo id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><csymbol cd="latexml" id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\sim</annotation></semantics></math><math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="10K" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mrow id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mn id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><times id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">10</cn><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">10K</annotation></semantics></math> devices participate in FL training simultaneously <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts key architectural components that are needed to ensure the success of FL at scale.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We adopt the terminology of FL system components from <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>.</span></span></span>
These components work together to enable the collaborative and decentralized training process in FL.
In addition to the <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">aggregator</span> and the <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_bold">client</span>,
the <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_bold">coordinator</span> oversees the flow of FL operations. It acts as an orchestrator that facilitates seamless interactions among aggregators, selectors, and clients by applying the client selection scheme and instructing the selector to map the selected clients to backend aggregators <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>.
The <span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_bold">selector</span> plays two roles. First, it ensures that a diverse set of clients participate in the FL process to capture a representative sample of the distributed data.
Second, it acts as a gateway that mediates communication (<span id="S2.SS2.p2.1.5" class="ltx_text ltx_font_italic">i.e., </span>queuing, load balancing) between (leaf) aggregators and clients <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2405.10968/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Generic architectures for FL systems: (a) Serverful FL systems <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>; (b) Serverless FL systems <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022a</a>; <a href="#bib.bib29" title="" class="ltx_ref">b</a>); Chadha et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. Note that for simplicity, we skip the hierarchy in the diagram (b).</figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Need for hierarchical aggregation:</span>
The growing number of participating clients in FL requires the system to be scalable to accommodate the computational requirements of aggregating model updates from a large number of distributed clients.
This primarily motivates the use of <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">hierarchical aggregation</span> potentially involving multiple levels of aggregation in the FL process <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Jayaram et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022b</a>)</cite>, as depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a).
Essentially, hierarchical aggregation is structured as a single-rooted tree.
Each level in the tree includes multiple parallel aggregation tasks that are executed by one of potentially multiple aggregators.
The communication during the hierarchical aggregation task takes place across multiple levels:
The model updates from smaller subgroups of clients are aggregated by the lower-level aggregators (<span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">i.e., </span>leaf) and passed onto higher-level aggregators (<span id="S2.SS2.p3.1.4" class="ltx_text ltx_font_italic">i.e., </span>top),
until a global model is obtained.
This parallel aggregation at the lower levels can provide speedup and reduce queueing of model updates. </p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Motivation and Challenges for Serverless FL</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">State-of-the-art FL systems <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> rely on a “serverful” design
that relies on a fixed pool of dedicated resources (<span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g., </span>CPU and memory), using a pool of provisioned VMs.
Resizing the pool often takes a long time (e.g., 6 to 45 minutes on AWS <cite class="ltx_cite ltx_citemacro_cite">Scheller (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>).

Serverless computing, on the other hand, brings fine-grained resource elasticity
by provisioning functions (typically as containers) dynamically based on demand, ensuring that the right amount of resources is allocated only when needed.
</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">In FL, serverless computing can be used to provide efficient model aggregation, adapting to varying numbers of clients.
It eliminates the need to maintain dedicated resource pools for the aggregation service, thereby improving overall efficiency compared to the current “serverful” deployments.
</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Prior Work on Serverless FL.</span> A number of FL system designs have been proposed using serverless computing <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022a</a>; <a href="#bib.bib29" title="" class="ltx_ref">b</a>); Grafberger et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>.
A common abstract architecture of a serverless FL system and its key components is shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b).
But, prior approaches still face the following challenges:
</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">Indirect networking:</span>
Unlike a “serverful” design (Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a)), a serverless FL system executes aggregators as serverless functions. Serverless function chaining can support hierarchical aggregation as well as communication between aggregators. However, because serverless functions are ephemeral and stateless (and thus unable to retain stateful information like routes), these chains typically only support <span id="S2.SS3.p4.1.2" class="ltx_text ltx_font_italic">indirect</span> networking between functions.
This raises the need for a stateful, persistent networking component (Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b)), such as a message broker or external storage services,<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For consistency, we use the generic term “message broker” to denote such a networking component throughout this paper.</span></span></span> to maintain routes and exchange messages <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>.
However, having such a networking component in the internal datapath between serverless functions adds unnecessary overhead (20% added delay as in Fig. <a href="#S6.F7.sf1" title="In Figure 7 ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(a)</span></a>).</p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">Inefficient message queuing:</span>
In addition to supporting function chaining, the message broker (Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b)) also acts as a message queue to buffer incoming model updates from clients while aggregators are being spawned by the serverless control plane <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022a</a>; <a href="#bib.bib29" title="" class="ltx_ref">b</a>)</cite>.
However, the message broker and dedicated queues add overhead and delay to the aggregation service.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">Heavyweight sidecar:</span>
Scheduling serverless functions typically requires metrics collection, often using a sidecar.
This container-based sidecar introduces additional network processing
in the datapath, requiring the interception and forwarding of model updates. This leads to complex data pipelines (involving extra communication hops between aggregators) and increased communication overheads due to the reliance on kernel-based networking <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para ltx_noindent">
<p id="S2.SS3.p7.1" class="ltx_p"><span id="S2.SS3.p7.1.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">Application-agnostic, simple, autoscaling:</span>
Current serverless autoscaler designs typically rely on a simplistic threshold based on user input (<span id="S2.SS3.p7.1.2" class="ltx_text ltx_font_italic">e.g., </span>request per second, concurrency) for scaling decisions <cite class="ltx_cite ltx_citemacro_cite">aut (<a href="#bib.bib2" title="" class="ltx_ref">2023b</a>; <a href="#bib.bib1" title="" class="ltx_ref">a</a>)</cite>, often being unaware of application needs.
This design, agnostic of the hierarchical structure of FL aggregation, is limited in its ability to optimize the system to maximize parallelism, <span id="S2.SS3.p7.1.3" class="ltx_text ltx_font_italic">i.e., </span>the number of levels and the number of aggregators at each level.
Looking at Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a), as we go up the levels in the hierarchy, fewer aggregators are needed.
This can be leveraged to potentially reuse the lower-level aggregators as we proceed up the hierarchy.
Further, since hierarchical aggregation uses function chaining, current “reactive” autoscaling designs lead to a cascading effect <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021b</a>)</cite> of the cold-start delays when scaling a function chain.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para ltx_noindent">
<p id="S2.SS3.p8.1" class="ltx_p"><span id="S2.SS3.p8.1.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">Locality-agnostic placement:</span>
<span id="S2.SS3.p8.1.2" class="ltx_text ltx_font_italic">Intra-node</span> communication can be faster than <span id="S2.SS3.p8.1.3" class="ltx_text ltx_font_italic">inter-node</span> communication by avoiding a lot of networking overheads <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite> and using state-of-the-art serverless data plane designs with <span id="S2.SS3.p8.1.4" class="ltx_text ltx_font_italic">shared memory</span> <cite class="ltx_cite ltx_citemacro_cite">Shillaker &amp; Pietzuch (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>); Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>); Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2023</a>)</cite>. However, leveraging the benefits of shared memory effectively can be challenging when dealing with a large hierarchy of aggregators that cannot be accommodated within a single node. This requires careful function placement by taking into account the impact of communication between aggregators. Inter-node communication typically still uses kernel-based networking.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>LIFL Overview</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We aim to address the
aforementioned limitations (§<a href="#S2.SS3" title="2.3 Motivation and Challenges for Serverless FL ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>) and develop LIFL—a high-performance, lightweight, and elastic serverless platform for FL,
utilizing hierarchical aggregation.
We focus on the following innovations of LIFL:
</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2405.10968/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The overall architecture of LIFL.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">(1) High-performance intra-node dataplane:</span> LIFL incorporates shared memory processing to provide a <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">zero-copy</span> communication channel between FL aggregators placed on the same node (§<a href="#S4.SS1" title="4.1 Shared Memory for Hierarchical Aggregation ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). This avoids heavyweight kernel networking overheads, especially data copies <cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>, as model updates are often large, <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">e.g., </span>a model update from
ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> is <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.p2.1.m1.1a"><mo id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><csymbol cd="latexml" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\sim</annotation></semantics></math>230 MBytes.
Shared memory can also eliminate other overheads such as protocol processing, serialization/de-serialization, kernel/userspace boundary crossing, and interrupts.
</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">(2) In-place message queuing:</span>
We extensively leverage shared memory in LIFL to offer “in-place” message queuing (§<a href="#S4.SS2" title="4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Messages (<span id="S3.p3.1.2" class="ltx_text ltx_font_italic">i.e., </span>model updates) from selected clients are directly buffered in shared memory and can be instantly accessed by the aggregators when they are ready. This eliminates dedicated message queues and their associated queuing delays.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">(3) Lightweight eBPF-based sidecar:</span>
We incorporate the extended Berkeley Packet Filter (eBPF <cite class="ltx_cite ltx_citemacro_cite">ebp (<a href="#bib.bib3" title="" class="ltx_ref">2023a</a>)</cite>) into LIFL to build a lightweight sidecar (§<a href="#S4.SS3" title="4.3 eBPF-based Sidecar ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>) to provide important functionality, <span id="S3.p4.1.2" class="ltx_text ltx_font_italic">e.g., </span>metrics collection. Unlike
a container-based sidecar,
LIFL’s sidecar runs as eBPF code attached at in-kernel hooks, avoiding the need for dedicated resources.
We further utilize the eBPF-based sidecar to support <span id="S3.p4.1.3" class="ltx_text ltx_font_italic">direct</span> networking between aggregators (§<a href="#S4.SS4" title="4.4 Direct Routing with Hierarchical Aggregation ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>), completely replacing the message broker.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">(4) A cost-effective orchestration heuristic:</span>
LIFL orchestrates the model aggregation to fully exploit the improved serverless dataplane by employing several strategies:
(1) locality-aware placement that partitions levels with large traffic into node-affinity groups to make the best use of shared memory processing (§<a href="#S5.SS1" title="5.1 Locality-aware Placement and Load Balancing ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>);
(2) hierarchy-aware scaling that dynamically adjusts the configuration of hierarchical aggregation (§<a href="#S5.SS2" title="5.2 Planning the Hierarchy for Aggregation ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>), and
(3) opportunistic reuse of the aggregator runtime from a lower level (§<a href="#S5.SS3" title="5.3 Opportunistic Reuse of Aggregator Instances ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Architectural overview of LIFL:</span>
Fig. <a href="#S3.F3" title="Figure 3 ‣ 3 LIFL Overview" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the overall architecture of LIFL.
LIFL maintains a shared memory object store on each worker node to enable zero-copy communication between aggregators.
To support in-place message queuing, LIFL introduces a gateway on each worker node that receives model updates from remote clients. The gateway performs a consolidated, one-time payload processing to queue the received model updates to shared memory.
Each aggregator in LIFL has attached to it an eBPF-based sidecar
for lightweight metrics collection.
Aggregators in LIFL are stateless, so new ones start without state synchronization upon an aggregator failure. LIFL detects client failures with keep-alive heartbeats and enhances resilience by over-provisioning the number of clients.
In the control plane, a LIFL agent is deployed on each worker node to manage the lifecycle (<span id="S3.p6.1.2" class="ltx_text ltx_font_italic">e.g., </span>creation, termination) of aggregators, following instructions from the LIFL control plane.
The LIFL coordinator, a cluster-wide control plane component, is used for interactions between the FL job designer (ML engineer) and the serverless control plane (<span id="S3.p6.1.3" class="ltx_text ltx_font_italic">e.g., </span>autoscaler, placement engine).<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Note: Even though the serverless control plane has serverful, always-on components (<span id="footnote4.1" class="ltx_text ltx_font_italic">e.g., </span>autoscaler, placement engine), are shared and their overheads are amortized across multiple workloads, especially at scale.</span></span></span>
It works with the serverless control plane to execute LIFL’s orchestration flow (§<a href="#S5" title="5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Optimizing the Serverless Data-Plane in LIFL</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Shared Memory for Hierarchical Aggregation</h3>

<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.10968/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.10968/assets/x5.png" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="152" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Impact of data plane performance on hierarchical aggregation. (upper fig.:) No hierarchy(NH); (lower fig.:) With hierarchy(WH). Top: top aggregator; LF: leaf aggregator. “Network” denotes the data transfer tasks of model updates; “Agg.” denotes the aggregation tasks; “Eval.” denotes the evaluation tasks.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Assessing data plane with hierarchical aggregation:</span>
We now assess the importance of a high-performance data plane to truly deliver on the promise of hierarchical aggregation.
We consider a baseline (denoted <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">NH</span>) with a single aggregator without hierarchy.
We evaluate the hierarchical aggregation service that has one top aggregator and four leaf aggregators (denoted <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">WH</span>). All aggregators are placed on the same node.
We consider eight trainers to train a ResNet-152 model using FEMNIST dataset.
Note that we always deploy trainers on separate nodes, to both be realistic (trainers are remote) and to avoid contention for resources on the node.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Shared Memory for Hierarchical Aggregation ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the execution times for the representative FL stages under different settings.
Note that we only show the receiving part of the networking task (“Network” in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Shared Memory for Hierarchical Aggregation ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) to simplify the figure.
Compared to the baseline (<span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">NH</span>), <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">WH</span> does not exhibit a significant improvement overall, though it uses hierarchical aggregation.
The average completion time per round with <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">WH</span> is 57 seconds, while for <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">NH</span> is 59.8 seconds.
This is mainly because of the contention for network processing between leaf aggregators when they send/receive intermediate model updates to/from the top aggregator.
This highlights the critical need for a high-performance and streamlined data plane for hierarchical aggregation.
LIFL incorporates shared memory processing when the serverless aggregator functions are co-located on the same node. This enables fast and efficient communication, mitigating the impact of networking on hierarchical aggregation (demonstrated in Fig. <a href="#S6.F7" title="Figure 7 ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). Working jointly with our locality-aware placement scheme (§<a href="#S5.SS1" title="5.1 Locality-aware Placement and Load Balancing ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>), LIFL can minimize the need for inter-node model update transfers. Consequently, LIFL maximizes the advantages of our efficient intra-node shared memory data plane that substantially reduces communication overheads.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Shared memory object store:</span>
The LIFL agent is responsible for the allocation/recycling/destruction of the shared memory buffer in the object store.
In addition, LIFL only allows immutable (read-only) objects to guarantee the safe sharing of model updates, eliminating the need for locks.
The agent periodically checkpoints the model parameters to an external persistent storage service (more details in Appendix-<a href="#A2" title="Appendix B Model checkpoints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>In-place Message Queuing</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Representative message queuing solutions:</span>
Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> enumerates message queuing solutions for various serverful and serverless alternatives.
In the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">monolithic</span> serverful setup (used in <cite class="ltx_cite ltx_citemacro_cite">Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>), the model update is directly buffered into an in-memory queue residing in the aggregator, deployed as a persistent and stateful application.
Another serverful setup, used in <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, deploys aggregators as ephemeral, stateless <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">microservices</span>, requiring an additional persistent, stateful message broker to buffer model updates from clients before being consumed by the stateless aggregator.
Switching to the <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_italic">basic</span> serverless setup (used in <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022b</a>)</cite>), model updates are also buffered at a message broker, as the aggregator is now deployed as an ephemeral, stateless serverless function. Before being consumed by the aggregator, the model update has to pass through the sidecar. Finally, in LIFL, the gateway buffers the model update directly into the shared memory, which can then be seamlessly accessed by the aggregator.
The distinct data pipelines between the client, message queue, and aggregator impose varying degrees of overheads.
Our evaluation (details in Appendix-<a href="#A6" title="Appendix F In-place message queueing benefit" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>) shows that LIFL’s in-place message queuing achieves the best efficiency and performance (equivalent to a monolithic, serverful design) among all alternatives in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2405.10968/assets/x6.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Message queuing solutions.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Message queuing pipeline in LIFL:</span>
The gateway at each worker node is addressable/accessible by FL clients. It receives model updates from clients or from the gateway on another worker node, and performs necessary network processing (<span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">e.g., </span>protocol processing, serialization, deserialization, data type conversion, <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_italic">etc</span>) before writing the model updates into shared memory. This avoids duplicate processing when local aggregators access model updates in shared memory.
A step-by-step explanation of the processing flow of the message queuing in LIFL is given in Appendix-<a href="#A3" title="Appendix C Message queueing flow in LIFL: Receive (RX) and Transmit (TX)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">We apply vertical scaling of the gateway by dynamically adjusting the number of assigned CPU cores based on the load level. This avoids the gateway becoming the dataplane bottleneck and impacting the aggregation speed.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>eBPF-based Sidecar</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">LIFL’s eBPF-based sidecar is built with a set of eBPF programs attached to each aggregator’s socket interface, using its in-kernel SKMSG hook <cite class="ltx_cite ltx_citemacro_cite">Red Hat, Inc. (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>.
The execution of the eBPF-based sidecar is triggered by the invocation of the <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">send()</span> system call, which is captured by the SKMSG hook as an eBPF event. This ensures that the eBPF-based sidecar is strictly event-driven and consumes <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">no</span> CPU resources when idle. We use the eBPF-based sidecar to collect necessary metrics (<span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">e.g., </span>execution time of the aggregation task) to facilitate the orchestration in LIFL (§<a href="#S5" title="5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Metrics collection:</span>
Upon invocation, the eBPF-based sidecar collects and stores metrics to an eBPF map (<span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">metrics map</span>)
on the local worker node.
The eBPF map is an in-kernel, configurable key-value table that can be accessed by the eBPF program during execution <cite class="ltx_cite ltx_citemacro_cite">ebp (<a href="#bib.bib5" title="" class="ltx_ref">2023c</a>)</cite>.
The LIFL agent, on the other hand, periodically retrieves the latest metrics from the <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">metrics map</span> and feeds the metrics back to the metrics server (Fig. <a href="#S3.F3" title="Figure 3 ‣ 3 LIFL Overview" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) in the serverless control plane.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Direct Routing with Hierarchical Aggregation</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">Direct networking between functions is not allowed in existing serverless environments because serverless functions are considered to be stateless and ephemeral. This implies that there are no long-lived, direct connections between a pair of function instances. As a result, they use an intermediate networking component (<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">e.g., </span>message broker) to act as a stateful, persistent component to manage state, <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_italic">i.e., </span>routes between functions.
However, the main drawback is that it adds unnecessary overhead by involving the additional networking component(s) in the datapath, making indirect networking between functions heavyweight.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">LIFL improves serverless networking within hierarchical aggregation by allowing direct routing between aggregators, both within a node and between nodes. The key is to offload the stateful processing to eBPF, using the <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_italic">sockmap</span> <cite class="ltx_cite ltx_citemacro_cite">Red Hat, Inc. (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> to support flexible intra-node routing exploiting shared memory, and inter-node routing with the help of the per-node gateway, as depicted in Fig. <a href="#A1.F12" title="Figure 12 ‣ Appendix A Message flow of intra-node and inter-node routing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.
The <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_italic">sockmap</span> is a special eBPF map (<span id="S4.SS4.p2.1.3" class="ltx_text ltx_font_typewriter">BPF_MAP_TYPE_SOCKMAP</span> <cite class="ltx_cite ltx_citemacro_cite">Red Hat, Inc. (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>) that maintains references to the registered socket interfaces.
We take the approach from <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> to implement intra-node direct routing in LIFL. For details of intra-/inter-node routing in LIFL, refer to Appendix-<a href="#A1" title="Appendix A Message flow of intra-node and inter-node routing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>LIFL’s Control Plane Design</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Locality-aware Placement and Load Balancing</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">The placement of aggregators can lead to different routing behaviors:
When aggregators with cross-level data dependencies are placed on the same node, the shared memory processing and eBPF-based sidecar can facilitate intra-node routing.

When these aggregators are placed across different nodes, the gateway has to perform inter-node routing.
To minimize the transfer of model updates in LIFL, we take a data-centric strategy like <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2023</a>)</cite> that is aware of the locality of model updates and places the aggregator close to the model updates.
As such, the in-place message queuing (§<a href="#S4.SS2" title="4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), which is, in fact, the result of load balancing (clients to worker node mapping), directly affects the effectiveness of the locality-aware placement of the aggregators.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.14" class="ltx_p">Our objective of load balancing involves two crucial criteria:
<span id="S5.SS1.p2.14.1" class="ltx_text ltx_font_bold">(1)</span> Minimizing inter-node communication
while maximizing the utilization of shared memory within each node.
<span id="S5.SS1.p2.14.2" class="ltx_text ltx_font_bold">(2)</span> Ensuring the
residual service capacity of the worker node meets the demand; the residual service capacity (<math id="S5.SS1.p2.1.m1.2" class="ltx_Math" alttext="RC_{i,t}" display="inline"><semantics id="S5.SS1.p2.1.m1.2a"><mrow id="S5.SS1.p2.1.m1.2.3" xref="S5.SS1.p2.1.m1.2.3.cmml"><mi id="S5.SS1.p2.1.m1.2.3.2" xref="S5.SS1.p2.1.m1.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.1.m1.2.3.1" xref="S5.SS1.p2.1.m1.2.3.1.cmml">​</mo><msub id="S5.SS1.p2.1.m1.2.3.3" xref="S5.SS1.p2.1.m1.2.3.3.cmml"><mi id="S5.SS1.p2.1.m1.2.3.3.2" xref="S5.SS1.p2.1.m1.2.3.3.2.cmml">C</mi><mrow id="S5.SS1.p2.1.m1.2.2.2.4" xref="S5.SS1.p2.1.m1.2.2.2.3.cmml"><mi id="S5.SS1.p2.1.m1.1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p2.1.m1.2.2.2.4.1" xref="S5.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.1.m1.2.2.2.2" xref="S5.SS1.p2.1.m1.2.2.2.2.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.2b"><apply id="S5.SS1.p2.1.m1.2.3.cmml" xref="S5.SS1.p2.1.m1.2.3"><times id="S5.SS1.p2.1.m1.2.3.1.cmml" xref="S5.SS1.p2.1.m1.2.3.1"></times><ci id="S5.SS1.p2.1.m1.2.3.2.cmml" xref="S5.SS1.p2.1.m1.2.3.2">𝑅</ci><apply id="S5.SS1.p2.1.m1.2.3.3.cmml" xref="S5.SS1.p2.1.m1.2.3.3"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.2.3.3.1.cmml" xref="S5.SS1.p2.1.m1.2.3.3">subscript</csymbol><ci id="S5.SS1.p2.1.m1.2.3.3.2.cmml" xref="S5.SS1.p2.1.m1.2.3.3.2">𝐶</ci><list id="S5.SS1.p2.1.m1.2.2.2.3.cmml" xref="S5.SS1.p2.1.m1.2.2.2.4"><ci id="S5.SS1.p2.1.m1.1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1.1">𝑖</ci><ci id="S5.SS1.p2.1.m1.2.2.2.2.cmml" xref="S5.SS1.p2.1.m1.2.2.2.2">𝑡</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.2c">RC_{i,t}</annotation></semantics></math>) of worker node <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">i</annotation></semantics></math> at time <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mi id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><ci id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">t</annotation></semantics></math> is determined by <math id="S5.SS1.p2.4.m4.7" class="ltx_Math" alttext="RC_{i,t}=MC_{i}-(k_{i,t}\times E_{i,t})" display="inline"><semantics id="S5.SS1.p2.4.m4.7a"><mrow id="S5.SS1.p2.4.m4.7.7" xref="S5.SS1.p2.4.m4.7.7.cmml"><mrow id="S5.SS1.p2.4.m4.7.7.3" xref="S5.SS1.p2.4.m4.7.7.3.cmml"><mi id="S5.SS1.p2.4.m4.7.7.3.2" xref="S5.SS1.p2.4.m4.7.7.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.4.m4.7.7.3.1" xref="S5.SS1.p2.4.m4.7.7.3.1.cmml">​</mo><msub id="S5.SS1.p2.4.m4.7.7.3.3" xref="S5.SS1.p2.4.m4.7.7.3.3.cmml"><mi id="S5.SS1.p2.4.m4.7.7.3.3.2" xref="S5.SS1.p2.4.m4.7.7.3.3.2.cmml">C</mi><mrow id="S5.SS1.p2.4.m4.2.2.2.4" xref="S5.SS1.p2.4.m4.2.2.2.3.cmml"><mi id="S5.SS1.p2.4.m4.1.1.1.1" xref="S5.SS1.p2.4.m4.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p2.4.m4.2.2.2.4.1" xref="S5.SS1.p2.4.m4.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.4.m4.2.2.2.2" xref="S5.SS1.p2.4.m4.2.2.2.2.cmml">t</mi></mrow></msub></mrow><mo id="S5.SS1.p2.4.m4.7.7.2" xref="S5.SS1.p2.4.m4.7.7.2.cmml">=</mo><mrow id="S5.SS1.p2.4.m4.7.7.1" xref="S5.SS1.p2.4.m4.7.7.1.cmml"><mrow id="S5.SS1.p2.4.m4.7.7.1.3" xref="S5.SS1.p2.4.m4.7.7.1.3.cmml"><mi id="S5.SS1.p2.4.m4.7.7.1.3.2" xref="S5.SS1.p2.4.m4.7.7.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.4.m4.7.7.1.3.1" xref="S5.SS1.p2.4.m4.7.7.1.3.1.cmml">​</mo><msub id="S5.SS1.p2.4.m4.7.7.1.3.3" xref="S5.SS1.p2.4.m4.7.7.1.3.3.cmml"><mi id="S5.SS1.p2.4.m4.7.7.1.3.3.2" xref="S5.SS1.p2.4.m4.7.7.1.3.3.2.cmml">C</mi><mi id="S5.SS1.p2.4.m4.7.7.1.3.3.3" xref="S5.SS1.p2.4.m4.7.7.1.3.3.3.cmml">i</mi></msub></mrow><mo id="S5.SS1.p2.4.m4.7.7.1.2" xref="S5.SS1.p2.4.m4.7.7.1.2.cmml">−</mo><mrow id="S5.SS1.p2.4.m4.7.7.1.1.1" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS1.p2.4.m4.7.7.1.1.1.2" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.cmml">(</mo><mrow id="S5.SS1.p2.4.m4.7.7.1.1.1.1" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.cmml"><msub id="S5.SS1.p2.4.m4.7.7.1.1.1.1.2" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.cmml"><mi id="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.2" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.2.cmml">k</mi><mrow id="S5.SS1.p2.4.m4.4.4.2.4" xref="S5.SS1.p2.4.m4.4.4.2.3.cmml"><mi id="S5.SS1.p2.4.m4.3.3.1.1" xref="S5.SS1.p2.4.m4.3.3.1.1.cmml">i</mi><mo id="S5.SS1.p2.4.m4.4.4.2.4.1" xref="S5.SS1.p2.4.m4.4.4.2.3.cmml">,</mo><mi id="S5.SS1.p2.4.m4.4.4.2.2" xref="S5.SS1.p2.4.m4.4.4.2.2.cmml">t</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.4.m4.7.7.1.1.1.1.1" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.1.cmml">×</mo><msub id="S5.SS1.p2.4.m4.7.7.1.1.1.1.3" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.cmml"><mi id="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.2" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.2.cmml">E</mi><mrow id="S5.SS1.p2.4.m4.6.6.2.4" xref="S5.SS1.p2.4.m4.6.6.2.3.cmml"><mi id="S5.SS1.p2.4.m4.5.5.1.1" xref="S5.SS1.p2.4.m4.5.5.1.1.cmml">i</mi><mo id="S5.SS1.p2.4.m4.6.6.2.4.1" xref="S5.SS1.p2.4.m4.6.6.2.3.cmml">,</mo><mi id="S5.SS1.p2.4.m4.6.6.2.2" xref="S5.SS1.p2.4.m4.6.6.2.2.cmml">t</mi></mrow></msub></mrow><mo stretchy="false" id="S5.SS1.p2.4.m4.7.7.1.1.1.3" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.7b"><apply id="S5.SS1.p2.4.m4.7.7.cmml" xref="S5.SS1.p2.4.m4.7.7"><eq id="S5.SS1.p2.4.m4.7.7.2.cmml" xref="S5.SS1.p2.4.m4.7.7.2"></eq><apply id="S5.SS1.p2.4.m4.7.7.3.cmml" xref="S5.SS1.p2.4.m4.7.7.3"><times id="S5.SS1.p2.4.m4.7.7.3.1.cmml" xref="S5.SS1.p2.4.m4.7.7.3.1"></times><ci id="S5.SS1.p2.4.m4.7.7.3.2.cmml" xref="S5.SS1.p2.4.m4.7.7.3.2">𝑅</ci><apply id="S5.SS1.p2.4.m4.7.7.3.3.cmml" xref="S5.SS1.p2.4.m4.7.7.3.3"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.7.7.3.3.1.cmml" xref="S5.SS1.p2.4.m4.7.7.3.3">subscript</csymbol><ci id="S5.SS1.p2.4.m4.7.7.3.3.2.cmml" xref="S5.SS1.p2.4.m4.7.7.3.3.2">𝐶</ci><list id="S5.SS1.p2.4.m4.2.2.2.3.cmml" xref="S5.SS1.p2.4.m4.2.2.2.4"><ci id="S5.SS1.p2.4.m4.1.1.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1.1.1">𝑖</ci><ci id="S5.SS1.p2.4.m4.2.2.2.2.cmml" xref="S5.SS1.p2.4.m4.2.2.2.2">𝑡</ci></list></apply></apply><apply id="S5.SS1.p2.4.m4.7.7.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1"><minus id="S5.SS1.p2.4.m4.7.7.1.2.cmml" xref="S5.SS1.p2.4.m4.7.7.1.2"></minus><apply id="S5.SS1.p2.4.m4.7.7.1.3.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3"><times id="S5.SS1.p2.4.m4.7.7.1.3.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3.1"></times><ci id="S5.SS1.p2.4.m4.7.7.1.3.2.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3.2">𝑀</ci><apply id="S5.SS1.p2.4.m4.7.7.1.3.3.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3.3"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.7.7.1.3.3.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3.3">subscript</csymbol><ci id="S5.SS1.p2.4.m4.7.7.1.3.3.2.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3.3.2">𝐶</ci><ci id="S5.SS1.p2.4.m4.7.7.1.3.3.3.cmml" xref="S5.SS1.p2.4.m4.7.7.1.3.3.3">𝑖</ci></apply></apply><apply id="S5.SS1.p2.4.m4.7.7.1.1.1.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1"><times id="S5.SS1.p2.4.m4.7.7.1.1.1.1.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.1"></times><apply id="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.2">subscript</csymbol><ci id="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.2.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.2.2">𝑘</ci><list id="S5.SS1.p2.4.m4.4.4.2.3.cmml" xref="S5.SS1.p2.4.m4.4.4.2.4"><ci id="S5.SS1.p2.4.m4.3.3.1.1.cmml" xref="S5.SS1.p2.4.m4.3.3.1.1">𝑖</ci><ci id="S5.SS1.p2.4.m4.4.4.2.2.cmml" xref="S5.SS1.p2.4.m4.4.4.2.2">𝑡</ci></list></apply><apply id="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.1.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.3">subscript</csymbol><ci id="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.2.cmml" xref="S5.SS1.p2.4.m4.7.7.1.1.1.1.3.2">𝐸</ci><list id="S5.SS1.p2.4.m4.6.6.2.3.cmml" xref="S5.SS1.p2.4.m4.6.6.2.4"><ci id="S5.SS1.p2.4.m4.5.5.1.1.cmml" xref="S5.SS1.p2.4.m4.5.5.1.1">𝑖</ci><ci id="S5.SS1.p2.4.m4.6.6.2.2.cmml" xref="S5.SS1.p2.4.m4.6.6.2.2">𝑡</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.7c">RC_{i,t}=MC_{i}-(k_{i,t}\times E_{i,t})</annotation></semantics></math>. Here, <math id="S5.SS1.p2.5.m5.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="S5.SS1.p2.5.m5.1a"><mrow id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml"><mi id="S5.SS1.p2.5.m5.1.1.2" xref="S5.SS1.p2.5.m5.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.5.m5.1.1.1" xref="S5.SS1.p2.5.m5.1.1.1.cmml">​</mo><msub id="S5.SS1.p2.5.m5.1.1.3" xref="S5.SS1.p2.5.m5.1.1.3.cmml"><mi id="S5.SS1.p2.5.m5.1.1.3.2" xref="S5.SS1.p2.5.m5.1.1.3.2.cmml">C</mi><mi id="S5.SS1.p2.5.m5.1.1.3.3" xref="S5.SS1.p2.5.m5.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><apply id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1"><times id="S5.SS1.p2.5.m5.1.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1.1"></times><ci id="S5.SS1.p2.5.m5.1.1.2.cmml" xref="S5.SS1.p2.5.m5.1.1.2">𝑀</ci><apply id="S5.SS1.p2.5.m5.1.1.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.5.m5.1.1.3.1.cmml" xref="S5.SS1.p2.5.m5.1.1.3">subscript</csymbol><ci id="S5.SS1.p2.5.m5.1.1.3.2.cmml" xref="S5.SS1.p2.5.m5.1.1.3.2">𝐶</ci><ci id="S5.SS1.p2.5.m5.1.1.3.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">MC_{i}</annotation></semantics></math> represents the maximum service capacity<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We compute <math id="footnote5.m1.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="footnote5.m1.1b"><mrow id="footnote5.m1.1.1" xref="footnote5.m1.1.1.cmml"><mi id="footnote5.m1.1.1.2" xref="footnote5.m1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="footnote5.m1.1.1.1" xref="footnote5.m1.1.1.1.cmml">​</mo><msub id="footnote5.m1.1.1.3" xref="footnote5.m1.1.1.3.cmml"><mi id="footnote5.m1.1.1.3.2" xref="footnote5.m1.1.1.3.2.cmml">C</mi><mi id="footnote5.m1.1.1.3.3" xref="footnote5.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="footnote5.m1.1c"><apply id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1"><times id="footnote5.m1.1.1.1.cmml" xref="footnote5.m1.1.1.1"></times><ci id="footnote5.m1.1.1.2.cmml" xref="footnote5.m1.1.1.2">𝑀</ci><apply id="footnote5.m1.1.1.3.cmml" xref="footnote5.m1.1.1.3"><csymbol cd="ambiguous" id="footnote5.m1.1.1.3.1.cmml" xref="footnote5.m1.1.1.3">subscript</csymbol><ci id="footnote5.m1.1.1.3.2.cmml" xref="footnote5.m1.1.1.3.2">𝐶</ci><ci id="footnote5.m1.1.1.3.3.cmml" xref="footnote5.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.1d">MC_{i}</annotation></semantics></math> offline; for details, refer to Appendix-<a href="#A5" title="Appendix E Maximum Service Capacity of Worker Nodes" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</span></span></span>, denoting the maximum number of model updates that can be aggregated simultaneously on worker node <math id="S5.SS1.p2.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.6.m6.1a"><mi id="S5.SS1.p2.6.m6.1.1" xref="S5.SS1.p2.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.6.m6.1b"><ci id="S5.SS1.p2.6.m6.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.6.m6.1c">i</annotation></semantics></math>. The value of <math id="S5.SS1.p2.7.m7.2" class="ltx_Math" alttext="k_{i,t}" display="inline"><semantics id="S5.SS1.p2.7.m7.2a"><msub id="S5.SS1.p2.7.m7.2.3" xref="S5.SS1.p2.7.m7.2.3.cmml"><mi id="S5.SS1.p2.7.m7.2.3.2" xref="S5.SS1.p2.7.m7.2.3.2.cmml">k</mi><mrow id="S5.SS1.p2.7.m7.2.2.2.4" xref="S5.SS1.p2.7.m7.2.2.2.3.cmml"><mi id="S5.SS1.p2.7.m7.1.1.1.1" xref="S5.SS1.p2.7.m7.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p2.7.m7.2.2.2.4.1" xref="S5.SS1.p2.7.m7.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.7.m7.2.2.2.2" xref="S5.SS1.p2.7.m7.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.7.m7.2b"><apply id="S5.SS1.p2.7.m7.2.3.cmml" xref="S5.SS1.p2.7.m7.2.3"><csymbol cd="ambiguous" id="S5.SS1.p2.7.m7.2.3.1.cmml" xref="S5.SS1.p2.7.m7.2.3">subscript</csymbol><ci id="S5.SS1.p2.7.m7.2.3.2.cmml" xref="S5.SS1.p2.7.m7.2.3.2">𝑘</ci><list id="S5.SS1.p2.7.m7.2.2.2.3.cmml" xref="S5.SS1.p2.7.m7.2.2.2.4"><ci id="S5.SS1.p2.7.m7.1.1.1.1.cmml" xref="S5.SS1.p2.7.m7.1.1.1.1">𝑖</ci><ci id="S5.SS1.p2.7.m7.2.2.2.2.cmml" xref="S5.SS1.p2.7.m7.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.7.m7.2c">k_{i,t}</annotation></semantics></math> is the arrival rate of model updates directed to worker node <math id="S5.SS1.p2.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.8.m8.1a"><mi id="S5.SS1.p2.8.m8.1.1" xref="S5.SS1.p2.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.8.m8.1b"><ci id="S5.SS1.p2.8.m8.1.1.cmml" xref="S5.SS1.p2.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.8.m8.1c">i</annotation></semantics></math> at time <math id="S5.SS1.p2.9.m9.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S5.SS1.p2.9.m9.1a"><mi id="S5.SS1.p2.9.m9.1.1" xref="S5.SS1.p2.9.m9.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.9.m9.1b"><ci id="S5.SS1.p2.9.m9.1.1.cmml" xref="S5.SS1.p2.9.m9.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.9.m9.1c">t</annotation></semantics></math>, and <math id="S5.SS1.p2.10.m10.2" class="ltx_Math" alttext="E_{i,t}" display="inline"><semantics id="S5.SS1.p2.10.m10.2a"><msub id="S5.SS1.p2.10.m10.2.3" xref="S5.SS1.p2.10.m10.2.3.cmml"><mi id="S5.SS1.p2.10.m10.2.3.2" xref="S5.SS1.p2.10.m10.2.3.2.cmml">E</mi><mrow id="S5.SS1.p2.10.m10.2.2.2.4" xref="S5.SS1.p2.10.m10.2.2.2.3.cmml"><mi id="S5.SS1.p2.10.m10.1.1.1.1" xref="S5.SS1.p2.10.m10.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p2.10.m10.2.2.2.4.1" xref="S5.SS1.p2.10.m10.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.10.m10.2.2.2.2" xref="S5.SS1.p2.10.m10.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.10.m10.2b"><apply id="S5.SS1.p2.10.m10.2.3.cmml" xref="S5.SS1.p2.10.m10.2.3"><csymbol cd="ambiguous" id="S5.SS1.p2.10.m10.2.3.1.cmml" xref="S5.SS1.p2.10.m10.2.3">subscript</csymbol><ci id="S5.SS1.p2.10.m10.2.3.2.cmml" xref="S5.SS1.p2.10.m10.2.3.2">𝐸</ci><list id="S5.SS1.p2.10.m10.2.2.2.3.cmml" xref="S5.SS1.p2.10.m10.2.2.2.4"><ci id="S5.SS1.p2.10.m10.1.1.1.1.cmml" xref="S5.SS1.p2.10.m10.1.1.1.1">𝑖</ci><ci id="S5.SS1.p2.10.m10.2.2.2.2.cmml" xref="S5.SS1.p2.10.m10.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.10.m10.2c">E_{i,t}</annotation></semantics></math> is the average execution time required to aggregate a model update
on node <math id="S5.SS1.p2.11.m11.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.11.m11.1a"><mi id="S5.SS1.p2.11.m11.1.1" xref="S5.SS1.p2.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.11.m11.1b"><ci id="S5.SS1.p2.11.m11.1.1.cmml" xref="S5.SS1.p2.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.11.m11.1c">i</annotation></semantics></math>. We can also get a coarse-grained estimate on the queue length (<math id="S5.SS1.p2.12.m12.6" class="ltx_Math" alttext="Q_{i,t}=k_{i,t}\times E_{i,t}" display="inline"><semantics id="S5.SS1.p2.12.m12.6a"><mrow id="S5.SS1.p2.12.m12.6.7" xref="S5.SS1.p2.12.m12.6.7.cmml"><msub id="S5.SS1.p2.12.m12.6.7.2" xref="S5.SS1.p2.12.m12.6.7.2.cmml"><mi id="S5.SS1.p2.12.m12.6.7.2.2" xref="S5.SS1.p2.12.m12.6.7.2.2.cmml">Q</mi><mrow id="S5.SS1.p2.12.m12.2.2.2.4" xref="S5.SS1.p2.12.m12.2.2.2.3.cmml"><mi id="S5.SS1.p2.12.m12.1.1.1.1" xref="S5.SS1.p2.12.m12.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p2.12.m12.2.2.2.4.1" xref="S5.SS1.p2.12.m12.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.12.m12.2.2.2.2" xref="S5.SS1.p2.12.m12.2.2.2.2.cmml">t</mi></mrow></msub><mo id="S5.SS1.p2.12.m12.6.7.1" xref="S5.SS1.p2.12.m12.6.7.1.cmml">=</mo><mrow id="S5.SS1.p2.12.m12.6.7.3" xref="S5.SS1.p2.12.m12.6.7.3.cmml"><msub id="S5.SS1.p2.12.m12.6.7.3.2" xref="S5.SS1.p2.12.m12.6.7.3.2.cmml"><mi id="S5.SS1.p2.12.m12.6.7.3.2.2" xref="S5.SS1.p2.12.m12.6.7.3.2.2.cmml">k</mi><mrow id="S5.SS1.p2.12.m12.4.4.2.4" xref="S5.SS1.p2.12.m12.4.4.2.3.cmml"><mi id="S5.SS1.p2.12.m12.3.3.1.1" xref="S5.SS1.p2.12.m12.3.3.1.1.cmml">i</mi><mo id="S5.SS1.p2.12.m12.4.4.2.4.1" xref="S5.SS1.p2.12.m12.4.4.2.3.cmml">,</mo><mi id="S5.SS1.p2.12.m12.4.4.2.2" xref="S5.SS1.p2.12.m12.4.4.2.2.cmml">t</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.12.m12.6.7.3.1" xref="S5.SS1.p2.12.m12.6.7.3.1.cmml">×</mo><msub id="S5.SS1.p2.12.m12.6.7.3.3" xref="S5.SS1.p2.12.m12.6.7.3.3.cmml"><mi id="S5.SS1.p2.12.m12.6.7.3.3.2" xref="S5.SS1.p2.12.m12.6.7.3.3.2.cmml">E</mi><mrow id="S5.SS1.p2.12.m12.6.6.2.4" xref="S5.SS1.p2.12.m12.6.6.2.3.cmml"><mi id="S5.SS1.p2.12.m12.5.5.1.1" xref="S5.SS1.p2.12.m12.5.5.1.1.cmml">i</mi><mo id="S5.SS1.p2.12.m12.6.6.2.4.1" xref="S5.SS1.p2.12.m12.6.6.2.3.cmml">,</mo><mi id="S5.SS1.p2.12.m12.6.6.2.2" xref="S5.SS1.p2.12.m12.6.6.2.2.cmml">t</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.12.m12.6b"><apply id="S5.SS1.p2.12.m12.6.7.cmml" xref="S5.SS1.p2.12.m12.6.7"><eq id="S5.SS1.p2.12.m12.6.7.1.cmml" xref="S5.SS1.p2.12.m12.6.7.1"></eq><apply id="S5.SS1.p2.12.m12.6.7.2.cmml" xref="S5.SS1.p2.12.m12.6.7.2"><csymbol cd="ambiguous" id="S5.SS1.p2.12.m12.6.7.2.1.cmml" xref="S5.SS1.p2.12.m12.6.7.2">subscript</csymbol><ci id="S5.SS1.p2.12.m12.6.7.2.2.cmml" xref="S5.SS1.p2.12.m12.6.7.2.2">𝑄</ci><list id="S5.SS1.p2.12.m12.2.2.2.3.cmml" xref="S5.SS1.p2.12.m12.2.2.2.4"><ci id="S5.SS1.p2.12.m12.1.1.1.1.cmml" xref="S5.SS1.p2.12.m12.1.1.1.1">𝑖</ci><ci id="S5.SS1.p2.12.m12.2.2.2.2.cmml" xref="S5.SS1.p2.12.m12.2.2.2.2">𝑡</ci></list></apply><apply id="S5.SS1.p2.12.m12.6.7.3.cmml" xref="S5.SS1.p2.12.m12.6.7.3"><times id="S5.SS1.p2.12.m12.6.7.3.1.cmml" xref="S5.SS1.p2.12.m12.6.7.3.1"></times><apply id="S5.SS1.p2.12.m12.6.7.3.2.cmml" xref="S5.SS1.p2.12.m12.6.7.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.12.m12.6.7.3.2.1.cmml" xref="S5.SS1.p2.12.m12.6.7.3.2">subscript</csymbol><ci id="S5.SS1.p2.12.m12.6.7.3.2.2.cmml" xref="S5.SS1.p2.12.m12.6.7.3.2.2">𝑘</ci><list id="S5.SS1.p2.12.m12.4.4.2.3.cmml" xref="S5.SS1.p2.12.m12.4.4.2.4"><ci id="S5.SS1.p2.12.m12.3.3.1.1.cmml" xref="S5.SS1.p2.12.m12.3.3.1.1">𝑖</ci><ci id="S5.SS1.p2.12.m12.4.4.2.2.cmml" xref="S5.SS1.p2.12.m12.4.4.2.2">𝑡</ci></list></apply><apply id="S5.SS1.p2.12.m12.6.7.3.3.cmml" xref="S5.SS1.p2.12.m12.6.7.3.3"><csymbol cd="ambiguous" id="S5.SS1.p2.12.m12.6.7.3.3.1.cmml" xref="S5.SS1.p2.12.m12.6.7.3.3">subscript</csymbol><ci id="S5.SS1.p2.12.m12.6.7.3.3.2.cmml" xref="S5.SS1.p2.12.m12.6.7.3.3.2">𝐸</ci><list id="S5.SS1.p2.12.m12.6.6.2.3.cmml" xref="S5.SS1.p2.12.m12.6.6.2.4"><ci id="S5.SS1.p2.12.m12.5.5.1.1.cmml" xref="S5.SS1.p2.12.m12.5.5.1.1">𝑖</ci><ci id="S5.SS1.p2.12.m12.6.6.2.2.cmml" xref="S5.SS1.p2.12.m12.6.6.2.2">𝑡</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.12.m12.6c">Q_{i,t}=k_{i,t}\times E_{i,t}</annotation></semantics></math>) of node <math id="S5.SS1.p2.13.m13.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.13.m13.1a"><mi id="S5.SS1.p2.13.m13.1.1" xref="S5.SS1.p2.13.m13.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.13.m13.1b"><ci id="S5.SS1.p2.13.m13.1.1.cmml" xref="S5.SS1.p2.13.m13.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.13.m13.1c">i</annotation></semantics></math> at time <math id="S5.SS1.p2.14.m14.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S5.SS1.p2.14.m14.1a"><mi id="S5.SS1.p2.14.m14.1.1" xref="S5.SS1.p2.14.m14.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.14.m14.1b"><ci id="S5.SS1.p2.14.m14.1.1.cmml" xref="S5.SS1.p2.14.m14.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.14.m14.1c">t</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">We approach the load balancing task as a bin-packing problem, aiming to
allocate model updates from clients
to a minimal number of worker nodes, while ensuring that
the residual service capacity of each worker node is not exceeded. This naturally reduces the inter-node communication as much as possible, since the communication between a particular pair of worker nodes only happens once. We use <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">BestFit</span> for the bin-packing, as it concentrates load onto the fewest nodes possible, to reduce inter-node traffic and maximize shared memory use. In contrast, <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">WorstFit</span> spreads the load across more nodes, similar to the “Least Connection” policy in Knative (§<a href="#S6.SS1" title="6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>). Furthermore, <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">FirstFit</span> focuses on reducing search complexity without being locality-aware.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2405.10968/assets/x7.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Control plane orchestration in LIFL: The autoscaler periodically re-plans the hierarchy based on the arrival rate of each worker node. The LIFL coordinator applies reusing of aggregators.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Planning the Hierarchy for Aggregation</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">The goal of hierarchy-aware autoscaling is to maximize the parallelism of aggregation at each level, given the number of model updates to be aggregated.
This can minimize the completion time of each level and thus minimize the aggregation completion time (ACT) for hierarchical aggregation.
We plan a hierarchical aggregation structure within each node, tailored to the number of pending model updates (<math id="S5.SS2.p1.1.m1.2" class="ltx_Math" alttext="Q_{i,t}" display="inline"><semantics id="S5.SS2.p1.1.m1.2a"><msub id="S5.SS2.p1.1.m1.2.3" xref="S5.SS2.p1.1.m1.2.3.cmml"><mi id="S5.SS2.p1.1.m1.2.3.2" xref="S5.SS2.p1.1.m1.2.3.2.cmml">Q</mi><mrow id="S5.SS2.p1.1.m1.2.2.2.4" xref="S5.SS2.p1.1.m1.2.2.2.3.cmml"><mi id="S5.SS2.p1.1.m1.1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p1.1.m1.2.2.2.4.1" xref="S5.SS2.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p1.1.m1.2.2.2.2" xref="S5.SS2.p1.1.m1.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.2b"><apply id="S5.SS2.p1.1.m1.2.3.cmml" xref="S5.SS2.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.2.3.1.cmml" xref="S5.SS2.p1.1.m1.2.3">subscript</csymbol><ci id="S5.SS2.p1.1.m1.2.3.2.cmml" xref="S5.SS2.p1.1.m1.2.3.2">𝑄</ci><list id="S5.SS2.p1.1.m1.2.2.2.3.cmml" xref="S5.SS2.p1.1.m1.2.2.2.4"><ci id="S5.SS2.p1.1.m1.1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1">𝑖</ci><ci id="S5.SS2.p1.1.m1.2.2.2.2.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.2c">Q_{i,t}</annotation></semantics></math>) in the message queue.
Every node produces an intermediate model update that is dispatched to the node chosen to have the top aggregator that updates the global model.
This approach significantly reduces the need for cross-node transfers for intermediate model updates.
</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.10" class="ltx_p">LIFL periodically adjusts (<span id="S5.SS2.p2.10.1" class="ltx_text ltx_font_italic">i.e., </span>scales) the hierarchy on node <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">i</annotation></semantics></math>, guided by our estimates of <math id="S5.SS2.p2.2.m2.2" class="ltx_Math" alttext="Q_{i,t}" display="inline"><semantics id="S5.SS2.p2.2.m2.2a"><msub id="S5.SS2.p2.2.m2.2.3" xref="S5.SS2.p2.2.m2.2.3.cmml"><mi id="S5.SS2.p2.2.m2.2.3.2" xref="S5.SS2.p2.2.m2.2.3.2.cmml">Q</mi><mrow id="S5.SS2.p2.2.m2.2.2.2.4" xref="S5.SS2.p2.2.m2.2.2.2.3.cmml"><mi id="S5.SS2.p2.2.m2.1.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p2.2.m2.2.2.2.4.1" xref="S5.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p2.2.m2.2.2.2.2" xref="S5.SS2.p2.2.m2.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.2b"><apply id="S5.SS2.p2.2.m2.2.3.cmml" xref="S5.SS2.p2.2.m2.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.2.3.1.cmml" xref="S5.SS2.p2.2.m2.2.3">subscript</csymbol><ci id="S5.SS2.p2.2.m2.2.3.2.cmml" xref="S5.SS2.p2.2.m2.2.3.2">𝑄</ci><list id="S5.SS2.p2.2.m2.2.2.2.3.cmml" xref="S5.SS2.p2.2.m2.2.2.2.4"><ci id="S5.SS2.p2.2.m2.1.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1">𝑖</ci><ci id="S5.SS2.p2.2.m2.2.2.2.2.cmml" xref="S5.SS2.p2.2.m2.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.2c">Q_{i,t}</annotation></semantics></math>.
To prevent excess resource allocation due to short-term spikes in <math id="S5.SS2.p2.3.m3.2" class="ltx_Math" alttext="Q_{i,t}" display="inline"><semantics id="S5.SS2.p2.3.m3.2a"><msub id="S5.SS2.p2.3.m3.2.3" xref="S5.SS2.p2.3.m3.2.3.cmml"><mi id="S5.SS2.p2.3.m3.2.3.2" xref="S5.SS2.p2.3.m3.2.3.2.cmml">Q</mi><mrow id="S5.SS2.p2.3.m3.2.2.2.4" xref="S5.SS2.p2.3.m3.2.2.2.3.cmml"><mi id="S5.SS2.p2.3.m3.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p2.3.m3.2.2.2.4.1" xref="S5.SS2.p2.3.m3.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p2.3.m3.2.2.2.2" xref="S5.SS2.p2.3.m3.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.2b"><apply id="S5.SS2.p2.3.m3.2.3.cmml" xref="S5.SS2.p2.3.m3.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.2.3.1.cmml" xref="S5.SS2.p2.3.m3.2.3">subscript</csymbol><ci id="S5.SS2.p2.3.m3.2.3.2.cmml" xref="S5.SS2.p2.3.m3.2.3.2">𝑄</ci><list id="S5.SS2.p2.3.m3.2.2.2.3.cmml" xref="S5.SS2.p2.3.m3.2.2.2.4"><ci id="S5.SS2.p2.3.m3.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1">𝑖</ci><ci id="S5.SS2.p2.3.m3.2.2.2.2.cmml" xref="S5.SS2.p2.3.m3.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.2c">Q_{i,t}</annotation></semantics></math>, we employ the Exponentially Weighted Moving Average (EWMA) to smooth <math id="S5.SS2.p2.4.m4.2" class="ltx_Math" alttext="Q_{i,t}" display="inline"><semantics id="S5.SS2.p2.4.m4.2a"><msub id="S5.SS2.p2.4.m4.2.3" xref="S5.SS2.p2.4.m4.2.3.cmml"><mi id="S5.SS2.p2.4.m4.2.3.2" xref="S5.SS2.p2.4.m4.2.3.2.cmml">Q</mi><mrow id="S5.SS2.p2.4.m4.2.2.2.4" xref="S5.SS2.p2.4.m4.2.2.2.3.cmml"><mi id="S5.SS2.p2.4.m4.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p2.4.m4.2.2.2.4.1" xref="S5.SS2.p2.4.m4.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p2.4.m4.2.2.2.2" xref="S5.SS2.p2.4.m4.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.2b"><apply id="S5.SS2.p2.4.m4.2.3.cmml" xref="S5.SS2.p2.4.m4.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.2.3.1.cmml" xref="S5.SS2.p2.4.m4.2.3">subscript</csymbol><ci id="S5.SS2.p2.4.m4.2.3.2.cmml" xref="S5.SS2.p2.4.m4.2.3.2">𝑄</ci><list id="S5.SS2.p2.4.m4.2.2.2.3.cmml" xref="S5.SS2.p2.4.m4.2.2.2.4"><ci id="S5.SS2.p2.4.m4.1.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1">𝑖</ci><ci id="S5.SS2.p2.4.m4.2.2.2.2.cmml" xref="S5.SS2.p2.4.m4.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.2c">Q_{i,t}</annotation></semantics></math>: <math id="S5.SS2.p2.5.m5.7" class="ltx_Math" alttext="Q_{i,t}=\alpha\times Q_{i,t-1}+(1-\alpha)\times Q_{i,t}" display="inline"><semantics id="S5.SS2.p2.5.m5.7a"><mrow id="S5.SS2.p2.5.m5.7.7" xref="S5.SS2.p2.5.m5.7.7.cmml"><msub id="S5.SS2.p2.5.m5.7.7.3" xref="S5.SS2.p2.5.m5.7.7.3.cmml"><mi id="S5.SS2.p2.5.m5.7.7.3.2" xref="S5.SS2.p2.5.m5.7.7.3.2.cmml">Q</mi><mrow id="S5.SS2.p2.5.m5.2.2.2.4" xref="S5.SS2.p2.5.m5.2.2.2.3.cmml"><mi id="S5.SS2.p2.5.m5.1.1.1.1" xref="S5.SS2.p2.5.m5.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p2.5.m5.2.2.2.4.1" xref="S5.SS2.p2.5.m5.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p2.5.m5.2.2.2.2" xref="S5.SS2.p2.5.m5.2.2.2.2.cmml">t</mi></mrow></msub><mo id="S5.SS2.p2.5.m5.7.7.2" xref="S5.SS2.p2.5.m5.7.7.2.cmml">=</mo><mrow id="S5.SS2.p2.5.m5.7.7.1" xref="S5.SS2.p2.5.m5.7.7.1.cmml"><mrow id="S5.SS2.p2.5.m5.7.7.1.3" xref="S5.SS2.p2.5.m5.7.7.1.3.cmml"><mi id="S5.SS2.p2.5.m5.7.7.1.3.2" xref="S5.SS2.p2.5.m5.7.7.1.3.2.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p2.5.m5.7.7.1.3.1" xref="S5.SS2.p2.5.m5.7.7.1.3.1.cmml">×</mo><msub id="S5.SS2.p2.5.m5.7.7.1.3.3" xref="S5.SS2.p2.5.m5.7.7.1.3.3.cmml"><mi id="S5.SS2.p2.5.m5.7.7.1.3.3.2" xref="S5.SS2.p2.5.m5.7.7.1.3.3.2.cmml">Q</mi><mrow id="S5.SS2.p2.5.m5.4.4.2.2" xref="S5.SS2.p2.5.m5.4.4.2.3.cmml"><mi id="S5.SS2.p2.5.m5.3.3.1.1" xref="S5.SS2.p2.5.m5.3.3.1.1.cmml">i</mi><mo id="S5.SS2.p2.5.m5.4.4.2.2.2" xref="S5.SS2.p2.5.m5.4.4.2.3.cmml">,</mo><mrow id="S5.SS2.p2.5.m5.4.4.2.2.1" xref="S5.SS2.p2.5.m5.4.4.2.2.1.cmml"><mi id="S5.SS2.p2.5.m5.4.4.2.2.1.2" xref="S5.SS2.p2.5.m5.4.4.2.2.1.2.cmml">t</mi><mo id="S5.SS2.p2.5.m5.4.4.2.2.1.1" xref="S5.SS2.p2.5.m5.4.4.2.2.1.1.cmml">−</mo><mn id="S5.SS2.p2.5.m5.4.4.2.2.1.3" xref="S5.SS2.p2.5.m5.4.4.2.2.1.3.cmml">1</mn></mrow></mrow></msub></mrow><mo id="S5.SS2.p2.5.m5.7.7.1.2" xref="S5.SS2.p2.5.m5.7.7.1.2.cmml">+</mo><mrow id="S5.SS2.p2.5.m5.7.7.1.1" xref="S5.SS2.p2.5.m5.7.7.1.1.cmml"><mrow id="S5.SS2.p2.5.m5.7.7.1.1.1.1" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS2.p2.5.m5.7.7.1.1.1.1.2" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.cmml"><mn id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.2" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.2.cmml">1</mn><mo id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.1" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.1.cmml">−</mo><mi id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.3" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.3.cmml">α</mi></mrow><mo rspace="0.055em" stretchy="false" id="S5.SS2.p2.5.m5.7.7.1.1.1.1.3" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S5.SS2.p2.5.m5.7.7.1.1.2" xref="S5.SS2.p2.5.m5.7.7.1.1.2.cmml">×</mo><msub id="S5.SS2.p2.5.m5.7.7.1.1.3" xref="S5.SS2.p2.5.m5.7.7.1.1.3.cmml"><mi id="S5.SS2.p2.5.m5.7.7.1.1.3.2" xref="S5.SS2.p2.5.m5.7.7.1.1.3.2.cmml">Q</mi><mrow id="S5.SS2.p2.5.m5.6.6.2.4" xref="S5.SS2.p2.5.m5.6.6.2.3.cmml"><mi id="S5.SS2.p2.5.m5.5.5.1.1" xref="S5.SS2.p2.5.m5.5.5.1.1.cmml">i</mi><mo id="S5.SS2.p2.5.m5.6.6.2.4.1" xref="S5.SS2.p2.5.m5.6.6.2.3.cmml">,</mo><mi id="S5.SS2.p2.5.m5.6.6.2.2" xref="S5.SS2.p2.5.m5.6.6.2.2.cmml">t</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.7b"><apply id="S5.SS2.p2.5.m5.7.7.cmml" xref="S5.SS2.p2.5.m5.7.7"><eq id="S5.SS2.p2.5.m5.7.7.2.cmml" xref="S5.SS2.p2.5.m5.7.7.2"></eq><apply id="S5.SS2.p2.5.m5.7.7.3.cmml" xref="S5.SS2.p2.5.m5.7.7.3"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.7.7.3.1.cmml" xref="S5.SS2.p2.5.m5.7.7.3">subscript</csymbol><ci id="S5.SS2.p2.5.m5.7.7.3.2.cmml" xref="S5.SS2.p2.5.m5.7.7.3.2">𝑄</ci><list id="S5.SS2.p2.5.m5.2.2.2.3.cmml" xref="S5.SS2.p2.5.m5.2.2.2.4"><ci id="S5.SS2.p2.5.m5.1.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1.1.1">𝑖</ci><ci id="S5.SS2.p2.5.m5.2.2.2.2.cmml" xref="S5.SS2.p2.5.m5.2.2.2.2">𝑡</ci></list></apply><apply id="S5.SS2.p2.5.m5.7.7.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1"><plus id="S5.SS2.p2.5.m5.7.7.1.2.cmml" xref="S5.SS2.p2.5.m5.7.7.1.2"></plus><apply id="S5.SS2.p2.5.m5.7.7.1.3.cmml" xref="S5.SS2.p2.5.m5.7.7.1.3"><times id="S5.SS2.p2.5.m5.7.7.1.3.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1.3.1"></times><ci id="S5.SS2.p2.5.m5.7.7.1.3.2.cmml" xref="S5.SS2.p2.5.m5.7.7.1.3.2">𝛼</ci><apply id="S5.SS2.p2.5.m5.7.7.1.3.3.cmml" xref="S5.SS2.p2.5.m5.7.7.1.3.3"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.7.7.1.3.3.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1.3.3">subscript</csymbol><ci id="S5.SS2.p2.5.m5.7.7.1.3.3.2.cmml" xref="S5.SS2.p2.5.m5.7.7.1.3.3.2">𝑄</ci><list id="S5.SS2.p2.5.m5.4.4.2.3.cmml" xref="S5.SS2.p2.5.m5.4.4.2.2"><ci id="S5.SS2.p2.5.m5.3.3.1.1.cmml" xref="S5.SS2.p2.5.m5.3.3.1.1">𝑖</ci><apply id="S5.SS2.p2.5.m5.4.4.2.2.1.cmml" xref="S5.SS2.p2.5.m5.4.4.2.2.1"><minus id="S5.SS2.p2.5.m5.4.4.2.2.1.1.cmml" xref="S5.SS2.p2.5.m5.4.4.2.2.1.1"></minus><ci id="S5.SS2.p2.5.m5.4.4.2.2.1.2.cmml" xref="S5.SS2.p2.5.m5.4.4.2.2.1.2">𝑡</ci><cn type="integer" id="S5.SS2.p2.5.m5.4.4.2.2.1.3.cmml" xref="S5.SS2.p2.5.m5.4.4.2.2.1.3">1</cn></apply></list></apply></apply><apply id="S5.SS2.p2.5.m5.7.7.1.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1"><times id="S5.SS2.p2.5.m5.7.7.1.1.2.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.2"></times><apply id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1"><minus id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.1"></minus><cn type="integer" id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.2">1</cn><ci id="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.1.1.1.3">𝛼</ci></apply><apply id="S5.SS2.p2.5.m5.7.7.1.1.3.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.7.7.1.1.3.1.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.3">subscript</csymbol><ci id="S5.SS2.p2.5.m5.7.7.1.1.3.2.cmml" xref="S5.SS2.p2.5.m5.7.7.1.1.3.2">𝑄</ci><list id="S5.SS2.p2.5.m5.6.6.2.3.cmml" xref="S5.SS2.p2.5.m5.6.6.2.4"><ci id="S5.SS2.p2.5.m5.5.5.1.1.cmml" xref="S5.SS2.p2.5.m5.5.5.1.1">𝑖</ci><ci id="S5.SS2.p2.5.m5.6.6.2.2.cmml" xref="S5.SS2.p2.5.m5.6.6.2.2">𝑡</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.7c">Q_{i,t}=\alpha\times Q_{i,t-1}+(1-\alpha)\times Q_{i,t}</annotation></semantics></math>, where <math id="S5.SS2.p2.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><mi id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><ci id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">\alpha</annotation></semantics></math> is the EWMA coefficient. We set <math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="\alpha=0.7" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><mrow id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml"><mi id="S5.SS2.p2.7.m7.1.1.2" xref="S5.SS2.p2.7.m7.1.1.2.cmml">α</mi><mo id="S5.SS2.p2.7.m7.1.1.1" xref="S5.SS2.p2.7.m7.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.7.m7.1.1.3" xref="S5.SS2.p2.7.m7.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><apply id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1"><eq id="S5.SS2.p2.7.m7.1.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1.1"></eq><ci id="S5.SS2.p2.7.m7.1.1.2.cmml" xref="S5.SS2.p2.7.m7.1.1.2">𝛼</ci><cn type="float" id="S5.SS2.p2.7.m7.1.1.3.cmml" xref="S5.SS2.p2.7.m7.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">\alpha=0.7</annotation></semantics></math> based on it yielding the best results in our experiments.
Our current implementation supports a two-level <span id="S5.SS2.p2.10.2" class="ltx_text ltx_font_italic">k</span>-ary tree hierarchy on each node, comprising a “central” middle aggregator responsible for aggregating model updates from <math id="S5.SS2.p2.8.m8.2" class="ltx_Math" alttext="Q_{i,t}/I" display="inline"><semantics id="S5.SS2.p2.8.m8.2a"><mrow id="S5.SS2.p2.8.m8.2.3" xref="S5.SS2.p2.8.m8.2.3.cmml"><msub id="S5.SS2.p2.8.m8.2.3.2" xref="S5.SS2.p2.8.m8.2.3.2.cmml"><mi id="S5.SS2.p2.8.m8.2.3.2.2" xref="S5.SS2.p2.8.m8.2.3.2.2.cmml">Q</mi><mrow id="S5.SS2.p2.8.m8.2.2.2.4" xref="S5.SS2.p2.8.m8.2.2.2.3.cmml"><mi id="S5.SS2.p2.8.m8.1.1.1.1" xref="S5.SS2.p2.8.m8.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p2.8.m8.2.2.2.4.1" xref="S5.SS2.p2.8.m8.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p2.8.m8.2.2.2.2" xref="S5.SS2.p2.8.m8.2.2.2.2.cmml">t</mi></mrow></msub><mo id="S5.SS2.p2.8.m8.2.3.1" xref="S5.SS2.p2.8.m8.2.3.1.cmml">/</mo><mi id="S5.SS2.p2.8.m8.2.3.3" xref="S5.SS2.p2.8.m8.2.3.3.cmml">I</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.2b"><apply id="S5.SS2.p2.8.m8.2.3.cmml" xref="S5.SS2.p2.8.m8.2.3"><divide id="S5.SS2.p2.8.m8.2.3.1.cmml" xref="S5.SS2.p2.8.m8.2.3.1"></divide><apply id="S5.SS2.p2.8.m8.2.3.2.cmml" xref="S5.SS2.p2.8.m8.2.3.2"><csymbol cd="ambiguous" id="S5.SS2.p2.8.m8.2.3.2.1.cmml" xref="S5.SS2.p2.8.m8.2.3.2">subscript</csymbol><ci id="S5.SS2.p2.8.m8.2.3.2.2.cmml" xref="S5.SS2.p2.8.m8.2.3.2.2">𝑄</ci><list id="S5.SS2.p2.8.m8.2.2.2.3.cmml" xref="S5.SS2.p2.8.m8.2.2.2.4"><ci id="S5.SS2.p2.8.m8.1.1.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1.1.1">𝑖</ci><ci id="S5.SS2.p2.8.m8.2.2.2.2.cmml" xref="S5.SS2.p2.8.m8.2.2.2.2">𝑡</ci></list></apply><ci id="S5.SS2.p2.8.m8.2.3.3.cmml" xref="S5.SS2.p2.8.m8.2.3.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.2c">Q_{i,t}/I</annotation></semantics></math> leaf aggregators, where <math id="S5.SS2.p2.9.m9.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.SS2.p2.9.m9.1a"><mi id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.1b"><ci id="S5.SS2.p2.9.m9.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.1c">I</annotation></semantics></math> is the number of model updates of clients per leaf aggregator.
Given that the steps within a LIFL aggregator (Fig. <a href="#A6.F14" title="Figure 14 ‣ F.1 Stateful “tax” in LIFL ‣ Appendix F In-place message queueing benefit" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>) are executed sequentially, we want to maximize the parallelism by having a limited <math id="S5.SS2.p2.10.m10.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.SS2.p2.10.m10.1a"><mi id="S5.SS2.p2.10.m10.1.1" xref="S5.SS2.p2.10.m10.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.10.m10.1b"><ci id="S5.SS2.p2.10.m10.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.10.m10.1c">I</annotation></semantics></math> to be small (<span id="S5.SS2.p2.10.3" class="ltx_text ltx_font_italic">e.g., </span>at 2), ensuring that a leaf aggregator experiences minimal waiting time after receiving the initial update from the first client.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">LIFL re-plans the hierarchy on each worker node
periodically. This involves estimating
<math id="S5.SS2.p3.1.m1.2" class="ltx_Math" alttext="Q_{i,t}" display="inline"><semantics id="S5.SS2.p3.1.m1.2a"><msub id="S5.SS2.p3.1.m1.2.3" xref="S5.SS2.p3.1.m1.2.3.cmml"><mi id="S5.SS2.p3.1.m1.2.3.2" xref="S5.SS2.p3.1.m1.2.3.2.cmml">Q</mi><mrow id="S5.SS2.p3.1.m1.2.2.2.4" xref="S5.SS2.p3.1.m1.2.2.2.3.cmml"><mi id="S5.SS2.p3.1.m1.1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.1.cmml">i</mi><mo id="S5.SS2.p3.1.m1.2.2.2.4.1" xref="S5.SS2.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS2.p3.1.m1.2.2.2.2" xref="S5.SS2.p3.1.m1.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.2b"><apply id="S5.SS2.p3.1.m1.2.3.cmml" xref="S5.SS2.p3.1.m1.2.3"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.2.3.1.cmml" xref="S5.SS2.p3.1.m1.2.3">subscript</csymbol><ci id="S5.SS2.p3.1.m1.2.3.2.cmml" xref="S5.SS2.p3.1.m1.2.3.2">𝑄</ci><list id="S5.SS2.p3.1.m1.2.2.2.3.cmml" xref="S5.SS2.p3.1.m1.2.2.2.4"><ci id="S5.SS2.p3.1.m1.1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1">𝑖</ci><ci id="S5.SS2.p3.1.m1.2.2.2.2.cmml" xref="S5.SS2.p3.1.m1.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.2c">Q_{i,t}</annotation></semantics></math> across the worker nodes and creates/terminates aggregators accordingly. The LIFL control plane updates the routes between aggregators based on the renewed hierarchy (details in Appendix-<a href="#A1" title="Appendix A Message flow of intra-node and inter-node routing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Opportunistic Reuse of Aggregator Instances</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">The scaling policy in LIFL incorporates an opportunistic “reuse” scheme to maximize the utilization of warm aggregator instances since aggregators in LIFL use homogenized runtimes (Fig. <a href="#A6.F14" title="Figure 14 ‣ F.1 Stateful “tax” in LIFL ‣ Appendix F In-place message queueing benefit" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>) with the same code and libs.
This sidesteps the cascading effect <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021b</a>)</cite> when starting up a hierarchy of aggregators (in fact function chains).</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">Given a hierarchy of aggregators selected on the node, LIFL picks a leaf aggregator that has already completed its aggregation task and is idle. LIFL converts its role to a middle aggregator on that node. No further change is required as LIFL’s aggregator runtime is stateless.
LIFL selects the first middle aggregator that completes its local aggregation task and converts it to be the top aggregator responsible for updating the global model.
This minimizes the need to start up new instances for higher-level aggregators, and avoids additional startup delays.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Eager aggregation in LIFL</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.1" class="ltx_p">LIFL employs eager aggregation (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) leveraging its more flexible and dynamic timing of the aggregation process. Eager aggregation performs timely aggregation as model updates arrive, even if it triggers the cold start of an aggregator (when no idle-but-warm aggregator is available). This takes advantage of the <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">overlap</span> between the start-up delay and transfers of model updates, allowing eager aggregation to mask cold starts up until the last model update. It also mitigates congestion that can occur when trying to aggregate all model updates simultaneously. In contrast, lazy aggregation aggregates all model updates in a batch when the aggregation goal is reached. But, the arrival of local model updates from trainers can be spread over a relatively long duration.
Our evaluation shows eager aggregation achieves a 20% reduction on ACT (Fig. <a href="#S6.F8.sf1" title="In Figure 8 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>).
We implement eager aggregation in LIFL following the step-based processing model described in Appendix-<a href="#A7" title="Appendix G Step-based Processing Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>.
LIFL updates the version of the global model whenever the aggregation goal is achieved.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation &amp; Analysis</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We quantify the performance gain and resource savings by using LIFL, starting with analyzing a set of microbenchmarks to understand the different design considerations of LIFL, including shared memory processing, the effectiveness and overheads of LIFL’s orchestration scheme.
We then demonstrate the benefits of LIFL from a system-level perspective using real FL workloads.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Baseline Systems:</span>
We implement several baseline FL systems for LIFL to compare against.
<span id="S6.p2.1.2" class="ltx_text ltx_font_bold">(1) “Serverful system” (SF):</span>
The “serverful system” is implemented following the design described in <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. Both of them adopt the architecture depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a).
<span id="S6.p2.1.3" class="ltx_text ltx_font_bold">(2) “Serverless system” (SL):</span>
The baseline “serverless system” is implemented following the design described in FedKeeper <cite class="ltx_cite ltx_citemacro_cite">Chadha et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> and AdaFed <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022b</a>)</cite> that uses the architecture depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Anatomy of Systems for Federated Learning ‣ 2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b). We choose Knative <cite class="ltx_cite ltx_citemacro_cite">kna (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> as the serverless framework to build these alternatives.
We utilize the open-source Flame platform <cite class="ltx_cite ltx_citemacro_cite">fla (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite> to provide necessary FL components, <span id="S6.p2.1.4" class="ltx_text ltx_font_italic">e.g., </span>coordinator, selector, aggregator, and client.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Implementation of LIFL:</span>
We implement LIFL based on SPRIGHT <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, a lightweight, high-performance serverless framework.
LIFL includes object store support, model checkpoints, and routing support for hierarchical aggregation. LIFL uses Python’s multiprocessing package to implement the shared memory pool instead of the DPDK-based shared memory pool used in the original implementation of SPRIGHT.
The current implementation of LIFL only supports synchronous FL. Supporting asynchronous FL is part of our future work.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Testbed setup:</span>
We leverage the NSF Cloudlab <cite class="ltx_cite ltx_citemacro_cite">Duplyakin et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. The nodes we used have a 64-core Intel Cascade Lake CPU@2.8 GHz, 192GB memory, and a 10Gb NIC. We use Ubuntu 20.04 with kernel version 5.16.</p>
</div>
<figure id="S6.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x8.png" id="S6.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="216" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>latency of a single model update transfer (intra-node)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x9.png" id="S6.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="216" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>CPU usage of a single model update transfer (intra-node)</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.10968/assets/results/shm-hier-agg-dataplane-legend.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="359" height="19" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x10.png" id="S6.F7.sf3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="152" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>LIFL’s Aggregation Timing (with ResNet-152)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Data plane improvement for hierarchical aggregation: Serverful (<span id="S6.F7.6.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>), Serverless (<span id="S6.F7.7.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>), and LIFL. <span id="S6.F7.8.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>’s latency includes contributions of <span id="S6.F7.9.4" class="ltx_text ltx_font_typewriter ltx_font_bold">+SC</span> (sidecar) and <span id="S6.F7.10.5" class="ltx_text ltx_font_typewriter ltx_font_bold">+MB</span> (message broker).</figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Microbenchmark Analysis</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_bold">Data plane improvement for hierarchical aggregation:</span>
To understand the improvements in data plane performance of hierarchical aggregation with LIFL’s shared memory processing,
we use the same aggregation hierarchy as in §<a href="#S4.SS1" title="4.1 Shared Memory for Hierarchical Aggregation ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, comprising one top aggregator and four leaf aggregators.
All aggregators are placed on the same node.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.p2.3" class="ltx_p">We consider the following serverful and serverless alternatives: (1) The serverful setup (<span id="S6.SS1.p2.3.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>) establishes direct networking
channels (based on gRPC) between leaf aggregators and the top aggregator;
(2) the serverless setup (<span id="S6.SS1.p2.3.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>) uses indirect networking to connect leaf aggregators and the top aggregator, through a message broker on the same node. Each aggregator has a container-based sidecar to mediate inbound and outbound traffic;
(3) the LIFL setup uses shared memory for communication between aggregators.
We consider three ML models with distinct sizes: ResNet-18 (<math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mo id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">\sim</annotation></semantics></math>44MB), ResNet-34 (<math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><mo id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><csymbol cd="latexml" id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">\sim</annotation></semantics></math>83MB), and ResNet-152 (<math id="S6.SS1.p2.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.p2.3.m3.1a"><mo id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><csymbol cd="latexml" id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">\sim</annotation></semantics></math>232MB).</p>
</div>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.2" class="ltx_p">Fig. <a href="#S6.F7.sf1" title="In Figure 7 ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(a)</span></a> shows the latency breakdown of a single model update transfer between the leaf aggregator and top aggregator for different model sizes. We specially mark the share of sidecar (<span id="S6.SS1.p3.2.1" class="ltx_text ltx_font_typewriter ltx_font_bold">+SC</span>) and message broker (<span id="S6.SS1.p3.2.2" class="ltx_text ltx_font_typewriter ltx_font_bold">+MB</span>) for the serverless setup.
<span id="S6.SS1.p3.2.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> consistently results in 2<math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><times id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\times</annotation></semantics></math> and 6<math id="S6.SS1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><mo id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><times id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">\times</annotation></semantics></math> higher latency than <span id="S6.SS1.p3.2.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> and LIFL, respectively. The significant CPU usage of <span id="S6.SS1.p3.2.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> (Fig. <a href="#S6.F7.sf2" title="In Figure 7 ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a>)
clearly shows the poor efficiency and performance of the indirect networking used in the serverless setup, caused by its use of the message broker and heavyweight sidecar.
We see that LIFL is considerably better than <span id="S6.SS1.p3.2.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> and <span id="S6.SS1.p3.2.7" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> in terms of both CPU usage and latency.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para ltx_noindent">
<p id="S6.SS1.p4.1" class="ltx_p">Fig. <a href="#S6.F7.sf3" title="In Figure 7 ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(c)</span></a> shows the timing of various FL processing tasks during hierarchical aggregation when using LIFL’s data plane.
It is clear that LIFL’s shared memory processing helps reduce the overhead and improve the performance of the data plane with hierarchical aggregation.
LIFL completes each round in just 44.9 seconds compared to 57 seconds on average even for the serverful setup in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Shared Memory for Hierarchical Aggregation ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Further, through careful placement, aggregators in LIFL can fully exploit the high-speed intra-node data plane over shared memory, as discussed next.</p>
</div>
<figure id="S6.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x11.png" id="S6.F8.sf1.g1" class="ltx_graphics ltx_img_landscape" width="216" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Agg. Completion Time</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x12.png" id="S6.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="216" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Cumulative CPU Time</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x13.png" id="S6.F8.sf3.g1" class="ltx_graphics ltx_img_landscape" width="216" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span># of aggregators created</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F8.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x14.png" id="S6.F8.sf4.g1" class="ltx_graphics ltx_img_landscape" width="216" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span># of nodes used</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Improvement with LIFL’s orchestration, with \footnotesize\textbt{i}⃝ being additions to baseline LIFL; <span id="S6.F8.2.1" class="ltx_text ltx_font_italic">x-axis is the number of model updates arriving at the aggregation service concurrently.</span></figcaption>
</figure>
<div id="S6.SS1.p5" class="ltx_para ltx_noindent">
<p id="S6.SS1.p5.2" class="ltx_p"><span id="S6.SS1.p5.2.1" class="ltx_text ltx_font_bold">Improved orchestration in LIFL:</span>
We now quantify the benefits of LIFL’s orchestration in improving hierarchical aggregation.
We demonstrate the effectiveness of LIFL by applying: \footnotesize\textbt{1}⃝ locality-aware placement (§<a href="#S5.SS1" title="5.1 Locality-aware Placement and Load Balancing ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>), \footnotesize\textbt{2}⃝ hierarchy-planning (§<a href="#S5.SS2" title="5.2 Planning the Hierarchy for Aggregation ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>), \footnotesize\textbt{3}⃝ aggregator reuse (§<a href="#S5.SS3" title="5.3 Opportunistic Reuse of Aggregator Instances ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>), and \footnotesize\textbt{4}⃝ eager aggregation (§<a href="#S5.SS4" title="5.4 Eager aggregation in LIFL ‣ 5 LIFL’s Control Plane Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>) step-by-step.
We use five nodes for this experiment. The maximum service capacity (<math id="S6.SS1.p5.1.m1.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="S6.SS1.p5.1.m1.1a"><mrow id="S6.SS1.p5.1.m1.1.1" xref="S6.SS1.p5.1.m1.1.1.cmml"><mi id="S6.SS1.p5.1.m1.1.1.2" xref="S6.SS1.p5.1.m1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p5.1.m1.1.1.1" xref="S6.SS1.p5.1.m1.1.1.1.cmml">​</mo><msub id="S6.SS1.p5.1.m1.1.1.3" xref="S6.SS1.p5.1.m1.1.1.3.cmml"><mi id="S6.SS1.p5.1.m1.1.1.3.2" xref="S6.SS1.p5.1.m1.1.1.3.2.cmml">C</mi><mi id="S6.SS1.p5.1.m1.1.1.3.3" xref="S6.SS1.p5.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.1.m1.1b"><apply id="S6.SS1.p5.1.m1.1.1.cmml" xref="S6.SS1.p5.1.m1.1.1"><times id="S6.SS1.p5.1.m1.1.1.1.cmml" xref="S6.SS1.p5.1.m1.1.1.1"></times><ci id="S6.SS1.p5.1.m1.1.1.2.cmml" xref="S6.SS1.p5.1.m1.1.1.2">𝑀</ci><apply id="S6.SS1.p5.1.m1.1.1.3.cmml" xref="S6.SS1.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.p5.1.m1.1.1.3.1.cmml" xref="S6.SS1.p5.1.m1.1.1.3">subscript</csymbol><ci id="S6.SS1.p5.1.m1.1.1.3.2.cmml" xref="S6.SS1.p5.1.m1.1.1.3.2">𝐶</ci><ci id="S6.SS1.p5.1.m1.1.1.3.3.cmml" xref="S6.SS1.p5.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.1.m1.1c">MC_{i}</annotation></semantics></math>) of each node in our testbed is 20.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Our testbed nodes are homogeneous, hence all <math id="footnote6.m1.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="footnote6.m1.1b"><mrow id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml"><mi id="footnote6.m1.1.1.2" xref="footnote6.m1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="footnote6.m1.1.1.1" xref="footnote6.m1.1.1.1.cmml">​</mo><msub id="footnote6.m1.1.1.3" xref="footnote6.m1.1.1.3.cmml"><mi id="footnote6.m1.1.1.3.2" xref="footnote6.m1.1.1.3.2.cmml">C</mi><mi id="footnote6.m1.1.1.3.3" xref="footnote6.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><apply id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1"><times id="footnote6.m1.1.1.1.cmml" xref="footnote6.m1.1.1.1"></times><ci id="footnote6.m1.1.1.2.cmml" xref="footnote6.m1.1.1.2">𝑀</ci><apply id="footnote6.m1.1.1.3.cmml" xref="footnote6.m1.1.1.3"><csymbol cd="ambiguous" id="footnote6.m1.1.1.3.1.cmml" xref="footnote6.m1.1.1.3">subscript</csymbol><ci id="footnote6.m1.1.1.3.2.cmml" xref="footnote6.m1.1.1.3.2">𝐶</ci><ci id="footnote6.m1.1.1.3.3.cmml" xref="footnote6.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">MC_{i}</annotation></semantics></math> are the same. With heterogeneous nodes, <math id="footnote6.m2.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="footnote6.m2.1b"><mrow id="footnote6.m2.1.1" xref="footnote6.m2.1.1.cmml"><mi id="footnote6.m2.1.1.2" xref="footnote6.m2.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="footnote6.m2.1.1.1" xref="footnote6.m2.1.1.1.cmml">​</mo><msub id="footnote6.m2.1.1.3" xref="footnote6.m2.1.1.3.cmml"><mi id="footnote6.m2.1.1.3.2" xref="footnote6.m2.1.1.3.2.cmml">C</mi><mi id="footnote6.m2.1.1.3.3" xref="footnote6.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m2.1c"><apply id="footnote6.m2.1.1.cmml" xref="footnote6.m2.1.1"><times id="footnote6.m2.1.1.1.cmml" xref="footnote6.m2.1.1.1"></times><ci id="footnote6.m2.1.1.2.cmml" xref="footnote6.m2.1.1.2">𝑀</ci><apply id="footnote6.m2.1.1.3.cmml" xref="footnote6.m2.1.1.3"><csymbol cd="ambiguous" id="footnote6.m2.1.1.3.1.cmml" xref="footnote6.m2.1.1.3">subscript</csymbol><ci id="footnote6.m2.1.1.3.2.cmml" xref="footnote6.m2.1.1.3.2">𝐶</ci><ci id="footnote6.m2.1.1.3.3.cmml" xref="footnote6.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m2.1d">MC_{i}</annotation></semantics></math> may vary.</span></span></span>
We focus on two aspects: resource consumption and Aggregation Completion Time (ACT) to aggregate a given number of model updates. In this experiment, we assume the estimated <math id="S6.SS1.p5.2.m2.2" class="ltx_Math" alttext="Q_{i,t}" display="inline"><semantics id="S6.SS1.p5.2.m2.2a"><msub id="S6.SS1.p5.2.m2.2.3" xref="S6.SS1.p5.2.m2.2.3.cmml"><mi id="S6.SS1.p5.2.m2.2.3.2" xref="S6.SS1.p5.2.m2.2.3.2.cmml">Q</mi><mrow id="S6.SS1.p5.2.m2.2.2.2.4" xref="S6.SS1.p5.2.m2.2.2.2.3.cmml"><mi id="S6.SS1.p5.2.m2.1.1.1.1" xref="S6.SS1.p5.2.m2.1.1.1.1.cmml">i</mi><mo id="S6.SS1.p5.2.m2.2.2.2.4.1" xref="S6.SS1.p5.2.m2.2.2.2.3.cmml">,</mo><mi id="S6.SS1.p5.2.m2.2.2.2.2" xref="S6.SS1.p5.2.m2.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.2.m2.2b"><apply id="S6.SS1.p5.2.m2.2.3.cmml" xref="S6.SS1.p5.2.m2.2.3"><csymbol cd="ambiguous" id="S6.SS1.p5.2.m2.2.3.1.cmml" xref="S6.SS1.p5.2.m2.2.3">subscript</csymbol><ci id="S6.SS1.p5.2.m2.2.3.2.cmml" xref="S6.SS1.p5.2.m2.2.3.2">𝑄</ci><list id="S6.SS1.p5.2.m2.2.2.2.3.cmml" xref="S6.SS1.p5.2.m2.2.2.2.4"><ci id="S6.SS1.p5.2.m2.1.1.1.1.cmml" xref="S6.SS1.p5.2.m2.1.1.1.1">𝑖</ci><ci id="S6.SS1.p5.2.m2.2.2.2.2.cmml" xref="S6.SS1.p5.2.m2.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.2.m2.2c">Q_{i,t}</annotation></semantics></math> is equal to the actual queue length on each active node. We focus on the importance of having warm aggregators based on the pre-planned hierarchy, to avoid the cold start penalty.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para ltx_noindent">
<p id="S6.SS1.p6.1" class="ltx_p">We compare LIFL against a baseline serverless control plane using hierarchical aggregation (<span id="S6.SS1.p6.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-H</span> in Fig. <a href="#S6.F8" title="Figure 8 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). <span id="S6.SS1.p6.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-H</span> employs LIFL’s shared memory data plane (so both have the same data plane) with Knative’s “Least Connection” load balancing strategy <cite class="ltx_cite ltx_citemacro_cite">Mittal et al. (<a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite> that assigns newly arrived model updates to the node with the smallest queue length.
The aggregators in <span id="S6.SS1.p6.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-H</span> use lazy aggregation by default. The ML model used is ResNet-152. Note that the latency to transmit a single model update of ResNet-152 across nodes (on the current testbed) is <math id="S6.SS1.p6.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.p6.1.m1.1a"><mo id="S6.SS1.p6.1.m1.1.1" xref="S6.SS1.p6.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p6.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.p6.1.m1.1.1.cmml" xref="S6.SS1.p6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p6.1.m1.1c">\sim</annotation></semantics></math>4.2 seconds.</p>
</div>
<div id="S6.SS1.p7" class="ltx_para ltx_noindent">
<p id="S6.SS1.p7.6" class="ltx_p">By using locality-aware placement, LIFL also achieves 2.1<math id="S6.SS1.p7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p7.1.m1.1a"><mo id="S6.SS1.p7.1.m1.1.1" xref="S6.SS1.p7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.1.m1.1b"><times id="S6.SS1.p7.1.m1.1.1.cmml" xref="S6.SS1.p7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.1.m1.1c">\times</annotation></semantics></math> and 1.13<math id="S6.SS1.p7.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p7.2.m2.1a"><mo id="S6.SS1.p7.2.m2.1.1" xref="S6.SS1.p7.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.2.m2.1b"><times id="S6.SS1.p7.2.m2.1.1.cmml" xref="S6.SS1.p7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.2.m2.1c">\times</annotation></semantics></math> ACT reduction than <span id="S6.SS1.p7.6.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-H</span> (for 20 and 60 model updates in Fig. <a href="#S6.F8.sf1" title="In Figure 8 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>). This improvement is attributed to LIFL’s bin-packing strategy, which effectively consolidates aggregators onto the same node to fully exploit shared memory processing.
Applying hierarchy-planning and reusing warm aggregator instances (+\footnotesize\textbt{1}⃝+\footnotesize\textbt{2}⃝+\footnotesize\textbt{3}⃝) further reduce <math id="S6.SS1.p7.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.p7.3.m3.1a"><mo id="S6.SS1.p7.3.m3.1.1" xref="S6.SS1.p7.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.3.m3.1b"><csymbol cd="latexml" id="S6.SS1.p7.3.m3.1.1.cmml" xref="S6.SS1.p7.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.3.m3.1c">\sim</annotation></semantics></math>1.22<math id="S6.SS1.p7.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p7.4.m4.1a"><mo id="S6.SS1.p7.4.m4.1.1" xref="S6.SS1.p7.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.4.m4.1b"><times id="S6.SS1.p7.4.m4.1.1.cmml" xref="S6.SS1.p7.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.4.m4.1c">\times</annotation></semantics></math> ACT of LIFL, as keeping aggregators warm mitigates the cold start delay that exists in both <span id="S6.SS1.p7.6.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-H</span> and (+\footnotesize\textbt{1}⃝).
Further, after enabling eager aggregation (+\footnotesize\textbt{1}⃝+\footnotesize\textbt{2}⃝+\footnotesize\textbt{3}⃝+\footnotesize\textbt{4}⃝), LIFL allows higher-level aggregators to consume and aggregate the model updates in a timely manner, effectively avoiding the intermediate model updates (produced by the lower-level aggregators) being queued up at the higher-level aggregators. This saves <math id="S6.SS1.p7.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS1.p7.5.m5.1a"><mo id="S6.SS1.p7.5.m5.1.1" xref="S6.SS1.p7.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.5.m5.1b"><csymbol cd="latexml" id="S6.SS1.p7.5.m5.1.1.cmml" xref="S6.SS1.p7.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.5.m5.1c">\sim</annotation></semantics></math>1.2<math id="S6.SS1.p7.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p7.6.m6.1a"><mo id="S6.SS1.p7.6.m6.1.1" xref="S6.SS1.p7.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.6.m6.1b"><times id="S6.SS1.p7.6.m6.1.1.cmml" xref="S6.SS1.p7.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.6.m6.1c">\times</annotation></semantics></math> in ACT compared to (+\footnotesize\textbt{1}⃝+\footnotesize\textbt{2}⃝+\footnotesize\textbt{3}⃝) that uses lazy aggregation.</p>
</div>
<div id="S6.SS1.p8" class="ltx_para ltx_noindent">
<p id="S6.SS1.p8.1" class="ltx_p">While being effective in reducing ACT, LIFL also helps to reduce costs. Just using locality-aware placement (+\footnotesize\textbt{1}⃝ in Fig. <a href="#S6.F8.sf2" title="In Figure 8 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>), LIFL can save considerable CPU overhead by reducing inter-node data transfers (with 20 and 60 model updates). Enabling aggregator reuse saves additional CPU cycles, as it avoids having the CPU initialize new aggregators. For 100 model updates though, the service capacity of all five nodes would be maxed out, reaching the limit of the benefit of LIFL’s orchestration. However, the data plane improvement of LIFL can still make it outperform the basic serverful and serverless setups, as demonstrated in Fig. <a href="#S6.F7" title="Figure 7 ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S6.SS1.p9" class="ltx_para ltx_noindent">
<p id="S6.SS1.p9.1" class="ltx_p">As shown in Fig. <a href="#S6.F8.sf3" title="In Figure 8 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(c)</span></a>, LIFL reduces the number of aggregators created, by packing more aggregators into fewer nodes.
After we apply locality-aware placement to LIFL (+\footnotesize\textbt{1}⃝), LIFL can also reduce the number of nodes used considerably (see Fig. <a href="#S6.F8.sf4" title="In Figure 8 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(d)</span></a>): Given 20, 60, and 100 model updates, LIFL’s locality-aware placement efficiently packs them into 1, 3, and 5 nodes, respectively.
This avoids repeatedly creating a middle aggregator on each of the 5 nodes (except when the service capacity of all 5 nodes is fully consumed).
On the other hand, <span id="S6.SS1.p9.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-H</span> uses all 5 nodes throughout,
uniformly distributing model updates across all 5 available nodes. This will lead to additional cross-node data transfers, regardless of available model updates.
Note that the service capacity of all 5 nodes is fully consumed for 100 model updates.</p>
</div>
<div id="S6.SS1.p10" class="ltx_para ltx_noindent">
<p id="S6.SS1.p10.1" class="ltx_p"><span id="S6.SS1.p10.1.1" class="ltx_text ltx_font_bold">Orchestration overhead of LIFL:</span>
We evaluate the orchestration overhead of LIFL, given a different number of clients.
The time for completing the locality-aware placement in LIFL is less than 17 milliseconds, even with 10K clients, which is the maximum number of client settings observed in Google’s production FL stack <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>.
Compared to the ACT, which takes several tens of seconds with a large amount of clients, this overhead for locality-aware placement is negligible.
The EWMA estimator for hierarchy-planning takes 0.2 milliseconds per estimate, which is also negligible compared to the 2-minute cycle time used by LIFL to re-plan the hierarchy on each worker node.
The aggregator reuse and eager aggregation incur almost no overhead, as they do not require active involvement of the LIFL control plane.</p>
</div>
<figure id="S6.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2405.10968/assets/x15.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="230" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2405.10968/assets/x16.png" id="S6.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="230" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2405.10968/assets/x17.png" id="S6.F9.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="230" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2405.10968/assets/x18.png" id="S6.F9.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="230" height="168" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S6.F9.3.1" class="ltx_text ltx_font_bold">ResNet-18:</span> (a) Time-to-accuracy and (b) Cost-to-accuracy; <span id="S6.F9.4.2" class="ltx_text ltx_font_bold">ResNet-152:</span> (c) Time-to-accuracy and (d) Cost-to-accuracy.</figcaption>
</figure>
<figure id="S6.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x19.png" id="S6.F10.sf1.g1" class="ltx_graphics ltx_img_landscape" width="138" height="50" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Update arrival rate</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x20.png" id="S6.F10.sf2.g1" class="ltx_graphics ltx_img_landscape" width="138" height="50" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span># of active aggregators</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F10.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x21.png" id="S6.F10.sf3.g1" class="ltx_graphics ltx_img_landscape" width="138" height="50" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Cumul. CPU time (seconds) per round</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F10.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x22.png" id="S6.F10.sf4.g1" class="ltx_graphics ltx_img_landscape" width="138" height="50" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Update arrival rate</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F10.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x23.png" id="S6.F10.sf5.g1" class="ltx_graphics ltx_img_landscape" width="138" height="50" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span># of active aggregators</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F10.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x24.png" id="S6.F10.sf6.g1" class="ltx_graphics ltx_img_landscape" width="138" height="50" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Cumul. CPU time (seconds) per round</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="S6.F10.2.1" class="ltx_text ltx_font_bold">ResNet-18 (a, b, c), ResNet-152 (d, e, f):</span> Time series of arrival rate, number of active aggregators, and cumulative CPU time (seconds) per round.</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>FL Workloads Setup</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p">Our aim is to demonstrate the generality of LIFL in improving performance and reducing the cost of FL from a system-level perspective.
We consider synchronous FL (using FedAvg <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>) to justify LIFL’s design.
We use Stochastic Gradient Descent on the client. Clients are configured with a batch size of 32 in a local training epoch, with the learning rate set to 0.01.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Benchmark selection:</span>
We consider image classification, training ResNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> models with the FEMNIST dataset <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite>.
We use non-IID datasets from FedScale <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite> (with its real client-data mapping) to keep the setting realistic with different data distributions across the client population.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para ltx_noindent">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Configuration of clients:</span>
We consider two distinct client setups:
(<span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-18</span> setup) We use the client in this setup to train a ResNet-18 model. Clients are considered to be mobile devices with limited computing capacity, available only when each has battery power and is connected to a data (<span id="S6.SS2.p3.1.3" class="ltx_text ltx_font_italic">e.g., </span>WiFi) network. This results in high variability in the number of mobile devices available to perform training tasks. As such, we let each client hibernate for a random interval within [0, 60] seconds to emulate the dynamic availability of typical mobile device behavior. This generates varying loads over time, as shown in Fig. <a href="#S6.F10.sf1" title="In Figure 10 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>, justifying the need for scaling with a serverless framework as well as LIFL.
(<span id="S6.SS2.p3.1.4" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-152</span> setup) The client in this setup trains the relatively heavyweight ResNet-152 model. The client is considered to be a server with substantial computing capacity and is highly available. As such, we keep clients in this setup always-on. This results in a more stable arrival pattern of model updates, as shown in Fig. <a href="#S6.F10.sf4" title="In Figure 10 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(d)</span></a>.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para ltx_noindent">
<p id="S6.SS2.p4.1" class="ltx_p">We use a total of 20 physical nodes with 5 nodes used to run aggregators. We use 4 nodes as leaf/middle aggregators and dedicate one node to be the top aggregator. To deliver the benefits of a “serverful system” (<span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>), we always maximize the resource allocation to the aggregators and keep them warm throughout the experiment. For the serverless setup (<span id="S6.SS2.p4.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> and LIFL), we create aggregators on demand.</p>
</div>
<div id="S6.SS2.p5" class="ltx_para ltx_noindent">
<p id="S6.SS2.p5.1" class="ltx_p">We use the remaining 15 physical nodes to run the clients. In the <span id="S6.SS2.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-18</span> setup, since we consider clients to be compute-constrained mobile devices, we run eight clients on the same physical node, so each client only gets a small share of the compute capacity of the physical node.
Therefore, in the <span id="S6.SS2.p5.1.2" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-18</span> setup, we can keep 120 simultaneously active clients in each round.
In the <span id="S6.SS2.p5.1.3" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-152</span> setup, we treat the client as a server node, so we dedicate a physical node for a ResNet-152 client.
In this <span id="S6.SS2.p5.1.4" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-152</span> setup, we keep 15 simultaneously active clients in each round. The active clients are selected from a total of 2,800 real clients provided by FedScale <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Putting It All Together</h3>

<div id="S6.SS3.p1" class="ltx_para ltx_noindent">
<p id="S6.SS3.p1.2" class="ltx_p">(<span id="S6.SS3.p1.2.1" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-18</span>) <span id="S6.SS3.p1.2.2" class="ltx_text ltx_font_bold">Time to Accuracy:</span>
We compare the time-to-accuracy
of LIFL against <span id="S6.SS3.p1.2.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> and <span id="S6.SS3.p1.2.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>. To reach 70% accuracy of ResNet-18 (Fig. <a href="#S6.F9" title="Figure 9 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (a)), LIFL takes only 0.9 hours (wall clock time), which is 1.6<math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mo id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><times id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">\times</annotation></semantics></math> faster than <span id="S6.SS3.p1.2.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> (1.4 hours). Compared to <span id="S6.SS3.p1.2.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> which takes 2.4 hours, LIFL is 2.7<math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mo id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><times id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">\times</annotation></semantics></math> faster.
The improvement with LIFL can be attributed to the shared memory data plane and the improved orchestration to effectively utilize resources, thereby reducing ACT (see §<a href="#S6.SS1" title="6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>).</p>
</div>
<div id="S6.SS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS3.p2.1" class="ltx_p">The time spent by the <span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> aggregation service increases due to a combination of factors including sidecar overhead, function chaining, and simplistic orchestration. Frequent start-up of the aggregators in <span id="S6.SS3.p2.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> (Fig. <a href="#S6.F10.sf2" title="In Figure 10 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>) adds delays to the aggregation (for the first arrival update in a round).
This increased aggregation time of <span id="S6.SS3.p2.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> eventually hurts the time-to-accuracy (70%), making it even slower than <span id="S6.SS3.p2.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para ltx_noindent">
<p id="S6.SS3.p3.1" class="ltx_p">(<span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-18</span>) <span id="S6.SS3.p3.1.2" class="ltx_text ltx_font_bold">Cost savings with LIFL:</span>
LIFL achieves significant cost savings compared to <span id="S6.SS3.p3.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> and <span id="S6.SS3.p3.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>.
We focus on the cumulative costs (CPU time) consumed by the aggregation service to achieve a certain model accuracy.
To reach the 70% accuracy level of ResNet-18 (Fig. <a href="#S6.F9" title="Figure 9 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (b)), LIFL consumes 4.5 CPU hours, which is 1.8<math id="S6.SS3.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p3.1.m1.1a"><mo id="S6.SS3.p3.1.m1.1.1" xref="S6.SS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p3.1.m1.1b"><times id="S6.SS3.p3.1.m1.1.1.cmml" xref="S6.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p3.1.m1.1c">\times</annotation></semantics></math> less than <span id="S6.SS3.p3.1.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> (8 CPU hours).
Further, <span id="S6.SS3.p3.1.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>, with its simplistic fixed resource allocation, keeps aggregators “always-on”, constantly occupying its CPU allocation (Fig. <a href="#S6.F10.sf2" title="In Figure 10 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>).
LIFL adapts well to the arrival rate of model updates and re-plans (scales) the hierarchy accordingly, using resources to match demand.
Also note that the LIFL’s aggregator, when deployed as a Kubernetes pod or container, is also cheaper (smaller resource allocation) than <span id="S6.SS3.p3.1.7" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span>, as LIFL requires less CPU to complete the same amount of aggregation tasks (Fig. <a href="#S6.F10.sf3" title="In Figure 10 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(c)</span></a>).</p>
</div>
<div id="S6.SS3.p4" class="ltx_para ltx_noindent">
<p id="S6.SS3.p4.1" class="ltx_p">In contrast, <span id="S6.SS3.p4.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> consumes much more CPU (26 CPU hours) to achieve the 70% accuracy level of ResNet-18 (Fig. <a href="#S6.F9" title="Figure 9 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (b)) compared to LIFL (4.5 CPU hours).
Although <span id="S6.SS3.p4.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> has relatively fewer active aggregators over time (Fig. <a href="#S6.F10.sf2" title="In Figure 10 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>),
its data plane and sidecar overheads, and the CPU consumed for start-up results in <span id="S6.SS3.p4.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> having more than 5<math id="S6.SS3.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p4.1.m1.1a"><mo id="S6.SS3.p4.1.m1.1.1" xref="S6.SS3.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.1.m1.1b"><times id="S6.SS3.p4.1.m1.1.1.cmml" xref="S6.SS3.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.1.m1.1c">\times</annotation></semantics></math> the CPU consumption of LIFL. This higher CPU time cost per round (for the same amount of aggregation work completed) requires the cloud service provider to allocate far more resources to the aggregator (<span id="S6.SS3.p4.1.4" class="ltx_text ltx_font_italic">e.g., </span>as a pod), making a single aggregator in <span id="S6.SS3.p4.1.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> much more expensive than both <span id="S6.SS3.p4.1.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> and LIFL.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para ltx_noindent">
<p id="S6.SS3.p5.2" class="ltx_p">(<span id="S6.SS3.p5.2.1" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-152</span>) <span id="S6.SS3.p5.2.2" class="ltx_text ltx_font_bold">Time to Accuracy:</span>
Fig. <a href="#S6.F9" title="Figure 9 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (c) shows the time-to-accuracy of the different alternatives for RestNet-152. To reach 70% accuracy, LIFL takes 1.9 hours (wall clock time), which is 1.15<math id="S6.SS3.p5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p5.1.m1.1a"><mo id="S6.SS3.p5.1.m1.1.1" xref="S6.SS3.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.1.m1.1b"><times id="S6.SS3.p5.1.m1.1.1.cmml" xref="S6.SS3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.1.m1.1c">\times</annotation></semantics></math> faster than <span id="S6.SS3.p5.2.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> (2.2 hours). Comparatively, <span id="S6.SS3.p5.2.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> takes 3.2 hours. LIFL is 1.68<math id="S6.SS3.p5.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p5.2.m2.1a"><mo id="S6.SS3.p5.2.m2.1.1" xref="S6.SS3.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.2.m2.1b"><times id="S6.SS3.p5.2.m2.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.2.m2.1c">\times</annotation></semantics></math> faster than <span id="S6.SS3.p5.2.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>.
The heavy-weight sidecar, slow function chaining, function startup delays, and simplistic orchestration, are responsible for the larger time-to-accuracy of <span id="S6.SS3.p5.2.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> for ResNet-152, just as we saw with the ResNet-18 workload, as well as with the microbenchmark analysis.</p>
</div>
<div id="S6.SS3.p6" class="ltx_para ltx_noindent">
<p id="S6.SS3.p6.1" class="ltx_p">(<span id="S6.SS3.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">ResNet-152</span>) <span id="S6.SS3.p6.1.2" class="ltx_text ltx_font_bold">Cost savings with LIFL:</span>
As Fig. <a href="#S6.F9" title="Figure 9 ‣ 6.1 Microbenchmark Analysis ‣ 6 Evaluation &amp; Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (d) shows, LIFL again achieves significant cost savings (on cumulative CPU time) compared to <span id="S6.SS3.p6.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> and <span id="S6.SS3.p6.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>.
To reach the 70% accuracy level of the ResNet-152 model, LIFL consumes 4.76 CPU hours, which is 1.43<math id="S6.SS3.p6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p6.1.m1.1a"><mo id="S6.SS3.p6.1.m1.1.1" xref="S6.SS3.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p6.1.m1.1b"><times id="S6.SS3.p6.1.m1.1.1.cmml" xref="S6.SS3.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p6.1.m1.1c">\times</annotation></semantics></math> less than <span id="S6.SS3.p6.1.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SF</span> (6.81 CPU hours).
In contrast, <span id="S6.SS3.p6.1.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span> consumes much more CPU (20.4 CPU hours) to achieve the same 70% accuracy level compared to LIFL.
This again is consistent with what we observed from the ResNet-18 workload, highlighting the advantage of LIFL.</p>
</div>
<div id="S6.SS3.p7" class="ltx_para ltx_noindent">
<p id="S6.SS3.p7.1" class="ltx_p"><span id="S6.SS3.p7.1.1" class="ltx_text ltx_font_bold">Summary:</span>
LIFL takes advantage of the fine-grained elasticity of serverless to scale the aggregation service based on load changes,
saving CPU consumption compared to serverful alternatives.
When comparing LIFL with <span id="S6.SS3.p7.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL</span>, LIFL is even more compelling, with far lower CPU consumption because of LIFL’s orchestration scheme and lightweight data plane (as we saw from the microbenchmark analysis). Thus, LIFL shows that it truly leverages the elasticity promise of the serverless computing paradigm.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Related work</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We have discussed the pros and cons of prior work on serverful <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> and serverless <cite class="ltx_cite ltx_citemacro_cite">Jayaram et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022b</a>; <a href="#bib.bib28" title="" class="ltx_ref">a</a>); Chadha et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> FL systems in §<a href="#S2" title="2 Background and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. LIFL goes beyond these prior designs with an optimized serverless infrastructure and efficient orchestration to truly realize the promise of serverless computing. We now discuss work related to LIFL from other perspectives.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Federated Learning:</span>
As a fast-evolving ML technology, a large body of work has been proposed for FL; the proposals in <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>); Li et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020a</a>); Nguyen et al. (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>); Li et al. (<a href="#bib.bib39" title="" class="ltx_ref">2020b</a>); Reddi et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> focus on FL algorithms while others investigate how to select FL clients or datasets more intelligently <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023a</a>); Abdelmoniem et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>); Jiang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>); Nishio &amp; Yonetani (<a href="#bib.bib47" title="" class="ltx_ref">2019</a>); Shin et al. (<a href="#bib.bib60" title="" class="ltx_ref">2022</a>); Guo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>); Lalitha et al. (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>); Elzohairy et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023b</a>)</cite> seeks to schedule FL jobs across a shared set of FL clients with less contention and reduce job scheduling delays. These efforts are orthogonal to LIFL because LIFL focuses on system-level optimization of model aggregation of FL. This makes LIFL a good complement to these efforts by providing an efficient and high-performance FL system to bring various FL approaches to the ground.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p">Several open-source FL platforms, <span id="S7.p3.1.1" class="ltx_text ltx_font_italic">e.g., </span>Flame <cite class="ltx_cite ltx_citemacro_cite">fla (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>, FATE <cite class="ltx_cite ltx_citemacro_cite">fat (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>, OpenFL <cite class="ltx_cite ltx_citemacro_cite">ope (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, FedML <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>, IBM federated learning <cite class="ltx_cite ltx_citemacro_cite">Ludwig et al. (<a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> have been launched to facilitate the promotion and adoption of FL in both research and applications. These platforms assume themselves to be a serverful design with static, inflexible deployment, which makes them unprepared for large-scale FL. LIFL can be used as a representative case to guide the future development of these platforms.</p>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">Serverless computing optimization:</span>
Recent advances in serverless computing have triggered extensive research endeavors dedicated to optimizing its system design. Significantly, a prominent amount of investigation revolves around the enhancement of resource provisioning, function deployment, load balancing <cite class="ltx_cite ltx_citemacro_cite">Singhvi et al. (<a href="#bib.bib61" title="" class="ltx_ref">2021</a>); Mittal et al. (<a href="#bib.bib45" title="" class="ltx_ref">2021</a>); Tariq et al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>); Bhasi et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Park et al. (<a href="#bib.bib49" title="" class="ltx_ref">2021a</a>); Kaffes et al. (<a href="#bib.bib34" title="" class="ltx_ref">2022</a>); Jin et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>, runtime overhead reduction <cite class="ltx_cite ltx_citemacro_cite">Agache et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>); Akkus et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>); Shillaker &amp; Pietzuch (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>); Gadepalli et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>); Oakes et al. (<a href="#bib.bib48" title="" class="ltx_ref">2018</a>)</cite>, and mitigation of function startup delay <cite class="ltx_cite ltx_citemacro_cite">Fu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>); Shahrad et al. (<a href="#bib.bib58" title="" class="ltx_ref">2020b</a>); Lin &amp; Glikson (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>); Fuerst &amp; Sharma (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>); Schall et al. (<a href="#bib.bib55" title="" class="ltx_ref">2022</a>); Ustiugov et al. (<a href="#bib.bib63" title="" class="ltx_ref">2021</a>); Wang et al. (<a href="#bib.bib64" title="" class="ltx_ref">2021</a>)</cite> within serverless platforms. Furthermore, substantial efforts have been directed towards addressing the data plane overheads inherent in serverless architectures <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>); Jia &amp; Witchel (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>); Shillaker &amp; Pietzuch (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>); Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2023</a>)</cite>, characterized by heavyweight function chaining and sidecar proxy.</p>
</div>
<div id="S7.p5" class="ltx_para ltx_noindent">
<p id="S7.p5.1" class="ltx_p">Our work, combines the advantages of data plane optimization (<span id="S7.p5.1.1" class="ltx_text ltx_font_italic">i.e., </span>shared memory for hierarchical aggregation, in-place message queuing, event-driven sidecars, etc), to unlock the full potential of serverless computing, facilitating efficient and cost-effective FL in the cloud.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para ltx_noindent">
<p id="S8.p1.6" class="ltx_p">LIFL is an optimized serverless FL system aimed at making FL more efficient and significantly lowering its operational cost.
LIFL adopts hierarchical aggregation to support FL at scale.
Its serverless infrastructure leverages shared memory processing to offer high-speed yet efficient intra-node data plane and event-driven sidecar functionality to facilitate communication within hierarchical aggregation.
LIFL’s orchestration scheme adjusts the aggregation hierarchy based on load and, maximizes the utilization of shared memory through intelligent placement and reuse of aggregation function instances, thus saving the cost.
Our evaluation shows that LIFL’s optimized data and control planes improve the resource efficiency of the aggregation service by more than <span id="S8.p1.1.1" class="ltx_text ltx_font_bold">5<math id="S8.p1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.1.1.m1.1a"><mo id="S8.p1.1.1.m1.1.1" xref="S8.p1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.1.1.m1.1b"><times id="S8.p1.1.1.m1.1.1.cmml" xref="S8.p1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.1.1.m1.1c">\times</annotation></semantics></math></span>, compared to existing serverless FL systems, with <span id="S8.p1.2.2" class="ltx_text ltx_font_bold">2.7<math id="S8.p1.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.2.2.m1.1a"><mo id="S8.p1.2.2.m1.1.1" xref="S8.p1.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.2.2.m1.1b"><times id="S8.p1.2.2.m1.1.1.cmml" xref="S8.p1.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.2.2.m1.1c">\times</annotation></semantics></math></span> reduction on time-to-accuracy for ResNet-18.
LIFL also achieves <span id="S8.p1.3.3" class="ltx_text ltx_font_bold">1.8<math id="S8.p1.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.3.3.m1.1a"><mo id="S8.p1.3.3.m1.1.1" xref="S8.p1.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.3.3.m1.1b"><times id="S8.p1.3.3.m1.1.1.cmml" xref="S8.p1.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.3.3.m1.1c">\times</annotation></semantics></math></span> better efficiency and <span id="S8.p1.4.4" class="ltx_text ltx_font_bold">1.6<math id="S8.p1.4.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.4.4.m1.1a"><mo id="S8.p1.4.4.m1.1.1" xref="S8.p1.4.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.4.4.m1.1b"><times id="S8.p1.4.4.m1.1.1.cmml" xref="S8.p1.4.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.4.4.m1.1c">\times</annotation></semantics></math></span> speedup on time-to-accuracy than a serverful system.
In training ResNet-152 to reach 70% accuracy, LIFL is <span id="S8.p1.5.5" class="ltx_text ltx_font_bold">1.68<math id="S8.p1.5.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.5.5.m1.1a"><mo id="S8.p1.5.5.m1.1.1" xref="S8.p1.5.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.5.5.m1.1b"><times id="S8.p1.5.5.m1.1.1.cmml" xref="S8.p1.5.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.5.5.m1.1c">\times</annotation></semantics></math></span> faster than an existing serverless FL system, while reducing CPU costs by <span id="S8.p1.6.6" class="ltx_text ltx_font_bold">4.23<math id="S8.p1.6.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.6.6.m1.1a"><mo id="S8.p1.6.6.m1.1.1" xref="S8.p1.6.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.6.6.m1.1b"><times id="S8.p1.6.6.m1.1.1.cmml" xref="S8.p1.6.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.6.6.m1.1c">\times</annotation></semantics></math></span>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">We thank the US NSF for their generous support through grants CRI-1823270, CNS-1818971, and Cisco for their generous gift and support.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">aut (2023a)</span>
<span class="ltx_bibblock">
Autoscaling - Knative.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://knative.dev/docs/serving/autoscaling/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://knative.dev/docs/serving/autoscaling/</a>, 2023a.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">aut (2023b)</span>
<span class="ltx_bibblock">
Autoscaling - OpenFaaS.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://docs.openfaas.com/architecture/autoscaling/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.openfaas.com/architecture/autoscaling/</a>, 2023b.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ebp (2023a)</span>
<span class="ltx_bibblock">
extended Berkeley Packet Filter.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ebpf.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ebpf.io/</a>, 2023a.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ebp (2023b)</span>
<span class="ltx_bibblock">
BPF-HELPERS - list of eBPF helper functions.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://manpages.ubuntu.com/manpages/focal/en/man7/bpf-helpers.7.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://manpages.ubuntu.com/manpages/focal/en/man7/bpf-helpers.7.html</a>, 2023b.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ebp (2023c)</span>
<span class="ltx_bibblock">
BPF maps.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://docs.kernel.org/bpf/maps.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.kernel.org/bpf/maps.html</a>, 2023c.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">fat (2023)</span>
<span class="ltx_bibblock">
Fate: An Industrial Grade Federated Learning Framework.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://fate.fedai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fate.fedai.org/</a>, 2023.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">kna (2023)</span>
<span class="ltx_bibblock">
Knative.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://knative.dev" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://knative.dev</a>, 2023.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ope (2023)</span>
<span class="ltx_bibblock">
Open Federated Learning (OpenFL) - An Open-Source Framework For Federated Learning.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/intel/openfl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/intel/openfl</a>, 2023.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">fla (2024)</span>
<span class="ltx_bibblock">
Flame: a federated learning system for the edge.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/cisco-open/flame" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/cisco-open/flame</a>, 2024.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelmoniem et al. (2023)</span>
<span class="ltx_bibblock">
Abdelmoniem, A. M., Sahu, A. N., Canini, M., and Fahmy, S. A.

</span>
<span class="ltx_bibblock">Refl: Resource-efficient federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighteenth European Conference on Computer Systems</em>, EuroSys ’23, pp.  215–232, New York, NY, USA, 2023. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450394871.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3552326.3567485</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3552326.3567485" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3552326.3567485</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agache et al. (2020)</span>
<span class="ltx_bibblock">
Agache, A., Brooker, M., Iordache, A., Liguori, A., Neugebauer, R., Piwonka, P., and Popa, D.-M.

</span>
<span class="ltx_bibblock">Firecracker: Lightweight virtualization for serverless applications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)</em>, pp.  419–434, Santa Clara, CA, February 2020. USENIX Association.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-13-7.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/nsdi20/presentation/agache" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/nsdi20/presentation/agache</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akkus et al. (2018)</span>
<span class="ltx_bibblock">
Akkus, I. E., Chen, R., Rimac, I., Stein, M., Satzke, K., Beck, A., Aditya, P., and Hilt, V.

</span>
<span class="ltx_bibblock">SAND: Towards High-Performance serverless computing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2018 USENIX Annual Technical Conference (USENIX ATC 18)</em>, pp.  923–935, Boston, MA, July 2018. USENIX Association.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-01-4.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc18/presentation/akkus" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc18/presentation/akkus</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhasi et al. (2021)</span>
<span class="ltx_bibblock">
Bhasi, V. M., Gunasekaran, J. R., Thinakaran, P., Mishra, C. S., Kandemir, M. T., and Das, C.

</span>
<span class="ltx_bibblock">Kraken: Adaptive container provisioning for deploying dynamic dags in serverless platforms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Symposium on Cloud Computing</em>, SoCC ’21, pp.  153–167, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450386388.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3472883.3486992</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3472883.3486992" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472883.3486992</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2019)</span>
<span class="ltx_bibblock">
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon, C., Konečný, J., Mazzocchi, S., McMahan, B., Van Overveldt, T., Petrou, D., Ramage, D., and Roselander, J.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, volume 1, pp.  374–388, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlsys.org/paper/2019/file/bd686fd640be98efaae0091fa301e613-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlsys.org/paper/2019/file/bd686fd640be98efaae0091fa301e613-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2021)</span>
<span class="ltx_bibblock">
Cai, Q., Chaudhary, S., Vuppalapati, M., Hwang, J., and Agarwal, R.

</span>
<span class="ltx_bibblock">Understanding host network stack overheads.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM SIGCOMM 2021 Conference</em>, SIGCOMM ’21, pp.  65–77, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450383837.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3452296.3472888</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3452296.3472888" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3452296.3472888</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chadha et al. (2021)</span>
<span class="ltx_bibblock">
Chadha, M., Jindal, A., and Gerndt, M.

</span>
<span class="ltx_bibblock">Towards federated learning using faas fabric.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Sixth International Workshop on Serverless Computing</em>, WoSC’20, pp.  49–54, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450382045.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3429880.3430100</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3429880.3430100" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3429880.3430100</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daga et al. (2023)</span>
<span class="ltx_bibblock">
Daga, H., Shin, J., Garg, D., Gavrilovska, A., Lee, M., and Kompella, R. R.

</span>
<span class="ltx_bibblock">Flame: Simplifying topology extension in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 ACM Symposium on Cloud Computing</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duplyakin et al. (2019)</span>
<span class="ltx_bibblock">
Duplyakin, D., Ricci, R., Maricq, A., Wong, G., Duerig, J., Eide, E., Stoller, L., Hibler, M., Johnson, D., Webb, K., Akella, A., Wang, K., Ricart, G., Landweber, L., Elliott, C., Zink, M., Cecchet, E., Kar, S., and Mishra, P.

</span>
<span class="ltx_bibblock">The design and operation of CloudLab.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2019 USENIX Annual Technical Conference (USENIX ATC 19)</em>, pp.  1–14, Renton, WA, July 2019. USENIX Association.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-03-8.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc19/presentation/duplyakin" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc19/presentation/duplyakin</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elzohairy et al. (2022)</span>
<span class="ltx_bibblock">
Elzohairy, M., Chadha, M., Jindal, A., Grafberger, A., Gu, J., Gerndt, M., and Abboud, O.

</span>
<span class="ltx_bibblock">Fedlesscan: Mitigating stragglers in serverless federated learning, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.05739" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.05739</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2020)</span>
<span class="ltx_bibblock">
Fu, S., Mittal, R., Zhang, L., and Ratnasamy, S.

</span>
<span class="ltx_bibblock">Fast and efficient container startup at the edge via dependency scheduling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">3rd USENIX Workshop on Hot Topics in Edge Computing (HotEdge 20)</em>. USENIX Association, June 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/hotedge20/presentation/fu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/hotedge20/presentation/fu</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fuerst &amp; Sharma (2021)</span>
<span class="ltx_bibblock">
Fuerst, A. and Sharma, P.

</span>
<span class="ltx_bibblock">Faascache: Keeping serverless computing alive with greedy-dual caching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, ASPLOS ’21, pp.  386–400, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450383172.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3445814.3446757</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3445814.3446757" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3445814.3446757</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadepalli et al. (2020)</span>
<span class="ltx_bibblock">
Gadepalli, P. K., McBride, S., Peach, G., Cherkasova, L., and Parmer, G.

</span>
<span class="ltx_bibblock">Sledge: A serverless-first, light-weight wasm runtime for the edge.

</span>
<span class="ltx_bibblock">Middleware ’20, pp.  265–279, New York, NY, USA, 2020. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450381536.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3423211.3425680</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3423211.3425680" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3423211.3425680</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grafberger et al. (2021)</span>
<span class="ltx_bibblock">
Grafberger, A., Chadha, M., Jindal, A., Gu, J., and Gerndt, M.

</span>
<span class="ltx_bibblock">Fedless: Secure and scalable federated learning using serverless computing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Big Data (Big Data)</em>, pp.  164–173, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/BigData52589.2021.9672067</span>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2022)</span>
<span class="ltx_bibblock">
Guo, Y., Sun, Y., Hu, R., and Gong, Y.

</span>
<span class="ltx_bibblock">Hybrid local sgd for federated learning with heterogeneous communications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
He, C., Li, S., So, J., Zeng, X., Zhang, M., Wang, H., Wang, X., Vepakomma, P., Singh, A., Qiu, H., Zhu, X., Wang, J., Shen, L., Zhao, P., Kang, Y., Liu, Y., Raskar, R., Yang, Q., Annavaram, M., and Avestimehr, S.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine learning, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2007.13518" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2007.13518</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  770–778, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huba et al. (2022)</span>
<span class="ltx_bibblock">
Huba, D., Nguyen, J., Malik, K., Zhu, R., Rabbat, M., Yousefpour, A., Wu, C.-J., Zhan, H., Ustinov, P., Srinivas, H., Wang, K., Shoumikhin, A., Min, J., and Malek, M.

</span>
<span class="ltx_bibblock">Papaya: Practical, private, and scalable federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, volume 4, pp.  814–832, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlsys.org/paper/2022/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlsys.org/paper/2022/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayaram et al. (2022a)</span>
<span class="ltx_bibblock">
Jayaram, K., Muthusamy, V., Thomas, G., Verma, A., and Purcell, M.

</span>
<span class="ltx_bibblock">Lambda fl: Serverless aggregation for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">International Workshop on Trustable, Verifiable and Auditable Federated Learning</em>, pp.  9, 2022a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayaram et al. (2022b)</span>
<span class="ltx_bibblock">
Jayaram, K. R., Muthusamy, V., Thomas, G., Verma, A., and Purcell, M.

</span>
<span class="ltx_bibblock">Adaptive aggregation for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Big Data (Big Data)</em>, pp.  180–185, 2022b.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/BigData55660.2022.10021119</span>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayaram et al. (2022c)</span>
<span class="ltx_bibblock">
Jayaram, K. R., Verma, A., Thomas, G., and Muthusamy, V.

</span>
<span class="ltx_bibblock">Just-in-time aggregation for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2022 30th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</em>, pp.  1–8, 2022c.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/MASCOTS56607.2022.00009</span>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia &amp; Witchel (2021)</span>
<span class="ltx_bibblock">
Jia, Z. and Witchel, E.

</span>
<span class="ltx_bibblock">Nightcore: Efficient and scalable serverless computing for latency-sensitive, interactive microservices.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, ASPLOS ’21, pp.  152–166, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450383172.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3445814.3446701</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3445814.3446701" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3445814.3446701</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2022)</span>
<span class="ltx_bibblock">
Jiang, Z., Wang, W., Li, B., and Li, B.

</span>
<span class="ltx_bibblock">Pisces: Efficient federated learning via guided asynchronous training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Symposium on Cloud Computing</em>, SoCC ’22, pp.  370–385, New York, NY, USA, 2022. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450394147.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3542929.3563463</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3542929.3563463" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3542929.3563463</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2023)</span>
<span class="ltx_bibblock">
Jin, C., Zhang, Z., Xiang, X., Zou, S., Huang, G., Liu, X., and Jin, X.

</span>
<span class="ltx_bibblock">Ditto: Efficient serverless analytics with elastic parallelism.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGCOMM 2023 Conference</em>, ACM SIGCOMM ’23, pp.  406–419, New York, NY, USA, 2023. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400702365.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3603269.3604816</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3603269.3604816" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3603269.3604816</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaffes et al. (2022)</span>
<span class="ltx_bibblock">
Kaffes, K., Yadwadkar, N. J., and Kozyrakis, C.

</span>
<span class="ltx_bibblock">Hermod: Principled and practical scheduling for serverless functions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Symposium on Cloud Computing</em>, SoCC ’22, pp.  289–305, New York, NY, USA, 2022. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450394147.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3542929.3563468</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3542929.3563468" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3542929.3563468</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2021)</span>
<span class="ltx_bibblock">
Lai, F., Zhu, X., Madhyastha, H. V., and Chowdhury, M.

</span>
<span class="ltx_bibblock">Oort: Efficient federated learning via guided participant selection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">USENIX Symposium on Operating Systems Design and Implementation, OSDI</em>, pp.  19–35. USENIX Association, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2022)</span>
<span class="ltx_bibblock">
Lai, F., Dai, Y., Singapuram, S., Liu, J., Zhu, X., Madhyastha, H., and Chowdhury, M.

</span>
<span class="ltx_bibblock">Fedscale: Benchmarking model and system performance of federated learning at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.  11814–11827. PMLR, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lalitha et al. (2019)</span>
<span class="ltx_bibblock">
Lalitha, A., Kilinc, O. C., Javidi, T., and Koushanfar, F.

</span>
<span class="ltx_bibblock">Peer-to-peer federated learning on graphs, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1901.11173" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1901.11173</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, 2:429–450, 2020a.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Li, T., Sanjabi, M., Beirami, A., and Smith, V.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=ByexElSYDr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=ByexElSYDr</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin &amp; Glikson (2019)</span>
<span class="ltx_bibblock">
Lin, P.-M. and Glikson, A.

</span>
<span class="ltx_bibblock">Mitigating cold starts in serverless platforms: A pool-based approach, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1903.12221" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1903.12221</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Liu, J., Lai, F., Dai, Y., Akella, A., Madhyastha, H. V., and Chowdhury, M.

</span>
<span class="ltx_bibblock">Auxo: Efficient federated learning via scalable client clustering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 ACM Symposium on Cloud Computing</em>, SoCC ’23, pp.  125–141, New York, NY, USA, 2023a. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400703874.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3620678.3624651</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3620678.3624651" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3620678.3624651</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Liu, J., Lai, F., Ding, D., Zhang, Y., and Chowdhury, M.

</span>
<span class="ltx_bibblock">Venn: Resource management across federated learning jobs, 2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.08298" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.08298</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ludwig et al. (2020)</span>
<span class="ltx_bibblock">
Ludwig, H., Baracaldo, N., Thomas, G., Zhou, Y., Anwar, A., Rajamoni, S., Ong, Y., Radhakrishnan, J., Verma, A., Sinn, M., et al.

</span>
<span class="ltx_bibblock">Ibm federated learning: an enterprise framework white paper v0. 1.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.10987</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2007.10987" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2007.10987</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized Data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em>, volume 54 of <em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  1273–1282. PMLR, 20–22 Apr 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v54/mcmahan17a.html</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittal et al. (2021)</span>
<span class="ltx_bibblock">
Mittal, V., Qi, S., Bhattacharya, R., Lyu, X., Li, J., Kulkarni, S. G., Li, D., Hwang, J., Ramakrishnan, K. K., and Wood, T.

</span>
<span class="ltx_bibblock">Mu: An efficient, fair and responsive serverless framework for resource-constrained edge clouds.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Symposium on Cloud Computing</em>, SoCC ’21, pp.  168–181, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450386388.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3472883.3487014</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3472883.3487014" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472883.3487014</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2022)</span>
<span class="ltx_bibblock">
Nguyen, J., Malik, K., Zhan, H., Yousefpour, A., Rabbat, M., Malek, M., and Huba, D.

</span>
<span class="ltx_bibblock">Federated learning with buffered asynchronous aggregation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 25th International Conference on Artificial Intelligence and Statistics</em>, volume 151 of <em id="bib.bib46.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  3581–3607. PMLR, 28–30 Mar 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v151/nguyen22b.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v151/nguyen22b.html</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio &amp; Yonetani (2019)</span>
<span class="ltx_bibblock">
Nishio, T. and Yonetani, R.

</span>
<span class="ltx_bibblock">Client selection for federated learning with heterogeneous resources in mobile edge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ICC 2019-2019 IEEE international conference on communications (ICC)</em>, pp.  1–7. IEEE, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oakes et al. (2018)</span>
<span class="ltx_bibblock">
Oakes, E., Yang, L., Zhou, D., Houck, K., Harter, T., Arpaci-Dusseau, A., and Arpaci-Dusseau, R.

</span>
<span class="ltx_bibblock">SOCK: Rapid task provisioning with Serverless-Optimized containers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">2018 USENIX Annual Technical Conference (USENIX ATC 18)</em>, pp.  57–70, Boston, MA, July 2018. USENIX Association.

</span>
<span class="ltx_bibblock">ISBN 978-1-931971-44-7.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc18/presentation/oakes" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc18/presentation/oakes</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021a)</span>
<span class="ltx_bibblock">
Park, J., Choi, B., Lee, C., and Han, D.

</span>
<span class="ltx_bibblock">Graf: A graph neural network based proactive resource allocation framework for slo-oriented microservices.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th International Conference on Emerging Networking EXperiments and Technologies</em>, CoNEXT ’21, pp.  154–167, New York, NY, USA, 2021a. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450390989.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3485983.3494866</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3485983.3494866" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3485983.3494866</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021b)</span>
<span class="ltx_bibblock">
Park, J., Choi, B., Lee, C., and Han, D.

</span>
<span class="ltx_bibblock">Graf: A graph neural network based proactive resource allocation framework for slo-oriented microservices.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th International Conference on Emerging Networking EXperiments and Technologies</em>, CoNEXT ’21, pp.  154–167, New York, NY, USA, 2021b. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450390989.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3485983.3494866</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3485983.3494866" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3485983.3494866</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2021)</span>
<span class="ltx_bibblock">
Qi, S., Kulkarni, S. G., and Ramakrishnan, K. K.

</span>
<span class="ltx_bibblock">Assessing container network interface plugins: Functionality, performance, and scalability.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network and Service Management</em>, 18(1):656–671, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TNSM.2020.3047545</span>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2022)</span>
<span class="ltx_bibblock">
Qi, S., Monis, L., Zeng, Z., Wang, I.-c., and Ramakrishnan, K. K.

</span>
<span class="ltx_bibblock">Spright: Extracting the server from serverless computing! high-performance ebpf-based event-driven, shared-memory processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGCOMM 2022 Conference</em>, SIGCOMM ’22, pp.  780–794, New York, NY, USA, 2022. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450394208.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3544216.3544259</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3544216.3544259" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3544216.3544259</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Red Hat, Inc. (2022)</span>
<span class="ltx_bibblock">
Red Hat, Inc.

</span>
<span class="ltx_bibblock">Understanding the eBPF networking features in RHEL.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/assembly_understanding-the-ebpf-features-in-rhel-8_configuring-and-managing-networking" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/assembly_understanding-the-ebpf-features-in-rhel-8_configuring-and-managing-networking</a>, 2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. (2020)</span>
<span class="ltx_bibblock">
Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Konečný, J., Kumar, S., and McMahan, H. B.

</span>
<span class="ltx_bibblock">Adaptive federated optimization, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2003.00295" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2003.00295</a>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schall et al. (2022)</span>
<span class="ltx_bibblock">
Schall, D., Margaritov, A., Ustiugov, D., Sandberg, A., and Grot, B.

</span>
<span class="ltx_bibblock">Lukewarm serverless functions: Characterization and optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual International Symposium on Computer Architecture</em>, ISCA ’22, pp.  757–770, New York, NY, USA, 2022. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450386104.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3470496.3527390</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3470496.3527390" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3470496.3527390</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheller (2023)</span>
<span class="ltx_bibblock">
Scheller, B.

</span>
<span class="ltx_bibblock">Best practices for resizing and automatic scaling in Amazon EMR.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aws.amazon.com/blogs/big-data/best-practices-for-resizing-and-automatic-scaling-in-amazon-emr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/blogs/big-data/best-practices-for-resizing-and-automatic-scaling-in-amazon-emr/</a>, 2023.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_smallcaps">[online]</span>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahrad et al. (2020a)</span>
<span class="ltx_bibblock">
Shahrad, M., Fonseca, R., Goiri, I., Chaudhry, G., Batum, P., Cooke, J., Laureano, E., Tresness, C., Russinovich, M., and Bianchini, R.

</span>
<span class="ltx_bibblock">Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</em>, pp.  205–218. USENIX Association, July 2020a.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-14-4.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc20/presentation/shahrad" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc20/presentation/shahrad</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahrad et al. (2020b)</span>
<span class="ltx_bibblock">
Shahrad, M., Fonseca, R., Goiri, I., Chaudhry, G., Batum, P., Cooke, J., Laureano, E., Tresness, C., Russinovich, M., and Bianchini, R.

</span>
<span class="ltx_bibblock">Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</em>, pp.  205–218. USENIX Association, July 2020b.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-14-4.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc20/presentation/shahrad" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc20/presentation/shahrad</a>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shillaker &amp; Pietzuch (2020)</span>
<span class="ltx_bibblock">
Shillaker, S. and Pietzuch, P.

</span>
<span class="ltx_bibblock">Faasm: Lightweight isolation for efficient stateful serverless computing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</em>, pp.  419–433. USENIX Association, July 2020.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-14-4.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc20/presentation/shillaker" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc20/presentation/shillaker</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2022)</span>
<span class="ltx_bibblock">
Shin, J., Li, Y., Liu, Y., and Lee, S.-J.

</span>
<span class="ltx_bibblock">Fedbalancer: Data and pace control for efficient federated learning on heterogeneous clients.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services</em>, MobiSys ’22, pp.  436–449, New York, NY, USA, 2022. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450391856.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3498361.3538917</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3498361.3538917" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3498361.3538917</a>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhvi et al. (2021)</span>
<span class="ltx_bibblock">
Singhvi, A., Balasubramanian, A., Houck, K., Shaikh, M. D., Venkataraman, S., and Akella, A.

</span>
<span class="ltx_bibblock">Atoll: A scalable low-latency serverless platform.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Symposium on Cloud Computing</em>, SoCC ’21, pp.  138–152, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450386388.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3472883.3486981</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3472883.3486981" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472883.3486981</a>.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tariq et al. (2020)</span>
<span class="ltx_bibblock">
Tariq, A., Pahl, A., Nimmagadda, S., Rozner, E., and Lanka, S.

</span>
<span class="ltx_bibblock">Sequoia: Enabling quality-of-service in serverless computing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th ACM Symposium on Cloud Computing</em>, SoCC ’20, pp.  311–327, New York, NY, USA, 2020. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450381376.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3419111.3421306</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3419111.3421306" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3419111.3421306</a>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ustiugov et al. (2021)</span>
<span class="ltx_bibblock">
Ustiugov, D., Petrov, P., Kogias, M., Bugnion, E., and Grot, B.

</span>
<span class="ltx_bibblock">Benchmarking, analysis, and optimization of serverless function snapshots.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, ASPLOS ’21, pp.  559–572, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450383172.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3445814.3446714</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3445814.3446714" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3445814.3446714</a>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, A., Chang, S., Tian, H., Wang, H., Yang, H., Li, H., Du, R., and Cheng, Y.

</span>
<span class="ltx_bibblock">FaaSNet: Scalable and fast provisioning of custom serverless container runtimes at alibaba cloud function compute.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">2021 USENIX Annual Technical Conference (USENIX ATC 21)</em>, pp.  443–457. USENIX Association, July 2021.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-23-6.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/atc21/presentation/wang-ao" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc21/presentation/wang-ao</a>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Yang, J., Shi, R., and Ni, B.

</span>
<span class="ltx_bibblock">Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">IEEE 18th International Symposium on Biomedical Imaging (ISBI)</em>, pp.  191–195, 2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yu, M., Cao, T., Wang, W., and Chen, R.

</span>
<span class="ltx_bibblock">Following the data, not the function: Rethinking function orchestration in serverless computing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)</em>, pp.  1489–1504, Boston, MA, April 2023. USENIX Association.

</span>
<span class="ltx_bibblock">ISBN 978-1-939133-33-5.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/nsdi23/presentation/yu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/nsdi23/presentation/yu</a>.

</span>
</li>
</ul>
</section>
<figure id="A0.F11" class="ltx_figure"><img src="/html/2405.10968/assets/x25.png" id="A0.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Asynchronous FL <cite class="ltx_cite ltx_citemacro_cite">Huba et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> with different aggregation timing (“Eager” and “Lazy”).</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Message flow of intra-node and inter-node routing</h2>

<figure id="A1.F12" class="ltx_figure"><img src="/html/2405.10968/assets/x26.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Intra-/inter-node direct routing within hierarchical aggregation.</figcaption>
</figure>
<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text ltx_font_bold">Intra-node routing:</span>
LIFL makes full use of its shared memory support to facilitate <span id="A1.p1.1.2" class="ltx_text ltx_font_italic">zero-copy</span> exchange of model updates between aggregators.
The shared memory object in LIFL is addressed by the object key, which is a 16 byte string randomly generated by the shared memory manager when it initializes shared memory objects. We also assign each aggregator a unique ID. The zero-copy data exchange between aggregators depends on delivering the object key, as the data is kept in place in shared memory.</p>
</div>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p">LIFL utilizes eBPF’s SKMSG (integrated in the eBPF-based sidecar), combined with eBPF’s sockmap <cite class="ltx_cite ltx_citemacro_cite">Red Hat, Inc. (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>, to pass the object key between aggregators on the same node.
Upon receiving the object key,
the SKMSG program uses the ID of the source aggregator as the key to look up the <span id="A1.p2.1.1" class="ltx_text ltx_font_italic">sockmap</span> to find the socket interface of the destination aggregator so that the object key may be delivered to it for access of the shared memory object.</p>
</div>
<div id="A1.p3" class="ltx_para ltx_noindent">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text ltx_font_bold">Inter-node routing:</span> When the source aggregator communicates with a destination aggregator on a different node, it sends the object key to the local gateway first. The local gateway uses the object key to retrieve the model update from shared memory and performs the necessary payload transformation. It then uses the source aggregator ID to look up the inter-node routing table to obtain the destination aggregator ID and the IP address of the remote node hosting the destination aggregator. The model update is sent through the remote node’s gateway to the destination aggregator.
The remote gateway stores the received model update in shared memory and uses SKMSG to notify the destination aggregator, along with the local object key.</p>
</div>
<div id="A1.p4" class="ltx_para ltx_noindent">
<p id="A1.p4.1" class="ltx_p"><span id="A1.p4.1.1" class="ltx_text ltx_font_bold">Online hierarchy update:</span>
LIFL re-configures intra-/inter-node routes
each time the hierarchy is updated.
The routing manager in the LIFL agent takes the DAG input (generated by the TAG, §<a href="#A4" title="Appendix D Abstraction for fine-grained control" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>) from the control plane that describes the connectivity between aggregators, and correspondingly updates routes into the inter-node routing table in the gateway and in-kernel
<span id="A1.p4.1.2" class="ltx_text ltx_font_italic">sockmap</span>,
using the userspace eBPF helper, <span id="A1.p4.1.3" class="ltx_text ltx_font_typewriter">bpf_map_update_elem()</span> <cite class="ltx_cite ltx_citemacro_cite">ebp (<a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite>.
The TAG describes the cross-level data dependency between aggregators.
</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Model checkpoints</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">We support model checkpoints, where the model parameters are periodically saved to an external storage service to ensure data persistence and potential recovery in case of failures. The checkpointing occurs after the aggregator completes the aggregation of specified model updates, where the aggregator submits a request to the LIFL agent to perform model checkpoints asynchronously in the background.
This prevents checkpoint delays from being added to the aggregation completion time.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Message queueing flow in LIFL: Receive (RX) and Transmit (TX)</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">On the receive (RX) path, protocol processing by the kernel TCP/IP stack is first performed. The gateway running in userspace receives the raw L7 payload from the kernel and then
extracts the model updates (encoded as <span id="A3.p1.1.1" class="ltx_text ltx_font_typewriter">tensor</span> data type), depending on the adopted L7 protocol (<span id="A3.p1.1.2" class="ltx_text ltx_font_italic">e.g., </span>gRPC, MQTT).
We convert the model update from <span id="A3.p1.1.3" class="ltx_text ltx_font_typewriter">tensor</span> data type to <span id="A3.p1.1.4" class="ltx_text ltx_font_typewriter">NumpyArray</span> before writing it to shared memory, as Python’s <span id="A3.p1.1.5" class="ltx_text ltx_font_typewriter">multiprocessing</span> module
does not support manipulation of the <span id="A3.p1.1.6" class="ltx_text ltx_font_typewriter">tensor</span> data type.
On the transmit (TX) path, the reverse payload processing is done.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Abstraction for fine-grained control</h2>

<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.1" class="ltx_p">To facilitate fine-grained control of LIFL’s orchestration,
we treat an aggregator process within a sandboxed runtime (<span id="A4.p1.1.1" class="ltx_text ltx_font_italic">e.g., </span>container) as the atomic unit for management.
The control plane needs a generic means to describe connectivity between components and placement affinity.
We make use of Topology Abstraction Graph (<span id="A4.p1.1.2" class="ltx_text ltx_font_italic">TAG</span>) in Flame <cite class="ltx_cite ltx_citemacro_cite">Daga et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> to describe the aggregator-to-aggregator connectivity and aggregator-client connectivity.
Each node in such a graph is associated with a “<span id="A4.p1.1.3" class="ltx_text ltx_font_italic">role</span>” metadata, denoted as either aggregator or client. A “<span id="A4.p1.1.4" class="ltx_text ltx_font_italic">channel</span>” metadata denotes the underlying communication mechanism (<span id="A4.p1.1.5" class="ltx_text ltx_font_italic">e.g., </span>intra-node shared memory, inter-node kernel networking) used for connectivity.</p>
</div>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.1" class="ltx_p">We configure the placement-affinity to facilitate locality-aware placement through the <span id="A4.p2.1.1" class="ltx_text ltx_font_italic">groupBy</span> attribute in the channel abstraction, which
accepts a string as a label to specify a group. Therefore, keeping the same label in the attribute allows us to cluster roles into a group.
The LIFL coordinator enables necessary orchestration decisions, <span id="A4.p2.1.2" class="ltx_text ltx_font_italic">e.g., </span>runtime reuse and locality-aware placement, through manipulation of these abstractions (role and channel).</p>
</div>
<figure id="A4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x27.png" id="A4.F13.sf1.g1" class="ltx_graphics ltx_img_square" width="138" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CPU cost</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x28.png" id="A4.F13.sf2.g1" class="ltx_graphics ltx_img_square" width="138" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Memory cost</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F13.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.10968/assets/x29.png" id="A4.F13.sf3.g1" class="ltx_graphics ltx_img_square" width="138" height="153" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>End-to-end delay</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Message queuing overheads.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Maximum Service Capacity of Worker Nodes</h2>

<div id="A5.p1" class="ltx_para ltx_noindent">
<p id="A5.p1.11" class="ltx_p">LIFL actively monitors both <math id="A5.p1.1.m1.2" class="ltx_Math" alttext="E_{i,t}" display="inline"><semantics id="A5.p1.1.m1.2a"><msub id="A5.p1.1.m1.2.3" xref="A5.p1.1.m1.2.3.cmml"><mi id="A5.p1.1.m1.2.3.2" xref="A5.p1.1.m1.2.3.2.cmml">E</mi><mrow id="A5.p1.1.m1.2.2.2.4" xref="A5.p1.1.m1.2.2.2.3.cmml"><mi id="A5.p1.1.m1.1.1.1.1" xref="A5.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="A5.p1.1.m1.2.2.2.4.1" xref="A5.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="A5.p1.1.m1.2.2.2.2" xref="A5.p1.1.m1.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.2b"><apply id="A5.p1.1.m1.2.3.cmml" xref="A5.p1.1.m1.2.3"><csymbol cd="ambiguous" id="A5.p1.1.m1.2.3.1.cmml" xref="A5.p1.1.m1.2.3">subscript</csymbol><ci id="A5.p1.1.m1.2.3.2.cmml" xref="A5.p1.1.m1.2.3.2">𝐸</ci><list id="A5.p1.1.m1.2.2.2.3.cmml" xref="A5.p1.1.m1.2.2.2.4"><ci id="A5.p1.1.m1.1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1.1">𝑖</ci><ci id="A5.p1.1.m1.2.2.2.2.cmml" xref="A5.p1.1.m1.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.2c">E_{i,t}</annotation></semantics></math>
and the arrival rate <math id="A5.p1.2.m2.2" class="ltx_Math" alttext="k_{i,t}" display="inline"><semantics id="A5.p1.2.m2.2a"><msub id="A5.p1.2.m2.2.3" xref="A5.p1.2.m2.2.3.cmml"><mi id="A5.p1.2.m2.2.3.2" xref="A5.p1.2.m2.2.3.2.cmml">k</mi><mrow id="A5.p1.2.m2.2.2.2.4" xref="A5.p1.2.m2.2.2.2.3.cmml"><mi id="A5.p1.2.m2.1.1.1.1" xref="A5.p1.2.m2.1.1.1.1.cmml">i</mi><mo id="A5.p1.2.m2.2.2.2.4.1" xref="A5.p1.2.m2.2.2.2.3.cmml">,</mo><mi id="A5.p1.2.m2.2.2.2.2" xref="A5.p1.2.m2.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.2b"><apply id="A5.p1.2.m2.2.3.cmml" xref="A5.p1.2.m2.2.3"><csymbol cd="ambiguous" id="A5.p1.2.m2.2.3.1.cmml" xref="A5.p1.2.m2.2.3">subscript</csymbol><ci id="A5.p1.2.m2.2.3.2.cmml" xref="A5.p1.2.m2.2.3.2">𝑘</ci><list id="A5.p1.2.m2.2.2.2.3.cmml" xref="A5.p1.2.m2.2.2.2.4"><ci id="A5.p1.2.m2.1.1.1.1.cmml" xref="A5.p1.2.m2.1.1.1.1">𝑖</ci><ci id="A5.p1.2.m2.2.2.2.2.cmml" xref="A5.p1.2.m2.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.2c">k_{i,t}</annotation></semantics></math> using the sidecar in §<a href="#S4.SS3" title="4.3 eBPF-based Sidecar ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
We determine the value of <math id="A5.p1.3.m3.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="A5.p1.3.m3.1a"><mrow id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml"><mi id="A5.p1.3.m3.1.1.2" xref="A5.p1.3.m3.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="A5.p1.3.m3.1.1.1" xref="A5.p1.3.m3.1.1.1.cmml">​</mo><msub id="A5.p1.3.m3.1.1.3" xref="A5.p1.3.m3.1.1.3.cmml"><mi id="A5.p1.3.m3.1.1.3.2" xref="A5.p1.3.m3.1.1.3.2.cmml">C</mi><mi id="A5.p1.3.m3.1.1.3.3" xref="A5.p1.3.m3.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><apply id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1"><times id="A5.p1.3.m3.1.1.1.cmml" xref="A5.p1.3.m3.1.1.1"></times><ci id="A5.p1.3.m3.1.1.2.cmml" xref="A5.p1.3.m3.1.1.2">𝑀</ci><apply id="A5.p1.3.m3.1.1.3.cmml" xref="A5.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="A5.p1.3.m3.1.1.3.1.cmml" xref="A5.p1.3.m3.1.1.3">subscript</csymbol><ci id="A5.p1.3.m3.1.1.3.2.cmml" xref="A5.p1.3.m3.1.1.3.2">𝐶</ci><ci id="A5.p1.3.m3.1.1.3.3.cmml" xref="A5.p1.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">MC_{i}</annotation></semantics></math> offline. We incrementally increase the arrival rate <math id="A5.p1.4.m4.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="A5.p1.4.m4.1a"><msub id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml"><mi id="A5.p1.4.m4.1.1.2" xref="A5.p1.4.m4.1.1.2.cmml">k</mi><mi id="A5.p1.4.m4.1.1.3" xref="A5.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><apply id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A5.p1.4.m4.1.1.1.cmml" xref="A5.p1.4.m4.1.1">subscript</csymbol><ci id="A5.p1.4.m4.1.1.2.cmml" xref="A5.p1.4.m4.1.1.2">𝑘</ci><ci id="A5.p1.4.m4.1.1.3.cmml" xref="A5.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">k_{i}</annotation></semantics></math> to node <math id="A5.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A5.p1.5.m5.1a"><mi id="A5.p1.5.m5.1.1" xref="A5.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A5.p1.5.m5.1b"><ci id="A5.p1.5.m5.1.1.cmml" xref="A5.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.5.m5.1c">i</annotation></semantics></math>. Let <math id="A5.p1.6.m6.1" class="ltx_Math" alttext="k^{\prime}_{i}" display="inline"><semantics id="A5.p1.6.m6.1a"><msubsup id="A5.p1.6.m6.1.1" xref="A5.p1.6.m6.1.1.cmml"><mi id="A5.p1.6.m6.1.1.2.2" xref="A5.p1.6.m6.1.1.2.2.cmml">k</mi><mi id="A5.p1.6.m6.1.1.3" xref="A5.p1.6.m6.1.1.3.cmml">i</mi><mo id="A5.p1.6.m6.1.1.2.3" xref="A5.p1.6.m6.1.1.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="A5.p1.6.m6.1b"><apply id="A5.p1.6.m6.1.1.cmml" xref="A5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A5.p1.6.m6.1.1.1.cmml" xref="A5.p1.6.m6.1.1">subscript</csymbol><apply id="A5.p1.6.m6.1.1.2.cmml" xref="A5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A5.p1.6.m6.1.1.2.1.cmml" xref="A5.p1.6.m6.1.1">superscript</csymbol><ci id="A5.p1.6.m6.1.1.2.2.cmml" xref="A5.p1.6.m6.1.1.2.2">𝑘</ci><ci id="A5.p1.6.m6.1.1.2.3.cmml" xref="A5.p1.6.m6.1.1.2.3">′</ci></apply><ci id="A5.p1.6.m6.1.1.3.cmml" xref="A5.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.6.m6.1c">k^{\prime}_{i}</annotation></semantics></math> and <math id="A5.p1.7.m7.1" class="ltx_Math" alttext="E^{\prime}_{t}" display="inline"><semantics id="A5.p1.7.m7.1a"><msubsup id="A5.p1.7.m7.1.1" xref="A5.p1.7.m7.1.1.cmml"><mi id="A5.p1.7.m7.1.1.2.2" xref="A5.p1.7.m7.1.1.2.2.cmml">E</mi><mi id="A5.p1.7.m7.1.1.3" xref="A5.p1.7.m7.1.1.3.cmml">t</mi><mo id="A5.p1.7.m7.1.1.2.3" xref="A5.p1.7.m7.1.1.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="A5.p1.7.m7.1b"><apply id="A5.p1.7.m7.1.1.cmml" xref="A5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A5.p1.7.m7.1.1.1.cmml" xref="A5.p1.7.m7.1.1">subscript</csymbol><apply id="A5.p1.7.m7.1.1.2.cmml" xref="A5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A5.p1.7.m7.1.1.2.1.cmml" xref="A5.p1.7.m7.1.1">superscript</csymbol><ci id="A5.p1.7.m7.1.1.2.2.cmml" xref="A5.p1.7.m7.1.1.2.2">𝐸</ci><ci id="A5.p1.7.m7.1.1.2.3.cmml" xref="A5.p1.7.m7.1.1.2.3">′</ci></apply><ci id="A5.p1.7.m7.1.1.3.cmml" xref="A5.p1.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.7.m7.1c">E^{\prime}_{t}</annotation></semantics></math> denote the arrival rate and average execution time at the point we observe a significant increase in <math id="A5.p1.8.m8.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="A5.p1.8.m8.1a"><msub id="A5.p1.8.m8.1.1" xref="A5.p1.8.m8.1.1.cmml"><mi id="A5.p1.8.m8.1.1.2" xref="A5.p1.8.m8.1.1.2.cmml">E</mi><mi id="A5.p1.8.m8.1.1.3" xref="A5.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A5.p1.8.m8.1b"><apply id="A5.p1.8.m8.1.1.cmml" xref="A5.p1.8.m8.1.1"><csymbol cd="ambiguous" id="A5.p1.8.m8.1.1.1.cmml" xref="A5.p1.8.m8.1.1">subscript</csymbol><ci id="A5.p1.8.m8.1.1.2.cmml" xref="A5.p1.8.m8.1.1.2">𝐸</ci><ci id="A5.p1.8.m8.1.1.3.cmml" xref="A5.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.8.m8.1c">E_{i}</annotation></semantics></math>. This indicates that node <math id="A5.p1.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A5.p1.9.m9.1a"><mi id="A5.p1.9.m9.1.1" xref="A5.p1.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A5.p1.9.m9.1b"><ci id="A5.p1.9.m9.1.1.cmml" xref="A5.p1.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.9.m9.1c">i</annotation></semantics></math> is becoming overloaded and we estimate <math id="A5.p1.10.m10.1" class="ltx_Math" alttext="MC_{i}" display="inline"><semantics id="A5.p1.10.m10.1a"><mrow id="A5.p1.10.m10.1.1" xref="A5.p1.10.m10.1.1.cmml"><mi id="A5.p1.10.m10.1.1.2" xref="A5.p1.10.m10.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="A5.p1.10.m10.1.1.1" xref="A5.p1.10.m10.1.1.1.cmml">​</mo><msub id="A5.p1.10.m10.1.1.3" xref="A5.p1.10.m10.1.1.3.cmml"><mi id="A5.p1.10.m10.1.1.3.2" xref="A5.p1.10.m10.1.1.3.2.cmml">C</mi><mi id="A5.p1.10.m10.1.1.3.3" xref="A5.p1.10.m10.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.10.m10.1b"><apply id="A5.p1.10.m10.1.1.cmml" xref="A5.p1.10.m10.1.1"><times id="A5.p1.10.m10.1.1.1.cmml" xref="A5.p1.10.m10.1.1.1"></times><ci id="A5.p1.10.m10.1.1.2.cmml" xref="A5.p1.10.m10.1.1.2">𝑀</ci><apply id="A5.p1.10.m10.1.1.3.cmml" xref="A5.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="A5.p1.10.m10.1.1.3.1.cmml" xref="A5.p1.10.m10.1.1.3">subscript</csymbol><ci id="A5.p1.10.m10.1.1.3.2.cmml" xref="A5.p1.10.m10.1.1.3.2">𝐶</ci><ci id="A5.p1.10.m10.1.1.3.3.cmml" xref="A5.p1.10.m10.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.10.m10.1c">MC_{i}</annotation></semantics></math> as <math id="A5.p1.11.m11.1" class="ltx_Math" alttext="k^{\prime}_{i}\times E^{\prime}_{i}" display="inline"><semantics id="A5.p1.11.m11.1a"><mrow id="A5.p1.11.m11.1.1" xref="A5.p1.11.m11.1.1.cmml"><msubsup id="A5.p1.11.m11.1.1.2" xref="A5.p1.11.m11.1.1.2.cmml"><mi id="A5.p1.11.m11.1.1.2.2.2" xref="A5.p1.11.m11.1.1.2.2.2.cmml">k</mi><mi id="A5.p1.11.m11.1.1.2.3" xref="A5.p1.11.m11.1.1.2.3.cmml">i</mi><mo id="A5.p1.11.m11.1.1.2.2.3" xref="A5.p1.11.m11.1.1.2.2.3.cmml">′</mo></msubsup><mo lspace="0.222em" rspace="0.222em" id="A5.p1.11.m11.1.1.1" xref="A5.p1.11.m11.1.1.1.cmml">×</mo><msubsup id="A5.p1.11.m11.1.1.3" xref="A5.p1.11.m11.1.1.3.cmml"><mi id="A5.p1.11.m11.1.1.3.2.2" xref="A5.p1.11.m11.1.1.3.2.2.cmml">E</mi><mi id="A5.p1.11.m11.1.1.3.3" xref="A5.p1.11.m11.1.1.3.3.cmml">i</mi><mo id="A5.p1.11.m11.1.1.3.2.3" xref="A5.p1.11.m11.1.1.3.2.3.cmml">′</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.11.m11.1b"><apply id="A5.p1.11.m11.1.1.cmml" xref="A5.p1.11.m11.1.1"><times id="A5.p1.11.m11.1.1.1.cmml" xref="A5.p1.11.m11.1.1.1"></times><apply id="A5.p1.11.m11.1.1.2.cmml" xref="A5.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="A5.p1.11.m11.1.1.2.1.cmml" xref="A5.p1.11.m11.1.1.2">subscript</csymbol><apply id="A5.p1.11.m11.1.1.2.2.cmml" xref="A5.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="A5.p1.11.m11.1.1.2.2.1.cmml" xref="A5.p1.11.m11.1.1.2">superscript</csymbol><ci id="A5.p1.11.m11.1.1.2.2.2.cmml" xref="A5.p1.11.m11.1.1.2.2.2">𝑘</ci><ci id="A5.p1.11.m11.1.1.2.2.3.cmml" xref="A5.p1.11.m11.1.1.2.2.3">′</ci></apply><ci id="A5.p1.11.m11.1.1.2.3.cmml" xref="A5.p1.11.m11.1.1.2.3">𝑖</ci></apply><apply id="A5.p1.11.m11.1.1.3.cmml" xref="A5.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="A5.p1.11.m11.1.1.3.1.cmml" xref="A5.p1.11.m11.1.1.3">subscript</csymbol><apply id="A5.p1.11.m11.1.1.3.2.cmml" xref="A5.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="A5.p1.11.m11.1.1.3.2.1.cmml" xref="A5.p1.11.m11.1.1.3">superscript</csymbol><ci id="A5.p1.11.m11.1.1.3.2.2.cmml" xref="A5.p1.11.m11.1.1.3.2.2">𝐸</ci><ci id="A5.p1.11.m11.1.1.3.2.3.cmml" xref="A5.p1.11.m11.1.1.3.2.3">′</ci></apply><ci id="A5.p1.11.m11.1.1.3.3.cmml" xref="A5.p1.11.m11.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.11.m11.1c">k^{\prime}_{i}\times E^{\prime}_{i}</annotation></semantics></math>.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>In-place message queueing benefit</h2>

<div id="A6.p1" class="ltx_para ltx_noindent">
<p id="A6.p1.3" class="ltx_p">We examine LIFL’s in-place message queuing through a comparison with the serverful and serverless alternatives depicted in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, including the <span id="A6.p1.3.1" class="ltx_text ltx_font_italic">monolithic</span> serverful setup (denoted as <span id="A6.p1.3.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-mono</span>), the <span id="A6.p1.3.3" class="ltx_text ltx_font_italic">microservice</span>-based serverful setup (denoted as <span id="A6.p1.3.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-micro</span>), and the <span id="A6.p1.3.5" class="ltx_text ltx_font_italic">basic</span> serverless setup (denoted as <span id="A6.p1.3.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-B</span>).
We quantify the overheads of message queuing for a single model update transfer between
the client to the aggregator.
We consider three metrics: (1) the total memory consumed for
queuing the model update along the data pipeline; (2) the CPU cycles spent in the data pipeline;
and (3) the end-to-end networking delay from the client to the aggregator.
Note that we exclude the overhead on the client-side.
We consider three ML models with distinct sizes: (<span id="A6.p1.3.7" class="ltx_text ltx_font_typewriter ltx_font_bold">M1</span>) ResNet-18 (<math id="A6.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p1.1.m1.1a"><mo id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><csymbol cd="latexml" id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">\sim</annotation></semantics></math>44MB), (<span id="A6.p1.3.8" class="ltx_text ltx_font_typewriter ltx_font_bold">M2</span>) ResNet-34 (<math id="A6.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p1.2.m2.1a"><mo id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><csymbol cd="latexml" id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">\sim</annotation></semantics></math>83MB), and (<span id="A6.p1.3.9" class="ltx_text ltx_font_typewriter ltx_font_bold">M3</span>) ResNet-152 (<math id="A6.p1.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p1.3.m3.1a"><mo id="A6.p1.3.m3.1.1" xref="A6.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p1.3.m3.1b"><csymbol cd="latexml" id="A6.p1.3.m3.1.1.cmml" xref="A6.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.3.m3.1c">\sim</annotation></semantics></math>232MB).</p>
</div>
<div id="A6.p2" class="ltx_para ltx_noindent">
<p id="A6.p2.1" class="ltx_p">Fig. <a href="#A4.F13" title="Figure 13 ‣ Appendix D Abstraction for fine-grained control" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the results of CPU, memory cost and end-to-end networking delay. The memory consumption in <span id="A6.p2.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-mono</span> is mainly from the in-memory queue inside the aggregator. For LIFL it is primarily consumed by the shared memory used to buffer the model update.
But, <span id="A6.p2.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-B</span> consumes 3<math id="A6.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A6.p2.1.m1.1a"><mo id="A6.p2.1.m1.1.1" xref="A6.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A6.p2.1.m1.1b"><times id="A6.p2.1.m1.1.1.cmml" xref="A6.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A6.p2.1.m1.1c">\times</annotation></semantics></math> more memory than <span id="A6.p2.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-mono</span> and LIFL. The extra memory consumption of <span id="A6.p2.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-B</span> comes from the use of sidecar and message broker, both of which need to locally buffer the model update. <span id="A6.p2.1.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-micro</span>, on the other hand, saves one queuing stage at the sidecar, but still incurs the queuing at the message broker and consuming extra memory.
LIFL’s in-place message queuing totally eliminates these unnecessary queuing stages.</p>
</div>
<div id="A6.p3" class="ltx_para ltx_noindent">
<p id="A6.p3.8" class="ltx_p">Looking at the CPU consumption, LIFL is <math id="A6.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p3.1.m1.1a"><mo id="A6.p3.1.m1.1.1" xref="A6.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p3.1.m1.1b"><csymbol cd="latexml" id="A6.p3.1.m1.1.1.cmml" xref="A6.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.1.m1.1c">\sim</annotation></semantics></math>1.5<math id="A6.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A6.p3.2.m2.1a"><mo id="A6.p3.2.m2.1.1" xref="A6.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A6.p3.2.m2.1b"><times id="A6.p3.2.m2.1.1.cmml" xref="A6.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.2.m2.1c">\times</annotation></semantics></math> and <math id="A6.p3.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p3.3.m3.1a"><mo id="A6.p3.3.m3.1.1" xref="A6.p3.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p3.3.m3.1b"><csymbol cd="latexml" id="A6.p3.3.m3.1.1.cmml" xref="A6.p3.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.3.m3.1c">\sim</annotation></semantics></math>1.9<math id="A6.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A6.p3.4.m4.1a"><mo id="A6.p3.4.m4.1.1" xref="A6.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A6.p3.4.m4.1b"><times id="A6.p3.4.m4.1.1.cmml" xref="A6.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.4.m4.1c">\times</annotation></semantics></math> less than <span id="A6.p3.8.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-B</span> and <span id="A6.p3.8.2" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-micro</span>, respectively. In terms of the end-to-end networking delay (client to aggregator), LIFL is <math id="A6.p3.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p3.5.m5.1a"><mo id="A6.p3.5.m5.1.1" xref="A6.p3.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p3.5.m5.1b"><csymbol cd="latexml" id="A6.p3.5.m5.1.1.cmml" xref="A6.p3.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.5.m5.1c">\sim</annotation></semantics></math>1.3<math id="A6.p3.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A6.p3.6.m6.1a"><mo id="A6.p3.6.m6.1.1" xref="A6.p3.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A6.p3.6.m6.1b"><times id="A6.p3.6.m6.1.1.cmml" xref="A6.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.6.m6.1c">\times</annotation></semantics></math> and <math id="A6.p3.7.m7.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A6.p3.7.m7.1a"><mo id="A6.p3.7.m7.1.1" xref="A6.p3.7.m7.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A6.p3.7.m7.1b"><csymbol cd="latexml" id="A6.p3.7.m7.1.1.cmml" xref="A6.p3.7.m7.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.7.m7.1c">\sim</annotation></semantics></math>1.7<math id="A6.p3.8.m8.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A6.p3.8.m8.1a"><mo id="A6.p3.8.m8.1.1" xref="A6.p3.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A6.p3.8.m8.1b"><times id="A6.p3.8.m8.1.1.cmml" xref="A6.p3.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.8.m8.1c">\times</annotation></semantics></math> less than <span id="A6.p3.8.3" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-B</span> and <span id="A6.p3.8.4" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-micro</span>, respectively. LIFL’s improvement in CPU cost and networking delay, compared to <span id="A6.p3.8.5" class="ltx_text ltx_font_typewriter ltx_font_bold">SL-B</span> and <span id="A6.p3.8.6" class="ltx_text ltx_font_typewriter ltx_font_bold">SF-micro</span>, are also a result of the elimination of the sidecar and message broker from the data pipeline, and the message queuing if far more efficient.
This illustrates the benefits of LIFL’s in-place message queuing, achieving the equivalent efficiency and performance of a monolithic, serverful design (with far less resource consumption as we see for typical FL applications).
</p>
</div>
<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Stateful “tax” in LIFL</h3>

<div id="A6.SS1.p1" class="ltx_para ltx_noindent">
<p id="A6.SS1.p1.1" class="ltx_p">The per-node gateway is a key component that enables a number of data plane functionalities in LIFL, including in-place message queuing and inter-node data transfer. Unlike stateless aggregators, the gateway is deployed as a stateful, persistent component on every LIFL worker node. This raises the concern about the stateful “tax”, <span id="A6.SS1.p1.1.1" class="ltx_text ltx_font_italic">i.e., </span>the CPU/memory cost of having stateful components in the FL system.</p>
</div>
<div id="A6.SS1.p2" class="ltx_para ltx_noindent">
<p id="A6.SS1.p2.1" class="ltx_p">On the other hand, a stateful “tax” of some form commonly exists in serverful and serverless alternatives, as shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The stateful component in a monolithic serverful setup is the aggregator itself, running as an “always-on” monolith. In the microservice-based serverful setup, the message broker is the stateful component, as is the case for the basic serverless setup.
We quantitatively compare the stateful “tax” of LIFL’s gateway with serverful and serverless alternatives in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The result in §<a href="#S4.SS2" title="4.2 In-place Message Queuing ‣ 4 Optimizing the Serverless Data-Plane in LIFL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> shows that stateful “tax” in LIFL is the lowest.</p>
</div>
<figure id="A6.F14" class="ltx_figure"><img src="/html/2405.10968/assets/x30.png" id="A6.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Step-based processing model.</figcaption>
</figure>
</section>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Step-based Processing Model</h2>

<div id="A7.p1" class="ltx_para ltx_noindent">
<p id="A7.p1.1" class="ltx_p">The basic processing model of an LIFL aggregator can be abstracted as a multiple-producer, single-consumer pattern, as shown in Fig. <a href="#A6.F14" title="Figure 14 ‣ F.1 Stateful “tax” in LIFL ‣ Appendix F In-place message queueing benefit" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. Multiple upstream producers (clients or aggregators) are mapped to a single consumer (aggregator only). The single consumer gathers model updates from assigned producers and computes the aggregated model update.</p>
</div>
<div id="A7.p2" class="ltx_para ltx_noindent">
<p id="A7.p2.1" class="ltx_p">Looking deeper into the aggregator, LIFL adopts a step-based processing model. At the core of this design is a processing pipeline of three steps: <span id="A7.p2.1.1" class="ltx_text ltx_font_bold">(1)</span> <span id="A7.p2.1.2" class="ltx_text ltx_font_typewriter">Recv:</span> Receive model updates from all assigned producers. The received model update is enqueued in a FIFO queue. In LIFL, the object key of the model update is enqueued as the actual model update resides in shared memory; <span id="A7.p2.1.3" class="ltx_text ltx_font_bold">(2)</span> <span id="A7.p2.1.4" class="ltx_text ltx_font_typewriter">Agg:</span> Aggregator dequeues a model update from the FIFO queue in <span id="A7.p2.1.5" class="ltx_text ltx_font_typewriter">Recv</span> and then aggregates it. The <span id="A7.p2.1.6" class="ltx_text ltx_font_typewriter">Agg</span> step checks if the aggregation goal is met after the dequeued update is aggregated.
If the aggregation goal is not met, <span id="A7.p2.1.7" class="ltx_text ltx_font_typewriter">Agg</span> is repeated until the aggregation goal is met, before moving to <span id="A7.p2.1.8" class="ltx_text ltx_font_typewriter">Send</span>; and <span id="A7.p2.1.9" class="ltx_text ltx_font_bold">(3)</span> <span id="A7.p2.1.10" class="ltx_text ltx_font_typewriter">Send:</span> sends the final model update to the designated consumer.
The execution of <span id="A7.p2.1.11" class="ltx_text ltx_font_typewriter">Recv</span> and <span id="A7.p2.1.12" class="ltx_text ltx_font_typewriter">Agg</span> overlaps to enable eager aggregation, <span id="A7.p2.1.13" class="ltx_text ltx_font_italic">i.e., </span>once the <span id="A7.p2.1.14" class="ltx_text ltx_font_typewriter">Recv</span> step receives a model update, it immediately passes the model update to <span id="A7.p2.1.15" class="ltx_text ltx_font_typewriter">Agg</span> step for aggregation.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Anonymous Authors"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Federated Learning, Serverless Computing, eBPF"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="mlsys 2024"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="LIFL: A Lightweight, Event-driven Serverless Platform for Federated Learning"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.10967" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.10968" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.10968">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.10968" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.10969" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 16:31:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
