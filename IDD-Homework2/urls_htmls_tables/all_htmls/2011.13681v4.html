<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2011.13681] Point and Ask: Incorporating Pointing into Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) has become one of the key benchmarks of visual recognition progress. Multiple VQA extensions have been explored to better simulate real-world settings: different question formulations, c…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Point and Ask: Incorporating Pointing into Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Point and Ask: Incorporating Pointing into Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2011.13681">

<!--Generated on Fri Mar  8 13:39:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Point and Ask: Incorporating Pointing into Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arjun Mani , Nobline Yoo, Will Hinthorn , Olga Russakovsky 
<br class="ltx_break">Princeton University
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">asm2290@columbia.edu, olgarus@cs.princeton.edu</span>
</span><span class="ltx_author_notes">AM is currently a PhD student at Columbia University. Work was conducted while he was an undergraduate student at Princeton University. Columbia email address is provided above for contact regarding this work.WH is currently at Robust Intelligence. Work was conducted while he was an undergraduate student at Princeton University.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Visual Question Answering (VQA) has become one of the key benchmarks of visual recognition progress. Multiple VQA extensions have been explored to better simulate real-world settings: different question formulations, changing training and test distributions, conversational consistency in dialogues, and explanation-based answering. In this work, we further expand this space by considering visual questions that include a spatial point of reference. Pointing is a nearly universal gesture among humans, and real-world VQA is likely to involve a gesture towards the target region.</p>
<p id="id3.id2" class="ltx_p">Concretely, we (1) introduce and motivate point-input questions as an extension of VQA, (2) define three novel classes of questions within this space, and (3) for each class, introduce both a benchmark dataset and a series of model designs to handle its unique challenges. There are two key distinctions from prior work. First, we explicitly design the benchmarks to require the point input, i.e., we ensure that the visual question cannot be answered accurately without the spatial reference. Second, we explicitly explore the more realistic point spatial input rather than the standard but unnatural bounding box input. Through our exploration we uncover and address several visual recognition challenges, including the ability to reason both locally and globally about the image, and to effectively combine visual, language and spatial inputs. Code is available at: <a target="_blank" href="https://github.com/princetonvisualai/pointingqa" title="" class="ltx_ref ltx_href"><em id="id3.id2.1.1" class="ltx_emph ltx_font_italic">github.com/princetonvisualai/pointingqa</em></a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) has emerged as a popular and challenging task in computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. When the task was first introduced, the challenge of answering a natural language question about an image was already a significant leap beyond more conventional tasks such as object recognition. Since then, questions in VQA benchmarks have been increasingly growing in complexity: for example, “Are there any cups to the left of the tray on top of the table?” is an example question from the recent GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) benchmark. Such questions effectively test the ability of VQA agents to parse very complex sentences, but arguably are becoming less realistic.
In a real-world setting, it’s unlikely that a human would use the phrase “to the left of the tray on top of the table.” It’s far more likely that they would instead ask “Are there any cups <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">over there</em>” and point to the left of the tray. In fact, human psychology literature shows that pointing to interesting objects or situations is one of the first ways by which babies communicate intention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Understanding pointing as part of a visually grounded dialog with humans would naturally be a key ability of real-world AI systems.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2011.13681/assets/fig1keynote3.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Three types of visual questions requiring a point input (red) which we propose and analyze in this work.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We thus propose to expand the space of VQA by considering visual questions that further include a spatial point of reference for context. Prior works have used visual grounding to expand the question space of VQA: e.g., Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> introduced “which” question with image regions as candidate answers; Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> contains questions that are associated with particular regions in the image; GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> leverages the grounded scene graph in its question construction process. There are two key distinctions of our proposal from this line of work. First, we explicitly design the benchmarks to <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">require</em> the point input, i.e., we ensure that the visual question cannot be answered accurately without the spatial reference. Second, we explore the more realistic <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">point</em> spatial input rather than the standard but unnatural bounding box used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We introduce a set of tasks exploring different aspects and challenges of point-based spatial disambiguation, shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In all tasks the input is an image and a single pixel in the image corresponding to the spatial grounding. The target output is a multiple choice answer.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We first consider two narrow settings: (1) <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span>, where only the local region around the point is relevant to the question, e.g., “What color is <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">this</em> shirt?” where <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">this</em> is specified by a pixel in the image (Sec. <a href="#S3" title="3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and (2) <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">PointQA-LookTwice</span>, which requires a global understanding of the image, e.g., “is another shirt <em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">this</em> color?” (Sec. <a href="#S4" title="4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). For each we construct a corresponding dataset from Visual Genome annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, with 57,628 questions across 18,830 images and 57,405 questions across 34,676 images, respectively. We then modify and benchmark VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to incorporate the point input.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Finally, we consider the general setting of unconstrained questions which require point disambiguation in <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> by adapting human-written questions from the Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> dataset, resulting in 319,300 questions over 25,420 images (Sec. <a href="#S5" title="5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). We modify the state-of-the-art Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> models to incorporate the point input, and demonstrate their effectiveness in this new setting.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To summarize, our work advances VQA along a new dimension. Concretely, we (1) introduce and motivate point-input questions as an extension of VQA, (2) design a set of benchmark datasets, and (3) introduce effective model extensions to handle the unique challenges of this space.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Spatial Grounding in VQA.</span> Visual grounding has become a central idea in the VQA community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. It is increasingly seen as important that VQA models localize the object being asked about to answer a visual question. This idea has influenced the development of several datasets, including Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In these datasets, bounding boxes are provided for each object mentioned in the question or answer. To encourage grounding and counteract language priors, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> introduced the VQA 2.0 Dataset in 2017, which consists of complementary images for each question.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> further introduce the VQA-CP dataset, where priors differ in training and test splits. All these works indicate the importance of visual grounding for the VQA task.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">A few works have used visual grounding to expand the question space of VQA. The authors of Visual7W introduce a <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">pointing QA</span> task, involving a ‘which’ question and image regions as candidate answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. They use their manually collected object-level groundings to construct such questions. A number of works have also explored grounding in the context of embodied question answering, which requires an agent to explore an environment to answer a visual question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Other grounding-based questions include region-based QAs in Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, where the question is associated with a particular region of the image. However, it is not necessary for answering the question that the region be provided: we sampled 100 questions randomly and only 17% actually required the region to produce the correct answer. To our knowledge, we are the first to introduce a benchmark where a spatial grounding signal is explicitly required to answer a question.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The importance of visual grounding has also influenced the development of VQA models. In particular, most state-of-the-art VQA models have an attention mechanism on the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The relative success of these models indicates the importance of successful visual grounding in the VQA task. Several works include pixel-wise prediction as a primary or auxiliary task of the VQA model. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> mine ground-truth attention maps from Visual Genome and include attention prediction explicitly as an auxiliary task of the model. Other works output a visual justification for the answer as a heatmap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> or a semantic segmentation of the visual entities relevant to the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, the challenge of actually accepting a spatial grounding input into VQA models has not been previously explored.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Point input.</span> Despite being ubiquitous for humans, pointing as a way of communicating intention has been underexplored in computer vision. Studies in the robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> or human-computer interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> communities have largely been limited to simple, constrained environments. In vision, pointing has been used as a form of cost-effective supervision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> but never examined in depth beyond cost-accuracy tradeoffs. The use of pointing as a communicative gesture in humans  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> motivates deeper study in a computer vision context.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span>: reasoning about a region</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We now begin our exploration of the space of pointing questions, starting with a simpler <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> setting: questions involves queries about the attributes of a particular object (e.g. “What color is <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">this</em> car?”). A local region around the point is completely sufficient to answer the question. In Sec. <a href="#S3.SS1" title="3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> we construct a corresponding dataset using existing annotations from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. We design the benchmark such that a point is <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">required</em> to answer the question, ensuring that the image as a whole contains multiple possible answers.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> allows us to investigate the capabilities of the model to accept a point input, without additional complexities introduced in Sections <a href="#S4" title="4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5" title="5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Importantly, in Sec. <a href="#S3.SS3" title="3.3 PointQA-Local evaluation ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> we demonstrate that the model is able to understand the question and the task enough to vary attention around the point as needed (consider “What color is this shirt?” versus “What action is this person doing?”).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> dataset contains questions that require reasoning over an image region, with the point input disambiguating the correct region. We use three templates:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">What color is <em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">this</em> [object class]?</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">What shape is <em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">this</em> [object class]?</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">What action is <em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">this</em> [object class] doing?</p>
</div>
</li>
</ol>
<p id="S3.SS1.p1.2" class="ltx_p">We generate the dataset from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which consists of 108,249 images with each image annotated on average with 35 objects, 26 object attributes, and 21 pairwise relationships between the objects.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Attribute selection.</span> We select the 100 most common attributes from Visual Genome (e.g. red, round, small), which account for 70.1% of annotations. We manually group these attributes into four general categories: color, shape, action, and size. We only use the first three categories, as size is a relative rather than an absolute property of the object.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.18" class="ltx_p"><span id="S3.SS1.p3.18.1" class="ltx_text ltx_font_bold">Question Generation.</span> On every image in Visual Genome, we search for examples that satisfy the constraints of <span id="S3.SS1.p3.18.2" class="ltx_text ltx_font_bold">PointQA-Local</span> . Concretely, we examine pairs of bounding boxes <math id="S3.SS1.p3.1.m1.2" class="ltx_Math" alttext="b_{i},b_{j}" display="inline"><semantics id="S3.SS1.p3.1.m1.2a"><mrow id="S3.SS1.p3.1.m1.2.2.2" xref="S3.SS1.p3.1.m1.2.2.3.cmml"><msub id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.cmml">b</mi><mi id="S3.SS1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.1.m1.2.2.2.3" xref="S3.SS1.p3.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.1.m1.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.p3.1.m1.2.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.2.cmml">b</mi><mi id="S3.SS1.p3.1.m1.2.2.2.2.3" xref="S3.SS1.p3.1.m1.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.2b"><list id="S3.SS1.p3.1.m1.2.2.3.cmml" xref="S3.SS1.p3.1.m1.2.2.2"><apply id="S3.SS1.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2">𝑏</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p3.1.m1.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.2">𝑏</ci><ci id="S3.SS1.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.2c">b_{i},b_{j}</annotation></semantics></math> annotated with corresponding object classes <math id="S3.SS1.p3.2.m2.2" class="ltx_Math" alttext="o_{i},o_{j}" display="inline"><semantics id="S3.SS1.p3.2.m2.2a"><mrow id="S3.SS1.p3.2.m2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.3.cmml"><msub id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.2.cmml">o</mi><mi id="S3.SS1.p3.2.m2.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.2.m2.2.2.2.3" xref="S3.SS1.p3.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.2.m2.2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.2.2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.2.2.cmml">o</mi><mi id="S3.SS1.p3.2.m2.2.2.2.2.3" xref="S3.SS1.p3.2.m2.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.2b"><list id="S3.SS1.p3.2.m2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.2.2.2"><apply id="S3.SS1.p3.2.m2.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.2">𝑜</ci><ci id="S3.SS1.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p3.2.m2.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.2.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.2">𝑜</ci><ci id="S3.SS1.p3.2.m2.2.2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.2c">o_{i},o_{j}</annotation></semantics></math> and the sets of attributes <math id="S3.SS1.p3.3.m3.2" class="ltx_Math" alttext="A_{i},A_{j}" display="inline"><semantics id="S3.SS1.p3.3.m3.2a"><mrow id="S3.SS1.p3.3.m3.2.2.2" xref="S3.SS1.p3.3.m3.2.2.3.cmml"><msub id="S3.SS1.p3.3.m3.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.1.1.2" xref="S3.SS1.p3.3.m3.1.1.1.1.2.cmml">A</mi><mi id="S3.SS1.p3.3.m3.1.1.1.1.3" xref="S3.SS1.p3.3.m3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.3.m3.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.3.m3.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.cmml"><mi id="S3.SS1.p3.3.m3.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.cmml">A</mi><mi id="S3.SS1.p3.3.m3.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.2b"><list id="S3.SS1.p3.3.m3.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2"><apply id="S3.SS1.p3.3.m3.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.2">𝐴</ci><ci id="S3.SS1.p3.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p3.3.m3.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.2.2.2.2.1.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2">𝐴</ci><ci id="S3.SS1.p3.3.m3.2.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.2c">A_{i},A_{j}</annotation></semantics></math>. If <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="o_{i}=o_{j}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><msub id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2.2" xref="S3.SS1.p3.4.m4.1.1.2.2.cmml">o</mi><mi id="S3.SS1.p3.4.m4.1.1.2.3" xref="S3.SS1.p3.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p3.4.m4.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.cmml">=</mo><msub id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">o</mi><mi id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><eq id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1"></eq><apply id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.2.1.cmml" xref="S3.SS1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2.2">𝑜</ci><ci id="S3.SS1.p3.4.m4.1.1.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">𝑜</ci><ci id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">o_{i}=o_{j}</annotation></semantics></math>, so the object classes are the same, we then look for a pair of attributes <math id="S3.SS1.p3.5.m5.2" class="ltx_Math" alttext="a_{i}\in A_{i},a_{j}\in A_{j}" display="inline"><semantics id="S3.SS1.p3.5.m5.2a"><mrow id="S3.SS1.p3.5.m5.2.2.2" xref="S3.SS1.p3.5.m5.2.2.3.cmml"><mrow id="S3.SS1.p3.5.m5.1.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.1.cmml"><msub id="S3.SS1.p3.5.m5.1.1.1.1.2" xref="S3.SS1.p3.5.m5.1.1.1.1.2.cmml"><mi id="S3.SS1.p3.5.m5.1.1.1.1.2.2" xref="S3.SS1.p3.5.m5.1.1.1.1.2.2.cmml">a</mi><mi id="S3.SS1.p3.5.m5.1.1.1.1.2.3" xref="S3.SS1.p3.5.m5.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p3.5.m5.1.1.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.1.1.cmml">∈</mo><msub id="S3.SS1.p3.5.m5.1.1.1.1.3" xref="S3.SS1.p3.5.m5.1.1.1.1.3.cmml"><mi id="S3.SS1.p3.5.m5.1.1.1.1.3.2" xref="S3.SS1.p3.5.m5.1.1.1.1.3.2.cmml">A</mi><mi id="S3.SS1.p3.5.m5.1.1.1.1.3.3" xref="S3.SS1.p3.5.m5.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS1.p3.5.m5.2.2.2.3" xref="S3.SS1.p3.5.m5.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p3.5.m5.2.2.2.2" xref="S3.SS1.p3.5.m5.2.2.2.2.cmml"><msub id="S3.SS1.p3.5.m5.2.2.2.2.2" xref="S3.SS1.p3.5.m5.2.2.2.2.2.cmml"><mi id="S3.SS1.p3.5.m5.2.2.2.2.2.2" xref="S3.SS1.p3.5.m5.2.2.2.2.2.2.cmml">a</mi><mi id="S3.SS1.p3.5.m5.2.2.2.2.2.3" xref="S3.SS1.p3.5.m5.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.SS1.p3.5.m5.2.2.2.2.1" xref="S3.SS1.p3.5.m5.2.2.2.2.1.cmml">∈</mo><msub id="S3.SS1.p3.5.m5.2.2.2.2.3" xref="S3.SS1.p3.5.m5.2.2.2.2.3.cmml"><mi id="S3.SS1.p3.5.m5.2.2.2.2.3.2" xref="S3.SS1.p3.5.m5.2.2.2.2.3.2.cmml">A</mi><mi id="S3.SS1.p3.5.m5.2.2.2.2.3.3" xref="S3.SS1.p3.5.m5.2.2.2.2.3.3.cmml">j</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.2b"><apply id="S3.SS1.p3.5.m5.2.2.3.cmml" xref="S3.SS1.p3.5.m5.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.2.2.3a.cmml" xref="S3.SS1.p3.5.m5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p3.5.m5.1.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1"><in id="S3.SS1.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.1"></in><apply id="S3.SS1.p3.5.m5.1.1.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.1.1.2.2.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.2.2">𝑎</ci><ci id="S3.SS1.p3.5.m5.1.1.1.1.2.3.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p3.5.m5.1.1.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.1.3.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.1.1.3.2.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.3.2">𝐴</ci><ci id="S3.SS1.p3.5.m5.1.1.1.1.3.3.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.SS1.p3.5.m5.2.2.2.2.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2"><in id="S3.SS1.p3.5.m5.2.2.2.2.1.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.1"></in><apply id="S3.SS1.p3.5.m5.2.2.2.2.2.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.2.2">𝑎</ci><ci id="S3.SS1.p3.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.2.3">𝑗</ci></apply><apply id="S3.SS1.p3.5.m5.2.2.2.2.3.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.2.2.2.2.3.1.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.3">subscript</csymbol><ci id="S3.SS1.p3.5.m5.2.2.2.2.3.2.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.3.2">𝐴</ci><ci id="S3.SS1.p3.5.m5.2.2.2.2.3.3.cmml" xref="S3.SS1.p3.5.m5.2.2.2.2.3.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.2c">a_{i}\in A_{i},a_{j}\in A_{j}</annotation></semantics></math> such that <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="a_{i}\neq a_{j}" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mrow id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><msub id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2.2" xref="S3.SS1.p3.6.m6.1.1.2.2.cmml">a</mi><mi id="S3.SS1.p3.6.m6.1.1.2.3" xref="S3.SS1.p3.6.m6.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p3.6.m6.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.cmml">≠</mo><msub id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml"><mi id="S3.SS1.p3.6.m6.1.1.3.2" xref="S3.SS1.p3.6.m6.1.1.3.2.cmml">a</mi><mi id="S3.SS1.p3.6.m6.1.1.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><neq id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1"></neq><apply id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.2.1.cmml" xref="S3.SS1.p3.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2.2">𝑎</ci><ci id="S3.SS1.p3.6.m6.1.1.2.3.cmml" xref="S3.SS1.p3.6.m6.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.2">𝑎</ci><ci id="S3.SS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">a_{i}\neq a_{j}</annotation></semantics></math> and <math id="S3.SS1.p3.7.m7.2" class="ltx_Math" alttext="a_{i},a_{j}" display="inline"><semantics id="S3.SS1.p3.7.m7.2a"><mrow id="S3.SS1.p3.7.m7.2.2.2" xref="S3.SS1.p3.7.m7.2.2.3.cmml"><msub id="S3.SS1.p3.7.m7.1.1.1.1" xref="S3.SS1.p3.7.m7.1.1.1.1.cmml"><mi id="S3.SS1.p3.7.m7.1.1.1.1.2" xref="S3.SS1.p3.7.m7.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.7.m7.1.1.1.1.3" xref="S3.SS1.p3.7.m7.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.7.m7.2.2.2.3" xref="S3.SS1.p3.7.m7.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.7.m7.2.2.2.2" xref="S3.SS1.p3.7.m7.2.2.2.2.cmml"><mi id="S3.SS1.p3.7.m7.2.2.2.2.2" xref="S3.SS1.p3.7.m7.2.2.2.2.2.cmml">a</mi><mi id="S3.SS1.p3.7.m7.2.2.2.2.3" xref="S3.SS1.p3.7.m7.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.2b"><list id="S3.SS1.p3.7.m7.2.2.3.cmml" xref="S3.SS1.p3.7.m7.2.2.2"><apply id="S3.SS1.p3.7.m7.1.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.7.m7.1.1.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1.2">𝑎</ci><ci id="S3.SS1.p3.7.m7.1.1.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p3.7.m7.2.2.2.2.cmml" xref="S3.SS1.p3.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.2.2.2.2.1.cmml" xref="S3.SS1.p3.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.7.m7.2.2.2.2.2.cmml" xref="S3.SS1.p3.7.m7.2.2.2.2.2">𝑎</ci><ci id="S3.SS1.p3.7.m7.2.2.2.2.3.cmml" xref="S3.SS1.p3.7.m7.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.2c">a_{i},a_{j}</annotation></semantics></math> are of the same category (color, shape or action). We construct a question <math id="S3.SS1.p3.8.m8.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><mi id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><ci id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">q</annotation></semantics></math> of the form “What <math id="S3.SS1.p3.9.m9.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p3.9.m9.1a"><mi id="S3.SS1.p3.9.m9.1.1" xref="S3.SS1.p3.9.m9.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.1b"><ci id="S3.SS1.p3.9.m9.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.1c">C</annotation></semantics></math> is this <math id="S3.SS1.p3.10.m10.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S3.SS1.p3.10.m10.1a"><mi id="S3.SS1.p3.10.m10.1.1" xref="S3.SS1.p3.10.m10.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m10.1b"><ci id="S3.SS1.p3.10.m10.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m10.1c">O</annotation></semantics></math>?”, where <math id="S3.SS1.p3.11.m11.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p3.11.m11.1a"><mi id="S3.SS1.p3.11.m11.1.1" xref="S3.SS1.p3.11.m11.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m11.1b"><ci id="S3.SS1.p3.11.m11.1.1.cmml" xref="S3.SS1.p3.11.m11.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m11.1c">C</annotation></semantics></math> is the attribute category and <math id="S3.SS1.p3.12.m12.1" class="ltx_Math" alttext="O=o_{j}=o_{k}" display="inline"><semantics id="S3.SS1.p3.12.m12.1a"><mrow id="S3.SS1.p3.12.m12.1.1" xref="S3.SS1.p3.12.m12.1.1.cmml"><mi id="S3.SS1.p3.12.m12.1.1.2" xref="S3.SS1.p3.12.m12.1.1.2.cmml">O</mi><mo id="S3.SS1.p3.12.m12.1.1.3" xref="S3.SS1.p3.12.m12.1.1.3.cmml">=</mo><msub id="S3.SS1.p3.12.m12.1.1.4" xref="S3.SS1.p3.12.m12.1.1.4.cmml"><mi id="S3.SS1.p3.12.m12.1.1.4.2" xref="S3.SS1.p3.12.m12.1.1.4.2.cmml">o</mi><mi id="S3.SS1.p3.12.m12.1.1.4.3" xref="S3.SS1.p3.12.m12.1.1.4.3.cmml">j</mi></msub><mo id="S3.SS1.p3.12.m12.1.1.5" xref="S3.SS1.p3.12.m12.1.1.5.cmml">=</mo><msub id="S3.SS1.p3.12.m12.1.1.6" xref="S3.SS1.p3.12.m12.1.1.6.cmml"><mi id="S3.SS1.p3.12.m12.1.1.6.2" xref="S3.SS1.p3.12.m12.1.1.6.2.cmml">o</mi><mi id="S3.SS1.p3.12.m12.1.1.6.3" xref="S3.SS1.p3.12.m12.1.1.6.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.12.m12.1b"><apply id="S3.SS1.p3.12.m12.1.1.cmml" xref="S3.SS1.p3.12.m12.1.1"><and id="S3.SS1.p3.12.m12.1.1a.cmml" xref="S3.SS1.p3.12.m12.1.1"></and><apply id="S3.SS1.p3.12.m12.1.1b.cmml" xref="S3.SS1.p3.12.m12.1.1"><eq id="S3.SS1.p3.12.m12.1.1.3.cmml" xref="S3.SS1.p3.12.m12.1.1.3"></eq><ci id="S3.SS1.p3.12.m12.1.1.2.cmml" xref="S3.SS1.p3.12.m12.1.1.2">𝑂</ci><apply id="S3.SS1.p3.12.m12.1.1.4.cmml" xref="S3.SS1.p3.12.m12.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m12.1.1.4.1.cmml" xref="S3.SS1.p3.12.m12.1.1.4">subscript</csymbol><ci id="S3.SS1.p3.12.m12.1.1.4.2.cmml" xref="S3.SS1.p3.12.m12.1.1.4.2">𝑜</ci><ci id="S3.SS1.p3.12.m12.1.1.4.3.cmml" xref="S3.SS1.p3.12.m12.1.1.4.3">𝑗</ci></apply></apply><apply id="S3.SS1.p3.12.m12.1.1c.cmml" xref="S3.SS1.p3.12.m12.1.1"><eq id="S3.SS1.p3.12.m12.1.1.5.cmml" xref="S3.SS1.p3.12.m12.1.1.5"></eq><share href="#S3.SS1.p3.12.m12.1.1.4.cmml" id="S3.SS1.p3.12.m12.1.1d.cmml" xref="S3.SS1.p3.12.m12.1.1"></share><apply id="S3.SS1.p3.12.m12.1.1.6.cmml" xref="S3.SS1.p3.12.m12.1.1.6"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m12.1.1.6.1.cmml" xref="S3.SS1.p3.12.m12.1.1.6">subscript</csymbol><ci id="S3.SS1.p3.12.m12.1.1.6.2.cmml" xref="S3.SS1.p3.12.m12.1.1.6.2">𝑜</ci><ci id="S3.SS1.p3.12.m12.1.1.6.3.cmml" xref="S3.SS1.p3.12.m12.1.1.6.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.12.m12.1c">O=o_{j}=o_{k}</annotation></semantics></math> the object class. We then add two examples into our dataset: (1) question <math id="S3.SS1.p3.13.m13.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p3.13.m13.1a"><mi id="S3.SS1.p3.13.m13.1.1" xref="S3.SS1.p3.13.m13.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.13.m13.1b"><ci id="S3.SS1.p3.13.m13.1.1.cmml" xref="S3.SS1.p3.13.m13.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.13.m13.1c">q</annotation></semantics></math>, point to the center of the bounding box <math id="S3.SS1.p3.14.m14.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S3.SS1.p3.14.m14.1a"><msub id="S3.SS1.p3.14.m14.1.1" xref="S3.SS1.p3.14.m14.1.1.cmml"><mi id="S3.SS1.p3.14.m14.1.1.2" xref="S3.SS1.p3.14.m14.1.1.2.cmml">b</mi><mi id="S3.SS1.p3.14.m14.1.1.3" xref="S3.SS1.p3.14.m14.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.14.m14.1b"><apply id="S3.SS1.p3.14.m14.1.1.cmml" xref="S3.SS1.p3.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m14.1.1.1.cmml" xref="S3.SS1.p3.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p3.14.m14.1.1.2.cmml" xref="S3.SS1.p3.14.m14.1.1.2">𝑏</ci><ci id="S3.SS1.p3.14.m14.1.1.3.cmml" xref="S3.SS1.p3.14.m14.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.14.m14.1c">b_{i}</annotation></semantics></math>, and answer <math id="S3.SS1.p3.15.m15.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S3.SS1.p3.15.m15.1a"><msub id="S3.SS1.p3.15.m15.1.1" xref="S3.SS1.p3.15.m15.1.1.cmml"><mi id="S3.SS1.p3.15.m15.1.1.2" xref="S3.SS1.p3.15.m15.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.15.m15.1.1.3" xref="S3.SS1.p3.15.m15.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.15.m15.1b"><apply id="S3.SS1.p3.15.m15.1.1.cmml" xref="S3.SS1.p3.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.15.m15.1.1.1.cmml" xref="S3.SS1.p3.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p3.15.m15.1.1.2.cmml" xref="S3.SS1.p3.15.m15.1.1.2">𝑎</ci><ci id="S3.SS1.p3.15.m15.1.1.3.cmml" xref="S3.SS1.p3.15.m15.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.15.m15.1c">a_{i}</annotation></semantics></math>, and (2) question <math id="S3.SS1.p3.16.m16.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p3.16.m16.1a"><mi id="S3.SS1.p3.16.m16.1.1" xref="S3.SS1.p3.16.m16.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.16.m16.1b"><ci id="S3.SS1.p3.16.m16.1.1.cmml" xref="S3.SS1.p3.16.m16.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.16.m16.1c">q</annotation></semantics></math>, point to center of <math id="S3.SS1.p3.17.m17.1" class="ltx_Math" alttext="b_{j}" display="inline"><semantics id="S3.SS1.p3.17.m17.1a"><msub id="S3.SS1.p3.17.m17.1.1" xref="S3.SS1.p3.17.m17.1.1.cmml"><mi id="S3.SS1.p3.17.m17.1.1.2" xref="S3.SS1.p3.17.m17.1.1.2.cmml">b</mi><mi id="S3.SS1.p3.17.m17.1.1.3" xref="S3.SS1.p3.17.m17.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.17.m17.1b"><apply id="S3.SS1.p3.17.m17.1.1.cmml" xref="S3.SS1.p3.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.17.m17.1.1.1.cmml" xref="S3.SS1.p3.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p3.17.m17.1.1.2.cmml" xref="S3.SS1.p3.17.m17.1.1.2">𝑏</ci><ci id="S3.SS1.p3.17.m17.1.1.3.cmml" xref="S3.SS1.p3.17.m17.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.17.m17.1c">b_{j}</annotation></semantics></math> and answer <math id="S3.SS1.p3.18.m18.1" class="ltx_Math" alttext="a_{j}" display="inline"><semantics id="S3.SS1.p3.18.m18.1a"><msub id="S3.SS1.p3.18.m18.1.1" xref="S3.SS1.p3.18.m18.1.1.cmml"><mi id="S3.SS1.p3.18.m18.1.1.2" xref="S3.SS1.p3.18.m18.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.18.m18.1.1.3" xref="S3.SS1.p3.18.m18.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.18.m18.1b"><apply id="S3.SS1.p3.18.m18.1.1.cmml" xref="S3.SS1.p3.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.18.m18.1.1.1.cmml" xref="S3.SS1.p3.18.m18.1.1">subscript</csymbol><ci id="S3.SS1.p3.18.m18.1.1.2.cmml" xref="S3.SS1.p3.18.m18.1.1.2">𝑎</ci><ci id="S3.SS1.p3.18.m18.1.1.3.cmml" xref="S3.SS1.p3.18.m18.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.18.m18.1c">a_{j}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.2" class="ltx_p">We performed several optimizations to ensure quality. Instances where IoU<math id="S3.SS1.p4.1.m1.2" class="ltx_Math" alttext="(b_{i},b_{j})\geq t" display="inline"><semantics id="S3.SS1.p4.1.m1.2a"><mrow id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml"><mrow id="S3.SS1.p4.1.m1.2.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p4.1.m1.2.2.2.2.3" xref="S3.SS1.p4.1.m1.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.cmml">b</mi><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p4.1.m1.2.2.2.2.4" xref="S3.SS1.p4.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p4.1.m1.2.2.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.p4.1.m1.2.2.2.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.2.2.2.cmml">b</mi><mi id="S3.SS1.p4.1.m1.2.2.2.2.2.3" xref="S3.SS1.p4.1.m1.2.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS1.p4.1.m1.2.2.2.2.5" xref="S3.SS1.p4.1.m1.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS1.p4.1.m1.2.2.3" xref="S3.SS1.p4.1.m1.2.2.3.cmml">≥</mo><mi id="S3.SS1.p4.1.m1.2.2.4" xref="S3.SS1.p4.1.m1.2.2.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><apply id="S3.SS1.p4.1.m1.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2"><geq id="S3.SS1.p4.1.m1.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.3"></geq><interval closure="open" id="S3.SS1.p4.1.m1.2.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2"><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2">𝑏</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p4.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2.2.2">𝑏</ci><ci id="S3.SS1.p4.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2.2.3">𝑗</ci></apply></interval><ci id="S3.SS1.p4.1.m1.2.2.4.cmml" xref="S3.SS1.p4.1.m1.2.2.4">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">(b_{i},b_{j})\geq t</annotation></semantics></math> were filtered out (we set <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="t=0.2" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS1.p4.2.m2.1.1.1" xref="S3.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><eq id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1.1"></eq><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝑡</ci><cn type="float" id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">t=0.2</annotation></semantics></math> for high precision). Any object with more than one attribute in the same category was excluded (e.g., a striped shirt annotated as both “red” and “white”) to avoid confusion. Visually similar or identical attributes (e.g., ‘blonde’, ‘yellow’) were collapsed into one attribute to reduce noise.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Statistics.</span> The final dataset consists of 57,628 questions across 18,380 images, with 20 unique answers. Due to the high representation of color attributes in Visual Genome, 97.2% of questions are about color, with the remaining being about shape or action. Although biased in this way, the relative simplicity of color questions allows us to focus on the challenge of incorporating a point input. Although most questions have exactly two unique answers without a disambiguating point, 11.9% of questions have greater than two answers. Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows example questions in the dataset which illustrate the challenges of answering point-input questions, especially when the point can be placed on a non-relevant part of an object (Fig <a href="#S3.F2" title="Figure 2 ‣ 3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a, Fig <a href="#S3.F2" title="Figure 2 ‣ 3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>c) or on another smaller, occluding object (Fig <a href="#S3.F2" title="Figure 2 ‣ 3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2011.13681/assets/Picture2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Examples of questions in the <span id="S3.F2.4.2.1" class="ltx_text ltx_font_bold">PointQA-Local</span> Dataset across color, shape, and action. Point is in red.</span></figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">The images are randomly divided into (1) <em id="S3.SS1.p6.1.1" class="ltx_emph ltx_font_italic">train</em>, 70%, with 40,409 questions across 12,867 images; (2) <em id="S3.SS1.p6.1.2" class="ltx_emph ltx_font_italic">val</em>, 10% with 5,724 questions across 1,838 images; (3) <em id="S3.SS1.p6.1.3" class="ltx_emph ltx_font_italic">test-dev</em>, 10%, with 5,673 questions across 1,838 images, and (4) <em id="S3.SS1.p6.1.4" class="ltx_emph ltx_font_italic">test-final</em>, 10%, at 5,910 questions across 1,837 images.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p"><span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_bold">Human accuracy.</span> One final question is whether simulating the pointing as simply the center of the bounding box is a sufficiently understandable spatial disambiguation cue, and whether the dataset as a whole is reliable. To evaluate this, we run a small human study on 100 random questions of <em id="S3.SS1.p7.1.2" class="ltx_emph ltx_font_italic">test-final</em>. Please see Appendix <a href="#A1.SS2" title="A.2 PointQA-Local ‣ Appendix A Human Evaluations ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a> for details on human evaluation.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We modify the commonly-used Pythia model  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to incorporate the point input <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.3" xref="S3.SS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><interval closure="open" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑥</ci><ci id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">(x,y)</annotation></semantics></math>. We consider the more complex transformer-based models in Sec. <a href="#S5" title="5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>; here the language variations of the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> dataset are not particularly complex and thus we choose a somewhat simpler and easier-to-analyze model.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.11" class="ltx_p"><span id="S3.SS2.p2.11.1" class="ltx_text ltx_font_bold">Background.</span> Standard VQA systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> represent the image using a set of bounding box proposals from an object detection model, typically Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Concretely: (a) a Region Proposal Network extracts a set of candidate regions along with their “objectness” scores, (b) non-max suppression is performed to remove similar boxes, and (c) the top remaining <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">N</annotation></semantics></math> regions are processed by an <span id="S3.SS2.p2.11.2" class="ltx_text ltx_font_typewriter">ROIPool</span> operation to generate a set of visual features <math id="S3.SS2.p2.2.m2.2" class="ltx_Math" alttext="\mathbf{v}_{1}\dots\mathbf{v}_{N},\mathbf{v}_{i}\in\mathbb{R}^{D}" display="inline"><semantics id="S3.SS2.p2.2.m2.2a"><mrow id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml"><mrow id="S3.SS2.p2.2.m2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml"><mrow id="S3.SS2.p2.2.m2.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.cmml"><msub id="S3.SS2.p2.2.m2.1.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2.2.cmml">𝐯</mi><mn id="S3.SS2.p2.2.m2.1.1.1.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1.1.1.1a" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml">​</mo><msub id="S3.SS2.p2.2.m2.1.1.1.1.1.4" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.4.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4.2.cmml">𝐯</mi><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.4.3" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4.3.cmml">N</mi></msub></mrow><mo id="S3.SS2.p2.2.m2.2.2.2.2.3" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.2.m2.2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS2.p2.2.m2.2.2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.2.2.cmml">𝐯</mi><mi id="S3.SS2.p2.2.m2.2.2.2.2.2.3" xref="S3.SS2.p2.2.m2.2.2.2.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.SS2.p2.2.m2.2.2.3" xref="S3.SS2.p2.2.m2.2.2.3.cmml">∈</mo><msup id="S3.SS2.p2.2.m2.2.2.4" xref="S3.SS2.p2.2.m2.2.2.4.cmml"><mi id="S3.SS2.p2.2.m2.2.2.4.2" xref="S3.SS2.p2.2.m2.2.2.4.2.cmml">ℝ</mi><mi id="S3.SS2.p2.2.m2.2.2.4.3" xref="S3.SS2.p2.2.m2.2.2.4.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.2b"><apply id="S3.SS2.p2.2.m2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2"><in id="S3.SS2.p2.2.m2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.3"></in><list id="S3.SS2.p2.2.m2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2"><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1"></times><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2.2">𝐯</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.3">…</ci><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.4.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.4.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.4.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4.2">𝐯</ci><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.4.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.4.3">𝑁</ci></apply></apply><apply id="S3.SS2.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2.2">𝐯</ci><ci id="S3.SS2.p2.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2.3">𝑖</ci></apply></list><apply id="S3.SS2.p2.2.m2.2.2.4.cmml" xref="S3.SS2.p2.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.2.4.1.cmml" xref="S3.SS2.p2.2.m2.2.2.4">superscript</csymbol><ci id="S3.SS2.p2.2.m2.2.2.4.2.cmml" xref="S3.SS2.p2.2.m2.2.2.4.2">ℝ</ci><ci id="S3.SS2.p2.2.m2.2.2.4.3.cmml" xref="S3.SS2.p2.2.m2.2.2.4.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.2c">\mathbf{v}_{1}\dots\mathbf{v}_{N},\mathbf{v}_{i}\in\mathbb{R}^{D}</annotation></semantics></math>. In step (d), the question is encoded by a recurrent model into <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{q}\in\mathbb{R}^{M}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">𝐪</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml">M</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><in id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></in><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝐪</ci><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathbf{q}\in\mathbb{R}^{M}</annotation></semantics></math>. Then: (e) <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathbf{q}</annotation></semantics></math> is combined with <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{v}_{1}\dots\mathbf{v}_{N}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><msub id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">𝐯</mi><mn id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1a" xref="S3.SS2.p2.5.m5.1.1.1.cmml">​</mo><msub id="S3.SS2.p2.5.m5.1.1.4" xref="S3.SS2.p2.5.m5.1.1.4.cmml"><mi id="S3.SS2.p2.5.m5.1.1.4.2" xref="S3.SS2.p2.5.m5.1.1.4.2.cmml">𝐯</mi><mi id="S3.SS2.p2.5.m5.1.1.4.3" xref="S3.SS2.p2.5.m5.1.1.4.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></times><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">𝐯</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">1</cn></apply><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">…</ci><apply id="S3.SS2.p2.5.m5.1.1.4.cmml" xref="S3.SS2.p2.5.m5.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.4.1.cmml" xref="S3.SS2.p2.5.m5.1.1.4">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.4.2.cmml" xref="S3.SS2.p2.5.m5.1.1.4.2">𝐯</ci><ci id="S3.SS2.p2.5.m5.1.1.4.3.cmml" xref="S3.SS2.p2.5.m5.1.1.4.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathbf{v}_{1}\dots\mathbf{v}_{N}</annotation></semantics></math> to compute a normalized attention vector <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{a}\in\mathbb{R}^{N}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">𝐚</mi><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml"><mi id="S3.SS2.p2.6.m6.1.1.3.2" xref="S3.SS2.p2.6.m6.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p2.6.m6.1.1.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><in id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></in><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝐚</ci><apply id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.2">ℝ</ci><ci id="S3.SS2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\mathbf{a}\in\mathbb{R}^{N}</annotation></semantics></math> over the <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">N</annotation></semantics></math> region proposals, (f) the probabilistic attention <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="\mathbf{a}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">𝐚</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝐚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">\mathbf{a}</annotation></semantics></math> is used to compute a weighted average of the region features and obtain the image representation <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="\mathbf{v}\in\mathbb{R}^{D}" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mrow id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">𝐯</mi><mo id="S3.SS2.p2.9.m9.1.1.1" xref="S3.SS2.p2.9.m9.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml"><mi id="S3.SS2.p2.9.m9.1.1.3.2" xref="S3.SS2.p2.9.m9.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p2.9.m9.1.1.3.3" xref="S3.SS2.p2.9.m9.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><in id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1.1"></in><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝐯</ci><apply id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.3.1.cmml" xref="S3.SS2.p2.9.m9.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.3.2.cmml" xref="S3.SS2.p2.9.m9.1.1.3.2">ℝ</ci><ci id="S3.SS2.p2.9.m9.1.1.3.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">\mathbf{v}\in\mathbb{R}^{D}</annotation></semantics></math>, and (g) <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><mi id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><ci id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">\mathbf{q}</annotation></semantics></math> and <math id="S3.SS2.p2.11.m11.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S3.SS2.p2.11.m11.1a"><mi id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml">𝐯</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.1b"><ci id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">𝐯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.1c">\mathbf{v}</annotation></semantics></math> are projected to a common vector space, combined via element-wise multiplication followed by fully-connected layers and a softmax activation to predict over the answer distribution. Steps (a)-(c) use a pretrained detection model, while (d)-(g) are trained.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.3" class="ltx_p"><span id="S3.SS2.p3.3.1" class="ltx_text ltx_font_bold">Modifications.</span> We choose the simplest way of incorporating the point input into the system above: in step (a) above, we remove all candidate regions that do not contain the point <math id="S3.SS2.p3.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.3.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.2.3.2.1" xref="S3.SS2.p3.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p3.1.m1.2.3.2.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS2.p3.1.m1.2.3.2.3" xref="S3.SS2.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><interval closure="open" id="S3.SS2.p3.1.m1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.3.2"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑥</ci><ci id="S3.SS2.p3.1.m1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">(x,y)</annotation></semantics></math>. The rest of the pipeline is unchanged, apart from zero-padding to <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">N</annotation></semantics></math> regions if necessary (we use <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="N=100" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><eq id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></eq><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">N=100</annotation></semantics></math>, as is standard). Our approach produces the effect of a pointing gesture by restricting the view of the model to objects around the point. It has the additional advantage of allowing us to analyze the learned attention around the point, yielding insights into the behavior of the model.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> evaluation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We benchmark the model extensions on the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> dataset, focusing on insights into the new task.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Implementation details.</span> We train the model on the <em id="S3.SS3.p2.1.2" class="ltx_emph ltx_font_italic">train</em> subset of the data. For extracting the region features, we use a Feature Pyramid Network (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> with a ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> backbone and IOU of 0.5 for non-maximum suppression, as in the VQA Challenge implementation of Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The model was trained using AdaMax with a learning rate of 0.002. We use early stopping on the <em id="S3.SS3.p2.1.3" class="ltx_emph ltx_font_italic">val</em> set with patience of 500 iterations.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Ablation studies.</span> Table <a href="#S3.T1" title="Table 1 ‣ 3.3 PointQA-Local evaluation ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports results on <em id="S3.SS3.p3.1.2" class="ltx_emph ltx_font_italic">test-dev</em> where we experiment with several different methods for incorporating a point input. Our strategy of removing regions not containing the point and allowing the model to learn attention over the rest achieves at least 5.8% improvement over simpler strategies such as removing all but the smallest or the highest-scoring region proposal containing the point. Improvement is particularly significant on action questions, where other methods perform 12.6% worse.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row">Strategy</th>
<td id="S3.T1.2.1.1.3" class="ltx_td ltx_align_left">QIPBA</td>
<td id="S3.T1.2.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center">Overall</td>
<td id="S3.T1.2.1.1.5" class="ltx_td ltx_nopad_l ltx_align_center">Color</td>
<td id="S3.T1.2.1.1.6" class="ltx_td ltx_nopad_l ltx_align_center">Action</td>
<td id="S3.T1.2.1.1.7" class="ltx_td ltx_nopad_l ltx_align_center">Shape</td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.T1.2.2.2.1.1" class="ltx_text">Priors</span></th>
<th id="S3.T1.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t">Q-only</th>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">+ - - - -</td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">27.8</td>
<td id="S3.T1.2.2.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">25.8</td>
<td id="S3.T1.2.2.2.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">45.5</td>
<td id="S3.T1.2.2.2.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">32.1</td>
</tr>
<tr id="S3.T1.2.3.3" class="ltx_tr">
<th id="S3.T1.2.3.3.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row">Modal-A</th>
<td id="S3.T1.2.3.3.2" class="ltx_td ltx_nopad_l ltx_align_center">- - - - +</td>
<td id="S3.T1.2.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center">52.2</td>
<td id="S3.T1.2.3.3.4" class="ltx_td ltx_nopad_l ltx_align_center">52.0</td>
<td id="S3.T1.2.3.3.5" class="ltx_td ltx_nopad_l ltx_align_center">60.1</td>
<td id="S3.T1.2.3.3.6" class="ltx_td ltx_nopad_l ltx_align_center">52.8</td>
</tr>
<tr id="S3.T1.2.4.4" class="ltx_tr">
<th id="S3.T1.2.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.2.4.4.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t">Full Img</th>
<td id="S3.T1.2.4.4.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">+ + - - -</td>
<td id="S3.T1.2.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">37.4</td>
<td id="S3.T1.2.4.4.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">37.2</td>
<td id="S3.T1.2.4.4.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">45.4</td>
<td id="S3.T1.2.4.4.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">35.9</td>
</tr>
<tr id="S3.T1.2.5.5" class="ltx_tr">
<th id="S3.T1.2.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.2.5.5.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row">Top-score</th>
<td id="S3.T1.2.5.5.3" class="ltx_td ltx_nopad_l ltx_align_center">+ + + - -</td>
<td id="S3.T1.2.5.5.4" class="ltx_td ltx_nopad_l ltx_align_center">44.8</td>
<td id="S3.T1.2.5.5.5" class="ltx_td ltx_nopad_l ltx_align_center">44.4</td>
<td id="S3.T1.2.5.5.6" class="ltx_td ltx_nopad_l ltx_align_center">62.2</td>
<td id="S3.T1.2.5.5.7" class="ltx_td ltx_nopad_l ltx_align_center">43.4</td>
</tr>
<tr id="S3.T1.2.6.6" class="ltx_tr">
<th id="S3.T1.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Model</th>
<th id="S3.T1.2.6.6.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row">Smallest</th>
<td id="S3.T1.2.6.6.3" class="ltx_td ltx_nopad_l ltx_align_center">+ + + - -</td>
<td id="S3.T1.2.6.6.4" class="ltx_td ltx_nopad_l ltx_align_center">69.2</td>
<td id="S3.T1.2.6.6.5" class="ltx_td ltx_nopad_l ltx_align_center">69.6</td>
<td id="S3.T1.2.6.6.6" class="ltx_td ltx_nopad_l ltx_align_center">58.0</td>
<td id="S3.T1.2.6.6.7" class="ltx_td ltx_nopad_l ltx_align_center">56.6</td>
</tr>
<tr id="S3.T1.2.7.7" class="ltx_tr">
<th id="S3.T1.2.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.2.7.7.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S3.T1.2.7.7.3" class="ltx_td ltx_nopad_l ltx_align_center">+ + + - -</td>
<td id="S3.T1.2.7.7.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.2.7.7.4.1" class="ltx_text ltx_font_bold">75.0</span></td>
<td id="S3.T1.2.7.7.5" class="ltx_td ltx_nopad_l ltx_align_center">75.4</td>
<td id="S3.T1.2.7.7.6" class="ltx_td ltx_nopad_l ltx_align_center">66.4</td>
<td id="S3.T1.2.7.7.7" class="ltx_td ltx_nopad_l ltx_align_center">56.6</td>
</tr>
<tr id="S3.T1.2.8.8" class="ltx_tr">
<th id="S3.T1.2.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<th id="S3.T1.2.8.8.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t">GT box</th>
<td id="S3.T1.2.8.8.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t">+ + - + -</td>
<td id="S3.T1.2.8.8.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t">80.2</td>
<td id="S3.T1.2.8.8.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t">80.7</td>
<td id="S3.T1.2.8.8.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t">67.1</td>
<td id="S3.T1.2.8.8.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t">60.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.10.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.11.2" class="ltx_text" style="font-size:90%;">Accuracy (in %) of different models and baselines on the Local-QA <em id="S3.T1.11.2.1" class="ltx_emph ltx_font_italic">test-dev</em> split. The QIPBA column indicates the model input: <span id="S3.T1.11.2.2" class="ltx_text ltx_font_bold">Q</span>uestion, <span id="S3.T1.11.2.3" class="ltx_text ltx_font_bold">I</span>mage, <span id="S3.T1.11.2.4" class="ltx_text ltx_font_bold">P</span>oint, ground truth <span id="S3.T1.11.2.5" class="ltx_text ltx_font_bold">B</span>ounding box, and/or the set of all correct <span id="S3.T1.11.2.6" class="ltx_text ltx_font_bold">A</span>nswers in the image. Modal-A is a baseline oracle that selects the mode answer among all answers that are <em id="S3.T1.11.2.7" class="ltx_emph ltx_font_italic">correct</em> in this image. Q-only relies only on dataset language priors; Full Img is the original VQA model, not using the point input; Top-score and Smallest take the features of only the highest-scoring or smallest region proposal containing the point respectively (thus restricting the attention to a single region). Our proposed strategy outperforms these alternatives and even performs close to when the ground truth box instead of the point is provided during training and testing (bottom row).</span></figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Attention analysis.</span> An average of 27 region proposals containing the point are provided as input to the model. The attention is relatively peaked, with the maximum attention of 0.548 on average in <em id="S3.SS3.p4.1.2" class="ltx_emph ltx_font_italic">test-dev</em>. An important property is the model’s ability to attend based on the question. For the 799 questions in <em id="S3.SS3.p4.1.3" class="ltx_emph ltx_font_italic">test-dev</em> that ask “What color is this shirt?”, we can change the question to “What action is this person doing?” This increases the median size of the max-attention region from 2,997 to 5,451 pixels, indicating that the model shifts its attention accordingly. (Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.3 PointQA-Local evaluation ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2011.13681/assets/fig5keynote4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">The Pythia-based local region model (Sec. <a href="#S3.SS2" title="3.2 PointQA-Local models ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) successfully varies attention around the point in response to a question. (Darker = higher attention; point in red).</span></figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2011.13681/assets/figs/spatial-vs-verbal.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">In both examples, the standard verbal-only VQA model (top) fails to understand the verbal disambiguation and picks an answer corresponding to the wrong object instance; ours (bottom) correctly incorporates the point input.</span></figcaption>
</figure>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Spatial vs Verbal Disambiguation.</span> Finally, we note for many <span id="S3.SS3.p5.1.2" class="ltx_text ltx_font_bold">PointQA-Local</span> questions, a disambiguating phrase could be provided as a substitute for a point (e.g. “What color is the car <em id="S3.SS3.p5.1.3" class="ltx_emph ltx_font_italic">on the left</em>?”) Such verbal disambiguation is often unnatural and can grow expensive depending on the object of interest; this partially explains why humans prefer to point and motivates pointing as a more realistic setup for a broad set of visual questions. We provide a brief comparison between verbal and spatial disambiguation.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.2" class="ltx_p"><em id="S3.SS3.p6.2.1" class="ltx_emph ltx_font_italic">Data.</em> To do so, we collect a small dataset using human-written questions from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. A parser is used to detect the question subject, and we (1) ensure this object appears multiple times in the image and (2) search for prepositional phrases appended to the question subject to detect verbal disambiguation. Using object/attribute annotations, the question’s subject is matched to an object in the image, and a “spatial version” of the question is generated by removing the verbal disambiguation and instead generating a point from the center of the target object’s bounding box. We thus obtain a verbally disambiguating version of the dataset <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="D_{V}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><msub id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">𝐷</ci><ci id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">D_{V}</annotation></semantics></math> and a spatial disambiguating version <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><msub id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml"><mi id="S3.SS3.p6.2.m2.1.1.2" xref="S3.SS3.p6.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS3.p6.2.m2.1.1.3" xref="S3.SS3.p6.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><apply id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.2.m2.1.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p6.2.m2.1.1.2.cmml" xref="S3.SS3.p6.2.m2.1.1.2">𝐷</ci><ci id="S3.SS3.p6.2.m2.1.1.3.cmml" xref="S3.SS3.p6.2.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">D_{S}</annotation></semantics></math>, with 1,855 questions across 1,575 images in each. We use 80% for training and 10% each for validation and testing.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.3" class="ltx_p"><em id="S3.SS3.p7.3.1" class="ltx_emph ltx_font_italic">Results.</em> Our model leveraging the point input achieves an accuracy of 66.5% on <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><msub id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mi id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS3.p7.1.m1.1.1.3" xref="S3.SS3.p7.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2">𝐷</ci><ci id="S3.SS3.p7.1.m1.1.1.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">D_{S}</annotation></semantics></math>, while the Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> baseline relying on verbal disambiguation achieves only 26.5% on <math id="S3.SS3.p7.2.m2.1" class="ltx_Math" alttext="D_{V}" display="inline"><semantics id="S3.SS3.p7.2.m2.1a"><msub id="S3.SS3.p7.2.m2.1.1" xref="S3.SS3.p7.2.m2.1.1.cmml"><mi id="S3.SS3.p7.2.m2.1.1.2" xref="S3.SS3.p7.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS3.p7.2.m2.1.1.3" xref="S3.SS3.p7.2.m2.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.1b"><apply id="S3.SS3.p7.2.m2.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m2.1.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p7.2.m2.1.1.2.cmml" xref="S3.SS3.p7.2.m2.1.1.2">𝐷</ci><ci id="S3.SS3.p7.2.m2.1.1.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.1c">D_{V}</annotation></semantics></math> (Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.3 PointQA-Local evaluation ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Note that a question-only model gets 22.2% accuracy on <math id="S3.SS3.p7.3.m3.1" class="ltx_Math" alttext="D_{V}" display="inline"><semantics id="S3.SS3.p7.3.m3.1a"><msub id="S3.SS3.p7.3.m3.1.1" xref="S3.SS3.p7.3.m3.1.1.cmml"><mi id="S3.SS3.p7.3.m3.1.1.2" xref="S3.SS3.p7.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS3.p7.3.m3.1.1.3" xref="S3.SS3.p7.3.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.3.m3.1b"><apply id="S3.SS3.p7.3.m3.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.3.m3.1.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p7.3.m3.1.1.2.cmml" xref="S3.SS3.p7.3.m3.1.1.2">𝐷</ci><ci id="S3.SS3.p7.3.m3.1.1.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.3.m3.1c">D_{V}</annotation></semantics></math>; thus, the baseline does not effectively understand verbal disambiguation. This further motivates point-input questions as an extension of VQA.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>PointQA-LookTwice: reasoning about a local region in the broader image context</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We next consider the more general <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> setting which requires situating a local region in the broader context of the image. A natural example is counting questions such as “How many of <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">this</em> animal are there?”, where the model must identify the relevant object around the point and then use this information to attend to the entire image. Correspondingly, in Section <a href="#S4.SS1" title="4.1 PointQA-LookTwice dataset ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> we construct a dataset of counting questions from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> such that looking at a local region around the point is insufficient to answer the question. We then introduce a new model in Section <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> that includes global attention and show that it successfully reasons about the point in the broader image context.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We construct the <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> dataset leveraging the 99,860 human-written counting questions in Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> (e.g., “how many trucks are there?”). We turn it into pointing QA by further leveraging the object annotations as in Sec. <a href="#S3.SS1" title="3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Question Generation.</span> For each question, we extract the subject of the question (e.g., “truck”) and attempt to match it to an annotated region in the image. If multiple regions exist, one is chosen at random; if none exist, the question is removed. From this filtered question set, questions about object classes appearing less than 100 times total are further removed. The remaining set of object classes are manually grouped into three super-categories: beings (people and animals), vehicles (ex. cars, planes), and objects (ex. laptops, umbrellas).
From each question of the form e.g., “How many [object class] are there?” we generate two new questions by replacing the “[object class]” either with (1) “these [supercategory]” or (2) just “these.” We disambiguate the reference through the point input, simulated as the center of the object bounding box. The answer remains the same for all three questions; we thus train the model to locally infer the object category from the point and then count globally. From the raw object counts we bin the possible answers to “1”, “2”, and “<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><gt id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&gt;</annotation></semantics></math> 2”, such that the model predicts one of three answers to the counting question.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.4" class="ltx_p"><span id="S4.SS1.p3.4.1" class="ltx_text ltx_font_bold">Counteracting priors.</span> Our goal is to construct questions that <em id="S4.SS1.p3.4.2" class="ltx_emph ltx_font_italic">require</em> the point input and cannot be answered without it. Thus, we enforce that for each image <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">I</annotation></semantics></math> in the evaluation set, there are at least two questions <math id="S4.SS1.p3.2.m2.2" class="ltx_Math" alttext="q_{i},q_{j}" display="inline"><semantics id="S4.SS1.p3.2.m2.2a"><mrow id="S4.SS1.p3.2.m2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.3.cmml"><msub id="S4.SS1.p3.2.m2.1.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.1.1.2" xref="S4.SS1.p3.2.m2.1.1.1.1.2.cmml">q</mi><mi id="S4.SS1.p3.2.m2.1.1.1.1.3" xref="S4.SS1.p3.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p3.2.m2.2.2.2.3" xref="S4.SS1.p3.2.m2.2.2.3.cmml">,</mo><msub id="S4.SS1.p3.2.m2.2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.2.2.cmml"><mi id="S4.SS1.p3.2.m2.2.2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.2.2.2.cmml">q</mi><mi id="S4.SS1.p3.2.m2.2.2.2.2.3" xref="S4.SS1.p3.2.m2.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.2b"><list id="S4.SS1.p3.2.m2.2.2.3.cmml" xref="S4.SS1.p3.2.m2.2.2.2"><apply id="S4.SS1.p3.2.m2.1.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.2">𝑞</ci><ci id="S4.SS1.p3.2.m2.1.1.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S4.SS1.p3.2.m2.2.2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.2.2.2.2.1.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p3.2.m2.2.2.2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.2">𝑞</ci><ci id="S4.SS1.p3.2.m2.2.2.2.2.3.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.2c">q_{i},q_{j}</annotation></semantics></math> such that <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="o_{i}\neq o_{j}" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><msub id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml"><mi id="S4.SS1.p3.3.m3.1.1.2.2" xref="S4.SS1.p3.3.m3.1.1.2.2.cmml">o</mi><mi id="S4.SS1.p3.3.m3.1.1.2.3" xref="S4.SS1.p3.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">≠</mo><msub id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml"><mi id="S4.SS1.p3.3.m3.1.1.3.2" xref="S4.SS1.p3.3.m3.1.1.3.2.cmml">o</mi><mi id="S4.SS1.p3.3.m3.1.1.3.3" xref="S4.SS1.p3.3.m3.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><neq id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1"></neq><apply id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.2.1.cmml" xref="S4.SS1.p3.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.2.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2.2">𝑜</ci><ci id="S4.SS1.p3.3.m3.1.1.2.3.cmml" xref="S4.SS1.p3.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.3.1.cmml" xref="S4.SS1.p3.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.3.2.cmml" xref="S4.SS1.p3.3.m3.1.1.3.2">𝑜</ci><ci id="S4.SS1.p3.3.m3.1.1.3.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">o_{i}\neq o_{j}</annotation></semantics></math> (different objects), and <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="a_{i}\neq a_{j}" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><msub id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml"><mi id="S4.SS1.p3.4.m4.1.1.2.2" xref="S4.SS1.p3.4.m4.1.1.2.2.cmml">a</mi><mi id="S4.SS1.p3.4.m4.1.1.2.3" xref="S4.SS1.p3.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">≠</mo><msub id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3.cmml"><mi id="S4.SS1.p3.4.m4.1.1.3.2" xref="S4.SS1.p3.4.m4.1.1.3.2.cmml">a</mi><mi id="S4.SS1.p3.4.m4.1.1.3.3" xref="S4.SS1.p3.4.m4.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><neq id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1"></neq><apply id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.2.1.cmml" xref="S4.SS1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.2.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2.2">𝑎</ci><ci id="S4.SS1.p3.4.m4.1.1.2.3.cmml" xref="S4.SS1.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.3.1.cmml" xref="S4.SS1.p3.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.3.2.cmml" xref="S4.SS1.p3.4.m4.1.1.3.2">𝑎</ci><ci id="S4.SS1.p3.4.m4.1.1.3.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">a_{i}\neq a_{j}</annotation></semantics></math> (different answers).</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.5" class="ltx_p">The remaining generated questions are added to the training set. To prevent priors in the training set, for each question <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><msub id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">q</mi><mi id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">𝑞</ci><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">q_{i}</annotation></semantics></math> we use Visual Genome object annotations to generate a question <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="q_{j}" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><msub id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">q</mi><mi id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">𝑞</ci><ci id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">q_{j}</annotation></semantics></math> with <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="o_{i}\neq o_{j}" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mrow id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><msub id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2.2" xref="S4.SS1.p4.3.m3.1.1.2.2.cmml">o</mi><mi id="S4.SS1.p4.3.m3.1.1.2.3" xref="S4.SS1.p4.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.p4.3.m3.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.cmml">≠</mo><msub id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml"><mi id="S4.SS1.p4.3.m3.1.1.3.2" xref="S4.SS1.p4.3.m3.1.1.3.2.cmml">o</mi><mi id="S4.SS1.p4.3.m3.1.1.3.3" xref="S4.SS1.p4.3.m3.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><neq id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1"></neq><apply id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.2.1.cmml" xref="S4.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.2.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2.2">𝑜</ci><ci id="S4.SS1.p4.3.m3.1.1.2.3.cmml" xref="S4.SS1.p4.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.3.1.cmml" xref="S4.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.3.2.cmml" xref="S4.SS1.p4.3.m3.1.1.3.2">𝑜</ci><ci id="S4.SS1.p4.3.m3.1.1.3.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">o_{i}\neq o_{j}</annotation></semantics></math> and <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="a_{i}\neq a_{j}" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><mrow id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml"><msub id="S4.SS1.p4.4.m4.1.1.2" xref="S4.SS1.p4.4.m4.1.1.2.cmml"><mi id="S4.SS1.p4.4.m4.1.1.2.2" xref="S4.SS1.p4.4.m4.1.1.2.2.cmml">a</mi><mi id="S4.SS1.p4.4.m4.1.1.2.3" xref="S4.SS1.p4.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.p4.4.m4.1.1.1" xref="S4.SS1.p4.4.m4.1.1.1.cmml">≠</mo><msub id="S4.SS1.p4.4.m4.1.1.3" xref="S4.SS1.p4.4.m4.1.1.3.cmml"><mi id="S4.SS1.p4.4.m4.1.1.3.2" xref="S4.SS1.p4.4.m4.1.1.3.2.cmml">a</mi><mi id="S4.SS1.p4.4.m4.1.1.3.3" xref="S4.SS1.p4.4.m4.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1"><neq id="S4.SS1.p4.4.m4.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1.1"></neq><apply id="S4.SS1.p4.4.m4.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m4.1.1.2.1.cmml" xref="S4.SS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p4.4.m4.1.1.2.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2.2">𝑎</ci><ci id="S4.SS1.p4.4.m4.1.1.2.3.cmml" xref="S4.SS1.p4.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.p4.4.m4.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m4.1.1.3.1.cmml" xref="S4.SS1.p4.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS1.p4.4.m4.1.1.3.2.cmml" xref="S4.SS1.p4.4.m4.1.1.3.2">𝑎</ci><ci id="S4.SS1.p4.4.m4.1.1.3.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">a_{i}\neq a_{j}</annotation></semantics></math>. Object counts are generated from the IoU-filtered object annotations in the image and may be imperfect due to under-counting (thus not used during evaluation). The original question is constructed as “How many <math id="S4.SS1.p4.5.m5.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S4.SS1.p4.5.m5.1a"><msub id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml"><mi id="S4.SS1.p4.5.m5.1.1.2" xref="S4.SS1.p4.5.m5.1.1.2.cmml">o</mi><mi id="S4.SS1.p4.5.m5.1.1.3" xref="S4.SS1.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.1b"><apply id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.5.m5.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p4.5.m5.1.1.2.cmml" xref="S4.SS1.p4.5.m5.1.1.2">𝑜</ci><ci id="S4.SS1.p4.5.m5.1.1.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.1c">o_{i}</annotation></semantics></math> are there?”, with the object class in its plural form for readability, and then converted into two generic question types as above.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.5" class="ltx_p"><span id="S4.SS1.p5.5.1" class="ltx_text ltx_font_bold">Statistics.</span> The <span id="S4.SS1.p5.5.2" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> dataset contains 57,405 questions across 34,676 images, including (1) <em id="S4.SS1.p5.5.3" class="ltx_emph ltx_font_italic">train</em> with <math id="S4.SS1.p5.1.m1.2" class="ltx_Math" alttext="37,981" display="inline"><semantics id="S4.SS1.p5.1.m1.2a"><mrow id="S4.SS1.p5.1.m1.2.3.2" xref="S4.SS1.p5.1.m1.2.3.1.cmml"><mn id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">37</mn><mo id="S4.SS1.p5.1.m1.2.3.2.1" xref="S4.SS1.p5.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p5.1.m1.2.2" xref="S4.SS1.p5.1.m1.2.2.cmml">981</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.2b"><list id="S4.SS1.p5.1.m1.2.3.1.cmml" xref="S4.SS1.p5.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">37</cn><cn type="integer" id="S4.SS1.p5.1.m1.2.2.cmml" xref="S4.SS1.p5.1.m1.2.2">981</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.2c">37,981</annotation></semantics></math> human-written and <math id="S4.SS1.p5.2.m2.2" class="ltx_Math" alttext="14,713" display="inline"><semantics id="S4.SS1.p5.2.m2.2a"><mrow id="S4.SS1.p5.2.m2.2.3.2" xref="S4.SS1.p5.2.m2.2.3.1.cmml"><mn id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">14</mn><mo id="S4.SS1.p5.2.m2.2.3.2.1" xref="S4.SS1.p5.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.p5.2.m2.2.2" xref="S4.SS1.p5.2.m2.2.2.cmml">713</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.2b"><list id="S4.SS1.p5.2.m2.2.3.1.cmml" xref="S4.SS1.p5.2.m2.2.3.2"><cn type="integer" id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">14</cn><cn type="integer" id="S4.SS1.p5.2.m2.2.2.cmml" xref="S4.SS1.p5.2.m2.2.2">713</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.2c">14,713</annotation></semantics></math> automatically-generated questions across 32,925 images, (2) <em id="S4.SS1.p5.5.4" class="ltx_emph ltx_font_italic">val</em> with <math id="S4.SS1.p5.3.m3.1" class="ltx_Math" alttext="997" display="inline"><semantics id="S4.SS1.p5.3.m3.1a"><mn id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml">997</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><cn type="integer" id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1">997</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">997</annotation></semantics></math> human-written questions across 380 images, and (3) <em id="S4.SS1.p5.5.5" class="ltx_emph ltx_font_italic">test</em> with <math id="S4.SS1.p5.4.m4.2" class="ltx_Math" alttext="3,714" display="inline"><semantics id="S4.SS1.p5.4.m4.2a"><mrow id="S4.SS1.p5.4.m4.2.3.2" xref="S4.SS1.p5.4.m4.2.3.1.cmml"><mn id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml">3</mn><mo id="S4.SS1.p5.4.m4.2.3.2.1" xref="S4.SS1.p5.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS1.p5.4.m4.2.2" xref="S4.SS1.p5.4.m4.2.2.cmml">714</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.2b"><list id="S4.SS1.p5.4.m4.2.3.1.cmml" xref="S4.SS1.p5.4.m4.2.3.2"><cn type="integer" id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1">3</cn><cn type="integer" id="S4.SS1.p5.4.m4.2.2.cmml" xref="S4.SS1.p5.4.m4.2.2">714</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.2c">3,714</annotation></semantics></math> human-written questions across <math id="S4.SS1.p5.5.m5.2" class="ltx_Math" alttext="1,371" display="inline"><semantics id="S4.SS1.p5.5.m5.2a"><mrow id="S4.SS1.p5.5.m5.2.3.2" xref="S4.SS1.p5.5.m5.2.3.1.cmml"><mn id="S4.SS1.p5.5.m5.1.1" xref="S4.SS1.p5.5.m5.1.1.cmml">1</mn><mo id="S4.SS1.p5.5.m5.2.3.2.1" xref="S4.SS1.p5.5.m5.2.3.1.cmml">,</mo><mn id="S4.SS1.p5.5.m5.2.2" xref="S4.SS1.p5.5.m5.2.2.cmml">371</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.5.m5.2b"><list id="S4.SS1.p5.5.m5.2.3.1.cmml" xref="S4.SS1.p5.5.m5.2.3.2"><cn type="integer" id="S4.SS1.p5.5.m5.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1">1</cn><cn type="integer" id="S4.SS1.p5.5.m5.2.2.cmml" xref="S4.SS1.p5.5.m5.2.2">371</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.5.m5.2c">1,371</annotation></semantics></math> images.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.11" class="ltx_p">The answer distribution is reasonably balanced: the answers “1”, “2”, and “<math id="S4.SS1.p6.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS1.p6.1.m1.1a"><mo id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><gt id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">&gt;</annotation></semantics></math> 2” appear with <math id="S4.SS1.p6.2.m2.1" class="ltx_Math" alttext="35.4\%" display="inline"><semantics id="S4.SS1.p6.2.m2.1a"><mrow id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml"><mn id="S4.SS1.p6.2.m2.1.1.2" xref="S4.SS1.p6.2.m2.1.1.2.cmml">35.4</mn><mo id="S4.SS1.p6.2.m2.1.1.1" xref="S4.SS1.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><apply id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p6.2.m2.1.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.2.m2.1.1.2.cmml" xref="S4.SS1.p6.2.m2.1.1.2">35.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">35.4\%</annotation></semantics></math>, <math id="S4.SS1.p6.3.m3.1" class="ltx_Math" alttext="33.7\%" display="inline"><semantics id="S4.SS1.p6.3.m3.1a"><mrow id="S4.SS1.p6.3.m3.1.1" xref="S4.SS1.p6.3.m3.1.1.cmml"><mn id="S4.SS1.p6.3.m3.1.1.2" xref="S4.SS1.p6.3.m3.1.1.2.cmml">33.7</mn><mo id="S4.SS1.p6.3.m3.1.1.1" xref="S4.SS1.p6.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.3.m3.1b"><apply id="S4.SS1.p6.3.m3.1.1.cmml" xref="S4.SS1.p6.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p6.3.m3.1.1.1.cmml" xref="S4.SS1.p6.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.3.m3.1.1.2.cmml" xref="S4.SS1.p6.3.m3.1.1.2">33.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.3.m3.1c">33.7\%</annotation></semantics></math>, and <math id="S4.SS1.p6.4.m4.1" class="ltx_Math" alttext="30.9\%" display="inline"><semantics id="S4.SS1.p6.4.m4.1a"><mrow id="S4.SS1.p6.4.m4.1.1" xref="S4.SS1.p6.4.m4.1.1.cmml"><mn id="S4.SS1.p6.4.m4.1.1.2" xref="S4.SS1.p6.4.m4.1.1.2.cmml">30.9</mn><mo id="S4.SS1.p6.4.m4.1.1.1" xref="S4.SS1.p6.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.4.m4.1b"><apply id="S4.SS1.p6.4.m4.1.1.cmml" xref="S4.SS1.p6.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.p6.4.m4.1.1.1.cmml" xref="S4.SS1.p6.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.4.m4.1.1.2.cmml" xref="S4.SS1.p6.4.m4.1.1.2">30.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.4.m4.1c">30.9\%</annotation></semantics></math> frequency respectively in the test set. The most common objects in the dataset are people (<math id="S4.SS1.p6.5.m5.1" class="ltx_Math" alttext="29.7\%" display="inline"><semantics id="S4.SS1.p6.5.m5.1a"><mrow id="S4.SS1.p6.5.m5.1.1" xref="S4.SS1.p6.5.m5.1.1.cmml"><mn id="S4.SS1.p6.5.m5.1.1.2" xref="S4.SS1.p6.5.m5.1.1.2.cmml">29.7</mn><mo id="S4.SS1.p6.5.m5.1.1.1" xref="S4.SS1.p6.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.5.m5.1b"><apply id="S4.SS1.p6.5.m5.1.1.cmml" xref="S4.SS1.p6.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.p6.5.m5.1.1.1.cmml" xref="S4.SS1.p6.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.5.m5.1.1.2.cmml" xref="S4.SS1.p6.5.m5.1.1.2">29.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.5.m5.1c">29.7\%</annotation></semantics></math>), cars (<math id="S4.SS1.p6.6.m6.1" class="ltx_Math" alttext="3.1\%" display="inline"><semantics id="S4.SS1.p6.6.m6.1a"><mrow id="S4.SS1.p6.6.m6.1.1" xref="S4.SS1.p6.6.m6.1.1.cmml"><mn id="S4.SS1.p6.6.m6.1.1.2" xref="S4.SS1.p6.6.m6.1.1.2.cmml">3.1</mn><mo id="S4.SS1.p6.6.m6.1.1.1" xref="S4.SS1.p6.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.6.m6.1b"><apply id="S4.SS1.p6.6.m6.1.1.cmml" xref="S4.SS1.p6.6.m6.1.1"><csymbol cd="latexml" id="S4.SS1.p6.6.m6.1.1.1.cmml" xref="S4.SS1.p6.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.6.m6.1.1.2.cmml" xref="S4.SS1.p6.6.m6.1.1.2">3.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.6.m6.1c">3.1\%</annotation></semantics></math>), and signs (<math id="S4.SS1.p6.7.m7.1" class="ltx_Math" alttext="2.9\%" display="inline"><semantics id="S4.SS1.p6.7.m7.1a"><mrow id="S4.SS1.p6.7.m7.1.1" xref="S4.SS1.p6.7.m7.1.1.cmml"><mn id="S4.SS1.p6.7.m7.1.1.2" xref="S4.SS1.p6.7.m7.1.1.2.cmml">2.9</mn><mo id="S4.SS1.p6.7.m7.1.1.1" xref="S4.SS1.p6.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.7.m7.1b"><apply id="S4.SS1.p6.7.m7.1.1.cmml" xref="S4.SS1.p6.7.m7.1.1"><csymbol cd="latexml" id="S4.SS1.p6.7.m7.1.1.1.cmml" xref="S4.SS1.p6.7.m7.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.7.m7.1.1.2.cmml" xref="S4.SS1.p6.7.m7.1.1.2">2.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.7.m7.1c">2.9\%</annotation></semantics></math>). Due to the high representation of people, the most common super-category is beings (<math id="S4.SS1.p6.8.m8.1" class="ltx_Math" alttext="48.9\%" display="inline"><semantics id="S4.SS1.p6.8.m8.1a"><mrow id="S4.SS1.p6.8.m8.1.1" xref="S4.SS1.p6.8.m8.1.1.cmml"><mn id="S4.SS1.p6.8.m8.1.1.2" xref="S4.SS1.p6.8.m8.1.1.2.cmml">48.9</mn><mo id="S4.SS1.p6.8.m8.1.1.1" xref="S4.SS1.p6.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.8.m8.1b"><apply id="S4.SS1.p6.8.m8.1.1.cmml" xref="S4.SS1.p6.8.m8.1.1"><csymbol cd="latexml" id="S4.SS1.p6.8.m8.1.1.1.cmml" xref="S4.SS1.p6.8.m8.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.8.m8.1.1.2.cmml" xref="S4.SS1.p6.8.m8.1.1.2">48.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.8.m8.1c">48.9\%</annotation></semantics></math>), followed by objects (<math id="S4.SS1.p6.9.m9.1" class="ltx_Math" alttext="36.7\%" display="inline"><semantics id="S4.SS1.p6.9.m9.1a"><mrow id="S4.SS1.p6.9.m9.1.1" xref="S4.SS1.p6.9.m9.1.1.cmml"><mn id="S4.SS1.p6.9.m9.1.1.2" xref="S4.SS1.p6.9.m9.1.1.2.cmml">36.7</mn><mo id="S4.SS1.p6.9.m9.1.1.1" xref="S4.SS1.p6.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.9.m9.1b"><apply id="S4.SS1.p6.9.m9.1.1.cmml" xref="S4.SS1.p6.9.m9.1.1"><csymbol cd="latexml" id="S4.SS1.p6.9.m9.1.1.1.cmml" xref="S4.SS1.p6.9.m9.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.9.m9.1.1.2.cmml" xref="S4.SS1.p6.9.m9.1.1.2">36.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.9.m9.1c">36.7\%</annotation></semantics></math>) and vehicles (<math id="S4.SS1.p6.10.m10.1" class="ltx_Math" alttext="14.3\%" display="inline"><semantics id="S4.SS1.p6.10.m10.1a"><mrow id="S4.SS1.p6.10.m10.1.1" xref="S4.SS1.p6.10.m10.1.1.cmml"><mn id="S4.SS1.p6.10.m10.1.1.2" xref="S4.SS1.p6.10.m10.1.1.2.cmml">14.3</mn><mo id="S4.SS1.p6.10.m10.1.1.1" xref="S4.SS1.p6.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.10.m10.1b"><apply id="S4.SS1.p6.10.m10.1.1.cmml" xref="S4.SS1.p6.10.m10.1.1"><csymbol cd="latexml" id="S4.SS1.p6.10.m10.1.1.1.cmml" xref="S4.SS1.p6.10.m10.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.10.m10.1.1.2.cmml" xref="S4.SS1.p6.10.m10.1.1.2">14.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.10.m10.1c">14.3\%</annotation></semantics></math>). Given only the object name, a model can achieve an accuracy of at most <math id="S4.SS1.p6.11.m11.1" class="ltx_Math" alttext="42.1\%" display="inline"><semantics id="S4.SS1.p6.11.m11.1a"><mrow id="S4.SS1.p6.11.m11.1.1" xref="S4.SS1.p6.11.m11.1.1.cmml"><mn id="S4.SS1.p6.11.m11.1.1.2" xref="S4.SS1.p6.11.m11.1.1.2.cmml">42.1</mn><mo id="S4.SS1.p6.11.m11.1.1.1" xref="S4.SS1.p6.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.11.m11.1b"><apply id="S4.SS1.p6.11.m11.1.1.cmml" xref="S4.SS1.p6.11.m11.1.1"><csymbol cd="latexml" id="S4.SS1.p6.11.m11.1.1.1.cmml" xref="S4.SS1.p6.11.m11.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p6.11.m11.1.1.2.cmml" xref="S4.SS1.p6.11.m11.1.1.2">42.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.11.m11.1c">42.1\%</annotation></semantics></math>, indicating that visual reasoning is necessary. Example questions are shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.1 PointQA-LookTwice dataset ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold">Human accuracy.</span> As in Sec. <a href="#S3.SS1" title="3.1 PointQA-Local dataset ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> we run a small-scale human study on 100 <em id="S4.SS1.p7.1.2" class="ltx_emph ltx_font_italic">test</em> questions to evaluate the reliability of the data. Please see Appendix <a href="#A1.SS3" title="A.3 PointQA-LookTwice ‣ Appendix A Human Evaluations ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a> for details on human evaluation.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2011.13681/assets/figs/looktwice_all_2.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text" style="font-size:90%;">Example entries in the <span id="S4.F5.4.2.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> Dataset. Point is in red and supercategory is underlined.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> model</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.6" class="ltx_p"><span id="S4.SS2.p1.6.1" class="ltx_text ltx_font_bold">Local-only.</span> We adapt the Pythia-based model of Sec. <a href="#S3.SS2" title="3.2 PointQA-Local models ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> to this new setting, producing a model that combines local with global contextual reasoning. So far we had a set of visual features <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\{\mathbf{v}_{i}^{pt}\}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">{</mo><msubsup id="S4.SS2.p1.1.m1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S4.SS2.p1.1.m1.1.1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.1.1.2.3.cmml">i</mi><mrow id="S4.SS2.p1.1.m1.1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1.1.3.1" xref="S4.SS2.p1.1.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.1.1.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><set id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1"><apply id="S4.SS2.p1.1.m1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1">superscript</csymbol><apply id="S4.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2">𝐯</ci><ci id="S4.SS2.p1.1.m1.1.1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.3.1"></times><ci id="S4.SS2.p1.1.m1.1.1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.3.2">𝑝</ci><ci id="S4.SS2.p1.1.m1.1.1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.3.3">𝑡</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\{\mathbf{v}_{i}^{pt}\}</annotation></semantics></math> corresponding to region proposals containing the point <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="pt" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑝</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">pt</annotation></semantics></math>, and we computed an attention vector over these proposals. Concretely, we projected the visual features <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="\{\mathbf{v}_{i}^{pt}\}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.3.m3.1.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">{</mo><msubsup id="S4.SS2.p1.3.m3.1.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.1.1.2.2" xref="S4.SS2.p1.3.m3.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S4.SS2.p1.3.m3.1.1.1.1.2.3" xref="S4.SS2.p1.3.m3.1.1.1.1.2.3.cmml">i</mi><mrow id="S4.SS2.p1.3.m3.1.1.1.1.3" xref="S4.SS2.p1.3.m3.1.1.1.1.3.cmml"><mi id="S4.SS2.p1.3.m3.1.1.1.1.3.2" xref="S4.SS2.p1.3.m3.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.1.1.3.1" xref="S4.SS2.p1.3.m3.1.1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.3.m3.1.1.1.1.3.3" xref="S4.SS2.p1.3.m3.1.1.1.1.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S4.SS2.p1.3.m3.1.1.1.3" xref="S4.SS2.p1.3.m3.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><set id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.1"><apply id="S4.SS2.p1.3.m3.1.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1">superscript</csymbol><apply id="S4.SS2.p1.3.m3.1.1.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.1.2.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.1.1.2.2.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.2.2">𝐯</ci><ci id="S4.SS2.p1.3.m3.1.1.1.1.2.3.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.p1.3.m3.1.1.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.3"><times id="S4.SS2.p1.3.m3.1.1.1.1.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.3.1"></times><ci id="S4.SS2.p1.3.m3.1.1.1.1.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.3.2">𝑝</ci><ci id="S4.SS2.p1.3.m3.1.1.1.1.3.3.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.3.3">𝑡</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\{\mathbf{v}_{i}^{pt}\}</annotation></semantics></math> and the text features of the question <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\mathbf{q}</annotation></semantics></math> to a common space, multiplied them element-wise and then ran through fully-connected layers to produce an attention vector, which is then normalized to a probability distribution <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{a}^{pt}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><msup id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">𝐚</mi><mrow id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml"><mi id="S4.SS2.p1.5.m5.1.1.3.2" xref="S4.SS2.p1.5.m5.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.3.1" xref="S4.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">superscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝐚</ci><apply id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><times id="S4.SS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3.1"></times><ci id="S4.SS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.2">𝑝</ci><ci id="S4.SS2.p1.5.m5.1.1.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">\mathbf{a}^{pt}</annotation></semantics></math>. This allowed us to produce <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{v}^{pt}=\sum_{i}\mathbf{a}_{i}^{pt}\mathbf{v}_{i}^{pt}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><msup id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2.2" xref="S4.SS2.p1.6.m6.1.1.2.2.cmml">𝐯</mi><mrow id="S4.SS2.p1.6.m6.1.1.2.3" xref="S4.SS2.p1.6.m6.1.1.2.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2.3.2" xref="S4.SS2.p1.6.m6.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.2.3.1" xref="S4.SS2.p1.6.m6.1.1.2.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.2.3.3" xref="S4.SS2.p1.6.m6.1.1.2.3.3.cmml">t</mi></mrow></msup><mo rspace="0.111em" id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml"><msub id="S4.SS2.p1.6.m6.1.1.3.1" xref="S4.SS2.p1.6.m6.1.1.3.1.cmml"><mo id="S4.SS2.p1.6.m6.1.1.3.1.2" xref="S4.SS2.p1.6.m6.1.1.3.1.2.cmml">∑</mo><mi id="S4.SS2.p1.6.m6.1.1.3.1.3" xref="S4.SS2.p1.6.m6.1.1.3.1.3.cmml">i</mi></msub><mrow id="S4.SS2.p1.6.m6.1.1.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.cmml"><msubsup id="S4.SS2.p1.6.m6.1.1.3.2.2" xref="S4.SS2.p1.6.m6.1.1.3.2.2.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2.2.2.2" xref="S4.SS2.p1.6.m6.1.1.3.2.2.2.2.cmml">𝐚</mi><mi id="S4.SS2.p1.6.m6.1.1.3.2.2.2.3" xref="S4.SS2.p1.6.m6.1.1.3.2.2.2.3.cmml">i</mi><mrow id="S4.SS2.p1.6.m6.1.1.3.2.2.3" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2.2.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.2.2.3.1" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.2.2.3.3" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.3.cmml">t</mi></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.2.1" xref="S4.SS2.p1.6.m6.1.1.3.2.1.cmml">​</mo><msubsup id="S4.SS2.p1.6.m6.1.1.3.2.3" xref="S4.SS2.p1.6.m6.1.1.3.2.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2.3.2.2" xref="S4.SS2.p1.6.m6.1.1.3.2.3.2.2.cmml">𝐯</mi><mi id="S4.SS2.p1.6.m6.1.1.3.2.3.2.3" xref="S4.SS2.p1.6.m6.1.1.3.2.3.2.3.cmml">i</mi><mrow id="S4.SS2.p1.6.m6.1.1.3.2.3.3" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2.3.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.2.3.3.1" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.2.3.3.3" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.3.cmml">t</mi></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><eq id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></eq><apply id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.2.1.cmml" xref="S4.SS2.p1.6.m6.1.1.2">superscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.2.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2.2">𝐯</ci><apply id="S4.SS2.p1.6.m6.1.1.2.3.cmml" xref="S4.SS2.p1.6.m6.1.1.2.3"><times id="S4.SS2.p1.6.m6.1.1.2.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.2.3.1"></times><ci id="S4.SS2.p1.6.m6.1.1.2.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2.3.2">𝑝</ci><ci id="S4.SS2.p1.6.m6.1.1.2.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><apply id="S4.SS2.p1.6.m6.1.1.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.1">subscript</csymbol><sum id="S4.SS2.p1.6.m6.1.1.3.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.1.2"></sum><ci id="S4.SS2.p1.6.m6.1.1.3.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.1.3">𝑖</ci></apply><apply id="S4.SS2.p1.6.m6.1.1.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2"><times id="S4.SS2.p1.6.m6.1.1.3.2.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.1"></times><apply id="S4.SS2.p1.6.m6.1.1.3.2.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.2.2.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2">superscript</csymbol><apply id="S4.SS2.p1.6.m6.1.1.3.2.2.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.2.2.2.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.3.2.2.2.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2.2.2">𝐚</ci><ci id="S4.SS2.p1.6.m6.1.1.3.2.2.2.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2.2.3">𝑖</ci></apply><apply id="S4.SS2.p1.6.m6.1.1.3.2.2.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3"><times id="S4.SS2.p1.6.m6.1.1.3.2.2.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.1"></times><ci id="S4.SS2.p1.6.m6.1.1.3.2.2.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.2">𝑝</ci><ci id="S4.SS2.p1.6.m6.1.1.3.2.2.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.2.3.3">𝑡</ci></apply></apply><apply id="S4.SS2.p1.6.m6.1.1.3.2.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.2.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3">superscript</csymbol><apply id="S4.SS2.p1.6.m6.1.1.3.2.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.2.3.2.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.3.2.3.2.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3.2.2">𝐯</ci><ci id="S4.SS2.p1.6.m6.1.1.3.2.3.2.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3.2.3">𝑖</ci></apply><apply id="S4.SS2.p1.6.m6.1.1.3.2.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3"><times id="S4.SS2.p1.6.m6.1.1.3.2.3.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.1"></times><ci id="S4.SS2.p1.6.m6.1.1.3.2.3.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.2">𝑝</ci><ci id="S4.SS2.p1.6.m6.1.1.3.2.3.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">\mathbf{v}^{pt}=\sum_{i}\mathbf{a}_{i}^{pt}\mathbf{v}_{i}^{pt}</annotation></semantics></math> describing the relevant information of the local region.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.13" class="ltx_p"><span id="S4.SS2.p2.13.1" class="ltx_text ltx_font_bold">Global attention.</span> Now we further use this local information to attend to the entire image: concretely, we project the visual features <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\{\mathbf{v}_{i}\}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">{</mo><msub id="S4.SS2.p2.1.m1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.2.cmml">𝐯</mi><mi id="S4.SS2.p2.1.m1.1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><set id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.1"><apply id="S4.SS2.p2.1.m1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.2">𝐯</ci><ci id="S4.SS2.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\{\mathbf{v}_{i}\}</annotation></semantics></math> from <em id="S4.SS2.p2.13.2" class="ltx_emph ltx_font_italic">all</em> region proposals, the text features <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathbf{q}</annotation></semantics></math>, and the new <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{v}^{pt}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><msup id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.3.1" xref="S4.SS2.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">superscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝐯</ci><apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><times id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.1"></times><ci id="S4.SS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.2">𝑝</ci><ci id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\mathbf{v}^{pt}</annotation></semantics></math> to a common subspace, and then as before element-wise multiply and run through fully-connected layers to compute the normalized attention <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{a}^{all}" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><msup id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">𝐚</mi><mrow id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml"><mi id="S4.SS2.p2.4.m4.1.1.3.2" xref="S4.SS2.p2.4.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.1.3.1" xref="S4.SS2.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.4.m4.1.1.3.3" xref="S4.SS2.p2.4.m4.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.1.3.1a" xref="S4.SS2.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.4.m4.1.1.3.4" xref="S4.SS2.p2.4.m4.1.1.3.4.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">superscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">𝐚</ci><apply id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3"><times id="S4.SS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3.1"></times><ci id="S4.SS2.p2.4.m4.1.1.3.2.cmml" xref="S4.SS2.p2.4.m4.1.1.3.2">𝑎</ci><ci id="S4.SS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3">𝑙</ci><ci id="S4.SS2.p2.4.m4.1.1.3.4.cmml" xref="S4.SS2.p2.4.m4.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\mathbf{a}^{all}</annotation></semantics></math>, yielding the global representation <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{v}^{all}=\sum_{i}\mathbf{a}^{all}_{i}\mathbf{v}_{i}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><msup id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2.2" xref="S4.SS2.p2.5.m5.1.1.2.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.5.m5.1.1.2.3" xref="S4.SS2.p2.5.m5.1.1.2.3.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2.3.2" xref="S4.SS2.p2.5.m5.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.2.3.1" xref="S4.SS2.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.2.3.3" xref="S4.SS2.p2.5.m5.1.1.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.2.3.1a" xref="S4.SS2.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.2.3.4" xref="S4.SS2.p2.5.m5.1.1.2.3.4.cmml">l</mi></mrow></msup><mo rspace="0.111em" id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml"><msub id="S4.SS2.p2.5.m5.1.1.3.1" xref="S4.SS2.p2.5.m5.1.1.3.1.cmml"><mo id="S4.SS2.p2.5.m5.1.1.3.1.2" xref="S4.SS2.p2.5.m5.1.1.3.1.2.cmml">∑</mo><mi id="S4.SS2.p2.5.m5.1.1.3.1.3" xref="S4.SS2.p2.5.m5.1.1.3.1.3.cmml">i</mi></msub><mrow id="S4.SS2.p2.5.m5.1.1.3.2" xref="S4.SS2.p2.5.m5.1.1.3.2.cmml"><msubsup id="S4.SS2.p2.5.m5.1.1.3.2.2" xref="S4.SS2.p2.5.m5.1.1.3.2.2.cmml"><mi id="S4.SS2.p2.5.m5.1.1.3.2.2.2.2" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.2.cmml">𝐚</mi><mi id="S4.SS2.p2.5.m5.1.1.3.2.2.3" xref="S4.SS2.p2.5.m5.1.1.3.2.2.3.cmml">i</mi><mrow id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.cmml"><mi id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.2" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.1" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.3" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.1a" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.4" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.4.cmml">l</mi></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.3.2.1" xref="S4.SS2.p2.5.m5.1.1.3.2.1.cmml">​</mo><msub id="S4.SS2.p2.5.m5.1.1.3.2.3" xref="S4.SS2.p2.5.m5.1.1.3.2.3.cmml"><mi id="S4.SS2.p2.5.m5.1.1.3.2.3.2" xref="S4.SS2.p2.5.m5.1.1.3.2.3.2.cmml">𝐯</mi><mi id="S4.SS2.p2.5.m5.1.1.3.2.3.3" xref="S4.SS2.p2.5.m5.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><eq id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></eq><apply id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.2.1.cmml" xref="S4.SS2.p2.5.m5.1.1.2">superscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.2.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2.2">𝐯</ci><apply id="S4.SS2.p2.5.m5.1.1.2.3.cmml" xref="S4.SS2.p2.5.m5.1.1.2.3"><times id="S4.SS2.p2.5.m5.1.1.2.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.2.3.1"></times><ci id="S4.SS2.p2.5.m5.1.1.2.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2.3.2">𝑎</ci><ci id="S4.SS2.p2.5.m5.1.1.2.3.3.cmml" xref="S4.SS2.p2.5.m5.1.1.2.3.3">𝑙</ci><ci id="S4.SS2.p2.5.m5.1.1.2.3.4.cmml" xref="S4.SS2.p2.5.m5.1.1.2.3.4">𝑙</ci></apply></apply><apply id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3"><apply id="S4.SS2.p2.5.m5.1.1.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.3.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1">subscript</csymbol><sum id="S4.SS2.p2.5.m5.1.1.3.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1.2"></sum><ci id="S4.SS2.p2.5.m5.1.1.3.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1.3">𝑖</ci></apply><apply id="S4.SS2.p2.5.m5.1.1.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2"><times id="S4.SS2.p2.5.m5.1.1.3.2.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.1"></times><apply id="S4.SS2.p2.5.m5.1.1.3.2.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.3.2.2.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2">subscript</csymbol><apply id="S4.SS2.p2.5.m5.1.1.3.2.2.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.3.2.2.2.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2">superscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.3.2.2.2.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.2">𝐚</ci><apply id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3"><times id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.1"></times><ci id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.2">𝑎</ci><ci id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.3">𝑙</ci><ci id="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.4.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.2.3.4">𝑙</ci></apply></apply><ci id="S4.SS2.p2.5.m5.1.1.3.2.2.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2.3">𝑖</ci></apply><apply id="S4.SS2.p2.5.m5.1.1.3.2.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.3.2.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.3">subscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.3.2.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.3.2">𝐯</ci><ci id="S4.SS2.p2.5.m5.1.1.3.2.3.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\mathbf{v}^{all}=\sum_{i}\mathbf{a}^{all}_{i}\mathbf{v}_{i}</annotation></semantics></math>. The intuition is that <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{v}^{pt}" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><msup id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml"><mi id="S4.SS2.p2.6.m6.1.1.3.2" xref="S4.SS2.p2.6.m6.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.6.m6.1.1.3.1" xref="S4.SS2.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.6.m6.1.1.3.3" xref="S4.SS2.p2.6.m6.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">superscript</csymbol><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">𝐯</ci><apply id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3"><times id="S4.SS2.p2.6.m6.1.1.3.1.cmml" xref="S4.SS2.p2.6.m6.1.1.3.1"></times><ci id="S4.SS2.p2.6.m6.1.1.3.2.cmml" xref="S4.SS2.p2.6.m6.1.1.3.2">𝑝</ci><ci id="S4.SS2.p2.6.m6.1.1.3.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">\mathbf{v}^{pt}</annotation></semantics></math> extracts local information relevant to the question, which is then used to attend to relevant regions in the whole image. Given these three streams of information, <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mi id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><ci id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">\mathbf{q}</annotation></semantics></math>, <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="\mathbf{v}^{pt}" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><msup id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml"><mi id="S4.SS2.p2.8.m8.1.1.2" xref="S4.SS2.p2.8.m8.1.1.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.8.m8.1.1.3" xref="S4.SS2.p2.8.m8.1.1.3.cmml"><mi id="S4.SS2.p2.8.m8.1.1.3.2" xref="S4.SS2.p2.8.m8.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.8.m8.1.1.3.1" xref="S4.SS2.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.8.m8.1.1.3.3" xref="S4.SS2.p2.8.m8.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m8.1.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">superscript</csymbol><ci id="S4.SS2.p2.8.m8.1.1.2.cmml" xref="S4.SS2.p2.8.m8.1.1.2">𝐯</ci><apply id="S4.SS2.p2.8.m8.1.1.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3"><times id="S4.SS2.p2.8.m8.1.1.3.1.cmml" xref="S4.SS2.p2.8.m8.1.1.3.1"></times><ci id="S4.SS2.p2.8.m8.1.1.3.2.cmml" xref="S4.SS2.p2.8.m8.1.1.3.2">𝑝</ci><ci id="S4.SS2.p2.8.m8.1.1.3.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">\mathbf{v}^{pt}</annotation></semantics></math> and <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="\mathbf{v}^{all}" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><msup id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml"><mi id="S4.SS2.p2.9.m9.1.1.2" xref="S4.SS2.p2.9.m9.1.1.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.9.m9.1.1.3" xref="S4.SS2.p2.9.m9.1.1.3.cmml"><mi id="S4.SS2.p2.9.m9.1.1.3.2" xref="S4.SS2.p2.9.m9.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.9.m9.1.1.3.1" xref="S4.SS2.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.9.m9.1.1.3.3" xref="S4.SS2.p2.9.m9.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.9.m9.1.1.3.1a" xref="S4.SS2.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.9.m9.1.1.3.4" xref="S4.SS2.p2.9.m9.1.1.3.4.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><apply id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.9.m9.1.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1">superscript</csymbol><ci id="S4.SS2.p2.9.m9.1.1.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2">𝐯</ci><apply id="S4.SS2.p2.9.m9.1.1.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3"><times id="S4.SS2.p2.9.m9.1.1.3.1.cmml" xref="S4.SS2.p2.9.m9.1.1.3.1"></times><ci id="S4.SS2.p2.9.m9.1.1.3.2.cmml" xref="S4.SS2.p2.9.m9.1.1.3.2">𝑎</ci><ci id="S4.SS2.p2.9.m9.1.1.3.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3.3">𝑙</ci><ci id="S4.SS2.p2.9.m9.1.1.3.4.cmml" xref="S4.SS2.p2.9.m9.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">\mathbf{v}^{all}</annotation></semantics></math>, we combine them through pairwise element-wise multiplication and concatenation (multiply <math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><mi id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><ci id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">\mathbf{q}</annotation></semantics></math> and <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="\mathbf{v}^{pt}" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><msup id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mi id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml"><mi id="S4.SS2.p2.11.m11.1.1.3.2" xref="S4.SS2.p2.11.m11.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.11.m11.1.1.3.1" xref="S4.SS2.p2.11.m11.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.11.m11.1.1.3.3" xref="S4.SS2.p2.11.m11.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1">superscript</csymbol><ci id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">𝐯</ci><apply id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3"><times id="S4.SS2.p2.11.m11.1.1.3.1.cmml" xref="S4.SS2.p2.11.m11.1.1.3.1"></times><ci id="S4.SS2.p2.11.m11.1.1.3.2.cmml" xref="S4.SS2.p2.11.m11.1.1.3.2">𝑝</ci><ci id="S4.SS2.p2.11.m11.1.1.3.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">\mathbf{v}^{pt}</annotation></semantics></math>, and <math id="S4.SS2.p2.12.m12.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS2.p2.12.m12.1a"><mi id="S4.SS2.p2.12.m12.1.1" xref="S4.SS2.p2.12.m12.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.12.m12.1b"><ci id="S4.SS2.p2.12.m12.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.12.m12.1c">\mathbf{q}</annotation></semantics></math> and <math id="S4.SS2.p2.13.m13.1" class="ltx_Math" alttext="\mathbf{v}^{img}" display="inline"><semantics id="S4.SS2.p2.13.m13.1a"><msup id="S4.SS2.p2.13.m13.1.1" xref="S4.SS2.p2.13.m13.1.1.cmml"><mi id="S4.SS2.p2.13.m13.1.1.2" xref="S4.SS2.p2.13.m13.1.1.2.cmml">𝐯</mi><mrow id="S4.SS2.p2.13.m13.1.1.3" xref="S4.SS2.p2.13.m13.1.1.3.cmml"><mi id="S4.SS2.p2.13.m13.1.1.3.2" xref="S4.SS2.p2.13.m13.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m13.1.1.3.1" xref="S4.SS2.p2.13.m13.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.13.m13.1.1.3.3" xref="S4.SS2.p2.13.m13.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m13.1.1.3.1a" xref="S4.SS2.p2.13.m13.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.13.m13.1.1.3.4" xref="S4.SS2.p2.13.m13.1.1.3.4.cmml">g</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.13.m13.1b"><apply id="S4.SS2.p2.13.m13.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.13.m13.1.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1">superscript</csymbol><ci id="S4.SS2.p2.13.m13.1.1.2.cmml" xref="S4.SS2.p2.13.m13.1.1.2">𝐯</ci><apply id="S4.SS2.p2.13.m13.1.1.3.cmml" xref="S4.SS2.p2.13.m13.1.1.3"><times id="S4.SS2.p2.13.m13.1.1.3.1.cmml" xref="S4.SS2.p2.13.m13.1.1.3.1"></times><ci id="S4.SS2.p2.13.m13.1.1.3.2.cmml" xref="S4.SS2.p2.13.m13.1.1.3.2">𝑖</ci><ci id="S4.SS2.p2.13.m13.1.1.3.3.cmml" xref="S4.SS2.p2.13.m13.1.1.3.3">𝑚</ci><ci id="S4.SS2.p2.13.m13.1.1.3.4.cmml" xref="S4.SS2.p2.13.m13.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.13.m13.1c">\mathbf{v}^{img}</annotation></semantics></math>, and concatenate the two results) followed by fully connected layers and a softmax output (step (g) in Sec. <a href="#S3.SS2" title="3.2 PointQA-Local models ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> evaluation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We benchmark the new global model trained on the <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">train</em> set of <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> using the setup from Sec. <a href="#S3.SS3" title="3.3 PointQA-Local evaluation ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Test accuracy.</span> When trained to answer the question “How many of <em id="S4.SS3.p2.1.2" class="ltx_emph ltx_font_italic">these</em> are there?” along with a disambiguating point input, the model achieves an accuracy of 56.5%, significantly higher than an image-only version (without the point) at 46.1% and the modal answer (“1”) baseline at 35.4%. As expected it is somewhat behind having access to the object’s bounding box at 60.2%. In Table <a href="#S4.T2" title="Table 2 ‣ 4.3 PointQA-LookTwice evaluation ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we demonstrate how the accuracy changes with decreasing the ambiguity of the question: specifying the supercategory (being, vehicle or object) boosts the accuracy from 56.5% to 59.1%, and naming the object class further boosts it to 62.8% (in fact, as expected making the need for spatial supervision irrelevant, as the image-only model achieves 62.7% in this setting).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3">Spatial Disambiguation</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S4.T2.2.1.1.3.1" class="ltx_text">Prior</span></th>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">None</th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.2.2.2.3.1" class="ltx_text ltx_font_bold">Point</span></th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Box</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">How many of <em id="S4.T2.2.3.1.1.1" class="ltx_emph ltx_font_italic">these</em>…</th>
<td id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">46.1</td>
<td id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">56.5</td>
<td id="S4.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.2</td>
<td id="S4.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">37.8</td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S4.T2.2.4.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.4.2.1.1.1" class="ltx_tr">
<td id="S4.T2.2.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">How many of <em id="S4.T2.2.4.2.1.1.1.1.1" class="ltx_emph ltx_font_italic">these</em>
</td>
</tr>
<tr id="S4.T2.2.4.2.1.1.2" class="ltx_tr">
<td id="S4.T2.2.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">[supercategory]…</td>
</tr>
</table>
</th>
<td id="S4.T2.2.4.2.2" class="ltx_td ltx_align_center">53.1</td>
<td id="S4.T2.2.4.2.3" class="ltx_td ltx_align_center">59.l</td>
<td id="S4.T2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r">59.8</td>
<td id="S4.T2.2.4.2.5" class="ltx_td ltx_align_center">38.6</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">How many [object]…</th>
<td id="S4.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_b">62.7</td>
<td id="S4.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_b">62.8</td>
<td id="S4.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">61.4</td>
<td id="S4.T2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_b">40.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.6.2" class="ltx_text" style="font-size:90%;">Results of the Pythia-based global model (Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) on the <span id="S4.T2.6.2.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> <em id="S4.T2.6.2.2" class="ltx_emph ltx_font_italic">test</em>. Rows are the question asked; columns are the disambiguation provided. Prior is a language-only model.</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2011.13681/assets/fig8keynote4.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.5.2" class="ltx_text" style="font-size:90%;">Attention of the Pythia-based model on a question in the <span id="S4.F6.5.2.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> <em id="S4.F6.5.2.2" class="ltx_emph ltx_font_italic">test</em> set. Point is in red.</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Attention analysis.</span> We examine the model trained on supercategory questions. There are two types of attention: attention around the point is relatively peaked, with an average max attention of 0.42; as expected, global attention on the image is diffuse with an average max attention of 0.07. An example result is shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.3 PointQA-LookTwice evaluation ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Global vs local-only attention.</span> We demonstrate the need for the new global attention added in Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> by comparing with the previous model which only considers the local regions around the point. The local-only model achieves an accuracy of 55.9% on supercategory questions, significantly lower than the global model at 59.1%. As expected, this is primarily due to under-counting (since the local-only model is not attending to other object instances).
Concretely, on questions with the answer “1”, the global model performs slightly better than the local-only model (75.64% vs. 74.12%); however the difference is greater for answer <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mo id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><geq id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">\geq</annotation></semantics></math>2, where the global model achieves 50.04% and local model only 45.84%. Finally, as a sanity-check, we verified that the models achieve equal accuracy on the simpler <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_bold">PointQA-Local</span> dataset, indicating that the global model is well-suited for a wider range of pointing-based questions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>PointQA-General: generalized reasoning from a point input</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Equipped with the insights of Sec. <a href="#S3" title="3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4" title="4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where we examined well-structured pointing questions, we now turn our attention to the unconstrained setting where the questions become much more complex and the models must reason about image, textual, and spatial input in full generality. We construct a new dataset, modify state-of-the-art transformer-based models, and perform analysis in this new setting to round out our exploration.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We generate the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> dataset by adapting the human-written questions from Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.2" class="ltx_p"><span id="S5.SS1.p2.2.1" class="ltx_text ltx_font_bold">Question Generation.</span> “Which” questions in Visual7W are paired with bounding boxes as multiple choice answers; for example, they might ask “Which pillow is closest to the window?”, where one of the four provided boxes is correct. We transform these questions into pointing-QA using the following formula (where <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">X</annotation></semantics></math> is the subject and <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">Y</annotation></semantics></math> is a description):</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.4" class="ltx_p">“Which <math id="S5.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i1.p1.1.m1.1a"><mi id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><ci id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">X</annotation></semantics></math> is <math id="S5.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i1.p1.2.m2.1a"><mi id="S5.I1.i1.p1.2.m2.1.1" xref="S5.I1.i1.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.2.m2.1b"><ci id="S5.I1.i1.p1.2.m2.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.2.m2.1c">Y</annotation></semantics></math>” becomes “Is this <math id="S5.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i1.p1.3.m3.1a"><mi id="S5.I1.i1.p1.3.m3.1.1" xref="S5.I1.i1.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.3.m3.1b"><ci id="S5.I1.i1.p1.3.m3.1.1.cmml" xref="S5.I1.i1.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.3.m3.1c">X</annotation></semantics></math> <math id="S5.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i1.p1.4.m4.1a"><mi id="S5.I1.i1.p1.4.m4.1.1" xref="S5.I1.i1.p1.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.4.m4.1b"><ci id="S5.I1.i1.p1.4.m4.1.1.cmml" xref="S5.I1.i1.p1.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.4.m4.1c">Y</annotation></semantics></math>?”</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.4" class="ltx_p">“Which <math id="S5.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i2.p1.1.m1.1a"><mi id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><ci id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">X</annotation></semantics></math> are <math id="S5.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i2.p1.2.m2.1a"><mi id="S5.I1.i2.p1.2.m2.1.1" xref="S5.I1.i2.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.2.m2.1b"><ci id="S5.I1.i2.p1.2.m2.1.1.cmml" xref="S5.I1.i2.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.2.m2.1c">Y</annotation></semantics></math>” becomes “Are these <math id="S5.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i2.p1.3.m3.1a"><mi id="S5.I1.i2.p1.3.m3.1.1" xref="S5.I1.i2.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.3.m3.1b"><ci id="S5.I1.i2.p1.3.m3.1.1.cmml" xref="S5.I1.i2.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.3.m3.1c">X</annotation></semantics></math> <math id="S5.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i2.p1.4.m4.1a"><mi id="S5.I1.i2.p1.4.m4.1.1" xref="S5.I1.i2.p1.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.4.m4.1b"><ci id="S5.I1.i2.p1.4.m4.1.1.cmml" xref="S5.I1.i2.p1.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.4.m4.1c">Y</annotation></semantics></math>?”</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.4" class="ltx_p">“Which <math id="S5.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i3.p1.1.m1.1a"><mi id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><ci id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">X</annotation></semantics></math> has <math id="S5.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i3.p1.2.m2.1a"><mi id="S5.I1.i3.p1.2.m2.1.1" xref="S5.I1.i3.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.2.m2.1b"><ci id="S5.I1.i3.p1.2.m2.1.1.cmml" xref="S5.I1.i3.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.2.m2.1c">Y</annotation></semantics></math>” becomes “Does this <math id="S5.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i3.p1.3.m3.1a"><mi id="S5.I1.i3.p1.3.m3.1.1" xref="S5.I1.i3.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.3.m3.1b"><ci id="S5.I1.i3.p1.3.m3.1.1.cmml" xref="S5.I1.i3.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.3.m3.1c">X</annotation></semantics></math> have <math id="S5.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i3.p1.4.m4.1a"><mi id="S5.I1.i3.p1.4.m4.1.1" xref="S5.I1.i3.p1.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.4.m4.1b"><ci id="S5.I1.i3.p1.4.m4.1.1.cmml" xref="S5.I1.i3.p1.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.4.m4.1c">Y</annotation></semantics></math>?”</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.4" class="ltx_p">“Which <math id="S5.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i4.p1.1.m1.1a"><mi id="S5.I1.i4.p1.1.m1.1.1" xref="S5.I1.i4.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.1.m1.1b"><ci id="S5.I1.i4.p1.1.m1.1.1.cmml" xref="S5.I1.i4.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.1.m1.1c">X</annotation></semantics></math> have <math id="S5.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i4.p1.2.m2.1a"><mi id="S5.I1.i4.p1.2.m2.1.1" xref="S5.I1.i4.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.2.m2.1b"><ci id="S5.I1.i4.p1.2.m2.1.1.cmml" xref="S5.I1.i4.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.2.m2.1c">Y</annotation></semantics></math>” becomes “Do these <math id="S5.I1.i4.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.I1.i4.p1.3.m3.1a"><mi id="S5.I1.i4.p1.3.m3.1.1" xref="S5.I1.i4.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.3.m3.1b"><ci id="S5.I1.i4.p1.3.m3.1.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.3.m3.1c">X</annotation></semantics></math> have <math id="S5.I1.i4.p1.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S5.I1.i4.p1.4.m4.1a"><mi id="S5.I1.i4.p1.4.m4.1.1" xref="S5.I1.i4.p1.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.4.m4.1b"><ci id="S5.I1.i4.p1.4.m4.1.1.cmml" xref="S5.I1.i4.p1.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.4.m4.1c">Y</annotation></semantics></math>?”</p>
</div>
</li>
</ul>
<p id="S5.SS1.p2.3" class="ltx_p">A high proportion of human-written questions follow one of these templates; questions that do not are not included. From each included question in Visual7W we generate two pointing questions: (1) using the correct bounding box, with the answer “yes” and (2) using one of the three incorrect boxes selected randomly with the answer “no”. The point input is simulated as the center of the bounding box. Example questions are shown in Fig. <a href="#S5.F7" title="Figure 7 ‣ 5.1 PointQA-General dataset ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2011.13681/assets/Picture3.png" id="S5.F7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.4.2" class="ltx_text" style="font-size:90%;">Examples entries in the <span id="S5.F7.4.2.1" class="ltx_text ltx_font_bold">PointQA-General</span> Dataset. Point is indicated in red.</span></figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Statistics.</span> The final dataset consists of 319,300 questions over 25,420 images, with the “yes” and “no” answers equally balanced. The dataset is divided into (1) <em id="S5.SS1.p3.1.2" class="ltx_emph ltx_font_italic">train</em> (255,072 questions across 20,337 images), (2) <em id="S5.SS1.p3.1.3" class="ltx_emph ltx_font_italic">val</em> (32,276 questions across 2,541 images), and (3) <em id="S5.SS1.p3.1.4" class="ltx_emph ltx_font_italic">test</em> (31,952 questions across 2,542 images). The most common subjects asked about are “part” (e.g. “Is <em id="S5.SS1.p3.1.5" class="ltx_emph ltx_font_italic">this</em> part of the desktop computer touching the carpet?”; 6.2% of the questions), “object” (4.4%), “person” (4.3%), “item” (2.7%), “man” (1.6%), “tree” (1.3%), “animal” (1.2%), and “food” (1.0%). The reference descriptions are quite complex: they contain 6 words on average (<math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="95\%" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">95</mn><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">95\%</annotation></semantics></math> fall between 3 and 12 words) compared to the much simpler attributes of <span id="S5.SS1.p3.1.6" class="ltx_text ltx_font_bold">PointQA-Local</span> which are typically only 1 word long.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Human accuracy.</span> As for the previous two datasets we ran a small-scale human study on 100 <em id="S5.SS1.p4.1.2" class="ltx_emph ltx_font_italic">test</em> questions. Please see Appendix <a href="#A1.SS1" title="A.1 PointQA-General ‣ Appendix A Human Evaluations ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for details on human evaluation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> models</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Given the increased complexity of the questions in <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> we adapt two recent transformer-based VQA models to our pointing-based VQA task, MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> (in addition to the Pythia-based model of Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). At a high-level MCAN computes a unidirectional attention like Pythia where the text influences attention over the image; by contrast in the bidirectional approach of LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> each modality influences attention over the other (Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>a). We now describe the model details.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2011.13681/assets/quesimg18.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="45" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2011.13681/assets/quesimgpoint.png" id="S5.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="110" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2011.13681/assets/threewayatt.png" id="S5.F8.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="103" height="116" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">Left shows the different attention dependency structures between image and text; middle shows when the “point stream” is added (Sec. <a href="#S5.SS2" title="5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>). Blue arrows indicate the bidirectional structure and black arrows unidirectional. Right shows the exact way in which cross-attention is implemented for transformer models with three streams.</span></figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.14" class="ltx_p"><span id="S5.SS2.p2.14.1" class="ltx_text ltx_font_bold">MCAN-based.</span> MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is the winner of the the 2019 VQA Challenge. It works as follows: (1) The question <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\mathbf{q}</annotation></semantics></math> is broken into tokens <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="\{q_{j}\}" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.2.m2.1.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.2.m2.1.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.1.1.2" xref="S5.SS2.p2.2.m2.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p2.2.m2.1.1.1.1.3" xref="S5.SS2.p2.2.m2.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.2.m2.1.1.1.3" xref="S5.SS2.p2.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><set id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.1"><apply id="S5.SS2.p2.2.m2.1.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">\{q_{j}\}</annotation></semantics></math> and is encoded by stacking several self-attention layers <math id="S5.SS2.p2.3.m3.4" class="ltx_Math" alttext="\{q_{j}\}_{\ell+1}=A(Q_{\ell},K_{\ell},V_{\ell})" display="inline"><semantics id="S5.SS2.p2.3.m3.4a"><mrow id="S5.SS2.p2.3.m3.4.4" xref="S5.SS2.p2.3.m3.4.4.cmml"><msub id="S5.SS2.p2.3.m3.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.cmml"><mrow id="S5.SS2.p2.3.m3.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.3.m3.1.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.3.m3.1.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p2.3.m3.1.1.1.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.3.m3.1.1.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S5.SS2.p2.3.m3.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.3.cmml"><mi mathvariant="normal" id="S5.SS2.p2.3.m3.1.1.1.3.2" xref="S5.SS2.p2.3.m3.1.1.1.3.2.cmml">ℓ</mi><mo id="S5.SS2.p2.3.m3.1.1.1.3.1" xref="S5.SS2.p2.3.m3.1.1.1.3.1.cmml">+</mo><mn id="S5.SS2.p2.3.m3.1.1.1.3.3" xref="S5.SS2.p2.3.m3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S5.SS2.p2.3.m3.4.4.5" xref="S5.SS2.p2.3.m3.4.4.5.cmml">=</mo><mrow id="S5.SS2.p2.3.m3.4.4.4" xref="S5.SS2.p2.3.m3.4.4.4.cmml"><mi id="S5.SS2.p2.3.m3.4.4.4.5" xref="S5.SS2.p2.3.m3.4.4.4.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.3.m3.4.4.4.4" xref="S5.SS2.p2.3.m3.4.4.4.4.cmml">​</mo><mrow id="S5.SS2.p2.3.m3.4.4.4.3.3" xref="S5.SS2.p2.3.m3.4.4.4.3.4.cmml"><mo stretchy="false" id="S5.SS2.p2.3.m3.4.4.4.3.3.4" xref="S5.SS2.p2.3.m3.4.4.4.3.4.cmml">(</mo><msub id="S5.SS2.p2.3.m3.2.2.2.1.1.1" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1.cmml"><mi id="S5.SS2.p2.3.m3.2.2.2.1.1.1.2" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1.2.cmml">Q</mi><mi mathvariant="normal" id="S5.SS2.p2.3.m3.2.2.2.1.1.1.3" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p2.3.m3.4.4.4.3.3.5" xref="S5.SS2.p2.3.m3.4.4.4.3.4.cmml">,</mo><msub id="S5.SS2.p2.3.m3.3.3.3.2.2.2" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2.cmml"><mi id="S5.SS2.p2.3.m3.3.3.3.2.2.2.2" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2.2.cmml">K</mi><mi mathvariant="normal" id="S5.SS2.p2.3.m3.3.3.3.2.2.2.3" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p2.3.m3.4.4.4.3.3.6" xref="S5.SS2.p2.3.m3.4.4.4.3.4.cmml">,</mo><msub id="S5.SS2.p2.3.m3.4.4.4.3.3.3" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3.cmml"><mi id="S5.SS2.p2.3.m3.4.4.4.3.3.3.2" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3.2.cmml">V</mi><mi mathvariant="normal" id="S5.SS2.p2.3.m3.4.4.4.3.3.3.3" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3.3.cmml">ℓ</mi></msub><mo stretchy="false" id="S5.SS2.p2.3.m3.4.4.4.3.3.7" xref="S5.SS2.p2.3.m3.4.4.4.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.4b"><apply id="S5.SS2.p2.3.m3.4.4.cmml" xref="S5.SS2.p2.3.m3.4.4"><eq id="S5.SS2.p2.3.m3.4.4.5.cmml" xref="S5.SS2.p2.3.m3.4.4.5"></eq><apply id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1">subscript</csymbol><set id="S5.SS2.p2.3.m3.1.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1"><apply id="S5.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.3">𝑗</ci></apply></set><apply id="S5.SS2.p2.3.m3.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.3"><plus id="S5.SS2.p2.3.m3.1.1.1.3.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.3.1"></plus><ci id="S5.SS2.p2.3.m3.1.1.1.3.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.3.2">ℓ</ci><cn type="integer" id="S5.SS2.p2.3.m3.1.1.1.3.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.3.3">1</cn></apply></apply><apply id="S5.SS2.p2.3.m3.4.4.4.cmml" xref="S5.SS2.p2.3.m3.4.4.4"><times id="S5.SS2.p2.3.m3.4.4.4.4.cmml" xref="S5.SS2.p2.3.m3.4.4.4.4"></times><ci id="S5.SS2.p2.3.m3.4.4.4.5.cmml" xref="S5.SS2.p2.3.m3.4.4.4.5">𝐴</ci><vector id="S5.SS2.p2.3.m3.4.4.4.3.4.cmml" xref="S5.SS2.p2.3.m3.4.4.4.3.3"><apply id="S5.SS2.p2.3.m3.2.2.2.1.1.1.cmml" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.2.2.2.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.3.m3.2.2.2.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1.2">𝑄</ci><ci id="S5.SS2.p2.3.m3.2.2.2.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.2.2.2.1.1.1.3">ℓ</ci></apply><apply id="S5.SS2.p2.3.m3.3.3.3.2.2.2.cmml" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.3.3.3.2.2.2.1.cmml" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2">subscript</csymbol><ci id="S5.SS2.p2.3.m3.3.3.3.2.2.2.2.cmml" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2.2">𝐾</ci><ci id="S5.SS2.p2.3.m3.3.3.3.2.2.2.3.cmml" xref="S5.SS2.p2.3.m3.3.3.3.2.2.2.3">ℓ</ci></apply><apply id="S5.SS2.p2.3.m3.4.4.4.3.3.3.cmml" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.4.4.4.3.3.3.1.cmml" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3">subscript</csymbol><ci id="S5.SS2.p2.3.m3.4.4.4.3.3.3.2.cmml" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3.2">𝑉</ci><ci id="S5.SS2.p2.3.m3.4.4.4.3.3.3.3.cmml" xref="S5.SS2.p2.3.m3.4.4.4.3.3.3.3">ℓ</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.4c">\{q_{j}\}_{\ell+1}=A(Q_{\ell},K_{\ell},V_{\ell})</annotation></semantics></math>, where <math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">A</annotation></semantics></math> is the self-attention mechanism and <math id="S5.SS2.p2.5.m5.3" class="ltx_Math" alttext="(Q_{\ell},K_{\ell},V_{\ell})" display="inline"><semantics id="S5.SS2.p2.5.m5.3a"><mrow id="S5.SS2.p2.5.m5.3.3.3" xref="S5.SS2.p2.5.m5.3.3.4.cmml"><mo stretchy="false" id="S5.SS2.p2.5.m5.3.3.3.4" xref="S5.SS2.p2.5.m5.3.3.4.cmml">(</mo><msub id="S5.SS2.p2.5.m5.1.1.1.1" xref="S5.SS2.p2.5.m5.1.1.1.1.cmml"><mi id="S5.SS2.p2.5.m5.1.1.1.1.2" xref="S5.SS2.p2.5.m5.1.1.1.1.2.cmml">Q</mi><mi mathvariant="normal" id="S5.SS2.p2.5.m5.1.1.1.1.3" xref="S5.SS2.p2.5.m5.1.1.1.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p2.5.m5.3.3.3.5" xref="S5.SS2.p2.5.m5.3.3.4.cmml">,</mo><msub id="S5.SS2.p2.5.m5.2.2.2.2" xref="S5.SS2.p2.5.m5.2.2.2.2.cmml"><mi id="S5.SS2.p2.5.m5.2.2.2.2.2" xref="S5.SS2.p2.5.m5.2.2.2.2.2.cmml">K</mi><mi mathvariant="normal" id="S5.SS2.p2.5.m5.2.2.2.2.3" xref="S5.SS2.p2.5.m5.2.2.2.2.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p2.5.m5.3.3.3.6" xref="S5.SS2.p2.5.m5.3.3.4.cmml">,</mo><msub id="S5.SS2.p2.5.m5.3.3.3.3" xref="S5.SS2.p2.5.m5.3.3.3.3.cmml"><mi id="S5.SS2.p2.5.m5.3.3.3.3.2" xref="S5.SS2.p2.5.m5.3.3.3.3.2.cmml">V</mi><mi mathvariant="normal" id="S5.SS2.p2.5.m5.3.3.3.3.3" xref="S5.SS2.p2.5.m5.3.3.3.3.3.cmml">ℓ</mi></msub><mo stretchy="false" id="S5.SS2.p2.5.m5.3.3.3.7" xref="S5.SS2.p2.5.m5.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.3b"><vector id="S5.SS2.p2.5.m5.3.3.4.cmml" xref="S5.SS2.p2.5.m5.3.3.3"><apply id="S5.SS2.p2.5.m5.1.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.1.1.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.5.m5.1.1.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.1.1.2">𝑄</ci><ci id="S5.SS2.p2.5.m5.1.1.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.1.1.3">ℓ</ci></apply><apply id="S5.SS2.p2.5.m5.2.2.2.2.cmml" xref="S5.SS2.p2.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.2.2.2.2.1.cmml" xref="S5.SS2.p2.5.m5.2.2.2.2">subscript</csymbol><ci id="S5.SS2.p2.5.m5.2.2.2.2.2.cmml" xref="S5.SS2.p2.5.m5.2.2.2.2.2">𝐾</ci><ci id="S5.SS2.p2.5.m5.2.2.2.2.3.cmml" xref="S5.SS2.p2.5.m5.2.2.2.2.3">ℓ</ci></apply><apply id="S5.SS2.p2.5.m5.3.3.3.3.cmml" xref="S5.SS2.p2.5.m5.3.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.3.3.3.3.1.cmml" xref="S5.SS2.p2.5.m5.3.3.3.3">subscript</csymbol><ci id="S5.SS2.p2.5.m5.3.3.3.3.2.cmml" xref="S5.SS2.p2.5.m5.3.3.3.3.2">𝑉</ci><ci id="S5.SS2.p2.5.m5.3.3.3.3.3.cmml" xref="S5.SS2.p2.5.m5.3.3.3.3.3">ℓ</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.3c">(Q_{\ell},K_{\ell},V_{\ell})</annotation></semantics></math> are the queries, keys, and values derived from <math id="S5.SS2.p2.6.m6.1" class="ltx_Math" alttext="\{q_{j}\}_{\ell}" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><msub id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml"><mrow id="S5.SS2.p2.6.m6.1.1.1.1" xref="S5.SS2.p2.6.m6.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.6.m6.1.1.1.1.2" xref="S5.SS2.p2.6.m6.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.6.m6.1.1.1.1.1" xref="S5.SS2.p2.6.m6.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.6.m6.1.1.1.1.1.2" xref="S5.SS2.p2.6.m6.1.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p2.6.m6.1.1.1.1.1.3" xref="S5.SS2.p2.6.m6.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.6.m6.1.1.1.1.3" xref="S5.SS2.p2.6.m6.1.1.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p2.6.m6.1.1.3" xref="S5.SS2.p2.6.m6.1.1.3.cmml">ℓ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><apply id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1">subscript</csymbol><set id="S5.SS2.p2.6.m6.1.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.1.1"><apply id="S5.SS2.p2.6.m6.1.1.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.6.m6.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p2.6.m6.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p2.6.m6.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3">ℓ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">\{q_{j}\}_{\ell}</annotation></semantics></math> at layer <math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><mi mathvariant="normal" id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><ci id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">\ell</annotation></semantics></math>. (2) The image representation over visual features derived from region proposals <math id="S5.SS2.p2.8.m8.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}" display="inline"><semantics id="S5.SS2.p2.8.m8.1a"><mrow id="S5.SS2.p2.8.m8.1.1.1" xref="S5.SS2.p2.8.m8.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.8.m8.1.1.1.2" xref="S5.SS2.p2.8.m8.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.8.m8.1.1.1.1" xref="S5.SS2.p2.8.m8.1.1.1.1.cmml"><mi id="S5.SS2.p2.8.m8.1.1.1.1.2" xref="S5.SS2.p2.8.m8.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.SS2.p2.8.m8.1.1.1.1.3" xref="S5.SS2.p2.8.m8.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.8.m8.1.1.1.3" xref="S5.SS2.p2.8.m8.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><set id="S5.SS2.p2.8.m8.1.1.2.cmml" xref="S5.SS2.p2.8.m8.1.1.1"><apply id="S5.SS2.p2.8.m8.1.1.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.8.m8.1.1.1.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.8.m8.1.1.1.1.2.cmml" xref="S5.SS2.p2.8.m8.1.1.1.1.2">𝐯</ci><ci id="S5.SS2.p2.8.m8.1.1.1.1.3.cmml" xref="S5.SS2.p2.8.m8.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">\{\mathbf{v}_{j}\}</annotation></semantics></math> is computed using a stack of self-attention and cross-attention layers. In each cross-attention layer, the keys and values are derived from the question representation at final layer <math id="S5.SS2.p2.9.m9.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S5.SS2.p2.9.m9.1a"><mi id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.1b"><ci id="S5.SS2.p2.9.m9.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.1c">L</annotation></semantics></math>; in other words, <math id="S5.SS2.p2.10.m10.4" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}_{\ell+1}=A(Q_{\ell},W_{K}\{q_{j}\}_{L},W_{V}\{q_{j}\}_{L})" display="inline"><semantics id="S5.SS2.p2.10.m10.4a"><mrow id="S5.SS2.p2.10.m10.4.4" xref="S5.SS2.p2.10.m10.4.4.cmml"><msub id="S5.SS2.p2.10.m10.1.1.1" xref="S5.SS2.p2.10.m10.1.1.1.cmml"><mrow id="S5.SS2.p2.10.m10.1.1.1.1.1" xref="S5.SS2.p2.10.m10.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.10.m10.1.1.1.1.1.2" xref="S5.SS2.p2.10.m10.1.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.10.m10.1.1.1.1.1.1" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.10.m10.1.1.1.1.1.1.2" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.SS2.p2.10.m10.1.1.1.1.1.1.3" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.10.m10.1.1.1.1.1.3" xref="S5.SS2.p2.10.m10.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S5.SS2.p2.10.m10.1.1.1.3" xref="S5.SS2.p2.10.m10.1.1.1.3.cmml"><mi mathvariant="normal" id="S5.SS2.p2.10.m10.1.1.1.3.2" xref="S5.SS2.p2.10.m10.1.1.1.3.2.cmml">ℓ</mi><mo id="S5.SS2.p2.10.m10.1.1.1.3.1" xref="S5.SS2.p2.10.m10.1.1.1.3.1.cmml">+</mo><mn id="S5.SS2.p2.10.m10.1.1.1.3.3" xref="S5.SS2.p2.10.m10.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S5.SS2.p2.10.m10.4.4.5" xref="S5.SS2.p2.10.m10.4.4.5.cmml">=</mo><mrow id="S5.SS2.p2.10.m10.4.4.4" xref="S5.SS2.p2.10.m10.4.4.4.cmml"><mi id="S5.SS2.p2.10.m10.4.4.4.5" xref="S5.SS2.p2.10.m10.4.4.4.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.10.m10.4.4.4.4" xref="S5.SS2.p2.10.m10.4.4.4.4.cmml">​</mo><mrow id="S5.SS2.p2.10.m10.4.4.4.3.3" xref="S5.SS2.p2.10.m10.4.4.4.3.4.cmml"><mo stretchy="false" id="S5.SS2.p2.10.m10.4.4.4.3.3.4" xref="S5.SS2.p2.10.m10.4.4.4.3.4.cmml">(</mo><msub id="S5.SS2.p2.10.m10.2.2.2.1.1.1" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1.cmml"><mi id="S5.SS2.p2.10.m10.2.2.2.1.1.1.2" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1.2.cmml">Q</mi><mi mathvariant="normal" id="S5.SS2.p2.10.m10.2.2.2.1.1.1.3" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p2.10.m10.4.4.4.3.3.5" xref="S5.SS2.p2.10.m10.4.4.4.3.4.cmml">,</mo><mrow id="S5.SS2.p2.10.m10.3.3.3.2.2.2" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.cmml"><msub id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.cmml"><mi id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.2" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.2.cmml">W</mi><mi id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.3" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.3.cmml">K</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.10.m10.3.3.3.2.2.2.2" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.2.cmml">​</mo><msub id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.cmml"><mrow id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.2" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.cmml"><mi id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.2" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.3" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.3" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.3" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.3.cmml">L</mi></msub></mrow><mo id="S5.SS2.p2.10.m10.4.4.4.3.3.6" xref="S5.SS2.p2.10.m10.4.4.4.3.4.cmml">,</mo><mrow id="S5.SS2.p2.10.m10.4.4.4.3.3.3" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.cmml"><msub id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.cmml"><mi id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.2" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.2.cmml">W</mi><mi id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.3" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.3.cmml">V</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.10.m10.4.4.4.3.3.3.2" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.2.cmml">​</mo><msub id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.cmml"><mrow id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.2" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.cmml"><mi id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.2" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.3" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.3" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.3" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.3.cmml">L</mi></msub></mrow><mo stretchy="false" id="S5.SS2.p2.10.m10.4.4.4.3.3.7" xref="S5.SS2.p2.10.m10.4.4.4.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.10.m10.4b"><apply id="S5.SS2.p2.10.m10.4.4.cmml" xref="S5.SS2.p2.10.m10.4.4"><eq id="S5.SS2.p2.10.m10.4.4.5.cmml" xref="S5.SS2.p2.10.m10.4.4.5"></eq><apply id="S5.SS2.p2.10.m10.1.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.1.1.1.2.cmml" xref="S5.SS2.p2.10.m10.1.1.1">subscript</csymbol><set id="S5.SS2.p2.10.m10.1.1.1.1.2.cmml" xref="S5.SS2.p2.10.m10.1.1.1.1.1"><apply id="S5.SS2.p2.10.m10.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.10.m10.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1.2">𝐯</ci><ci id="S5.SS2.p2.10.m10.1.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.10.m10.1.1.1.1.1.1.3">𝑗</ci></apply></set><apply id="S5.SS2.p2.10.m10.1.1.1.3.cmml" xref="S5.SS2.p2.10.m10.1.1.1.3"><plus id="S5.SS2.p2.10.m10.1.1.1.3.1.cmml" xref="S5.SS2.p2.10.m10.1.1.1.3.1"></plus><ci id="S5.SS2.p2.10.m10.1.1.1.3.2.cmml" xref="S5.SS2.p2.10.m10.1.1.1.3.2">ℓ</ci><cn type="integer" id="S5.SS2.p2.10.m10.1.1.1.3.3.cmml" xref="S5.SS2.p2.10.m10.1.1.1.3.3">1</cn></apply></apply><apply id="S5.SS2.p2.10.m10.4.4.4.cmml" xref="S5.SS2.p2.10.m10.4.4.4"><times id="S5.SS2.p2.10.m10.4.4.4.4.cmml" xref="S5.SS2.p2.10.m10.4.4.4.4"></times><ci id="S5.SS2.p2.10.m10.4.4.4.5.cmml" xref="S5.SS2.p2.10.m10.4.4.4.5">𝐴</ci><vector id="S5.SS2.p2.10.m10.4.4.4.3.4.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3"><apply id="S5.SS2.p2.10.m10.2.2.2.1.1.1.cmml" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.2.2.2.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.10.m10.2.2.2.1.1.1.2.cmml" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1.2">𝑄</ci><ci id="S5.SS2.p2.10.m10.2.2.2.1.1.1.3.cmml" xref="S5.SS2.p2.10.m10.2.2.2.1.1.1.3">ℓ</ci></apply><apply id="S5.SS2.p2.10.m10.3.3.3.2.2.2.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2"><times id="S5.SS2.p2.10.m10.3.3.3.2.2.2.2.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.2"></times><apply id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.1.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3">subscript</csymbol><ci id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.2.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.2">𝑊</ci><ci id="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.3.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.3.3">𝐾</ci></apply><apply id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.2.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1">subscript</csymbol><set id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.2.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1"><apply id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.2.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.3.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.3.cmml" xref="S5.SS2.p2.10.m10.3.3.3.2.2.2.1.3">𝐿</ci></apply></apply><apply id="S5.SS2.p2.10.m10.4.4.4.3.3.3.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3"><times id="S5.SS2.p2.10.m10.4.4.4.3.3.3.2.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.2"></times><apply id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.1.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3">subscript</csymbol><ci id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.2.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.2">𝑊</ci><ci id="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.3.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.3.3">𝑉</ci></apply><apply id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.2.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1">subscript</csymbol><set id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.2.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1"><apply id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.1.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.2.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.3.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.3.cmml" xref="S5.SS2.p2.10.m10.4.4.4.3.3.3.1.3">𝐿</ci></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.10.m10.4c">\{\mathbf{v}_{j}\}_{\ell+1}=A(Q_{\ell},W_{K}\{q_{j}\}_{L},W_{V}\{q_{j}\}_{L})</annotation></semantics></math> for weights <math id="S5.SS2.p2.11.m11.1" class="ltx_Math" alttext="W_{K}" display="inline"><semantics id="S5.SS2.p2.11.m11.1a"><msub id="S5.SS2.p2.11.m11.1.1" xref="S5.SS2.p2.11.m11.1.1.cmml"><mi id="S5.SS2.p2.11.m11.1.1.2" xref="S5.SS2.p2.11.m11.1.1.2.cmml">W</mi><mi id="S5.SS2.p2.11.m11.1.1.3" xref="S5.SS2.p2.11.m11.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.11.m11.1b"><apply id="S5.SS2.p2.11.m11.1.1.cmml" xref="S5.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.11.m11.1.1.1.cmml" xref="S5.SS2.p2.11.m11.1.1">subscript</csymbol><ci id="S5.SS2.p2.11.m11.1.1.2.cmml" xref="S5.SS2.p2.11.m11.1.1.2">𝑊</ci><ci id="S5.SS2.p2.11.m11.1.1.3.cmml" xref="S5.SS2.p2.11.m11.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.11.m11.1c">W_{K}</annotation></semantics></math> and <math id="S5.SS2.p2.12.m12.1" class="ltx_Math" alttext="W_{V}" display="inline"><semantics id="S5.SS2.p2.12.m12.1a"><msub id="S5.SS2.p2.12.m12.1.1" xref="S5.SS2.p2.12.m12.1.1.cmml"><mi id="S5.SS2.p2.12.m12.1.1.2" xref="S5.SS2.p2.12.m12.1.1.2.cmml">W</mi><mi id="S5.SS2.p2.12.m12.1.1.3" xref="S5.SS2.p2.12.m12.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.12.m12.1b"><apply id="S5.SS2.p2.12.m12.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.12.m12.1.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1">subscript</csymbol><ci id="S5.SS2.p2.12.m12.1.1.2.cmml" xref="S5.SS2.p2.12.m12.1.1.2">𝑊</ci><ci id="S5.SS2.p2.12.m12.1.1.3.cmml" xref="S5.SS2.p2.12.m12.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.12.m12.1c">W_{V}</annotation></semantics></math> corresponding to keys and values respectively. (3) The feature representations at the final layer <math id="S5.SS2.p2.13.m13.1" class="ltx_Math" alttext="\{q_{j}\}_{L}" display="inline"><semantics id="S5.SS2.p2.13.m13.1a"><msub id="S5.SS2.p2.13.m13.1.1" xref="S5.SS2.p2.13.m13.1.1.cmml"><mrow id="S5.SS2.p2.13.m13.1.1.1.1" xref="S5.SS2.p2.13.m13.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.13.m13.1.1.1.1.2" xref="S5.SS2.p2.13.m13.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.13.m13.1.1.1.1.1" xref="S5.SS2.p2.13.m13.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.13.m13.1.1.1.1.1.2" xref="S5.SS2.p2.13.m13.1.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p2.13.m13.1.1.1.1.1.3" xref="S5.SS2.p2.13.m13.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.13.m13.1.1.1.1.3" xref="S5.SS2.p2.13.m13.1.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p2.13.m13.1.1.3" xref="S5.SS2.p2.13.m13.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.13.m13.1b"><apply id="S5.SS2.p2.13.m13.1.1.cmml" xref="S5.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.13.m13.1.1.2.cmml" xref="S5.SS2.p2.13.m13.1.1">subscript</csymbol><set id="S5.SS2.p2.13.m13.1.1.1.2.cmml" xref="S5.SS2.p2.13.m13.1.1.1.1"><apply id="S5.SS2.p2.13.m13.1.1.1.1.1.cmml" xref="S5.SS2.p2.13.m13.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.13.m13.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.13.m13.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.13.m13.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.13.m13.1.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p2.13.m13.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.13.m13.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p2.13.m13.1.1.3.cmml" xref="S5.SS2.p2.13.m13.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.13.m13.1c">\{q_{j}\}_{L}</annotation></semantics></math> and <math id="S5.SS2.p2.14.m14.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}_{L}" display="inline"><semantics id="S5.SS2.p2.14.m14.1a"><msub id="S5.SS2.p2.14.m14.1.1" xref="S5.SS2.p2.14.m14.1.1.cmml"><mrow id="S5.SS2.p2.14.m14.1.1.1.1" xref="S5.SS2.p2.14.m14.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.14.m14.1.1.1.1.2" xref="S5.SS2.p2.14.m14.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p2.14.m14.1.1.1.1.1" xref="S5.SS2.p2.14.m14.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.14.m14.1.1.1.1.1.2" xref="S5.SS2.p2.14.m14.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.SS2.p2.14.m14.1.1.1.1.1.3" xref="S5.SS2.p2.14.m14.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p2.14.m14.1.1.1.1.3" xref="S5.SS2.p2.14.m14.1.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p2.14.m14.1.1.3" xref="S5.SS2.p2.14.m14.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.14.m14.1b"><apply id="S5.SS2.p2.14.m14.1.1.cmml" xref="S5.SS2.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.14.m14.1.1.2.cmml" xref="S5.SS2.p2.14.m14.1.1">subscript</csymbol><set id="S5.SS2.p2.14.m14.1.1.1.2.cmml" xref="S5.SS2.p2.14.m14.1.1.1.1"><apply id="S5.SS2.p2.14.m14.1.1.1.1.1.cmml" xref="S5.SS2.p2.14.m14.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.14.m14.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.14.m14.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.14.m14.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.14.m14.1.1.1.1.1.2">𝐯</ci><ci id="S5.SS2.p2.14.m14.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.14.m14.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p2.14.m14.1.1.3.cmml" xref="S5.SS2.p2.14.m14.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.14.m14.1c">\{\mathbf{v}_{j}\}_{L}</annotation></semantics></math> are pooled, fused, and fed into a classifier.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.3" class="ltx_p">To adapt this model to further accept the point input, we compute visual features <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="\{\mathbf{v}^{pt}_{j}\}_{L}" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><msub id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mrow id="S5.SS2.p3.1.m1.1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p3.1.m1.1.1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.1.2.cmml">{</mo><msubsup id="S5.SS2.p3.1.m1.1.1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS2.p3.1.m1.1.1.1.1.1.2.2" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S5.SS2.p3.1.m1.1.1.1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.1.1.1.3.cmml">j</mi><mrow id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.cmml"><mi id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.2" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.1" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.3" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S5.SS2.p3.1.m1.1.1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1">subscript</csymbol><set id="S5.SS2.p3.1.m1.1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1"><apply id="S5.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1">subscript</csymbol><apply id="S5.SS2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1">superscript</csymbol><ci id="S5.SS2.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.2">𝐯</ci><apply id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3"><times id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.1"></times><ci id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.2.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.2">𝑝</ci><ci id="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.3.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S5.SS2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\{\mathbf{v}^{pt}_{j}\}_{L}</annotation></semantics></math> corresponding only to the regions containing the point <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="pt" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mrow id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mi id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p3.2.m2.1.1.1" xref="S5.SS2.p3.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><times id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1.1"></times><ci id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">𝑝</ci><ci id="S5.SS2.p3.2.m2.1.1.3.cmml" xref="S5.SS2.p3.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">pt</annotation></semantics></math> (the “point stream”), conditioned on the question as in the original model. Then in each cross-attention layer for the <em id="S5.SS2.p3.3.1" class="ltx_emph ltx_font_italic">image</em> stream, we condition attention on information from both the question and point by concatenating the representations <math id="S5.SS2.p3.3.m3.3" class="ltx_Math" alttext="\{f_{j}\}_{\ell}=\{q_{j}\}_{\ell}\oplus\{\mathbf{v}^{pt}_{j}\}_{\ell}" display="inline"><semantics id="S5.SS2.p3.3.m3.3a"><mrow id="S5.SS2.p3.3.m3.3.3" xref="S5.SS2.p3.3.m3.3.3.cmml"><msub id="S5.SS2.p3.3.m3.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.cmml"><mrow id="S5.SS2.p3.3.m3.1.1.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p3.3.m3.1.1.1.1.1.2" xref="S5.SS2.p3.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p3.3.m3.1.1.1.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S5.SS2.p3.3.m3.1.1.1.1.1.1.2" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml">f</mi><mi id="S5.SS2.p3.3.m3.1.1.1.1.1.1.3" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p3.3.m3.1.1.1.1.1.3" xref="S5.SS2.p3.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p3.3.m3.1.1.1.3" xref="S5.SS2.p3.3.m3.1.1.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p3.3.m3.3.3.4" xref="S5.SS2.p3.3.m3.3.3.4.cmml">=</mo><mrow id="S5.SS2.p3.3.m3.3.3.3" xref="S5.SS2.p3.3.m3.3.3.3.cmml"><msub id="S5.SS2.p3.3.m3.2.2.2.1" xref="S5.SS2.p3.3.m3.2.2.2.1.cmml"><mrow id="S5.SS2.p3.3.m3.2.2.2.1.1.1" xref="S5.SS2.p3.3.m3.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p3.3.m3.2.2.2.1.1.1.2" xref="S5.SS2.p3.3.m3.2.2.2.1.1.2.cmml">{</mo><msub id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.cmml"><mi id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.2" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.3" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p3.3.m3.2.2.2.1.1.1.3" xref="S5.SS2.p3.3.m3.2.2.2.1.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p3.3.m3.2.2.2.1.3" xref="S5.SS2.p3.3.m3.2.2.2.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p3.3.m3.3.3.3.3" xref="S5.SS2.p3.3.m3.3.3.3.3.cmml">⊕</mo><msub id="S5.SS2.p3.3.m3.3.3.3.2" xref="S5.SS2.p3.3.m3.3.3.3.2.cmml"><mrow id="S5.SS2.p3.3.m3.3.3.3.2.1.1" xref="S5.SS2.p3.3.m3.3.3.3.2.1.2.cmml"><mo stretchy="false" id="S5.SS2.p3.3.m3.3.3.3.2.1.1.2" xref="S5.SS2.p3.3.m3.3.3.3.2.1.2.cmml">{</mo><msubsup id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.cmml"><mi id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.2" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.2.cmml">𝐯</mi><mi id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.3" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.3.cmml">j</mi><mrow id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.cmml"><mi id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.2" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.1" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.3" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S5.SS2.p3.3.m3.3.3.3.2.1.1.3" xref="S5.SS2.p3.3.m3.3.3.3.2.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p3.3.m3.3.3.3.2.3" xref="S5.SS2.p3.3.m3.3.3.3.2.3.cmml">ℓ</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.3b"><apply id="S5.SS2.p3.3.m3.3.3.cmml" xref="S5.SS2.p3.3.m3.3.3"><eq id="S5.SS2.p3.3.m3.3.3.4.cmml" xref="S5.SS2.p3.3.m3.3.3.4"></eq><apply id="S5.SS2.p3.3.m3.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.1.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.1">subscript</csymbol><set id="S5.SS2.p3.3.m3.1.1.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.1.1.1"><apply id="S5.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1.2">𝑓</ci><ci id="S5.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S5.SS2.p3.3.m3.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p3.3.m3.1.1.1.3.cmml" xref="S5.SS2.p3.3.m3.1.1.1.3">ℓ</ci></apply><apply id="S5.SS2.p3.3.m3.3.3.3.cmml" xref="S5.SS2.p3.3.m3.3.3.3"><csymbol cd="latexml" id="S5.SS2.p3.3.m3.3.3.3.3.cmml" xref="S5.SS2.p3.3.m3.3.3.3.3">direct-sum</csymbol><apply id="S5.SS2.p3.3.m3.2.2.2.1.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.2.2.2.1.2.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1">subscript</csymbol><set id="S5.SS2.p3.3.m3.2.2.2.1.1.2.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1"><apply id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.1.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.2.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.3.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p3.3.m3.2.2.2.1.3.cmml" xref="S5.SS2.p3.3.m3.2.2.2.1.3">ℓ</ci></apply><apply id="S5.SS2.p3.3.m3.3.3.3.2.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.3.3.3.2.2.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2">subscript</csymbol><set id="S5.SS2.p3.3.m3.3.3.3.2.1.2.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1"><apply id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.1.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1">subscript</csymbol><apply id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.1.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1">superscript</csymbol><ci id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.2.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.2">𝐯</ci><apply id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3"><times id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.1.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.1"></times><ci id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.2.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.2">𝑝</ci><ci id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.3.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.3.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p3.3.m3.3.3.3.2.3.cmml" xref="S5.SS2.p3.3.m3.3.3.3.2.3">ℓ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.3c">\{f_{j}\}_{\ell}=\{q_{j}\}_{\ell}\oplus\{\mathbf{v}^{pt}_{j}\}_{\ell}</annotation></semantics></math> and then computing the keys and values (Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>c). We then compute the multimodal fusion function:</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex1.m1.1" class="ltx_Math" alttext="\displaystyle z_{1}=\text{LayerNorm}(W_{1}^{T}\{q_{j}\}_{L}+W_{2}^{T}\{\mathbf{v}^{pt}_{j}\}_{L})," display="inline"><semantics id="S5.Ex1.m1.1a"><mrow id="S5.Ex1.m1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.cmml"><mrow id="S5.Ex1.m1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.cmml"><msub id="S5.Ex1.m1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.3.cmml"><mi id="S5.Ex1.m1.1.1.1.1.3.2" xref="S5.Ex1.m1.1.1.1.1.3.2.cmml">z</mi><mn id="S5.Ex1.m1.1.1.1.1.3.3" xref="S5.Ex1.m1.1.1.1.1.3.3.cmml">1</mn></msub><mo id="S5.Ex1.m1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.Ex1.m1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.cmml"><mtext id="S5.Ex1.m1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.3a.cmml">LayerNorm</mtext><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.Ex1.m1.1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">W</mi><mn id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">1</mn><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">L</mi></msub></mrow><mo id="S5.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml"><msubsup id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">W</mi><mn id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">2</mn><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><msub id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.cmml"><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mo stretchy="false" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml">{</mo><msubsup id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml">j</mi><mrow id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.cmml"><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.2" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.1" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.1.cmml">​</mo><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml">}</mo></mrow><mi id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.3.cmml">L</mi></msub></mrow></mrow><mo stretchy="false" id="S5.Ex1.m1.1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.Ex1.m1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.1b"><apply id="S5.Ex1.m1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1"><eq id="S5.Ex1.m1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.2"></eq><apply id="S5.Ex1.m1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.3.1.cmml" xref="S5.Ex1.m1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex1.m1.1.1.1.1.3.2.cmml" xref="S5.Ex1.m1.1.1.1.1.3.2">𝑧</ci><cn type="integer" id="S5.Ex1.m1.1.1.1.1.3.3.cmml" xref="S5.Ex1.m1.1.1.1.1.3.3">1</cn></apply><apply id="S5.Ex1.m1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1"><times id="S5.Ex1.m1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.2"></times><ci id="S5.Ex1.m1.1.1.1.1.1.3a.cmml" xref="S5.Ex1.m1.1.1.1.1.1.3"><mtext id="S5.Ex1.m1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.3">LayerNorm</mtext></ci><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1"><plus id="S5.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.3"></plus><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1"><times id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.2">𝑊</ci><cn type="integer" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.3">1</cn></apply><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3">𝑇</ci></apply><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><set id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3">𝐿</ci></apply></apply><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2"><times id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.2"></times><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.2">𝑊</ci><cn type="integer" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.2.3">2</cn></apply><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.3.3">𝑇</ci></apply><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1">subscript</csymbol><set id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1"><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1">subscript</csymbol><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1">superscript</csymbol><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2">𝐯</ci><apply id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3"><times id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.1"></times><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.2">𝑝</ci><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.1.1.2.1.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.1c">\displaystyle z_{1}=\text{LayerNorm}(W_{1}^{T}\{q_{j}\}_{L}+W_{2}^{T}\{\mathbf{v}^{pt}_{j}\}_{L}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S5.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex2.m1.1" class="ltx_Math" alttext="\displaystyle z_{2}=\text{LayerNorm}(W_{1}^{T}\{q_{j}\}_{L}+W_{3}^{T}\{\mathbf{v}_{j}\}_{L})," display="inline"><semantics id="S5.Ex2.m1.1a"><mrow id="S5.Ex2.m1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.cmml"><mrow id="S5.Ex2.m1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.cmml"><msub id="S5.Ex2.m1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.3.2" xref="S5.Ex2.m1.1.1.1.1.3.2.cmml">z</mi><mn id="S5.Ex2.m1.1.1.1.1.3.3" xref="S5.Ex2.m1.1.1.1.1.3.3.cmml">2</mn></msub><mo id="S5.Ex2.m1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.Ex2.m1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.cmml"><mtext id="S5.Ex2.m1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.3a.cmml">LayerNorm</mtext><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S5.Ex2.m1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex2.m1.1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">W</mi><mn id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">1</mn><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">L</mi></msub></mrow><mo id="S5.Ex2.m1.1.1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml"><msubsup id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">W</mi><mn id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">3</mn><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><msub id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml">{</mo><msub id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml">}</mo></mrow><mi id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.3.cmml">L</mi></msub></mrow></mrow><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.Ex2.m1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex2.m1.1b"><apply id="S5.Ex2.m1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1"><eq id="S5.Ex2.m1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.2"></eq><apply id="S5.Ex2.m1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.3.2">𝑧</ci><cn type="integer" id="S5.Ex2.m1.1.1.1.1.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.3.3">2</cn></apply><apply id="S5.Ex2.m1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1"><times id="S5.Ex2.m1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.2"></times><ci id="S5.Ex2.m1.1.1.1.1.1.3a.cmml" xref="S5.Ex2.m1.1.1.1.1.1.3"><mtext id="S5.Ex2.m1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.3">LayerNorm</mtext></ci><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1"><plus id="S5.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.3"></plus><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1"><times id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.2">𝑊</ci><cn type="integer" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.2.3">1</cn></apply><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.3.3">𝑇</ci></apply><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><set id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3">𝐿</ci></apply></apply><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2"><times id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.2"></times><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.2">𝑊</ci><cn type="integer" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.3">3</cn></apply><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3">𝑇</ci></apply><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1">subscript</csymbol><set id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1"><apply id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.2">𝐯</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1.1.2.1.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex2.m1.1c">\displaystyle z_{2}=\text{LayerNorm}(W_{1}^{T}\{q_{j}\}_{L}+W_{3}^{T}\{\mathbf{v}_{j}\}_{L}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S5.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex3.m1.1" class="ltx_Math" alttext="\displaystyle z=z_{1}\oplus z_{2}," display="inline"><semantics id="S5.Ex3.m1.1a"><mrow id="S5.Ex3.m1.1.1.1" xref="S5.Ex3.m1.1.1.1.1.cmml"><mrow id="S5.Ex3.m1.1.1.1.1" xref="S5.Ex3.m1.1.1.1.1.cmml"><mi id="S5.Ex3.m1.1.1.1.1.2" xref="S5.Ex3.m1.1.1.1.1.2.cmml">z</mi><mo id="S5.Ex3.m1.1.1.1.1.1" xref="S5.Ex3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S5.Ex3.m1.1.1.1.1.3" xref="S5.Ex3.m1.1.1.1.1.3.cmml"><msub id="S5.Ex3.m1.1.1.1.1.3.2" xref="S5.Ex3.m1.1.1.1.1.3.2.cmml"><mi id="S5.Ex3.m1.1.1.1.1.3.2.2" xref="S5.Ex3.m1.1.1.1.1.3.2.2.cmml">z</mi><mn id="S5.Ex3.m1.1.1.1.1.3.2.3" xref="S5.Ex3.m1.1.1.1.1.3.2.3.cmml">1</mn></msub><mo id="S5.Ex3.m1.1.1.1.1.3.1" xref="S5.Ex3.m1.1.1.1.1.3.1.cmml">⊕</mo><msub id="S5.Ex3.m1.1.1.1.1.3.3" xref="S5.Ex3.m1.1.1.1.1.3.3.cmml"><mi id="S5.Ex3.m1.1.1.1.1.3.3.2" xref="S5.Ex3.m1.1.1.1.1.3.3.2.cmml">z</mi><mn id="S5.Ex3.m1.1.1.1.1.3.3.3" xref="S5.Ex3.m1.1.1.1.1.3.3.3.cmml">2</mn></msub></mrow></mrow><mo id="S5.Ex3.m1.1.1.1.2" xref="S5.Ex3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex3.m1.1b"><apply id="S5.Ex3.m1.1.1.1.1.cmml" xref="S5.Ex3.m1.1.1.1"><eq id="S5.Ex3.m1.1.1.1.1.1.cmml" xref="S5.Ex3.m1.1.1.1.1.1"></eq><ci id="S5.Ex3.m1.1.1.1.1.2.cmml" xref="S5.Ex3.m1.1.1.1.1.2">𝑧</ci><apply id="S5.Ex3.m1.1.1.1.1.3.cmml" xref="S5.Ex3.m1.1.1.1.1.3"><csymbol cd="latexml" id="S5.Ex3.m1.1.1.1.1.3.1.cmml" xref="S5.Ex3.m1.1.1.1.1.3.1">direct-sum</csymbol><apply id="S5.Ex3.m1.1.1.1.1.3.2.cmml" xref="S5.Ex3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.1.1.1.3.2.1.cmml" xref="S5.Ex3.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S5.Ex3.m1.1.1.1.1.3.2.2.cmml" xref="S5.Ex3.m1.1.1.1.1.3.2.2">𝑧</ci><cn type="integer" id="S5.Ex3.m1.1.1.1.1.3.2.3.cmml" xref="S5.Ex3.m1.1.1.1.1.3.2.3">1</cn></apply><apply id="S5.Ex3.m1.1.1.1.1.3.3.cmml" xref="S5.Ex3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.1.1.1.3.3.1.cmml" xref="S5.Ex3.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S5.Ex3.m1.1.1.1.1.3.3.2.cmml" xref="S5.Ex3.m1.1.1.1.1.3.3.2">𝑧</ci><cn type="integer" id="S5.Ex3.m1.1.1.1.1.3.3.3.cmml" xref="S5.Ex3.m1.1.1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex3.m1.1c">\displaystyle z=z_{1}\oplus z_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS2.p4.1" class="ltx_p">followed by a linear classifier over the answer space. This dependency between streams is shown in Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>b (black).</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.2" class="ltx_p"><span id="S5.SS2.p5.2.1" class="ltx_text ltx_font_bold">LXMERT-based.</span> LXMERT implements a bidirectional approach to attention; first, the question and visual features are passed through a stack of self-attention layers to obtain <math id="S5.SS2.p5.1.m1.1" class="ltx_Math" alttext="\{q_{j}\}_{L}" display="inline"><semantics id="S5.SS2.p5.1.m1.1a"><msub id="S5.SS2.p5.1.m1.1.1" xref="S5.SS2.p5.1.m1.1.1.cmml"><mrow id="S5.SS2.p5.1.m1.1.1.1.1" xref="S5.SS2.p5.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p5.1.m1.1.1.1.1.2" xref="S5.SS2.p5.1.m1.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p5.1.m1.1.1.1.1.1" xref="S5.SS2.p5.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS2.p5.1.m1.1.1.1.1.1.2" xref="S5.SS2.p5.1.m1.1.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p5.1.m1.1.1.1.1.1.3" xref="S5.SS2.p5.1.m1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p5.1.m1.1.1.1.1.3" xref="S5.SS2.p5.1.m1.1.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p5.1.m1.1.1.3" xref="S5.SS2.p5.1.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.1.m1.1b"><apply id="S5.SS2.p5.1.m1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.1.m1.1.1.2.cmml" xref="S5.SS2.p5.1.m1.1.1">subscript</csymbol><set id="S5.SS2.p5.1.m1.1.1.1.2.cmml" xref="S5.SS2.p5.1.m1.1.1.1.1"><apply id="S5.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p5.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.p5.1.m1.1.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p5.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.p5.1.m1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p5.1.m1.1.1.3.cmml" xref="S5.SS2.p5.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.1.m1.1c">\{q_{j}\}_{L}</annotation></semantics></math> and <math id="S5.SS2.p5.2.m2.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}_{L}" display="inline"><semantics id="S5.SS2.p5.2.m2.1a"><msub id="S5.SS2.p5.2.m2.1.1" xref="S5.SS2.p5.2.m2.1.1.cmml"><mrow id="S5.SS2.p5.2.m2.1.1.1.1" xref="S5.SS2.p5.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p5.2.m2.1.1.1.1.2" xref="S5.SS2.p5.2.m2.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p5.2.m2.1.1.1.1.1" xref="S5.SS2.p5.2.m2.1.1.1.1.1.cmml"><mi id="S5.SS2.p5.2.m2.1.1.1.1.1.2" xref="S5.SS2.p5.2.m2.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.SS2.p5.2.m2.1.1.1.1.1.3" xref="S5.SS2.p5.2.m2.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p5.2.m2.1.1.1.1.3" xref="S5.SS2.p5.2.m2.1.1.1.2.cmml">}</mo></mrow><mi id="S5.SS2.p5.2.m2.1.1.3" xref="S5.SS2.p5.2.m2.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.2.m2.1b"><apply id="S5.SS2.p5.2.m2.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.2.m2.1.1.2.cmml" xref="S5.SS2.p5.2.m2.1.1">subscript</csymbol><set id="S5.SS2.p5.2.m2.1.1.1.2.cmml" xref="S5.SS2.p5.2.m2.1.1.1.1"><apply id="S5.SS2.p5.2.m2.1.1.1.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.2.m2.1.1.1.1.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p5.2.m2.1.1.1.1.1.2.cmml" xref="S5.SS2.p5.2.m2.1.1.1.1.1.2">𝐯</ci><ci id="S5.SS2.p5.2.m2.1.1.1.1.1.3.cmml" xref="S5.SS2.p5.2.m2.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p5.2.m2.1.1.3.cmml" xref="S5.SS2.p5.2.m2.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.2.m2.1c">\{\mathbf{v}_{j}\}_{L}</annotation></semantics></math> as above. Then both are passed through a stack of “cross-modality encoders” that include a cross-attention layer followed by a self-attention layer and small feed-forward layer. The cross-attention layer exchanges the keys and values of the other modality (exactly as MCAN but for <em id="S5.SS2.p5.2.2" class="ltx_emph ltx_font_italic">both</em> image and text).</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.7" class="ltx_p">We modify LXMERT similarly to MCAN to accept the point input: it now has three streams corresponding to the question <math id="S5.SS2.p6.1.m1.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S5.SS2.p6.1.m1.1a"><mi id="S5.SS2.p6.1.m1.1.1" xref="S5.SS2.p6.1.m1.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><ci id="S5.SS2.p6.1.m1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">\mathbf{q}</annotation></semantics></math>, the global image stream corresponding to all image regions <math id="S5.SS2.p6.2.m2.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S5.SS2.p6.2.m2.1a"><mi id="S5.SS2.p6.2.m2.1.1" xref="S5.SS2.p6.2.m2.1.1.cmml">𝐯</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.2.m2.1b"><ci id="S5.SS2.p6.2.m2.1.1.cmml" xref="S5.SS2.p6.2.m2.1.1">𝐯</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.2.m2.1c">\mathbf{v}</annotation></semantics></math>, and a local point stream corresponding to only the regions <math id="S5.SS2.p6.3.m3.1" class="ltx_Math" alttext="\mathbf{v}^{pt}" display="inline"><semantics id="S5.SS2.p6.3.m3.1a"><msup id="S5.SS2.p6.3.m3.1.1" xref="S5.SS2.p6.3.m3.1.1.cmml"><mi id="S5.SS2.p6.3.m3.1.1.2" xref="S5.SS2.p6.3.m3.1.1.2.cmml">𝐯</mi><mrow id="S5.SS2.p6.3.m3.1.1.3" xref="S5.SS2.p6.3.m3.1.1.3.cmml"><mi id="S5.SS2.p6.3.m3.1.1.3.2" xref="S5.SS2.p6.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p6.3.m3.1.1.3.1" xref="S5.SS2.p6.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p6.3.m3.1.1.3.3" xref="S5.SS2.p6.3.m3.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.3.m3.1b"><apply id="S5.SS2.p6.3.m3.1.1.cmml" xref="S5.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.3.m3.1.1.1.cmml" xref="S5.SS2.p6.3.m3.1.1">superscript</csymbol><ci id="S5.SS2.p6.3.m3.1.1.2.cmml" xref="S5.SS2.p6.3.m3.1.1.2">𝐯</ci><apply id="S5.SS2.p6.3.m3.1.1.3.cmml" xref="S5.SS2.p6.3.m3.1.1.3"><times id="S5.SS2.p6.3.m3.1.1.3.1.cmml" xref="S5.SS2.p6.3.m3.1.1.3.1"></times><ci id="S5.SS2.p6.3.m3.1.1.3.2.cmml" xref="S5.SS2.p6.3.m3.1.1.3.2">𝑝</ci><ci id="S5.SS2.p6.3.m3.1.1.3.3.cmml" xref="S5.SS2.p6.3.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.3.m3.1c">\mathbf{v}^{pt}</annotation></semantics></math>. Concretely, for the cross-attention layer at layer <math id="S5.SS2.p6.4.m4.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S5.SS2.p6.4.m4.1a"><mi mathvariant="normal" id="S5.SS2.p6.4.m4.1.1" xref="S5.SS2.p6.4.m4.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.4.m4.1b"><ci id="S5.SS2.p6.4.m4.1.1.cmml" xref="S5.SS2.p6.4.m4.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.4.m4.1c">\ell</annotation></semantics></math> operating on the visual features <math id="S5.SS2.p6.5.m5.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}_{\ell}" display="inline"><semantics id="S5.SS2.p6.5.m5.1a"><msub id="S5.SS2.p6.5.m5.1.1" xref="S5.SS2.p6.5.m5.1.1.cmml"><mrow id="S5.SS2.p6.5.m5.1.1.1.1" xref="S5.SS2.p6.5.m5.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p6.5.m5.1.1.1.1.2" xref="S5.SS2.p6.5.m5.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p6.5.m5.1.1.1.1.1" xref="S5.SS2.p6.5.m5.1.1.1.1.1.cmml"><mi id="S5.SS2.p6.5.m5.1.1.1.1.1.2" xref="S5.SS2.p6.5.m5.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.SS2.p6.5.m5.1.1.1.1.1.3" xref="S5.SS2.p6.5.m5.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p6.5.m5.1.1.1.1.3" xref="S5.SS2.p6.5.m5.1.1.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p6.5.m5.1.1.3" xref="S5.SS2.p6.5.m5.1.1.3.cmml">ℓ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.5.m5.1b"><apply id="S5.SS2.p6.5.m5.1.1.cmml" xref="S5.SS2.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.5.m5.1.1.2.cmml" xref="S5.SS2.p6.5.m5.1.1">subscript</csymbol><set id="S5.SS2.p6.5.m5.1.1.1.2.cmml" xref="S5.SS2.p6.5.m5.1.1.1.1"><apply id="S5.SS2.p6.5.m5.1.1.1.1.1.cmml" xref="S5.SS2.p6.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.5.m5.1.1.1.1.1.1.cmml" xref="S5.SS2.p6.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p6.5.m5.1.1.1.1.1.2.cmml" xref="S5.SS2.p6.5.m5.1.1.1.1.1.2">𝐯</ci><ci id="S5.SS2.p6.5.m5.1.1.1.1.1.3.cmml" xref="S5.SS2.p6.5.m5.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p6.5.m5.1.1.3.cmml" xref="S5.SS2.p6.5.m5.1.1.3">ℓ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.5.m5.1c">\{\mathbf{v}_{j}\}_{\ell}</annotation></semantics></math>, we concatenate <math id="S5.SS2.p6.6.m6.3" class="ltx_Math" alttext="\{f_{j}\}_{\ell}=\{q_{j}\}_{\ell}\oplus\{\mathbf{v}^{pt}_{j}\}_{\ell}" display="inline"><semantics id="S5.SS2.p6.6.m6.3a"><mrow id="S5.SS2.p6.6.m6.3.3" xref="S5.SS2.p6.6.m6.3.3.cmml"><msub id="S5.SS2.p6.6.m6.1.1.1" xref="S5.SS2.p6.6.m6.1.1.1.cmml"><mrow id="S5.SS2.p6.6.m6.1.1.1.1.1" xref="S5.SS2.p6.6.m6.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p6.6.m6.1.1.1.1.1.2" xref="S5.SS2.p6.6.m6.1.1.1.1.2.cmml">{</mo><msub id="S5.SS2.p6.6.m6.1.1.1.1.1.1" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1.cmml"><mi id="S5.SS2.p6.6.m6.1.1.1.1.1.1.2" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1.2.cmml">f</mi><mi id="S5.SS2.p6.6.m6.1.1.1.1.1.1.3" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p6.6.m6.1.1.1.1.1.3" xref="S5.SS2.p6.6.m6.1.1.1.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p6.6.m6.1.1.1.3" xref="S5.SS2.p6.6.m6.1.1.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p6.6.m6.3.3.4" xref="S5.SS2.p6.6.m6.3.3.4.cmml">=</mo><mrow id="S5.SS2.p6.6.m6.3.3.3" xref="S5.SS2.p6.6.m6.3.3.3.cmml"><msub id="S5.SS2.p6.6.m6.2.2.2.1" xref="S5.SS2.p6.6.m6.2.2.2.1.cmml"><mrow id="S5.SS2.p6.6.m6.2.2.2.1.1.1" xref="S5.SS2.p6.6.m6.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p6.6.m6.2.2.2.1.1.1.2" xref="S5.SS2.p6.6.m6.2.2.2.1.1.2.cmml">{</mo><msub id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.cmml"><mi id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.2" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.2.cmml">q</mi><mi id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.3" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p6.6.m6.2.2.2.1.1.1.3" xref="S5.SS2.p6.6.m6.2.2.2.1.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p6.6.m6.2.2.2.1.3" xref="S5.SS2.p6.6.m6.2.2.2.1.3.cmml">ℓ</mi></msub><mo id="S5.SS2.p6.6.m6.3.3.3.3" xref="S5.SS2.p6.6.m6.3.3.3.3.cmml">⊕</mo><msub id="S5.SS2.p6.6.m6.3.3.3.2" xref="S5.SS2.p6.6.m6.3.3.3.2.cmml"><mrow id="S5.SS2.p6.6.m6.3.3.3.2.1.1" xref="S5.SS2.p6.6.m6.3.3.3.2.1.2.cmml"><mo stretchy="false" id="S5.SS2.p6.6.m6.3.3.3.2.1.1.2" xref="S5.SS2.p6.6.m6.3.3.3.2.1.2.cmml">{</mo><msubsup id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.cmml"><mi id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.2" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.2.cmml">𝐯</mi><mi id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.3" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.3.cmml">j</mi><mrow id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.cmml"><mi id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.2" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.1" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.3" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S5.SS2.p6.6.m6.3.3.3.2.1.1.3" xref="S5.SS2.p6.6.m6.3.3.3.2.1.2.cmml">}</mo></mrow><mi mathvariant="normal" id="S5.SS2.p6.6.m6.3.3.3.2.3" xref="S5.SS2.p6.6.m6.3.3.3.2.3.cmml">ℓ</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.6.m6.3b"><apply id="S5.SS2.p6.6.m6.3.3.cmml" xref="S5.SS2.p6.6.m6.3.3"><eq id="S5.SS2.p6.6.m6.3.3.4.cmml" xref="S5.SS2.p6.6.m6.3.3.4"></eq><apply id="S5.SS2.p6.6.m6.1.1.1.cmml" xref="S5.SS2.p6.6.m6.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.1.1.1.2.cmml" xref="S5.SS2.p6.6.m6.1.1.1">subscript</csymbol><set id="S5.SS2.p6.6.m6.1.1.1.1.2.cmml" xref="S5.SS2.p6.6.m6.1.1.1.1.1"><apply id="S5.SS2.p6.6.m6.1.1.1.1.1.1.cmml" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p6.6.m6.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1.2">𝑓</ci><ci id="S5.SS2.p6.6.m6.1.1.1.1.1.1.3.cmml" xref="S5.SS2.p6.6.m6.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p6.6.m6.1.1.1.3.cmml" xref="S5.SS2.p6.6.m6.1.1.1.3">ℓ</ci></apply><apply id="S5.SS2.p6.6.m6.3.3.3.cmml" xref="S5.SS2.p6.6.m6.3.3.3"><csymbol cd="latexml" id="S5.SS2.p6.6.m6.3.3.3.3.cmml" xref="S5.SS2.p6.6.m6.3.3.3.3">direct-sum</csymbol><apply id="S5.SS2.p6.6.m6.2.2.2.1.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.2.2.2.1.2.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1">subscript</csymbol><set id="S5.SS2.p6.6.m6.2.2.2.1.1.2.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1"><apply id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.1.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.2.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.2">𝑞</ci><ci id="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.3.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p6.6.m6.2.2.2.1.3.cmml" xref="S5.SS2.p6.6.m6.2.2.2.1.3">ℓ</ci></apply><apply id="S5.SS2.p6.6.m6.3.3.3.2.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.3.3.3.2.2.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2">subscript</csymbol><set id="S5.SS2.p6.6.m6.3.3.3.2.1.2.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1"><apply id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.1.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1">subscript</csymbol><apply id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.1.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1">superscript</csymbol><ci id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.2.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.2">𝐯</ci><apply id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3"><times id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.1.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.1"></times><ci id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.2.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.2">𝑝</ci><ci id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.3.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.3.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.1.1.1.3">𝑗</ci></apply></set><ci id="S5.SS2.p6.6.m6.3.3.3.2.3.cmml" xref="S5.SS2.p6.6.m6.3.3.3.2.3">ℓ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.6.m6.3c">\{f_{j}\}_{\ell}=\{q_{j}\}_{\ell}\oplus\{\mathbf{v}^{pt}_{j}\}_{\ell}</annotation></semantics></math> and compute keys and values on top of this representation; we do this similarly for the other modalities at layer <math id="S5.SS2.p6.7.m7.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S5.SS2.p6.7.m7.1a"><mi mathvariant="normal" id="S5.SS2.p6.7.m7.1.1" xref="S5.SS2.p6.7.m7.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.7.m7.1b"><ci id="S5.SS2.p6.7.m7.1.1.cmml" xref="S5.SS2.p6.7.m7.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.7.m7.1c">\ell</annotation></semantics></math>. Thus, at each layer each modality influences attention over the other in the cross-attention modules (Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>b, blue). As in LXMERT the final language features are pooled and fed into a classifier.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para ltx_noindent">
<p id="S5.SS2.p7.2" class="ltx_p"><span id="S5.SS2.p7.2.1" class="ltx_text ltx_font_bold">Two-Stream vs. Three-Stream.</span> An alternative way of incorporating both local and contextual information is to concatenate the image features <math id="S5.SS2.p7.1.m1.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}" display="inline"><semantics id="S5.SS2.p7.1.m1.1a"><mrow id="S5.SS2.p7.1.m1.1.1.1" xref="S5.SS2.p7.1.m1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p7.1.m1.1.1.1.2" xref="S5.SS2.p7.1.m1.1.1.2.cmml">{</mo><msub id="S5.SS2.p7.1.m1.1.1.1.1" xref="S5.SS2.p7.1.m1.1.1.1.1.cmml"><mi id="S5.SS2.p7.1.m1.1.1.1.1.2" xref="S5.SS2.p7.1.m1.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.SS2.p7.1.m1.1.1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.SS2.p7.1.m1.1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.m1.1b"><set id="S5.SS2.p7.1.m1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.1"><apply id="S5.SS2.p7.1.m1.1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p7.1.m1.1.1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.1.1.2">𝐯</ci><ci id="S5.SS2.p7.1.m1.1.1.1.1.3.cmml" xref="S5.SS2.p7.1.m1.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.m1.1c">\{\mathbf{v}_{j}\}</annotation></semantics></math> and the point features <math id="S5.SS2.p7.2.m2.1" class="ltx_Math" alttext="\{\mathbf{v}^{pt}_{j}\}" display="inline"><semantics id="S5.SS2.p7.2.m2.1a"><mrow id="S5.SS2.p7.2.m2.1.1.1" xref="S5.SS2.p7.2.m2.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p7.2.m2.1.1.1.2" xref="S5.SS2.p7.2.m2.1.1.2.cmml">{</mo><msubsup id="S5.SS2.p7.2.m2.1.1.1.1" xref="S5.SS2.p7.2.m2.1.1.1.1.cmml"><mi id="S5.SS2.p7.2.m2.1.1.1.1.2.2" xref="S5.SS2.p7.2.m2.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S5.SS2.p7.2.m2.1.1.1.1.3" xref="S5.SS2.p7.2.m2.1.1.1.1.3.cmml">j</mi><mrow id="S5.SS2.p7.2.m2.1.1.1.1.2.3" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.cmml"><mi id="S5.SS2.p7.2.m2.1.1.1.1.2.3.2" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.2.m2.1.1.1.1.2.3.1" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS2.p7.2.m2.1.1.1.1.2.3.3" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S5.SS2.p7.2.m2.1.1.1.3" xref="S5.SS2.p7.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.2.m2.1b"><set id="S5.SS2.p7.2.m2.1.1.2.cmml" xref="S5.SS2.p7.2.m2.1.1.1"><apply id="S5.SS2.p7.2.m2.1.1.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.2.m2.1.1.1.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1">subscript</csymbol><apply id="S5.SS2.p7.2.m2.1.1.1.1.2.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.2.m2.1.1.1.1.2.1.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1">superscript</csymbol><ci id="S5.SS2.p7.2.m2.1.1.1.1.2.2.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1.2.2">𝐯</ci><apply id="S5.SS2.p7.2.m2.1.1.1.1.2.3.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3"><times id="S5.SS2.p7.2.m2.1.1.1.1.2.3.1.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.1"></times><ci id="S5.SS2.p7.2.m2.1.1.1.1.2.3.2.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.2">𝑝</ci><ci id="S5.SS2.p7.2.m2.1.1.1.1.2.3.3.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S5.SS2.p7.2.m2.1.1.1.1.3.cmml" xref="S5.SS2.p7.2.m2.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.2.m2.1c">\{\mathbf{v}^{pt}_{j}\}</annotation></semantics></math> into the same stream without modifying the VQA model itself. We explore this alternative in Sec <a href="#S5.SS3" title="5.3 PointQA-General evaluation ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a> which we term <span id="S5.SS2.p7.2.2" class="ltx_text ltx_font_bold">two-stream</span>, compared to the <span id="S5.SS2.p7.2.3" class="ltx_text ltx_font_bold">three-stream</span> approach we describe in this section that treats the point as its own stream and modifies the cross-attention accordingly.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> evaluation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We evaluate the Pythia-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> model of Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> and the MCAN-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and LXMERT-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> models of Sec. <a href="#S5.SS2" title="5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> on the challenging task of <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> .</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.5" class="ltx_p"><span id="S5.SS3.p2.5.1" class="ltx_text ltx_font_bold">Implementation Details.</span>. For MCAN we set <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="L=2" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">L</mi><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><eq id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></eq><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">L=2</annotation></semantics></math>, since we found that increasing number of layers did not improve performance. For LXMERT following the original paper, we set a higher number of layers for the language modality; concretely we set <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="N_{L}=5" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mrow id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><msub id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml"><mi id="S5.SS3.p2.2.m2.1.1.2.2" xref="S5.SS3.p2.2.m2.1.1.2.2.cmml">N</mi><mi id="S5.SS3.p2.2.m2.1.1.2.3" xref="S5.SS3.p2.2.m2.1.1.2.3.cmml">L</mi></msub><mo id="S5.SS3.p2.2.m2.1.1.1" xref="S5.SS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><eq id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1"></eq><apply id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p2.2.m2.1.1.2.1.cmml" xref="S5.SS3.p2.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS3.p2.2.m2.1.1.2.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2.2">𝑁</ci><ci id="S5.SS3.p2.2.m2.1.1.2.3.cmml" xref="S5.SS3.p2.2.m2.1.1.2.3">𝐿</ci></apply><cn type="integer" id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">N_{L}=5</annotation></semantics></math> and <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="N_{Img}=N_{Pt}=3" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mrow id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml"><msub id="S5.SS3.p2.3.m3.1.1.2" xref="S5.SS3.p2.3.m3.1.1.2.cmml"><mi id="S5.SS3.p2.3.m3.1.1.2.2" xref="S5.SS3.p2.3.m3.1.1.2.2.cmml">N</mi><mrow id="S5.SS3.p2.3.m3.1.1.2.3" xref="S5.SS3.p2.3.m3.1.1.2.3.cmml"><mi id="S5.SS3.p2.3.m3.1.1.2.3.2" xref="S5.SS3.p2.3.m3.1.1.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.3.m3.1.1.2.3.1" xref="S5.SS3.p2.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S5.SS3.p2.3.m3.1.1.2.3.3" xref="S5.SS3.p2.3.m3.1.1.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.3.m3.1.1.2.3.1a" xref="S5.SS3.p2.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S5.SS3.p2.3.m3.1.1.2.3.4" xref="S5.SS3.p2.3.m3.1.1.2.3.4.cmml">g</mi></mrow></msub><mo id="S5.SS3.p2.3.m3.1.1.3" xref="S5.SS3.p2.3.m3.1.1.3.cmml">=</mo><msub id="S5.SS3.p2.3.m3.1.1.4" xref="S5.SS3.p2.3.m3.1.1.4.cmml"><mi id="S5.SS3.p2.3.m3.1.1.4.2" xref="S5.SS3.p2.3.m3.1.1.4.2.cmml">N</mi><mrow id="S5.SS3.p2.3.m3.1.1.4.3" xref="S5.SS3.p2.3.m3.1.1.4.3.cmml"><mi id="S5.SS3.p2.3.m3.1.1.4.3.2" xref="S5.SS3.p2.3.m3.1.1.4.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.3.m3.1.1.4.3.1" xref="S5.SS3.p2.3.m3.1.1.4.3.1.cmml">​</mo><mi id="S5.SS3.p2.3.m3.1.1.4.3.3" xref="S5.SS3.p2.3.m3.1.1.4.3.3.cmml">t</mi></mrow></msub><mo id="S5.SS3.p2.3.m3.1.1.5" xref="S5.SS3.p2.3.m3.1.1.5.cmml">=</mo><mn id="S5.SS3.p2.3.m3.1.1.6" xref="S5.SS3.p2.3.m3.1.1.6.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><apply id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1"><and id="S5.SS3.p2.3.m3.1.1a.cmml" xref="S5.SS3.p2.3.m3.1.1"></and><apply id="S5.SS3.p2.3.m3.1.1b.cmml" xref="S5.SS3.p2.3.m3.1.1"><eq id="S5.SS3.p2.3.m3.1.1.3.cmml" xref="S5.SS3.p2.3.m3.1.1.3"></eq><apply id="S5.SS3.p2.3.m3.1.1.2.cmml" xref="S5.SS3.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p2.3.m3.1.1.2.1.cmml" xref="S5.SS3.p2.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS3.p2.3.m3.1.1.2.2.cmml" xref="S5.SS3.p2.3.m3.1.1.2.2">𝑁</ci><apply id="S5.SS3.p2.3.m3.1.1.2.3.cmml" xref="S5.SS3.p2.3.m3.1.1.2.3"><times id="S5.SS3.p2.3.m3.1.1.2.3.1.cmml" xref="S5.SS3.p2.3.m3.1.1.2.3.1"></times><ci id="S5.SS3.p2.3.m3.1.1.2.3.2.cmml" xref="S5.SS3.p2.3.m3.1.1.2.3.2">𝐼</ci><ci id="S5.SS3.p2.3.m3.1.1.2.3.3.cmml" xref="S5.SS3.p2.3.m3.1.1.2.3.3">𝑚</ci><ci id="S5.SS3.p2.3.m3.1.1.2.3.4.cmml" xref="S5.SS3.p2.3.m3.1.1.2.3.4">𝑔</ci></apply></apply><apply id="S5.SS3.p2.3.m3.1.1.4.cmml" xref="S5.SS3.p2.3.m3.1.1.4"><csymbol cd="ambiguous" id="S5.SS3.p2.3.m3.1.1.4.1.cmml" xref="S5.SS3.p2.3.m3.1.1.4">subscript</csymbol><ci id="S5.SS3.p2.3.m3.1.1.4.2.cmml" xref="S5.SS3.p2.3.m3.1.1.4.2">𝑁</ci><apply id="S5.SS3.p2.3.m3.1.1.4.3.cmml" xref="S5.SS3.p2.3.m3.1.1.4.3"><times id="S5.SS3.p2.3.m3.1.1.4.3.1.cmml" xref="S5.SS3.p2.3.m3.1.1.4.3.1"></times><ci id="S5.SS3.p2.3.m3.1.1.4.3.2.cmml" xref="S5.SS3.p2.3.m3.1.1.4.3.2">𝑃</ci><ci id="S5.SS3.p2.3.m3.1.1.4.3.3.cmml" xref="S5.SS3.p2.3.m3.1.1.4.3.3">𝑡</ci></apply></apply></apply><apply id="S5.SS3.p2.3.m3.1.1c.cmml" xref="S5.SS3.p2.3.m3.1.1"><eq id="S5.SS3.p2.3.m3.1.1.5.cmml" xref="S5.SS3.p2.3.m3.1.1.5"></eq><share href="#S5.SS3.p2.3.m3.1.1.4.cmml" id="S5.SS3.p2.3.m3.1.1d.cmml" xref="S5.SS3.p2.3.m3.1.1"></share><cn type="integer" id="S5.SS3.p2.3.m3.1.1.6.cmml" xref="S5.SS3.p2.3.m3.1.1.6">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">N_{Img}=N_{Pt}=3</annotation></semantics></math>. We set the number of cross-modality encoders <math id="S5.SS3.p2.4.m4.1" class="ltx_Math" alttext="N_{X}" display="inline"><semantics id="S5.SS3.p2.4.m4.1a"><msub id="S5.SS3.p2.4.m4.1.1" xref="S5.SS3.p2.4.m4.1.1.cmml"><mi id="S5.SS3.p2.4.m4.1.1.2" xref="S5.SS3.p2.4.m4.1.1.2.cmml">N</mi><mi id="S5.SS3.p2.4.m4.1.1.3" xref="S5.SS3.p2.4.m4.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.4.m4.1b"><apply id="S5.SS3.p2.4.m4.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.4.m4.1.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS3.p2.4.m4.1.1.2.cmml" xref="S5.SS3.p2.4.m4.1.1.2">𝑁</ci><ci id="S5.SS3.p2.4.m4.1.1.3.cmml" xref="S5.SS3.p2.4.m4.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.4.m4.1c">N_{X}</annotation></semantics></math> to be 3. Both models were trained using Adam with early stopping and a patience of 5000 iterations. We used a learning rate of <math id="S5.SS3.p2.5.m5.2" class="ltx_Math" alttext="\{5,2.5\}e{\text{-}}5" display="inline"><semantics id="S5.SS3.p2.5.m5.2a"><mrow id="S5.SS3.p2.5.m5.2.3" xref="S5.SS3.p2.5.m5.2.3.cmml"><mrow id="S5.SS3.p2.5.m5.2.3.2.2" xref="S5.SS3.p2.5.m5.2.3.2.1.cmml"><mo stretchy="false" id="S5.SS3.p2.5.m5.2.3.2.2.1" xref="S5.SS3.p2.5.m5.2.3.2.1.cmml">{</mo><mn id="S5.SS3.p2.5.m5.1.1" xref="S5.SS3.p2.5.m5.1.1.cmml">5</mn><mo id="S5.SS3.p2.5.m5.2.3.2.2.2" xref="S5.SS3.p2.5.m5.2.3.2.1.cmml">,</mo><mn id="S5.SS3.p2.5.m5.2.2" xref="S5.SS3.p2.5.m5.2.2.cmml">2.5</mn><mo stretchy="false" id="S5.SS3.p2.5.m5.2.3.2.2.3" xref="S5.SS3.p2.5.m5.2.3.2.1.cmml">}</mo></mrow><mo lspace="0em" rspace="0em" id="S5.SS3.p2.5.m5.2.3.1" xref="S5.SS3.p2.5.m5.2.3.1.cmml">​</mo><mi id="S5.SS3.p2.5.m5.2.3.3" xref="S5.SS3.p2.5.m5.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.5.m5.2.3.1a" xref="S5.SS3.p2.5.m5.2.3.1.cmml">​</mo><mtext id="S5.SS3.p2.5.m5.2.3.4" xref="S5.SS3.p2.5.m5.2.3.4a.cmml">-</mtext><mo lspace="0em" rspace="0em" id="S5.SS3.p2.5.m5.2.3.1b" xref="S5.SS3.p2.5.m5.2.3.1.cmml">​</mo><mn id="S5.SS3.p2.5.m5.2.3.5" xref="S5.SS3.p2.5.m5.2.3.5.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.5.m5.2b"><apply id="S5.SS3.p2.5.m5.2.3.cmml" xref="S5.SS3.p2.5.m5.2.3"><times id="S5.SS3.p2.5.m5.2.3.1.cmml" xref="S5.SS3.p2.5.m5.2.3.1"></times><set id="S5.SS3.p2.5.m5.2.3.2.1.cmml" xref="S5.SS3.p2.5.m5.2.3.2.2"><cn type="integer" id="S5.SS3.p2.5.m5.1.1.cmml" xref="S5.SS3.p2.5.m5.1.1">5</cn><cn type="float" id="S5.SS3.p2.5.m5.2.2.cmml" xref="S5.SS3.p2.5.m5.2.2">2.5</cn></set><ci id="S5.SS3.p2.5.m5.2.3.3.cmml" xref="S5.SS3.p2.5.m5.2.3.3">𝑒</ci><ci id="S5.SS3.p2.5.m5.2.3.4a.cmml" xref="S5.SS3.p2.5.m5.2.3.4"><mtext id="S5.SS3.p2.5.m5.2.3.4.cmml" xref="S5.SS3.p2.5.m5.2.3.4">-</mtext></ci><cn type="integer" id="S5.SS3.p2.5.m5.2.3.5.cmml" xref="S5.SS3.p2.5.m5.2.3.5">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.5.m5.2c">\{5,2.5\}e{\text{-}}5</annotation></semantics></math> for MCAN and LXMERT respectively with a warmup/decay schedule. Details for Pythia are in Sec. <a href="#S3.SS3" title="3.3 PointQA-Local evaluation ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Test Results.</span> Results on the <span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_bold">PointQA-General</span> dataset are shown in Table 3. The single highest accuracy is achieved using our three-stream approach with the MCAN-based model. Across all models, the three-stream approach outperforms any of the ablations that use one (<em id="S5.SS3.p3.1.3" class="ltx_emph ltx_font_italic">Q-Only</em>) or two streams (<em id="S5.SS3.p3.1.4" class="ltx_emph ltx_font_italic">Image+Q</em>, <em id="S5.SS3.p3.1.5" class="ltx_emph ltx_font_italic">Point+Q</em>). The high overall accuracy of our three-stream approach and its improvement over <em id="S5.SS3.p3.1.6" class="ltx_emph ltx_font_italic">Point+Q</em> indicates the benefit of adding the point as a separate stream and modifying the model cross-attention to create a rich set of contextual interactions between the streams. The <em id="S5.SS3.p3.1.7" class="ltx_emph ltx_font_italic">Image+Q</em> and <em id="S5.SS3.p3.1.8" class="ltx_emph ltx_font_italic">Q-Only</em> models perform no better than random chance since for each image-question pair in Visual7W we generate two questions with opposite answers.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">The MCAN-based method appears to achieve the strongest performance, even for the <em id="S5.SS3.p4.1.1" class="ltx_emph ltx_font_italic">Point+Q</em> ablation; this suggests it might make more effective use of the limited contextual information available in this setting. For both MCAN and Pythia, the two-stream approach makes less effective use of contextual information than three-stream. Surprisingly, the LXMERT model achieves higher accuracy on the two-stream approach. One possible reason is that the pooling strategy used by LXMERT only uses the language representation; it is possible that pooling from all three streams would improve performance of the three-stream method. Qualitative results are shown in Fig. <a href="#S5.F9" title="Figure 9 ‣ 5.3 PointQA-General evaluation ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.6.1.1" class="ltx_tr">
<td id="S5.T3.6.1.1.1" class="ltx_td"></td>
<th id="S5.T3.6.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Pythia</th>
<th id="S5.T3.6.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">MCAN</th>
<th id="S5.T3.6.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">LXMERT</th>
</tr>
<tr id="S5.T3.6.2.2" class="ltx_tr">
<td id="S5.T3.6.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Q-Only</td>
<td id="S5.T3.6.2.2.2" class="ltx_td ltx_align_left ltx_border_t">50.00</td>
<td id="S5.T3.6.2.2.3" class="ltx_td ltx_align_left ltx_border_t">50.00</td>
<td id="S5.T3.6.2.2.4" class="ltx_td ltx_align_left ltx_border_t">50.00</td>
</tr>
<tr id="S5.T3.6.3.3" class="ltx_tr">
<td id="S5.T3.6.3.3.1" class="ltx_td ltx_align_left">Image+Q</td>
<td id="S5.T3.6.3.3.2" class="ltx_td ltx_align_left">50.00</td>
<td id="S5.T3.6.3.3.3" class="ltx_td ltx_align_left">50.00</td>
<td id="S5.T3.6.3.3.4" class="ltx_td ltx_align_left">50.00</td>
</tr>
<tr id="S5.T3.6.4.4" class="ltx_tr">
<td id="S5.T3.6.4.4.1" class="ltx_td ltx_align_left">Point+Q</td>
<td id="S5.T3.6.4.4.2" class="ltx_td ltx_align_left">81.81</td>
<td id="S5.T3.6.4.4.3" class="ltx_td ltx_align_left">82.60</td>
<td id="S5.T3.6.4.4.4" class="ltx_td ltx_align_left">81.33</td>
</tr>
<tr id="S5.T3.6.5.5" class="ltx_tr">
<td id="S5.T3.6.5.5.1" class="ltx_td ltx_align_left ltx_border_t">Two-Stream</td>
<td id="S5.T3.6.5.5.2" class="ltx_td ltx_align_left ltx_border_t">77.84</td>
<td id="S5.T3.6.5.5.3" class="ltx_td ltx_align_left ltx_border_t">81.62</td>
<td id="S5.T3.6.5.5.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T3.6.5.5.4.1" class="ltx_text ltx_font_bold">82.41</span></td>
</tr>
<tr id="S5.T3.6.6.6" class="ltx_tr">
<td id="S5.T3.6.6.6.1" class="ltx_td ltx_align_left">Three-Stream</td>
<td id="S5.T3.6.6.6.2" class="ltx_td ltx_align_left"><span id="S5.T3.6.6.6.2.1" class="ltx_text ltx_font_bold">83.12</span></td>
<td id="S5.T3.6.6.6.3" class="ltx_td ltx_align_left"><span id="S5.T3.6.6.6.3.1" class="ltx_text ltx_font_bold">83.21</span></td>
<td id="S5.T3.6.6.6.4" class="ltx_td ltx_align_left">81.71</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.11.3.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Accuracy on the PointQA-General dataset<span id="S5.T3.4.2.2" class="ltx_text ltx_font_medium">. We evaluate the following methods and baselines: (1) Three-Stream and Two-Stream as described in Sec <a href="#S5.SS2" title="5.2 PointQA-General models ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, (2) <em id="S5.T3.4.2.2.1" class="ltx_emph ltx_font_italic">Q-only</em> relies only on the language of the question; (3) <em id="S5.T3.4.2.2.2" class="ltx_emph ltx_font_italic">Image+Q</em> provides the question and visual features <math id="S5.T3.3.1.1.m1.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}\}" display="inline"><semantics id="S5.T3.3.1.1.m1.1b"><mrow id="S5.T3.3.1.1.m1.1.1.1" xref="S5.T3.3.1.1.m1.1.1.2.cmml"><mo stretchy="false" id="S5.T3.3.1.1.m1.1.1.1.2" xref="S5.T3.3.1.1.m1.1.1.2.cmml">{</mo><msub id="S5.T3.3.1.1.m1.1.1.1.1" xref="S5.T3.3.1.1.m1.1.1.1.1.cmml"><mi id="S5.T3.3.1.1.m1.1.1.1.1.2" xref="S5.T3.3.1.1.m1.1.1.1.1.2.cmml">𝐯</mi><mi id="S5.T3.3.1.1.m1.1.1.1.1.3" xref="S5.T3.3.1.1.m1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S5.T3.3.1.1.m1.1.1.1.3" xref="S5.T3.3.1.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.m1.1c"><set id="S5.T3.3.1.1.m1.1.1.2.cmml" xref="S5.T3.3.1.1.m1.1.1.1"><apply id="S5.T3.3.1.1.m1.1.1.1.1.cmml" xref="S5.T3.3.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.T3.3.1.1.m1.1.1.1.1.1.cmml" xref="S5.T3.3.1.1.m1.1.1.1.1">subscript</csymbol><ci id="S5.T3.3.1.1.m1.1.1.1.1.2.cmml" xref="S5.T3.3.1.1.m1.1.1.1.1.2">𝐯</ci><ci id="S5.T3.3.1.1.m1.1.1.1.1.3.cmml" xref="S5.T3.3.1.1.m1.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.m1.1d">\{\mathbf{v}_{j}\}</annotation></semantics></math> corresponding to the entire image, and (4) <em id="S5.T3.4.2.2.3" class="ltx_emph ltx_font_italic">Point+Q</em> provides the question and only the features <math id="S5.T3.4.2.2.m2.1" class="ltx_Math" alttext="\{\mathbf{v}_{j}^{pt}\}" display="inline"><semantics id="S5.T3.4.2.2.m2.1b"><mrow id="S5.T3.4.2.2.m2.1.1.1" xref="S5.T3.4.2.2.m2.1.1.2.cmml"><mo stretchy="false" id="S5.T3.4.2.2.m2.1.1.1.2" xref="S5.T3.4.2.2.m2.1.1.2.cmml">{</mo><msubsup id="S5.T3.4.2.2.m2.1.1.1.1" xref="S5.T3.4.2.2.m2.1.1.1.1.cmml"><mi id="S5.T3.4.2.2.m2.1.1.1.1.2.2" xref="S5.T3.4.2.2.m2.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S5.T3.4.2.2.m2.1.1.1.1.2.3" xref="S5.T3.4.2.2.m2.1.1.1.1.2.3.cmml">j</mi><mrow id="S5.T3.4.2.2.m2.1.1.1.1.3" xref="S5.T3.4.2.2.m2.1.1.1.1.3.cmml"><mi id="S5.T3.4.2.2.m2.1.1.1.1.3.2" xref="S5.T3.4.2.2.m2.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.T3.4.2.2.m2.1.1.1.1.3.1" xref="S5.T3.4.2.2.m2.1.1.1.1.3.1.cmml">​</mo><mi id="S5.T3.4.2.2.m2.1.1.1.1.3.3" xref="S5.T3.4.2.2.m2.1.1.1.1.3.3.cmml">t</mi></mrow></msubsup><mo stretchy="false" id="S5.T3.4.2.2.m2.1.1.1.3" xref="S5.T3.4.2.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.2.2.m2.1c"><set id="S5.T3.4.2.2.m2.1.1.2.cmml" xref="S5.T3.4.2.2.m2.1.1.1"><apply id="S5.T3.4.2.2.m2.1.1.1.1.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.T3.4.2.2.m2.1.1.1.1.1.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1">superscript</csymbol><apply id="S5.T3.4.2.2.m2.1.1.1.1.2.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.T3.4.2.2.m2.1.1.1.1.2.1.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1">subscript</csymbol><ci id="S5.T3.4.2.2.m2.1.1.1.1.2.2.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1.2.2">𝐯</ci><ci id="S5.T3.4.2.2.m2.1.1.1.1.2.3.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1.2.3">𝑗</ci></apply><apply id="S5.T3.4.2.2.m2.1.1.1.1.3.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1.3"><times id="S5.T3.4.2.2.m2.1.1.1.1.3.1.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1.3.1"></times><ci id="S5.T3.4.2.2.m2.1.1.1.1.3.2.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1.3.2">𝑝</ci><ci id="S5.T3.4.2.2.m2.1.1.1.1.3.3.cmml" xref="S5.T3.4.2.2.m2.1.1.1.1.3.3">𝑡</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.2.2.m2.1d">\{\mathbf{v}_{j}^{pt}\}</annotation></semantics></math> for the region proposals containing the point (for Pythia this is the local-only model of Sec. <a href="#S3" title="3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Note that all these baselines are outperformed by our proposed models which capture local and global contextual information, and that the highest accuracy is achieved with our three-stream approach on the MCAN model.</span></span></figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2011.13681/assets/qualgeneral7.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.4.2" class="ltx_text" style="font-size:90%;">Examples in the <span id="S5.F9.4.2.1" class="ltx_text ltx_font_bold">PointQA-General</span> test-set where our three-stream approach (w/ Pythia) is correct and the Point+Q ablation does not have contextual information.</span></figcaption>
</figure>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">Using Image Context.</span> Certain questions in the <span id="S5.SS3.p5.1.2" class="ltx_text ltx_font_bold">PointQA-General</span> Dataset require reasoning about the image context beyond a local region around the point, such as questions involving comparison (e.g. “Is this zebra the <em id="S5.SS3.p5.1.3" class="ltx_emph ltx_font_italic">most</em> obstructed from view?”) or directional reasoning (“Are these crayons on a table <em id="S5.SS3.p5.1.4" class="ltx_emph ltx_font_italic">near</em> a cat?”). Viewing only the object being directly pointed to is insufficient to answer these questions. The accuracy improvement of the three-stream model over the <em id="S5.SS3.p5.1.5" class="ltx_emph ltx_font_italic">Point+Q</em> ablation is particularly significant for such questions containing comparison/directional words; e.g. for questions with the word ‘farthest’, the accuracy difference of the two models (w/ Pythia) is 3.66% vs. 1.81% overall. This result holds strongly across several such words, suggesting that the improved performance of the three-stream model is due to its ability to effectively incorporate the broader image context. Examples in Fig. <a href="#S5.F9" title="Figure 9 ‣ 5.3 PointQA-General evaluation ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> also indicate this effect.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Comparison Across Three Settings</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Having examined three settings, we take the opportunity to briefly articulate two key features that emerge consistently in the PointQA-task: (1) the need for visual grounding, and (2) the challenge of reasoning jointly around the point input and across the whole image. We note that these features in combination make PointQA a challenging task for existing VQA systems; we have already argued for its importance to human-machine communication.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Need for Visual Grounding.</span> Despite the presence of the point to guide attention, the model must still ground the text in the image (a key challenge in VQA). To verify this, we removed the subject from all questions across the three datasets (e.g. “What color is <em id="S6.p2.1.2" class="ltx_emph ltx_font_italic">this</em> shirt?” becomes “What color is <em id="S6.p2.1.3" class="ltx_emph ltx_font_italic">this</em>?”), to test if the model indeed benefits from grounding the object word in the image, when given the point input. For PointQA-General, the accuracy (w/ Pythia) drops most significantly from 83.1% to 75.1%; the accuracy for PointQA-LookTwice drops from 62.8% to 56.5% (Table <a href="#S4.T2" title="Table 2 ‣ 4.3 PointQA-LookTwice evaluation ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), and even for the simpler PointQA-Local setting accuracy drops from 75.0% to 73.5%. Moreover, questions in PointQA-General often refer to objects beyond the one directly pointed to, necessitating grounding of those objects in the image (e.g. for the question “Is this person the farthest from the <em id="S6.p2.1.4" class="ltx_emph ltx_font_italic">yellow box</em>?”, the ‘yellow box’ must be grounded in the image). In a small sample of 100 questions from PointQA-General more than half (55) have this characteristic. Thus visual grounding is a necessary feature of the PointQA task.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Reasoning Beyond the Point Location.</span> For an approximate comparison of accuracy on the three settings we limit each setting to the two most common answers (‘white and ’black’ for PointQA-Local, ’1’ and ’2’ for PointQA-LookTwice) and standardize the model (Pythia three-stream, Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). PointQA-Local expectedly has the highest accuracy at 90.7%, while PointQA-LookTwice has a lower accuracy of 68.1%. This gap partially suggests a key challenge of point-input questions is reasoning about the direct point location <em id="S6.p3.1.2" class="ltx_emph ltx_font_italic">jointly with</em> the surrounding image context (assessed in the counting questions in PointQA-LookTwice). This is further evidenced by examining those questions in PointQA-General which similarly require contextual reasoning. Accuracy across the comparison words we consider in Sec. <a href="#S5.SS3" title="5.3 PointQA-General evaluation ‣ 5 PointQA-General: generalized reasoning from a point input ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a> is 82.2%, lower than the overall accuracy of 83.1%. Thus reasoning beyond the immediate point location using the image context is another key challenge of the PointQA task, unique from standard VQA.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In summary, we introduced three novel types of visual questions <em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">requiring</em> a spatial point input for disambiguation. For each question type we created a benchmark dataset and model design and performed extensive analysis. We hope our work inspires further research in this space of questions.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgements</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This work is partially supported by
Samsung and by the Princeton SEAS Project-X Funding
Award. Thank you to Karthik Narasimhan, Zeyu Wang,
Felix Yu, Zhiwei Deng, and Deniz Oktay for helpful discussions and feedback on this work. We would also like to thank Sunnie Kim, Zeyu Wang, Sharon Zhang, Angelina Wang, Nicole Meister, Dora Zhao, Ozge Yalcinkaya, Vikram Ramaswamy, and Anat Kleiman for participating in the human evaluations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Don’t Just Assume; Look and Answer: Overcoming Priors for
Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision
(ICCV)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">What’s the point: Semantic segmentation with point supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Material recognition in the wild with the materials in context
database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and
Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Embodied Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F.
Moura, Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Visual dialog.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">VQS: Linking Segmentations to Questions and Answers for
Supervised Attention in VQA and Question-Focused Semantic
Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision
(ICCV)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter
Fox, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Iqa: Visual question answering in interactive environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA Matter: Elevating the Role of Image
Understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Michael Hild, Motonobu Hashimoto, and Kazunobu Yoshida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Object recognition via recognition of finger pointing actions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Image Analysis and Processing</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Drew A. Hudson and Christopher D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">GQA: A New Dataset for Real-World Visual Reasoning
and Compositional Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Suyog Dutt Jain and Kristen Grauman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Click carving: Segmenting objects in video with point clicks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI Conference on Human Computation and Crowdsourcing
(HCOMP)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">In defense of grid features for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Visual Genome: Connecting Language and Vision Using
Crowdsourced Dense Image Annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath
Hariharan, and Serge J. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, pages 936–944, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural
Information Processing Systems</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, volume 32. Curran Associates, Inc., 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Bertram F Malle, Louis J Moses, and Dare A Baldwin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Intentions and intentionality: Foundations of social cognition</span><span id="bib.bib19.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">MIT press, 2001.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
David Merrill and Pattie Maes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Augmenting looking, pointing and reaching gestures to enhance the
searching and browsing of physical objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Pervasive Computing</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">. 2007.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
John Oates and Andrew Grayson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Cognitive and language development in children</span><span id="bib.bib21.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">Blackwell; Open University Press, 2004.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Dim P. Papadopoulos, Alasdair D. F. Clarke, Frank Keller, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Training object class detectors from eye tracking data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference of Computer Vision (ECCV)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele,
Trevor Darrell, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Multimodal Explanations: Justifying Decisions and Pointing to
the Evidence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Syed Shaukat Raza Abidi, MaryAnn Williams, and Benjamin Johnston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Human pointing as a robot directive.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Human-robot interaction</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
39:1137–1149, 2015.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Allison Sauppe and Bilge Mutlu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Robot deictics: how gesture and context shape referential
communication.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Human-Robot Interaction</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Lxmert: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Deep modular co-attention networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Yu Jiang*, Vivek Natarajan*, Xinlei Chen*, Marcus Rohrbach, Dhruv Batra,
and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Pythia v0.1: the winning entry to the vqa challenge 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Yundong Zhang, Juan Carlos Niebles, and Alvaro Soto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Interpretable Visual Question Answering by Visual Grounding
From Attention Supervision Mining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Winter Conference on Applications of Computer
Vision (WACV)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Visual7w: Grounded Question Answering in Images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Human Evaluations</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We conduct human evaluations of the <span id="A1.p1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span>, <span id="A1.p1.1.2" class="ltx_text ltx_font_bold">PointQA-LookTwice</span>, and <span id="A1.p1.1.3" class="ltx_text ltx_font_bold">PointQA-General</span> datasets to check quality and ensure the reliability of our data collection pipeline. Our methodology is as follows. We select 100 questions at random from each of the test subsets of the three datasets. For each dataset, we ask three volunteers to answer the 100 questions sampled from that dataset; thus our study consists of a total of 9 participants distributed evenly across the three datasets. We send each participant a Jupyter notebook displaying the set of 100 questions and provide the following instructions for each question:</p>
</div>
<div id="A1.p2" class="ltx_para">
<blockquote id="A1.p2.1" class="ltx_quote ltx_displayquote">
<p id="A1.p2.1.1" class="ltx_p"><span id="A1.p2.1.1.1" class="ltx_text ltx_inline-quote">Please view the image and the small red point in the image, and answer the question to the best of your ability. The red point indicates what the question is asking about. Type in the number corresponding to the answer. Click [Enter] to move on to the next question.</span></p>
</blockquote>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">We received 300 responses per dataset (three annotators each for the 100 questions). We record the overall human accuracy (with the denominator being the 300 responses) and for each incorrect response, the potential reason to understand common failure modes in our datasets.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>PointQA-General</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We begin with the main benchmark we introduce in this paper, the <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_bold">PointQA-General</span> Dataset of 319,300 questions across 25,420 images. Human accuracy across the 100-question subset (300 annotator answers) is <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_bold">90.7%</span>. This high accuracy indicates that our question construction and point generation process yields sensible point-input questions that can be reasonably answered by humans. The accuracy of the three-stream model w/ Pythia (Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) on this 100-question subset is 82.0%; the substantial human-machine performance gap indicates that the <span id="A1.SS1.p1.1.3" class="ltx_text ltx_font_bold">PointQA-General</span> dataset is a challenging benchmark for point-input VQA and there is still scope for further model development in this question space.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">For the questions in this subset where human annotators disagreed with our dataset answers, we analyze and categorize the reasons for disagreement. The most common are:</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">“Other reasonable answer” (4.0%). Another answer in the dataset - in this case <em id="A1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">the</em> other answer since yes/no questions - was a reasonable choice for the question. These correspond to questions in Visual7W that are challenging for human annotators.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">“Point not on object” (2.3%). The location of the point is incorrect and the point does not refer to the object being asked about. The fact that this accounts for a very small fraction of errors confirms that drawing the point at the center of the bounding box is a reasonable approach overall.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">“Wrong attention” (1.3%). The human annotator pays attention to the wrong object when answering the question. This sometimes occurs when a bounding box answer that is incorrect in Visual7w refers to another object than is asked about in the question, so the annotator pays attention to the correct bounding box. However again this accounts for a small proportion of errors in the dataset.</p>
</div>
</li>
</ol>
<p id="A1.SS1.p2.2" class="ltx_p">The remaining disagreements are “question doesn’t make sense” (1.0%), no obvious reason (0.3%) and obstruction (0.3%).</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>PointQA-Local</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Human accuracy on the 100 question subset of the <span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> Dataset is 75.7%; this indicates that our questions can be reasonably answered by human annotators. By comparison the accuracy of the <span id="A1.SS2.p1.1.2" class="ltx_text ltx_font_bold">PointQA-Local</span> model (Section <a href="#S3.SS2" title="3.2 PointQA-Local models ‣ 3 PointQA-Local: reasoning about a region ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) on this question subset is actually 82.0%; this can be reasoned by the fact that annotators are presented with a larger set of answers (20) in <span id="A1.SS2.p1.1.3" class="ltx_text ltx_font_bold">PointQA-Local</span> and when annotating quickly can often choose answers that are reasonable but ‘less correct’ (as discussed below). By contrast in the more robust <span id="A1.SS2.p1.1.4" class="ltx_text ltx_font_bold">PointQA-General</span> setting the annotators have fewer answer choices and the human-machine gap is a result of the complexity of the question language and higher-level reasoning involved.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">We further analyze the most common form of errors on the <span id="A1.SS2.p2.1.1" class="ltx_text ltx_font_bold">PointQA-Local</span> Dataset. Similar to the <span id="A1.SS2.p2.1.2" class="ltx_text ltx_font_bold">PointQA-General</span> Dataset, the human errors come from “other reasonable answer” (12.3%), “ambiguous point” (4.3%), “obstruction” (3.3%), “wrong attention” (2.3%), “answer misannotation (1.0%) and “no obvious answer” (1.0%). “Ambiguous point” refers to cases where it is ambiguous which object is being referred to by the point. “Obstruction” refers to cases where the object in question is obstructed by another object in the image or the point. Qualitative examples are shown in Fig. <a href="#A1.F10" title="Figure 10 ‣ A.2 PointQA-Local ‣ Appendix A Human Evaluations ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. The prevalence of the “other reasonable answer” category indicates that Visual Genome annotations from which we derived our questions can be accurate but incomplete (for example in Fig. <a href="#A1.F10" title="Figure 10 ‣ A.2 PointQA-Local ‣ Appendix A Human Evaluations ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> on the left, the coat is most accurately labeled gray and not brown, although both labels could be reasonably included in the annotations).</p>
</div>
<figure id="A1.F10" class="ltx_figure"><img src="/html/2011.13681/assets/figs/LocalExamples.png" id="A1.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="303" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A1.F10.4.2" class="ltx_text" style="font-size:90%;">Examples of common failure modes in the <span id="A1.F10.4.2.1" class="ltx_text ltx_font_bold">PointQA-Local</span> Dataset.</span></figcaption>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>PointQA-LookTwice</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">Human accuracy on the 100-question subset of the <span id="A1.SS3.p1.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> Dataset is 79.3%; by comparison the global model of Sec. <a href="#S4.SS2" title="4.2 PointQA-LookTwice model ‣ 4 PointQA-LookTwice: reasoning about a local region in the broader image context ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> achieves 77.0%. The most common failure modes are “answer misannotation” (7.7%; i.e., the answer in the dataset is incorrect), “variable local-to-global reasoning” (6.7%; i.e., the human annotator considered the object pointed to as more generic or specific than intended in the question, and thus over/under-counted), “other instances hard to see” (3.3%), “wrong attention” (1.3%), “ambiguous point” (1.0%), and “no obvious reason” (0.7%). “Variable local-to-global reasoning” is an important failure mode since replacing the object name with a supercategory (e.g. “these objects”) might cause the annotator to count incorrectly; however, the high overall accuracy indicates this is a not a widespread issue. Qualitative examples are shown in Fig. <a href="#A1.F11" title="Figure 11 ‣ A.3 PointQA-LookTwice ‣ Appendix A Human Evaluations ‣ Point and Ask: Incorporating Pointing into Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. In the study, we provided the annotators with possible answer choices of 1 through 7; when evaluating the responses, we narrowed the choices down to 1, 2, <math id="A1.SS3.p1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="A1.SS3.p1.1.m1.1a"><mo id="A1.SS3.p1.1.m1.1.1" xref="A1.SS3.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.1.m1.1b"><gt id="A1.SS3.p1.1.m1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.1.m1.1c">&gt;</annotation></semantics></math>2.</p>
</div>
<figure id="A1.F11" class="ltx_figure"><img src="/html/2011.13681/assets/figs/LookExamples.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="319" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A1.F11.4.2" class="ltx_text" style="font-size:90%;">Examples of common failure modes in the <span id="A1.F11.4.2.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> Dataset.</span></figcaption>
</figure>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p">Finally, we note that questions were unintentionally drawn from the training set of the <span id="A1.SS3.p2.1.1" class="ltx_text ltx_font_bold">PointQA-LookTwice</span> Dataset. This does not fundamentally affect the quality of our human studies. The model accuracy number of 77.0% is when excluding these questions from the training set, but could still be inflated. One thing to note is that the source of human disagreement resulting from misannotation in the dataset would be substantially lower without questions from the training set (since the test set consists exclusively of human-written questions).</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2011.13680" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2011.13681" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2011.13681">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2011.13681" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2011.13683" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 13:39:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
